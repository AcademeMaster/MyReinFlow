import torch
import torch.nn as nn
import torch.nn.functional as F
from torchcfm.conditional_flow_matching import ConditionalFlowMatcher
from typing import Dict


class TimeConditionedFlowModel(nn.Module):
    """带时间条件约束的流匹配模型"""
    
    def __init__(self, obs_dim, action_dim, config):
        """
        参数:
            obs_dim: 观测维度
            action_dim: 动作维度
            config: Config对象
        """
        super().__init__()
        self.config = config
        
        # 时间特征嵌入
        self.time_embed = nn.Sequential(
            nn.Linear(1, config.time_dim),
            nn.SiLU(),
            nn.Linear(config.time_dim, config.time_dim)
        )
        
        # 观测特征处理
        self.obs_embed = nn.Sequential(
            nn.Linear(obs_dim, config.hidden_dim),
            nn.SiLU(),
            nn.Linear(config.hidden_dim, config.hidden_dim)
        )
        
        # 噪声特征处理
        self.noise_embed = nn.Sequential(
            nn.Linear(action_dim, config.hidden_dim),
            nn.SiLU(),
            nn.Linear(config.hidden_dim, config.hidden_dim)
        )
        
        # 联合处理模块
        self.joint_processor = nn.Sequential(
            nn.Linear(config.hidden_dim * 2 + config.time_dim, config.hidden_dim * 2),
            nn.SiLU(),
            nn.Linear(config.hidden_dim * 2, config.hidden_dim),
            nn.SiLU(),
            nn.Linear(config.hidden_dim, action_dim)
        )
    
    def forward(self, obs, t, noise):
        """
        前向传播
        参数:
            obs: 观测张量 [B, obs_dim]
            t: 时间步张量 [B]
            noise: 噪声张量 [B, pred_horizon, action_dim]
        
        返回:
            速度场 [B, pred_horizon, action_dim]
        """
        # 获取模型所在的设备
        device = next(self.parameters()).device
        
        # 将输入张量移动到模型设备
        obs = obs.to(device)
        t = t.to(device)
        noise = noise.to(device)
        
        # 嵌入时间
        t_emb = self.time_embed(t.unsqueeze(-1).float())  # [B, time_dim]
        
        # 嵌入观测
        obs_emb = self.obs_embed(obs)  # [B, hidden_dim]
        
        # 嵌入噪声
        # 对每个时间步独立处理噪声
        B, H, A = noise.shape
        noise_emb = self.noise_embed(noise.view(B * H, A))  # [B*H, hidden_dim]
        noise_emb = noise_emb.view(B, H, -1)  # [B, H, hidden_dim]
        
        # 合并特征
        obs_emb = obs_emb.unsqueeze(1).expand(-1, H, -1)  # [B, H, hidden_dim]
        t_emb = t_emb.unsqueeze(1).expand(-1, H, -1)  # [B, H, time_dim]
        
        combined = torch.cat([obs_emb, noise_emb, t_emb], dim=-1)  # [B, H, hidden_dim*2+time_dim]
        
        # 预测速度场
        velocity = self.joint_processor(combined)  # [B, H, action_dim]
        
        return velocity


class FlowPolicyAgent:
    """流策略代理"""
    
    def __init__(self, obs_dim: int, action_dim: int, config: "Config"):
        """
        初始化流策略代理
        
        参数:
            obs_dim: 观测维度
            action_dim: 动作维度
            config: 配置对象
        """
        from config import Config  # 避免循环导入
        self.config: Config = config
        self.model = TimeConditionedFlowModel(obs_dim, action_dim, config)
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.learning_rate)
        self.flow_matcher = ConditionalFlowMatcher(sigma=config.sigma)

    @torch.no_grad()
    def predict_action_chunk(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        预测动作块
        
        参数:
            batch: 包含观测数据的批次字典
            
        返回:
            预测的动作张量
        """
        self.model.eval()
        # 获取模型所在的设备
        device = next(self.model.parameters()).device
        
        observations = batch["observations"].to(device)  # [B, obs_horizon, obs_dim]
        
        # 使用最新的观测作为条件
        obs_cond = observations[:, -1, :]  # [B, obs_dim]
        
        # 迭代求解器生成动作
        noise = torch.randn(observations.size(0), self.config.pred_horizon, self.config.action_dim).to(device)
        
        for step in range(self.config.inference_steps):
            t_val = torch.full((observations.size(0),), step / self.config.inference_steps, device=device, dtype=torch.float32)
            update = self.model(obs_cond, t_val, noise)
            noise = noise + (1.0 / self.config.inference_steps) * update
            
        return noise

    @torch.no_grad()
    def select_action(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Select a single action given environment observations.

        This method handles caching a history of observations and an action trajectory generated by the
        underlying diffusion model. Here's how it works:
          - `n_obs_steps` steps worth of observations are cached (for the first steps, the observation is
            copied `n_obs_steps` times to fill the cache).
          - The diffusion model generates `horizon` steps worth of actions.
          - `n_action_steps` worth of actions are actually kept for execution, starting from the current step.
        Schematically this looks like:
            ----------------------------------------------------------------------------------------------
            (legend: o = n_obs_steps, h = horizon, a = n_action_steps)
            |timestep            | n-o+1 | n-o+2 | ..... | n     | ..... | n+a-1 | n+a   | ..... | n-o+h |
            |observation is used | YES   | YES   | YES   | YES   | NO    | NO    | NO    | NO    | NO    |
            |action is generated | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   |
            |action is used      | NO    | NO    | NO    | YES   | YES   | YES   | NO    | NO    | NO    |
            ----------------------------------------------------------------------------------------------
        Note that this means we require: `n_action_steps <= horizon - n_obs_steps + 1`. Also, note that
        "horizon" may not the best name to describe what the variable actually means, because this period is
        actually measured from the first observation which (if `n_obs_steps` > 1) happened in the past.
        """
        # NOTE: for offline evaluation, we have action in the batch, so we need to pop it out
        # 目前实现仅返回预测的动作块的第一个动作
        action_chunk = self.predict_action_chunk(batch)
        return action_chunk[:, 0, :]  # 返回第一个动作

    def forward(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        前向传播，计算流匹配损失
        
        参数:
            batch: 包含观测和动作的批次字典
            
        返回:
            计算的损失值
        """
        # 获取模型所在的设备
        device = next(self.model.parameters()).device
        
        # 将输入数据移动到模型设备
        observations = batch["observations"].to(device)  # [B, obs_horizon, obs_dim]
        actions = batch["actions"].to(device)  # [B, pred_horizon, action_dim]
        
        # 流匹配 - 使用设备一致的噪声生成
        noise = torch.randn_like(actions, device=device)
        flow_data = self.flow_matcher.sample_location_and_conditional_flow(noise, actions)
        t, xt, ut = flow_data[:3]

        
        # 使用最新的观测作为条件
        obs_cond = observations[:, -1, :]  # [B, obs_dim]
        
        # 预测速度场
        vt = self.model(obs_cond, t, xt)  # [B, pred_horizon, action_dim]
        
        # 计算损失
        loss = F.mse_loss(vt, ut)
        return loss



