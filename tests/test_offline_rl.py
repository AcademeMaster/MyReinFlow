#!/usr/bin/env python3
"""
ç¦»çº¿å¼ºåŒ–å­¦ä¹ åŠŸèƒ½æµ‹è¯•è„šæœ¬
"""

import sys
import os
import torch
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from offlineflowrl.meanflow_ql import (
    Config, 
    ImprovedMeanFlowPolicyAgent, 
    DoubleCriticObsAct, 
    ConservativeMeanFQLModel,
    ReplayBuffer
)


def test_model_creation():
    """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
    print("æµ‹è¯•æ¨¡å‹åˆ›å»º...")
    
    # åˆ›å»ºé…ç½®
    config = Config()
    config.pred_horizon = 5
    config.hidden_dim = 128
    config.time_dim = 32
    
    # è®¾ç½®ç»´åº¦
    obs_dim = 10
    action_dim = 4
    action_horizon = 5
    
    try:
        # åˆ›å»ºç»„ä»¶
        actor = ImprovedMeanFlowPolicyAgent(obs_dim, action_dim, config)
        critic = DoubleCriticObsAct(obs_dim, action_dim, config.hidden_dim, action_horizon)
        model = ConservativeMeanFQLModel(actor, critic, config)
        
        print("âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return False


def test_forward_pass():
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("æµ‹è¯•å‰å‘ä¼ æ’­...")
    
    # åˆ›å»ºé…ç½®
    config = Config()
    config.pred_horizon = 5
    config.hidden_dim = 128
    
    # è®¾ç½®ç»´åº¦
    obs_dim = 10
    action_dim = 4
    action_horizon = 5
    batch_size = 2  # å‡å°æ‰¹æ¬¡å¤§å°ä»¥é¿å…é—®é¢˜
    
    # ç¡®å®šè®¾å¤‡
    device = torch.device(config.device)
    
    try:
        # åˆ›å»ºç»„ä»¶
        actor = ImprovedMeanFlowPolicyAgent(obs_dim, action_dim, config)
        critic = DoubleCriticObsAct(obs_dim, action_dim, config.hidden_dim, action_horizon)
        model = ConservativeMeanFQLModel(actor, critic, config)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®å¹¶ç§»åˆ°æ­£ç¡®è®¾å¤‡
        obs = torch.randn(batch_size, obs_dim).to(device)
        actions = torch.randn(batch_size, action_horizon, action_dim).to(device)
        next_obs = torch.randn(batch_size, obs_dim).to(device)
        rewards = torch.randn(batch_size, action_horizon).to(device)
        terminated = torch.zeros(batch_size, 1).to(device)
        
        # æµ‹è¯•critic loss
        critic_loss, critic_info = model.loss_critic(
            obs, actions, next_obs, rewards, terminated, config.gamma
        )
        
        # æµ‹è¯•actor loss - åˆ›å»ºæ­£ç¡®çš„æ‰¹æ¬¡æ•°æ®
        batch = {
            "observations": obs,
            "actions": actions
        }
        actor_loss, actor_info = model.loss_actor(obs, actions)
        
        print("âœ“ å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ")
        print(f"  Critic loss: {critic_loss.item():.6f}")
        print(f"  Actor loss: {actor_loss.item():.6f}")
        return True
    except Exception as e:
        print(f"âœ— å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_replay_buffer():
    """æµ‹è¯•ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    print("æµ‹è¯•ç»éªŒå›æ”¾ç¼“å†²åŒº...")
    
    try:
        # åˆ›å»ºç¼“å†²åŒº
        buffer = ReplayBuffer(capacity=1000)
        
        # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
        for i in range(5):
            transition = {
                "observations": np.random.randn(1, 10).astype(np.float32),
                "actions": np.random.randn(1, 5, 4).astype(np.float32),
                "next_observations": np.random.randn(1, 10).astype(np.float32),
                "rewards": np.random.randn(1, 5).astype(np.float32),
                "terminated": np.random.choice([0, 1], size=(1, 1)).astype(np.float32)
            }
            buffer.add(transition)
        
        # æµ‹è¯•é‡‡æ ·
        batch = buffer.sample(3)
        
        print("âœ“ ç»éªŒå›æ”¾ç¼“å†²åŒºæµ‹è¯•æˆåŠŸ")
        print(f"  ç¼“å†²åŒºå¤§å°: {len(buffer)}")
        print(f"  é‡‡æ ·æ‰¹æ¬¡è§‚æµ‹å½¢çŠ¶: {batch['observations'].shape}")
        return True
    except Exception as e:
        print(f"âœ— ç»éªŒå›æ”¾ç¼“å†²åŒºæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_agent_sampling():
    """æµ‹è¯•æ™ºèƒ½ä½“é‡‡æ ·"""
    print("æµ‹è¯•æ™ºèƒ½ä½“é‡‡æ ·...")
    
    # åˆ›å»ºé…ç½®
    config = Config()
    config.pred_horizon = 5
    config.hidden_dim = 128
    
    # è®¾ç½®ç»´åº¦
    obs_dim = 10
    action_dim = 4
    
    try:
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = ImprovedMeanFlowPolicyAgent(obs_dim, action_dim, config)
        
        # åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡ - ä½¿ç”¨æ­£ç¡®çš„ç»´åº¦ [batch_size, obs_horizon, obs_dim]
        batch = {
            "observations": torch.randn(2, 1, obs_dim)  # [batch_size, obs_horizon, obs_dim]
        }
        
        # æµ‹è¯•åŠ¨ä½œé¢„æµ‹
        actions = agent.predict_action_chunk(batch, n_steps=5)
        
        print("âœ“ æ™ºèƒ½ä½“é‡‡æ ·æµ‹è¯•æˆåŠŸ")
        print(f"  åŠ¨ä½œå½¢çŠ¶: {actions.shape}")
        return True
    except Exception as e:
        print(f"âœ— æ™ºèƒ½ä½“é‡‡æ ·æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹ç¦»çº¿å¼ºåŒ–å­¦ä¹ åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
    torch.manual_seed(42)
    np.random.seed(42)
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    tests = [
        test_model_creation,
        test_agent_sampling,
        test_replay_buffer,
        test_forward_pass
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
        print()
    
    print("=" * 50)
    print(f"æµ‹è¯•å®Œæˆ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥!")
        return 1


if __name__ == "__main__":
    sys.exit(main())