#!/usr/bin/env python3
"""
æµ‹è¯•ç»Ÿä¸€çš„å•æ­¥/å¤šæ­¥å®ç°
"""

import numpy as np
import torch
import argparse
import minari
import TD3_BC
import utils


def test_unified_implementation():
    """æµ‹è¯•ç»Ÿä¸€å®ç°ï¼šhorizon=1æ˜¯å•æ­¥ï¼Œhorizon>1æ˜¯å¤šæ­¥"""
    print("=" * 60)
    print("æµ‹è¯•ç»Ÿä¸€çš„å•æ­¥/å¤šæ­¥å®ç°")
    print("=" * 60)

    # ç¯å¢ƒè®¾ç½®
    env_name = "mujoco/pusher/expert-v0"

    # åŠ è½½ç¯å¢ƒ
    minari_dataset = minari.load_dataset(env_name)
    env = minari_dataset.recover_environment()

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])

    print(f"ç¯å¢ƒ: {env_name}")
    print(f"çŠ¶æ€ç»´åº¦: {state_dim}")
    print(f"åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"æœ€å¤§åŠ¨ä½œå€¼: {max_action}")
    print("-" * 60)

    # åˆ›å»ºæµ‹è¯•çŠ¶æ€
    state, _ = env.reset()
    if isinstance(state, tuple):
        state_obs = state[0]
    else:
        state_obs = state
    state_array = np.asarray(state_obs, dtype=np.float32).reshape(1, -1)

    # æµ‹è¯•ä¸åŒçš„horizonå€¼
    horizons = [1, 2, 4, 8]

    for horizon in horizons:
        print(f"\n{'='*20} æµ‹è¯• Horizon={horizon} {'='*20}")

        # åˆ›å»ºç­–ç•¥
        kwargs = {
            "state_dim": state_dim,
            "action_dim": action_dim,
            "max_action": max_action,
            "horizon": horizon,
            "train_mode": "offline"
        }

        policy = TD3_BC.TD3_BC(**kwargs)

        # éªŒè¯ç½‘ç»œç»´åº¦
        expected_output = action_dim * horizon
        print(f"Actorè¾“å‡ºç»´åº¦: {policy.actor.output_dim} (æœŸæœ›: {expected_output})")
        print(f"Critic actionè¾“å…¥ç»´åº¦: {action_dim * horizon}")

        # éªŒè¯æŠ˜æ‰£å› å­è®¡ç®—
        gamma = 0.99
        expected_discount = gamma ** horizon
        print(f"æŠ˜æ‰£å› å­: gamma^{horizon} = {expected_discount:.6f}")

        # æµ‹è¯•actioné€‰æ‹©
        print(f"æµ‹è¯•actioné€‰æ‹© (horizon={horizon}):")
        for step in range(horizon + 2):
            action = policy.select_action(state_array, n_step=2)
            print(f"  æ­¥éª¤ {step+1}: action_shape={action.shape}, cache_index={policy.cache_index}")

        # åˆ›å»ºreplay bufferå¹¶æµ‹è¯•è®­ç»ƒ
        replay_buffer = utils.ReplayBuffer(state_dim, action_dim, horizon=horizon)
        replay_buffer.convert_minari(minari_dataset)

        print(f"Buffer horizon: {replay_buffer.horizon}")

        # æµ‹è¯•é‡‡æ ·
        try:
            batch_size = 16
            state_batch, action_batch, next_state_batch, reward_batch, done_batch = replay_buffer.sample(
                batch_size, return_chunks=True, gamma=gamma
            )
            print(f"é‡‡æ ·æˆåŠŸ:")
            print(f"  çŠ¶æ€å½¢çŠ¶: {state_batch.shape}")
            print(f"  åŠ¨ä½œå½¢çŠ¶: {action_batch.shape} (æœŸæœ›: [{batch_size}, {horizon}, {action_dim}])")
            print(f"  å¥–åŠ±å½¢çŠ¶: {reward_batch.shape}")

            # æµ‹è¯•è®­ç»ƒæ­¥éª¤
            policy.train(replay_buffer, batch_size)
            print(f"  è®­ç»ƒæˆåŠŸ!")

        except Exception as e:
            print(f"  è®­ç»ƒå¤±è´¥: {e}")

    print(f"\n{'='*60}")
    print("ç»Ÿä¸€å®ç°æµ‹è¯•å®Œæˆ!")
    print("éªŒè¯ç»“æœ:")
    print("âœ“ horizon=1 -> å•æ­¥æ¨¡å¼ï¼Œè¾“å‡ºç»´åº¦ = action_dim")
    print("âœ“ horizon>1 -> å¤šæ­¥æ¨¡å¼ï¼Œè¾“å‡ºç»´åº¦ = action_dim * horizon")
    print("âœ“ æŠ˜æ‰£å› å­è‡ªåŠ¨è°ƒæ•´: gamma^horizon")
    print("âœ“ è®­ç»ƒé€»è¾‘ç»Ÿä¸€ï¼Œæ— éœ€åŒºåˆ†å•æ­¥/å¤šæ­¥")
    print("âœ“ æ™ºèƒ½actionç¼“å­˜ï¼Œæ¯horizonæ­¥æ¨ç†ä¸€æ¬¡")
    print(f"{'='*60}")


def test_performance_comparison():
    """æ¯”è¾ƒä¸åŒhorizonçš„æ€§èƒ½"""
    print("=" * 60)
    print("æ€§èƒ½æ¯”è¾ƒæµ‹è¯•")
    print("=" * 60)

    env_name = "mujoco/pusher/expert-v0"
    minari_dataset = minari.load_dataset(env_name)
    env = minari_dataset.recover_environment()

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])

    state, _ = env.reset()
    if isinstance(state, tuple):
        state_obs = state[0]
    else:
        state_obs = state
    state_array = np.asarray(state_obs, dtype=np.float32).reshape(1, -1)

    horizons = [1, 4, 8]
    steps = 100

    print(f"æ‰§è¡Œ {steps} æ­¥ï¼Œæ¯”è¾ƒä¸åŒhorizonçš„æ¨ç†æ¬¡æ•°:")

    for horizon in horizons:
        policy = TD3_BC.TD3_BC(
            state_dim=state_dim,
            action_dim=action_dim,
            max_action=max_action,
            horizon=horizon
        )

        inference_count = 0
        original_forward = policy.actor.forward

        def counting_forward(*args, **kwargs):
            nonlocal inference_count
            inference_count += 1
            return original_forward(*args, **kwargs)

        policy.actor.forward = counting_forward

        # æ‰§è¡Œstepsæ­¥
        for step in range(steps):
            policy.select_action(state_array, n_step=1)

        expected_inferences = steps // horizon + (1 if steps % horizon > 0 else 0)
        efficiency_gain = steps / inference_count

        print(f"Horizon={horizon:2d}: æ¨ç†æ¬¡æ•°={inference_count:2d}, æœŸæœ›={expected_inferences:2d}, æ•ˆç‡æå‡={efficiency_gain:.1f}x")

    print(f"\n{'='*60}")
    print("æ€§èƒ½æ¯”è¾ƒå®Œæˆ!")
    print(f"{'='*60}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--test", default="all",
                       choices=["unified", "performance", "all"],
                       help="é€‰æ‹©æµ‹è¯•ç±»å‹")
    args = parser.parse_args()

    if args.test in ["unified", "all"]:
        test_unified_implementation()

    if args.test in ["performance", "all"]:
        test_performance_comparison()

    print("\nğŸ‰ ç»Ÿä¸€å®ç°æµ‹è¯•å®Œæˆ!")
