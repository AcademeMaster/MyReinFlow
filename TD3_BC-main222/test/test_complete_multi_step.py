#!/usr/bin/env python3
"""
æµ‹è¯•å¤šæ­¥actionå®ç°çš„è„šæœ¬
"""

import numpy as np
import torch
import argparse
import minari
import TD3_BC
import utils


def test_multi_step_implementation():
    """æµ‹è¯•å®Œæ•´çš„å¤šæ­¥actionå®ç°"""
    print("=" * 60)
    print("æµ‹è¯•å¤šæ­¥actionå®Œæ•´å®ç°")
    print("=" * 60)

    # ç¯å¢ƒè®¾ç½®
    env_name = "mujoco/pusher/expert-v0"
    horizon = 4

    # åŠ è½½ç¯å¢ƒ
    minari_dataset = minari.load_dataset(env_name)
    env = minari_dataset.recover_environment()

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])

    print(f"ç¯å¢ƒ: {env_name}")
    print(f"çŠ¶æ€ç»´åº¦: {state_dim}")
    print(f"åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"æœ€å¤§åŠ¨ä½œå€¼: {max_action}")
    print(f"Horizon: {horizon}")
    print("-" * 60)

    # åˆ›å»ºç­–ç•¥
    kwargs = {
        "state_dim": state_dim,
        "action_dim": action_dim,
        "max_action": max_action,
        "horizon": horizon,
        "train_mode": "offline"
    }

    policy = TD3_BC.TD3_BC(**kwargs)

    # éªŒè¯ç½‘ç»œç»´åº¦
    print("éªŒè¯ç½‘ç»œç»“æ„:")
    print(f"Actorè¾“å‡ºç»´åº¦: {policy.actor.output_dim} (åº”è¯¥æ˜¯ {action_dim * horizon})")
    print(f"Critic actionè¾“å…¥ç»´åº¦: {action_dim * horizon}")

    # æµ‹è¯•çŠ¶æ€
    state, _ = env.reset()
    if isinstance(state, tuple):
        state_obs = state[0]
    else:
        state_obs = state
    state_array = np.asarray(state_obs, dtype=np.float32).reshape(1, -1)

    print(f"æµ‹è¯•çŠ¶æ€å½¢çŠ¶: {state_array.shape}")

    # æµ‹è¯•Actorè¾“å‡º
    print("\næµ‹è¯•Actorå¤šæ­¥è¾“å‡º:")
    with torch.no_grad():
        state_tensor = torch.FloatTensor(state_array).to(policy.actor.device)
        multi_step_output = policy.actor(state_tensor, n_steps=5)
        print(f"åŸå§‹è¾“å‡ºå½¢çŠ¶: {multi_step_output.shape}")
        print(f"åº”è¯¥æ˜¯: [1, {action_dim * horizon}]")

        # æµ‹è¯•é‡å¡‘åŠŸèƒ½
        reshaped = policy.actor.reshape_multi_step_action(multi_step_output)
        print(f"é‡å¡‘åå½¢çŠ¶: {reshaped.shape}")
        print(f"åº”è¯¥æ˜¯: [1, {horizon}, {action_dim}]")

    # æµ‹è¯•ç²¾ç®€ç‰ˆselect_action
    print("\næµ‹è¯•ç²¾ç®€ç‰ˆselect_action:")
    for step in range(horizon + 2):  # å¤šæ‰§è¡Œå‡ æ­¥éªŒè¯ç¼“å­˜æœºåˆ¶
        print(f"æ­¥éª¤ {step + 1}:")
        action = policy.select_action(state_array, n_step=3)
        print(f"  è·å¾—actionå½¢çŠ¶: {action.shape}")
        print(f"  actionå€¼: {action[:3]}... (å‰3ä¸ªå€¼)")
        print(f"  ç¼“å­˜ç´¢å¼•: {policy.cache_index}")

    print("\n=" * 60)
    print("åŸºç¡€åŠŸèƒ½æµ‹è¯•å®Œæˆ!")
    print("=" * 60)


def test_multi_step_training():
    """æµ‹è¯•å¤šæ­¥è®­ç»ƒåŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•å¤šæ­¥è®­ç»ƒåŠŸèƒ½")
    print("=" * 60)

    env_name = "mujoco/pusher/expert-v0"
    horizon = 4
    batch_size = 16

    # åŠ è½½ç¯å¢ƒå’Œæ•°æ®
    minari_dataset = minari.load_dataset(env_name)
    env = minari_dataset.recover_environment()

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])

    # åˆ›å»ºç­–ç•¥å’Œbuffer
    kwargs = {
        "state_dim": state_dim,
        "action_dim": action_dim,
        "max_action": max_action,
        "horizon": horizon,
        "train_mode": "offline"
    }

    policy = TD3_BC.TD3_BC(**kwargs)
    replay_buffer = utils.ReplayBuffer(state_dim, action_dim, horizon=horizon)

    # åŠ è½½å°‘é‡æ•°æ®ç”¨äºæµ‹è¯•
    print("åŠ è½½æµ‹è¯•æ•°æ®...")
    replay_buffer.convert_minari(minari_dataset)
    print(f"Bufferå¤§å°: {replay_buffer.size}")

    # æµ‹è¯•æ ‡å‡†é‡‡æ ·
    print("\næµ‹è¯•æ ‡å‡†é‡‡æ ·:")
    try:
        state, action, next_state, reward, done = replay_buffer.sample(batch_size)
        print(f"âœ“ æ ‡å‡†é‡‡æ ·æˆåŠŸ:")
        print(f"  çŠ¶æ€å½¢çŠ¶: {state.shape}")
        print(f"  åŠ¨ä½œå½¢çŠ¶: {action.shape}")
        print(f"  å¥–åŠ±å½¢çŠ¶: {reward.shape}")
    except Exception as e:
        print(f"âœ— æ ‡å‡†é‡‡æ ·å¤±è´¥: {e}")

    # æµ‹è¯•å¤šæ­¥é‡‡æ ·
    print("\næµ‹è¯•å¤šæ­¥é‡‡æ ·:")
    try:
        state, action_chunks, next_state, multi_rewards, done = replay_buffer.sample(
            batch_size, return_chunks=True, gamma=0.99
        )
        print(f"âœ“ å¤šæ­¥é‡‡æ ·æˆåŠŸ:")
        print(f"  çŠ¶æ€å½¢çŠ¶: {state.shape}")
        print(f"  åŠ¨ä½œå—å½¢çŠ¶: {action_chunks.shape}")
        print(f"  å¤šæ­¥å¥–åŠ±å½¢çŠ¶: {multi_rewards.shape}")
        print(f"  å¤šæ­¥å¥–åŠ±èŒƒå›´: [{multi_rewards.min().item():.3f}, {multi_rewards.max().item():.3f}]")
    except Exception as e:
        print(f"âœ— å¤šæ­¥é‡‡æ ·å¤±è´¥: {e}")
        return

    # æµ‹è¯•è®­ç»ƒæ­¥éª¤
    print("\næµ‹è¯•è®­ç»ƒæ­¥éª¤:")
    try:
        print("æ‰§è¡Œå¤šæ­¥è®­ç»ƒ...")
        policy.train(replay_buffer, batch_size, use_multi_step=True)
        print("âœ“ å¤šæ­¥è®­ç»ƒæˆåŠŸ!")

        print("æ‰§è¡Œæ ‡å‡†è®­ç»ƒ...")
        policy.train(replay_buffer, batch_size, use_multi_step=False)
        print("âœ“ æ ‡å‡†è®­ç»ƒæˆåŠŸ!")

        print("è®­ç»ƒåŠŸèƒ½éªŒè¯å®Œæˆ!")

    except Exception as e:
        print(f"âœ— è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    print("\n=" * 60)
    print("å¤šæ­¥è®­ç»ƒæµ‹è¯•å®Œæˆ!")
    print("=" * 60)


def test_td_computation():
    """æµ‹è¯•TDè®¡ç®—æ˜¯å¦æ­£ç¡®å®ç°äº†r + gamma^h * target_Q"""
    print("=" * 60)
    print("æµ‹è¯•TDè®¡ç®—å…¬å¼")
    print("=" * 60)

    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
    batch_size = 4
    horizon = 3
    gamma = 0.99

    # æ¨¡æ‹Ÿå¥–åŠ±å’Œç›®æ ‡Qå€¼
    reward = torch.tensor([[1.0], [2.0], [0.5], [1.5]])
    target_Q = torch.tensor([[10.0], [8.0], [12.0], [9.0]])
    done = torch.tensor([[0.0], [0.0], [1.0], [0.0]])  # ç¬¬3ä¸ªepisodeç»“æŸ

    # è®¡ç®—æ ‡å‡†TDç›®æ ‡ (gamma^1)
    standard_discount = gamma
    standard_target = reward + (1.0 - done) * standard_discount * target_Q

    # è®¡ç®—å¤šæ­¥TDç›®æ ‡ (gamma^h)
    multi_step_discount = gamma ** horizon
    multi_step_target = reward + (1.0 - done) * multi_step_discount * target_Q

    print(f"æµ‹è¯•å‚æ•°:")
    print(f"  Horizon: {horizon}")
    print(f"  Gamma: {gamma}")
    print(f"  æ ‡å‡†æŠ˜æ‰£: {standard_discount:.4f}")
    print(f"  å¤šæ­¥æŠ˜æ‰£: {multi_step_discount:.4f}")

    print(f"\nå¥–åŠ±: {reward.flatten().tolist()}")
    print(f"ç›®æ ‡Q: {target_Q.flatten().tolist()}")
    print(f"Doneæ ‡å¿—: {done.flatten().tolist()}")

    print(f"\næ ‡å‡†TDç›®æ ‡: {standard_target.flatten().tolist()}")
    print(f"å¤šæ­¥TDç›®æ ‡: {multi_step_target.flatten().tolist()}")

    # è®¡ç®—å·®å¼‚
    difference = multi_step_target - standard_target
    print(f"å·®å¼‚: {difference.flatten().tolist()}")

    print("\néªŒè¯:")
    for i in range(batch_size):
        if done[i].item() == 1.0:
            expected = reward[i].item()  # episodeç»“æŸï¼Œåªæœ‰å¥–åŠ±
            print(f"  æ ·æœ¬{i+1} (episodeç»“æŸ): {multi_step_target[i].item():.3f} == {expected:.3f} âœ“")
        else:
            expected = reward[i].item() + multi_step_discount * target_Q[i].item()
            print(f"  æ ·æœ¬{i+1}: {multi_step_target[i].item():.3f} == {expected:.3f} âœ“")

    print("\n=" * 60)
    print("TDè®¡ç®—éªŒè¯å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--test", default="all",
                       choices=["implementation", "training", "td", "all"],
                       help="é€‰æ‹©æµ‹è¯•ç±»å‹")
    args = parser.parse_args()

    if args.test in ["implementation", "all"]:
        test_multi_step_implementation()

    if args.test in ["training", "all"]:
        test_multi_step_training()

    if args.test in ["td", "all"]:
        test_td_computation()

    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ! å¤šæ­¥actionç³»ç»Ÿå·²å°±ç»ª!")
