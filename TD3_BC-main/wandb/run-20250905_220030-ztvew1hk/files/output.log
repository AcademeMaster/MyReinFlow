开始纯在线训练...
Episode 1: Steps=100, Reward=-157.216, Buffer_size=100
Episode 2: Steps=100, Reward=-165.512, Buffer_size=200
Episode 3: Steps=100, Reward=-149.450, Buffer_size=300
Episode 4: Steps=100, Reward=-141.149, Buffer_size=400
Episode 5: Steps=100, Reward=-140.177, Buffer_size=500
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 5: -315.713
Episode 6: Steps=100, Reward=-152.812, Buffer_size=600
Episode 7: Steps=100, Reward=-145.883, Buffer_size=700
Episode 8: Steps=100, Reward=-161.255, Buffer_size=800
Episode 9: Steps=100, Reward=-159.915, Buffer_size=900
Episode 10: Steps=100, Reward=-159.626, Buffer_size=1000
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 10: -315.713
Episode 11: Steps=100, Reward=-149.444, Buffer_size=1100
Episode 12: Steps=100, Reward=-158.240, Buffer_size=1200
Episode 13: Steps=100, Reward=-142.721, Buffer_size=1300
Episode 14: Steps=100, Reward=-137.305, Buffer_size=1400
Episode 15: Steps=100, Reward=-162.885, Buffer_size=1500
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 15: -315.713
Episode 16: Steps=100, Reward=-160.110, Buffer_size=1600
Episode 17: Steps=100, Reward=-150.470, Buffer_size=1700
Episode 18: Steps=100, Reward=-146.888, Buffer_size=1800
Episode 19: Steps=100, Reward=-163.958, Buffer_size=1900
Episode 20: Steps=100, Reward=-151.442, Buffer_size=2000
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 20: -315.713
Episode 21: Steps=100, Reward=-163.735, Buffer_size=2100
Episode 22: Steps=100, Reward=-146.289, Buffer_size=2200
Episode 23: Steps=100, Reward=-147.034, Buffer_size=2300
Episode 24: Steps=100, Reward=-157.610, Buffer_size=2400
Episode 25: Steps=100, Reward=-154.143, Buffer_size=2500
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 25: -315.713
Episode 26: Steps=100, Reward=-164.749, Buffer_size=2600
Episode 27: Steps=100, Reward=-147.878, Buffer_size=2700
Episode 28: Steps=100, Reward=-141.520, Buffer_size=2800
Episode 29: Steps=100, Reward=-139.941, Buffer_size=2900
Episode 30: Steps=100, Reward=-152.063, Buffer_size=3000
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 30: -315.713
Episode 31: Steps=100, Reward=-149.761, Buffer_size=3100
Episode 32: Steps=100, Reward=-140.157, Buffer_size=3200
Episode 33: Steps=100, Reward=-134.048, Buffer_size=3300
Episode 34: Steps=100, Reward=-147.821, Buffer_size=3400
Episode 35: Steps=100, Reward=-146.114, Buffer_size=3500
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 35: -315.713
Episode 36: Steps=100, Reward=-154.500, Buffer_size=3600
Episode 37: Steps=100, Reward=-146.704, Buffer_size=3700
Episode 38: Steps=100, Reward=-144.067, Buffer_size=3800
Episode 39: Steps=100, Reward=-152.343, Buffer_size=3900
Episode 40: Steps=100, Reward=-136.390, Buffer_size=4000
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 40: -315.713
Episode 41: Steps=100, Reward=-148.813, Buffer_size=4100
Episode 42: Steps=100, Reward=-151.495, Buffer_size=4200
Episode 43: Steps=100, Reward=-147.435, Buffer_size=4300
Episode 44: Steps=100, Reward=-148.154, Buffer_size=4400
Episode 45: Steps=100, Reward=-147.336, Buffer_size=4500
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 45: -315.713
Episode 46: Steps=100, Reward=-167.018, Buffer_size=4600
Episode 47: Steps=100, Reward=-158.124, Buffer_size=4700
Episode 48: Steps=100, Reward=-154.699, Buffer_size=4800
Episode 49: Steps=100, Reward=-142.964, Buffer_size=4900
Episode 50: Steps=100, Reward=-152.383, Buffer_size=5000
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 50: -315.713
=== Actor Training Debug (Iteration 1) ===
Q mean: -54.983784
Q std: 33.937878
Actor loss: 54.987640
Action reg: 0.003855
  l1.weight: grad_norm = 0.417763
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 1.659543
Total gradient norm: 2.741745
=== Actor Training Debug (Iteration 2) ===
Q mean: -32.460327
Q std: 30.632071
Actor loss: 32.464233
Action reg: 0.003904
  l1.weight: grad_norm = 0.443704
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 1.929784
Total gradient norm: 3.309110
=== Actor Training Debug (Iteration 3) ===
Q mean: -6.164338
Q std: 28.435030
Actor loss: 6.168175
Action reg: 0.003837
  l1.weight: grad_norm = 0.460454
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 2.009657
Total gradient norm: 3.439011
=== Actor Training Debug (Iteration 4) ===
Q mean: -0.684217
Q std: 26.122881
Actor loss: 0.688115
Action reg: 0.003898
  l1.weight: grad_norm = 0.528241
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 2.419027
Total gradient norm: 4.042189
=== Actor Training Debug (Iteration 5) ===
Q mean: -9.671413
Q std: 24.540358
Actor loss: 9.675308
Action reg: 0.003895
  l1.weight: grad_norm = 0.332349
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 1.623853
Total gradient norm: 2.819523
=== Actor Training Debug (Iteration 6) ===
Q mean: -29.132107
Q std: 26.792185
Actor loss: 29.136017
Action reg: 0.003911
  l1.weight: grad_norm = 0.339520
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 1.455782
Total gradient norm: 2.382528
=== Actor Training Debug (Iteration 7) ===
Q mean: -37.499718
Q std: 26.587957
Actor loss: 37.503635
Action reg: 0.003918
  l1.weight: grad_norm = 0.358544
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 1.268399
Total gradient norm: 1.970509
=== Actor Training Debug (Iteration 8) ===
Q mean: -38.186764
Q std: 25.247097
Actor loss: 38.190678
Action reg: 0.003914
  l1.weight: grad_norm = 0.216485
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.860458
Total gradient norm: 1.313601
=== Actor Training Debug (Iteration 9) ===
Q mean: -34.628738
Q std: 26.316788
Actor loss: 34.632645
Action reg: 0.003905
  l1.weight: grad_norm = 0.236752
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.773213
Total gradient norm: 1.117429
=== Actor Training Debug (Iteration 10) ===
Q mean: -23.911257
Q std: 22.902447
Actor loss: 23.915178
Action reg: 0.003922
  l1.weight: grad_norm = 0.249141
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.835559
Total gradient norm: 1.324193
=== Actor Training Debug (Iteration 11) ===
Q mean: -14.565759
Q std: 23.413706
Actor loss: 14.569715
Action reg: 0.003956
  l1.weight: grad_norm = 0.218052
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.740204
Total gradient norm: 1.082388
=== Actor Training Debug (Iteration 12) ===
Q mean: -13.831988
Q std: 23.511709
Actor loss: 13.835895
Action reg: 0.003906
  l1.weight: grad_norm = 0.179317
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.607564
Total gradient norm: 0.924469
=== Actor Training Debug (Iteration 13) ===
Q mean: -20.124130
Q std: 22.727158
Actor loss: 20.128061
Action reg: 0.003931
  l1.weight: grad_norm = 0.181220
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.604916
Total gradient norm: 0.922460
=== Actor Training Debug (Iteration 14) ===
Q mean: -29.858849
Q std: 25.225878
Actor loss: 29.862791
Action reg: 0.003943
  l1.weight: grad_norm = 0.122696
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.439988
Total gradient norm: 0.650482
=== Actor Training Debug (Iteration 15) ===
Q mean: -33.125145
Q std: 26.338427
Actor loss: 33.129063
Action reg: 0.003919
  l1.weight: grad_norm = 0.123569
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.415412
Total gradient norm: 0.594995
=== Actor Training Debug (Iteration 16) ===
Q mean: -31.092501
Q std: 23.660986
Actor loss: 31.096447
Action reg: 0.003945
  l1.weight: grad_norm = 0.105138
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.337680
Total gradient norm: 0.486026
=== Actor Training Debug (Iteration 17) ===
Q mean: -26.642754
Q std: 25.748459
Actor loss: 26.646717
Action reg: 0.003964
  l1.weight: grad_norm = 0.113441
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.373155
Total gradient norm: 0.501506
=== Actor Training Debug (Iteration 18) ===
Q mean: -23.628166
Q std: 25.141048
Actor loss: 23.632116
Action reg: 0.003949
  l1.weight: grad_norm = 0.088104
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.303519
Total gradient norm: 0.476284
=== Actor Training Debug (Iteration 19) ===
Q mean: -25.322990
Q std: 26.874445
Actor loss: 25.326920
Action reg: 0.003929
  l1.weight: grad_norm = 0.175580
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.618594
Total gradient norm: 0.954281
=== Actor Training Debug (Iteration 20) ===
Q mean: -24.104101
Q std: 23.991463
Actor loss: 24.108027
Action reg: 0.003926
  l1.weight: grad_norm = 0.144830
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.545341
Total gradient norm: 0.843368
=== Actor Training Debug (Iteration 21) ===
Q mean: -25.707676
Q std: 24.234392
Actor loss: 25.711611
Action reg: 0.003934
  l1.weight: grad_norm = 0.177967
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.555380
Total gradient norm: 0.798162
=== Actor Training Debug (Iteration 22) ===
Q mean: -26.966194
Q std: 25.416044
Actor loss: 26.970127
Action reg: 0.003933
  l1.weight: grad_norm = 0.091416
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.319219
Total gradient norm: 0.465305
=== Actor Training Debug (Iteration 23) ===
Q mean: -27.106297
Q std: 26.090565
Actor loss: 27.110275
Action reg: 0.003979
  l1.weight: grad_norm = 0.169037
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.544323
Total gradient norm: 0.830939
=== Actor Training Debug (Iteration 24) ===
Q mean: -22.393423
Q std: 22.558081
Actor loss: 22.397371
Action reg: 0.003949
  l1.weight: grad_norm = 0.119416
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.460956
Total gradient norm: 0.723640
=== Actor Training Debug (Iteration 25) ===
Q mean: -26.628571
Q std: 24.557251
Actor loss: 26.632507
Action reg: 0.003938
  l1.weight: grad_norm = 0.076021
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.280766
Total gradient norm: 0.429717
=== Actor Training Debug (Iteration 26) ===
Q mean: -26.216610
Q std: 23.525833
Actor loss: 26.220566
Action reg: 0.003956
  l1.weight: grad_norm = 0.070750
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.240710
Total gradient norm: 0.343347
=== Actor Training Debug (Iteration 27) ===
Q mean: -24.869522
Q std: 24.564436
Actor loss: 24.873487
Action reg: 0.003966
  l1.weight: grad_norm = 0.110286
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.381336
Total gradient norm: 0.568402
=== Actor Training Debug (Iteration 28) ===
Q mean: -27.952484
Q std: 24.619625
Actor loss: 27.956413
Action reg: 0.003930
  l1.weight: grad_norm = 0.116612
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.423767
Total gradient norm: 0.644697
=== Actor Training Debug (Iteration 29) ===
Q mean: -30.027168
Q std: 25.463697
Actor loss: 30.031136
Action reg: 0.003968
  l1.weight: grad_norm = 0.050233
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.193082
Total gradient norm: 0.307346
=== Actor Training Debug (Iteration 30) ===
Q mean: -21.287514
Q std: 24.816975
Actor loss: 21.291458
Action reg: 0.003944
  l1.weight: grad_norm = 0.168408
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.597044
Total gradient norm: 0.966385
=== Actor Training Debug (Iteration 31) ===
Q mean: -23.723425
Q std: 28.151114
Actor loss: 23.727367
Action reg: 0.003943
  l1.weight: grad_norm = 0.072274
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.227935
Total gradient norm: 0.341025
=== Actor Training Debug (Iteration 32) ===
Q mean: -23.042736
Q std: 25.288591
Actor loss: 23.046671
Action reg: 0.003934
  l1.weight: grad_norm = 0.082393
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.246481
Total gradient norm: 0.330414
=== Actor Training Debug (Iteration 33) ===
Q mean: -26.135136
Q std: 24.995234
Actor loss: 26.138994
Action reg: 0.003859
  l1.weight: grad_norm = 0.053334
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.174234
Total gradient norm: 0.235448
=== Actor Training Debug (Iteration 34) ===
Q mean: -28.334381
Q std: 26.313467
Actor loss: 28.338305
Action reg: 0.003923
  l1.weight: grad_norm = 0.066046
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.205786
Total gradient norm: 0.314279
=== Actor Training Debug (Iteration 35) ===
Q mean: -25.417259
Q std: 23.527891
Actor loss: 25.421156
Action reg: 0.003897
  l1.weight: grad_norm = 0.115085
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.416849
Total gradient norm: 0.676447
=== Actor Training Debug (Iteration 36) ===
Q mean: -22.268129
Q std: 23.702103
Actor loss: 22.272058
Action reg: 0.003930
  l1.weight: grad_norm = 0.117748
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.423499
Total gradient norm: 0.603937
=== Actor Training Debug (Iteration 37) ===
Q mean: -24.828171
Q std: 23.457708
Actor loss: 24.832113
Action reg: 0.003943
  l1.weight: grad_norm = 0.148293
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.453670
Total gradient norm: 0.642707
=== Actor Training Debug (Iteration 38) ===
Q mean: -27.639290
Q std: 23.783636
Actor loss: 27.643213
Action reg: 0.003923
  l1.weight: grad_norm = 0.106375
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.341211
Total gradient norm: 0.509016
=== Actor Training Debug (Iteration 39) ===
Q mean: -29.915928
Q std: 24.543127
Actor loss: 29.919813
Action reg: 0.003885
  l1.weight: grad_norm = 0.077677
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.255586
Total gradient norm: 0.376134
=== Actor Training Debug (Iteration 40) ===
Q mean: -27.387897
Q std: 23.380859
Actor loss: 27.391850
Action reg: 0.003952
  l1.weight: grad_norm = 0.140679
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.516872
Total gradient norm: 0.780355
=== Actor Training Debug (Iteration 41) ===
Q mean: -25.636311
Q std: 24.884550
Actor loss: 25.640280
Action reg: 0.003970
  l1.weight: grad_norm = 0.101567
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.333054
Total gradient norm: 0.492654
=== Actor Training Debug (Iteration 42) ===
Q mean: -26.915020
Q std: 25.781271
Actor loss: 26.918995
Action reg: 0.003975
  l1.weight: grad_norm = 0.114935
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.400545
Total gradient norm: 0.594406
=== Actor Training Debug (Iteration 43) ===
Q mean: -25.110649
Q std: 23.027340
Actor loss: 25.114594
Action reg: 0.003944
  l1.weight: grad_norm = 0.075511
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.232346
Total gradient norm: 0.327765
=== Actor Training Debug (Iteration 44) ===
Q mean: -23.482504
Q std: 23.486820
Actor loss: 23.486464
Action reg: 0.003959
  l1.weight: grad_norm = 0.083353
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.339086
Total gradient norm: 0.522316
=== Actor Training Debug (Iteration 45) ===
Q mean: -21.554729
Q std: 23.001055
Actor loss: 21.558594
Action reg: 0.003864
  l1.weight: grad_norm = 0.088219
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.290457
Total gradient norm: 0.424345
=== Actor Training Debug (Iteration 46) ===
Q mean: -23.500978
Q std: 22.797674
Actor loss: 23.504921
Action reg: 0.003942
  l1.weight: grad_norm = 0.091307
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.269311
Total gradient norm: 0.377218
=== Actor Training Debug (Iteration 47) ===
Q mean: -29.201092
Q std: 24.669260
Actor loss: 29.205029
Action reg: 0.003937
  l1.weight: grad_norm = 0.082488
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.278496
Total gradient norm: 0.421022
=== Actor Training Debug (Iteration 48) ===
Q mean: -31.998167
Q std: 27.456430
Actor loss: 32.002083
Action reg: 0.003916
  l1.weight: grad_norm = 0.058541
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.209322
Total gradient norm: 0.320911
=== Actor Training Debug (Iteration 49) ===
Q mean: -28.433498
Q std: 23.710455
Actor loss: 28.437452
Action reg: 0.003954
  l1.weight: grad_norm = 0.132067
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.496235
Total gradient norm: 0.807294
=== Actor Training Debug (Iteration 50) ===
Q mean: -20.467529
Q std: 24.535414
Actor loss: 20.471489
Action reg: 0.003959
  l1.weight: grad_norm = 0.127317
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.458102
Total gradient norm: 0.737791
=== Actor Training Debug (Iteration 51) ===
Q mean: -23.160141
Q std: 23.974522
Actor loss: 23.164080
Action reg: 0.003938
  l1.weight: grad_norm = 0.045966
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.144664
Total gradient norm: 0.200524
=== Actor Training Debug (Iteration 52) ===
Q mean: -24.108036
Q std: 24.377308
Actor loss: 24.112000
Action reg: 0.003963
  l1.weight: grad_norm = 0.074685
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.219082
Total gradient norm: 0.285131
=== Actor Training Debug (Iteration 53) ===
Q mean: -31.323421
Q std: 25.586658
Actor loss: 31.327362
Action reg: 0.003940
  l1.weight: grad_norm = 0.112263
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.366812
Total gradient norm: 0.560545
=== Actor Training Debug (Iteration 54) ===
Q mean: -31.105862
Q std: 23.069798
Actor loss: 31.109779
Action reg: 0.003917
  l1.weight: grad_norm = 0.052063
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.160702
Total gradient norm: 0.231094
=== Actor Training Debug (Iteration 55) ===
Q mean: -27.373835
Q std: 24.253393
Actor loss: 27.377781
Action reg: 0.003946
  l1.weight: grad_norm = 0.114564
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.440189
Total gradient norm: 0.683668
=== Actor Training Debug (Iteration 56) ===
Q mean: -22.838120
Q std: 26.087145
Actor loss: 22.842070
Action reg: 0.003950
  l1.weight: grad_norm = 0.059782
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.212938
Total gradient norm: 0.340373
=== Actor Training Debug (Iteration 57) ===
Q mean: -22.153618
Q std: 22.602739
Actor loss: 22.157549
Action reg: 0.003931
  l1.weight: grad_norm = 0.100970
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.338037
Total gradient norm: 0.493453
=== Actor Training Debug (Iteration 58) ===
Q mean: -23.760403
Q std: 27.221033
Actor loss: 23.764341
Action reg: 0.003939
  l1.weight: grad_norm = 0.057601
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.195372
Total gradient norm: 0.301738
=== Actor Training Debug (Iteration 59) ===
Q mean: -29.059914
Q std: 26.076359
Actor loss: 29.063883
Action reg: 0.003969
  l1.weight: grad_norm = 0.060314
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.199264
Total gradient norm: 0.291637
=== Actor Training Debug (Iteration 60) ===
Q mean: -31.648376
Q std: 24.234758
Actor loss: 31.652325
Action reg: 0.003949
  l1.weight: grad_norm = 0.076712
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.246461
Total gradient norm: 0.372242
=== Actor Training Debug (Iteration 61) ===
Q mean: -30.194113
Q std: 24.121395
Actor loss: 30.198029
Action reg: 0.003916
  l1.weight: grad_norm = 0.059735
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.203016
Total gradient norm: 0.309091
=== Actor Training Debug (Iteration 62) ===
Q mean: -22.978767
Q std: 24.905869
Actor loss: 22.982721
Action reg: 0.003953
  l1.weight: grad_norm = 0.141882
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.433648
Total gradient norm: 0.666523
=== Actor Training Debug (Iteration 63) ===
Q mean: -22.231331
Q std: 23.410652
Actor loss: 22.235296
Action reg: 0.003965
  l1.weight: grad_norm = 0.068070
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.248207
Total gradient norm: 0.401937
=== Actor Training Debug (Iteration 64) ===
Q mean: -26.131580
Q std: 24.548805
Actor loss: 26.135538
Action reg: 0.003958
  l1.weight: grad_norm = 0.151567
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.491612
Total gradient norm: 0.708444
=== Actor Training Debug (Iteration 65) ===
Q mean: -30.196266
Q std: 24.027119
Actor loss: 30.200253
Action reg: 0.003986
  l1.weight: grad_norm = 0.049159
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.131492
Total gradient norm: 0.176145
=== Actor Training Debug (Iteration 66) ===
Q mean: -30.852360
Q std: 24.877216
Actor loss: 30.856325
Action reg: 0.003966
  l1.weight: grad_norm = 0.151265
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.475869
Total gradient norm: 0.747886
=== Actor Training Debug (Iteration 67) ===
Q mean: -23.996553
Q std: 23.162901
Actor loss: 24.000511
Action reg: 0.003958
  l1.weight: grad_norm = 0.134223
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.464802
Total gradient norm: 0.696536
=== Actor Training Debug (Iteration 68) ===
Q mean: -24.441025
Q std: 23.241835
Actor loss: 24.444973
Action reg: 0.003948
  l1.weight: grad_norm = 0.056903
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.182943
Total gradient norm: 0.282276
=== Actor Training Debug (Iteration 69) ===
Q mean: -25.658266
Q std: 24.718933
Actor loss: 25.662231
Action reg: 0.003966
  l1.weight: grad_norm = 0.073672
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.251437
Total gradient norm: 0.391189
=== Actor Training Debug (Iteration 70) ===
Q mean: -25.033297
Q std: 25.130058
Actor loss: 25.037270
Action reg: 0.003973
  l1.weight: grad_norm = 0.081520
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.272622
Total gradient norm: 0.425848
=== Actor Training Debug (Iteration 71) ===
Q mean: -27.688641
Q std: 26.963057
Actor loss: 27.692581
Action reg: 0.003940
  l1.weight: grad_norm = 0.054975
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.164836
Total gradient norm: 0.247698
=== Actor Training Debug (Iteration 72) ===
Q mean: -26.499191
Q std: 25.203104
Actor loss: 26.503098
Action reg: 0.003906
  l1.weight: grad_norm = 0.102477
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.362874
Total gradient norm: 0.544672
=== Actor Training Debug (Iteration 73) ===
Q mean: -24.386959
Q std: 26.703577
Actor loss: 24.390913
Action reg: 0.003954
  l1.weight: grad_norm = 0.048461
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.144674
Total gradient norm: 0.205469
=== Actor Training Debug (Iteration 74) ===
Q mean: -25.879471
Q std: 25.014650
Actor loss: 25.883417
Action reg: 0.003946
  l1.weight: grad_norm = 0.086482
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.327083
Total gradient norm: 0.570679
=== Actor Training Debug (Iteration 75) ===
Q mean: -27.136375
Q std: 24.728167
Actor loss: 27.140305
Action reg: 0.003930
  l1.weight: grad_norm = 0.081724
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.250522
Total gradient norm: 0.330971
=== Actor Training Debug (Iteration 76) ===
Q mean: -25.855713
Q std: 24.067633
Actor loss: 25.859678
Action reg: 0.003966
  l1.weight: grad_norm = 0.104560
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.371262
Total gradient norm: 0.564649
=== Actor Training Debug (Iteration 77) ===
Q mean: -26.648214
Q std: 25.228445
Actor loss: 26.652132
Action reg: 0.003918
  l1.weight: grad_norm = 0.079132
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.226073
Total gradient norm: 0.296439
=== Actor Training Debug (Iteration 78) ===
Q mean: -27.737171
Q std: 24.630993
Actor loss: 27.741138
Action reg: 0.003967
  l1.weight: grad_norm = 0.092813
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.302556
Total gradient norm: 0.407720
=== Actor Training Debug (Iteration 79) ===
Q mean: -25.252735
Q std: 24.560581
Actor loss: 25.256691
Action reg: 0.003956
  l1.weight: grad_norm = 0.070270
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.199951
Total gradient norm: 0.281187
=== Actor Training Debug (Iteration 80) ===
Q mean: -27.095631
Q std: 24.417936
Actor loss: 27.099588
Action reg: 0.003957
  l1.weight: grad_norm = 0.089156
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.288193
Total gradient norm: 0.409867
=== Actor Training Debug (Iteration 81) ===
Q mean: -26.183102
Q std: 24.555347
Actor loss: 26.187021
Action reg: 0.003920
  l1.weight: grad_norm = 0.061131
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.191516
Total gradient norm: 0.267204
=== Actor Training Debug (Iteration 82) ===
Q mean: -23.619453
Q std: 24.368057
Actor loss: 23.623413
Action reg: 0.003960
  l1.weight: grad_norm = 0.054434
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.183635
Total gradient norm: 0.269665
=== Actor Training Debug (Iteration 83) ===
Q mean: -27.017376
Q std: 28.176716
Actor loss: 27.021309
Action reg: 0.003932
  l1.weight: grad_norm = 0.107406
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.365914
Total gradient norm: 0.578704
=== Actor Training Debug (Iteration 84) ===
Q mean: -29.782879
Q std: 25.668936
Actor loss: 29.786795
Action reg: 0.003915
  l1.weight: grad_norm = 0.036950
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.136043
Total gradient norm: 0.193354
=== Actor Training Debug (Iteration 85) ===
Q mean: -27.262444
Q std: 24.343460
Actor loss: 27.266399
Action reg: 0.003957
  l1.weight: grad_norm = 0.033386
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.093174
Total gradient norm: 0.137834
=== Actor Training Debug (Iteration 86) ===
Q mean: -26.384182
Q std: 25.744261
Actor loss: 26.388157
Action reg: 0.003976
  l1.weight: grad_norm = 0.035985
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.124009
Total gradient norm: 0.178727
=== Actor Training Debug (Iteration 87) ===
Q mean: -23.514420
Q std: 24.490747
Actor loss: 23.518379
Action reg: 0.003960
  l1.weight: grad_norm = 0.076401
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.242757
Total gradient norm: 0.367833
=== Actor Training Debug (Iteration 88) ===
Q mean: -28.941889
Q std: 24.552368
Actor loss: 28.945784
Action reg: 0.003894
  l1.weight: grad_norm = 0.043491
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.126864
Total gradient norm: 0.185631
=== Actor Training Debug (Iteration 89) ===
Q mean: -28.168705
Q std: 23.741085
Actor loss: 28.172644
Action reg: 0.003939
  l1.weight: grad_norm = 0.047910
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.170595
Total gradient norm: 0.254384
=== Actor Training Debug (Iteration 90) ===
Q mean: -28.314587
Q std: 22.996344
Actor loss: 28.318579
Action reg: 0.003991
  l1.weight: grad_norm = 0.043479
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.122959
Total gradient norm: 0.166307
=== Actor Training Debug (Iteration 91) ===
Q mean: -20.888483
Q std: 22.859013
Actor loss: 20.892454
Action reg: 0.003970
  l1.weight: grad_norm = 0.048783
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.159192
Total gradient norm: 0.248051
=== Actor Training Debug (Iteration 92) ===
Q mean: -23.028278
Q std: 25.490475
Actor loss: 23.032265
Action reg: 0.003986
  l1.weight: grad_norm = 0.055769
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.180782
Total gradient norm: 0.263966
=== Actor Training Debug (Iteration 93) ===
Q mean: -23.784630
Q std: 23.790430
Actor loss: 23.788589
Action reg: 0.003961
  l1.weight: grad_norm = 0.037370
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.131484
Total gradient norm: 0.210968
=== Actor Training Debug (Iteration 94) ===
Q mean: -31.709835
Q std: 28.770716
Actor loss: 31.713734
Action reg: 0.003899
  l1.weight: grad_norm = 0.047558
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.180670
Total gradient norm: 0.320361
=== Actor Training Debug (Iteration 95) ===
Q mean: -31.382019
Q std: 26.868309
Actor loss: 31.385996
Action reg: 0.003976
  l1.weight: grad_norm = 0.034900
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.120443
Total gradient norm: 0.186563
=== Actor Training Debug (Iteration 96) ===
Q mean: -33.100910
Q std: 28.445757
Actor loss: 33.104855
Action reg: 0.003946
  l1.weight: grad_norm = 0.096181
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.293225
Total gradient norm: 0.430541
=== Actor Training Debug (Iteration 97) ===
Q mean: -23.545567
Q std: 25.536461
Actor loss: 23.549528
Action reg: 0.003962
  l1.weight: grad_norm = 0.015915
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.055292
Total gradient norm: 0.085204
=== Actor Training Debug (Iteration 98) ===
Q mean: -23.495520
Q std: 22.549391
Actor loss: 23.499453
Action reg: 0.003934
  l1.weight: grad_norm = 0.034021
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.096661
Total gradient norm: 0.146756
=== Actor Training Debug (Iteration 99) ===
Q mean: -19.359272
Q std: 22.019335
Actor loss: 19.363211
Action reg: 0.003939
  l1.weight: grad_norm = 0.070500
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.217558
Total gradient norm: 0.311423
=== Actor Training Debug (Iteration 100) ===
Q mean: -23.185246
Q std: 21.137886
Actor loss: 23.189207
Action reg: 0.003961
  l1.weight: grad_norm = 0.101202
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.308503
Total gradient norm: 0.492844
Episode 51: Steps=100, Reward=-268.642, Buffer_size=5100
=== Actor Training Debug (Iteration 101) ===
Q mean: -30.830976
Q std: 23.201975
Actor loss: 30.834921
Action reg: 0.003945
  l1.weight: grad_norm = 0.026521
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.089962
Total gradient norm: 0.133459
=== Actor Training Debug (Iteration 102) ===
Q mean: -32.917446
Q std: 23.230715
Actor loss: 32.921375
Action reg: 0.003930
  l1.weight: grad_norm = 0.056860
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.179443
Total gradient norm: 0.251482
=== Actor Training Debug (Iteration 103) ===
Q mean: -30.857286
Q std: 24.792902
Actor loss: 30.861185
Action reg: 0.003899
  l1.weight: grad_norm = 0.052261
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.159561
Total gradient norm: 0.218354
=== Actor Training Debug (Iteration 104) ===
Q mean: -23.891346
Q std: 23.277203
Actor loss: 23.895294
Action reg: 0.003948
  l1.weight: grad_norm = 0.027269
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.087407
Total gradient norm: 0.140003
=== Actor Training Debug (Iteration 105) ===
Q mean: -24.807402
Q std: 23.402355
Actor loss: 24.811272
Action reg: 0.003870
  l1.weight: grad_norm = 0.044233
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.164050
Total gradient norm: 0.264624
=== Actor Training Debug (Iteration 106) ===
Q mean: -26.345154
Q std: 24.605824
Actor loss: 26.349079
Action reg: 0.003926
  l1.weight: grad_norm = 0.043664
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.140750
Total gradient norm: 0.212791
=== Actor Training Debug (Iteration 107) ===
Q mean: -28.210220
Q std: 23.559008
Actor loss: 28.214201
Action reg: 0.003981
  l1.weight: grad_norm = 0.067401
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.209864
Total gradient norm: 0.293860
=== Actor Training Debug (Iteration 108) ===
Q mean: -28.197472
Q std: 22.657192
Actor loss: 28.201416
Action reg: 0.003944
  l1.weight: grad_norm = 0.116505
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.415251
Total gradient norm: 0.646947
=== Actor Training Debug (Iteration 109) ===
Q mean: -27.184536
Q std: 23.229559
Actor loss: 27.188480
Action reg: 0.003944
  l1.weight: grad_norm = 0.039668
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.133599
Total gradient norm: 0.194431
=== Actor Training Debug (Iteration 110) ===
Q mean: -27.749481
Q std: 23.391262
Actor loss: 27.753426
Action reg: 0.003944
  l1.weight: grad_norm = 0.059167
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.214113
Total gradient norm: 0.328455
=== Actor Training Debug (Iteration 111) ===
Q mean: -24.895966
Q std: 24.232929
Actor loss: 24.899866
Action reg: 0.003900
  l1.weight: grad_norm = 0.046109
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.141834
Total gradient norm: 0.195120
=== Actor Training Debug (Iteration 112) ===
Q mean: -26.574747
Q std: 24.708639
Actor loss: 26.578674
Action reg: 0.003927
  l1.weight: grad_norm = 0.087990
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.334369
Total gradient norm: 0.495728
=== Actor Training Debug (Iteration 113) ===
Q mean: -26.601288
Q std: 22.458715
Actor loss: 26.605274
Action reg: 0.003987
  l1.weight: grad_norm = 0.064403
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.247997
Total gradient norm: 0.392325
=== Actor Training Debug (Iteration 114) ===
Q mean: -27.103024
Q std: 25.720613
Actor loss: 27.106998
Action reg: 0.003974
  l1.weight: grad_norm = 0.044451
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.160282
Total gradient norm: 0.263525
=== Actor Training Debug (Iteration 115) ===
Q mean: -30.349857
Q std: 26.404240
Actor loss: 30.353786
Action reg: 0.003929
  l1.weight: grad_norm = 0.080197
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.266491
Total gradient norm: 0.401978
=== Actor Training Debug (Iteration 116) ===
Q mean: -24.813547
Q std: 22.493670
Actor loss: 24.817505
Action reg: 0.003957
  l1.weight: grad_norm = 0.058348
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.194286
Total gradient norm: 0.299933
=== Actor Training Debug (Iteration 117) ===
Q mean: -21.286644
Q std: 21.846594
Actor loss: 21.290602
Action reg: 0.003957
  l1.weight: grad_norm = 0.055541
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.237787
Total gradient norm: 0.386579
=== Actor Training Debug (Iteration 118) ===
Q mean: -28.246223
Q std: 22.429977
Actor loss: 28.250160
Action reg: 0.003937
  l1.weight: grad_norm = 0.070763
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.246890
Total gradient norm: 0.394587
=== Actor Training Debug (Iteration 119) ===
Q mean: -29.488342
Q std: 23.250284
Actor loss: 29.492260
Action reg: 0.003918
  l1.weight: grad_norm = 0.105867
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.493512
Total gradient norm: 0.854423
=== Actor Training Debug (Iteration 120) ===
Q mean: -28.571381
Q std: 24.755564
Actor loss: 28.575304
Action reg: 0.003923
  l1.weight: grad_norm = 0.228772
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.907329
Total gradient norm: 1.517576
=== Actor Training Debug (Iteration 121) ===
Q mean: -27.066830
Q std: 23.012703
Actor loss: 27.070801
Action reg: 0.003970
  l1.weight: grad_norm = 0.123046
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.491877
Total gradient norm: 0.835895
=== Actor Training Debug (Iteration 122) ===
Q mean: -26.221909
Q std: 23.882013
Actor loss: 26.225830
Action reg: 0.003922
  l1.weight: grad_norm = 0.093580
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.402848
Total gradient norm: 0.697092
=== Actor Training Debug (Iteration 123) ===
Q mean: -27.194416
Q std: 24.162607
Actor loss: 27.198341
Action reg: 0.003925
  l1.weight: grad_norm = 0.085063
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.344416
Total gradient norm: 0.568799
=== Actor Training Debug (Iteration 124) ===
Q mean: -25.561146
Q std: 23.590357
Actor loss: 25.565102
Action reg: 0.003955
  l1.weight: grad_norm = 0.150864
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.567962
Total gradient norm: 0.864822
=== Actor Training Debug (Iteration 125) ===
Q mean: -26.062742
Q std: 24.497536
Actor loss: 26.066696
Action reg: 0.003955
  l1.weight: grad_norm = 0.114230
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.513226
Total gradient norm: 0.891215
=== Actor Training Debug (Iteration 126) ===
Q mean: -23.649145
Q std: 24.328632
Actor loss: 23.653090
Action reg: 0.003945
  l1.weight: grad_norm = 0.080252
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.310260
Total gradient norm: 0.469991
=== Actor Training Debug (Iteration 127) ===
Q mean: -29.152391
Q std: 26.167269
Actor loss: 29.156368
Action reg: 0.003977
  l1.weight: grad_norm = 0.034227
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.122895
Total gradient norm: 0.194487
=== Actor Training Debug (Iteration 128) ===
Q mean: -27.913631
Q std: 25.016512
Actor loss: 27.917591
Action reg: 0.003961
  l1.weight: grad_norm = 0.048693
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.167824
Total gradient norm: 0.285936
=== Actor Training Debug (Iteration 129) ===
Q mean: -27.367105
Q std: 24.494831
Actor loss: 27.371077
Action reg: 0.003972
  l1.weight: grad_norm = 0.066837
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.191554
Total gradient norm: 0.253760
=== Actor Training Debug (Iteration 130) ===
Q mean: -24.435352
Q std: 22.904375
Actor loss: 24.439331
Action reg: 0.003979
  l1.weight: grad_norm = 0.058589
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.176171
Total gradient norm: 0.265975
=== Actor Training Debug (Iteration 131) ===
Q mean: -29.792650
Q std: 26.257561
Actor loss: 29.796598
Action reg: 0.003947
  l1.weight: grad_norm = 0.038087
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.100409
Total gradient norm: 0.140272
=== Actor Training Debug (Iteration 132) ===
Q mean: -29.405128
Q std: 23.979713
Actor loss: 29.409105
Action reg: 0.003978
  l1.weight: grad_norm = 0.054438
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.162252
Total gradient norm: 0.289804
=== Actor Training Debug (Iteration 133) ===
Q mean: -26.428740
Q std: 23.271770
Actor loss: 26.432732
Action reg: 0.003992
  l1.weight: grad_norm = 0.083199
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.279709
Total gradient norm: 0.461160
=== Actor Training Debug (Iteration 134) ===
Q mean: -25.923632
Q std: 22.434662
Actor loss: 25.927530
Action reg: 0.003898
  l1.weight: grad_norm = 0.037592
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.135386
Total gradient norm: 0.233887
=== Actor Training Debug (Iteration 135) ===
Q mean: -26.867893
Q std: 23.286098
Actor loss: 26.871855
Action reg: 0.003962
  l1.weight: grad_norm = 0.102763
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.307034
Total gradient norm: 0.445219
=== Actor Training Debug (Iteration 136) ===
Q mean: -26.621534
Q std: 24.740824
Actor loss: 26.625437
Action reg: 0.003902
  l1.weight: grad_norm = 0.030490
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.111406
Total gradient norm: 0.201203
=== Actor Training Debug (Iteration 137) ===
Q mean: -25.785524
Q std: 23.470358
Actor loss: 25.789505
Action reg: 0.003980
  l1.weight: grad_norm = 0.051165
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.152540
Total gradient norm: 0.243407
=== Actor Training Debug (Iteration 138) ===
Q mean: -31.638931
Q std: 23.902872
Actor loss: 31.642876
Action reg: 0.003945
  l1.weight: grad_norm = 0.074723
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.211239
Total gradient norm: 0.282690
=== Actor Training Debug (Iteration 139) ===
Q mean: -28.459339
Q std: 25.262293
Actor loss: 28.463287
Action reg: 0.003949
  l1.weight: grad_norm = 0.038496
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.135021
Total gradient norm: 0.238913
=== Actor Training Debug (Iteration 140) ===
Q mean: -25.670902
Q std: 24.086739
Actor loss: 25.674862
Action reg: 0.003959
  l1.weight: grad_norm = 0.047629
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.154062
Total gradient norm: 0.260849
=== Actor Training Debug (Iteration 141) ===
Q mean: -25.399330
Q std: 22.707300
Actor loss: 25.403265
Action reg: 0.003934
  l1.weight: grad_norm = 0.050461
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.192089
Total gradient norm: 0.329282
=== Actor Training Debug (Iteration 142) ===
Q mean: -28.353840
Q std: 20.929583
Actor loss: 28.357782
Action reg: 0.003942
  l1.weight: grad_norm = 0.054325
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.188625
Total gradient norm: 0.317653
=== Actor Training Debug (Iteration 143) ===
Q mean: -26.745430
Q std: 22.036701
Actor loss: 26.749363
Action reg: 0.003933
  l1.weight: grad_norm = 0.079129
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.249140
Total gradient norm: 0.383220
=== Actor Training Debug (Iteration 144) ===
Q mean: -26.169209
Q std: 24.781261
Actor loss: 26.173183
Action reg: 0.003976
  l1.weight: grad_norm = 0.055838
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.157127
Total gradient norm: 0.226034
=== Actor Training Debug (Iteration 145) ===
Q mean: -26.935730
Q std: 24.462791
Actor loss: 26.939602
Action reg: 0.003873
  l1.weight: grad_norm = 0.031374
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.103543
Total gradient norm: 0.162636
=== Actor Training Debug (Iteration 146) ===
Q mean: -26.412752
Q std: 24.654303
Actor loss: 26.416729
Action reg: 0.003978
  l1.weight: grad_norm = 0.152807
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.466178
Total gradient norm: 0.674643
=== Actor Training Debug (Iteration 147) ===
Q mean: -28.805141
Q std: 23.734417
Actor loss: 28.809135
Action reg: 0.003994
  l1.weight: grad_norm = 0.074228
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.283745
Total gradient norm: 0.491022
=== Actor Training Debug (Iteration 148) ===
Q mean: -25.802219
Q std: 24.601065
Actor loss: 25.806190
Action reg: 0.003971
  l1.weight: grad_norm = 0.089892
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.285897
Total gradient norm: 0.448625
=== Actor Training Debug (Iteration 149) ===
Q mean: -28.659605
Q std: 25.250612
Actor loss: 28.663567
Action reg: 0.003962
  l1.weight: grad_norm = 0.052068
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.139879
Total gradient norm: 0.182543
=== Actor Training Debug (Iteration 150) ===
Q mean: -27.445921
Q std: 25.243481
Actor loss: 27.449896
Action reg: 0.003975
  l1.weight: grad_norm = 0.110233
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.356331
Total gradient norm: 0.495360
=== Actor Training Debug (Iteration 151) ===
Q mean: -25.701218
Q std: 25.433990
Actor loss: 25.705166
Action reg: 0.003947
  l1.weight: grad_norm = 0.035826
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.108221
Total gradient norm: 0.159326
=== Actor Training Debug (Iteration 152) ===
Q mean: -28.849922
Q std: 25.904289
Actor loss: 28.853863
Action reg: 0.003940
  l1.weight: grad_norm = 0.090105
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.285956
Total gradient norm: 0.436916
=== Actor Training Debug (Iteration 153) ===
Q mean: -27.439243
Q std: 24.509903
Actor loss: 27.443167
Action reg: 0.003924
  l1.weight: grad_norm = 0.051892
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.158078
Total gradient norm: 0.247013
=== Actor Training Debug (Iteration 154) ===
Q mean: -26.614021
Q std: 23.533895
Actor loss: 26.617958
Action reg: 0.003937
  l1.weight: grad_norm = 0.005607
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.020407
Total gradient norm: 0.035509
=== Actor Training Debug (Iteration 155) ===
Q mean: -26.430994
Q std: 23.899958
Actor loss: 26.434942
Action reg: 0.003949
  l1.weight: grad_norm = 0.038060
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.130015
Total gradient norm: 0.219338
=== Actor Training Debug (Iteration 156) ===
Q mean: -26.709667
Q std: 23.480278
Actor loss: 26.713627
Action reg: 0.003959
  l1.weight: grad_norm = 0.051413
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.195737
Total gradient norm: 0.332833
=== Actor Training Debug (Iteration 157) ===
Q mean: -28.682953
Q std: 25.566830
Actor loss: 28.686903
Action reg: 0.003950
  l1.weight: grad_norm = 0.020792
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.066409
Total gradient norm: 0.089879
=== Actor Training Debug (Iteration 158) ===
Q mean: -26.845947
Q std: 22.630482
Actor loss: 26.849928
Action reg: 0.003981
  l1.weight: grad_norm = 0.019774
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.060506
Total gradient norm: 0.082357
=== Actor Training Debug (Iteration 159) ===
Q mean: -26.826429
Q std: 19.455498
Actor loss: 26.830391
Action reg: 0.003961
  l1.weight: grad_norm = 0.066524
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.210059
Total gradient norm: 0.358063
=== Actor Training Debug (Iteration 160) ===
Q mean: -28.549860
Q std: 22.198406
Actor loss: 28.553820
Action reg: 0.003959
  l1.weight: grad_norm = 0.066722
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.212656
Total gradient norm: 0.372106
=== Actor Training Debug (Iteration 161) ===
Q mean: -26.622181
Q std: 21.041451
Actor loss: 26.626160
Action reg: 0.003979
  l1.weight: grad_norm = 0.018191
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.058619
Total gradient norm: 0.089868
=== Actor Training Debug (Iteration 162) ===
Q mean: -26.920158
Q std: 23.178976
Actor loss: 26.924116
Action reg: 0.003958
  l1.weight: grad_norm = 0.070911
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.206283
Total gradient norm: 0.281849
=== Actor Training Debug (Iteration 163) ===
Q mean: -25.182400
Q std: 23.772560
Actor loss: 25.186321
Action reg: 0.003922
  l1.weight: grad_norm = 0.013091
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.044854
Total gradient norm: 0.070285
=== Actor Training Debug (Iteration 164) ===
Q mean: -28.225628
Q std: 26.199177
Actor loss: 28.229580
Action reg: 0.003953
  l1.weight: grad_norm = 0.004405
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.012608
Total gradient norm: 0.016962
=== Actor Training Debug (Iteration 165) ===
Q mean: -30.241858
Q std: 25.023247
Actor loss: 30.245777
Action reg: 0.003920
  l1.weight: grad_norm = 0.019095
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.067124
Total gradient norm: 0.107796
=== Actor Training Debug (Iteration 166) ===
Q mean: -29.899868
Q std: 21.863121
Actor loss: 29.903818
Action reg: 0.003950
  l1.weight: grad_norm = 0.018225
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.050506
Total gradient norm: 0.075955
=== Actor Training Debug (Iteration 167) ===
Q mean: -25.113949
Q std: 23.322165
Actor loss: 25.117912
Action reg: 0.003964
  l1.weight: grad_norm = 0.021382
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.063831
Total gradient norm: 0.092505
=== Actor Training Debug (Iteration 168) ===
Q mean: -23.832489
Q std: 21.117569
Actor loss: 23.836441
Action reg: 0.003952
  l1.weight: grad_norm = 0.016171
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.053210
Total gradient norm: 0.079078
=== Actor Training Debug (Iteration 169) ===
Q mean: -27.580261
Q std: 22.900625
Actor loss: 27.584259
Action reg: 0.003998
  l1.weight: grad_norm = 0.010262
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.033481
Total gradient norm: 0.048888
=== Actor Training Debug (Iteration 170) ===
Q mean: -29.734863
Q std: 22.201834
Actor loss: 29.738796
Action reg: 0.003932
  l1.weight: grad_norm = 0.042137
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.138091
Total gradient norm: 0.230732
=== Actor Training Debug (Iteration 171) ===
Q mean: -28.496902
Q std: 24.644337
Actor loss: 28.500834
Action reg: 0.003931
  l1.weight: grad_norm = 0.076089
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.238372
Total gradient norm: 0.329644
=== Actor Training Debug (Iteration 172) ===
Q mean: -29.117960
Q std: 24.176540
Actor loss: 29.121939
Action reg: 0.003978
  l1.weight: grad_norm = 0.063294
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.206420
Total gradient norm: 0.360564
=== Actor Training Debug (Iteration 173) ===
Q mean: -24.816761
Q std: 22.556793
Actor loss: 24.820724
Action reg: 0.003964
  l1.weight: grad_norm = 0.045512
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.132320
Total gradient norm: 0.192585
=== Actor Training Debug (Iteration 174) ===
Q mean: -24.140377
Q std: 23.586237
Actor loss: 24.144279
Action reg: 0.003903
  l1.weight: grad_norm = 0.073965
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.194201
Total gradient norm: 0.256548
=== Actor Training Debug (Iteration 175) ===
Q mean: -30.940929
Q std: 22.792795
Actor loss: 30.944920
Action reg: 0.003990
  l1.weight: grad_norm = 0.062811
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.220192
Total gradient norm: 0.351745
=== Actor Training Debug (Iteration 176) ===
Q mean: -29.978466
Q std: 23.357155
Actor loss: 29.982430
Action reg: 0.003964
  l1.weight: grad_norm = 0.062362
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.184152
Total gradient norm: 0.254670
=== Actor Training Debug (Iteration 177) ===
Q mean: -32.777115
Q std: 25.135433
Actor loss: 32.781067
Action reg: 0.003951
  l1.weight: grad_norm = 0.052525
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.173289
Total gradient norm: 0.278832
=== Actor Training Debug (Iteration 178) ===
Q mean: -26.095032
Q std: 22.572699
Actor loss: 26.098953
Action reg: 0.003921
  l1.weight: grad_norm = 0.033873
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.097729
Total gradient norm: 0.137079
=== Actor Training Debug (Iteration 179) ===
Q mean: -25.588121
Q std: 24.151705
Actor loss: 25.592072
Action reg: 0.003951
  l1.weight: grad_norm = 0.047000
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.151318
Total gradient norm: 0.246883
=== Actor Training Debug (Iteration 180) ===
Q mean: -27.273260
Q std: 21.774714
Actor loss: 27.277208
Action reg: 0.003948
  l1.weight: grad_norm = 0.037465
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.122599
Total gradient norm: 0.196618
=== Actor Training Debug (Iteration 181) ===
Q mean: -27.002590
Q std: 24.160976
Actor loss: 27.006554
Action reg: 0.003964
  l1.weight: grad_norm = 0.060749
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.178884
Total gradient norm: 0.262435
=== Actor Training Debug (Iteration 182) ===
Q mean: -29.428150
Q std: 23.700891
Actor loss: 29.432131
Action reg: 0.003980
  l1.weight: grad_norm = 0.022159
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.064446
Total gradient norm: 0.087115
=== Actor Training Debug (Iteration 183) ===
Q mean: -28.706993
Q std: 22.964727
Actor loss: 28.710978
Action reg: 0.003984
  l1.weight: grad_norm = 0.082019
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.270754
Total gradient norm: 0.435963
=== Actor Training Debug (Iteration 184) ===
Q mean: -24.354597
Q std: 21.162127
Actor loss: 24.358593
Action reg: 0.003996
  l1.weight: grad_norm = 0.007690
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.027750
Total gradient norm: 0.044244
=== Actor Training Debug (Iteration 185) ===
Q mean: -25.794107
Q std: 23.885576
Actor loss: 25.798084
Action reg: 0.003977
  l1.weight: grad_norm = 0.082734
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.270026
Total gradient norm: 0.417335
=== Actor Training Debug (Iteration 186) ===
Q mean: -25.213978
Q std: 22.426870
Actor loss: 25.217951
Action reg: 0.003973
  l1.weight: grad_norm = 0.059262
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.180985
Total gradient norm: 0.262019
=== Actor Training Debug (Iteration 187) ===
Q mean: -22.985558
Q std: 23.179651
Actor loss: 22.989510
Action reg: 0.003952
  l1.weight: grad_norm = 0.029928
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.097349
Total gradient norm: 0.148498
=== Actor Training Debug (Iteration 188) ===
Q mean: -30.118607
Q std: 24.599955
Actor loss: 30.122557
Action reg: 0.003950
  l1.weight: grad_norm = 0.059594
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.172505
Total gradient norm: 0.228821
=== Actor Training Debug (Iteration 189) ===
Q mean: -29.621174
Q std: 24.335270
Actor loss: 29.625170
Action reg: 0.003996
  l1.weight: grad_norm = 0.025513
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.084470
Total gradient norm: 0.136154
=== Actor Training Debug (Iteration 190) ===
Q mean: -23.337036
Q std: 22.647547
Actor loss: 23.341003
Action reg: 0.003968
  l1.weight: grad_norm = 0.025925
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.078556
Total gradient norm: 0.116492
=== Actor Training Debug (Iteration 191) ===
Q mean: -26.063560
Q std: 23.352552
Actor loss: 26.067495
Action reg: 0.003935
  l1.weight: grad_norm = 0.052303
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.171964
Total gradient norm: 0.277662
=== Actor Training Debug (Iteration 192) ===
Q mean: -29.153442
Q std: 23.795315
Actor loss: 29.157408
Action reg: 0.003965
  l1.weight: grad_norm = 0.034896
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.119308
Total gradient norm: 0.198889
=== Actor Training Debug (Iteration 193) ===
Q mean: -25.772739
Q std: 22.966690
Actor loss: 25.776674
Action reg: 0.003935
  l1.weight: grad_norm = 0.011161
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.032684
Total gradient norm: 0.047517
=== Actor Training Debug (Iteration 194) ===
Q mean: -24.863272
Q std: 23.206924
Actor loss: 24.867222
Action reg: 0.003950
  l1.weight: grad_norm = 0.092300
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.252284
Total gradient norm: 0.359472
=== Actor Training Debug (Iteration 195) ===
Q mean: -22.149412
Q std: 22.109303
Actor loss: 22.153324
Action reg: 0.003913
  l1.weight: grad_norm = 0.047052
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.156528
Total gradient norm: 0.248965
=== Actor Training Debug (Iteration 196) ===
Q mean: -30.854694
Q std: 24.324949
Actor loss: 30.858671
Action reg: 0.003976
  l1.weight: grad_norm = 0.035574
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.111504
Total gradient norm: 0.160716
=== Actor Training Debug (Iteration 197) ===
Q mean: -28.409954
Q std: 23.539150
Actor loss: 28.413889
Action reg: 0.003935
  l1.weight: grad_norm = 0.040785
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.129805
Total gradient norm: 0.214254
=== Actor Training Debug (Iteration 198) ===
Q mean: -28.099678
Q std: 22.523157
Actor loss: 28.103624
Action reg: 0.003947
  l1.weight: grad_norm = 0.047080
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.144902
Total gradient norm: 0.214463
=== Actor Training Debug (Iteration 199) ===
Q mean: -27.549078
Q std: 22.505684
Actor loss: 27.553040
Action reg: 0.003962
  l1.weight: grad_norm = 0.034743
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.122999
Total gradient norm: 0.199445
=== Actor Training Debug (Iteration 200) ===
Q mean: -21.535065
Q std: 22.509743
Actor loss: 21.539028
Action reg: 0.003964
  l1.weight: grad_norm = 0.067649
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.248552
Total gradient norm: 0.409209
=== Actor Training Debug (Iteration 201) ===
Q mean: -25.747259
Q std: 24.757616
Actor loss: 25.751192
Action reg: 0.003934
  l1.weight: grad_norm = 0.027459
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.083896
Total gradient norm: 0.122267
=== Actor Training Debug (Iteration 202) ===
Q mean: -28.772179
Q std: 25.442739
Actor loss: 28.776142
Action reg: 0.003963
  l1.weight: grad_norm = 0.046245
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.127651
Total gradient norm: 0.173946
=== Actor Training Debug (Iteration 203) ===
Q mean: -30.148874
Q std: 24.393642
Actor loss: 30.152824
Action reg: 0.003951
  l1.weight: grad_norm = 0.034213
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.120050
Total gradient norm: 0.197615
=== Actor Training Debug (Iteration 204) ===
Q mean: -27.100941
Q std: 24.034481
Actor loss: 27.104898
Action reg: 0.003958
  l1.weight: grad_norm = 0.066471
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.194808
Total gradient norm: 0.274414
=== Actor Training Debug (Iteration 205) ===
Q mean: -23.820793
Q std: 23.598972
Actor loss: 23.824776
Action reg: 0.003983
  l1.weight: grad_norm = 0.013067
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.039084
Total gradient norm: 0.056553
=== Actor Training Debug (Iteration 206) ===
Q mean: -26.462069
Q std: 19.597725
Actor loss: 26.466059
Action reg: 0.003991
  l1.weight: grad_norm = 0.075627
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.226371
Total gradient norm: 0.303829
=== Actor Training Debug (Iteration 207) ===
Q mean: -30.194374
Q std: 21.906536
Actor loss: 30.198324
Action reg: 0.003951
  l1.weight: grad_norm = 0.039214
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.130363
Total gradient norm: 0.231602
=== Actor Training Debug (Iteration 208) ===
Q mean: -30.428646
Q std: 22.751190
Actor loss: 30.432579
Action reg: 0.003932
  l1.weight: grad_norm = 0.021957
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.086286
Total gradient norm: 0.156209
=== Actor Training Debug (Iteration 209) ===
Q mean: -25.783627
Q std: 21.220016
Actor loss: 25.787600
Action reg: 0.003974
  l1.weight: grad_norm = 0.118960
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.395364
Total gradient norm: 0.638730
=== Actor Training Debug (Iteration 210) ===
Q mean: -25.065121
Q std: 23.729971
Actor loss: 25.069082
Action reg: 0.003962
  l1.weight: grad_norm = 0.091261
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.303571
Total gradient norm: 0.430483
=== Actor Training Debug (Iteration 211) ===
Q mean: -26.949364
Q std: 23.701342
Actor loss: 26.953300
Action reg: 0.003937
  l1.weight: grad_norm = 0.032659
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.105640
Total gradient norm: 0.168799
=== Actor Training Debug (Iteration 212) ===
Q mean: -27.079058
Q std: 23.962259
Actor loss: 27.083040
Action reg: 0.003983
  l1.weight: grad_norm = 0.003298
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.011392
Total gradient norm: 0.019424
=== Actor Training Debug (Iteration 213) ===
Q mean: -29.626554
Q std: 23.397367
Actor loss: 29.630503
Action reg: 0.003948
  l1.weight: grad_norm = 0.024076
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.087867
Total gradient norm: 0.149731
=== Actor Training Debug (Iteration 214) ===
Q mean: -25.909534
Q std: 19.772730
Actor loss: 25.913500
Action reg: 0.003965
  l1.weight: grad_norm = 0.068721
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.210539
Total gradient norm: 0.304866
=== Actor Training Debug (Iteration 215) ===
Q mean: -26.657570
Q std: 21.188286
Actor loss: 26.661549
Action reg: 0.003979
  l1.weight: grad_norm = 0.060521
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.189546
Total gradient norm: 0.281724
=== Actor Training Debug (Iteration 216) ===
Q mean: -24.724365
Q std: 21.650618
Actor loss: 24.728363
Action reg: 0.003997
  l1.weight: grad_norm = 0.017275
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.055241
Total gradient norm: 0.086022
=== Actor Training Debug (Iteration 217) ===
Q mean: -28.139683
Q std: 22.176147
Actor loss: 28.143648
Action reg: 0.003964
  l1.weight: grad_norm = 0.026289
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.074303
Total gradient norm: 0.098020
=== Actor Training Debug (Iteration 218) ===
Q mean: -28.184441
Q std: 25.281036
Actor loss: 28.188395
Action reg: 0.003953
  l1.weight: grad_norm = 0.002128
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.006661
Total gradient norm: 0.010664
=== Actor Training Debug (Iteration 219) ===
Q mean: -26.836773
Q std: 24.196997
Actor loss: 26.840706
Action reg: 0.003933
  l1.weight: grad_norm = 0.030292
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.092936
Total gradient norm: 0.135091
=== Actor Training Debug (Iteration 220) ===
Q mean: -26.991140
Q std: 23.050129
Actor loss: 26.995102
Action reg: 0.003962
  l1.weight: grad_norm = 0.090457
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.265802
Total gradient norm: 0.409651
=== Actor Training Debug (Iteration 221) ===
Q mean: -27.892300
Q std: 20.931471
Actor loss: 27.896269
Action reg: 0.003969
  l1.weight: grad_norm = 0.004048
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.013001
Total gradient norm: 0.019281
=== Actor Training Debug (Iteration 222) ===
Q mean: -28.567316
Q std: 23.463646
Actor loss: 28.571260
Action reg: 0.003945
  l1.weight: grad_norm = 0.043622
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.156385
Total gradient norm: 0.278062
=== Actor Training Debug (Iteration 223) ===
Q mean: -30.863865
Q std: 24.896326
Actor loss: 30.867832
Action reg: 0.003967
  l1.weight: grad_norm = 0.014211
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.038090
Total gradient norm: 0.051382
=== Actor Training Debug (Iteration 224) ===
Q mean: -25.255890
Q std: 25.084164
Actor loss: 25.259869
Action reg: 0.003979
  l1.weight: grad_norm = 0.037244
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.109992
Total gradient norm: 0.146288
=== Actor Training Debug (Iteration 225) ===
Q mean: -26.838257
Q std: 23.529945
Actor loss: 26.842209
Action reg: 0.003953
  l1.weight: grad_norm = 0.011734
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.041712
Total gradient norm: 0.071894
=== Actor Training Debug (Iteration 226) ===
Q mean: -26.691124
Q std: 24.053406
Actor loss: 26.695072
Action reg: 0.003948
  l1.weight: grad_norm = 0.105703
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.325802
Total gradient norm: 0.544144
=== Actor Training Debug (Iteration 227) ===
Q mean: -28.337742
Q std: 23.508936
Actor loss: 28.341692
Action reg: 0.003951
  l1.weight: grad_norm = 0.034383
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.117322
Total gradient norm: 0.187795
=== Actor Training Debug (Iteration 228) ===
Q mean: -29.958385
Q std: 23.285547
Actor loss: 29.962381
Action reg: 0.003996
  l1.weight: grad_norm = 0.007819
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.020158
Total gradient norm: 0.026186
=== Actor Training Debug (Iteration 229) ===
Q mean: -28.483555
Q std: 23.313654
Actor loss: 28.487507
Action reg: 0.003953
  l1.weight: grad_norm = 0.020917
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.067071
Total gradient norm: 0.106223
=== Actor Training Debug (Iteration 230) ===
Q mean: -27.345919
Q std: 22.459286
Actor loss: 27.349901
Action reg: 0.003982
  l1.weight: grad_norm = 0.045572
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.141607
Total gradient norm: 0.220765
=== Actor Training Debug (Iteration 231) ===
Q mean: -26.777958
Q std: 22.798494
Actor loss: 26.781904
Action reg: 0.003947
  l1.weight: grad_norm = 0.042950
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.130502
Total gradient norm: 0.201373
=== Actor Training Debug (Iteration 232) ===
Q mean: -28.863995
Q std: 22.320070
Actor loss: 28.867979
Action reg: 0.003984
  l1.weight: grad_norm = 0.012210
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.037150
Total gradient norm: 0.053543
=== Actor Training Debug (Iteration 233) ===
Q mean: -30.468626
Q std: 22.809393
Actor loss: 30.472549
Action reg: 0.003923
  l1.weight: grad_norm = 0.010010
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.028150
Total gradient norm: 0.041051
=== Actor Training Debug (Iteration 234) ===
Q mean: -25.931273
Q std: 22.953249
Actor loss: 25.935251
Action reg: 0.003979
  l1.weight: grad_norm = 0.033323
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.110116
Total gradient norm: 0.181790
=== Actor Training Debug (Iteration 235) ===
Q mean: -24.020615
Q std: 22.362864
Actor loss: 24.024584
Action reg: 0.003969
  l1.weight: grad_norm = 0.003582
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.011093
Total gradient norm: 0.017230
=== Actor Training Debug (Iteration 236) ===
Q mean: -28.615675
Q std: 23.377033
Actor loss: 28.619629
Action reg: 0.003954
  l1.weight: grad_norm = 0.006419
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.020918
Total gradient norm: 0.030802
=== Actor Training Debug (Iteration 237) ===
Q mean: -27.528402
Q std: 23.751318
Actor loss: 27.532335
Action reg: 0.003933
  l1.weight: grad_norm = 0.062310
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.213668
Total gradient norm: 0.364516
=== Actor Training Debug (Iteration 238) ===
Q mean: -26.650742
Q std: 22.647835
Actor loss: 26.654671
Action reg: 0.003929
  l1.weight: grad_norm = 0.054813
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.147631
Total gradient norm: 0.194751
=== Actor Training Debug (Iteration 239) ===
Q mean: -26.998245
Q std: 20.517523
Actor loss: 27.002180
Action reg: 0.003934
  l1.weight: grad_norm = 0.021282
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.058804
Total gradient norm: 0.076120
=== Actor Training Debug (Iteration 240) ===
Q mean: -25.733067
Q std: 22.435673
Actor loss: 25.737003
Action reg: 0.003937
  l1.weight: grad_norm = 0.014585
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.039782
Total gradient norm: 0.052452
=== Actor Training Debug (Iteration 241) ===
Q mean: -32.395309
Q std: 24.183069
Actor loss: 32.399292
Action reg: 0.003982
  l1.weight: grad_norm = 0.010197
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.026585
Total gradient norm: 0.035269
=== Actor Training Debug (Iteration 242) ===
Q mean: -29.701920
Q std: 22.032558
Actor loss: 29.705837
Action reg: 0.003917
  l1.weight: grad_norm = 0.049183
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.152751
Total gradient norm: 0.215512
=== Actor Training Debug (Iteration 243) ===
Q mean: -26.526751
Q std: 24.641754
Actor loss: 26.530731
Action reg: 0.003980
  l1.weight: grad_norm = 0.029470
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.084868
Total gradient norm: 0.127506
=== Actor Training Debug (Iteration 244) ===
Q mean: -24.076296
Q std: 22.115957
Actor loss: 24.080265
Action reg: 0.003969
  l1.weight: grad_norm = 0.002834
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.010101
Total gradient norm: 0.018036
=== Actor Training Debug (Iteration 245) ===
Q mean: -27.326164
Q std: 22.618843
Actor loss: 27.330118
Action reg: 0.003953
  l1.weight: grad_norm = 0.012939
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.036264
Total gradient norm: 0.056957
=== Actor Training Debug (Iteration 246) ===
Q mean: -28.840139
Q std: 24.081587
Actor loss: 28.844103
Action reg: 0.003964
  l1.weight: grad_norm = 0.019000
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.053168
Total gradient norm: 0.071052
=== Actor Training Debug (Iteration 247) ===
Q mean: -25.125412
Q std: 21.097616
Actor loss: 25.129316
Action reg: 0.003905
  l1.weight: grad_norm = 0.066067
  l1.bias: grad_norm = 0.000579
  l2.weight: grad_norm = 0.177199
Total gradient norm: 0.226560
=== Actor Training Debug (Iteration 248) ===
Q mean: -27.849075
Q std: 22.778261
Actor loss: 27.853027
Action reg: 0.003953
  l1.weight: grad_norm = 0.024213
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.082113
Total gradient norm: 0.125324
=== Actor Training Debug (Iteration 249) ===
Q mean: -30.170742
Q std: 23.276661
Actor loss: 30.174696
Action reg: 0.003953
  l1.weight: grad_norm = 0.002814
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.010107
Total gradient norm: 0.016397
=== Actor Training Debug (Iteration 250) ===
Q mean: -27.843018
Q std: 22.290884
Actor loss: 27.846998
Action reg: 0.003981
  l1.weight: grad_norm = 0.090490
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.252158
Total gradient norm: 0.367202
=== Actor Training Debug (Iteration 251) ===
Q mean: -25.300232
Q std: 22.934761
Actor loss: 25.304180
Action reg: 0.003948
  l1.weight: grad_norm = 0.037429
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.103498
Total gradient norm: 0.149519
=== Actor Training Debug (Iteration 252) ===
Q mean: -28.565350
Q std: 24.375006
Actor loss: 28.569284
Action reg: 0.003935
  l1.weight: grad_norm = 0.025018
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.094032
Total gradient norm: 0.156226
=== Actor Training Debug (Iteration 253) ===
Q mean: -29.417240
Q std: 24.371555
Actor loss: 29.421217
Action reg: 0.003977
  l1.weight: grad_norm = 0.097617
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.364307
Total gradient norm: 0.660719
=== Actor Training Debug (Iteration 254) ===
Q mean: -28.664911
Q std: 22.249683
Actor loss: 28.668875
Action reg: 0.003964
  l1.weight: grad_norm = 0.014124
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.046368
Total gradient norm: 0.074977
=== Actor Training Debug (Iteration 255) ===
Q mean: -24.369276
Q std: 22.091993
Actor loss: 24.373226
Action reg: 0.003950
  l1.weight: grad_norm = 0.033305
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.087693
Total gradient norm: 0.116812
=== Actor Training Debug (Iteration 256) ===
Q mean: -24.062645
Q std: 22.716652
Actor loss: 24.066580
Action reg: 0.003935
  l1.weight: grad_norm = 0.039426
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.118899
Total gradient norm: 0.185760
=== Actor Training Debug (Iteration 257) ===
Q mean: -27.000427
Q std: 21.883366
Actor loss: 27.004343
Action reg: 0.003916
  l1.weight: grad_norm = 0.071686
  l1.bias: grad_norm = 0.000599
  l2.weight: grad_norm = 0.237898
Total gradient norm: 0.349465
=== Actor Training Debug (Iteration 258) ===
Q mean: -28.983574
Q std: 22.896820
Actor loss: 28.987543
Action reg: 0.003970
  l1.weight: grad_norm = 0.001635
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.005316
Total gradient norm: 0.008635
=== Actor Training Debug (Iteration 259) ===
Q mean: -26.765497
Q std: 22.303925
Actor loss: 26.769463
Action reg: 0.003966
  l1.weight: grad_norm = 0.030684
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.101849
Total gradient norm: 0.160765
=== Actor Training Debug (Iteration 260) ===
Q mean: -26.417286
Q std: 25.368582
Actor loss: 26.421242
Action reg: 0.003955
  l1.weight: grad_norm = 0.009557
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.026758
Total gradient norm: 0.036014
=== Actor Training Debug (Iteration 261) ===
Q mean: -25.487518
Q std: 22.174011
Actor loss: 25.491495
Action reg: 0.003976
  l1.weight: grad_norm = 0.090921
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.263744
Total gradient norm: 0.367771
=== Actor Training Debug (Iteration 262) ===
Q mean: -30.826370
Q std: 24.388060
Actor loss: 30.830349
Action reg: 0.003979
  l1.weight: grad_norm = 0.045181
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.141081
Total gradient norm: 0.228121
=== Actor Training Debug (Iteration 263) ===
Q mean: -29.560783
Q std: 23.638039
Actor loss: 29.564760
Action reg: 0.003977
  l1.weight: grad_norm = 0.027737
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.079567
Total gradient norm: 0.102463
=== Actor Training Debug (Iteration 264) ===
Q mean: -26.298626
Q std: 22.206884
Actor loss: 26.302589
Action reg: 0.003963
  l1.weight: grad_norm = 0.044729
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.146398
Total gradient norm: 0.224666
=== Actor Training Debug (Iteration 265) ===
Q mean: -27.263557
Q std: 22.952492
Actor loss: 27.267492
Action reg: 0.003935
  l1.weight: grad_norm = 0.014176
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.043501
Total gradient norm: 0.062065
=== Actor Training Debug (Iteration 266) ===
Q mean: -29.655708
Q std: 20.724552
Actor loss: 29.659674
Action reg: 0.003966
  l1.weight: grad_norm = 0.049807
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.151638
Total gradient norm: 0.230608
=== Actor Training Debug (Iteration 267) ===
Q mean: -32.122932
Q std: 23.319899
Actor loss: 32.126896
Action reg: 0.003964
  l1.weight: grad_norm = 0.029252
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.083525
Total gradient norm: 0.121268
=== Actor Training Debug (Iteration 268) ===
Q mean: -28.018051
Q std: 22.958555
Actor loss: 28.022049
Action reg: 0.003998
  l1.weight: grad_norm = 0.007060
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.020318
Total gradient norm: 0.031669
=== Actor Training Debug (Iteration 269) ===
Q mean: -24.570145
Q std: 21.796225
Actor loss: 24.574099
Action reg: 0.003954
  l1.weight: grad_norm = 0.012110
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.034636
Total gradient norm: 0.045009
=== Actor Training Debug (Iteration 270) ===
Q mean: -30.701637
Q std: 24.928991
Actor loss: 30.705599
Action reg: 0.003961
  l1.weight: grad_norm = 0.099643
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.282266
Total gradient norm: 0.359691
=== Actor Training Debug (Iteration 271) ===
Q mean: -29.526173
Q std: 24.952774
Actor loss: 29.530170
Action reg: 0.003997
  l1.weight: grad_norm = 0.033120
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.118550
Total gradient norm: 0.191466
=== Actor Training Debug (Iteration 272) ===
Q mean: -25.306124
Q std: 23.997019
Actor loss: 25.310061
Action reg: 0.003937
  l1.weight: grad_norm = 0.033782
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.123276
Total gradient norm: 0.210902
=== Actor Training Debug (Iteration 273) ===
Q mean: -27.947098
Q std: 21.693825
Actor loss: 27.951080
Action reg: 0.003983
  l1.weight: grad_norm = 0.008713
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.026243
Total gradient norm: 0.039687
=== Actor Training Debug (Iteration 274) ===
Q mean: -29.877472
Q std: 20.458029
Actor loss: 29.881437
Action reg: 0.003965
  l1.weight: grad_norm = 0.029910
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.090234
Total gradient norm: 0.133029
=== Actor Training Debug (Iteration 275) ===
Q mean: -30.117786
Q std: 22.121162
Actor loss: 30.121769
Action reg: 0.003983
  l1.weight: grad_norm = 0.009418
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.031407
Total gradient norm: 0.048127
=== Actor Training Debug (Iteration 276) ===
Q mean: -26.050774
Q std: 21.691107
Actor loss: 26.054695
Action reg: 0.003921
  l1.weight: grad_norm = 0.023024
  l1.bias: grad_norm = 0.000548
  l2.weight: grad_norm = 0.069734
Total gradient norm: 0.096896
=== Actor Training Debug (Iteration 277) ===
Q mean: -26.099789
Q std: 24.062330
Actor loss: 26.103781
Action reg: 0.003993
  l1.weight: grad_norm = 0.072858
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.252795
Total gradient norm: 0.396105
=== Actor Training Debug (Iteration 278) ===
Q mean: -32.318935
Q std: 25.468731
Actor loss: 32.322914
Action reg: 0.003979
  l1.weight: grad_norm = 0.035699
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.099753
Total gradient norm: 0.140547
=== Actor Training Debug (Iteration 279) ===
Q mean: -29.379471
Q std: 22.865200
Actor loss: 29.383448
Action reg: 0.003977
  l1.weight: grad_norm = 0.055249
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.180667
Total gradient norm: 0.307985
=== Actor Training Debug (Iteration 280) ===
Q mean: -30.665813
Q std: 21.925608
Actor loss: 30.669731
Action reg: 0.003918
  l1.weight: grad_norm = 0.037780
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.113540
Total gradient norm: 0.157523
=== Actor Training Debug (Iteration 281) ===
Q mean: -24.451885
Q std: 23.899969
Actor loss: 24.455820
Action reg: 0.003934
  l1.weight: grad_norm = 0.040328
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.117314
Total gradient norm: 0.157192
=== Actor Training Debug (Iteration 282) ===
Q mean: -30.434788
Q std: 23.663490
Actor loss: 30.438770
Action reg: 0.003982
  l1.weight: grad_norm = 0.012696
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.035957
Total gradient norm: 0.049311
=== Actor Training Debug (Iteration 283) ===
Q mean: -29.966537
Q std: 23.805166
Actor loss: 29.970476
Action reg: 0.003938
  l1.weight: grad_norm = 0.015939
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.051677
Total gradient norm: 0.088639
=== Actor Training Debug (Iteration 284) ===
Q mean: -26.295132
Q std: 22.902241
Actor loss: 26.299068
Action reg: 0.003937
  l1.weight: grad_norm = 0.005994
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.017376
Total gradient norm: 0.023608
=== Actor Training Debug (Iteration 285) ===
Q mean: -26.114948
Q std: 22.836283
Actor loss: 26.118872
Action reg: 0.003923
  l1.weight: grad_norm = 0.049814
  l1.bias: grad_norm = 0.000507
  l2.weight: grad_norm = 0.156007
Total gradient norm: 0.238159
=== Actor Training Debug (Iteration 286) ===
Q mean: -30.215940
Q std: 24.769163
Actor loss: 30.219925
Action reg: 0.003984
  l1.weight: grad_norm = 0.007555
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.025518
Total gradient norm: 0.043227
=== Actor Training Debug (Iteration 287) ===
Q mean: -30.614254
Q std: 23.715334
Actor loss: 30.618210
Action reg: 0.003955
  l1.weight: grad_norm = 0.003027
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.008368
Total gradient norm: 0.011642
=== Actor Training Debug (Iteration 288) ===
Q mean: -28.111797
Q std: 22.096773
Actor loss: 28.115736
Action reg: 0.003939
  l1.weight: grad_norm = 0.004601
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.014982
Total gradient norm: 0.025421
=== Actor Training Debug (Iteration 289) ===
Q mean: -26.386217
Q std: 22.795710
Actor loss: 26.390169
Action reg: 0.003952
  l1.weight: grad_norm = 0.009452
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.036565
Total gradient norm: 0.060059
=== Actor Training Debug (Iteration 290) ===
Q mean: -28.486147
Q std: 23.262445
Actor loss: 28.490053
Action reg: 0.003906
  l1.weight: grad_norm = 0.021858
  l1.bias: grad_norm = 0.000608
  l2.weight: grad_norm = 0.066037
Total gradient norm: 0.089731
=== Actor Training Debug (Iteration 291) ===
Q mean: -26.705944
Q std: 20.433458
Actor loss: 26.709877
Action reg: 0.003933
  l1.weight: grad_norm = 0.022819
  l1.bias: grad_norm = 0.000484
  l2.weight: grad_norm = 0.063571
Total gradient norm: 0.096756
=== Actor Training Debug (Iteration 292) ===
Q mean: -27.020645
Q std: 23.025803
Actor loss: 27.024597
Action reg: 0.003951
  l1.weight: grad_norm = 0.015625
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.050202
Total gradient norm: 0.081155
=== Actor Training Debug (Iteration 293) ===
Q mean: -27.449600
Q std: 22.909145
Actor loss: 27.453564
Action reg: 0.003964
  l1.weight: grad_norm = 0.048817
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.146009
Total gradient norm: 0.213222
=== Actor Training Debug (Iteration 294) ===
Q mean: -26.139290
Q std: 21.478367
Actor loss: 26.143209
Action reg: 0.003919
  l1.weight: grad_norm = 0.035469
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.113020
Total gradient norm: 0.159873
=== Actor Training Debug (Iteration 295) ===
Q mean: -29.501133
Q std: 20.734245
Actor loss: 29.505070
Action reg: 0.003938
  l1.weight: grad_norm = 0.006525
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.019236
Total gradient norm: 0.025572
=== Actor Training Debug (Iteration 296) ===
Q mean: -30.169930
Q std: 22.904854
Actor loss: 30.173912
Action reg: 0.003982
  l1.weight: grad_norm = 0.027616
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.087596
Total gradient norm: 0.152073
=== Actor Training Debug (Iteration 297) ===
Q mean: -28.215921
Q std: 21.606098
Actor loss: 28.219912
Action reg: 0.003990
  l1.weight: grad_norm = 0.046412
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.143564
Total gradient norm: 0.232023
=== Actor Training Debug (Iteration 298) ===
Q mean: -25.009268
Q std: 22.650492
Actor loss: 25.013216
Action reg: 0.003949
  l1.weight: grad_norm = 0.076412
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.210246
Total gradient norm: 0.336111
=== Actor Training Debug (Iteration 299) ===
Q mean: -28.487331
Q std: 22.905354
Actor loss: 28.491301
Action reg: 0.003969
  l1.weight: grad_norm = 0.008042
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.026696
Total gradient norm: 0.038870
=== Actor Training Debug (Iteration 300) ===
Q mean: -27.011469
Q std: 24.181929
Actor loss: 27.015436
Action reg: 0.003967
  l1.weight: grad_norm = 0.022515
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.075971
Total gradient norm: 0.121043
=== Actor Training Debug (Iteration 301) ===
Q mean: -26.632360
Q std: 23.236437
Actor loss: 26.636299
Action reg: 0.003940
  l1.weight: grad_norm = 0.011193
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.034884
Total gradient norm: 0.055165
=== Actor Training Debug (Iteration 302) ===
Q mean: -26.846195
Q std: 24.803419
Actor loss: 26.850147
Action reg: 0.003952
  l1.weight: grad_norm = 0.059837
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.180616
Total gradient norm: 0.288887
=== Actor Training Debug (Iteration 303) ===
Q mean: -29.371866
Q std: 22.599686
Actor loss: 29.375849
Action reg: 0.003983
  l1.weight: grad_norm = 0.004631
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.015644
Total gradient norm: 0.023940
=== Actor Training Debug (Iteration 304) ===
Q mean: -30.918087
Q std: 23.555414
Actor loss: 30.922085
Action reg: 0.003999
  l1.weight: grad_norm = 0.006374
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.020081
Total gradient norm: 0.030126
=== Actor Training Debug (Iteration 305) ===
Q mean: -27.171282
Q std: 20.076418
Actor loss: 27.175278
Action reg: 0.003996
  l1.weight: grad_norm = 0.019164
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.063898
Total gradient norm: 0.101466
=== Actor Training Debug (Iteration 306) ===
Q mean: -25.605778
Q std: 22.112885
Actor loss: 25.609760
Action reg: 0.003982
  l1.weight: grad_norm = 0.022009
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.070096
Total gradient norm: 0.101858
=== Actor Training Debug (Iteration 307) ===
Q mean: -28.707752
Q std: 23.271915
Actor loss: 28.711704
Action reg: 0.003951
  l1.weight: grad_norm = 0.073220
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.239768
Total gradient norm: 0.397081
=== Actor Training Debug (Iteration 308) ===
Q mean: -26.554386
Q std: 22.570015
Actor loss: 26.558340
Action reg: 0.003955
  l1.weight: grad_norm = 0.005341
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.016657
Total gradient norm: 0.022428
=== Actor Training Debug (Iteration 309) ===
Q mean: -31.590433
Q std: 23.331615
Actor loss: 31.594412
Action reg: 0.003979
  l1.weight: grad_norm = 0.071730
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.194127
Total gradient norm: 0.256910
=== Actor Training Debug (Iteration 310) ===
Q mean: -26.815895
Q std: 22.089695
Actor loss: 26.819847
Action reg: 0.003952
  l1.weight: grad_norm = 0.025932
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.080074
Total gradient norm: 0.120132
=== Actor Training Debug (Iteration 311) ===
Q mean: -28.872093
Q std: 21.558823
Actor loss: 28.876059
Action reg: 0.003965
  l1.weight: grad_norm = 0.021078
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.067272
Total gradient norm: 0.103662
=== Actor Training Debug (Iteration 312) ===
Q mean: -29.089882
Q std: 22.465504
Actor loss: 29.093859
Action reg: 0.003978
  l1.weight: grad_norm = 0.044084
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.142546
Total gradient norm: 0.210264
=== Actor Training Debug (Iteration 313) ===
Q mean: -30.600533
Q std: 20.841801
Actor loss: 30.604517
Action reg: 0.003984
  l1.weight: grad_norm = 0.010681
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.029574
Total gradient norm: 0.040662
=== Actor Training Debug (Iteration 314) ===
Q mean: -31.491747
Q std: 23.363340
Actor loss: 31.495710
Action reg: 0.003963
  l1.weight: grad_norm = 0.058956
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.163665
Total gradient norm: 0.229882
=== Actor Training Debug (Iteration 315) ===
Q mean: -24.467266
Q std: 21.304451
Actor loss: 24.471249
Action reg: 0.003983
  l1.weight: grad_norm = 0.087457
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.252260
Total gradient norm: 0.419770
=== Actor Training Debug (Iteration 316) ===
Q mean: -26.252415
Q std: 21.381123
Actor loss: 26.256311
Action reg: 0.003896
  l1.weight: grad_norm = 0.020622
  l1.bias: grad_norm = 0.000785
  l2.weight: grad_norm = 0.069053
Total gradient norm: 0.106018
=== Actor Training Debug (Iteration 317) ===
Q mean: -28.259605
Q std: 22.742302
Actor loss: 28.263559
Action reg: 0.003954
  l1.weight: grad_norm = 0.012125
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.039567
Total gradient norm: 0.065018
=== Actor Training Debug (Iteration 318) ===
Q mean: -30.702452
Q std: 22.877155
Actor loss: 30.706448
Action reg: 0.003997
  l1.weight: grad_norm = 0.022402
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.082591
Total gradient norm: 0.126060
=== Actor Training Debug (Iteration 319) ===
Q mean: -26.777573
Q std: 22.541361
Actor loss: 26.781521
Action reg: 0.003949
  l1.weight: grad_norm = 0.013826
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.050318
Total gradient norm: 0.084697
=== Actor Training Debug (Iteration 320) ===
Q mean: -25.192287
Q std: 20.151258
Actor loss: 25.196243
Action reg: 0.003956
  l1.weight: grad_norm = 0.006081
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.020680
Total gradient norm: 0.034414
=== Actor Training Debug (Iteration 321) ===
Q mean: -30.535101
Q std: 22.109411
Actor loss: 30.539040
Action reg: 0.003939
  l1.weight: grad_norm = 0.010651
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.034747
Total gradient norm: 0.049367
=== Actor Training Debug (Iteration 322) ===
Q mean: -29.308895
Q std: 22.325232
Actor loss: 29.312828
Action reg: 0.003933
  l1.weight: grad_norm = 0.073917
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.220415
Total gradient norm: 0.323499
=== Actor Training Debug (Iteration 323) ===
Q mean: -29.740528
Q std: 21.343185
Actor loss: 29.744507
Action reg: 0.003980
  l1.weight: grad_norm = 0.014450
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.041630
Total gradient norm: 0.059618
=== Actor Training Debug (Iteration 324) ===
Q mean: -26.503445
Q std: 22.778538
Actor loss: 26.507427
Action reg: 0.003983
  l1.weight: grad_norm = 0.008036
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.022525
Total gradient norm: 0.030760
=== Actor Training Debug (Iteration 325) ===
Q mean: -28.763912
Q std: 21.549263
Actor loss: 28.767910
Action reg: 0.003998
  l1.weight: grad_norm = 0.006749
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.021923
Total gradient norm: 0.035885
=== Actor Training Debug (Iteration 326) ===
Q mean: -30.490332
Q std: 23.203228
Actor loss: 30.494272
Action reg: 0.003941
  l1.weight: grad_norm = 0.001861
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.005766
Total gradient norm: 0.009333
=== Actor Training Debug (Iteration 327) ===
Q mean: -30.302380
Q std: 23.948212
Actor loss: 30.306303
Action reg: 0.003923
  l1.weight: grad_norm = 0.011311
  l1.bias: grad_norm = 0.000519
  l2.weight: grad_norm = 0.038191
Total gradient norm: 0.056356
=== Actor Training Debug (Iteration 328) ===
Q mean: -30.827282
Q std: 21.659601
Actor loss: 30.831203
Action reg: 0.003921
  l1.weight: grad_norm = 0.040049
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.134681
Total gradient norm: 0.243133
=== Actor Training Debug (Iteration 329) ===
Q mean: -28.123474
Q std: 23.042112
Actor loss: 28.127436
Action reg: 0.003962
  l1.weight: grad_norm = 0.054646
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.167606
Total gradient norm: 0.258192
=== Actor Training Debug (Iteration 330) ===
Q mean: -23.128147
Q std: 21.858889
Actor loss: 23.132080
Action reg: 0.003933
  l1.weight: grad_norm = 0.083002
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.248775
Total gradient norm: 0.420391
=== Actor Training Debug (Iteration 331) ===
Q mean: -28.549330
Q std: 23.372152
Actor loss: 28.553297
Action reg: 0.003967
  l1.weight: grad_norm = 0.008924
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.026185
Total gradient norm: 0.037903
=== Actor Training Debug (Iteration 332) ===
Q mean: -32.763420
Q std: 23.374121
Actor loss: 32.767387
Action reg: 0.003968
  l1.weight: grad_norm = 0.004919
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.014056
Total gradient norm: 0.019419
=== Actor Training Debug (Iteration 333) ===
Q mean: -34.253353
Q std: 22.714130
Actor loss: 34.257305
Action reg: 0.003953
  l1.weight: grad_norm = 0.023747
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.065590
Total gradient norm: 0.088425
=== Actor Training Debug (Iteration 334) ===
Q mean: -30.426231
Q std: 22.992367
Actor loss: 30.430222
Action reg: 0.003991
  l1.weight: grad_norm = 0.029706
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.081038
Total gradient norm: 0.105663
=== Actor Training Debug (Iteration 335) ===
Q mean: -24.768494
Q std: 21.997993
Actor loss: 24.772429
Action reg: 0.003935
  l1.weight: grad_norm = 0.050050
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.157092
Total gradient norm: 0.260938
=== Actor Training Debug (Iteration 336) ===
Q mean: -21.420849
Q std: 21.018538
Actor loss: 21.424799
Action reg: 0.003950
  l1.weight: grad_norm = 0.023523
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.064782
Total gradient norm: 0.084316
=== Actor Training Debug (Iteration 337) ===
Q mean: -25.905390
Q std: 21.653885
Actor loss: 25.909374
Action reg: 0.003984
  l1.weight: grad_norm = 0.007147
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.022719
Total gradient norm: 0.039564
=== Actor Training Debug (Iteration 338) ===
Q mean: -30.349957
Q std: 23.079857
Actor loss: 30.353924
Action reg: 0.003967
  l1.weight: grad_norm = 0.008231
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.027828
Total gradient norm: 0.049783
=== Actor Training Debug (Iteration 339) ===
Q mean: -31.495617
Q std: 21.160894
Actor loss: 31.499586
Action reg: 0.003969
  l1.weight: grad_norm = 0.023343
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.082744
Total gradient norm: 0.146645
=== Actor Training Debug (Iteration 340) ===
Q mean: -30.768398
Q std: 24.121687
Actor loss: 30.772354
Action reg: 0.003955
  l1.weight: grad_norm = 0.031279
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.100039
Total gradient norm: 0.153646
=== Actor Training Debug (Iteration 341) ===
Q mean: -24.911076
Q std: 20.849136
Actor loss: 24.915060
Action reg: 0.003985
  l1.weight: grad_norm = 0.001119
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.003466
Total gradient norm: 0.005554
=== Actor Training Debug (Iteration 342) ===
Q mean: -24.884953
Q std: 22.681353
Actor loss: 24.888880
Action reg: 0.003927
  l1.weight: grad_norm = 0.000772
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.003594
Total gradient norm: 0.007099
=== Actor Training Debug (Iteration 343) ===
Q mean: -24.928219
Q std: 20.712530
Actor loss: 24.932186
Action reg: 0.003968
  l1.weight: grad_norm = 0.039536
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.131071
Total gradient norm: 0.225529
=== Actor Training Debug (Iteration 344) ===
Q mean: -31.310463
Q std: 23.298128
Actor loss: 31.314415
Action reg: 0.003953
  l1.weight: grad_norm = 0.013930
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.039138
Total gradient norm: 0.055308
=== Actor Training Debug (Iteration 345) ===
Q mean: -34.623756
Q std: 24.443146
Actor loss: 34.627686
Action reg: 0.003927
  l1.weight: grad_norm = 0.006117
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.016697
Total gradient norm: 0.024232
=== Actor Training Debug (Iteration 346) ===
Q mean: -28.797401
Q std: 21.519911
Actor loss: 28.801373
Action reg: 0.003971
  l1.weight: grad_norm = 0.003172
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.009839
Total gradient norm: 0.016019
=== Actor Training Debug (Iteration 347) ===
Q mean: -25.944483
Q std: 22.581621
Actor loss: 25.948467
Action reg: 0.003984
  l1.weight: grad_norm = 0.003042
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.009213
Total gradient norm: 0.014577
=== Actor Training Debug (Iteration 348) ===
Q mean: -20.661751
Q std: 19.821228
Actor loss: 20.665718
Action reg: 0.003967
  l1.weight: grad_norm = 0.016261
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.057356
Total gradient norm: 0.090674
=== Actor Training Debug (Iteration 349) ===
Q mean: -24.798391
Q std: 22.609140
Actor loss: 24.802361
Action reg: 0.003969
  l1.weight: grad_norm = 0.004327
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.012797
Total gradient norm: 0.020574
=== Actor Training Debug (Iteration 350) ===
Q mean: -28.023659
Q std: 22.215048
Actor loss: 28.027643
Action reg: 0.003985
  l1.weight: grad_norm = 0.003197
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.008147
Total gradient norm: 0.012518
=== Actor Training Debug (Iteration 351) ===
Q mean: -33.976921
Q std: 24.603775
Actor loss: 33.980904
Action reg: 0.003981
  l1.weight: grad_norm = 0.038592
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.112809
Total gradient norm: 0.176339
=== Actor Training Debug (Iteration 352) ===
Q mean: -35.275200
Q std: 25.960258
Actor loss: 35.279179
Action reg: 0.003980
  l1.weight: grad_norm = 0.053153
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.152447
Total gradient norm: 0.199868
=== Actor Training Debug (Iteration 353) ===
Q mean: -24.745060
Q std: 19.221189
Actor loss: 24.749044
Action reg: 0.003985
  l1.weight: grad_norm = 0.003236
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.009183
Total gradient norm: 0.012553
=== Actor Training Debug (Iteration 354) ===
Q mean: -23.366020
Q std: 21.195019
Actor loss: 23.369987
Action reg: 0.003967
  l1.weight: grad_norm = 0.024627
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.079686
Total gradient norm: 0.126840
=== Actor Training Debug (Iteration 355) ===
Q mean: -26.749851
Q std: 24.531881
Actor loss: 26.753742
Action reg: 0.003890
  l1.weight: grad_norm = 0.056565
  l1.bias: grad_norm = 0.000903
  l2.weight: grad_norm = 0.151700
Total gradient norm: 0.202468
=== Actor Training Debug (Iteration 356) ===
Q mean: -31.107742
Q std: 23.325457
Actor loss: 31.111696
Action reg: 0.003954
  l1.weight: grad_norm = 0.016383
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.049661
Total gradient norm: 0.086515
=== Actor Training Debug (Iteration 357) ===
Q mean: -33.096107
Q std: 22.391624
Actor loss: 33.100033
Action reg: 0.003926
  l1.weight: grad_norm = 0.021797
  l1.bias: grad_norm = 0.000665
  l2.weight: grad_norm = 0.065258
Total gradient norm: 0.102424
=== Actor Training Debug (Iteration 358) ===
Q mean: -34.412369
Q std: 23.149685
Actor loss: 34.416279
Action reg: 0.003912
  l1.weight: grad_norm = 0.009352
  l1.bias: grad_norm = 0.000651
  l2.weight: grad_norm = 0.029191
Total gradient norm: 0.045043
=== Actor Training Debug (Iteration 359) ===
Q mean: -25.743622
Q std: 21.935480
Actor loss: 25.747602
Action reg: 0.003981
  l1.weight: grad_norm = 0.028656
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.081945
Total gradient norm: 0.110317
=== Actor Training Debug (Iteration 360) ===
Q mean: -25.955931
Q std: 21.248541
Actor loss: 25.959856
Action reg: 0.003925
  l1.weight: grad_norm = 0.013994
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.044632
Total gradient norm: 0.068949
=== Actor Training Debug (Iteration 361) ===
Q mean: -26.362434
Q std: 22.296486
Actor loss: 26.366371
Action reg: 0.003936
  l1.weight: grad_norm = 0.029460
  l1.bias: grad_norm = 0.000461
  l2.weight: grad_norm = 0.078239
Total gradient norm: 0.112276
=== Actor Training Debug (Iteration 362) ===
Q mean: -33.932465
Q std: 23.017096
Actor loss: 33.936398
Action reg: 0.003933
  l1.weight: grad_norm = 0.028645
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.096701
Total gradient norm: 0.163495
=== Actor Training Debug (Iteration 363) ===
Q mean: -32.601620
Q std: 21.516983
Actor loss: 32.605587
Action reg: 0.003966
  l1.weight: grad_norm = 0.023912
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.073683
Total gradient norm: 0.105023
=== Actor Training Debug (Iteration 364) ===
Q mean: -28.698036
Q std: 22.149141
Actor loss: 28.702015
Action reg: 0.003978
  l1.weight: grad_norm = 0.023706
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.065819
Total gradient norm: 0.086104
=== Actor Training Debug (Iteration 365) ===
Q mean: -25.959219
Q std: 22.041834
Actor loss: 25.963158
Action reg: 0.003939
  l1.weight: grad_norm = 0.017407
  l1.bias: grad_norm = 0.000435
  l2.weight: grad_norm = 0.056879
Total gradient norm: 0.085585
=== Actor Training Debug (Iteration 366) ===
Q mean: -23.517469
Q std: 22.360176
Actor loss: 23.521425
Action reg: 0.003955
  l1.weight: grad_norm = 0.006569
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.021535
Total gradient norm: 0.036339
=== Actor Training Debug (Iteration 367) ===
Q mean: -27.311897
Q std: 21.998865
Actor loss: 27.315836
Action reg: 0.003939
  l1.weight: grad_norm = 0.018090
  l1.bias: grad_norm = 0.000584
  l2.weight: grad_norm = 0.057199
Total gradient norm: 0.091564
=== Actor Training Debug (Iteration 368) ===
Q mean: -33.671989
Q std: 23.406799
Actor loss: 33.675972
Action reg: 0.003984
  l1.weight: grad_norm = 0.010905
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.035747
Total gradient norm: 0.057028
=== Actor Training Debug (Iteration 369) ===
Q mean: -34.574791
Q std: 24.071846
Actor loss: 34.578758
Action reg: 0.003968
  l1.weight: grad_norm = 0.016630
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.061248
Total gradient norm: 0.105684
=== Actor Training Debug (Iteration 370) ===
Q mean: -30.975285
Q std: 22.680593
Actor loss: 30.979284
Action reg: 0.003999
  l1.weight: grad_norm = 0.009648
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.031733
Total gradient norm: 0.051562
=== Actor Training Debug (Iteration 371) ===
Q mean: -24.738308
Q std: 21.228685
Actor loss: 24.742262
Action reg: 0.003953
  l1.weight: grad_norm = 0.011550
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.038241
Total gradient norm: 0.063445
=== Actor Training Debug (Iteration 372) ===
Q mean: -25.877880
Q std: 20.319759
Actor loss: 25.881794
Action reg: 0.003914
  l1.weight: grad_norm = 0.001626
  l1.bias: grad_norm = 0.000763
  l2.weight: grad_norm = 0.006345
Total gradient norm: 0.012015
=== Actor Training Debug (Iteration 373) ===
Q mean: -28.350887
Q std: 22.702862
Actor loss: 28.354855
Action reg: 0.003967
  l1.weight: grad_norm = 0.097009
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.304941
Total gradient norm: 0.501498
=== Actor Training Debug (Iteration 374) ===
Q mean: -35.958244
Q std: 24.144407
Actor loss: 35.962246
Action reg: 0.004000
  l1.weight: grad_norm = 0.000573
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001743
Total gradient norm: 0.002502
=== Actor Training Debug (Iteration 375) ===
Q mean: -34.374081
Q std: 23.158651
Actor loss: 34.378029
Action reg: 0.003949
  l1.weight: grad_norm = 0.041100
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.119776
Total gradient norm: 0.194261
=== Actor Training Debug (Iteration 376) ===
Q mean: -31.082825
Q std: 24.399948
Actor loss: 31.086748
Action reg: 0.003923
  l1.weight: grad_norm = 0.025504
  l1.bias: grad_norm = 0.000687
  l2.weight: grad_norm = 0.074846
Total gradient norm: 0.105001
=== Actor Training Debug (Iteration 377) ===
Q mean: -25.091213
Q std: 23.135660
Actor loss: 25.095184
Action reg: 0.003970
  l1.weight: grad_norm = 0.001225
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.004056
Total gradient norm: 0.007347
=== Actor Training Debug (Iteration 378) ===
Q mean: -23.888073
Q std: 23.024254
Actor loss: 23.892042
Action reg: 0.003969
  l1.weight: grad_norm = 0.001461
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.004844
Total gradient norm: 0.007715
=== Actor Training Debug (Iteration 379) ===
Q mean: -25.216633
Q std: 23.158016
Actor loss: 25.220629
Action reg: 0.003996
  l1.weight: grad_norm = 0.017611
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.053762
Total gradient norm: 0.090059
=== Actor Training Debug (Iteration 380) ===
Q mean: -30.791889
Q std: 23.688301
Actor loss: 30.795828
Action reg: 0.003939
  l1.weight: grad_norm = 0.004870
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.016935
Total gradient norm: 0.028008
=== Actor Training Debug (Iteration 381) ===
Q mean: -32.692657
Q std: 21.912008
Actor loss: 32.696655
Action reg: 0.003998
  l1.weight: grad_norm = 0.011420
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.032962
Total gradient norm: 0.052559
=== Actor Training Debug (Iteration 382) ===
Q mean: -33.090633
Q std: 22.310806
Actor loss: 33.094616
Action reg: 0.003983
  l1.weight: grad_norm = 0.011795
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.039995
Total gradient norm: 0.064235
=== Actor Training Debug (Iteration 383) ===
Q mean: -26.717293
Q std: 23.030878
Actor loss: 26.721273
Action reg: 0.003981
  l1.weight: grad_norm = 0.036526
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.115844
Total gradient norm: 0.195450
=== Actor Training Debug (Iteration 384) ===
Q mean: -25.908112
Q std: 22.038605
Actor loss: 25.912109
Action reg: 0.003997
  l1.weight: grad_norm = 0.043515
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.125376
Total gradient norm: 0.203648
=== Actor Training Debug (Iteration 385) ===
Q mean: -27.814869
Q std: 21.333111
Actor loss: 27.818836
Action reg: 0.003967
  l1.weight: grad_norm = 0.011063
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.037665
Total gradient norm: 0.065421
=== Actor Training Debug (Iteration 386) ===
Q mean: -30.960667
Q std: 22.481529
Actor loss: 30.964661
Action reg: 0.003994
  l1.weight: grad_norm = 0.035131
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.109542
Total gradient norm: 0.158495
=== Actor Training Debug (Iteration 387) ===
Q mean: -37.276119
Q std: 23.845083
Actor loss: 37.280075
Action reg: 0.003954
  l1.weight: grad_norm = 0.021458
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.063069
Total gradient norm: 0.087505
=== Actor Training Debug (Iteration 388) ===
Q mean: -32.153542
Q std: 23.692879
Actor loss: 32.157539
Action reg: 0.003998
  l1.weight: grad_norm = 0.028194
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.093325
Total gradient norm: 0.150483
=== Actor Training Debug (Iteration 389) ===
Q mean: -25.515879
Q std: 20.307795
Actor loss: 25.519836
Action reg: 0.003957
  l1.weight: grad_norm = 0.000684
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.002694
Total gradient norm: 0.005196
=== Actor Training Debug (Iteration 390) ===
Q mean: -22.286898
Q std: 18.136267
Actor loss: 22.290863
Action reg: 0.003966
  l1.weight: grad_norm = 0.010310
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.029469
Total gradient norm: 0.040853
=== Actor Training Debug (Iteration 391) ===
Q mean: -25.111027
Q std: 20.475458
Actor loss: 25.114996
Action reg: 0.003970
  l1.weight: grad_norm = 0.005074
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.013695
Total gradient norm: 0.018833
=== Actor Training Debug (Iteration 392) ===
Q mean: -34.020802
Q std: 21.651051
Actor loss: 34.024727
Action reg: 0.003926
  l1.weight: grad_norm = 0.024744
  l1.bias: grad_norm = 0.000561
  l2.weight: grad_norm = 0.069327
Total gradient norm: 0.090448
=== Actor Training Debug (Iteration 393) ===
Q mean: -33.693687
Q std: 23.596470
Actor loss: 33.697617
Action reg: 0.003928
  l1.weight: grad_norm = 0.019013
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.063056
Total gradient norm: 0.094988
=== Actor Training Debug (Iteration 394) ===
Q mean: -31.351215
Q std: 22.046820
Actor loss: 31.355181
Action reg: 0.003965
  l1.weight: grad_norm = 0.023862
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.060697
Total gradient norm: 0.080510
=== Actor Training Debug (Iteration 395) ===
Q mean: -30.362228
Q std: 21.268066
Actor loss: 30.366142
Action reg: 0.003913
  l1.weight: grad_norm = 0.001104
  l1.bias: grad_norm = 0.000728
  l2.weight: grad_norm = 0.005363
Total gradient norm: 0.010805
=== Actor Training Debug (Iteration 396) ===
Q mean: -26.223492
Q std: 23.873947
Actor loss: 26.227474
Action reg: 0.003983
  l1.weight: grad_norm = 0.018565
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.053326
Total gradient norm: 0.069027
=== Actor Training Debug (Iteration 397) ===
Q mean: -27.650574
Q std: 22.665947
Actor loss: 27.654554
Action reg: 0.003981
  l1.weight: grad_norm = 0.017159
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.062370
Total gradient norm: 0.095068
=== Actor Training Debug (Iteration 398) ===
Q mean: -29.450968
Q std: 23.315544
Actor loss: 29.454939
Action reg: 0.003972
  l1.weight: grad_norm = 0.010729
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.033559
Total gradient norm: 0.056563
=== Actor Training Debug (Iteration 399) ===
Q mean: -30.864952
Q std: 21.576767
Actor loss: 30.868931
Action reg: 0.003979
  l1.weight: grad_norm = 0.058240
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.152448
Total gradient norm: 0.209529
=== Actor Training Debug (Iteration 400) ===
Q mean: -27.186039
Q std: 22.385275
Actor loss: 27.190022
Action reg: 0.003983
  l1.weight: grad_norm = 0.011190
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.033534
Total gradient norm: 0.051356
=== Actor Training Debug (Iteration 401) ===
Q mean: -24.312305
Q std: 21.853582
Actor loss: 24.316271
Action reg: 0.003965
  l1.weight: grad_norm = 0.033514
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.124347
Total gradient norm: 0.218993
=== Actor Training Debug (Iteration 402) ===
Q mean: -27.418875
Q std: 21.491865
Actor loss: 27.422798
Action reg: 0.003923
  l1.weight: grad_norm = 0.027247
  l1.bias: grad_norm = 0.000557
  l2.weight: grad_norm = 0.097601
Total gradient norm: 0.147826
=== Actor Training Debug (Iteration 403) ===
Q mean: -28.096388
Q std: 21.201530
Actor loss: 28.100370
Action reg: 0.003983
  l1.weight: grad_norm = 0.006722
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.019668
Total gradient norm: 0.030636
=== Actor Training Debug (Iteration 404) ===
Q mean: -30.374653
Q std: 24.820549
Actor loss: 30.378611
Action reg: 0.003957
  l1.weight: grad_norm = 0.000781
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.002762
Total gradient norm: 0.004955
=== Actor Training Debug (Iteration 405) ===
Q mean: -29.594027
Q std: 23.371159
Actor loss: 29.598026
Action reg: 0.003999
  l1.weight: grad_norm = 0.004679
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.016247
Total gradient norm: 0.024174
=== Actor Training Debug (Iteration 406) ===
Q mean: -30.543755
Q std: 22.199554
Actor loss: 30.547724
Action reg: 0.003969
  l1.weight: grad_norm = 0.006705
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.020422
Total gradient norm: 0.033450
=== Actor Training Debug (Iteration 407) ===
Q mean: -28.108002
Q std: 20.481203
Actor loss: 28.111992
Action reg: 0.003990
  l1.weight: grad_norm = 0.084262
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.257955
Total gradient norm: 0.455023
=== Actor Training Debug (Iteration 408) ===
Q mean: -27.949776
Q std: 20.290615
Actor loss: 27.953747
Action reg: 0.003970
  l1.weight: grad_norm = 0.003375
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.012543
Total gradient norm: 0.021710
=== Actor Training Debug (Iteration 409) ===
Q mean: -27.407764
Q std: 22.311726
Actor loss: 27.411707
Action reg: 0.003943
  l1.weight: grad_norm = 0.001146
  l1.bias: grad_norm = 0.000639
  l2.weight: grad_norm = 0.004510
Total gradient norm: 0.008853
=== Actor Training Debug (Iteration 410) ===
Q mean: -28.729370
Q std: 20.823029
Actor loss: 28.733311
Action reg: 0.003940
  l1.weight: grad_norm = 0.016483
  l1.bias: grad_norm = 0.000566
  l2.weight: grad_norm = 0.050978
Total gradient norm: 0.087064
=== Actor Training Debug (Iteration 411) ===
Q mean: -28.611370
Q std: 21.302019
Actor loss: 28.615347
Action reg: 0.003976
  l1.weight: grad_norm = 0.041548
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.113828
Total gradient norm: 0.159807
=== Actor Training Debug (Iteration 412) ===
Q mean: -32.341930
Q std: 20.802973
Actor loss: 32.345917
Action reg: 0.003985
  l1.weight: grad_norm = 0.009457
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.028642
Total gradient norm: 0.038526
=== Actor Training Debug (Iteration 413) ===
Q mean: -29.158855
Q std: 22.026609
Actor loss: 29.162836
Action reg: 0.003980
  l1.weight: grad_norm = 0.045452
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.133578
Total gradient norm: 0.178625
=== Actor Training Debug (Iteration 414) ===
Q mean: -26.825569
Q std: 22.750549
Actor loss: 26.829519
Action reg: 0.003950
  l1.weight: grad_norm = 0.027020
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.079268
Total gradient norm: 0.119059
=== Actor Training Debug (Iteration 415) ===
Q mean: -26.618876
Q std: 23.021179
Actor loss: 26.622839
Action reg: 0.003964
  l1.weight: grad_norm = 0.047294
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.134270
Total gradient norm: 0.205151
=== Actor Training Debug (Iteration 416) ===
Q mean: -29.379601
Q std: 23.082695
Actor loss: 29.383526
Action reg: 0.003924
  l1.weight: grad_norm = 0.044051
  l1.bias: grad_norm = 0.000595
  l2.weight: grad_norm = 0.131541
Total gradient norm: 0.190758
=== Actor Training Debug (Iteration 417) ===
Q mean: -29.021105
Q std: 21.606979
Actor loss: 29.025076
Action reg: 0.003971
  l1.weight: grad_norm = 0.002779
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.010357
Total gradient norm: 0.016064
=== Actor Training Debug (Iteration 418) ===
Q mean: -28.749043
Q std: 21.842892
Actor loss: 28.753010
Action reg: 0.003967
  l1.weight: grad_norm = 0.010560
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.032652
Total gradient norm: 0.052286
=== Actor Training Debug (Iteration 419) ===
Q mean: -29.942371
Q std: 22.827038
Actor loss: 29.946327
Action reg: 0.003956
  l1.weight: grad_norm = 0.014098
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.040163
Total gradient norm: 0.061334
=== Actor Training Debug (Iteration 420) ===
Q mean: -30.601696
Q std: 23.318527
Actor loss: 30.605650
Action reg: 0.003954
  l1.weight: grad_norm = 0.050749
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.171595
Total gradient norm: 0.268619
=== Actor Training Debug (Iteration 421) ===
Q mean: -28.805954
Q std: 22.326246
Actor loss: 28.809923
Action reg: 0.003968
  l1.weight: grad_norm = 0.011934
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.034896
Total gradient norm: 0.046383
=== Actor Training Debug (Iteration 422) ===
Q mean: -28.659714
Q std: 24.089428
Actor loss: 28.663652
Action reg: 0.003939
  l1.weight: grad_norm = 0.010948
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.030146
Total gradient norm: 0.041217
=== Actor Training Debug (Iteration 423) ===
Q mean: -25.726288
Q std: 21.813765
Actor loss: 25.730228
Action reg: 0.003941
  l1.weight: grad_norm = 0.006233
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.022769
Total gradient norm: 0.037613
=== Actor Training Debug (Iteration 424) ===
Q mean: -26.869987
Q std: 21.115721
Actor loss: 26.873957
Action reg: 0.003970
  l1.weight: grad_norm = 0.015550
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.052863
Total gradient norm: 0.085785
=== Actor Training Debug (Iteration 425) ===
Q mean: -31.140972
Q std: 22.199806
Actor loss: 31.144941
Action reg: 0.003969
  l1.weight: grad_norm = 0.040105
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.124133
Total gradient norm: 0.183626
=== Actor Training Debug (Iteration 426) ===
Q mean: -28.679743
Q std: 23.129101
Actor loss: 28.683699
Action reg: 0.003956
  l1.weight: grad_norm = 0.012284
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.041120
Total gradient norm: 0.070837
=== Actor Training Debug (Iteration 427) ===
Q mean: -27.384613
Q std: 21.936203
Actor loss: 27.388569
Action reg: 0.003957
  l1.weight: grad_norm = 0.030830
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.089308
Total gradient norm: 0.140604
=== Actor Training Debug (Iteration 428) ===
Q mean: -29.342884
Q std: 21.817469
Actor loss: 29.346865
Action reg: 0.003981
  l1.weight: grad_norm = 0.013853
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.040570
Total gradient norm: 0.055472
=== Actor Training Debug (Iteration 429) ===
Q mean: -31.244949
Q std: 22.927479
Actor loss: 31.248934
Action reg: 0.003984
  l1.weight: grad_norm = 0.027355
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.085744
Total gradient norm: 0.146124
=== Actor Training Debug (Iteration 430) ===
Q mean: -28.482658
Q std: 21.063860
Actor loss: 28.486597
Action reg: 0.003939
  l1.weight: grad_norm = 0.045042
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.148252
Total gradient norm: 0.262642
=== Actor Training Debug (Iteration 431) ===
Q mean: -26.160284
Q std: 22.321978
Actor loss: 26.164209
Action reg: 0.003925
  l1.weight: grad_norm = 0.012194
  l1.bias: grad_norm = 0.000746
  l2.weight: grad_norm = 0.038628
Total gradient norm: 0.067038
=== Actor Training Debug (Iteration 432) ===
Q mean: -28.156425
Q std: 21.194956
Actor loss: 28.160391
Action reg: 0.003966
  l1.weight: grad_norm = 0.053496
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.153183
Total gradient norm: 0.213795
=== Actor Training Debug (Iteration 433) ===
Q mean: -29.967060
Q std: 21.413151
Actor loss: 29.971045
Action reg: 0.003984
  l1.weight: grad_norm = 0.009667
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.027190
Total gradient norm: 0.041807
=== Actor Training Debug (Iteration 434) ===
Q mean: -29.991056
Q std: 21.112406
Actor loss: 29.995041
Action reg: 0.003985
  l1.weight: grad_norm = 0.003479
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.010086
Total gradient norm: 0.016452
=== Actor Training Debug (Iteration 435) ===
Q mean: -29.780064
Q std: 22.071146
Actor loss: 29.784008
Action reg: 0.003944
  l1.weight: grad_norm = 0.001041
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.003556
Total gradient norm: 0.006594
=== Actor Training Debug (Iteration 436) ===
Q mean: -28.076565
Q std: 21.410402
Actor loss: 28.080534
Action reg: 0.003969
  l1.weight: grad_norm = 0.027168
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.093693
Total gradient norm: 0.156607
=== Actor Training Debug (Iteration 437) ===
Q mean: -29.486382
Q std: 23.388802
Actor loss: 29.490364
Action reg: 0.003982
  l1.weight: grad_norm = 0.019096
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.065448
Total gradient norm: 0.117084
=== Actor Training Debug (Iteration 438) ===
Q mean: -28.339703
Q std: 23.537741
Actor loss: 28.343670
Action reg: 0.003968
  l1.weight: grad_norm = 0.021526
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.064643
Total gradient norm: 0.082451
=== Actor Training Debug (Iteration 439) ===
Q mean: -26.678196
Q std: 21.675161
Actor loss: 26.682161
Action reg: 0.003965
  l1.weight: grad_norm = 0.052763
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.152502
Total gradient norm: 0.253531
=== Actor Training Debug (Iteration 440) ===
Q mean: -26.067492
Q std: 21.624565
Actor loss: 26.071472
Action reg: 0.003982
  l1.weight: grad_norm = 0.039064
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.114068
Total gradient norm: 0.175262
=== Actor Training Debug (Iteration 441) ===
Q mean: -27.326664
Q std: 23.373318
Actor loss: 27.330650
Action reg: 0.003985
  l1.weight: grad_norm = 0.000827
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.002722
Total gradient norm: 0.004201
=== Actor Training Debug (Iteration 442) ===
Q mean: -32.189621
Q std: 20.512108
Actor loss: 32.193577
Action reg: 0.003956
  l1.weight: grad_norm = 0.011858
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.040662
Total gradient norm: 0.062986
=== Actor Training Debug (Iteration 443) ===
Q mean: -33.135654
Q std: 23.132666
Actor loss: 33.139610
Action reg: 0.003957
  l1.weight: grad_norm = 0.004031
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.011849
Total gradient norm: 0.018841
=== Actor Training Debug (Iteration 444) ===
Q mean: -28.957649
Q std: 21.192066
Actor loss: 28.961630
Action reg: 0.003981
  l1.weight: grad_norm = 0.031874
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.109193
Total gradient norm: 0.183571
=== Actor Training Debug (Iteration 445) ===
Q mean: -29.476328
Q std: 21.888523
Actor loss: 29.480268
Action reg: 0.003941
  l1.weight: grad_norm = 0.007443
  l1.bias: grad_norm = 0.000525
  l2.weight: grad_norm = 0.023932
Total gradient norm: 0.038925
=== Actor Training Debug (Iteration 446) ===
Q mean: -26.765043
Q std: 20.097178
Actor loss: 26.769024
Action reg: 0.003980
  l1.weight: grad_norm = 0.021092
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.061838
Total gradient norm: 0.081344
=== Actor Training Debug (Iteration 447) ===
Q mean: -30.290281
Q std: 24.201553
Actor loss: 30.294266
Action reg: 0.003984
  l1.weight: grad_norm = 0.003078
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.009232
Total gradient norm: 0.012214
=== Actor Training Debug (Iteration 448) ===
Q mean: -28.876053
Q std: 24.595695
Actor loss: 28.880007
Action reg: 0.003955
  l1.weight: grad_norm = 0.015711
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.051958
Total gradient norm: 0.071184
=== Actor Training Debug (Iteration 449) ===
Q mean: -30.576242
Q std: 21.387608
Actor loss: 30.580198
Action reg: 0.003956
  l1.weight: grad_norm = 0.002099
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.006763
Total gradient norm: 0.011236
=== Actor Training Debug (Iteration 450) ===
Q mean: -31.217159
Q std: 24.853477
Actor loss: 31.221130
Action reg: 0.003972
  l1.weight: grad_norm = 0.001134
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.003665
Total gradient norm: 0.005761
=== Actor Training Debug (Iteration 451) ===
Q mean: -31.102928
Q std: 22.770668
Actor loss: 31.106899
Action reg: 0.003970
  l1.weight: grad_norm = 0.014068
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.036980
Total gradient norm: 0.048102
=== Actor Training Debug (Iteration 452) ===
Q mean: -29.142063
Q std: 22.407393
Actor loss: 29.146034
Action reg: 0.003971
  l1.weight: grad_norm = 0.000591
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.002059
Total gradient norm: 0.003943
=== Actor Training Debug (Iteration 453) ===
Q mean: -26.646320
Q std: 22.735312
Actor loss: 26.650278
Action reg: 0.003958
  l1.weight: grad_norm = 0.000922
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.003543
Total gradient norm: 0.006425
=== Actor Training Debug (Iteration 454) ===
Q mean: -30.806589
Q std: 24.054380
Actor loss: 30.810530
Action reg: 0.003941
  l1.weight: grad_norm = 0.011361
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.032874
Total gradient norm: 0.046266
=== Actor Training Debug (Iteration 455) ===
Q mean: -32.826958
Q std: 22.225773
Actor loss: 32.830898
Action reg: 0.003942
  l1.weight: grad_norm = 0.034840
  l1.bias: grad_norm = 0.000516
  l2.weight: grad_norm = 0.103849
Total gradient norm: 0.173894
=== Actor Training Debug (Iteration 456) ===
Q mean: -30.018806
Q std: 22.098822
Actor loss: 30.022778
Action reg: 0.003971
  l1.weight: grad_norm = 0.001791
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.005412
Total gradient norm: 0.008981
=== Actor Training Debug (Iteration 457) ===
Q mean: -27.425638
Q std: 20.954132
Actor loss: 27.429621
Action reg: 0.003983
  l1.weight: grad_norm = 0.015160
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.049290
Total gradient norm: 0.074892
=== Actor Training Debug (Iteration 458) ===
Q mean: -29.131466
Q std: 22.824928
Actor loss: 29.135406
Action reg: 0.003941
  l1.weight: grad_norm = 0.018536
  l1.bias: grad_norm = 0.000552
  l2.weight: grad_norm = 0.065449
Total gradient norm: 0.096922
=== Actor Training Debug (Iteration 459) ===
Q mean: -29.855995
Q std: 20.920868
Actor loss: 29.859966
Action reg: 0.003971
  l1.weight: grad_norm = 0.010982
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.039616
Total gradient norm: 0.062205
=== Actor Training Debug (Iteration 460) ===
Q mean: -31.407459
Q std: 22.885132
Actor loss: 31.411402
Action reg: 0.003943
  l1.weight: grad_norm = 0.002333
  l1.bias: grad_norm = 0.000585
  l2.weight: grad_norm = 0.007351
Total gradient norm: 0.012315
=== Actor Training Debug (Iteration 461) ===
Q mean: -27.728855
Q std: 22.497587
Actor loss: 27.732821
Action reg: 0.003966
  l1.weight: grad_norm = 0.004773
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.013899
Total gradient norm: 0.020752
=== Actor Training Debug (Iteration 462) ===
Q mean: -25.519815
Q std: 20.998980
Actor loss: 25.523815
Action reg: 0.004000
  l1.weight: grad_norm = 0.000092
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000292
Total gradient norm: 0.000504
=== Actor Training Debug (Iteration 463) ===
Q mean: -29.085299
Q std: 22.231674
Actor loss: 29.089264
Action reg: 0.003966
  l1.weight: grad_norm = 0.059569
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.184125
Total gradient norm: 0.286445
=== Actor Training Debug (Iteration 464) ===
Q mean: -31.438564
Q std: 21.436985
Actor loss: 31.442564
Action reg: 0.003999
  l1.weight: grad_norm = 0.001485
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.004166
Total gradient norm: 0.006957
=== Actor Training Debug (Iteration 465) ===
Q mean: -27.187395
Q std: 21.632881
Actor loss: 27.191349
Action reg: 0.003954
  l1.weight: grad_norm = 0.029045
  l1.bias: grad_norm = 0.000405
  l2.weight: grad_norm = 0.114050
Total gradient norm: 0.182292
=== Actor Training Debug (Iteration 466) ===
Q mean: -34.018524
Q std: 21.294483
Actor loss: 34.022511
Action reg: 0.003985
  l1.weight: grad_norm = 0.013974
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.041812
Total gradient norm: 0.067758
=== Actor Training Debug (Iteration 467) ===
Q mean: -29.563318
Q std: 20.698454
Actor loss: 29.567291
Action reg: 0.003973
  l1.weight: grad_norm = 0.000910
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.003004
Total gradient norm: 0.005436
=== Actor Training Debug (Iteration 468) ===
Q mean: -32.738472
Q std: 23.003637
Actor loss: 32.742413
Action reg: 0.003942
  l1.weight: grad_norm = 0.003009
  l1.bias: grad_norm = 0.000518
  l2.weight: grad_norm = 0.010000
Total gradient norm: 0.017154
=== Actor Training Debug (Iteration 469) ===
Q mean: -30.033243
Q std: 22.086071
Actor loss: 30.037186
Action reg: 0.003943
  l1.weight: grad_norm = 0.005841
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.018362
Total gradient norm: 0.032005
=== Actor Training Debug (Iteration 470) ===
Q mean: -31.128054
Q std: 21.106014
Actor loss: 31.132027
Action reg: 0.003972
  l1.weight: grad_norm = 0.001366
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.004319
Total gradient norm: 0.007090
=== Actor Training Debug (Iteration 471) ===
Q mean: -29.985508
Q std: 22.464701
Actor loss: 29.989447
Action reg: 0.003939
  l1.weight: grad_norm = 0.015623
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.043593
Total gradient norm: 0.058027
=== Actor Training Debug (Iteration 472) ===
Q mean: -29.363266
Q std: 22.846682
Actor loss: 29.367250
Action reg: 0.003984
  l1.weight: grad_norm = 0.014423
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.048003
Total gradient norm: 0.082034
=== Actor Training Debug (Iteration 473) ===
Q mean: -27.343649
Q std: 22.175854
Actor loss: 27.347614
Action reg: 0.003966
  l1.weight: grad_norm = 0.013220
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.037896
Total gradient norm: 0.049079
=== Actor Training Debug (Iteration 474) ===
Q mean: -30.714525
Q std: 22.802731
Actor loss: 30.718483
Action reg: 0.003957
  l1.weight: grad_norm = 0.000641
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.002494
Total gradient norm: 0.005078
=== Actor Training Debug (Iteration 475) ===
Q mean: -30.065140
Q std: 23.057545
Actor loss: 30.069092
Action reg: 0.003952
  l1.weight: grad_norm = 0.016570
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.052746
Total gradient norm: 0.075961
=== Actor Training Debug (Iteration 476) ===
Q mean: -32.838631
Q std: 22.869926
Actor loss: 32.842613
Action reg: 0.003982
  l1.weight: grad_norm = 0.023836
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.078088
Total gradient norm: 0.118665
=== Actor Training Debug (Iteration 477) ===
Q mean: -31.727194
Q std: 23.967146
Actor loss: 31.731150
Action reg: 0.003955
  l1.weight: grad_norm = 0.087794
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.324080
Total gradient norm: 0.538672
=== Actor Training Debug (Iteration 478) ===
Q mean: -27.505501
Q std: 22.511740
Actor loss: 27.509483
Action reg: 0.003982
  l1.weight: grad_norm = 0.017007
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.058515
Total gradient norm: 0.097929
=== Actor Training Debug (Iteration 479) ===
Q mean: -30.356247
Q std: 23.317730
Actor loss: 30.360205
Action reg: 0.003958
  l1.weight: grad_norm = 0.003039
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.009240
Total gradient norm: 0.015561
=== Actor Training Debug (Iteration 480) ===
Q mean: -29.168304
Q std: 22.852028
Actor loss: 29.172289
Action reg: 0.003985
  l1.weight: grad_norm = 0.009401
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.033245
Total gradient norm: 0.050268
=== Actor Training Debug (Iteration 481) ===
Q mean: -27.016317
Q std: 21.456873
Actor loss: 27.020302
Action reg: 0.003985
  l1.weight: grad_norm = 0.010233
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.033491
Total gradient norm: 0.061317
=== Actor Training Debug (Iteration 482) ===
Q mean: -30.317644
Q std: 19.710270
Actor loss: 30.321585
Action reg: 0.003941
  l1.weight: grad_norm = 0.012717
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.040427
Total gradient norm: 0.068136
=== Actor Training Debug (Iteration 483) ===
Q mean: -31.938129
Q std: 22.123920
Actor loss: 31.942112
Action reg: 0.003983
  l1.weight: grad_norm = 0.032764
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.114884
Total gradient norm: 0.198394
=== Actor Training Debug (Iteration 484) ===
Q mean: -31.940702
Q std: 20.351322
Actor loss: 31.944672
Action reg: 0.003969
  l1.weight: grad_norm = 0.016157
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.050077
Total gradient norm: 0.074000
=== Actor Training Debug (Iteration 485) ===
Q mean: -28.291283
Q std: 20.827309
Actor loss: 28.295265
Action reg: 0.003982
  l1.weight: grad_norm = 0.038471
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.103936
Total gradient norm: 0.143963
=== Actor Training Debug (Iteration 486) ===
Q mean: -26.157845
Q std: 21.233358
Actor loss: 26.161802
Action reg: 0.003958
  l1.weight: grad_norm = 0.005521
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.018455
Total gradient norm: 0.025707
=== Actor Training Debug (Iteration 487) ===
Q mean: -31.007082
Q std: 21.882687
Actor loss: 31.011040
Action reg: 0.003957
  l1.weight: grad_norm = 0.008414
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.026591
Total gradient norm: 0.041855
=== Actor Training Debug (Iteration 488) ===
Q mean: -33.398048
Q std: 21.467041
Actor loss: 33.401974
Action reg: 0.003927
  l1.weight: grad_norm = 0.020179
  l1.bias: grad_norm = 0.000630
  l2.weight: grad_norm = 0.061160
Total gradient norm: 0.104953
=== Actor Training Debug (Iteration 489) ===
Q mean: -32.760513
Q std: 23.903931
Actor loss: 32.764442
Action reg: 0.003927
  l1.weight: grad_norm = 0.015983
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.048026
Total gradient norm: 0.069090
=== Actor Training Debug (Iteration 490) ===
Q mean: -27.937229
Q std: 24.727327
Actor loss: 27.941198
Action reg: 0.003969
  l1.weight: grad_norm = 0.036333
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.111417
Total gradient norm: 0.176963
=== Actor Training Debug (Iteration 491) ===
Q mean: -25.624376
Q std: 22.355970
Actor loss: 25.628345
Action reg: 0.003970
  l1.weight: grad_norm = 0.000491
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.002006
Total gradient norm: 0.004049
=== Actor Training Debug (Iteration 492) ===
Q mean: -22.755781
Q std: 19.881031
Actor loss: 22.759724
Action reg: 0.003943
  l1.weight: grad_norm = 0.001156
  l1.bias: grad_norm = 0.000640
  l2.weight: grad_norm = 0.004493
Total gradient norm: 0.008795
=== Actor Training Debug (Iteration 493) ===
Q mean: -25.083498
Q std: 19.389065
Actor loss: 25.087454
Action reg: 0.003956
  l1.weight: grad_norm = 0.012583
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.047083
Total gradient norm: 0.075925
=== Actor Training Debug (Iteration 494) ===
Q mean: -31.207241
Q std: 19.517742
Actor loss: 31.211159
Action reg: 0.003917
  l1.weight: grad_norm = 0.001007
  l1.bias: grad_norm = 0.000779
  l2.weight: grad_norm = 0.005074
Total gradient norm: 0.010349
=== Actor Training Debug (Iteration 495) ===
Q mean: -33.523582
Q std: 21.136883
Actor loss: 33.527534
Action reg: 0.003954
  l1.weight: grad_norm = 0.017075
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.050348
Total gradient norm: 0.070815
=== Actor Training Debug (Iteration 496) ===
Q mean: -33.026154
Q std: 19.993216
Actor loss: 33.030151
Action reg: 0.003997
  l1.weight: grad_norm = 0.041513
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.147065
Total gradient norm: 0.239304
=== Actor Training Debug (Iteration 497) ===
Q mean: -26.392420
Q std: 19.438587
Actor loss: 26.396358
Action reg: 0.003939
  l1.weight: grad_norm = 0.038064
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.100787
Total gradient norm: 0.132284
=== Actor Training Debug (Iteration 498) ===
Q mean: -26.098518
Q std: 20.617599
Actor loss: 26.102461
Action reg: 0.003943
  l1.weight: grad_norm = 0.008437
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.023425
Total gradient norm: 0.032884
=== Actor Training Debug (Iteration 499) ===
Q mean: -29.853949
Q std: 21.393030
Actor loss: 29.857920
Action reg: 0.003971
  l1.weight: grad_norm = 0.002683
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.007502
Total gradient norm: 0.010099
=== Actor Training Debug (Iteration 500) ===
Q mean: -33.877800
Q std: 22.248293
Actor loss: 33.881798
Action reg: 0.003999
  l1.weight: grad_norm = 0.003046
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.010076
Total gradient norm: 0.015102
  Average reward: -341.094 | Average length: 100.0
Evaluation at episode 55: -341.094
=== Actor Training Debug (Iteration 501) ===
Q mean: -34.441368
Q std: 22.514429
Actor loss: 34.445335
Action reg: 0.003968
  l1.weight: grad_norm = 0.047644
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.166068
Total gradient norm: 0.289197
=== Actor Training Debug (Iteration 502) ===
Q mean: -29.927467
Q std: 22.783209
Actor loss: 29.931440
Action reg: 0.003972
  l1.weight: grad_norm = 0.001504
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.004439
Total gradient norm: 0.007594
=== Actor Training Debug (Iteration 503) ===
Q mean: -26.339417
Q std: 22.109308
Actor loss: 26.343390
Action reg: 0.003972
  l1.weight: grad_norm = 0.000974
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.002952
Total gradient norm: 0.004595
=== Actor Training Debug (Iteration 504) ===
Q mean: -24.635578
Q std: 21.300882
Actor loss: 24.639536
Action reg: 0.003958
  l1.weight: grad_norm = 0.009538
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.033441
Total gradient norm: 0.054301
=== Actor Training Debug (Iteration 505) ===
Q mean: -25.957394
Q std: 23.277441
Actor loss: 25.961348
Action reg: 0.003954
  l1.weight: grad_norm = 0.018529
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.057412
Total gradient norm: 0.085174
=== Actor Training Debug (Iteration 506) ===
Q mean: -29.091705
Q std: 21.510050
Actor loss: 29.095675
Action reg: 0.003969
  l1.weight: grad_norm = 0.017403
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.049598
Total gradient norm: 0.069210
=== Actor Training Debug (Iteration 507) ===
Q mean: -33.123806
Q std: 22.043486
Actor loss: 33.127792
Action reg: 0.003985
  l1.weight: grad_norm = 0.010234
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.030877
Total gradient norm: 0.052337
=== Actor Training Debug (Iteration 508) ===
Q mean: -31.578003
Q std: 23.027494
Actor loss: 31.581955
Action reg: 0.003952
  l1.weight: grad_norm = 0.045991
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.173097
Total gradient norm: 0.296863
=== Actor Training Debug (Iteration 509) ===
Q mean: -27.893621
Q std: 22.138477
Actor loss: 27.897579
Action reg: 0.003958
  l1.weight: grad_norm = 0.019522
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.066557
Total gradient norm: 0.111105
=== Actor Training Debug (Iteration 510) ===
Q mean: -26.762068
Q std: 21.500441
Actor loss: 26.766024
Action reg: 0.003957
  l1.weight: grad_norm = 0.028441
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.084793
Total gradient norm: 0.133664
=== Actor Training Debug (Iteration 511) ===
Q mean: -29.587528
Q std: 19.307526
Actor loss: 29.591471
Action reg: 0.003942
  l1.weight: grad_norm = 0.028857
  l1.bias: grad_norm = 0.000526
  l2.weight: grad_norm = 0.088868
Total gradient norm: 0.121617
=== Actor Training Debug (Iteration 512) ===
Q mean: -33.789635
Q std: 21.194447
Actor loss: 33.793591
Action reg: 0.003956
  l1.weight: grad_norm = 0.008243
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.026824
Total gradient norm: 0.045442
=== Actor Training Debug (Iteration 513) ===
Q mean: -26.993679
Q std: 20.116119
Actor loss: 26.997623
Action reg: 0.003945
  l1.weight: grad_norm = 0.014763
  l1.bias: grad_norm = 0.000544
  l2.weight: grad_norm = 0.053584
Total gradient norm: 0.084937
=== Actor Training Debug (Iteration 514) ===
Q mean: -28.719357
Q std: 22.056824
Actor loss: 28.723341
Action reg: 0.003984
  l1.weight: grad_norm = 0.032937
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.097294
Total gradient norm: 0.163712
=== Actor Training Debug (Iteration 515) ===
Q mean: -30.000206
Q std: 22.608957
Actor loss: 30.004145
Action reg: 0.003940
  l1.weight: grad_norm = 0.025328
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.081031
Total gradient norm: 0.112604
=== Actor Training Debug (Iteration 516) ===
Q mean: -32.263580
Q std: 22.596977
Actor loss: 32.267578
Action reg: 0.003999
  l1.weight: grad_norm = 0.002652
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.007788
Total gradient norm: 0.011724
=== Actor Training Debug (Iteration 517) ===
Q mean: -30.801102
Q std: 23.429014
Actor loss: 30.805088
Action reg: 0.003986
  l1.weight: grad_norm = 0.000517
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.001420
Total gradient norm: 0.002268
=== Actor Training Debug (Iteration 518) ===
Q mean: -29.897345
Q std: 22.357271
Actor loss: 29.901329
Action reg: 0.003985
  l1.weight: grad_norm = 0.009062
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.030400
Total gradient norm: 0.049999
=== Actor Training Debug (Iteration 519) ===
Q mean: -31.139376
Q std: 22.514141
Actor loss: 31.143349
Action reg: 0.003973
  l1.weight: grad_norm = 0.002984
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.010007
Total gradient norm: 0.018091
=== Actor Training Debug (Iteration 520) ===
Q mean: -30.361328
Q std: 20.329962
Actor loss: 30.365311
Action reg: 0.003983
  l1.weight: grad_norm = 0.043754
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.146332
Total gradient norm: 0.242856
=== Actor Training Debug (Iteration 521) ===
Q mean: -26.706909
Q std: 20.691025
Actor loss: 26.710852
Action reg: 0.003943
  l1.weight: grad_norm = 0.005346
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.016521
Total gradient norm: 0.026916
=== Actor Training Debug (Iteration 522) ===
Q mean: -29.603336
Q std: 22.518272
Actor loss: 29.607304
Action reg: 0.003968
  l1.weight: grad_norm = 0.020447
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.066467
Total gradient norm: 0.099833
=== Actor Training Debug (Iteration 523) ===
Q mean: -30.127396
Q std: 21.988348
Actor loss: 30.131365
Action reg: 0.003970
  l1.weight: grad_norm = 0.019716
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.076219
Total gradient norm: 0.130627
=== Actor Training Debug (Iteration 524) ===
Q mean: -32.541023
Q std: 23.042488
Actor loss: 32.544949
Action reg: 0.003925
  l1.weight: grad_norm = 0.028205
  l1.bias: grad_norm = 0.000623
  l2.weight: grad_norm = 0.102831
Total gradient norm: 0.184909
=== Actor Training Debug (Iteration 525) ===
Q mean: -31.458656
Q std: 21.580029
Actor loss: 31.462601
Action reg: 0.003944
  l1.weight: grad_norm = 0.000790
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.003366
Total gradient norm: 0.007015
=== Actor Training Debug (Iteration 526) ===
Q mean: -28.981388
Q std: 19.198523
Actor loss: 28.985329
Action reg: 0.003941
  l1.weight: grad_norm = 0.027405
  l1.bias: grad_norm = 0.000538
  l2.weight: grad_norm = 0.089344
Total gradient norm: 0.141803
=== Actor Training Debug (Iteration 527) ===
Q mean: -29.236288
Q std: 21.033648
Actor loss: 29.240229
Action reg: 0.003940
  l1.weight: grad_norm = 0.029815
  l1.bias: grad_norm = 0.000601
  l2.weight: grad_norm = 0.093826
Total gradient norm: 0.150468
=== Actor Training Debug (Iteration 528) ===
Q mean: -33.783150
Q std: 23.684107
Actor loss: 33.787106
Action reg: 0.003957
  l1.weight: grad_norm = 0.016597
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.050291
Total gradient norm: 0.083870
=== Actor Training Debug (Iteration 529) ===
Q mean: -31.391796
Q std: 22.349255
Actor loss: 31.395756
Action reg: 0.003960
  l1.weight: grad_norm = 0.002399
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.008149
Total gradient norm: 0.012719
=== Actor Training Debug (Iteration 530) ===
Q mean: -32.674076
Q std: 22.716370
Actor loss: 32.678062
Action reg: 0.003985
  l1.weight: grad_norm = 0.007563
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.025267
Total gradient norm: 0.043208
=== Actor Training Debug (Iteration 531) ===
Q mean: -27.168900
Q std: 21.462124
Actor loss: 27.172882
Action reg: 0.003983
  l1.weight: grad_norm = 0.003895
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.011831
Total gradient norm: 0.017744
=== Actor Training Debug (Iteration 532) ===
Q mean: -26.814915
Q std: 20.402498
Actor loss: 26.818853
Action reg: 0.003939
  l1.weight: grad_norm = 0.006114
  l1.bias: grad_norm = 0.000594
  l2.weight: grad_norm = 0.016401
Total gradient norm: 0.023501
=== Actor Training Debug (Iteration 533) ===
Q mean: -28.569592
Q std: 20.799765
Actor loss: 28.573547
Action reg: 0.003956
  l1.weight: grad_norm = 0.007996
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.023518
Total gradient norm: 0.037371
=== Actor Training Debug (Iteration 534) ===
Q mean: -33.980019
Q std: 20.762644
Actor loss: 33.983974
Action reg: 0.003956
  l1.weight: grad_norm = 0.001091
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.003835
Total gradient norm: 0.006788
=== Actor Training Debug (Iteration 535) ===
Q mean: -31.014244
Q std: 21.816004
Actor loss: 31.018190
Action reg: 0.003945
  l1.weight: grad_norm = 0.017969
  l1.bias: grad_norm = 0.000597
  l2.weight: grad_norm = 0.049955
Total gradient norm: 0.075050
=== Actor Training Debug (Iteration 536) ===
Q mean: -31.138147
Q std: 22.238583
Actor loss: 31.142107
Action reg: 0.003959
  l1.weight: grad_norm = 0.003967
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.011300
Total gradient norm: 0.018758
=== Actor Training Debug (Iteration 537) ===
Q mean: -25.295795
Q std: 18.970501
Actor loss: 25.299763
Action reg: 0.003968
  l1.weight: grad_norm = 0.027285
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.089762
Total gradient norm: 0.145700
=== Actor Training Debug (Iteration 538) ===
Q mean: -24.813610
Q std: 20.026508
Actor loss: 24.817554
Action reg: 0.003945
  l1.weight: grad_norm = 0.004418
  l1.bias: grad_norm = 0.000595
  l2.weight: grad_norm = 0.013894
Total gradient norm: 0.019385
=== Actor Training Debug (Iteration 539) ===
Q mean: -25.425358
Q std: 21.175137
Actor loss: 25.429352
Action reg: 0.003995
  l1.weight: grad_norm = 0.072583
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.234361
Total gradient norm: 0.353147
=== Actor Training Debug (Iteration 540) ===
Q mean: -32.348663
Q std: 22.086704
Actor loss: 32.352634
Action reg: 0.003971
  l1.weight: grad_norm = 0.012664
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.035413
Total gradient norm: 0.052042
=== Actor Training Debug (Iteration 541) ===
Q mean: -34.394993
Q std: 22.731323
Actor loss: 34.398975
Action reg: 0.003983
  l1.weight: grad_norm = 0.018283
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.059566
Total gradient norm: 0.098905
=== Actor Training Debug (Iteration 542) ===
Q mean: -30.576344
Q std: 20.652191
Actor loss: 30.580313
Action reg: 0.003970
  l1.weight: grad_norm = 0.030209
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.091434
Total gradient norm: 0.153199
=== Actor Training Debug (Iteration 543) ===
Q mean: -28.766571
Q std: 21.136307
Actor loss: 28.770527
Action reg: 0.003956
  l1.weight: grad_norm = 0.001839
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.006787
Total gradient norm: 0.011616
=== Actor Training Debug (Iteration 544) ===
Q mean: -30.106880
Q std: 20.872614
Actor loss: 30.110867
Action reg: 0.003986
  l1.weight: grad_norm = 0.008164
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.022316
Total gradient norm: 0.031689
=== Actor Training Debug (Iteration 545) ===
Q mean: -30.732445
Q std: 24.092194
Actor loss: 30.736374
Action reg: 0.003929
  l1.weight: grad_norm = 0.008216
  l1.bias: grad_norm = 0.000658
  l2.weight: grad_norm = 0.026988
Total gradient norm: 0.046458
=== Actor Training Debug (Iteration 546) ===
Q mean: -29.891996
Q std: 20.681396
Actor loss: 29.895967
Action reg: 0.003971
  l1.weight: grad_norm = 0.009338
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.028703
Total gradient norm: 0.043689
=== Actor Training Debug (Iteration 547) ===
Q mean: -29.581306
Q std: 21.913757
Actor loss: 29.585306
Action reg: 0.003999
  l1.weight: grad_norm = 0.006761
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.024073
Total gradient norm: 0.033845
=== Actor Training Debug (Iteration 548) ===
Q mean: -30.307991
Q std: 21.333290
Actor loss: 30.311964
Action reg: 0.003973
  l1.weight: grad_norm = 0.011997
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.042912
Total gradient norm: 0.073716
=== Actor Training Debug (Iteration 549) ===
Q mean: -30.333042
Q std: 21.528187
Actor loss: 30.337002
Action reg: 0.003959
  l1.weight: grad_norm = 0.008322
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.024811
Total gradient norm: 0.033752
=== Actor Training Debug (Iteration 550) ===
Q mean: -29.675444
Q std: 22.113970
Actor loss: 29.679394
Action reg: 0.003950
  l1.weight: grad_norm = 0.046221
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.114168
Total gradient norm: 0.148301
=== Actor Training Debug (Iteration 551) ===
Q mean: -27.886147
Q std: 20.771410
Actor loss: 27.890133
Action reg: 0.003986
  l1.weight: grad_norm = 0.004401
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.014949
Total gradient norm: 0.021784
=== Actor Training Debug (Iteration 552) ===
Q mean: -27.341526
Q std: 19.793072
Actor loss: 27.345413
Action reg: 0.003887
  l1.weight: grad_norm = 0.004989
  l1.bias: grad_norm = 0.001133
  l2.weight: grad_norm = 0.016679
Total gradient norm: 0.029757
=== Actor Training Debug (Iteration 553) ===
Q mean: -32.662685
Q std: 23.270767
Actor loss: 32.666672
Action reg: 0.003986
  l1.weight: grad_norm = 0.000723
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.002033
Total gradient norm: 0.003112
=== Actor Training Debug (Iteration 554) ===
Q mean: -27.895622
Q std: 19.807644
Actor loss: 27.899555
Action reg: 0.003933
  l1.weight: grad_norm = 0.001263
  l1.bias: grad_norm = 0.000679
  l2.weight: grad_norm = 0.004719
Total gradient norm: 0.008919
=== Actor Training Debug (Iteration 555) ===
Q mean: -30.684917
Q std: 22.383663
Actor loss: 30.688917
Action reg: 0.004000
  l1.weight: grad_norm = 0.002581
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.007216
Total gradient norm: 0.011082
=== Actor Training Debug (Iteration 556) ===
Q mean: -30.934475
Q std: 21.298277
Actor loss: 30.938446
Action reg: 0.003971
  l1.weight: grad_norm = 0.003593
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.012890
Total gradient norm: 0.023182
=== Actor Training Debug (Iteration 557) ===
Q mean: -31.114607
Q std: 20.074183
Actor loss: 31.118591
Action reg: 0.003984
  l1.weight: grad_norm = 0.008460
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.030113
Total gradient norm: 0.051953
=== Actor Training Debug (Iteration 558) ===
Q mean: -29.347786
Q std: 21.640972
Actor loss: 29.351782
Action reg: 0.003996
  l1.weight: grad_norm = 0.028848
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.091038
Total gradient norm: 0.159179
=== Actor Training Debug (Iteration 559) ===
Q mean: -30.332018
Q std: 22.749228
Actor loss: 30.335989
Action reg: 0.003971
  l1.weight: grad_norm = 0.005232
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.019314
Total gradient norm: 0.032783
=== Actor Training Debug (Iteration 560) ===
Q mean: -31.380371
Q std: 23.497572
Actor loss: 31.384342
Action reg: 0.003972
  l1.weight: grad_norm = 0.010954
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.039326
Total gradient norm: 0.060412
=== Actor Training Debug (Iteration 561) ===
Q mean: -30.599953
Q std: 22.644897
Actor loss: 30.603910
Action reg: 0.003958
  l1.weight: grad_norm = 0.010725
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.037046
Total gradient norm: 0.061989
=== Actor Training Debug (Iteration 562) ===
Q mean: -28.943161
Q std: 20.792246
Actor loss: 28.947142
Action reg: 0.003980
  l1.weight: grad_norm = 0.023476
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.067375
Total gradient norm: 0.089945
=== Actor Training Debug (Iteration 563) ===
Q mean: -23.817255
Q std: 20.614779
Actor loss: 23.821182
Action reg: 0.003927
  l1.weight: grad_norm = 0.016217
  l1.bias: grad_norm = 0.000641
  l2.weight: grad_norm = 0.041135
Total gradient norm: 0.057602
=== Actor Training Debug (Iteration 564) ===
Q mean: -28.517656
Q std: 21.398706
Actor loss: 28.521641
Action reg: 0.003984
  l1.weight: grad_norm = 0.035682
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.113957
Total gradient norm: 0.191053
=== Actor Training Debug (Iteration 565) ===
Q mean: -33.341152
Q std: 21.805965
Actor loss: 33.345123
Action reg: 0.003972
  l1.weight: grad_norm = 0.004023
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.012110
Total gradient norm: 0.018154
=== Actor Training Debug (Iteration 566) ===
Q mean: -31.481066
Q std: 21.088970
Actor loss: 31.485023
Action reg: 0.003957
  l1.weight: grad_norm = 0.009451
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.028523
Total gradient norm: 0.047281
=== Actor Training Debug (Iteration 567) ===
Q mean: -31.120785
Q std: 22.742905
Actor loss: 31.124743
Action reg: 0.003959
  l1.weight: grad_norm = 0.003699
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.013024
Total gradient norm: 0.021301
=== Actor Training Debug (Iteration 568) ===
Q mean: -27.081341
Q std: 19.796795
Actor loss: 27.085339
Action reg: 0.003998
  l1.weight: grad_norm = 0.007453
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.022772
Total gradient norm: 0.035777
=== Actor Training Debug (Iteration 569) ===
Q mean: -29.194614
Q std: 21.194752
Actor loss: 29.198570
Action reg: 0.003956
  l1.weight: grad_norm = 0.019788
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.066117
Total gradient norm: 0.114596
=== Actor Training Debug (Iteration 570) ===
Q mean: -33.065834
Q std: 22.296282
Actor loss: 33.069801
Action reg: 0.003968
  l1.weight: grad_norm = 0.016465
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.056734
Total gradient norm: 0.092153
=== Actor Training Debug (Iteration 571) ===
Q mean: -34.800785
Q std: 23.436340
Actor loss: 34.804745
Action reg: 0.003958
  l1.weight: grad_norm = 0.004315
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.011665
Total gradient norm: 0.017061
=== Actor Training Debug (Iteration 572) ===
Q mean: -28.794411
Q std: 22.551540
Actor loss: 28.798368
Action reg: 0.003957
  l1.weight: grad_norm = 0.017134
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.060420
Total gradient norm: 0.104016
=== Actor Training Debug (Iteration 573) ===
Q mean: -29.044281
Q std: 20.961395
Actor loss: 29.048225
Action reg: 0.003945
  l1.weight: grad_norm = 0.005929
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.015839
Total gradient norm: 0.021547
=== Actor Training Debug (Iteration 574) ===
Q mean: -28.279613
Q std: 21.506113
Actor loss: 28.283571
Action reg: 0.003958
  l1.weight: grad_norm = 0.022384
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.073722
Total gradient norm: 0.122450
=== Actor Training Debug (Iteration 575) ===
Q mean: -28.788746
Q std: 21.920412
Actor loss: 28.792730
Action reg: 0.003984
  l1.weight: grad_norm = 0.018012
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.053234
Total gradient norm: 0.085474
=== Actor Training Debug (Iteration 576) ===
Q mean: -28.513382
Q std: 20.617327
Actor loss: 28.517317
Action reg: 0.003935
  l1.weight: grad_norm = 0.035937
  l1.bias: grad_norm = 0.000670
  l2.weight: grad_norm = 0.124322
Total gradient norm: 0.183685
=== Actor Training Debug (Iteration 577) ===
Q mean: -30.284569
Q std: 21.204769
Actor loss: 30.288538
Action reg: 0.003970
  l1.weight: grad_norm = 0.013372
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.039882
Total gradient norm: 0.069170
=== Actor Training Debug (Iteration 578) ===
Q mean: -31.270765
Q std: 21.903357
Actor loss: 31.274710
Action reg: 0.003944
  l1.weight: grad_norm = 0.011140
  l1.bias: grad_norm = 0.000535
  l2.weight: grad_norm = 0.032246
Total gradient norm: 0.048514
=== Actor Training Debug (Iteration 579) ===
Q mean: -33.915699
Q std: 23.687836
Actor loss: 33.919643
Action reg: 0.003943
  l1.weight: grad_norm = 0.007523
  l1.bias: grad_norm = 0.000538
  l2.weight: grad_norm = 0.025419
Total gradient norm: 0.046137
=== Actor Training Debug (Iteration 580) ===
Q mean: -32.388474
Q std: 23.060177
Actor loss: 32.392448
Action reg: 0.003974
  l1.weight: grad_norm = 0.005006
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.014505
Total gradient norm: 0.023355
=== Actor Training Debug (Iteration 581) ===
Q mean: -28.836704
Q std: 22.557146
Actor loss: 28.840691
Action reg: 0.003986
  l1.weight: grad_norm = 0.005776
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.017443
Total gradient norm: 0.027045
=== Actor Training Debug (Iteration 582) ===
Q mean: -27.656368
Q std: 22.173046
Actor loss: 27.660326
Action reg: 0.003957
  l1.weight: grad_norm = 0.015587
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.050397
Total gradient norm: 0.085276
=== Actor Training Debug (Iteration 583) ===
Q mean: -32.082462
Q std: 25.006926
Actor loss: 32.086395
Action reg: 0.003931
  l1.weight: grad_norm = 0.035409
  l1.bias: grad_norm = 0.000709
  l2.weight: grad_norm = 0.113168
Total gradient norm: 0.155612
=== Actor Training Debug (Iteration 584) ===
Q mean: -32.574936
Q std: 22.658287
Actor loss: 32.578865
Action reg: 0.003929
  l1.weight: grad_norm = 0.018027
  l1.bias: grad_norm = 0.000738
  l2.weight: grad_norm = 0.054843
Total gradient norm: 0.089300
=== Actor Training Debug (Iteration 585) ===
Q mean: -29.007055
Q std: 17.963623
Actor loss: 29.011015
Action reg: 0.003960
  l1.weight: grad_norm = 0.000951
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.003417
Total gradient norm: 0.006244
=== Actor Training Debug (Iteration 586) ===
Q mean: -27.288750
Q std: 20.740507
Actor loss: 27.292747
Action reg: 0.003998
  l1.weight: grad_norm = 0.033000
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.110621
Total gradient norm: 0.172292
=== Actor Training Debug (Iteration 587) ===
Q mean: -28.866869
Q std: 21.045797
Actor loss: 28.870817
Action reg: 0.003948
  l1.weight: grad_norm = 0.000886
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.003532
Total gradient norm: 0.006922
=== Actor Training Debug (Iteration 588) ===
Q mean: -34.987061
Q std: 22.922573
Actor loss: 34.991016
Action reg: 0.003955
  l1.weight: grad_norm = 0.115578
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.369269
Total gradient norm: 0.634305
=== Actor Training Debug (Iteration 589) ===
Q mean: -31.108332
Q std: 21.888651
Actor loss: 31.112305
Action reg: 0.003973
  l1.weight: grad_norm = 0.000786
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.002385
Total gradient norm: 0.003958
=== Actor Training Debug (Iteration 590) ===
Q mean: -31.495937
Q std: 23.653004
Actor loss: 31.499903
Action reg: 0.003965
  l1.weight: grad_norm = 0.181344
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.516646
Total gradient norm: 0.797461
=== Actor Training Debug (Iteration 591) ===
Q mean: -29.623772
Q std: 22.529716
Actor loss: 29.627756
Action reg: 0.003984
  l1.weight: grad_norm = 0.019027
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.054318
Total gradient norm: 0.092713
=== Actor Training Debug (Iteration 592) ===
Q mean: -33.350037
Q std: 23.128584
Actor loss: 33.354012
Action reg: 0.003974
  l1.weight: grad_norm = 0.000889
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.002895
Total gradient norm: 0.005131
=== Actor Training Debug (Iteration 593) ===
Q mean: -30.233177
Q std: 21.615589
Actor loss: 30.237150
Action reg: 0.003974
  l1.weight: grad_norm = 0.000756
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.002216
Total gradient norm: 0.004062
=== Actor Training Debug (Iteration 594) ===
Q mean: -28.394983
Q std: 19.772207
Actor loss: 28.398972
Action reg: 0.003987
  l1.weight: grad_norm = 0.000427
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.001303
Total gradient norm: 0.002194
=== Actor Training Debug (Iteration 595) ===
Q mean: -28.560936
Q std: 20.265228
Actor loss: 28.564917
Action reg: 0.003981
  l1.weight: grad_norm = 0.057341
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.186026
Total gradient norm: 0.304710
=== Actor Training Debug (Iteration 596) ===
Q mean: -32.294739
Q std: 21.449913
Actor loss: 32.298698
Action reg: 0.003960
  l1.weight: grad_norm = 0.052044
  l1.bias: grad_norm = 0.000414
  l2.weight: grad_norm = 0.164550
Total gradient norm: 0.245934
=== Actor Training Debug (Iteration 597) ===
Q mean: -31.365294
Q std: 20.528410
Actor loss: 31.369267
Action reg: 0.003973
  l1.weight: grad_norm = 0.002046
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.005675
Total gradient norm: 0.008136
=== Actor Training Debug (Iteration 598) ===
Q mean: -28.964104
Q std: 19.545179
Actor loss: 28.968063
Action reg: 0.003960
  l1.weight: grad_norm = 0.004938
  l1.bias: grad_norm = 0.000509
  l2.weight: grad_norm = 0.016413
Total gradient norm: 0.029814
=== Actor Training Debug (Iteration 599) ===
Q mean: -28.051809
Q std: 19.075087
Actor loss: 28.055807
Action reg: 0.003998
  l1.weight: grad_norm = 0.012504
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.039759
Total gradient norm: 0.063827
=== Actor Training Debug (Iteration 600) ===
Q mean: -32.968899
Q std: 22.433683
Actor loss: 32.972885
Action reg: 0.003986
  l1.weight: grad_norm = 0.021184
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.065422
Total gradient norm: 0.108622
=== Actor Training Debug (Iteration 601) ===
Q mean: -32.073502
Q std: 23.792290
Actor loss: 32.077469
Action reg: 0.003966
  l1.weight: grad_norm = 0.050480
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.176395
Total gradient norm: 0.313400
=== Actor Training Debug (Iteration 602) ===
Q mean: -28.922260
Q std: 21.263109
Actor loss: 28.926197
Action reg: 0.003937
  l1.weight: grad_norm = 0.065696
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.176801
Total gradient norm: 0.238568
=== Actor Training Debug (Iteration 603) ===
Q mean: -30.120289
Q std: 21.660736
Actor loss: 30.124249
Action reg: 0.003961
  l1.weight: grad_norm = 0.012394
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.034388
Total gradient norm: 0.052936
=== Actor Training Debug (Iteration 604) ===
Q mean: -29.646614
Q std: 20.796688
Actor loss: 29.650585
Action reg: 0.003972
  l1.weight: grad_norm = 0.020854
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.064044
Total gradient norm: 0.102212
=== Actor Training Debug (Iteration 605) ===
Q mean: -31.838152
Q std: 22.124786
Actor loss: 31.842108
Action reg: 0.003957
  l1.weight: grad_norm = 0.023862
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.075942
Total gradient norm: 0.136887
=== Actor Training Debug (Iteration 606) ===
Q mean: -31.058386
Q std: 18.662205
Actor loss: 31.062370
Action reg: 0.003985
  l1.weight: grad_norm = 0.026556
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.074490
Total gradient norm: 0.111701
=== Actor Training Debug (Iteration 607) ===
Q mean: -28.584478
Q std: 19.386957
Actor loss: 28.588465
Action reg: 0.003987
  l1.weight: grad_norm = 0.001585
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.004788
Total gradient norm: 0.007879
=== Actor Training Debug (Iteration 608) ===
Q mean: -31.803526
Q std: 22.779009
Actor loss: 31.807486
Action reg: 0.003959
  l1.weight: grad_norm = 0.071530
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.235140
Total gradient norm: 0.365686
=== Actor Training Debug (Iteration 609) ===
Q mean: -35.379539
Q std: 24.196320
Actor loss: 35.383469
Action reg: 0.003928
  l1.weight: grad_norm = 0.048134
  l1.bias: grad_norm = 0.000760
  l2.weight: grad_norm = 0.143866
Total gradient norm: 0.251930
=== Actor Training Debug (Iteration 610) ===
Q mean: -27.912136
Q std: 18.651533
Actor loss: 27.916082
Action reg: 0.003947
  l1.weight: grad_norm = 0.001031
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.003860
Total gradient norm: 0.007680
=== Actor Training Debug (Iteration 611) ===
Q mean: -29.772997
Q std: 21.313633
Actor loss: 29.776966
Action reg: 0.003970
  l1.weight: grad_norm = 0.009070
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.027357
Total gradient norm: 0.044844
=== Actor Training Debug (Iteration 612) ===
Q mean: -32.096352
Q std: 20.647301
Actor loss: 32.100296
Action reg: 0.003944
  l1.weight: grad_norm = 0.042637
  l1.bias: grad_norm = 0.000637
  l2.weight: grad_norm = 0.129951
Total gradient norm: 0.204032
=== Actor Training Debug (Iteration 613) ===
Q mean: -32.120186
Q std: 24.389492
Actor loss: 32.124184
Action reg: 0.003998
  l1.weight: grad_norm = 0.000475
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001303
Total gradient norm: 0.001753
=== Actor Training Debug (Iteration 614) ===
Q mean: -29.998301
Q std: 19.688593
Actor loss: 30.002253
Action reg: 0.003953
  l1.weight: grad_norm = 0.049052
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.153550
Total gradient norm: 0.285306
=== Actor Training Debug (Iteration 615) ===
Q mean: -29.879848
Q std: 20.507053
Actor loss: 29.883806
Action reg: 0.003958
  l1.weight: grad_norm = 0.057597
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.155853
Total gradient norm: 0.258028
=== Actor Training Debug (Iteration 616) ===
Q mean: -29.290827
Q std: 21.149570
Actor loss: 29.294775
Action reg: 0.003948
  l1.weight: grad_norm = 0.020294
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.069684
Total gradient norm: 0.110014
=== Actor Training Debug (Iteration 617) ===
Q mean: -28.152117
Q std: 21.136528
Actor loss: 28.156113
Action reg: 0.003995
  l1.weight: grad_norm = 0.055591
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.170006
Total gradient norm: 0.271135
=== Actor Training Debug (Iteration 618) ===
Q mean: -31.156496
Q std: 20.733273
Actor loss: 31.160452
Action reg: 0.003956
  l1.weight: grad_norm = 0.053189
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.168236
Total gradient norm: 0.290881
=== Actor Training Debug (Iteration 619) ===
Q mean: -34.161209
Q std: 21.271544
Actor loss: 34.165142
Action reg: 0.003934
  l1.weight: grad_norm = 0.021802
  l1.bias: grad_norm = 0.000650
  l2.weight: grad_norm = 0.074235
Total gradient norm: 0.121488
=== Actor Training Debug (Iteration 620) ===
Q mean: -30.406061
Q std: 21.356798
Actor loss: 30.410044
Action reg: 0.003983
  l1.weight: grad_norm = 0.026694
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.082291
Total gradient norm: 0.106747
=== Actor Training Debug (Iteration 621) ===
Q mean: -27.103195
Q std: 20.737265
Actor loss: 27.107195
Action reg: 0.004000
  l1.weight: grad_norm = 0.004010
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.012365
Total gradient norm: 0.020893
=== Actor Training Debug (Iteration 622) ===
Q mean: -26.856581
Q std: 19.664663
Actor loss: 26.860554
Action reg: 0.003974
  l1.weight: grad_norm = 0.010030
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.030279
Total gradient norm: 0.049375
=== Actor Training Debug (Iteration 623) ===
Q mean: -31.161774
Q std: 21.067091
Actor loss: 31.165747
Action reg: 0.003974
  l1.weight: grad_norm = 0.018288
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.057005
Total gradient norm: 0.092203
=== Actor Training Debug (Iteration 624) ===
Q mean: -32.862007
Q std: 21.644796
Actor loss: 32.865963
Action reg: 0.003954
  l1.weight: grad_norm = 0.029714
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.092714
Total gradient norm: 0.155413
=== Actor Training Debug (Iteration 625) ===
Q mean: -30.323151
Q std: 20.292419
Actor loss: 30.327122
Action reg: 0.003970
  l1.weight: grad_norm = 0.025823
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.076110
Total gradient norm: 0.117882
=== Actor Training Debug (Iteration 626) ===
Q mean: -28.690929
Q std: 21.290926
Actor loss: 28.694891
Action reg: 0.003962
  l1.weight: grad_norm = 0.003887
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.014326
Total gradient norm: 0.026602
=== Actor Training Debug (Iteration 627) ===
Q mean: -33.792973
Q std: 21.510189
Actor loss: 33.796936
Action reg: 0.003963
  l1.weight: grad_norm = 0.005933
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.018242
Total gradient norm: 0.032613
=== Actor Training Debug (Iteration 628) ===
Q mean: -35.050148
Q std: 23.435682
Actor loss: 35.054142
Action reg: 0.003995
  l1.weight: grad_norm = 0.028843
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.084722
Total gradient norm: 0.132844
=== Actor Training Debug (Iteration 629) ===
Q mean: -29.209881
Q std: 21.161797
Actor loss: 29.213806
Action reg: 0.003926
  l1.weight: grad_norm = 0.001864
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.006287
Total gradient norm: 0.011527
=== Actor Training Debug (Iteration 630) ===
Q mean: -28.888046
Q std: 20.108076
Actor loss: 28.892017
Action reg: 0.003971
  l1.weight: grad_norm = 0.012580
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.034228
Total gradient norm: 0.050997
=== Actor Training Debug (Iteration 631) ===
Q mean: -32.802269
Q std: 22.625559
Actor loss: 32.806225
Action reg: 0.003954
  l1.weight: grad_norm = 0.019960
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.069457
Total gradient norm: 0.114325
=== Actor Training Debug (Iteration 632) ===
Q mean: -32.874332
Q std: 23.865801
Actor loss: 32.878288
Action reg: 0.003954
  l1.weight: grad_norm = 0.054113
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.186886
Total gradient norm: 0.336731
=== Actor Training Debug (Iteration 633) ===
Q mean: -29.670355
Q std: 21.963736
Actor loss: 29.674316
Action reg: 0.003962
  l1.weight: grad_norm = 0.014374
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.043875
Total gradient norm: 0.063482
=== Actor Training Debug (Iteration 634) ===
Q mean: -28.754478
Q std: 21.361074
Actor loss: 28.758425
Action reg: 0.003946
  l1.weight: grad_norm = 0.022470
  l1.bias: grad_norm = 0.000608
  l2.weight: grad_norm = 0.077591
Total gradient norm: 0.121527
=== Actor Training Debug (Iteration 635) ===
Q mean: -35.119385
Q std: 23.247446
Actor loss: 35.123360
Action reg: 0.003974
  l1.weight: grad_norm = 0.003653
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.010525
Total gradient norm: 0.015415
=== Actor Training Debug (Iteration 636) ===
Q mean: -32.463142
Q std: 23.788074
Actor loss: 32.467094
Action reg: 0.003951
  l1.weight: grad_norm = 0.001567
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.004754
Total gradient norm: 0.008736
=== Actor Training Debug (Iteration 637) ===
Q mean: -27.765596
Q std: 21.485588
Actor loss: 27.769543
Action reg: 0.003946
  l1.weight: grad_norm = 0.020101
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.069522
Total gradient norm: 0.122495
=== Actor Training Debug (Iteration 638) ===
Q mean: -27.608215
Q std: 20.744186
Actor loss: 27.612175
Action reg: 0.003960
  l1.weight: grad_norm = 0.026022
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.073199
Total gradient norm: 0.115705
=== Actor Training Debug (Iteration 639) ===
Q mean: -33.220375
Q std: 21.084024
Actor loss: 33.224346
Action reg: 0.003970
  l1.weight: grad_norm = 0.030834
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.084411
Total gradient norm: 0.118870
=== Actor Training Debug (Iteration 640) ===
Q mean: -33.895710
Q std: 21.691477
Actor loss: 33.899632
Action reg: 0.003920
  l1.weight: grad_norm = 0.007028
  l1.bias: grad_norm = 0.000812
  l2.weight: grad_norm = 0.022833
Total gradient norm: 0.038439
=== Actor Training Debug (Iteration 641) ===
Q mean: -30.503984
Q std: 20.057570
Actor loss: 30.507969
Action reg: 0.003985
  l1.weight: grad_norm = 0.034490
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.101648
Total gradient norm: 0.162624
=== Actor Training Debug (Iteration 642) ===
Q mean: -28.094683
Q std: 19.936653
Actor loss: 28.098644
Action reg: 0.003962
  l1.weight: grad_norm = 0.011058
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.033893
Total gradient norm: 0.054325
=== Actor Training Debug (Iteration 643) ===
Q mean: -34.360191
Q std: 22.505020
Actor loss: 34.364151
Action reg: 0.003960
  l1.weight: grad_norm = 0.052305
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.162366
Total gradient norm: 0.266919
=== Actor Training Debug (Iteration 644) ===
Q mean: -32.796379
Q std: 20.926598
Actor loss: 32.800354
Action reg: 0.003975
  l1.weight: grad_norm = 0.013735
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.041238
Total gradient norm: 0.061274
=== Actor Training Debug (Iteration 645) ===
Q mean: -30.324329
Q std: 21.109674
Actor loss: 30.328274
Action reg: 0.003945
  l1.weight: grad_norm = 0.075757
  l1.bias: grad_norm = 0.000559
  l2.weight: grad_norm = 0.247471
Total gradient norm: 0.455587
=== Actor Training Debug (Iteration 646) ===
Q mean: -30.588205
Q std: 19.093712
Actor loss: 30.592163
Action reg: 0.003958
  l1.weight: grad_norm = 0.086247
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.269551
Total gradient norm: 0.458074
=== Actor Training Debug (Iteration 647) ===
Q mean: -33.523819
Q std: 22.593744
Actor loss: 33.527817
Action reg: 0.003999
  l1.weight: grad_norm = 0.003524
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.009804
Total gradient norm: 0.013399
=== Actor Training Debug (Iteration 648) ===
Q mean: -33.563927
Q std: 23.988695
Actor loss: 33.567860
Action reg: 0.003932
  l1.weight: grad_norm = 0.009770
  l1.bias: grad_norm = 0.000731
  l2.weight: grad_norm = 0.030294
Total gradient norm: 0.048885
=== Actor Training Debug (Iteration 649) ===
Q mean: -30.089314
Q std: 20.432806
Actor loss: 30.093309
Action reg: 0.003997
  l1.weight: grad_norm = 0.027587
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.081470
Total gradient norm: 0.137102
=== Actor Training Debug (Iteration 650) ===
Q mean: -29.136913
Q std: 20.765667
Actor loss: 29.140871
Action reg: 0.003958
  l1.weight: grad_norm = 0.020326
  l1.bias: grad_norm = 0.000492
  l2.weight: grad_norm = 0.061829
Total gradient norm: 0.105871
=== Actor Training Debug (Iteration 651) ===
Q mean: -31.742281
Q std: 19.751741
Actor loss: 31.746218
Action reg: 0.003937
  l1.weight: grad_norm = 0.001133
  l1.bias: grad_norm = 0.000729
  l2.weight: grad_norm = 0.004408
Total gradient norm: 0.008778
=== Actor Training Debug (Iteration 652) ===
Q mean: -34.769470
Q std: 23.530819
Actor loss: 34.773430
Action reg: 0.003960
  l1.weight: grad_norm = 0.042457
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.152252
Total gradient norm: 0.261128
=== Actor Training Debug (Iteration 653) ===
Q mean: -30.520823
Q std: 19.606743
Actor loss: 30.524746
Action reg: 0.003924
  l1.weight: grad_norm = 0.054596
  l1.bias: grad_norm = 0.000781
  l2.weight: grad_norm = 0.186436
Total gradient norm: 0.311745
=== Actor Training Debug (Iteration 654) ===
Q mean: -32.533325
Q std: 20.814646
Actor loss: 32.537315
Action reg: 0.003988
  l1.weight: grad_norm = 0.003732
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.011950
Total gradient norm: 0.020775
=== Actor Training Debug (Iteration 655) ===
Q mean: -33.337124
Q std: 21.595318
Actor loss: 33.341045
Action reg: 0.003922
  l1.weight: grad_norm = 0.106959
  l1.bias: grad_norm = 0.000850
  l2.weight: grad_norm = 0.332171
Total gradient norm: 0.557690
=== Actor Training Debug (Iteration 656) ===
Q mean: -31.932077
Q std: 21.597857
Actor loss: 31.936026
Action reg: 0.003948
  l1.weight: grad_norm = 0.052786
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.173891
Total gradient norm: 0.284791
=== Actor Training Debug (Iteration 657) ===
Q mean: -27.995800
Q std: 21.331781
Actor loss: 27.999760
Action reg: 0.003959
  l1.weight: grad_norm = 0.001544
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.004551
Total gradient norm: 0.007278
=== Actor Training Debug (Iteration 658) ===
Q mean: -31.655016
Q std: 19.659029
Actor loss: 31.658991
Action reg: 0.003975
  l1.weight: grad_norm = 0.000855
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.002787
Total gradient norm: 0.005028
=== Actor Training Debug (Iteration 659) ===
Q mean: -34.288292
Q std: 20.744673
Actor loss: 34.292278
Action reg: 0.003986
  l1.weight: grad_norm = 0.001348
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.003908
Total gradient norm: 0.006111
=== Actor Training Debug (Iteration 660) ===
Q mean: -32.630898
Q std: 21.469883
Actor loss: 32.634876
Action reg: 0.003978
  l1.weight: grad_norm = 0.100573
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.290216
Total gradient norm: 0.459204
=== Actor Training Debug (Iteration 661) ===
Q mean: -33.882542
Q std: 20.057371
Actor loss: 33.886497
Action reg: 0.003957
  l1.weight: grad_norm = 0.022256
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.063067
Total gradient norm: 0.105141
=== Actor Training Debug (Iteration 662) ===
Q mean: -28.836990
Q std: 22.774141
Actor loss: 28.840961
Action reg: 0.003971
  l1.weight: grad_norm = 0.007412
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.021493
Total gradient norm: 0.030188
=== Actor Training Debug (Iteration 663) ===
Q mean: -30.702744
Q std: 21.162258
Actor loss: 30.706720
Action reg: 0.003976
  l1.weight: grad_norm = 0.005752
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.019293
Total gradient norm: 0.030449
=== Actor Training Debug (Iteration 664) ===
Q mean: -31.499687
Q std: 20.653288
Actor loss: 31.503658
Action reg: 0.003971
  l1.weight: grad_norm = 0.013503
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.043479
Total gradient norm: 0.070721
=== Actor Training Debug (Iteration 665) ===
Q mean: -30.910217
Q std: 18.300646
Actor loss: 30.914192
Action reg: 0.003974
  l1.weight: grad_norm = 0.017776
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.056193
Total gradient norm: 0.089105
=== Actor Training Debug (Iteration 666) ===
Q mean: -29.818405
Q std: 20.686598
Actor loss: 29.822369
Action reg: 0.003963
  l1.weight: grad_norm = 0.000744
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.002818
Total gradient norm: 0.005438
=== Actor Training Debug (Iteration 667) ===
Q mean: -28.370644
Q std: 19.096510
Actor loss: 28.374594
Action reg: 0.003951
  l1.weight: grad_norm = 0.009255
  l1.bias: grad_norm = 0.000588
  l2.weight: grad_norm = 0.029580
Total gradient norm: 0.054158
=== Actor Training Debug (Iteration 668) ===
Q mean: -32.421726
Q std: 21.847382
Actor loss: 32.425713
Action reg: 0.003984
  l1.weight: grad_norm = 0.032772
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.107076
Total gradient norm: 0.171618
=== Actor Training Debug (Iteration 669) ===
Q mean: -28.351921
Q std: 18.595093
Actor loss: 28.355824
Action reg: 0.003902
  l1.weight: grad_norm = 0.006209
  l1.bias: grad_norm = 0.001010
  l2.weight: grad_norm = 0.018866
Total gradient norm: 0.031867
=== Actor Training Debug (Iteration 670) ===
Q mean: -29.016888
Q std: 19.378927
Actor loss: 29.020836
Action reg: 0.003948
  l1.weight: grad_norm = 0.006539
  l1.bias: grad_norm = 0.000598
  l2.weight: grad_norm = 0.017923
Total gradient norm: 0.028120
=== Actor Training Debug (Iteration 671) ===
Q mean: -32.313492
Q std: 21.344126
Actor loss: 32.317451
Action reg: 0.003960
  l1.weight: grad_norm = 0.004072
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.011811
Total gradient norm: 0.019760
=== Actor Training Debug (Iteration 672) ===
Q mean: -33.259991
Q std: 22.635187
Actor loss: 33.263927
Action reg: 0.003935
  l1.weight: grad_norm = 0.024760
  l1.bias: grad_norm = 0.000644
  l2.weight: grad_norm = 0.079436
Total gradient norm: 0.136019
=== Actor Training Debug (Iteration 673) ===
Q mean: -30.588888
Q std: 21.279667
Actor loss: 30.592888
Action reg: 0.004000
  l1.weight: grad_norm = 0.002301
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.007371
Total gradient norm: 0.012329
=== Actor Training Debug (Iteration 674) ===
Q mean: -31.170040
Q std: 21.945164
Actor loss: 31.173975
Action reg: 0.003935
  l1.weight: grad_norm = 0.022498
  l1.bias: grad_norm = 0.000750
  l2.weight: grad_norm = 0.079953
Total gradient norm: 0.129408
=== Actor Training Debug (Iteration 675) ===
Q mean: -29.965675
Q std: 20.252165
Actor loss: 29.969631
Action reg: 0.003956
  l1.weight: grad_norm = 0.032645
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.096189
Total gradient norm: 0.132182
=== Actor Training Debug (Iteration 676) ===
Q mean: -32.364147
Q std: 22.005423
Actor loss: 32.368145
Action reg: 0.003998
  l1.weight: grad_norm = 0.011837
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.033416
Total gradient norm: 0.050871
=== Actor Training Debug (Iteration 677) ===
Q mean: -35.328239
Q std: 22.500504
Actor loss: 35.332199
Action reg: 0.003959
  l1.weight: grad_norm = 0.032263
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.094585
Total gradient norm: 0.156836
=== Actor Training Debug (Iteration 678) ===
Q mean: -30.632013
Q std: 20.074545
Actor loss: 30.635988
Action reg: 0.003975
  l1.weight: grad_norm = 0.016862
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.052559
Total gradient norm: 0.088021
=== Actor Training Debug (Iteration 679) ===
Q mean: -31.451908
Q std: 19.616926
Actor loss: 31.455872
Action reg: 0.003963
  l1.weight: grad_norm = 0.009237
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.029125
Total gradient norm: 0.048614
=== Actor Training Debug (Iteration 680) ===
Q mean: -33.826859
Q std: 22.018967
Actor loss: 33.830856
Action reg: 0.003996
  l1.weight: grad_norm = 0.021020
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.051619
Total gradient norm: 0.068889
=== Actor Training Debug (Iteration 681) ===
Q mean: -32.373283
Q std: 21.572010
Actor loss: 32.377247
Action reg: 0.003962
  l1.weight: grad_norm = 0.000699
  l1.bias: grad_norm = 0.000428
  l2.weight: grad_norm = 0.002473
Total gradient norm: 0.004992
=== Actor Training Debug (Iteration 682) ===
Q mean: -30.182209
Q std: 21.692554
Actor loss: 30.186155
Action reg: 0.003946
  l1.weight: grad_norm = 0.065041
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.228029
Total gradient norm: 0.389467
=== Actor Training Debug (Iteration 683) ===
Q mean: -31.232189
Q std: 20.157539
Actor loss: 31.236139
Action reg: 0.003951
  l1.weight: grad_norm = 0.000954
  l1.bias: grad_norm = 0.000589
  l2.weight: grad_norm = 0.003692
Total gradient norm: 0.007381
=== Actor Training Debug (Iteration 684) ===
Q mean: -33.664452
Q std: 19.994864
Actor loss: 33.668400
Action reg: 0.003949
  l1.weight: grad_norm = 0.067542
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.229940
Total gradient norm: 0.415595
=== Actor Training Debug (Iteration 685) ===
Q mean: -28.393539
Q std: 19.165298
Actor loss: 28.397478
Action reg: 0.003938
  l1.weight: grad_norm = 0.001903
  l1.bias: grad_norm = 0.000688
  l2.weight: grad_norm = 0.006632
Total gradient norm: 0.011986
=== Actor Training Debug (Iteration 686) ===
Q mean: -32.397541
Q std: 21.906710
Actor loss: 32.401527
Action reg: 0.003986
  l1.weight: grad_norm = 0.027978
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.092753
Total gradient norm: 0.156279
=== Actor Training Debug (Iteration 687) ===
Q mean: -30.816875
Q std: 20.148413
Actor loss: 30.820841
Action reg: 0.003965
  l1.weight: grad_norm = 0.001141
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.003433
Total gradient norm: 0.005680
=== Actor Training Debug (Iteration 688) ===
Q mean: -33.656845
Q std: 21.912926
Actor loss: 33.660805
Action reg: 0.003959
  l1.weight: grad_norm = 0.008969
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.030867
Total gradient norm: 0.055538
=== Actor Training Debug (Iteration 689) ===
Q mean: -29.745697
Q std: 20.739233
Actor loss: 29.749634
Action reg: 0.003937
  l1.weight: grad_norm = 0.000942
  l1.bias: grad_norm = 0.000709
  l2.weight: grad_norm = 0.003630
Total gradient norm: 0.007805
=== Actor Training Debug (Iteration 690) ===
Q mean: -32.782600
Q std: 21.864672
Actor loss: 32.786575
Action reg: 0.003975
  l1.weight: grad_norm = 0.024656
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.079317
Total gradient norm: 0.118470
=== Actor Training Debug (Iteration 691) ===
Q mean: -34.009628
Q std: 19.865173
Actor loss: 34.013603
Action reg: 0.003977
  l1.weight: grad_norm = 0.001769
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.005671
Total gradient norm: 0.009058
=== Actor Training Debug (Iteration 692) ===
Q mean: -33.640690
Q std: 20.639524
Actor loss: 33.644642
Action reg: 0.003950
  l1.weight: grad_norm = 0.020543
  l1.bias: grad_norm = 0.000582
  l2.weight: grad_norm = 0.066274
Total gradient norm: 0.104880
=== Actor Training Debug (Iteration 693) ===
Q mean: -30.001753
Q std: 21.469969
Actor loss: 30.005718
Action reg: 0.003966
  l1.weight: grad_norm = 0.000862
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.002860
Total gradient norm: 0.005467
=== Actor Training Debug (Iteration 694) ===
Q mean: -31.844028
Q std: 19.798616
Actor loss: 31.847977
Action reg: 0.003947
  l1.weight: grad_norm = 0.042825
  l1.bias: grad_norm = 0.000590
  l2.weight: grad_norm = 0.140723
Total gradient norm: 0.227574
=== Actor Training Debug (Iteration 695) ===
Q mean: -29.901573
Q std: 21.403158
Actor loss: 29.905508
Action reg: 0.003936
  l1.weight: grad_norm = 0.041125
  l1.bias: grad_norm = 0.000729
  l2.weight: grad_norm = 0.138408
Total gradient norm: 0.224291
=== Actor Training Debug (Iteration 696) ===
Q mean: -33.685089
Q std: 24.455639
Actor loss: 33.689026
Action reg: 0.003937
  l1.weight: grad_norm = 0.064466
  l1.bias: grad_norm = 0.000705
  l2.weight: grad_norm = 0.209528
Total gradient norm: 0.367482
=== Actor Training Debug (Iteration 697) ===
Q mean: -32.581619
Q std: 19.749153
Actor loss: 32.585567
Action reg: 0.003949
  l1.weight: grad_norm = 0.009382
  l1.bias: grad_norm = 0.000585
  l2.weight: grad_norm = 0.028841
Total gradient norm: 0.049690
=== Actor Training Debug (Iteration 698) ===
Q mean: -32.878025
Q std: 20.397251
Actor loss: 32.882011
Action reg: 0.003985
  l1.weight: grad_norm = 0.011050
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.034154
Total gradient norm: 0.049099
=== Actor Training Debug (Iteration 699) ===
Q mean: -30.683819
Q std: 18.343445
Actor loss: 30.687805
Action reg: 0.003986
  l1.weight: grad_norm = 0.008374
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.027600
Total gradient norm: 0.044695
=== Actor Training Debug (Iteration 700) ===
Q mean: -32.608288
Q std: 21.304754
Actor loss: 32.612274
Action reg: 0.003988
  l1.weight: grad_norm = 0.000801
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.002502
Total gradient norm: 0.003976
=== Actor Training Debug (Iteration 701) ===
Q mean: -36.544632
Q std: 21.163195
Actor loss: 36.548630
Action reg: 0.003998
  l1.weight: grad_norm = 0.003564
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.011973
Total gradient norm: 0.019824
=== Actor Training Debug (Iteration 702) ===
Q mean: -32.841652
Q std: 21.482037
Actor loss: 32.845612
Action reg: 0.003961
  l1.weight: grad_norm = 0.044892
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.155745
Total gradient norm: 0.245512
=== Actor Training Debug (Iteration 703) ===
Q mean: -29.902969
Q std: 21.487585
Actor loss: 29.906944
Action reg: 0.003975
  l1.weight: grad_norm = 0.049566
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.140885
Total gradient norm: 0.243772
=== Actor Training Debug (Iteration 704) ===
Q mean: -33.770531
Q std: 23.018051
Actor loss: 33.774471
Action reg: 0.003940
  l1.weight: grad_norm = 0.047716
  l1.bias: grad_norm = 0.000618
  l2.weight: grad_norm = 0.142460
Total gradient norm: 0.242213
=== Actor Training Debug (Iteration 705) ===
Q mean: -34.202507
Q std: 20.525730
Actor loss: 34.206490
Action reg: 0.003984
  l1.weight: grad_norm = 0.027486
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.090356
Total gradient norm: 0.143678
=== Actor Training Debug (Iteration 706) ===
Q mean: -34.017044
Q std: 18.989981
Actor loss: 34.021030
Action reg: 0.003987
  l1.weight: grad_norm = 0.005957
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.020432
Total gradient norm: 0.031316
=== Actor Training Debug (Iteration 707) ===
Q mean: -30.569603
Q std: 20.918818
Actor loss: 30.573532
Action reg: 0.003929
  l1.weight: grad_norm = 0.007604
  l1.bias: grad_norm = 0.000778
  l2.weight: grad_norm = 0.021714
Total gradient norm: 0.034619
=== Actor Training Debug (Iteration 708) ===
Q mean: -28.570061
Q std: 20.262918
Actor loss: 28.574026
Action reg: 0.003965
  l1.weight: grad_norm = 0.003327
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.008222
Total gradient norm: 0.011888
=== Actor Training Debug (Iteration 709) ===
Q mean: -30.417660
Q std: 20.667500
Actor loss: 30.421621
Action reg: 0.003961
  l1.weight: grad_norm = 0.014521
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.043422
Total gradient norm: 0.072014
=== Actor Training Debug (Iteration 710) ===
Q mean: -32.449635
Q std: 21.672129
Actor loss: 32.453609
Action reg: 0.003975
  l1.weight: grad_norm = 0.000944
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.003028
Total gradient norm: 0.004971
=== Actor Training Debug (Iteration 711) ===
Q mean: -35.386436
Q std: 23.359385
Actor loss: 35.390385
Action reg: 0.003949
  l1.weight: grad_norm = 0.008803
  l1.bias: grad_norm = 0.000556
  l2.weight: grad_norm = 0.029635
Total gradient norm: 0.051598
=== Actor Training Debug (Iteration 712) ===
Q mean: -31.894770
Q std: 21.218454
Actor loss: 31.898756
Action reg: 0.003986
  l1.weight: grad_norm = 0.027599
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.077667
Total gradient norm: 0.104814
=== Actor Training Debug (Iteration 713) ===
Q mean: -31.401783
Q std: 19.773653
Actor loss: 31.405746
Action reg: 0.003963
  l1.weight: grad_norm = 0.026813
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.085466
Total gradient norm: 0.132132
=== Actor Training Debug (Iteration 714) ===
Q mean: -30.991369
Q std: 21.457157
Actor loss: 30.995321
Action reg: 0.003953
  l1.weight: grad_norm = 0.000980
  l1.bias: grad_norm = 0.000562
  l2.weight: grad_norm = 0.003480
Total gradient norm: 0.006847
=== Actor Training Debug (Iteration 715) ===
Q mean: -35.441845
Q std: 21.778542
Actor loss: 35.445801
Action reg: 0.003956
  l1.weight: grad_norm = 0.003888
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.011173
Total gradient norm: 0.018233
=== Actor Training Debug (Iteration 716) ===
Q mean: -30.244080
Q std: 20.531473
Actor loss: 30.248022
Action reg: 0.003942
  l1.weight: grad_norm = 0.019062
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.057519
Total gradient norm: 0.083775
=== Actor Training Debug (Iteration 717) ===
Q mean: -30.849001
Q std: 21.037348
Actor loss: 30.852949
Action reg: 0.003949
  l1.weight: grad_norm = 0.053816
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.171871
Total gradient norm: 0.282468
=== Actor Training Debug (Iteration 718) ===
Q mean: -30.702408
Q std: 19.662424
Actor loss: 30.706392
Action reg: 0.003985
  l1.weight: grad_norm = 0.066792
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.218047
Total gradient norm: 0.348726
=== Actor Training Debug (Iteration 719) ===
Q mean: -31.863333
Q std: 19.384048
Actor loss: 31.867294
Action reg: 0.003962
  l1.weight: grad_norm = 0.009668
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.028557
Total gradient norm: 0.044746
=== Actor Training Debug (Iteration 720) ===
Q mean: -32.923088
Q std: 21.751968
Actor loss: 32.927090
Action reg: 0.004000
  l1.weight: grad_norm = 0.000215
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000684
Total gradient norm: 0.001129
=== Actor Training Debug (Iteration 721) ===
Q mean: -32.945488
Q std: 21.578892
Actor loss: 32.949440
Action reg: 0.003952
  l1.weight: grad_norm = 0.005497
  l1.bias: grad_norm = 0.000609
  l2.weight: grad_norm = 0.018092
Total gradient norm: 0.026056
=== Actor Training Debug (Iteration 722) ===
Q mean: -33.534924
Q std: 22.596010
Actor loss: 33.538910
Action reg: 0.003988
  l1.weight: grad_norm = 0.009424
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.028934
Total gradient norm: 0.047182
=== Actor Training Debug (Iteration 723) ===
Q mean: -31.596104
Q std: 20.878008
Actor loss: 31.600090
Action reg: 0.003987
  l1.weight: grad_norm = 0.003142
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.010393
Total gradient norm: 0.017621
=== Actor Training Debug (Iteration 724) ===
Q mean: -33.342056
Q std: 21.148096
Actor loss: 33.346020
Action reg: 0.003962
  l1.weight: grad_norm = 0.022255
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.068769
Total gradient norm: 0.116909
=== Actor Training Debug (Iteration 725) ===
Q mean: -31.411427
Q std: 21.217560
Actor loss: 31.415415
Action reg: 0.003988
  l1.weight: grad_norm = 0.005196
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.016394
Total gradient norm: 0.026781
=== Actor Training Debug (Iteration 726) ===
Q mean: -32.879948
Q std: 21.313232
Actor loss: 32.883907
Action reg: 0.003961
  l1.weight: grad_norm = 0.036012
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.107318
Total gradient norm: 0.169100
=== Actor Training Debug (Iteration 727) ===
Q mean: -32.770809
Q std: 21.633875
Actor loss: 32.774807
Action reg: 0.003997
  l1.weight: grad_norm = 0.002062
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.006094
Total gradient norm: 0.009801
=== Actor Training Debug (Iteration 728) ===
Q mean: -33.317974
Q std: 21.625586
Actor loss: 33.321934
Action reg: 0.003961
  l1.weight: grad_norm = 0.005328
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.016645
Total gradient norm: 0.029376
=== Actor Training Debug (Iteration 729) ===
Q mean: -32.637848
Q std: 21.756289
Actor loss: 32.641800
Action reg: 0.003952
  l1.weight: grad_norm = 0.005913
  l1.bias: grad_norm = 0.000566
  l2.weight: grad_norm = 0.019744
Total gradient norm: 0.034571
=== Actor Training Debug (Iteration 730) ===
Q mean: -31.720800
Q std: 20.657007
Actor loss: 31.724775
Action reg: 0.003975
  l1.weight: grad_norm = 0.023266
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.073115
Total gradient norm: 0.117469
=== Actor Training Debug (Iteration 731) ===
Q mean: -33.114002
Q std: 20.614780
Actor loss: 33.117966
Action reg: 0.003965
  l1.weight: grad_norm = 0.001429
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.004182
Total gradient norm: 0.006780
=== Actor Training Debug (Iteration 732) ===
Q mean: -35.477135
Q std: 22.047508
Actor loss: 35.481075
Action reg: 0.003942
  l1.weight: grad_norm = 0.009323
  l1.bias: grad_norm = 0.000546
  l2.weight: grad_norm = 0.032487
Total gradient norm: 0.056953
=== Actor Training Debug (Iteration 733) ===
Q mean: -34.468094
Q std: 21.286053
Actor loss: 34.472050
Action reg: 0.003955
  l1.weight: grad_norm = 0.000749
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.003056
Total gradient norm: 0.006137
=== Actor Training Debug (Iteration 734) ===
Q mean: -27.721817
Q std: 19.107916
Actor loss: 27.725733
Action reg: 0.003916
  l1.weight: grad_norm = 0.003838
  l1.bias: grad_norm = 0.000931
  l2.weight: grad_norm = 0.013707
Total gradient norm: 0.024352
=== Actor Training Debug (Iteration 735) ===
Q mean: -32.600082
Q std: 20.511179
Actor loss: 32.604061
Action reg: 0.003979
  l1.weight: grad_norm = 0.036579
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.125028
Total gradient norm: 0.229294
=== Actor Training Debug (Iteration 736) ===
Q mean: -35.078941
Q std: 22.279016
Actor loss: 35.082905
Action reg: 0.003964
  l1.weight: grad_norm = 0.017965
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.054422
Total gradient norm: 0.084275
=== Actor Training Debug (Iteration 737) ===
Q mean: -32.220764
Q std: 21.853514
Actor loss: 32.224716
Action reg: 0.003950
  l1.weight: grad_norm = 0.091429
  l1.bias: grad_norm = 0.000579
  l2.weight: grad_norm = 0.274875
Total gradient norm: 0.443215
=== Actor Training Debug (Iteration 738) ===
Q mean: -33.730850
Q std: 22.038034
Actor loss: 33.734798
Action reg: 0.003947
  l1.weight: grad_norm = 0.043081
  l1.bias: grad_norm = 0.000566
  l2.weight: grad_norm = 0.116039
Total gradient norm: 0.158693
=== Actor Training Debug (Iteration 739) ===
Q mean: -33.673038
Q std: 21.475462
Actor loss: 33.677025
Action reg: 0.003987
  l1.weight: grad_norm = 0.012458
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.036288
Total gradient norm: 0.047027
=== Actor Training Debug (Iteration 740) ===
Q mean: -32.385323
Q std: 20.830151
Actor loss: 32.389305
Action reg: 0.003981
  l1.weight: grad_norm = 0.001829
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.005812
Total gradient norm: 0.010018
=== Actor Training Debug (Iteration 741) ===
Q mean: -32.072056
Q std: 22.365528
Actor loss: 32.076004
Action reg: 0.003948
  l1.weight: grad_norm = 0.065690
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.197943
Total gradient norm: 0.342994
=== Actor Training Debug (Iteration 742) ===
Q mean: -32.049332
Q std: 21.682909
Actor loss: 32.053268
Action reg: 0.003937
  l1.weight: grad_norm = 0.042801
  l1.bias: grad_norm = 0.000639
  l2.weight: grad_norm = 0.118223
Total gradient norm: 0.176543
=== Actor Training Debug (Iteration 743) ===
Q mean: -32.636799
Q std: 20.944181
Actor loss: 32.640785
Action reg: 0.003985
  l1.weight: grad_norm = 0.033050
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.085049
Total gradient norm: 0.121721
=== Actor Training Debug (Iteration 744) ===
Q mean: -33.853584
Q std: 20.434544
Actor loss: 33.857582
Action reg: 0.003999
  l1.weight: grad_norm = 0.004884
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.015905
Total gradient norm: 0.026295
=== Actor Training Debug (Iteration 745) ===
Q mean: -31.060408
Q std: 21.245062
Actor loss: 31.064379
Action reg: 0.003972
  l1.weight: grad_norm = 0.037742
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.101953
Total gradient norm: 0.143679
=== Actor Training Debug (Iteration 746) ===
Q mean: -32.855259
Q std: 19.978357
Actor loss: 32.859192
Action reg: 0.003935
  l1.weight: grad_norm = 0.013959
  l1.bias: grad_norm = 0.000707
  l2.weight: grad_norm = 0.045999
Total gradient norm: 0.082814
=== Actor Training Debug (Iteration 747) ===
Q mean: -30.344410
Q std: 18.960173
Actor loss: 30.348377
Action reg: 0.003968
  l1.weight: grad_norm = 0.002964
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.009381
Total gradient norm: 0.016806
=== Actor Training Debug (Iteration 748) ===
Q mean: -30.600559
Q std: 21.317297
Actor loss: 30.604494
Action reg: 0.003934
  l1.weight: grad_norm = 0.020778
  l1.bias: grad_norm = 0.000735
  l2.weight: grad_norm = 0.076855
Total gradient norm: 0.143714
=== Actor Training Debug (Iteration 749) ===
Q mean: -31.836220
Q std: 20.324747
Actor loss: 31.840206
Action reg: 0.003987
  l1.weight: grad_norm = 0.003716
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.012651
Total gradient norm: 0.022683
=== Actor Training Debug (Iteration 750) ===
Q mean: -32.523121
Q std: 21.598000
Actor loss: 32.527107
Action reg: 0.003986
  l1.weight: grad_norm = 0.002880
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.008330
Total gradient norm: 0.013027
=== Actor Training Debug (Iteration 751) ===
Q mean: -30.225971
Q std: 20.449429
Actor loss: 30.229939
Action reg: 0.003968
  l1.weight: grad_norm = 0.001033
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.003150
Total gradient norm: 0.005362
=== Actor Training Debug (Iteration 752) ===
Q mean: -34.228191
Q std: 20.548767
Actor loss: 34.232155
Action reg: 0.003964
  l1.weight: grad_norm = 0.003261
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.010710
Total gradient norm: 0.018566
=== Actor Training Debug (Iteration 753) ===
Q mean: -30.411406
Q std: 21.705339
Actor loss: 30.415392
Action reg: 0.003986
  l1.weight: grad_norm = 0.011711
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.031397
Total gradient norm: 0.045730
=== Actor Training Debug (Iteration 754) ===
Q mean: -31.985628
Q std: 17.451246
Actor loss: 31.989626
Action reg: 0.003997
  l1.weight: grad_norm = 0.009896
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.029927
Total gradient norm: 0.045396
=== Actor Training Debug (Iteration 755) ===
Q mean: -33.769897
Q std: 19.852583
Actor loss: 33.773861
Action reg: 0.003964
  l1.weight: grad_norm = 0.001354
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.004282
Total gradient norm: 0.006987
=== Actor Training Debug (Iteration 756) ===
Q mean: -32.282860
Q std: 21.214758
Actor loss: 32.286812
Action reg: 0.003951
  l1.weight: grad_norm = 0.008316
  l1.bias: grad_norm = 0.000591
  l2.weight: grad_norm = 0.029402
Total gradient norm: 0.050783
=== Actor Training Debug (Iteration 757) ===
Q mean: -29.690651
Q std: 21.478544
Actor loss: 29.694637
Action reg: 0.003985
  l1.weight: grad_norm = 0.041217
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.132098
Total gradient norm: 0.213173
=== Actor Training Debug (Iteration 758) ===
Q mean: -32.979416
Q std: 21.304733
Actor loss: 32.983391
Action reg: 0.003976
  l1.weight: grad_norm = 0.000594
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.001680
Total gradient norm: 0.003158
=== Actor Training Debug (Iteration 759) ===
Q mean: -36.813366
Q std: 20.945534
Actor loss: 36.817364
Action reg: 0.004000
  l1.weight: grad_norm = 0.011326
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.036338
Total gradient norm: 0.058917
=== Actor Training Debug (Iteration 760) ===
Q mean: -33.997841
Q std: 21.184080
Actor loss: 34.001827
Action reg: 0.003988
  l1.weight: grad_norm = 0.035212
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.096948
Total gradient norm: 0.161489
=== Actor Training Debug (Iteration 761) ===
Q mean: -32.379791
Q std: 21.993811
Actor loss: 32.383755
Action reg: 0.003965
  l1.weight: grad_norm = 0.007349
  l1.bias: grad_norm = 0.000432
  l2.weight: grad_norm = 0.019676
Total gradient norm: 0.027561
=== Actor Training Debug (Iteration 762) ===
Q mean: -32.479401
Q std: 21.907650
Actor loss: 32.483402
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000001
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 763) ===
Q mean: -34.761841
Q std: 22.279398
Actor loss: 34.765820
Action reg: 0.003979
  l1.weight: grad_norm = 0.008359
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.028148
Total gradient norm: 0.045849
=== Actor Training Debug (Iteration 764) ===
Q mean: -32.467361
Q std: 20.480413
Actor loss: 32.471336
Action reg: 0.003976
  l1.weight: grad_norm = 0.024351
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.067484
Total gradient norm: 0.097786
=== Actor Training Debug (Iteration 765) ===
Q mean: -33.209564
Q std: 20.133673
Actor loss: 33.213482
Action reg: 0.003918
  l1.weight: grad_norm = 0.001584
  l1.bias: grad_norm = 0.000945
  l2.weight: grad_norm = 0.005831
Total gradient norm: 0.011444
=== Actor Training Debug (Iteration 766) ===
Q mean: -31.527664
Q std: 19.652645
Actor loss: 31.531605
Action reg: 0.003941
  l1.weight: grad_norm = 0.000900
  l1.bias: grad_norm = 0.000742
  l2.weight: grad_norm = 0.003802
Total gradient norm: 0.008017
=== Actor Training Debug (Iteration 767) ===
Q mean: -32.590485
Q std: 21.493563
Actor loss: 32.594437
Action reg: 0.003951
  l1.weight: grad_norm = 0.004731
  l1.bias: grad_norm = 0.000521
  l2.weight: grad_norm = 0.014249
Total gradient norm: 0.024870
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.019757
Total gradient norm: 0.033032
=== Actor Training Debug (Iteration 806) ===
Q mean: -32.311195
Q std: 22.015001
Actor loss: 32.315163
Action reg: 0.003966
  l1.weight: grad_norm = 0.000664
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.002451
Total gradient norm: 0.004827
=== Actor Training Debug (Iteration 807) ===
Q mean: -34.773277
Q std: 19.473289
Actor loss: 34.777229
Action reg: 0.003953
  l1.weight: grad_norm = 0.022146
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.074211
Total gradient norm: 0.122890
=== Actor Training Debug (Iteration 808) ===
Q mean: -32.678699
Q std: 18.050171
Actor loss: 32.682686
Action reg: 0.003985
  l1.weight: grad_norm = 0.020526
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.061542
Total gradient norm: 0.087488
=== Actor Training Debug (Iteration 809) ===
Q mean: -32.222061
Q std: 21.275785
Actor loss: 32.226051
Action reg: 0.003989
  l1.weight: grad_norm = 0.000503
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.001375
Total gradient norm: 0.002198
=== Actor Training Debug (Iteration 810) ===
Q mean: -31.224419
Q std: 19.631954
Actor loss: 31.228390
Action reg: 0.003972
  l1.weight: grad_norm = 0.042262
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.136077
Total gradient norm: 0.222642
=== Actor Training Debug (Iteration 811) ===
Q mean: -33.595047
Q std: 20.437311
Actor loss: 33.599045
Action reg: 0.004000
  l1.weight: grad_norm = 0.004045
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.014616
Total gradient norm: 0.024429
=== Actor Training Debug (Iteration 812) ===
Q mean: -33.714909
Q std: 21.571598
Actor loss: 33.718861
Action reg: 0.003952
  l1.weight: grad_norm = 0.023171
  l1.bias: grad_norm = 0.000550
  l2.weight: grad_norm = 0.069560
Total gradient norm: 0.114170
=== Actor Training Debug (Iteration 813) ===
Q mean: -31.691769
Q std: 20.091461
Actor loss: 31.695681
Action reg: 0.003911
  l1.weight: grad_norm = 0.001077
  l1.bias: grad_norm = 0.001059
  l2.weight: grad_norm = 0.005091
Total gradient norm: 0.011440
=== Actor Training Debug (Iteration 814) ===
Q mean: -32.866051
Q std: 20.258638
Actor loss: 32.870033
Action reg: 0.003984
  l1.weight: grad_norm = 0.038944
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.123283
Total gradient norm: 0.217864
=== Actor Training Debug (Iteration 815) ===
Q mean: -34.326454
Q std: 19.299109
Actor loss: 34.330406
Action reg: 0.003953
  l1.weight: grad_norm = 0.013934
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.039137
Total gradient norm: 0.051016
=== Actor Training Debug (Iteration 816) ===
Q mean: -33.018494
Q std: 20.903234
Actor loss: 33.022476
Action reg: 0.003984
  l1.weight: grad_norm = 0.050113
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.163593
Total gradient norm: 0.272200
=== Actor Training Debug (Iteration 817) ===
Q mean: -34.926971
Q std: 21.627972
Actor loss: 34.930927
Action reg: 0.003955
  l1.weight: grad_norm = 0.002828
  l1.bias: grad_norm = 0.000578
  l2.weight: grad_norm = 0.008929
Total gradient norm: 0.014182
=== Actor Training Debug (Iteration 818) ===
Q mean: -32.935528
Q std: 21.164581
Actor loss: 32.939461
Action reg: 0.003933
  l1.weight: grad_norm = 0.019267
  l1.bias: grad_norm = 0.000782
  l2.weight: grad_norm = 0.057027
Total gradient norm: 0.094655
=== Actor Training Debug (Iteration 819) ===
Q mean: -31.539915
Q std: 20.174120
Actor loss: 31.543882
Action reg: 0.003967
  l1.weight: grad_norm = 0.008565
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.029695
Total gradient norm: 0.053527
=== Actor Training Debug (Iteration 820) ===
Q mean: -31.856556
Q std: 20.502052
Actor loss: 31.860533
Action reg: 0.003978
  l1.weight: grad_norm = 0.000804
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.002224
Total gradient norm: 0.003913
=== Actor Training Debug (Iteration 821) ===
Q mean: -37.505150
Q std: 21.794344
Actor loss: 37.509106
Action reg: 0.003956
  l1.weight: grad_norm = 0.000995
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.003682
Total gradient norm: 0.007141
=== Actor Training Debug (Iteration 822) ===
Q mean: -33.481544
Q std: 21.829037
Actor loss: 33.485519
Action reg: 0.003976
  l1.weight: grad_norm = 0.029622
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.089928
Total gradient norm: 0.127518
=== Actor Training Debug (Iteration 823) ===
Q mean: -31.843485
Q std: 20.531635
Actor loss: 31.847445
Action reg: 0.003959
  l1.weight: grad_norm = 0.010653
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.028849
Total gradient norm: 0.045861
=== Actor Training Debug (Iteration 824) ===
Q mean: -31.163124
Q std: 20.170076
Actor loss: 31.167089
Action reg: 0.003966
  l1.weight: grad_norm = 0.002302
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.006459
Total gradient norm: 0.010631
=== Actor Training Debug (Iteration 825) ===
Q mean: -35.086563
Q std: 22.008280
Actor loss: 35.090530
Action reg: 0.003967
  l1.weight: grad_norm = 0.086538
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.285857
Total gradient norm: 0.537489
=== Actor Training Debug (Iteration 826) ===
Q mean: -37.639545
Q std: 21.148613
Actor loss: 37.643501
Action reg: 0.003956
  l1.weight: grad_norm = 0.049340
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.135594
Total gradient norm: 0.201724
=== Actor Training Debug (Iteration 827) ===
Q mean: -31.559303
Q std: 20.235235
Actor loss: 31.563259
Action reg: 0.003957
  l1.weight: grad_norm = 0.001912
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.006039
Total gradient norm: 0.010503
=== Actor Training Debug (Iteration 828) ===
Q mean: -32.053791
Q std: 19.206408
Actor loss: 32.057781
Action reg: 0.003989
  l1.weight: grad_norm = 0.001464
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.004607
Total gradient norm: 0.007556
=== Actor Training Debug (Iteration 829) ===
Q mean: -34.558449
Q std: 21.153849
Actor loss: 34.562435
Action reg: 0.003986
  l1.weight: grad_norm = 0.030458
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.090066
Total gradient norm: 0.142014
=== Actor Training Debug (Iteration 830) ===
Q mean: -33.331791
Q std: 22.026434
Actor loss: 33.335724
Action reg: 0.003933
  l1.weight: grad_norm = 0.033948
  l1.bias: grad_norm = 0.000750
  l2.weight: grad_norm = 0.098549
Total gradient norm: 0.154934
=== Actor Training Debug (Iteration 831) ===
Q mean: -30.780281
Q std: 19.758778
Actor loss: 30.784216
Action reg: 0.003934
  l1.weight: grad_norm = 0.028436
  l1.bias: grad_norm = 0.000720
  l2.weight: grad_norm = 0.074063
Total gradient norm: 0.125890
=== Actor Training Debug (Iteration 832) ===
Q mean: -32.939873
Q std: 20.789324
Actor loss: 32.943840
Action reg: 0.003966
  l1.weight: grad_norm = 0.000579
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.002018
Total gradient norm: 0.004301
=== Actor Training Debug (Iteration 833) ===
Q mean: -35.145370
Q std: 20.350811
Actor loss: 35.149361
Action reg: 0.003989
  l1.weight: grad_norm = 0.010743
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.031840
Total gradient norm: 0.053409
=== Actor Training Debug (Iteration 834) ===
Q mean: -33.041824
Q std: 19.363110
Actor loss: 33.045811
Action reg: 0.003985
  l1.weight: grad_norm = 0.055301
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.171815
Total gradient norm: 0.306872
=== Actor Training Debug (Iteration 835) ===
Q mean: -31.995426
Q std: 23.439915
Actor loss: 31.999363
Action reg: 0.003937
  l1.weight: grad_norm = 0.048939
  l1.bias: grad_norm = 0.000624
  l2.weight: grad_norm = 0.148260
Total gradient norm: 0.282924
=== Actor Training Debug (Iteration 836) ===
Q mean: -31.163614
Q std: 21.982918
Actor loss: 31.167557
Action reg: 0.003943
  l1.weight: grad_norm = 0.003799
  l1.bias: grad_norm = 0.000744
  l2.weight: grad_norm = 0.013124
Total gradient norm: 0.024570
=== Actor Training Debug (Iteration 837) ===
Q mean: -35.658878
Q std: 22.245472
Actor loss: 35.662865
Action reg: 0.003986
  l1.weight: grad_norm = 0.018509
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.062526
Total gradient norm: 0.116176
=== Actor Training Debug (Iteration 838) ===
Q mean: -32.547356
Q std: 20.639622
Actor loss: 32.551334
Action reg: 0.003979
  l1.weight: grad_norm = 0.029706
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.097835
Total gradient norm: 0.162296
=== Actor Training Debug (Iteration 839) ===
Q mean: -35.677345
Q std: 21.595201
Actor loss: 35.681335
Action reg: 0.003989
  l1.weight: grad_norm = 0.000331
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.000856
Total gradient norm: 0.001465
=== Actor Training Debug (Iteration 840) ===
Q mean: -34.912590
Q std: 22.243675
Actor loss: 34.916557
Action reg: 0.003965
  l1.weight: grad_norm = 0.032568
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.107954
Total gradient norm: 0.192650
=== Actor Training Debug (Iteration 841) ===
Q mean: -31.448843
Q std: 19.232399
Actor loss: 31.452831
Action reg: 0.003988
  l1.weight: grad_norm = 0.007798
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.023678
Total gradient norm: 0.040051
=== Actor Training Debug (Iteration 842) ===
Q mean: -32.679813
Q std: 19.818104
Actor loss: 32.683769
Action reg: 0.003958
  l1.weight: grad_norm = 0.004091
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.012477
Total gradient norm: 0.020658
=== Actor Training Debug (Iteration 843) ===
Q mean: -36.278652
Q std: 20.643875
Actor loss: 36.282619
Action reg: 0.003966
  l1.weight: grad_norm = 0.037862
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.125240
Total gradient norm: 0.211411
=== Actor Training Debug (Iteration 844) ===
Q mean: -34.421745
Q std: 21.454359
Actor loss: 34.425713
Action reg: 0.003966
  l1.weight: grad_norm = 0.033861
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.106128
Total gradient norm: 0.172266
=== Actor Training Debug (Iteration 845) ===
Q mean: -30.690704
Q std: 20.825539
Actor loss: 30.694681
Action reg: 0.003977
  l1.weight: grad_norm = 0.007865
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.021649
Total gradient norm: 0.030172
=== Actor Training Debug (Iteration 846) ===
Q mean: -34.797676
Q std: 20.530613
Actor loss: 34.801655
Action reg: 0.003980
  l1.weight: grad_norm = 0.001555
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.003578
Total gradient norm: 0.005012
=== Actor Training Debug (Iteration 847) ===
Q mean: -34.852612
Q std: 19.897989
Actor loss: 34.856571
Action reg: 0.003959
  l1.weight: grad_norm = 0.000678
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.002448
Total gradient norm: 0.005293
=== Actor Training Debug (Iteration 848) ===
Q mean: -34.280380
Q std: 19.697578
Actor loss: 34.284359
Action reg: 0.003979
  l1.weight: grad_norm = 0.000499
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.001489
Total gradient norm: 0.002928
=== Actor Training Debug (Iteration 849) ===
Q mean: -33.012054
Q std: 19.045013
Actor loss: 33.016045
Action reg: 0.003990
  l1.weight: grad_norm = 0.000593
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.002123
Total gradient norm: 0.004144
=== Actor Training Debug (Iteration 850) ===
Q mean: -35.891846
Q std: 21.138470
Actor loss: 35.895798
Action reg: 0.003953
  l1.weight: grad_norm = 0.007049
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.023369
Total gradient norm: 0.044013
=== Actor Training Debug (Iteration 851) ===
Q mean: -33.053833
Q std: 21.158300
Actor loss: 33.057808
Action reg: 0.003974
  l1.weight: grad_norm = 0.011998
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.033372
Total gradient norm: 0.048084
=== Actor Training Debug (Iteration 852) ===
Q mean: -32.949936
Q std: 19.795652
Actor loss: 32.953922
Action reg: 0.003987
  l1.weight: grad_norm = 0.030642
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.093207
Total gradient norm: 0.158905
=== Actor Training Debug (Iteration 853) ===
Q mean: -34.102985
Q std: 21.128431
Actor loss: 34.106941
Action reg: 0.003956
  l1.weight: grad_norm = 0.040217
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.111645
Total gradient norm: 0.193550
=== Actor Training Debug (Iteration 854) ===
Q mean: -34.221149
Q std: 20.786945
Actor loss: 34.225140
Action reg: 0.003991
  l1.weight: grad_norm = 0.000476
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.001241
Total gradient norm: 0.002052
=== Actor Training Debug (Iteration 855) ===
Q mean: -31.423702
Q std: 20.882710
Actor loss: 31.427660
Action reg: 0.003957
  l1.weight: grad_norm = 0.000626
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.002404
Total gradient norm: 0.005270
=== Actor Training Debug (Iteration 856) ===
Q mean: -32.816853
Q std: 20.314127
Actor loss: 32.820812
Action reg: 0.003958
  l1.weight: grad_norm = 0.003499
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.011097
Total gradient norm: 0.020687
=== Actor Training Debug (Iteration 857) ===
Q mean: -36.056740
Q std: 22.232498
Actor loss: 36.060688
Action reg: 0.003948
  l1.weight: grad_norm = 0.007694
  l1.bias: grad_norm = 0.000557
  l2.weight: grad_norm = 0.021600
Total gradient norm: 0.035687
=== Actor Training Debug (Iteration 858) ===
Q mean: -34.396019
Q std: 21.890757
Actor loss: 34.399982
Action reg: 0.003965
  l1.weight: grad_norm = 0.011380
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.039223
Total gradient norm: 0.062873
=== Actor Training Debug (Iteration 859) ===
Q mean: -32.872963
Q std: 21.064564
Actor loss: 32.876942
Action reg: 0.003978
  l1.weight: grad_norm = 0.003868
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.010631
Total gradient norm: 0.016921
=== Actor Training Debug (Iteration 860) ===
Q mean: -31.865475
Q std: 19.543531
Actor loss: 31.869452
Action reg: 0.003977
  l1.weight: grad_norm = 0.000608
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.001584
Total gradient norm: 0.003063
=== Actor Training Debug (Iteration 861) ===
Q mean: -38.770020
Q std: 21.965935
Actor loss: 38.774017
Action reg: 0.003998
  l1.weight: grad_norm = 0.010141
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.034696
Total gradient norm: 0.055187
=== Actor Training Debug (Iteration 862) ===
Q mean: -35.414948
Q std: 19.930906
Actor loss: 35.418926
Action reg: 0.003978
  l1.weight: grad_norm = 0.000445
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.001444
Total gradient norm: 0.002932
=== Actor Training Debug (Iteration 863) ===
Q mean: -34.050392
Q std: 19.018930
Actor loss: 34.054367
Action reg: 0.003976
  l1.weight: grad_norm = 0.021652
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.064623
Total gradient norm: 0.114355
=== Actor Training Debug (Iteration 864) ===
Q mean: -35.938408
Q std: 23.285082
Actor loss: 35.942375
Action reg: 0.003967
  l1.weight: grad_norm = 0.003020
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.008828
Total gradient norm: 0.015086
=== Actor Training Debug (Iteration 865) ===
Q mean: -35.204575
Q std: 21.191624
Actor loss: 35.208519
Action reg: 0.003946
  l1.weight: grad_norm = 0.008617
  l1.bias: grad_norm = 0.000556
  l2.weight: grad_norm = 0.025589
Total gradient norm: 0.042016
=== Actor Training Debug (Iteration 866) ===
Q mean: -31.020096
Q std: 19.873240
Actor loss: 31.024063
Action reg: 0.003967
  l1.weight: grad_norm = 0.001302
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.004047
Total gradient norm: 0.006790
=== Actor Training Debug (Iteration 867) ===
Q mean: -31.522198
Q std: 19.351505
Actor loss: 31.526176
Action reg: 0.003979
  l1.weight: grad_norm = 0.004854
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.013914
Total gradient norm: 0.022339
=== Actor Training Debug (Iteration 868) ===
Q mean: -36.350235
Q std: 20.070332
Actor loss: 36.354221
Action reg: 0.003988
  l1.weight: grad_norm = 0.000378
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.000980
Total gradient norm: 0.001723
=== Actor Training Debug (Iteration 869) ===
Q mean: -37.448975
Q std: 20.818417
Actor loss: 37.452961
Action reg: 0.003986
  l1.weight: grad_norm = 0.047661
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.142222
Total gradient norm: 0.247111
=== Actor Training Debug (Iteration 870) ===
Q mean: -33.008110
Q std: 21.633411
Actor loss: 33.012108
Action reg: 0.003998
  l1.weight: grad_norm = 0.024625
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.084193
Total gradient norm: 0.142716
=== Actor Training Debug (Iteration 871) ===
Q mean: -30.770372
Q std: 19.648058
Actor loss: 30.774349
Action reg: 0.003977
  l1.weight: grad_norm = 0.065046
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.225598
Total gradient norm: 0.398214
=== Actor Training Debug (Iteration 872) ===
Q mean: -29.186226
Q std: 20.931646
Actor loss: 29.190193
Action reg: 0.003968
  l1.weight: grad_norm = 0.000517
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.001917
Total gradient norm: 0.004119
=== Actor Training Debug (Iteration 873) ===
Q mean: -33.102066
Q std: 21.162054
Actor loss: 33.106045
Action reg: 0.003980
  l1.weight: grad_norm = 0.000535
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.001457
Total gradient norm: 0.002888
=== Actor Training Debug (Iteration 874) ===
Q mean: -39.230217
Q std: 22.996649
Actor loss: 39.234207
Action reg: 0.003991
  l1.weight: grad_norm = 0.004642
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.013284
Total gradient norm: 0.022338
=== Actor Training Debug (Iteration 875) ===
Q mean: -37.713219
Q std: 21.174513
Actor loss: 37.717205
Action reg: 0.003988
  l1.weight: grad_norm = 0.022741
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.070628
Total gradient norm: 0.109913
=== Actor Training Debug (Iteration 876) ===
Q mean: -35.359497
Q std: 21.412865
Actor loss: 35.363476
Action reg: 0.003978
  l1.weight: grad_norm = 0.009091
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.030375
Total gradient norm: 0.048817
=== Actor Training Debug (Iteration 877) ===
Q mean: -29.549179
Q std: 20.700468
Actor loss: 29.553165
Action reg: 0.003987
  l1.weight: grad_norm = 0.032842
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.107699
Total gradient norm: 0.174462
=== Actor Training Debug (Iteration 878) ===
Q mean: -32.762016
Q std: 17.766298
Actor loss: 32.765999
Action reg: 0.003982
  l1.weight: grad_norm = 0.001735
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.005067
Total gradient norm: 0.007527
=== Actor Training Debug (Iteration 879) ===
Q mean: -37.430641
Q std: 20.676052
Actor loss: 37.434620
Action reg: 0.003980
  l1.weight: grad_norm = 0.000446
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.001338
Total gradient norm: 0.002755
=== Actor Training Debug (Iteration 880) ===
Q mean: -33.720291
Q std: 19.898481
Actor loss: 33.724258
Action reg: 0.003966
  l1.weight: grad_norm = 0.023114
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.070408
Total gradient norm: 0.114204
=== Actor Training Debug (Iteration 881) ===
Q mean: -33.813400
Q std: 19.876505
Actor loss: 33.817387
Action reg: 0.003987
  l1.weight: grad_norm = 0.025686
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.063213
Total gradient norm: 0.088701
=== Actor Training Debug (Iteration 882) ===
Q mean: -31.468349
Q std: 21.958260
Actor loss: 31.472296
Action reg: 0.003945
  l1.weight: grad_norm = 0.055854
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.174187
Total gradient norm: 0.284489
=== Actor Training Debug (Iteration 883) ===
Q mean: -32.269012
Q std: 21.647326
Actor loss: 32.273003
Action reg: 0.003989
  l1.weight: grad_norm = 0.001557
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.004864
Total gradient norm: 0.008549
=== Actor Training Debug (Iteration 884) ===
Q mean: -36.622688
Q std: 20.446812
Actor loss: 36.626667
Action reg: 0.003980
  l1.weight: grad_norm = 0.002032
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.006547
Total gradient norm: 0.010931
=== Actor Training Debug (Iteration 885) ===
Q mean: -34.789192
Q std: 21.069122
Actor loss: 34.793190
Action reg: 0.003998
  l1.weight: grad_norm = 0.005855
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.016378
Total gradient norm: 0.022762
=== Actor Training Debug (Iteration 886) ===
Q mean: -33.072342
Q std: 19.469353
Actor loss: 33.076290
Action reg: 0.003949
  l1.weight: grad_norm = 0.000767
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.002887
Total gradient norm: 0.006217
=== Actor Training Debug (Iteration 887) ===
Q mean: -32.358852
Q std: 19.507023
Actor loss: 32.362820
Action reg: 0.003968
  l1.weight: grad_norm = 0.013208
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.041304
Total gradient norm: 0.071149
=== Actor Training Debug (Iteration 888) ===
Q mean: -33.903221
Q std: 21.794102
Actor loss: 33.907188
Action reg: 0.003967
  l1.weight: grad_norm = 0.033103
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.108458
Total gradient norm: 0.192589
=== Actor Training Debug (Iteration 889) ===
Q mean: -33.928131
Q std: 19.986477
Actor loss: 33.932117
Action reg: 0.003985
  l1.weight: grad_norm = 0.007522
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.023578
Total gradient norm: 0.037233
=== Actor Training Debug (Iteration 890) ===
Q mean: -32.561008
Q std: 19.727365
Actor loss: 32.564983
Action reg: 0.003975
  l1.weight: grad_norm = 0.029890
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.089181
Total gradient norm: 0.135257
=== Actor Training Debug (Iteration 891) ===
Q mean: -34.488285
Q std: 19.919035
Actor loss: 34.492287
Action reg: 0.004000
  l1.weight: grad_norm = 0.000436
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001480
Total gradient norm: 0.002721
=== Actor Training Debug (Iteration 892) ===
Q mean: -36.325603
Q std: 20.402054
Actor loss: 36.329590
Action reg: 0.003988
  l1.weight: grad_norm = 0.006777
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.018181
Total gradient norm: 0.025246
=== Actor Training Debug (Iteration 893) ===
Q mean: -35.179054
Q std: 21.117739
Actor loss: 35.183033
Action reg: 0.003979
  l1.weight: grad_norm = 0.020026
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.059021
Total gradient norm: 0.092598
=== Actor Training Debug (Iteration 894) ===
Q mean: -29.068169
Q std: 18.579681
Actor loss: 29.072147
Action reg: 0.003978
  l1.weight: grad_norm = 0.005199
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.016465
Total gradient norm: 0.028189
=== Actor Training Debug (Iteration 895) ===
Q mean: -34.111565
Q std: 19.776234
Actor loss: 34.115555
Action reg: 0.003989
  l1.weight: grad_norm = 0.004177
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.012579
Total gradient norm: 0.017971
=== Actor Training Debug (Iteration 896) ===
Q mean: -33.925652
Q std: 20.654411
Actor loss: 33.929615
Action reg: 0.003964
  l1.weight: grad_norm = 0.011659
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.040559
Total gradient norm: 0.077045
=== Actor Training Debug (Iteration 897) ===
Q mean: -35.831833
Q std: 21.075165
Actor loss: 35.835812
Action reg: 0.003981
  l1.weight: grad_norm = 0.016873
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.050257
Total gradient norm: 0.084419
=== Actor Training Debug (Iteration 898) ===
Q mean: -33.420654
Q std: 21.627621
Actor loss: 33.424641
Action reg: 0.003987
  l1.weight: grad_norm = 0.009527
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.028455
Total gradient norm: 0.049836
=== Actor Training Debug (Iteration 899) ===
Q mean: -36.752682
Q std: 22.405161
Actor loss: 36.756660
Action reg: 0.003980
  l1.weight: grad_norm = 0.001703
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.005430
Total gradient norm: 0.009215
=== Actor Training Debug (Iteration 900) ===
Q mean: -33.747704
Q std: 22.508739
Actor loss: 33.751659
Action reg: 0.003956
  l1.weight: grad_norm = 0.040081
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.112789
Total gradient norm: 0.171212
=== Actor Training Debug (Iteration 901) ===
Q mean: -36.133118
Q std: 20.290550
Actor loss: 36.137085
Action reg: 0.003966
  l1.weight: grad_norm = 0.009652
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.030838
Total gradient norm: 0.048789
=== Actor Training Debug (Iteration 902) ===
Q mean: -34.422531
Q std: 20.101269
Actor loss: 34.426510
Action reg: 0.003980
  l1.weight: grad_norm = 0.018772
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.055090
Total gradient norm: 0.092163
=== Actor Training Debug (Iteration 903) ===
Q mean: -33.382881
Q std: 18.137131
Actor loss: 33.386879
Action reg: 0.003997
  l1.weight: grad_norm = 0.013801
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.039786
Total gradient norm: 0.053531
=== Actor Training Debug (Iteration 904) ===
Q mean: -37.130272
Q std: 20.277334
Actor loss: 37.134251
Action reg: 0.003979
  l1.weight: grad_norm = 0.013013
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.042716
Total gradient norm: 0.076319
=== Actor Training Debug (Iteration 905) ===
Q mean: -35.127930
Q std: 20.332851
Actor loss: 35.131931
Action reg: 0.004000
  l1.weight: grad_norm = 0.000379
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001135
Total gradient norm: 0.001920
=== Actor Training Debug (Iteration 906) ===
Q mean: -29.173782
Q std: 18.659037
Actor loss: 29.177773
Action reg: 0.003990
  l1.weight: grad_norm = 0.001191
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.003487
Total gradient norm: 0.005730
=== Actor Training Debug (Iteration 907) ===
Q mean: -31.752863
Q std: 19.170368
Actor loss: 31.756815
Action reg: 0.003953
  l1.weight: grad_norm = 0.000655
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.002507
Total gradient norm: 0.005637
=== Actor Training Debug (Iteration 908) ===
Q mean: -36.753479
Q std: 20.800583
Actor loss: 36.757469
Action reg: 0.003990
  l1.weight: grad_norm = 0.001279
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.003778
Total gradient norm: 0.006403
=== Actor Training Debug (Iteration 909) ===
Q mean: -36.294216
Q std: 23.771122
Actor loss: 36.298195
Action reg: 0.003979
  l1.weight: grad_norm = 0.003335
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.010853
Total gradient norm: 0.019661
=== Actor Training Debug (Iteration 910) ===
Q mean: -32.827702
Q std: 21.160278
Actor loss: 32.831673
Action reg: 0.003971
  l1.weight: grad_norm = 0.002610
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.007757
Total gradient norm: 0.012917
=== Actor Training Debug (Iteration 911) ===
Q mean: -33.354774
Q std: 21.268484
Actor loss: 33.358742
Action reg: 0.003967
  l1.weight: grad_norm = 0.000852
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.002687
Total gradient norm: 0.004702
=== Actor Training Debug (Iteration 912) ===
Q mean: -33.074036
Q std: 21.683308
Actor loss: 33.077953
Action reg: 0.003919
  l1.weight: grad_norm = 0.004461
  l1.bias: grad_norm = 0.000771
  l2.weight: grad_norm = 0.012955
Total gradient norm: 0.021203
=== Actor Training Debug (Iteration 913) ===
Q mean: -36.863403
Q std: 22.053726
Actor loss: 36.867367
Action reg: 0.003965
  l1.weight: grad_norm = 0.014471
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.041808
Total gradient norm: 0.069656
=== Actor Training Debug (Iteration 914) ===
Q mean: -31.358967
Q std: 20.253515
Actor loss: 31.362955
Action reg: 0.003988
  l1.weight: grad_norm = 0.059779
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.208065
Total gradient norm: 0.390016
=== Actor Training Debug (Iteration 915) ===
Q mean: -30.977779
Q std: 18.256626
Actor loss: 30.981737
Action reg: 0.003958
  l1.weight: grad_norm = 0.045076
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.134903
Total gradient norm: 0.238821
=== Actor Training Debug (Iteration 916) ===
Q mean: -33.093254
Q std: 19.109303
Actor loss: 33.097202
Action reg: 0.003948
  l1.weight: grad_norm = 0.016735
  l1.bias: grad_norm = 0.000519
  l2.weight: grad_norm = 0.049079
Total gradient norm: 0.078649
=== Actor Training Debug (Iteration 917) ===
Q mean: -35.200142
Q std: 21.168058
Actor loss: 35.204071
Action reg: 0.003931
  l1.weight: grad_norm = 0.000676
  l1.bias: grad_norm = 0.000651
  l2.weight: grad_norm = 0.003266
Total gradient norm: 0.007687
=== Actor Training Debug (Iteration 918) ===
Q mean: -33.924980
Q std: 18.969950
Actor loss: 33.928951
Action reg: 0.003971
  l1.weight: grad_norm = 0.000527
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.001826
Total gradient norm: 0.003967
=== Actor Training Debug (Iteration 919) ===
Q mean: -31.006712
Q std: 19.628635
Actor loss: 31.010683
Action reg: 0.003972
  l1.weight: grad_norm = 0.000634
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.001875
Total gradient norm: 0.003956
=== Actor Training Debug (Iteration 920) ===
Q mean: -33.528770
Q std: 20.686987
Actor loss: 33.532726
Action reg: 0.003957
  l1.weight: grad_norm = 0.048232
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.165454
Total gradient norm: 0.263719
=== Actor Training Debug (Iteration 921) ===
Q mean: -37.066872
Q std: 21.556509
Actor loss: 37.070847
Action reg: 0.003975
  l1.weight: grad_norm = 0.016608
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.047042
Total gradient norm: 0.064769
=== Actor Training Debug (Iteration 922) ===
Q mean: -36.832314
Q std: 20.996008
Actor loss: 36.836300
Action reg: 0.003987
  l1.weight: grad_norm = 0.009263
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.029160
Total gradient norm: 0.048794
=== Actor Training Debug (Iteration 923) ===
Q mean: -35.511887
Q std: 19.768818
Actor loss: 35.515877
Action reg: 0.003988
  l1.weight: grad_norm = 0.003379
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.008173
Total gradient norm: 0.010405
=== Actor Training Debug (Iteration 924) ===
Q mean: -34.557194
Q std: 19.486540
Actor loss: 34.561184
Action reg: 0.003991
  l1.weight: grad_norm = 0.003862
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.011568
Total gradient norm: 0.019686
=== Actor Training Debug (Iteration 925) ===
Q mean: -35.777046
Q std: 20.500074
Actor loss: 35.781013
Action reg: 0.003965
  l1.weight: grad_norm = 0.095952
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.281424
Total gradient norm: 0.455669
=== Actor Training Debug (Iteration 926) ===
Q mean: -36.036003
Q std: 22.413036
Actor loss: 36.039982
Action reg: 0.003978
  l1.weight: grad_norm = 0.028638
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.080045
Total gradient norm: 0.134716
=== Actor Training Debug (Iteration 927) ===
Q mean: -32.938961
Q std: 19.989138
Actor loss: 32.942886
Action reg: 0.003927
  l1.weight: grad_norm = 0.001001
  l1.bias: grad_norm = 0.000748
  l2.weight: grad_norm = 0.004205
Total gradient norm: 0.009217
=== Actor Training Debug (Iteration 928) ===
Q mean: -33.938366
Q std: 22.521236
Actor loss: 33.942345
Action reg: 0.003978
  l1.weight: grad_norm = 0.026388
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.077497
Total gradient norm: 0.135726
=== Actor Training Debug (Iteration 929) ===
Q mean: -34.978474
Q std: 20.053898
Actor loss: 34.982441
Action reg: 0.003966
  l1.weight: grad_norm = 0.030746
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.089920
Total gradient norm: 0.165637
=== Actor Training Debug (Iteration 930) ===
Q mean: -37.217331
Q std: 20.081417
Actor loss: 37.221321
Action reg: 0.003991
  l1.weight: grad_norm = 0.000261
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.000732
Total gradient norm: 0.001455
=== Actor Training Debug (Iteration 931) ===
Q mean: -31.305216
Q std: 19.076851
Actor loss: 31.309185
Action reg: 0.003969
  l1.weight: grad_norm = 0.022731
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.072255
Total gradient norm: 0.118579
=== Actor Training Debug (Iteration 932) ===
Q mean: -31.585304
Q std: 19.122915
Actor loss: 31.589273
Action reg: 0.003968
  l1.weight: grad_norm = 0.006598
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.019639
Total gradient norm: 0.035363
=== Actor Training Debug (Iteration 933) ===
Q mean: -38.837891
Q std: 21.959419
Actor loss: 38.841858
Action reg: 0.003967
  l1.weight: grad_norm = 0.025033
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.075522
Total gradient norm: 0.128251
=== Actor Training Debug (Iteration 934) ===
Q mean: -36.117393
Q std: 19.544579
Actor loss: 36.121372
Action reg: 0.003978
  l1.weight: grad_norm = 0.007308
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.022303
Total gradient norm: 0.036765
=== Actor Training Debug (Iteration 935) ===
Q mean: -34.917000
Q std: 17.281879
Actor loss: 34.920998
Action reg: 0.003997
  l1.weight: grad_norm = 0.004309
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.010680
Total gradient norm: 0.014591
=== Actor Training Debug (Iteration 936) ===
Q mean: -30.603472
Q std: 17.808668
Actor loss: 30.607441
Action reg: 0.003970
  l1.weight: grad_norm = 0.065621
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.187734
Total gradient norm: 0.301214
=== Actor Training Debug (Iteration 937) ===
Q mean: -34.299782
Q std: 19.975307
Actor loss: 34.303772
Action reg: 0.003989
  l1.weight: grad_norm = 0.000315
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.000853
Total gradient norm: 0.001656
=== Actor Training Debug (Iteration 938) ===
Q mean: -37.538204
Q std: 22.088417
Actor loss: 37.542183
Action reg: 0.003980
  l1.weight: grad_norm = 0.000991
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.002733
Total gradient norm: 0.004174
=== Actor Training Debug (Iteration 939) ===
Q mean: -33.765388
Q std: 19.394533
Actor loss: 33.769356
Action reg: 0.003967
  l1.weight: grad_norm = 0.013827
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.047215
Total gradient norm: 0.078935
=== Actor Training Debug (Iteration 940) ===
Q mean: -34.920067
Q std: 20.938351
Actor loss: 34.924057
Action reg: 0.003989
  l1.weight: grad_norm = 0.000300
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.000809
Total gradient norm: 0.001518
=== Actor Training Debug (Iteration 941) ===
Q mean: -33.870476
Q std: 19.931471
Actor loss: 33.874454
Action reg: 0.003979
  l1.weight: grad_norm = 0.023323
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.057704
Total gradient norm: 0.082508
=== Actor Training Debug (Iteration 942) ===
Q mean: -34.127136
Q std: 19.212879
Actor loss: 34.131123
Action reg: 0.003986
  l1.weight: grad_norm = 0.009761
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.030575
Total gradient norm: 0.050647
=== Actor Training Debug (Iteration 943) ===
Q mean: -33.914688
Q std: 21.004364
Actor loss: 33.918633
Action reg: 0.003943
  l1.weight: grad_norm = 0.045011
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.123840
Total gradient norm: 0.169226
=== Actor Training Debug (Iteration 944) ===
Q mean: -35.105064
Q std: 18.670370
Actor loss: 35.109062
Action reg: 0.003999
  l1.weight: grad_norm = 0.005780
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.015993
Total gradient norm: 0.026229
=== Actor Training Debug (Iteration 945) ===
Q mean: -32.716690
Q std: 19.663965
Actor loss: 32.720654
Action reg: 0.003964
  l1.weight: grad_norm = 0.029661
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.090483
Total gradient norm: 0.178823
=== Actor Training Debug (Iteration 946) ===
Q mean: -33.683815
Q std: 21.343103
Actor loss: 33.687801
Action reg: 0.003988
  l1.weight: grad_norm = 0.030998
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.097029
Total gradient norm: 0.164605
=== Actor Training Debug (Iteration 947) ===
Q mean: -34.361893
Q std: 21.166620
Actor loss: 34.365871
Action reg: 0.003978
  l1.weight: grad_norm = 0.014467
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.046465
Total gradient norm: 0.089641
=== Actor Training Debug (Iteration 948) ===
Q mean: -33.706314
Q std: 19.178503
Actor loss: 33.710285
Action reg: 0.003970
  l1.weight: grad_norm = 0.005979
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.018777
Total gradient norm: 0.034601
=== Actor Training Debug (Iteration 949) ===
Q mean: -35.518112
Q std: 19.951582
Actor loss: 35.522091
Action reg: 0.003981
  l1.weight: grad_norm = 0.024024
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.074819
Total gradient norm: 0.126087
=== Actor Training Debug (Iteration 950) ===
Q mean: -31.407112
Q std: 21.399231
Actor loss: 31.411032
Action reg: 0.003919
  l1.weight: grad_norm = 0.015998
  l1.bias: grad_norm = 0.000742
  l2.weight: grad_norm = 0.044016
Total gradient norm: 0.073135
=== Actor Training Debug (Iteration 951) ===
Q mean: -31.870417
Q std: 20.230848
Actor loss: 31.874363
Action reg: 0.003946
  l1.weight: grad_norm = 0.024040
  l1.bias: grad_norm = 0.000507
  l2.weight: grad_norm = 0.077625
Total gradient norm: 0.130110
=== Actor Training Debug (Iteration 952) ===
Q mean: -36.116146
Q std: 22.159782
Actor loss: 36.120079
Action reg: 0.003932
  l1.weight: grad_norm = 0.004899
  l1.bias: grad_norm = 0.000615
  l2.weight: grad_norm = 0.014566
Total gradient norm: 0.025046
=== Actor Training Debug (Iteration 953) ===
Q mean: -34.755676
Q std: 21.536900
Actor loss: 34.759647
Action reg: 0.003971
  l1.weight: grad_norm = 0.000487
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.001544
Total gradient norm: 0.003352
=== Actor Training Debug (Iteration 954) ===
Q mean: -36.063396
Q std: 19.309864
Actor loss: 36.067352
Action reg: 0.003956
  l1.weight: grad_norm = 0.041322
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.136181
Total gradient norm: 0.222282
=== Actor Training Debug (Iteration 955) ===
Q mean: -35.539902
Q std: 19.309376
Actor loss: 35.543900
Action reg: 0.003999
  l1.weight: grad_norm = 0.023098
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.071647
Total gradient norm: 0.126011
=== Actor Training Debug (Iteration 956) ===
Q mean: -33.944885
Q std: 22.069483
Actor loss: 33.948822
Action reg: 0.003939
  l1.weight: grad_norm = 0.004813
  l1.bias: grad_norm = 0.000620
  l2.weight: grad_norm = 0.015748
Total gradient norm: 0.027457
=== Actor Training Debug (Iteration 957) ===
Q mean: -32.104164
Q std: 21.447069
Actor loss: 32.108131
Action reg: 0.003969
  l1.weight: grad_norm = 0.000576
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.002039
Total gradient norm: 0.004356
=== Actor Training Debug (Iteration 958) ===
Q mean: -34.840717
Q std: 18.736063
Actor loss: 34.844685
Action reg: 0.003968
  l1.weight: grad_norm = 0.048264
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.140063
Total gradient norm: 0.238589
=== Actor Training Debug (Iteration 959) ===
Q mean: -38.464264
Q std: 21.143267
Actor loss: 38.468254
Action reg: 0.003991
  l1.weight: grad_norm = 0.000224
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.000564
Total gradient norm: 0.001115
=== Actor Training Debug (Iteration 960) ===
Q mean: -36.447456
Q std: 19.094061
Actor loss: 36.451447
Action reg: 0.003990
  l1.weight: grad_norm = 0.003411
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.008771
Total gradient norm: 0.011697
=== Actor Training Debug (Iteration 961) ===
Q mean: -33.909023
Q std: 20.324722
Actor loss: 33.913010
Action reg: 0.003987
  l1.weight: grad_norm = 0.027307
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.091491
Total gradient norm: 0.156045
=== Actor Training Debug (Iteration 962) ===
Q mean: -34.792103
Q std: 20.529358
Actor loss: 34.796085
Action reg: 0.003981
  l1.weight: grad_norm = 0.001676
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.005144
Total gradient norm: 0.008516
=== Actor Training Debug (Iteration 963) ===
Q mean: -33.248711
Q std: 20.748539
Actor loss: 33.252701
Action reg: 0.003990
  l1.weight: grad_norm = 0.016167
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.045747
Total gradient norm: 0.082692
=== Actor Training Debug (Iteration 964) ===
Q mean: -37.692219
Q std: 20.095997
Actor loss: 37.696205
Action reg: 0.003986
  l1.weight: grad_norm = 0.031938
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.107324
Total gradient norm: 0.194862
=== Actor Training Debug (Iteration 965) ===
Q mean: -35.813000
Q std: 21.024511
Actor loss: 35.816986
Action reg: 0.003986
  l1.weight: grad_norm = 0.006883
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.019596
Total gradient norm: 0.029539
=== Actor Training Debug (Iteration 966) ===
Q mean: -37.356228
Q std: 21.520508
Actor loss: 37.360207
Action reg: 0.003977
  l1.weight: grad_norm = 0.004673
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.013573
Total gradient norm: 0.022361
=== Actor Training Debug (Iteration 967) ===
Q mean: -34.023869
Q std: 20.128952
Actor loss: 34.027828
Action reg: 0.003960
  l1.weight: grad_norm = 0.012092
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.037038
Total gradient norm: 0.061505
=== Actor Training Debug (Iteration 968) ===
Q mean: -34.641586
Q std: 22.713718
Actor loss: 34.645550
Action reg: 0.003962
  l1.weight: grad_norm = 0.000641
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.002060
Total gradient norm: 0.004484
=== Actor Training Debug (Iteration 969) ===
Q mean: -34.766365
Q std: 21.858639
Actor loss: 34.770340
Action reg: 0.003975
  l1.weight: grad_norm = 0.107954
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.318551
Total gradient norm: 0.537846
=== Actor Training Debug (Iteration 970) ===
Q mean: -35.741501
Q std: 18.863657
Actor loss: 35.745499
Action reg: 0.003998
  l1.weight: grad_norm = 0.028505
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.084668
Total gradient norm: 0.142061
=== Actor Training Debug (Iteration 971) ===
Q mean: -34.217621
Q std: 20.777843
Actor loss: 34.221569
Action reg: 0.003948
  l1.weight: grad_norm = 0.020048
  l1.bias: grad_norm = 0.000478
  l2.weight: grad_norm = 0.055747
Total gradient norm: 0.094293
=== Actor Training Debug (Iteration 972) ===
Q mean: -37.135586
Q std: 20.932093
Actor loss: 37.139572
Action reg: 0.003987
  l1.weight: grad_norm = 0.019367
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.048571
Total gradient norm: 0.065065
=== Actor Training Debug (Iteration 973) ===
Q mean: -38.315247
Q std: 21.676823
Actor loss: 38.319199
Action reg: 0.003952
  l1.weight: grad_norm = 0.025121
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.069342
Total gradient norm: 0.095485
=== Actor Training Debug (Iteration 974) ===
Q mean: -36.189705
Q std: 21.675314
Actor loss: 36.193653
Action reg: 0.003948
  l1.weight: grad_norm = 0.043706
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.142545
Total gradient norm: 0.225017
=== Actor Training Debug (Iteration 975) ===
Q mean: -32.309891
Q std: 19.092148
Actor loss: 32.313843
Action reg: 0.003951
  l1.weight: grad_norm = 0.039733
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.123495
Total gradient norm: 0.207135
=== Actor Training Debug (Iteration 976) ===
Q mean: -32.600128
Q std: 18.278473
Actor loss: 32.604088
Action reg: 0.003959
  l1.weight: grad_norm = 0.021169
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.055842
Total gradient norm: 0.074649
=== Actor Training Debug (Iteration 977) ===
Q mean: -37.409534
Q std: 21.975140
Actor loss: 37.413483
Action reg: 0.003947
  l1.weight: grad_norm = 0.010132
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.033126
Total gradient norm: 0.049971
=== Actor Training Debug (Iteration 978) ===
Q mean: -36.114723
Q std: 19.825317
Actor loss: 36.118702
Action reg: 0.003979
  l1.weight: grad_norm = 0.002485
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.007944
Total gradient norm: 0.013772
=== Actor Training Debug (Iteration 979) ===
Q mean: -34.349243
Q std: 20.793577
Actor loss: 34.353207
Action reg: 0.003964
  l1.weight: grad_norm = 0.010977
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.032637
Total gradient norm: 0.051180
=== Actor Training Debug (Iteration 980) ===
Q mean: -33.483967
Q std: 18.418648
Actor loss: 33.487946
Action reg: 0.003979
  l1.weight: grad_norm = 0.061078
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.183270
Total gradient norm: 0.266463
=== Actor Training Debug (Iteration 981) ===
Q mean: -34.093895
Q std: 20.452456
Actor loss: 34.097839
Action reg: 0.003946
  l1.weight: grad_norm = 0.006029
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.017089
Total gradient norm: 0.023298
=== Actor Training Debug (Iteration 982) ===
Q mean: -34.192276
Q std: 20.853493
Actor loss: 34.196255
Action reg: 0.003980
  l1.weight: grad_norm = 0.030552
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.103413
Total gradient norm: 0.175312
=== Actor Training Debug (Iteration 983) ===
Q mean: -37.220856
Q std: 19.398146
Actor loss: 37.224823
Action reg: 0.003968
  l1.weight: grad_norm = 0.004711
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.013942
Total gradient norm: 0.021173
=== Actor Training Debug (Iteration 984) ===
Q mean: -38.375034
Q std: 21.902401
Actor loss: 38.379013
Action reg: 0.003978
  l1.weight: grad_norm = 0.007060
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.021872
Total gradient norm: 0.035858
=== Actor Training Debug (Iteration 985) ===
Q mean: -33.268509
Q std: 20.359032
Actor loss: 33.272419
Action reg: 0.003911
  l1.weight: grad_norm = 0.022589
  l1.bias: grad_norm = 0.000791
  l2.weight: grad_norm = 0.061626
Total gradient norm: 0.095332
=== Actor Training Debug (Iteration 986) ===
Q mean: -33.851456
Q std: 20.780176
Actor loss: 33.855446
Action reg: 0.003992
  l1.weight: grad_norm = 0.006765
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.021057
Total gradient norm: 0.036000
=== Actor Training Debug (Iteration 987) ===
Q mean: -37.109634
Q std: 21.772686
Actor loss: 37.113636
Action reg: 0.004000
  l1.weight: grad_norm = 0.002654
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.007901
Total gradient norm: 0.013145
=== Actor Training Debug (Iteration 988) ===
Q mean: -35.039486
Q std: 21.630459
Actor loss: 35.043461
Action reg: 0.003973
  l1.weight: grad_norm = 0.000572
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.001479
Total gradient norm: 0.002996
=== Actor Training Debug (Iteration 989) ===
Q mean: -36.405018
Q std: 19.164854
Actor loss: 36.408997
Action reg: 0.003977
  l1.weight: grad_norm = 0.020942
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.071805
Total gradient norm: 0.120659
=== Actor Training Debug (Iteration 990) ===
Q mean: -36.664246
Q std: 19.543974
Actor loss: 36.668205
Action reg: 0.003959
  l1.weight: grad_norm = 0.005601
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.019818
Total gradient norm: 0.040984
=== Actor Training Debug (Iteration 991) ===
Q mean: -35.349339
Q std: 19.821598
Actor loss: 35.353287
Action reg: 0.003949
  l1.weight: grad_norm = 0.006422
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.019990
Total gradient norm: 0.035031
=== Actor Training Debug (Iteration 992) ===
Q mean: -35.127827
Q std: 22.279320
Actor loss: 35.131794
Action reg: 0.003968
  l1.weight: grad_norm = 0.013289
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.045909
Total gradient norm: 0.076460
=== Actor Training Debug (Iteration 993) ===
Q mean: -34.630867
Q std: 21.888908
Actor loss: 34.634842
Action reg: 0.003974
  l1.weight: grad_norm = 0.000865
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.002359
Total gradient norm: 0.003962
=== Actor Training Debug (Iteration 994) ===
Q mean: -36.382416
Q std: 22.227110
Actor loss: 36.386406
Action reg: 0.003990
  l1.weight: grad_norm = 0.002164
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.005647
Total gradient norm: 0.007774
=== Actor Training Debug (Iteration 995) ===
Q mean: -34.987358
Q std: 18.839954
Actor loss: 34.991333
Action reg: 0.003975
  l1.weight: grad_norm = 0.057446
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.193688
Total gradient norm: 0.342158
=== Actor Training Debug (Iteration 996) ===
Q mean: -34.534061
Q std: 20.749493
Actor loss: 34.538013
Action reg: 0.003951
  l1.weight: grad_norm = 0.007293
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.023390
Total gradient norm: 0.037895
=== Actor Training Debug (Iteration 997) ===
Q mean: -35.008186
Q std: 19.660412
Actor loss: 35.012157
Action reg: 0.003970
  l1.weight: grad_norm = 0.000356
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.001320
Total gradient norm: 0.003239
=== Actor Training Debug (Iteration 998) ===
Q mean: -36.515106
Q std: 20.098209
Actor loss: 36.519096
Action reg: 0.003989
  l1.weight: grad_norm = 0.003555
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.011591
Total gradient norm: 0.019413
=== Actor Training Debug (Iteration 999) ===
Q mean: -33.510376
Q std: 20.652231
Actor loss: 33.514355
Action reg: 0.003978
  l1.weight: grad_norm = 0.032590
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.087821
Total gradient norm: 0.128690
=== Actor Training Debug (Iteration 1000) ===
Q mean: -36.554634
Q std: 22.637873
Actor loss: 36.558613
Action reg: 0.003979
  l1.weight: grad_norm = 0.002668
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.008474
Total gradient norm: 0.012959
Step 6000: Critic Loss: 8.2298, Actor Loss: 36.5586, Q Value: -36.5546
  Average reward: -341.830 | Average length: 100.0
Evaluation at episode 60: -341.830
=== Actor Training Debug (Iteration 1001) ===
Q mean: -37.293480
Q std: 20.710236
Actor loss: 37.297462
Action reg: 0.003983
  l1.weight: grad_norm = 0.047288
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.148034
Total gradient norm: 0.249053
=== Actor Training Debug (Iteration 1002) ===
Q mean: -35.861668
Q std: 19.695194
Actor loss: 35.865631
Action reg: 0.003965
  l1.weight: grad_norm = 0.060498
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.199411
Total gradient norm: 0.310397
=== Actor Training Debug (Iteration 1003) ===
Q mean: -36.148930
Q std: 19.957262
Actor loss: 36.152908
Action reg: 0.003979
  l1.weight: grad_norm = 0.003577
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.008461
Total gradient norm: 0.011411
=== Actor Training Debug (Iteration 1004) ===
Q mean: -35.946804
Q std: 19.677250
Actor loss: 35.950783
Action reg: 0.003979
  l1.weight: grad_norm = 0.004133
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.013212
Total gradient norm: 0.024590
=== Actor Training Debug (Iteration 1005) ===
Q mean: -33.694103
Q std: 21.042461
Actor loss: 33.698074
Action reg: 0.003970
  l1.weight: grad_norm = 0.005532
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.016331
Total gradient norm: 0.028078
=== Actor Training Debug (Iteration 1006) ===
Q mean: -34.306225
Q std: 20.923815
Actor loss: 34.310192
Action reg: 0.003966
  l1.weight: grad_norm = 0.024089
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.067326
Total gradient norm: 0.109340
=== Actor Training Debug (Iteration 1007) ===
Q mean: -36.542908
Q std: 20.942928
Actor loss: 36.546898
Action reg: 0.003991
  l1.weight: grad_norm = 0.011616
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.039151
Total gradient norm: 0.072380
=== Actor Training Debug (Iteration 1008) ===
Q mean: -37.095177
Q std: 19.736132
Actor loss: 37.099155
Action reg: 0.003979
  l1.weight: grad_norm = 0.009547
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.029054
Total gradient norm: 0.043372
=== Actor Training Debug (Iteration 1009) ===
Q mean: -35.444221
Q std: 21.148933
Actor loss: 35.448189
Action reg: 0.003966
  l1.weight: grad_norm = 0.011738
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.043629
Total gradient norm: 0.085357
=== Actor Training Debug (Iteration 1010) ===
Q mean: -37.382706
Q std: 20.406786
Actor loss: 37.386673
Action reg: 0.003967
  l1.weight: grad_norm = 0.013935
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.046398
Total gradient norm: 0.090480
=== Actor Training Debug (Iteration 1011) ===
Q mean: -34.996544
Q std: 19.908602
Actor loss: 35.000530
Action reg: 0.003985
  l1.weight: grad_norm = 0.013129
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.043341
Total gradient norm: 0.089206
=== Actor Training Debug (Iteration 1012) ===
Q mean: -34.037704
Q std: 19.398296
Actor loss: 34.041695
Action reg: 0.003989
  l1.weight: grad_norm = 0.012836
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.035434
Total gradient norm: 0.046066
=== Actor Training Debug (Iteration 1013) ===
Q mean: -32.616962
Q std: 19.365482
Actor loss: 32.620953
Action reg: 0.003989
  l1.weight: grad_norm = 0.011861
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.042737
Total gradient norm: 0.080840
=== Actor Training Debug (Iteration 1014) ===
Q mean: -36.979145
Q std: 21.664717
Actor loss: 36.983078
Action reg: 0.003934
  l1.weight: grad_norm = 0.111651
  l1.bias: grad_norm = 0.000535
  l2.weight: grad_norm = 0.378175
Total gradient norm: 0.696116
=== Actor Training Debug (Iteration 1015) ===
Q mean: -37.195511
Q std: 20.116957
Actor loss: 37.199482
Action reg: 0.003969
  l1.weight: grad_norm = 0.074541
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.279725
Total gradient norm: 0.577858
=== Actor Training Debug (Iteration 1016) ===
Q mean: -34.293228
Q std: 20.141500
Actor loss: 34.297195
Action reg: 0.003967
  l1.weight: grad_norm = 0.029805
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.094407
Total gradient norm: 0.185157
=== Actor Training Debug (Iteration 1017) ===
Q mean: -31.596767
Q std: 19.796988
Actor loss: 31.600763
Action reg: 0.003995
  l1.weight: grad_norm = 0.034711
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.136725
Total gradient norm: 0.296969
=== Actor Training Debug (Iteration 1018) ===
Q mean: -35.978897
Q std: 21.696424
Actor loss: 35.982834
Action reg: 0.003935
  l1.weight: grad_norm = 0.233216
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.991619
Total gradient norm: 2.220305
=== Actor Training Debug (Iteration 1019) ===
Q mean: -38.495232
Q std: 22.202436
Actor loss: 38.499195
Action reg: 0.003962
  l1.weight: grad_norm = 0.368525
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 1.481752
Total gradient norm: 3.102946
=== Actor Training Debug (Iteration 1020) ===
Q mean: -37.772663
Q std: 21.209734
Actor loss: 37.776634
Action reg: 0.003970
  l1.weight: grad_norm = 0.131498
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.535292
Total gradient norm: 1.182065
=== Actor Training Debug (Iteration 1021) ===
Q mean: -35.075935
Q std: 20.308777
Actor loss: 35.079906
Action reg: 0.003971
  l1.weight: grad_norm = 0.253020
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 1.093952
Total gradient norm: 2.230696
=== Actor Training Debug (Iteration 1022) ===
Q mean: -34.334183
Q std: 18.164953
Actor loss: 34.338127
Action reg: 0.003943
  l1.weight: grad_norm = 0.288310
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 1.107201
Total gradient norm: 2.351075
=== Actor Training Debug (Iteration 1023) ===
Q mean: -36.789665
Q std: 18.754578
Actor loss: 36.793652
Action reg: 0.003985
  l1.weight: grad_norm = 0.029824
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.096667
Total gradient norm: 0.170625
=== Actor Training Debug (Iteration 1024) ===
Q mean: -37.558067
Q std: 20.981750
Actor loss: 37.562016
Action reg: 0.003949
  l1.weight: grad_norm = 0.024716
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.087888
Total gradient norm: 0.189139
=== Actor Training Debug (Iteration 1025) ===
Q mean: -36.094158
Q std: 23.342155
Actor loss: 36.098114
Action reg: 0.003955
  l1.weight: grad_norm = 0.042127
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.136323
Total gradient norm: 0.249846
=== Actor Training Debug (Iteration 1026) ===
Q mean: -33.840042
Q std: 21.879293
Actor loss: 33.844002
Action reg: 0.003958
  l1.weight: grad_norm = 0.031969
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.098593
Total gradient norm: 0.161322
=== Actor Training Debug (Iteration 1027) ===
Q mean: -33.076607
Q std: 19.544621
Actor loss: 33.080585
Action reg: 0.003979
  l1.weight: grad_norm = 0.021483
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.067746
Total gradient norm: 0.123805
=== Actor Training Debug (Iteration 1028) ===
Q mean: -36.393253
Q std: 19.931591
Actor loss: 36.397224
Action reg: 0.003971
  l1.weight: grad_norm = 0.004156
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.012712
Total gradient norm: 0.022726
=== Actor Training Debug (Iteration 1029) ===
Q mean: -37.027519
Q std: 20.091398
Actor loss: 37.031487
Action reg: 0.003968
  l1.weight: grad_norm = 0.032815
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.109830
Total gradient norm: 0.177839
=== Actor Training Debug (Iteration 1030) ===
Q mean: -35.538849
Q std: 20.438553
Actor loss: 35.542839
Action reg: 0.003991
  l1.weight: grad_norm = 0.002366
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.008659
Total gradient norm: 0.014639
=== Actor Training Debug (Iteration 1031) ===
Q mean: -34.717541
Q std: 20.620932
Actor loss: 34.721523
Action reg: 0.003982
  l1.weight: grad_norm = 0.003785
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.012994
Total gradient norm: 0.023530
=== Actor Training Debug (Iteration 1032) ===
Q mean: -33.305275
Q std: 18.644995
Actor loss: 33.309261
Action reg: 0.003988
  l1.weight: grad_norm = 0.007977
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.026388
Total gradient norm: 0.048919
=== Actor Training Debug (Iteration 1033) ===
Q mean: -36.316559
Q std: 20.198252
Actor loss: 36.320549
Action reg: 0.003988
  l1.weight: grad_norm = 0.010639
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.034699
Total gradient norm: 0.067607
=== Actor Training Debug (Iteration 1034) ===
Q mean: -36.882252
Q std: 20.095600
Actor loss: 36.886227
Action reg: 0.003976
  l1.weight: grad_norm = 0.017404
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.060677
Total gradient norm: 0.118955
=== Actor Training Debug (Iteration 1035) ===
Q mean: -34.825008
Q std: 19.049294
Actor loss: 34.828991
Action reg: 0.003982
  l1.weight: grad_norm = 0.011239
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.035890
Total gradient norm: 0.063410
=== Actor Training Debug (Iteration 1036) ===
Q mean: -34.919552
Q std: 18.402672
Actor loss: 34.923531
Action reg: 0.003979
  l1.weight: grad_norm = 0.061131
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.216987
Total gradient norm: 0.362308
=== Actor Training Debug (Iteration 1037) ===
Q mean: -36.631813
Q std: 19.808952
Actor loss: 36.635788
Action reg: 0.003975
  l1.weight: grad_norm = 0.076964
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.234287
Total gradient norm: 0.457857
=== Actor Training Debug (Iteration 1038) ===
Q mean: -36.421955
Q std: 19.057045
Actor loss: 36.425930
Action reg: 0.003974
  l1.weight: grad_norm = 0.000438
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.001719
Total gradient norm: 0.004366
=== Actor Training Debug (Iteration 1039) ===
Q mean: -36.216858
Q std: 21.166161
Actor loss: 36.220818
Action reg: 0.003958
  l1.weight: grad_norm = 0.130663
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.318011
Total gradient norm: 0.596171
=== Actor Training Debug (Iteration 1040) ===
Q mean: -37.366070
Q std: 21.211842
Actor loss: 37.370060
Action reg: 0.003990
  l1.weight: grad_norm = 0.000636
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.002067
Total gradient norm: 0.003850
=== Actor Training Debug (Iteration 1041) ===
Q mean: -33.656960
Q std: 20.704298
Actor loss: 33.660908
Action reg: 0.003949
  l1.weight: grad_norm = 0.000790
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.002725
Total gradient norm: 0.006565
=== Actor Training Debug (Iteration 1042) ===
Q mean: -35.664574
Q std: 20.396914
Actor loss: 35.668571
Action reg: 0.003998
  l1.weight: grad_norm = 0.027926
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.090024
Total gradient norm: 0.155045
=== Actor Training Debug (Iteration 1043) ===
Q mean: -38.337414
Q std: 18.865522
Actor loss: 38.341404
Action reg: 0.003989
  l1.weight: grad_norm = 0.003659
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.011213
Total gradient norm: 0.021354
=== Actor Training Debug (Iteration 1044) ===
Q mean: -37.607521
Q std: 19.706488
Actor loss: 37.611473
Action reg: 0.003952
  l1.weight: grad_norm = 0.006354
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.021567
Total gradient norm: 0.041373
=== Actor Training Debug (Iteration 1045) ===
Q mean: -34.577679
Q std: 19.394012
Actor loss: 34.581669
Action reg: 0.003990
  l1.weight: grad_norm = 0.000703
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.001932
Total gradient norm: 0.002948
=== Actor Training Debug (Iteration 1046) ===
Q mean: -35.307171
Q std: 19.607759
Actor loss: 35.311150
Action reg: 0.003981
  l1.weight: grad_norm = 0.000877
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.002659
Total gradient norm: 0.004462
=== Actor Training Debug (Iteration 1047) ===
Q mean: -37.405190
Q std: 20.323837
Actor loss: 37.409180
Action reg: 0.003989
  l1.weight: grad_norm = 0.003594
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.012132
Total gradient norm: 0.021455
=== Actor Training Debug (Iteration 1048) ===
Q mean: -37.330002
Q std: 19.935877
Actor loss: 37.333992
Action reg: 0.003992
  l1.weight: grad_norm = 0.017702
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.053126
Total gradient norm: 0.092991
=== Actor Training Debug (Iteration 1049) ===
Q mean: -36.394173
Q std: 20.923288
Actor loss: 36.398121
Action reg: 0.003949
  l1.weight: grad_norm = 0.039183
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.142965
Total gradient norm: 0.226773
=== Actor Training Debug (Iteration 1050) ===
Q mean: -34.424133
Q std: 19.808332
Actor loss: 34.428112
Action reg: 0.003978
  l1.weight: grad_norm = 0.033210
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.103626
Total gradient norm: 0.185780
=== Actor Training Debug (Iteration 1051) ===
Q mean: -38.285942
Q std: 18.506058
Actor loss: 38.289944
Action reg: 0.004000
  l1.weight: grad_norm = 0.006100
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.021381
Total gradient norm: 0.036067
=== Actor Training Debug (Iteration 1052) ===
Q mean: -35.996735
Q std: 19.633612
Actor loss: 36.000675
Action reg: 0.003941
  l1.weight: grad_norm = 0.026946
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.068123
Total gradient norm: 0.104384
=== Actor Training Debug (Iteration 1053) ===
Q mean: -35.338310
Q std: 19.145859
Actor loss: 35.342297
Action reg: 0.003985
  l1.weight: grad_norm = 0.029110
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.096811
Total gradient norm: 0.175303
=== Actor Training Debug (Iteration 1054) ===
Q mean: -32.822090
Q std: 20.424744
Actor loss: 32.826061
Action reg: 0.003973
  l1.weight: grad_norm = 0.000824
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.002370
Total gradient norm: 0.004619
=== Actor Training Debug (Iteration 1055) ===
Q mean: -34.171345
Q std: 19.181629
Actor loss: 34.175316
Action reg: 0.003972
  l1.weight: grad_norm = 0.002961
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.009004
Total gradient norm: 0.014783
=== Actor Training Debug (Iteration 1056) ===
Q mean: -35.288765
Q std: 19.006462
Actor loss: 35.292736
Action reg: 0.003971
  l1.weight: grad_norm = 0.026052
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.098756
Total gradient norm: 0.189214
=== Actor Training Debug (Iteration 1057) ===
Q mean: -32.676628
Q std: 20.781311
Actor loss: 32.680592
Action reg: 0.003962
  l1.weight: grad_norm = 0.000748
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.002535
Total gradient norm: 0.005655
=== Actor Training Debug (Iteration 1058) ===
Q mean: -34.632896
Q std: 19.138729
Actor loss: 34.636883
Action reg: 0.003986
  l1.weight: grad_norm = 0.034445
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.097283
Total gradient norm: 0.153898
=== Actor Training Debug (Iteration 1059) ===
Q mean: -38.403870
Q std: 18.719971
Actor loss: 38.407829
Action reg: 0.003959
  l1.weight: grad_norm = 0.004381
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.015846
Total gradient norm: 0.031541
=== Actor Training Debug (Iteration 1060) ===
Q mean: -38.542671
Q std: 20.391014
Actor loss: 38.546631
Action reg: 0.003960
  l1.weight: grad_norm = 0.016183
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.044161
Total gradient norm: 0.057740
=== Actor Training Debug (Iteration 1061) ===
Q mean: -35.581932
Q std: 20.743202
Actor loss: 35.585907
Action reg: 0.003975
  l1.weight: grad_norm = 0.002715
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.008898
Total gradient norm: 0.016790
=== Actor Training Debug (Iteration 1062) ===
Q mean: -33.193890
Q std: 20.722906
Actor loss: 33.197865
Action reg: 0.003976
  l1.weight: grad_norm = 0.062279
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.202889
Total gradient norm: 0.359173
=== Actor Training Debug (Iteration 1063) ===
Q mean: -35.081322
Q std: 18.955252
Actor loss: 35.085289
Action reg: 0.003966
  l1.weight: grad_norm = 0.029822
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.095775
Total gradient norm: 0.173461
=== Actor Training Debug (Iteration 1064) ===
Q mean: -41.078407
Q std: 23.673298
Actor loss: 41.082382
Action reg: 0.003976
  l1.weight: grad_norm = 0.022683
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.076201
Total gradient norm: 0.112765
=== Actor Training Debug (Iteration 1065) ===
Q mean: -40.271080
Q std: 21.194708
Actor loss: 40.275059
Action reg: 0.003979
  l1.weight: grad_norm = 0.018953
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.062195
Total gradient norm: 0.102786
=== Actor Training Debug (Iteration 1066) ===
Q mean: -40.122902
Q std: 22.939661
Actor loss: 40.126869
Action reg: 0.003966
  l1.weight: grad_norm = 0.017899
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.060544
Total gradient norm: 0.116799
=== Actor Training Debug (Iteration 1067) ===
Q mean: -32.532356
Q std: 19.877493
Actor loss: 32.536324
Action reg: 0.003967
  l1.weight: grad_norm = 0.078485
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.349572
Total gradient norm: 0.825433
=== Actor Training Debug (Iteration 1068) ===
Q mean: -29.121912
Q std: 18.290195
Actor loss: 29.125879
Action reg: 0.003968
  l1.weight: grad_norm = 0.163443
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.720881
Total gradient norm: 1.672367
=== Actor Training Debug (Iteration 1069) ===
Q mean: -32.712734
Q std: 20.909441
Actor loss: 32.716686
Action reg: 0.003951
  l1.weight: grad_norm = 0.101580
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.428727
Total gradient norm: 0.946109
=== Actor Training Debug (Iteration 1070) ===
Q mean: -38.849041
Q std: 19.204287
Actor loss: 38.853031
Action reg: 0.003992
  l1.weight: grad_norm = 0.089290
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.362552
Total gradient norm: 0.768071
=== Actor Training Debug (Iteration 1071) ===
Q mean: -42.527687
Q std: 19.923632
Actor loss: 42.531666
Action reg: 0.003981
  l1.weight: grad_norm = 0.350113
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 1.410160
Total gradient norm: 3.061875
=== Actor Training Debug (Iteration 1072) ===
Q mean: -41.593136
Q std: 21.626423
Actor loss: 41.597099
Action reg: 0.003962
  l1.weight: grad_norm = 0.184388
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.723705
Total gradient norm: 1.397276
=== Actor Training Debug (Iteration 1073) ===
Q mean: -33.721016
Q std: 19.919430
Actor loss: 33.724991
Action reg: 0.003974
  l1.weight: grad_norm = 0.043408
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.172768
Total gradient norm: 0.314784
=== Actor Training Debug (Iteration 1074) ===
Q mean: -33.149361
Q std: 18.017582
Actor loss: 33.153343
Action reg: 0.003981
  l1.weight: grad_norm = 0.021023
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.069349
Total gradient norm: 0.119680
=== Actor Training Debug (Iteration 1075) ===
Q mean: -35.709217
Q std: 21.474348
Actor loss: 35.713200
Action reg: 0.003983
  l1.weight: grad_norm = 0.000303
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.001019
Total gradient norm: 0.002758
=== Actor Training Debug (Iteration 1076) ===
Q mean: -37.493469
Q std: 20.925556
Actor loss: 37.497471
Action reg: 0.004000
  l1.weight: grad_norm = 0.000595
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001645
Total gradient norm: 0.002641
=== Actor Training Debug (Iteration 1077) ===
Q mean: -38.574249
Q std: 19.509270
Actor loss: 38.578205
Action reg: 0.003956
  l1.weight: grad_norm = 0.001604
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.004838
Total gradient norm: 0.009257
=== Actor Training Debug (Iteration 1078) ===
Q mean: -36.888863
Q std: 19.430670
Actor loss: 36.892822
Action reg: 0.003961
  l1.weight: grad_norm = 0.038986
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.104813
Total gradient norm: 0.132796
=== Actor Training Debug (Iteration 1079) ===
Q mean: -34.686378
Q std: 20.212940
Actor loss: 34.690369
Action reg: 0.003991
  l1.weight: grad_norm = 0.000344
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.000936
Total gradient norm: 0.001691
=== Actor Training Debug (Iteration 1080) ===
Q mean: -34.723267
Q std: 19.845703
Actor loss: 34.727245
Action reg: 0.003978
  l1.weight: grad_norm = 0.023302
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.079120
Total gradient norm: 0.141200
=== Actor Training Debug (Iteration 1081) ===
Q mean: -37.576813
Q std: 21.599777
Actor loss: 37.580795
Action reg: 0.003981
  l1.weight: grad_norm = 0.013639
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.047441
Total gradient norm: 0.084258
=== Actor Training Debug (Iteration 1082) ===
Q mean: -39.934082
Q std: 21.399107
Actor loss: 39.938072
Action reg: 0.003989
  l1.weight: grad_norm = 0.019988
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.068069
Total gradient norm: 0.122563
=== Actor Training Debug (Iteration 1083) ===
Q mean: -36.924198
Q std: 20.467987
Actor loss: 36.928181
Action reg: 0.003981
  l1.weight: grad_norm = 0.010361
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.026659
Total gradient norm: 0.038710
=== Actor Training Debug (Iteration 1084) ===
Q mean: -33.196617
Q std: 17.793747
Actor loss: 33.200596
Action reg: 0.003981
  l1.weight: grad_norm = 0.000315
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.001004
Total gradient norm: 0.002448
=== Actor Training Debug (Iteration 1085) ===
Q mean: -37.712349
Q std: 19.346609
Actor loss: 37.716331
Action reg: 0.003982
  l1.weight: grad_norm = 0.000355
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.001204
Total gradient norm: 0.003022
=== Actor Training Debug (Iteration 1086) ===
Q mean: -36.889469
Q std: 20.828337
Actor loss: 36.893440
Action reg: 0.003970
  l1.weight: grad_norm = 0.004053
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.012986
Total gradient norm: 0.022546
=== Actor Training Debug (Iteration 1087) ===
Q mean: -37.724499
Q std: 21.047091
Actor loss: 37.728485
Action reg: 0.003988
  l1.weight: grad_norm = 0.002231
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.008135
Total gradient norm: 0.016156
=== Actor Training Debug (Iteration 1088) ===
Q mean: -32.477158
Q std: 18.868395
Actor loss: 32.481140
Action reg: 0.003982
  l1.weight: grad_norm = 0.000401
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.001549
Total gradient norm: 0.003965
=== Actor Training Debug (Iteration 1089) ===
Q mean: -33.022430
Q std: 18.615198
Actor loss: 33.026409
Action reg: 0.003978
  l1.weight: grad_norm = 0.000941
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.002660
Total gradient norm: 0.004755
=== Actor Training Debug (Iteration 1090) ===
Q mean: -39.948311
Q std: 20.710155
Actor loss: 39.952251
Action reg: 0.003941
  l1.weight: grad_norm = 0.091545
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.283845
Total gradient norm: 0.543903
=== Actor Training Debug (Iteration 1091) ===
Q mean: -37.409935
Q std: 19.791107
Actor loss: 37.413910
Action reg: 0.003976
  l1.weight: grad_norm = 0.011931
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.032058
Total gradient norm: 0.048231
=== Actor Training Debug (Iteration 1092) ===
Q mean: -35.131912
Q std: 19.117929
Actor loss: 35.135876
Action reg: 0.003962
  l1.weight: grad_norm = 0.055451
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.153669
Total gradient norm: 0.283779
=== Actor Training Debug (Iteration 1093) ===
Q mean: -34.460754
Q std: 19.212217
Actor loss: 34.464752
Action reg: 0.003999
  l1.weight: grad_norm = 0.013303
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.039494
Total gradient norm: 0.070617
=== Actor Training Debug (Iteration 1094) ===
Q mean: -39.899040
Q std: 21.001719
Actor loss: 39.903038
Action reg: 0.003999
  l1.weight: grad_norm = 0.007936
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.026835
Total gradient norm: 0.046539
=== Actor Training Debug (Iteration 1095) ===
Q mean: -39.054619
Q std: 20.678125
Actor loss: 39.058590
Action reg: 0.003971
  l1.weight: grad_norm = 0.003750
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.012346
Total gradient norm: 0.020526
=== Actor Training Debug (Iteration 1096) ===
Q mean: -36.682961
Q std: 20.702028
Actor loss: 36.686928
Action reg: 0.003969
  l1.weight: grad_norm = 0.000395
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.001457
Total gradient norm: 0.004222
=== Actor Training Debug (Iteration 1097) ===
Q mean: -36.542877
Q std: 20.714851
Actor loss: 36.546856
Action reg: 0.003979
  l1.weight: grad_norm = 0.025185
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.074125
Total gradient norm: 0.110415
=== Actor Training Debug (Iteration 1098) ===
Q mean: -39.178772
Q std: 20.520727
Actor loss: 39.182762
Action reg: 0.003990
  l1.weight: grad_norm = 0.043171
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.145172
Total gradient norm: 0.270912
=== Actor Training Debug (Iteration 1099) ===
Q mean: -34.106129
Q std: 21.020987
Actor loss: 34.110111
Action reg: 0.003982
  l1.weight: grad_norm = 0.005657
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.018382
Total gradient norm: 0.033066
=== Actor Training Debug (Iteration 1100) ===
Q mean: -35.022232
Q std: 20.077654
Actor loss: 35.026215
Action reg: 0.003983
  l1.weight: grad_norm = 0.000294
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.001158
Total gradient norm: 0.003218
Episode 61: Steps=100, Reward=-281.250, Buffer_size=6100
=== Actor Training Debug (Iteration 1101) ===
Q mean: -34.248955
Q std: 18.494120
Actor loss: 34.252945
Action reg: 0.003991
  l1.weight: grad_norm = 0.004180
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.012665
Total gradient norm: 0.022936
=== Actor Training Debug (Iteration 1102) ===
Q mean: -39.141319
Q std: 20.733929
Actor loss: 39.145290
Action reg: 0.003971
  l1.weight: grad_norm = 0.000355
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.001434
Total gradient norm: 0.004254
=== Actor Training Debug (Iteration 1103) ===
Q mean: -39.631626
Q std: 20.138945
Actor loss: 39.635601
Action reg: 0.003974
  l1.weight: grad_norm = 0.001049
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.002787
Total gradient norm: 0.004717
=== Actor Training Debug (Iteration 1104) ===
Q mean: -39.718822
Q std: 19.198973
Actor loss: 39.722816
Action reg: 0.003993
  l1.weight: grad_norm = 0.002143
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.006741
Total gradient norm: 0.012625
=== Actor Training Debug (Iteration 1105) ===
Q mean: -33.587547
Q std: 19.019320
Actor loss: 33.591522
Action reg: 0.003974
  l1.weight: grad_norm = 0.000424
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.001453
Total gradient norm: 0.003819
=== Actor Training Debug (Iteration 1106) ===
Q mean: -34.283661
Q std: 20.308212
Actor loss: 34.287651
Action reg: 0.003991
  l1.weight: grad_norm = 0.003204
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.010360
Total gradient norm: 0.019826
=== Actor Training Debug (Iteration 1107) ===
Q mean: -34.087528
Q std: 19.383657
Actor loss: 34.091518
Action reg: 0.003991
  l1.weight: grad_norm = 0.000216
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.000745
Total gradient norm: 0.001940
=== Actor Training Debug (Iteration 1108) ===
Q mean: -35.770485
Q std: 20.801317
Actor loss: 35.774464
Action reg: 0.003979
  l1.weight: grad_norm = 0.020427
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.058108
Total gradient norm: 0.098525
=== Actor Training Debug (Iteration 1109) ===
Q mean: -39.938824
Q std: 20.448917
Actor loss: 39.942806
Action reg: 0.003984
  l1.weight: grad_norm = 0.001250
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.004004
Total gradient norm: 0.006677
=== Actor Training Debug (Iteration 1110) ===
Q mean: -36.535492
Q std: 18.622524
Actor loss: 36.539482
Action reg: 0.003990
  l1.weight: grad_norm = 0.000748
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.002376
Total gradient norm: 0.005067
=== Actor Training Debug (Iteration 1111) ===
Q mean: -32.129089
Q std: 19.064837
Actor loss: 32.133053
Action reg: 0.003963
  l1.weight: grad_norm = 0.000633
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.002826
Total gradient norm: 0.007891
=== Actor Training Debug (Iteration 1112) ===
Q mean: -37.191719
Q std: 19.567135
Actor loss: 37.195717
Action reg: 0.004000
  l1.weight: grad_norm = 0.002876
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.008762
Total gradient norm: 0.014581
=== Actor Training Debug (Iteration 1113) ===
Q mean: -42.219086
Q std: 24.037550
Actor loss: 42.223064
Action reg: 0.003977
  l1.weight: grad_norm = 0.070158
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.197199
Total gradient norm: 0.365896
=== Actor Training Debug (Iteration 1114) ===
Q mean: -38.630638
Q std: 22.029442
Actor loss: 38.634598
Action reg: 0.003960
  l1.weight: grad_norm = 0.058628
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.179867
Total gradient norm: 0.330323
=== Actor Training Debug (Iteration 1115) ===
Q mean: -35.912708
Q std: 20.523548
Actor loss: 35.916691
Action reg: 0.003982
  l1.weight: grad_norm = 0.001072
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.003377
Total gradient norm: 0.007322
=== Actor Training Debug (Iteration 1116) ===
Q mean: -31.826227
Q std: 19.207300
Actor loss: 31.830202
Action reg: 0.003974
  l1.weight: grad_norm = 0.006988
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.021354
Total gradient norm: 0.038318
=== Actor Training Debug (Iteration 1117) ===
Q mean: -34.891560
Q std: 19.733223
Actor loss: 34.895531
Action reg: 0.003971
  l1.weight: grad_norm = 0.009338
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.025324
Total gradient norm: 0.038924
=== Actor Training Debug (Iteration 1118) ===
Q mean: -40.246719
Q std: 19.642288
Actor loss: 40.250713
Action reg: 0.003993
  l1.weight: grad_norm = 0.000169
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.000561
Total gradient norm: 0.001424
=== Actor Training Debug (Iteration 1119) ===
Q mean: -40.477161
Q std: 21.495798
Actor loss: 40.481133
Action reg: 0.003971
  l1.weight: grad_norm = 0.000333
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.001619
Total gradient norm: 0.004991
=== Actor Training Debug (Iteration 1120) ===
Q mean: -37.293743
Q std: 20.765415
Actor loss: 37.297726
Action reg: 0.003981
  l1.weight: grad_norm = 0.050459
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.144235
Total gradient norm: 0.236449
=== Actor Training Debug (Iteration 1121) ===
Q mean: -34.450054
Q std: 20.711754
Actor loss: 34.454037
Action reg: 0.003981
  l1.weight: grad_norm = 0.000491
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.001667
Total gradient norm: 0.003969
=== Actor Training Debug (Iteration 1122) ===
Q mean: -34.156158
Q std: 18.324440
Actor loss: 34.160160
Action reg: 0.004000
  l1.weight: grad_norm = 0.000029
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000108
Total gradient norm: 0.000197
=== Actor Training Debug (Iteration 1123) ===
Q mean: -37.223209
Q std: 19.691162
Actor loss: 37.227200
Action reg: 0.003990
  l1.weight: grad_norm = 0.001931
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.006843
Total gradient norm: 0.012946
=== Actor Training Debug (Iteration 1124) ===
Q mean: -36.149281
Q std: 19.699760
Actor loss: 36.153271
Action reg: 0.003990
  l1.weight: grad_norm = 0.008239
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.026823
Total gradient norm: 0.051556
=== Actor Training Debug (Iteration 1125) ===
Q mean: -37.525089
Q std: 19.453737
Actor loss: 37.529064
Action reg: 0.003973
  l1.weight: grad_norm = 0.000369
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.001542
Total gradient norm: 0.004539
=== Actor Training Debug (Iteration 1126) ===
Q mean: -35.650532
Q std: 18.298716
Actor loss: 35.654507
Action reg: 0.003975
  l1.weight: grad_norm = 0.009118
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.028770
Total gradient norm: 0.052210
=== Actor Training Debug (Iteration 1127) ===
Q mean: -37.630173
Q std: 19.906651
Actor loss: 37.634140
Action reg: 0.003966
  l1.weight: grad_norm = 0.000431
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.001940
Total gradient norm: 0.005934
=== Actor Training Debug (Iteration 1128) ===
Q mean: -36.924194
Q std: 21.674746
Actor loss: 36.928185
Action reg: 0.003991
  l1.weight: grad_norm = 0.001867
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.006402
Total gradient norm: 0.012637
=== Actor Training Debug (Iteration 1129) ===
Q mean: -35.557144
Q std: 18.936796
Actor loss: 35.561142
Action reg: 0.003998
  l1.weight: grad_norm = 0.097537
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.280894
Total gradient norm: 0.519936
=== Actor Training Debug (Iteration 1130) ===
Q mean: -35.790100
Q std: 19.704628
Actor loss: 35.794067
Action reg: 0.003965
  l1.weight: grad_norm = 0.002326
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.007697
Total gradient norm: 0.015055
=== Actor Training Debug (Iteration 1131) ===
Q mean: -38.346176
Q std: 19.028248
Actor loss: 38.350140
Action reg: 0.003963
  l1.weight: grad_norm = 0.003867
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.012056
Total gradient norm: 0.023564
=== Actor Training Debug (Iteration 1132) ===
Q mean: -35.257442
Q std: 20.518078
Actor loss: 35.261414
Action reg: 0.003973
  l1.weight: grad_norm = 0.032452
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.104724
Total gradient norm: 0.203172
=== Actor Training Debug (Iteration 1133) ===
Q mean: -36.488892
Q std: 20.594942
Actor loss: 36.492874
Action reg: 0.003981
  l1.weight: grad_norm = 0.000939
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.003398
Total gradient norm: 0.006570
=== Actor Training Debug (Iteration 1134) ===
Q mean: -36.667622
Q std: 19.207869
Actor loss: 36.671604
Action reg: 0.003983
  l1.weight: grad_norm = 0.000334
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.001414
Total gradient norm: 0.004048
=== Actor Training Debug (Iteration 1135) ===
Q mean: -36.410149
Q std: 20.380720
Actor loss: 36.414089
Action reg: 0.003939
  l1.weight: grad_norm = 0.009705
  l1.bias: grad_norm = 0.000481
  l2.weight: grad_norm = 0.031181
Total gradient norm: 0.054835
=== Actor Training Debug (Iteration 1136) ===
Q mean: -40.509621
Q std: 20.367819
Actor loss: 40.513603
Action reg: 0.003983
  l1.weight: grad_norm = 0.001581
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.005227
Total gradient norm: 0.010283
=== Actor Training Debug (Iteration 1137) ===
Q mean: -36.751839
Q std: 20.455276
Actor loss: 36.755821
Action reg: 0.003984
  l1.weight: grad_norm = 0.019777
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.055015
Total gradient norm: 0.073994
=== Actor Training Debug (Iteration 1138) ===
Q mean: -31.213028
Q std: 18.805237
Actor loss: 31.216980
Action reg: 0.003952
  l1.weight: grad_norm = 0.055174
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.161017
Total gradient norm: 0.229801
=== Actor Training Debug (Iteration 1139) ===
Q mean: -33.840546
Q std: 19.624083
Actor loss: 33.844528
Action reg: 0.003983
  l1.weight: grad_norm = 0.000407
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.001537
Total gradient norm: 0.004184
=== Actor Training Debug (Iteration 1140) ===
Q mean: -36.928295
Q std: 21.470909
Actor loss: 36.932259
Action reg: 0.003965
  l1.weight: grad_norm = 0.004359
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.013299
Total gradient norm: 0.024406
=== Actor Training Debug (Iteration 1141) ===
Q mean: -37.810913
Q std: 18.857540
Actor loss: 37.814877
Action reg: 0.003963
  l1.weight: grad_norm = 0.005057
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.015522
Total gradient norm: 0.027451
=== Actor Training Debug (Iteration 1142) ===
Q mean: -36.621536
Q std: 19.485773
Actor loss: 36.625519
Action reg: 0.003984
  l1.weight: grad_norm = 0.002962
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.009412
Total gradient norm: 0.017983
=== Actor Training Debug (Iteration 1143) ===
Q mean: -36.268963
Q std: 19.976238
Actor loss: 36.272930
Action reg: 0.003967
  l1.weight: grad_norm = 0.010367
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.029821
Total gradient norm: 0.053780
=== Actor Training Debug (Iteration 1144) ===
Q mean: -36.486809
Q std: 18.853724
Actor loss: 36.490776
Action reg: 0.003969
  l1.weight: grad_norm = 0.000808
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.002876
Total gradient norm: 0.006247
=== Actor Training Debug (Iteration 1145) ===
Q mean: -37.576080
Q std: 19.649338
Actor loss: 37.580070
Action reg: 0.003991
  l1.weight: grad_norm = 0.000271
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.000749
Total gradient norm: 0.001716
=== Actor Training Debug (Iteration 1146) ===
Q mean: -36.833855
Q std: 19.471331
Actor loss: 36.837845
Action reg: 0.003991
  l1.weight: grad_norm = 0.000233
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.000793
Total gradient norm: 0.002030
=== Actor Training Debug (Iteration 1147) ===
Q mean: -33.552025
Q std: 18.269560
Actor loss: 33.556007
Action reg: 0.003982
  l1.weight: grad_norm = 0.012691
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.035946
Total gradient norm: 0.063835
=== Actor Training Debug (Iteration 1148) ===
Q mean: -38.384613
Q std: 20.613735
Actor loss: 38.388565
Action reg: 0.003953
  l1.weight: grad_norm = 0.053483
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.170727
Total gradient norm: 0.337238
=== Actor Training Debug (Iteration 1149) ===
Q mean: -38.040985
Q std: 19.142448
Actor loss: 38.044941
Action reg: 0.003957
  l1.weight: grad_norm = 0.000443
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.002321
Total gradient norm: 0.007424
=== Actor Training Debug (Iteration 1150) ===
Q mean: -34.184505
Q std: 20.301548
Actor loss: 34.188469
Action reg: 0.003963
  l1.weight: grad_norm = 0.000453
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.001991
Total gradient norm: 0.005968
=== Actor Training Debug (Iteration 1151) ===
Q mean: -34.680672
Q std: 20.057848
Actor loss: 34.684635
Action reg: 0.003965
  l1.weight: grad_norm = 0.003696
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.012356
Total gradient norm: 0.023872
=== Actor Training Debug (Iteration 1152) ===
Q mean: -36.912487
Q std: 19.912107
Actor loss: 36.916485
Action reg: 0.004000
  l1.weight: grad_norm = 0.011735
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.031545
Total gradient norm: 0.047906
=== Actor Training Debug (Iteration 1153) ===
Q mean: -36.992905
Q std: 19.767166
Actor loss: 36.996868
Action reg: 0.003965
  l1.weight: grad_norm = 0.001448
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.003900
Total gradient norm: 0.007208
=== Actor Training Debug (Iteration 1154) ===
Q mean: -33.587112
Q std: 18.756264
Actor loss: 33.591095
Action reg: 0.003981
  l1.weight: grad_norm = 0.010824
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.029258
Total gradient norm: 0.043437
=== Actor Training Debug (Iteration 1155) ===
Q mean: -36.139839
Q std: 19.780470
Actor loss: 36.143822
Action reg: 0.003983
  l1.weight: grad_norm = 0.020310
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.060136
Total gradient norm: 0.111823
=== Actor Training Debug (Iteration 1156) ===
Q mean: -39.001854
Q std: 20.085104
Actor loss: 39.005836
Action reg: 0.003982
  l1.weight: grad_norm = 0.003942
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.011702
Total gradient norm: 0.022388
=== Actor Training Debug (Iteration 1157) ===
Q mean: -39.623604
Q std: 20.276480
Actor loss: 39.627586
Action reg: 0.003981
  l1.weight: grad_norm = 0.000334
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.001086
Total gradient norm: 0.002828
=== Actor Training Debug (Iteration 1158) ===
Q mean: -36.650215
Q std: 19.161121
Actor loss: 36.654190
Action reg: 0.003975
  l1.weight: grad_norm = 0.000602
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.001970
Total gradient norm: 0.004441
=== Actor Training Debug (Iteration 1159) ===
Q mean: -34.610569
Q std: 19.141449
Actor loss: 34.614571
Action reg: 0.004000
  l1.weight: grad_norm = 0.000568
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001587
Total gradient norm: 0.002840
=== Actor Training Debug (Iteration 1160) ===
Q mean: -35.590309
Q std: 20.345411
Actor loss: 35.594288
Action reg: 0.003980
  l1.weight: grad_norm = 0.024144
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.074525
Total gradient norm: 0.146048
=== Actor Training Debug (Iteration 1161) ===
Q mean: -37.890762
Q std: 21.044353
Actor loss: 37.894737
Action reg: 0.003974
  l1.weight: grad_norm = 0.000511
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.001628
Total gradient norm: 0.004396
=== Actor Training Debug (Iteration 1162) ===
Q mean: -36.203972
Q std: 20.902115
Actor loss: 36.207924
Action reg: 0.003950
  l1.weight: grad_norm = 0.000582
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.002762
Total gradient norm: 0.008860
=== Actor Training Debug (Iteration 1163) ===
Q mean: -35.951836
Q std: 18.390440
Actor loss: 35.955799
Action reg: 0.003965
  l1.weight: grad_norm = 0.000513
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.002097
Total gradient norm: 0.006538
=== Actor Training Debug (Iteration 1164) ===
Q mean: -38.162083
Q std: 20.860077
Actor loss: 38.166031
Action reg: 0.003949
  l1.weight: grad_norm = 0.018173
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.071421
Total gradient norm: 0.134426
=== Actor Training Debug (Iteration 1165) ===
Q mean: -35.180275
Q std: 18.616653
Actor loss: 35.184250
Action reg: 0.003975
  l1.weight: grad_norm = 0.000308
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.001413
Total gradient norm: 0.004149
=== Actor Training Debug (Iteration 1166) ===
Q mean: -37.834625
Q std: 18.838961
Actor loss: 37.838585
Action reg: 0.003958
  l1.weight: grad_norm = 0.000830
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.003020
Total gradient norm: 0.008008
=== Actor Training Debug (Iteration 1167) ===
Q mean: -35.639812
Q std: 19.957716
Actor loss: 35.643787
Action reg: 0.003974
  l1.weight: grad_norm = 0.005659
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.019614
Total gradient norm: 0.037110
=== Actor Training Debug (Iteration 1168) ===
Q mean: -34.174332
Q std: 21.405396
Actor loss: 34.178310
Action reg: 0.003977
  l1.weight: grad_norm = 0.015300
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.047293
Total gradient norm: 0.087974
=== Actor Training Debug (Iteration 1169) ===
Q mean: -34.399834
Q std: 22.456011
Actor loss: 34.403809
Action reg: 0.003973
  l1.weight: grad_norm = 0.006410
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.019275
Total gradient norm: 0.032545
=== Actor Training Debug (Iteration 1170) ===
Q mean: -37.056252
Q std: 21.396492
Actor loss: 37.060223
Action reg: 0.003972
  l1.weight: grad_norm = 0.000369
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.001736
Total gradient norm: 0.005111
=== Actor Training Debug (Iteration 1171) ===
Q mean: -36.620483
Q std: 18.338627
Actor loss: 36.624474
Action reg: 0.003989
  l1.weight: grad_norm = 0.014552
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.038377
Total gradient norm: 0.057225
=== Actor Training Debug (Iteration 1172) ===
Q mean: -36.931248
Q std: 19.183788
Actor loss: 36.935230
Action reg: 0.003984
  l1.weight: grad_norm = 0.000278
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.001253
Total gradient norm: 0.003811
=== Actor Training Debug (Iteration 1173) ===
Q mean: -37.871620
Q std: 20.620085
Actor loss: 37.875603
Action reg: 0.003983
  l1.weight: grad_norm = 0.000435
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.001475
Total gradient norm: 0.003533
=== Actor Training Debug (Iteration 1174) ===
Q mean: -34.618896
Q std: 21.048151
Actor loss: 34.622871
Action reg: 0.003975
  l1.weight: grad_norm = 0.000369
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.001735
Total gradient norm: 0.005578
=== Actor Training Debug (Iteration 1175) ===
Q mean: -34.707703
Q std: 18.899946
Actor loss: 34.711685
Action reg: 0.003981
  l1.weight: grad_norm = 0.009942
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.031170
Total gradient norm: 0.050215
=== Actor Training Debug (Iteration 1176) ===
Q mean: -35.818108
Q std: 21.391460
Actor loss: 35.822086
Action reg: 0.003980
  l1.weight: grad_norm = 0.033761
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.111601
Total gradient norm: 0.229671
=== Actor Training Debug (Iteration 1177) ===
Q mean: -40.922985
Q std: 19.260637
Actor loss: 40.926964
Action reg: 0.003980
  l1.weight: grad_norm = 0.000412
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.001428
Total gradient norm: 0.003704
=== Actor Training Debug (Iteration 1178) ===
Q mean: -39.041073
Q std: 18.862255
Actor loss: 39.045071
Action reg: 0.003999
  l1.weight: grad_norm = 0.012837
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.043582
Total gradient norm: 0.075742
=== Actor Training Debug (Iteration 1179) ===
Q mean: -35.140579
Q std: 21.246290
Actor loss: 35.144547
Action reg: 0.003968
  l1.weight: grad_norm = 0.032406
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.106118
Total gradient norm: 0.182097
=== Actor Training Debug (Iteration 1180) ===
Q mean: -35.833954
Q std: 20.962425
Actor loss: 35.837944
Action reg: 0.003992
  l1.weight: grad_norm = 0.001220
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.004088
Total gradient norm: 0.007389
=== Actor Training Debug (Iteration 1181) ===
Q mean: -37.551590
Q std: 22.077641
Actor loss: 37.555553
Action reg: 0.003965
  l1.weight: grad_norm = 0.041233
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.123190
Total gradient norm: 0.226978
=== Actor Training Debug (Iteration 1182) ===
Q mean: -37.984482
Q std: 19.338665
Actor loss: 37.988445
Action reg: 0.003964
  l1.weight: grad_norm = 0.007532
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.022669
Total gradient norm: 0.042830
=== Actor Training Debug (Iteration 1183) ===
Q mean: -34.489540
Q std: 18.445520
Actor loss: 34.493526
Action reg: 0.003987
  l1.weight: grad_norm = 0.073230
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.208822
Total gradient norm: 0.270496
=== Actor Training Debug (Iteration 1184) ===
Q mean: -37.364243
Q std: 19.174299
Actor loss: 37.368225
Action reg: 0.003982
  l1.weight: grad_norm = 0.000304
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.001131
Total gradient norm: 0.003059
=== Actor Training Debug (Iteration 1185) ===
Q mean: -39.989468
Q std: 17.475702
Actor loss: 39.993450
Action reg: 0.003981
  l1.weight: grad_norm = 0.002370
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.006604
Total gradient norm: 0.009003
=== Actor Training Debug (Iteration 1186) ===
Q mean: -38.809692
Q std: 20.024694
Actor loss: 38.813671
Action reg: 0.003980
  l1.weight: grad_norm = 0.016182
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.048978
Total gradient norm: 0.089291
=== Actor Training Debug (Iteration 1187) ===
Q mean: -35.336777
Q std: 19.618645
Actor loss: 35.340763
Action reg: 0.003985
  l1.weight: grad_norm = 0.000474
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.001686
Total gradient norm: 0.004266
=== Actor Training Debug (Iteration 1188) ===
Q mean: -35.687069
Q std: 20.133942
Actor loss: 35.691025
Action reg: 0.003957
  l1.weight: grad_norm = 0.004640
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.012220
Total gradient norm: 0.023046
=== Actor Training Debug (Iteration 1189) ===
Q mean: -38.452904
Q std: 19.563105
Actor loss: 38.456894
Action reg: 0.003991
  l1.weight: grad_norm = 0.002735
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.008658
Total gradient norm: 0.014908
=== Actor Training Debug (Iteration 1190) ===
Q mean: -37.200119
Q std: 19.840134
Actor loss: 37.204082
Action reg: 0.003965
  l1.weight: grad_norm = 0.000823
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.003308
Total gradient norm: 0.009235
=== Actor Training Debug (Iteration 1191) ===
Q mean: -36.996609
Q std: 19.410572
Actor loss: 37.000599
Action reg: 0.003991
  l1.weight: grad_norm = 0.002009
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.006477
Total gradient norm: 0.011498
=== Actor Training Debug (Iteration 1192) ===
Q mean: -37.335869
Q std: 19.741385
Actor loss: 37.339844
Action reg: 0.003974
  l1.weight: grad_norm = 0.004316
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.014001
Total gradient norm: 0.026029
=== Actor Training Debug (Iteration 1193) ===
Q mean: -39.253937
Q std: 20.309072
Actor loss: 39.257927
Action reg: 0.003991
  l1.weight: grad_norm = 0.000180
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.000751
Total gradient norm: 0.001968
=== Actor Training Debug (Iteration 1194) ===
Q mean: -42.395451
Q std: 21.141216
Actor loss: 42.399437
Action reg: 0.003988
  l1.weight: grad_norm = 0.020979
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.066181
Total gradient norm: 0.107488
=== Actor Training Debug (Iteration 1195) ===
Q mean: -36.974510
Q std: 19.365740
Actor loss: 36.978500
Action reg: 0.003990
  l1.weight: grad_norm = 0.003370
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.012142
Total gradient norm: 0.021093
=== Actor Training Debug (Iteration 1196) ===
Q mean: -34.293388
Q std: 17.148336
Actor loss: 34.297390
Action reg: 0.004000
  l1.weight: grad_norm = 0.000333
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000878
Total gradient norm: 0.001240
=== Actor Training Debug (Iteration 1197) ===
Q mean: -36.561897
Q std: 17.393103
Actor loss: 36.565880
Action reg: 0.003981
  l1.weight: grad_norm = 0.022227
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.075518
Total gradient norm: 0.137524
=== Actor Training Debug (Iteration 1198) ===
Q mean: -39.526546
Q std: 19.625793
Actor loss: 39.530529
Action reg: 0.003983
  l1.weight: grad_norm = 0.005550
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.016693
Total gradient norm: 0.026022
=== Actor Training Debug (Iteration 1199) ===
Q mean: -37.920860
Q std: 19.539106
Actor loss: 37.924843
Action reg: 0.003982
  l1.weight: grad_norm = 0.000437
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.001512
Total gradient norm: 0.003673
=== Actor Training Debug (Iteration 1200) ===
Q mean: -37.254356
Q std: 18.624218
Actor loss: 37.258339
Action reg: 0.003981
  l1.weight: grad_norm = 0.011971
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.040230
Total gradient norm: 0.075547
=== Actor Training Debug (Iteration 1201) ===
Q mean: -38.527153
Q std: 23.332243
Actor loss: 38.531151
Action reg: 0.004000
  l1.weight: grad_norm = 0.002355
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.007054
Total gradient norm: 0.012630
=== Actor Training Debug (Iteration 1202) ===
Q mean: -34.876015
Q std: 19.191799
Actor loss: 34.879997
Action reg: 0.003982
  l1.weight: grad_norm = 0.001312
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.003984
Total gradient norm: 0.008139
=== Actor Training Debug (Iteration 1203) ===
Q mean: -34.689945
Q std: 20.323523
Actor loss: 34.693935
Action reg: 0.003991
  l1.weight: grad_norm = 0.000411
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.001309
Total gradient norm: 0.002845
=== Actor Training Debug (Iteration 1204) ===
Q mean: -40.843361
Q std: 19.269081
Actor loss: 40.847343
Action reg: 0.003983
  l1.weight: grad_norm = 0.000308
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.001250
Total gradient norm: 0.003743
=== Actor Training Debug (Iteration 1205) ===
Q mean: -39.283279
Q std: 18.990473
Actor loss: 39.287270
Action reg: 0.003991
  l1.weight: grad_norm = 0.000676
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.001951
Total gradient norm: 0.003462
=== Actor Training Debug (Iteration 1206) ===
Q mean: -37.750130
Q std: 18.955227
Actor loss: 37.754086
Action reg: 0.003954
  l1.weight: grad_norm = 0.000557
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.002752
Total gradient norm: 0.008950
=== Actor Training Debug (Iteration 1207) ===
Q mean: -36.041351
Q std: 18.997665
Actor loss: 36.045311
Action reg: 0.003958
  l1.weight: grad_norm = 0.000600
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.003321
Total gradient norm: 0.011352
=== Actor Training Debug (Iteration 1208) ===
Q mean: -39.558559
Q std: 19.058363
Actor loss: 39.562542
Action reg: 0.003982
  l1.weight: grad_norm = 0.008832
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.029483
Total gradient norm: 0.047728
=== Actor Training Debug (Iteration 1209) ===
Q mean: -38.440975
Q std: 21.588879
Actor loss: 38.444958
Action reg: 0.003983
  l1.weight: grad_norm = 0.000522
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.002001
Total gradient norm: 0.005043
=== Actor Training Debug (Iteration 1210) ===
Q mean: -35.856979
Q std: 20.402758
Actor loss: 35.860935
Action reg: 0.003955
  l1.weight: grad_norm = 0.000582
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.002890
Total gradient norm: 0.008620
=== Actor Training Debug (Iteration 1211) ===
Q mean: -36.577881
Q std: 18.951405
Actor loss: 36.581856
Action reg: 0.003977
  l1.weight: grad_norm = 0.000456
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.001886
Total gradient norm: 0.005583
=== Actor Training Debug (Iteration 1212) ===
Q mean: -39.879303
Q std: 20.182068
Actor loss: 39.883293
Action reg: 0.003991
  l1.weight: grad_norm = 0.008361
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.025560
Total gradient norm: 0.046781
=== Actor Training Debug (Iteration 1213) ===
Q mean: -41.371853
Q std: 20.301105
Actor loss: 41.375824
Action reg: 0.003972
  l1.weight: grad_norm = 0.000664
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.002715
Total gradient norm: 0.006832
=== Actor Training Debug (Iteration 1214) ===
Q mean: -40.534706
Q std: 19.775108
Actor loss: 40.538670
Action reg: 0.003962
  l1.weight: grad_norm = 0.011137
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.037706
Total gradient norm: 0.068818
=== Actor Training Debug (Iteration 1215) ===
Q mean: -36.950916
Q std: 19.335520
Actor loss: 36.954876
Action reg: 0.003958
  l1.weight: grad_norm = 0.000567
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.003039
Total gradient norm: 0.010088
=== Actor Training Debug (Iteration 1216) ===
Q mean: -37.500927
Q std: 20.039169
Actor loss: 37.504921
Action reg: 0.003992
  l1.weight: grad_norm = 0.000477
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.001616
Total gradient norm: 0.003136
=== Actor Training Debug (Iteration 1217) ===
Q mean: -39.095085
Q std: 19.768543
Actor loss: 39.099064
Action reg: 0.003980
  l1.weight: grad_norm = 0.010850
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.028784
Total gradient norm: 0.038000
=== Actor Training Debug (Iteration 1218) ===
Q mean: -36.636730
Q std: 18.738167
Actor loss: 36.640686
Action reg: 0.003957
  l1.weight: grad_norm = 0.000662
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.003000
Total gradient norm: 0.008963
=== Actor Training Debug (Iteration 1219) ===
Q mean: -35.904762
Q std: 20.318851
Actor loss: 35.908714
Action reg: 0.003953
  l1.weight: grad_norm = 0.036297
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.101782
Total gradient norm: 0.171035
=== Actor Training Debug (Iteration 1220) ===
Q mean: -40.765549
Q std: 20.370733
Actor loss: 40.769531
Action reg: 0.003982
  l1.weight: grad_norm = 0.001726
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.005752
Total gradient norm: 0.010342
=== Actor Training Debug (Iteration 1221) ===
Q mean: -36.622917
Q std: 19.654976
Actor loss: 36.626865
Action reg: 0.003949
  l1.weight: grad_norm = 0.000613
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.003272
Total gradient norm: 0.010641
=== Actor Training Debug (Iteration 1222) ===
Q mean: -35.236176
Q std: 19.900284
Actor loss: 35.240158
Action reg: 0.003983
  l1.weight: grad_norm = 0.003079
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.010227
Total gradient norm: 0.018098
=== Actor Training Debug (Iteration 1223) ===
Q mean: -39.146969
Q std: 19.485130
Actor loss: 39.150913
Action reg: 0.003946
  l1.weight: grad_norm = 0.000636
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.003183
Total gradient norm: 0.009952
=== Actor Training Debug (Iteration 1224) ===
Q mean: -37.261383
Q std: 21.299171
Actor loss: 37.265369
Action reg: 0.003985
  l1.weight: grad_norm = 0.022932
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.070466
Total gradient norm: 0.126763
=== Actor Training Debug (Iteration 1225) ===
Q mean: -38.356689
Q std: 19.624773
Actor loss: 38.360680
Action reg: 0.003991
  l1.weight: grad_norm = 0.000795
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.002554
Total gradient norm: 0.004688
=== Actor Training Debug (Iteration 1226) ===
Q mean: -39.299885
Q std: 19.580126
Actor loss: 39.303856
Action reg: 0.003971
  l1.weight: grad_norm = 0.000442
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.001735
Total gradient norm: 0.005305
=== Actor Training Debug (Iteration 1227) ===
Q mean: -37.966835
Q std: 20.979000
Actor loss: 37.970806
Action reg: 0.003972
  l1.weight: grad_norm = 0.009513
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.027856
Total gradient norm: 0.050537
=== Actor Training Debug (Iteration 1228) ===
Q mean: -37.427856
Q std: 19.220764
Actor loss: 37.431816
Action reg: 0.003958
  l1.weight: grad_norm = 0.001022
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.003724
Total gradient norm: 0.009981
=== Actor Training Debug (Iteration 1229) ===
Q mean: -41.071526
Q std: 20.032188
Actor loss: 41.075516
Action reg: 0.003990
  l1.weight: grad_norm = 0.009118
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.025327
Total gradient norm: 0.042249
=== Actor Training Debug (Iteration 1230) ===
Q mean: -38.202477
Q std: 21.036585
Actor loss: 38.206432
Action reg: 0.003957
  l1.weight: grad_norm = 0.000655
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.002822
Total gradient norm: 0.008459
=== Actor Training Debug (Iteration 1231) ===
Q mean: -34.071167
Q std: 18.965797
Actor loss: 34.075138
Action reg: 0.003972
  l1.weight: grad_norm = 0.000378
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.001764
Total gradient norm: 0.005539
=== Actor Training Debug (Iteration 1232) ===
Q mean: -34.577740
Q std: 19.397579
Actor loss: 34.581711
Action reg: 0.003971
  l1.weight: grad_norm = 0.012368
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.040157
Total gradient norm: 0.068792
=== Actor Training Debug (Iteration 1233) ===
Q mean: -38.809124
Q std: 18.024433
Actor loss: 38.813114
Action reg: 0.003991
  l1.weight: grad_norm = 0.000234
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.000819
Total gradient norm: 0.002073
=== Actor Training Debug (Iteration 1234) ===
Q mean: -39.308464
Q std: 21.273499
Actor loss: 39.312466
Action reg: 0.004000
  l1.weight: grad_norm = 0.000049
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000171
Total gradient norm: 0.000309
=== Actor Training Debug (Iteration 1235) ===
Q mean: -36.550461
Q std: 18.905510
Actor loss: 36.554451
Action reg: 0.003989
  l1.weight: grad_norm = 0.004831
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.014555
Total gradient norm: 0.027623
=== Actor Training Debug (Iteration 1236) ===
Q mean: -38.152527
Q std: 18.549585
Actor loss: 38.156509
Action reg: 0.003983
  l1.weight: grad_norm = 0.028547
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.084177
Total gradient norm: 0.153818
=== Actor Training Debug (Iteration 1237) ===
Q mean: -41.265289
Q std: 20.592194
Actor loss: 41.269260
Action reg: 0.003970
  l1.weight: grad_norm = 0.074471
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.266909
Total gradient norm: 0.488541
=== Actor Training Debug (Iteration 1238) ===
Q mean: -41.161797
Q std: 21.024122
Actor loss: 41.165771
Action reg: 0.003975
  l1.weight: grad_norm = 0.000464
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.002036
Total gradient norm: 0.005977
=== Actor Training Debug (Iteration 1239) ===
Q mean: -41.003555
Q std: 19.715544
Actor loss: 41.007557
Action reg: 0.004000
  l1.weight: grad_norm = 0.001095
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.003503
Total gradient norm: 0.006111
=== Actor Training Debug (Iteration 1240) ===
Q mean: -33.071228
Q std: 18.922546
Actor loss: 33.075211
Action reg: 0.003982
  l1.weight: grad_norm = 0.000331
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.001139
Total gradient norm: 0.003107
=== Actor Training Debug (Iteration 1241) ===
Q mean: -34.914398
Q std: 17.426641
Actor loss: 34.918388
Action reg: 0.003991
  l1.weight: grad_norm = 0.000240
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.000775
Total gradient norm: 0.002026
=== Actor Training Debug (Iteration 1242) ===
Q mean: -37.354713
Q std: 19.774866
Actor loss: 37.358696
Action reg: 0.003984
  l1.weight: grad_norm = 0.000815
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.002805
Total gradient norm: 0.006005
=== Actor Training Debug (Iteration 1243) ===
Q mean: -43.118969
Q std: 21.165922
Actor loss: 43.122952
Action reg: 0.003984
  l1.weight: grad_norm = 0.000315
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.001318
Total gradient norm: 0.004043
=== Actor Training Debug (Iteration 1244) ===
Q mean: -40.361904
Q std: 20.374338
Actor loss: 40.365860
Action reg: 0.003954
  l1.weight: grad_norm = 0.004599
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.015160
Total gradient norm: 0.028052
=== Actor Training Debug (Iteration 1245) ===
Q mean: -37.966003
Q std: 20.203243
Actor loss: 37.969990
Action reg: 0.003986
  l1.weight: grad_norm = 0.000470
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.001954
Total gradient norm: 0.005268
=== Actor Training Debug (Iteration 1246) ===
Q mean: -31.786415
Q std: 19.319796
Actor loss: 31.790398
Action reg: 0.003982
  l1.weight: grad_norm = 0.001364
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.004430
Total gradient norm: 0.007804
=== Actor Training Debug (Iteration 1247) ===
Q mean: -33.243980
Q std: 20.516567
Actor loss: 33.247944
Action reg: 0.003965
  l1.weight: grad_norm = 0.000688
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.002739
Total gradient norm: 0.007693
=== Actor Training Debug (Iteration 1248) ===
Q mean: -35.993530
Q std: 21.901531
Actor loss: 35.997505
Action reg: 0.003974
  l1.weight: grad_norm = 0.003671
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.009413
Total gradient norm: 0.014217
=== Actor Training Debug (Iteration 1249) ===
Q mean: -43.424370
Q std: 20.437214
Actor loss: 43.428360
Action reg: 0.003991
  l1.weight: grad_norm = 0.000598
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.001963
Total gradient norm: 0.003390
=== Actor Training Debug (Iteration 1250) ===
Q mean: -44.528664
Q std: 20.036890
Actor loss: 44.532642
Action reg: 0.003980
  l1.weight: grad_norm = 0.024106
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.079762
Total gradient norm: 0.137612
=== Actor Training Debug (Iteration 1251) ===
Q mean: -39.243988
Q std: 18.639009
Actor loss: 39.247963
Action reg: 0.003974
  l1.weight: grad_norm = 0.012784
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.042194
Total gradient norm: 0.072527
=== Actor Training Debug (Iteration 1252) ===
Q mean: -34.850929
Q std: 20.332397
Actor loss: 34.854889
Action reg: 0.003958
  l1.weight: grad_norm = 0.000583
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.003120
Total gradient norm: 0.010365
=== Actor Training Debug (Iteration 1253) ===
Q mean: -32.262161
Q std: 19.546127
Actor loss: 32.266144
Action reg: 0.003984
  l1.weight: grad_norm = 0.000881
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.003272
Total gradient norm: 0.007647
=== Actor Training Debug (Iteration 1254) ===
Q mean: -34.828007
Q std: 19.047083
Actor loss: 34.831985
Action reg: 0.003981
  l1.weight: grad_norm = 0.019440
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.069374
Total gradient norm: 0.129480
=== Actor Training Debug (Iteration 1255) ===
Q mean: -41.771988
Q std: 20.894135
Actor loss: 41.775986
Action reg: 0.003996
  l1.weight: grad_norm = 0.063644
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.217378
Total gradient norm: 0.396684
=== Actor Training Debug (Iteration 1256) ===
Q mean: -39.480686
Q std: 19.974754
Actor loss: 39.484665
Action reg: 0.003979
  l1.weight: grad_norm = 0.015362
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.046803
Total gradient norm: 0.068796
=== Actor Training Debug (Iteration 1257) ===
Q mean: -41.076225
Q std: 20.787861
Actor loss: 41.080196
Action reg: 0.003971
  l1.weight: grad_norm = 0.001732
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.005291
Total gradient norm: 0.009069
=== Actor Training Debug (Iteration 1258) ===
Q mean: -34.876060
Q std: 18.977249
Actor loss: 34.880039
Action reg: 0.003981
  l1.weight: grad_norm = 0.017183
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.057469
Total gradient norm: 0.102847
=== Actor Training Debug (Iteration 1259) ===
Q mean: -31.603056
Q std: 17.037931
Actor loss: 31.607018
Action reg: 0.003962
  l1.weight: grad_norm = 0.004473
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.014966
Total gradient norm: 0.026079
=== Actor Training Debug (Iteration 1260) ===
Q mean: -37.635670
Q std: 18.739325
Actor loss: 37.639648
Action reg: 0.003980
  l1.weight: grad_norm = 0.064784
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.212975
Total gradient norm: 0.383488
=== Actor Training Debug (Iteration 1261) ===
Q mean: -41.221573
Q std: 19.924444
Actor loss: 41.225563
Action reg: 0.003992
  l1.weight: grad_norm = 0.000147
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.000639
Total gradient norm: 0.001780
=== Actor Training Debug (Iteration 1262) ===
Q mean: -41.051704
Q std: 19.542002
Actor loss: 41.055687
Action reg: 0.003983
  l1.weight: grad_norm = 0.000393
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.001261
Total gradient norm: 0.003307
=== Actor Training Debug (Iteration 1263) ===
Q mean: -39.634388
Q std: 20.199606
Actor loss: 39.638371
Action reg: 0.003983
  l1.weight: grad_norm = 0.000480
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.002066
Total gradient norm: 0.005703
=== Actor Training Debug (Iteration 1264) ===
Q mean: -32.453087
Q std: 19.794224
Actor loss: 32.457069
Action reg: 0.003981
  l1.weight: grad_norm = 0.000381
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.001451
Total gradient norm: 0.004156
=== Actor Training Debug (Iteration 1265) ===
Q mean: -34.778408
Q std: 19.149603
Actor loss: 34.782398
Action reg: 0.003992
  l1.weight: grad_norm = 0.000263
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.000827
Total gradient norm: 0.001974
=== Actor Training Debug (Iteration 1266) ===
Q mean: -37.955360
Q std: 21.285158
Actor loss: 37.959362
Action reg: 0.004000
  l1.weight: grad_norm = 0.000449
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001234
Total gradient norm: 0.002005
=== Actor Training Debug (Iteration 1267) ===
Q mean: -40.848839
Q std: 21.888369
Actor loss: 40.852829
Action reg: 0.003991
  l1.weight: grad_norm = 0.017764
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.059445
Total gradient norm: 0.102673
=== Actor Training Debug (Iteration 1268) ===
Q mean: -40.143272
Q std: 20.802664
Actor loss: 40.147251
Action reg: 0.003980
  l1.weight: grad_norm = 0.015310
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.045243
Total gradient norm: 0.087557
=== Actor Training Debug (Iteration 1269) ===
Q mean: -39.335453
Q std: 17.140705
Actor loss: 39.339436
Action reg: 0.003981
  l1.weight: grad_norm = 0.108572
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.342492
Total gradient norm: 0.633442
=== Actor Training Debug (Iteration 1270) ===
Q mean: -34.447418
Q std: 18.286968
Actor loss: 34.451382
Action reg: 0.003962
  l1.weight: grad_norm = 0.011433
  l1.bias: grad_norm = 0.000428
  l2.weight: grad_norm = 0.036699
Total gradient norm: 0.064277
=== Actor Training Debug (Iteration 1271) ===
Q mean: -36.446808
Q std: 21.214911
Actor loss: 36.450790
Action reg: 0.003982
  l1.weight: grad_norm = 0.001539
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.005312
Total gradient norm: 0.009952
=== Actor Training Debug (Iteration 1272) ===
Q mean: -38.123543
Q std: 22.472044
Actor loss: 38.127533
Action reg: 0.003991
  l1.weight: grad_norm = 0.000277
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.001085
Total gradient norm: 0.002819
=== Actor Training Debug (Iteration 1273) ===
Q mean: -38.896763
Q std: 20.562624
Actor loss: 38.900734
Action reg: 0.003973
  l1.weight: grad_norm = 0.000395
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.001785
Total gradient norm: 0.005494
=== Actor Training Debug (Iteration 1274) ===
Q mean: -40.504280
Q std: 19.597183
Actor loss: 40.508270
Action reg: 0.003991
  l1.weight: grad_norm = 0.000594
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.002301
Total gradient norm: 0.005623
=== Actor Training Debug (Iteration 1275) ===
Q mean: -37.154289
Q std: 18.915985
Actor loss: 37.158272
Action reg: 0.003983
  l1.weight: grad_norm = 0.003835
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.012982
Total gradient norm: 0.028668
=== Actor Training Debug (Iteration 1276) ===
Q mean: -37.867779
Q std: 19.256851
Actor loss: 37.871738
Action reg: 0.003961
  l1.weight: grad_norm = 0.158411
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.578802
Total gradient norm: 1.274735
=== Actor Training Debug (Iteration 1277) ===
Q mean: -36.765083
Q std: 19.581121
Actor loss: 36.769051
Action reg: 0.003967
  l1.weight: grad_norm = 0.041728
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.142256
Total gradient norm: 0.259590
=== Actor Training Debug (Iteration 1278) ===
Q mean: -41.603233
Q std: 23.003994
Actor loss: 41.607182
Action reg: 0.003947
  l1.weight: grad_norm = 0.350859
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 1.185028
Total gradient norm: 2.402558
=== Actor Training Debug (Iteration 1279) ===
Q mean: -42.180077
Q std: 22.381363
Actor loss: 42.184048
Action reg: 0.003971
  l1.weight: grad_norm = 0.036952
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.138196
Total gradient norm: 0.278206
=== Actor Training Debug (Iteration 1280) ===
Q mean: -37.872070
Q std: 20.540989
Actor loss: 37.876064
Action reg: 0.003994
  l1.weight: grad_norm = 0.208341
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.762594
Total gradient norm: 1.769574
=== Actor Training Debug (Iteration 1281) ===
Q mean: -34.713631
Q std: 20.135241
Actor loss: 34.717606
Action reg: 0.003974
  l1.weight: grad_norm = 0.100390
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.422078
Total gradient norm: 0.916038
=== Actor Training Debug (Iteration 1282) ===
Q mean: -37.709621
Q std: 21.258984
Actor loss: 37.713573
Action reg: 0.003951
  l1.weight: grad_norm = 0.155115
  l1.bias: grad_norm = 0.000492
  l2.weight: grad_norm = 0.479905
Total gradient norm: 1.070064
=== Actor Training Debug (Iteration 1283) ===
Q mean: -39.718601
Q std: 20.895773
Actor loss: 39.722584
Action reg: 0.003983
  l1.weight: grad_norm = 0.036724
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.142558
Total gradient norm: 0.346667
=== Actor Training Debug (Iteration 1284) ===
Q mean: -40.052044
Q std: 17.367435
Actor loss: 40.056026
Action reg: 0.003982
  l1.weight: grad_norm = 0.030072
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.121417
Total gradient norm: 0.306909
=== Actor Training Debug (Iteration 1285) ===
Q mean: -37.932278
Q std: 19.868155
Actor loss: 37.936245
Action reg: 0.003966
  l1.weight: grad_norm = 0.023688
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.076200
Total gradient norm: 0.168240
=== Actor Training Debug (Iteration 1286) ===
Q mean: -38.104965
Q std: 20.299667
Actor loss: 38.108940
Action reg: 0.003973
  l1.weight: grad_norm = 0.130147
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.380144
Total gradient norm: 0.551922
=== Actor Training Debug (Iteration 1287) ===
Q mean: -37.751221
Q std: 18.998701
Actor loss: 37.755177
Action reg: 0.003956
  l1.weight: grad_norm = 0.065835
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.225654
Total gradient norm: 0.454992
=== Actor Training Debug (Iteration 1288) ===
Q mean: -39.911949
Q std: 20.054262
Actor loss: 39.915909
Action reg: 0.003961
  l1.weight: grad_norm = 0.711622
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 2.549717
Total gradient norm: 5.418089
=== Actor Training Debug (Iteration 1289) ===
Q mean: -37.101204
Q std: 19.194052
Actor loss: 37.105129
Action reg: 0.003927
  l1.weight: grad_norm = 0.076478
  l1.bias: grad_norm = 0.000739
  l2.weight: grad_norm = 0.253612
Total gradient norm: 0.465035
=== Actor Training Debug (Iteration 1290) ===
Q mean: -39.790031
Q std: 20.443165
Actor loss: 39.794014
Action reg: 0.003981
  l1.weight: grad_norm = 0.000508
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.001590
Total gradient norm: 0.004196
=== Actor Training Debug (Iteration 1291) ===
Q mean: -39.738091
Q std: 20.371216
Actor loss: 39.742073
Action reg: 0.003983
  l1.weight: grad_norm = 0.002806
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.008877
Total gradient norm: 0.017163
=== Actor Training Debug (Iteration 1292) ===
Q mean: -37.531132
Q std: 19.238453
Actor loss: 37.535114
Action reg: 0.003983
  l1.weight: grad_norm = 0.015067
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.043679
Total gradient norm: 0.074871
=== Actor Training Debug (Iteration 1293) ===
Q mean: -37.425606
Q std: 21.637718
Actor loss: 37.429588
Action reg: 0.003983
  l1.weight: grad_norm = 0.002365
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.007535
Total gradient norm: 0.013414
=== Actor Training Debug (Iteration 1294) ===
Q mean: -38.754341
Q std: 20.628744
Actor loss: 38.758331
Action reg: 0.003990
  l1.weight: grad_norm = 0.008178
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.028587
Total gradient norm: 0.051305
=== Actor Training Debug (Iteration 1295) ===
Q mean: -40.381058
Q std: 21.194351
Actor loss: 40.385036
Action reg: 0.003978
  l1.weight: grad_norm = 0.000396
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.001332
Total gradient norm: 0.003754
=== Actor Training Debug (Iteration 1296) ===
Q mean: -36.909767
Q std: 19.115044
Actor loss: 36.913750
Action reg: 0.003984
  l1.weight: grad_norm = 0.000337
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.000924
Total gradient norm: 0.002480
=== Actor Training Debug (Iteration 1297) ===
Q mean: -40.688404
Q std: 19.773733
Actor loss: 40.692398
Action reg: 0.003992
  l1.weight: grad_norm = 0.000266
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.000752
Total gradient norm: 0.001601
=== Actor Training Debug (Iteration 1298) ===
Q mean: -37.991970
Q std: 18.230711
Actor loss: 37.995945
Action reg: 0.003975
  l1.weight: grad_norm = 0.000337
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.001257
Total gradient norm: 0.003581
=== Actor Training Debug (Iteration 1299) ===
Q mean: -36.296272
Q std: 20.271145
Actor loss: 36.300247
Action reg: 0.003975
  l1.weight: grad_norm = 0.018598
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.057201
Total gradient norm: 0.105238
=== Actor Training Debug (Iteration 1300) ===
Q mean: -38.508881
Q std: 19.817234
Actor loss: 38.512859
Action reg: 0.003980
  l1.weight: grad_norm = 0.001818
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.005557
Total gradient norm: 0.009646
=== Actor Training Debug (Iteration 1301) ===
Q mean: -38.089008
Q std: 21.658722
Actor loss: 38.092999
Action reg: 0.003991
  l1.weight: grad_norm = 0.002472
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.007791
Total gradient norm: 0.013380
=== Actor Training Debug (Iteration 1302) ===
Q mean: -38.267113
Q std: 18.113419
Actor loss: 38.271099
Action reg: 0.003985
  l1.weight: grad_norm = 0.000367
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.000916
Total gradient norm: 0.002344
=== Actor Training Debug (Iteration 1303) ===
Q mean: -41.984879
Q std: 20.893944
Actor loss: 41.988834
Action reg: 0.003955
  l1.weight: grad_norm = 0.006881
  l1.bias: grad_norm = 0.000599
  l2.weight: grad_norm = 0.020879
Total gradient norm: 0.035796
=== Actor Training Debug (Iteration 1304) ===
Q mean: -38.006016
Q std: 19.759233
Actor loss: 38.010002
Action reg: 0.003984
  l1.weight: grad_norm = 0.000310
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.000831
Total gradient norm: 0.002179
=== Actor Training Debug (Iteration 1305) ===
Q mean: -37.536148
Q std: 19.050764
Actor loss: 37.540134
Action reg: 0.003986
  l1.weight: grad_norm = 0.000368
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.001075
Total gradient norm: 0.002546
=== Actor Training Debug (Iteration 1306) ===
Q mean: -37.254307
Q std: 20.288704
Actor loss: 37.258278
Action reg: 0.003971
  l1.weight: grad_norm = 0.000524
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.001891
Total gradient norm: 0.005372
=== Actor Training Debug (Iteration 1307) ===
Q mean: -37.330624
Q std: 19.605469
Actor loss: 37.334599
Action reg: 0.003976
  l1.weight: grad_norm = 0.000501
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.001478
Total gradient norm: 0.003771
=== Actor Training Debug (Iteration 1308) ===
Q mean: -36.896828
Q std: 19.932663
Actor loss: 36.900810
Action reg: 0.003982
  l1.weight: grad_norm = 0.006275
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.020593
Total gradient norm: 0.037921
=== Actor Training Debug (Iteration 1309) ===
Q mean: -39.178299
Q std: 21.101702
Actor loss: 39.182262
Action reg: 0.003964
  l1.weight: grad_norm = 0.001086
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.003767
Total gradient norm: 0.007288
=== Actor Training Debug (Iteration 1310) ===
Q mean: -39.961918
Q std: 21.721788
Actor loss: 39.965874
Action reg: 0.003956
  l1.weight: grad_norm = 0.056795
  l1.bias: grad_norm = 0.000505
  l2.weight: grad_norm = 0.188946
Total gradient norm: 0.323340
=== Actor Training Debug (Iteration 1311) ===
Q mean: -39.429768
Q std: 19.506359
Actor loss: 39.433731
Action reg: 0.003963
  l1.weight: grad_norm = 0.001286
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.004176
Total gradient norm: 0.007790
=== Actor Training Debug (Iteration 1312) ===
Q mean: -37.581955
Q std: 19.077126
Actor loss: 37.585949
Action reg: 0.003993
  l1.weight: grad_norm = 0.000230
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.000524
Total gradient norm: 0.001018
=== Actor Training Debug (Iteration 1313) ===
Q mean: -36.631622
Q std: 17.869776
Actor loss: 36.635612
Action reg: 0.003988
  l1.weight: grad_norm = 0.000432
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.001051
Total gradient norm: 0.002791
=== Actor Training Debug (Iteration 1314) ===
Q mean: -42.031265
Q std: 19.126572
Actor loss: 42.035240
Action reg: 0.003973
  l1.weight: grad_norm = 0.000497
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.001550
Total gradient norm: 0.004288
=== Actor Training Debug (Iteration 1315) ===
Q mean: -40.571335
Q std: 19.600498
Actor loss: 40.575321
Action reg: 0.003985
  l1.weight: grad_norm = 0.000331
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.000885
Total gradient norm: 0.002461
=== Actor Training Debug (Iteration 1316) ===
Q mean: -37.104820
Q std: 18.965605
Actor loss: 37.108784
Action reg: 0.003962
  l1.weight: grad_norm = 0.001648
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.004527
Total gradient norm: 0.009114
=== Actor Training Debug (Iteration 1317) ===
Q mean: -35.672791
Q std: 21.044525
Actor loss: 35.676792
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1318) ===
Q mean: -40.591576
Q std: 19.263498
Actor loss: 40.595566
Action reg: 0.003992
  l1.weight: grad_norm = 0.000322
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.000723
Total gradient norm: 0.001574
=== Actor Training Debug (Iteration 1319) ===
Q mean: -39.092751
Q std: 21.658680
Actor loss: 39.096718
Action reg: 0.003966
  l1.weight: grad_norm = 0.000681
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.002355
Total gradient norm: 0.006897
=== Actor Training Debug (Iteration 1320) ===
Q mean: -37.491203
Q std: 19.151741
Actor loss: 37.495197
Action reg: 0.003995
  l1.weight: grad_norm = 0.000309
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.000842
Total gradient norm: 0.002040
=== Actor Training Debug (Iteration 1321) ===
Q mean: -34.305824
Q std: 20.243647
Actor loss: 34.309780
Action reg: 0.003957
  l1.weight: grad_norm = 0.000614
  l1.bias: grad_norm = 0.000563
  l2.weight: grad_norm = 0.002547
Total gradient norm: 0.007387
=== Actor Training Debug (Iteration 1322) ===
Q mean: -38.652004
Q std: 19.422239
Actor loss: 38.655975
Action reg: 0.003972
  l1.weight: grad_norm = 0.000521
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.001782
Total gradient norm: 0.005211
=== Actor Training Debug (Iteration 1323) ===
Q mean: -38.934952
Q std: 18.462114
Actor loss: 38.938938
Action reg: 0.003986
  l1.weight: grad_norm = 0.000386
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.001023
Total gradient norm: 0.002666
=== Actor Training Debug (Iteration 1324) ===
Q mean: -37.222763
Q std: 20.138340
Actor loss: 37.226757
Action reg: 0.003993
  l1.weight: grad_norm = 0.000286
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.000607
Total gradient norm: 0.001355
=== Actor Training Debug (Iteration 1325) ===
Q mean: -38.032928
Q std: 20.073462
Actor loss: 38.036907
Action reg: 0.003979
  l1.weight: grad_norm = 0.000413
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.001256
Total gradient norm: 0.003369
=== Actor Training Debug (Iteration 1326) ===
Q mean: -42.912285
Q std: 21.451160
Actor loss: 42.916264
Action reg: 0.003979
  l1.weight: grad_norm = 0.000443
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.001275
Total gradient norm: 0.003525
=== Actor Training Debug (Iteration 1327) ===
Q mean: -41.783005
Q std: 19.429415
Actor loss: 41.786999
Action reg: 0.003994
  l1.weight: grad_norm = 0.000200
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.000487
Total gradient norm: 0.001268
=== Actor Training Debug (Iteration 1328) ===
Q mean: -35.553200
Q std: 19.246338
Actor loss: 35.557186
Action reg: 0.003985
  l1.weight: grad_norm = 0.000267
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.000807
Total gradient norm: 0.002195
=== Actor Training Debug (Iteration 1329) ===
Q mean: -36.025452
Q std: 18.000584
Actor loss: 36.029446
Action reg: 0.003992
  l1.weight: grad_norm = 0.000180
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.000474
Total gradient norm: 0.001337
=== Actor Training Debug (Iteration 1330) ===
Q mean: -39.044586
Q std: 19.896063
Actor loss: 39.048565
Action reg: 0.003978
  l1.weight: grad_norm = 0.000495
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.001345
Total gradient norm: 0.003663
=== Actor Training Debug (Iteration 1331) ===
Q mean: -40.022415
Q std: 18.819096
Actor loss: 40.026409
Action reg: 0.003995
  l1.weight: grad_norm = 0.000292
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.000784
Total gradient norm: 0.001897
=== Actor Training Debug (Iteration 1332) ===
Q mean: -36.774704
Q std: 19.067568
Actor loss: 36.778683
Action reg: 0.003980
  l1.weight: grad_norm = 0.000406
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.001139
Total gradient norm: 0.003062
=== Actor Training Debug (Iteration 1333) ===
Q mean: -38.235481
Q std: 20.192020
Actor loss: 38.239468
Action reg: 0.003986
  l1.weight: grad_norm = 0.000377
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.000937
Total gradient norm: 0.002370
=== Actor Training Debug (Iteration 1334) ===
Q mean: -41.004597
Q std: 20.033424
Actor loss: 41.008568
Action reg: 0.003971
  l1.weight: grad_norm = 0.000467
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.001549
Total gradient norm: 0.004246
=== Actor Training Debug (Iteration 1335) ===
Q mean: -40.899254
Q std: 19.868364
Actor loss: 40.903233
Action reg: 0.003978
  l1.weight: grad_norm = 0.000335
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.001077
Total gradient norm: 0.002889
=== Actor Training Debug (Iteration 1336) ===
Q mean: -38.962357
Q std: 19.414078
Actor loss: 38.966320
Action reg: 0.003964
  l1.weight: grad_norm = 0.000579
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.001920
Total gradient norm: 0.005471
=== Actor Training Debug (Iteration 1337) ===
Q mean: -33.302559
Q std: 18.638157
Actor loss: 33.306549
Action reg: 0.003992
  l1.weight: grad_norm = 0.000226
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.000542
Total gradient norm: 0.001307
=== Actor Training Debug (Iteration 1338) ===
Q mean: -38.334381
Q std: 21.148237
Actor loss: 38.338345
Action reg: 0.003964
  l1.weight: grad_norm = 0.000480
  l1.bias: grad_norm = 0.000405
  l2.weight: grad_norm = 0.001789
Total gradient norm: 0.005089
=== Actor Training Debug (Iteration 1339) ===
Q mean: -41.504616
Q std: 19.566763
Actor loss: 41.508602
Action reg: 0.003986
  l1.weight: grad_norm = 0.000285
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.000797
Total gradient norm: 0.001960
=== Actor Training Debug (Iteration 1340) ===
Q mean: -40.856094
Q std: 19.597651
Actor loss: 40.860096
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1341) ===
Q mean: -37.448357
Q std: 18.560648
Actor loss: 37.452347
Action reg: 0.003990
  l1.weight: grad_norm = 0.000413
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.000997
Total gradient norm: 0.002585
=== Actor Training Debug (Iteration 1342) ===
Q mean: -37.171505
Q std: 19.427185
Actor loss: 37.175495
Action reg: 0.003989
  l1.weight: grad_norm = 0.000332
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.000981
Total gradient norm: 0.002594
=== Actor Training Debug (Iteration 1343) ===
Q mean: -37.200211
Q std: 20.424593
Actor loss: 37.204197
Action reg: 0.003988
  l1.weight: grad_norm = 0.000307
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.000893
Total gradient norm: 0.002462
=== Actor Training Debug (Iteration 1344) ===
Q mean: -40.550125
Q std: 22.042664
Actor loss: 40.554096
Action reg: 0.003973
  l1.weight: grad_norm = 0.000317
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.001477
Total gradient norm: 0.004394
=== Actor Training Debug (Iteration 1345) ===
Q mean: -39.339581
Q std: 20.221905
Actor loss: 39.343552
Action reg: 0.003970
  l1.weight: grad_norm = 0.000382
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.001545
Total gradient norm: 0.004599
=== Actor Training Debug (Iteration 1346) ===
Q mean: -37.595184
Q std: 19.036947
Actor loss: 37.599178
Action reg: 0.003993
  l1.weight: grad_norm = 0.000168
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.000395
Total gradient norm: 0.000951
=== Actor Training Debug (Iteration 1347) ===
Q mean: -38.829308
Q std: 20.765444
Actor loss: 38.833302
Action reg: 0.003992
  l1.weight: grad_norm = 0.000169
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.000466
Total gradient norm: 0.001119
=== Actor Training Debug (Iteration 1348) ===
Q mean: -39.705814
Q std: 17.991858
Actor loss: 39.709793
Action reg: 0.003979
  l1.weight: grad_norm = 0.000334
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.001165
Total gradient norm: 0.003326
=== Actor Training Debug (Iteration 1349) ===
Q mean: -39.900963
Q std: 20.443056
Actor loss: 39.904922
Action reg: 0.003960
  l1.weight: grad_norm = 0.000557
  l1.bias: grad_norm = 0.000484
  l2.weight: grad_norm = 0.002392
Total gradient norm: 0.007181
=== Actor Training Debug (Iteration 1350) ===
Q mean: -39.143837
Q std: 19.447866
Actor loss: 39.147831
Action reg: 0.003992
  l1.weight: grad_norm = 0.000228
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.000469
Total gradient norm: 0.001115
=== Actor Training Debug (Iteration 1351) ===
Q mean: -39.289017
Q std: 21.514837
Actor loss: 39.293003
Action reg: 0.003986
  l1.weight: grad_norm = 0.000383
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.001004
Total gradient norm: 0.002633
=== Actor Training Debug (Iteration 1352) ===
Q mean: -37.106361
Q std: 20.455996
Actor loss: 37.110355
Action reg: 0.003995
  l1.weight: grad_norm = 0.000241
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.000553
Total gradient norm: 0.001381
=== Actor Training Debug (Iteration 1353) ===
Q mean: -39.604362
Q std: 21.624847
Actor loss: 39.608326
Action reg: 0.003962
  l1.weight: grad_norm = 0.000636
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.002113
Total gradient norm: 0.005781
=== Actor Training Debug (Iteration 1354) ===
Q mean: -41.400764
Q std: 19.040401
Actor loss: 41.404755
Action reg: 0.003992
  l1.weight: grad_norm = 0.000320
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.000635
Total gradient norm: 0.001304
=== Actor Training Debug (Iteration 1355) ===
Q mean: -38.627136
Q std: 20.729528
Actor loss: 38.631107
Action reg: 0.003973
  l1.weight: grad_norm = 0.000434
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.001771
Total gradient norm: 0.005387
=== Actor Training Debug (Iteration 1356) ===
Q mean: -39.946762
Q std: 19.856384
Actor loss: 39.950752
Action reg: 0.003991
  l1.weight: grad_norm = 0.000377
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.000806
Total gradient norm: 0.001910
=== Actor Training Debug (Iteration 1357) ===
Q mean: -39.643108
Q std: 20.447363
Actor loss: 39.647095
Action reg: 0.003986
  l1.weight: grad_norm = 0.000376
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.000976
Total gradient norm: 0.002568
=== Actor Training Debug (Iteration 1358) ===
Q mean: -37.908070
Q std: 18.684591
Actor loss: 37.912064
Action reg: 0.003995
  l1.weight: grad_norm = 0.000295
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.000795
Total gradient norm: 0.001928
=== Actor Training Debug (Iteration 1359) ===
Q mean: -39.989540
Q std: 19.694294
Actor loss: 39.993519
Action reg: 0.003980
  l1.weight: grad_norm = 0.000376
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.001190
Total gradient norm: 0.003178
=== Actor Training Debug (Iteration 1360) ===
Q mean: -37.574116
Q std: 19.937763
Actor loss: 37.578117
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1361) ===
Q mean: -40.309250
Q std: 19.725540
Actor loss: 40.313251
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1362) ===
Q mean: -40.880856
Q std: 20.467777
Actor loss: 40.884850
Action reg: 0.003994
  l1.weight: grad_norm = 0.000291
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.000662
Total gradient norm: 0.001593
=== Actor Training Debug (Iteration 1363) ===
Q mean: -36.575775
Q std: 19.729092
Actor loss: 36.579762
Action reg: 0.003988
  l1.weight: grad_norm = 0.000387
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.001113
Total gradient norm: 0.002933
=== Actor Training Debug (Iteration 1364) ===
Q mean: -36.626129
Q std: 19.793453
Actor loss: 36.630108
Action reg: 0.003979
  l1.weight: grad_norm = 0.000440
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.001183
Total gradient norm: 0.003075
=== Actor Training Debug (Iteration 1365) ===
Q mean: -42.204514
Q std: 21.119116
Actor loss: 42.208488
Action reg: 0.003974
  l1.weight: grad_norm = 0.000472
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.001654
Total gradient norm: 0.004654
=== Actor Training Debug (Iteration 1366) ===
Q mean: -39.917965
Q std: 20.744913
Actor loss: 39.921944
Action reg: 0.003978
  l1.weight: grad_norm = 0.000436
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.001401
Total gradient norm: 0.003895
=== Actor Training Debug (Iteration 1367) ===
Q mean: -39.020515
Q std: 19.742289
Actor loss: 39.024509
Action reg: 0.003993
  l1.weight: grad_norm = 0.000260
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.000602
Total gradient norm: 0.001463
=== Actor Training Debug (Iteration 1368) ===
Q mean: -38.881569
Q std: 20.610889
Actor loss: 38.885540
Action reg: 0.003972
  l1.weight: grad_norm = 0.000492
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.001648
Total gradient norm: 0.004600
=== Actor Training Debug (Iteration 1369) ===
Q mean: -37.779255
Q std: 21.245327
Actor loss: 37.783234
Action reg: 0.003980
  l1.weight: grad_norm = 0.000460
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.001315
Total gradient norm: 0.003540
=== Actor Training Debug (Iteration 1370) ===
Q mean: -40.534843
Q std: 19.270329
Actor loss: 40.538830
Action reg: 0.003985
  l1.weight: grad_norm = 0.000341
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.000977
Total gradient norm: 0.002791
=== Actor Training Debug (Iteration 1371) ===
Q mean: -39.059929
Q std: 19.891064
Actor loss: 39.063915
Action reg: 0.003986
  l1.weight: grad_norm = 0.000320
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.000913
Total gradient norm: 0.002490
=== Actor Training Debug (Iteration 1372) ===
Q mean: -36.859200
Q std: 20.345860
Actor loss: 36.863186
Action reg: 0.003987
  l1.weight: grad_norm = 0.000407
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.001052
Total gradient norm: 0.002821
=== Actor Training Debug (Iteration 1373) ===
Q mean: -38.260971
Q std: 20.304358
Actor loss: 38.264957
Action reg: 0.003986
  l1.weight: grad_norm = 0.000411
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.001118
Total gradient norm: 0.002821
=== Actor Training Debug (Iteration 1374) ===
Q mean: -40.043533
Q std: 18.613958
Actor loss: 40.047527
Action reg: 0.003996
  l1.weight: grad_norm = 0.000277
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.000749
Total gradient norm: 0.001821
=== Actor Training Debug (Iteration 1375) ===
Q mean: -39.330696
Q std: 18.292994
Actor loss: 39.334682
Action reg: 0.003986
  l1.weight: grad_norm = 0.000280
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.000853
Total gradient norm: 0.002302
=== Actor Training Debug (Iteration 1376) ===
Q mean: -37.036079
Q std: 19.770168
Actor loss: 37.040073
Action reg: 0.003993
  l1.weight: grad_norm = 0.000168
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.000428
Total gradient norm: 0.001025
=== Actor Training Debug (Iteration 1377) ===
Q mean: -40.987141
Q std: 19.280907
Actor loss: 40.991108
Action reg: 0.003969
  l1.weight: grad_norm = 0.000534
  l1.bias: grad_norm = 0.000454
  l2.weight: grad_norm = 0.001715
Total gradient norm: 0.004601
=== Actor Training Debug (Iteration 1378) ===
Q mean: -40.074951
Q std: 22.595730
Actor loss: 40.078930
Action reg: 0.003980
  l1.weight: grad_norm = 0.000422
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.001286
Total gradient norm: 0.003475
=== Actor Training Debug (Iteration 1379) ===
Q mean: -39.637047
Q std: 19.925739
Actor loss: 39.641033
Action reg: 0.003987
  l1.weight: grad_norm = 0.000273
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.000791
Total gradient norm: 0.002167
=== Actor Training Debug (Iteration 1380) ===
Q mean: -37.321510
Q std: 20.764137
Actor loss: 37.325497
Action reg: 0.003986
  l1.weight: grad_norm = 0.000352
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.000867
Total gradient norm: 0.002238
=== Actor Training Debug (Iteration 1381) ===
Q mean: -37.016243
Q std: 19.423338
Actor loss: 37.020245
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1382) ===
Q mean: -41.684872
Q std: 21.885410
Actor loss: 41.688835
Action reg: 0.003965
  l1.weight: grad_norm = 0.000618
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.002211
Total gradient norm: 0.006580
=== Actor Training Debug (Iteration 1383) ===
Q mean: -39.066872
Q std: 19.224316
Actor loss: 39.070847
Action reg: 0.003974
  l1.weight: grad_norm = 0.000385
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.001554
Total gradient norm: 0.004562
=== Actor Training Debug (Iteration 1384) ===
Q mean: -35.051048
Q std: 19.717886
Actor loss: 35.055023
Action reg: 0.003975
  l1.weight: grad_norm = 0.000682
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.001902
Total gradient norm: 0.005223
=== Actor Training Debug (Iteration 1385) ===
Q mean: -38.306664
Q std: 19.407024
Actor loss: 38.310658
Action reg: 0.003994
  l1.weight: grad_norm = 0.000273
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.000592
Total gradient norm: 0.001313
=== Actor Training Debug (Iteration 1386) ===
Q mean: -40.187012
Q std: 19.810997
Actor loss: 40.191002
Action reg: 0.003991
  l1.weight: grad_norm = 0.000373
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.000805
Total gradient norm: 0.001910
=== Actor Training Debug (Iteration 1387) ===
Q mean: -43.318249
Q std: 22.008192
Actor loss: 43.322220
Action reg: 0.003971
  l1.weight: grad_norm = 0.000479
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.001557
Total gradient norm: 0.004320
=== Actor Training Debug (Iteration 1388) ===
Q mean: -41.408791
Q std: 20.124725
Actor loss: 41.412773
Action reg: 0.003984
  l1.weight: grad_norm = 0.000263
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.000863
Total gradient norm: 0.002514
=== Actor Training Debug (Iteration 1389) ===
Q mean: -36.643883
Q std: 19.947680
Actor loss: 36.647854
Action reg: 0.003970
  l1.weight: grad_norm = 0.000579
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.001740
Total gradient norm: 0.004798
=== Actor Training Debug (Iteration 1390) ===
Q mean: -37.284054
Q std: 20.056927
Actor loss: 37.288036
Action reg: 0.003983
  l1.weight: grad_norm = 0.000396
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.001222
Total gradient norm: 0.003413
=== Actor Training Debug (Iteration 1391) ===
Q mean: -39.306335
Q std: 19.233088
Actor loss: 39.310329
Action reg: 0.003993
  l1.weight: grad_norm = 0.000185
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.000517
Total gradient norm: 0.001375
=== Actor Training Debug (Iteration 1392) ===
Q mean: -42.376633
Q std: 21.209120
Actor loss: 42.380611
Action reg: 0.003980
  l1.weight: grad_norm = 0.000278
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.000958
Total gradient norm: 0.002574
=== Actor Training Debug (Iteration 1393) ===
Q mean: -39.135208
Q std: 19.853409
Actor loss: 39.139202
Action reg: 0.003994
  l1.weight: grad_norm = 0.000160
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.000383
Total gradient norm: 0.000984
=== Actor Training Debug (Iteration 1394) ===
Q mean: -39.164570
Q std: 19.494135
Actor loss: 39.168571
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1395) ===
Q mean: -42.901848
Q std: 21.350128
Actor loss: 42.905834
Action reg: 0.003986
  l1.weight: grad_norm = 0.000394
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.000997
Total gradient norm: 0.002591
=== Actor Training Debug (Iteration 1396) ===
Q mean: -40.563774
Q std: 22.239143
Actor loss: 40.567753
Action reg: 0.003980
  l1.weight: grad_norm = 0.000405
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.001166
Total gradient norm: 0.003243
=== Actor Training Debug (Iteration 1397) ===
Q mean: -38.740501
Q std: 18.893574
Actor loss: 38.744503
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1398) ===
Q mean: -36.784348
Q std: 19.833109
Actor loss: 36.788338
Action reg: 0.003989
  l1.weight: grad_norm = 0.000261
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.000698
Total gradient norm: 0.001880
=== Actor Training Debug (Iteration 1399) ===
Q mean: -38.378773
Q std: 21.181063
Actor loss: 38.382751
Action reg: 0.003980
  l1.weight: grad_norm = 0.000316
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.001106
Total gradient norm: 0.003087
=== Actor Training Debug (Iteration 1400) ===
Q mean: -41.663120
Q std: 22.528101
Actor loss: 41.667114
Action reg: 0.003993
  l1.weight: grad_norm = 0.000245
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.000566
Total gradient norm: 0.001317
=== Actor Training Debug (Iteration 1401) ===
Q mean: -39.424988
Q std: 21.202150
Actor loss: 39.428940
Action reg: 0.003950
  l1.weight: grad_norm = 0.000628
  l1.bias: grad_norm = 0.000581
  l2.weight: grad_norm = 0.002464
Total gradient norm: 0.007340
=== Actor Training Debug (Iteration 1402) ===
Q mean: -38.158142
Q std: 20.635935
Actor loss: 38.162132
Action reg: 0.003992
  l1.weight: grad_norm = 0.000296
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.000626
Total gradient norm: 0.001561
=== Actor Training Debug (Iteration 1403) ===
Q mean: -37.000893
Q std: 19.032265
Actor loss: 37.004879
Action reg: 0.003986
  l1.weight: grad_norm = 0.000324
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.000830
Total gradient norm: 0.002163
=== Actor Training Debug (Iteration 1404) ===
Q mean: -38.593731
Q std: 19.152529
Actor loss: 38.597733
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1405) ===
Q mean: -40.933128
Q std: 20.454376
Actor loss: 40.937115
Action reg: 0.003985
  l1.weight: grad_norm = 0.000396
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.001188
Total gradient norm: 0.003155
=== Actor Training Debug (Iteration 1406) ===
Q mean: -42.782528
Q std: 20.169504
Actor loss: 42.786518
Action reg: 0.003989
  l1.weight: grad_norm = 0.000305
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.000861
Total gradient norm: 0.002498
=== Actor Training Debug (Iteration 1407) ===
Q mean: -36.548409
Q std: 17.478228
Actor loss: 36.552410
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1408) ===
Q mean: -38.065628
Q std: 19.734756
Actor loss: 38.069618
Action reg: 0.003988
  l1.weight: grad_norm = 0.000255
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.000770
Total gradient norm: 0.002127
=== Actor Training Debug (Iteration 1409) ===
Q mean: -39.750702
Q std: 18.729893
Actor loss: 39.754684
Action reg: 0.003984
  l1.weight: grad_norm = 0.000498
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.001231
Total gradient norm: 0.002851
=== Actor Training Debug (Iteration 1410) ===
Q mean: -41.802734
Q std: 21.672613
Actor loss: 41.806705
Action reg: 0.003971
  l1.weight: grad_norm = 0.000470
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.001581
Total gradient norm: 0.004485
=== Actor Training Debug (Iteration 1411) ===
Q mean: -41.444714
Q std: 22.039238
Actor loss: 41.448689
Action reg: 0.003977
  l1.weight: grad_norm = 0.000481
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.001672
Total gradient norm: 0.004878
=== Actor Training Debug (Iteration 1412) ===
Q mean: -37.008877
Q std: 19.326561
Actor loss: 37.012863
Action reg: 0.003988
  l1.weight: grad_norm = 0.000363
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.000868
Total gradient norm: 0.002281
=== Actor Training Debug (Iteration 1413) ===
Q mean: -37.331863
Q std: 18.385120
Actor loss: 37.335838
Action reg: 0.003977
  l1.weight: grad_norm = 0.000435
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.001302
Total gradient norm: 0.003540
=== Actor Training Debug (Iteration 1414) ===
Q mean: -38.906357
Q std: 19.909576
Actor loss: 38.910347
Action reg: 0.003988
  l1.weight: grad_norm = 0.000306
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.000806
Total gradient norm: 0.002182
=== Actor Training Debug (Iteration 1415) ===
Q mean: -41.399124
Q std: 20.692652
Actor loss: 41.403111
Action reg: 0.003987
  l1.weight: grad_norm = 0.000367
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.000930
Total gradient norm: 0.002465
=== Actor Training Debug (Iteration 1416) ===
Q mean: -43.011986
Q std: 21.494375
Actor loss: 43.015968
Action reg: 0.003984
  l1.weight: grad_norm = 0.000485
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.001353
Total gradient norm: 0.003359
=== Actor Training Debug (Iteration 1417) ===
Q mean: -38.314301
Q std: 20.056919
Actor loss: 38.318295
Action reg: 0.003993
  l1.weight: grad_norm = 0.000241
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.000559
Total gradient norm: 0.001439
=== Actor Training Debug (Iteration 1418) ===
Q mean: -37.554817
Q std: 19.781012
Actor loss: 37.558811
Action reg: 0.003994
  l1.weight: grad_norm = 0.000199
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.000495
Total gradient norm: 0.001177
=== Actor Training Debug (Iteration 1419) ===
Q mean: -39.656303
Q std: 19.673693
Actor loss: 39.660282
Action reg: 0.003979
  l1.weight: grad_norm = 0.000351
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.001009
Total gradient norm: 0.002659
=== Actor Training Debug (Iteration 1420) ===
Q mean: -42.261299
Q std: 21.539515
Actor loss: 42.265289
Action reg: 0.003989
  l1.weight: grad_norm = 0.000294
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.000801
Total gradient norm: 0.002258
=== Actor Training Debug (Iteration 1421) ===
Q mean: -38.311058
Q std: 19.817200
Actor loss: 38.315037
Action reg: 0.003980
  l1.weight: grad_norm = 0.000389
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.001162
Total gradient norm: 0.003205
=== Actor Training Debug (Iteration 1422) ===
Q mean: -41.592049
Q std: 20.794609
Actor loss: 41.596024
Action reg: 0.003974
  l1.weight: grad_norm = 0.000435
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.001430
Total gradient norm: 0.003962
=== Actor Training Debug (Iteration 1423) ===
Q mean: -39.648808
Q std: 19.718891
Actor loss: 39.652790
Action reg: 0.003983
  l1.weight: grad_norm = 0.000325
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.001102
Total gradient norm: 0.003178
=== Actor Training Debug (Iteration 1424) ===
Q mean: -38.723221
Q std: 21.385651
Actor loss: 38.727207
Action reg: 0.003988
  l1.weight: grad_norm = 0.000295
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.000780
Total gradient norm: 0.001952
=== Actor Training Debug (Iteration 1425) ===
Q mean: -41.042557
Q std: 21.146357
Actor loss: 41.046535
Action reg: 0.003978
  l1.weight: grad_norm = 0.000460
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.001263
Total gradient norm: 0.003220
=== Actor Training Debug (Iteration 1426) ===
Q mean: -41.272385
Q std: 19.776150
Actor loss: 41.276363
Action reg: 0.003980
  l1.weight: grad_norm = 0.000379
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.001210
Total gradient norm: 0.003527
=== Actor Training Debug (Iteration 1427) ===
Q mean: -40.471840
Q std: 19.452097
Actor loss: 40.475830
Action reg: 0.003988
  l1.weight: grad_norm = 0.000288
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.000845
Total gradient norm: 0.002445
=== Actor Training Debug (Iteration 1428) ===
Q mean: -37.584370
Q std: 19.681828
Actor loss: 37.588337
Action reg: 0.003966
  l1.weight: grad_norm = 0.000527
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.001847
Total gradient norm: 0.005305
=== Actor Training Debug (Iteration 1429) ===
Q mean: -37.772678
Q std: 19.017328
Actor loss: 37.776665
Action reg: 0.003985
  l1.weight: grad_norm = 0.000346
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.000927
Total gradient norm: 0.002480
=== Actor Training Debug (Iteration 1430) ===
Q mean: -42.797691
Q std: 20.533491
Actor loss: 42.801693
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1431) ===
Q mean: -40.161377
Q std: 21.750702
Actor loss: 40.165379
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1432) ===
Q mean: -39.009792
Q std: 22.014711
Actor loss: 39.013775
Action reg: 0.003984
  l1.weight: grad_norm = 0.000375
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.001119
Total gradient norm: 0.003234
=== Actor Training Debug (Iteration 1433) ===
Q mean: -38.203712
Q std: 21.232319
Actor loss: 38.207695
Action reg: 0.003981
  l1.weight: grad_norm = 0.000355
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.001203
Total gradient norm: 0.003441
=== Actor Training Debug (Iteration 1434) ===
Q mean: -40.315468
Q std: 22.248173
Actor loss: 40.319462
Action reg: 0.003995
  l1.weight: grad_norm = 0.000215
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.000596
Total gradient norm: 0.001454
=== Actor Training Debug (Iteration 1435) ===
Q mean: -40.388897
Q std: 21.041805
Actor loss: 40.392891
Action reg: 0.003992
  l1.weight: grad_norm = 0.000250
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.000589
Total gradient norm: 0.001381
=== Actor Training Debug (Iteration 1436) ===
Q mean: -37.177834
Q std: 20.574930
Actor loss: 37.181820
Action reg: 0.003986
  l1.weight: grad_norm = 0.000343
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.000990
Total gradient norm: 0.002728
=== Actor Training Debug (Iteration 1437) ===
Q mean: -40.368763
Q std: 20.432055
Actor loss: 40.372742
Action reg: 0.003980
  l1.weight: grad_norm = 0.000346
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.001014
Total gradient norm: 0.002780
=== Actor Training Debug (Iteration 1438) ===
Q mean: -41.535793
Q std: 19.807266
Actor loss: 41.539787
Action reg: 0.003993
  l1.weight: grad_norm = 0.000152
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.000464
Total gradient norm: 0.001146
=== Actor Training Debug (Iteration 1439) ===
Q mean: -38.628960
Q std: 18.642023
Actor loss: 38.632938
Action reg: 0.003977
  l1.weight: grad_norm = 0.000362
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.001167
Total gradient norm: 0.003416
=== Actor Training Debug (Iteration 1440) ===
Q mean: -38.319069
Q std: 21.036293
Actor loss: 38.323063
Action reg: 0.003995
  l1.weight: grad_norm = 0.000206
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.000603
Total gradient norm: 0.001494
=== Actor Training Debug (Iteration 1441) ===
Q mean: -40.097908
Q std: 20.735439
Actor loss: 40.101887
Action reg: 0.003978
  l1.weight: grad_norm = 0.000425
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.001190
Total gradient norm: 0.003031
=== Actor Training Debug (Iteration 1442) ===
Q mean: -40.605003
Q std: 22.601923
Actor loss: 40.608967
Action reg: 0.003962
  l1.weight: grad_norm = 0.000506
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.002116
Total gradient norm: 0.006443
=== Actor Training Debug (Iteration 1443) ===
Q mean: -39.337879
Q std: 18.062477
Actor loss: 39.341866
Action reg: 0.003986
  l1.weight: grad_norm = 0.000376
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.001003
Total gradient norm: 0.002726
=== Actor Training Debug (Iteration 1444) ===
Q mean: -43.880852
Q std: 22.202738
Actor loss: 43.884838
Action reg: 0.003986
  l1.weight: grad_norm = 0.000259
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.000781
Total gradient norm: 0.002004
=== Actor Training Debug (Iteration 1445) ===
Q mean: -43.053940
Q std: 21.203348
Actor loss: 43.057926
Action reg: 0.003987
  l1.weight: grad_norm = 0.000311
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.000821
Total gradient norm: 0.002179
=== Actor Training Debug (Iteration 1446) ===
Q mean: -36.091965
Q std: 21.535419
Actor loss: 36.095928
Action reg: 0.003963
  l1.weight: grad_norm = 0.000628
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.002405
Total gradient norm: 0.007203
=== Actor Training Debug (Iteration 1447) ===
Q mean: -37.126083
Q std: 18.846104
Actor loss: 37.130077
Action reg: 0.003993
  l1.weight: grad_norm = 0.000221
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.000498
Total gradient norm: 0.001165
=== Actor Training Debug (Iteration 1448) ===
Q mean: -37.591824
Q std: 20.280886
Actor loss: 37.595779
Action reg: 0.003955
  l1.weight: grad_norm = 0.000785
  l1.bias: grad_norm = 0.000577
  l2.weight: grad_norm = 0.002683
Total gradient norm: 0.008134
=== Actor Training Debug (Iteration 1449) ===
Q mean: -41.409107
Q std: 21.639791
Actor loss: 41.413101
Action reg: 0.003992
  l1.weight: grad_norm = 0.000250
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.000593
Total gradient norm: 0.001448
=== Actor Training Debug (Iteration 1450) ===
Q mean: -43.720345
Q std: 20.359274
Actor loss: 43.724327
Action reg: 0.003981
  l1.weight: grad_norm = 0.000336
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.001116
Total gradient norm: 0.003283
=== Actor Training Debug (Iteration 1451) ===
Q mean: -40.729164
Q std: 21.597397
Actor loss: 40.733124
Action reg: 0.003960
  l1.weight: grad_norm = 0.000476
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.001957
Total gradient norm: 0.005957
=== Actor Training Debug (Iteration 1452) ===
Q mean: -35.824986
Q std: 19.574175
Actor loss: 35.828953
Action reg: 0.003968
  l1.weight: grad_norm = 0.000405
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.001740
Total gradient norm: 0.005243
=== Actor Training Debug (Iteration 1453) ===
Q mean: -35.762421
Q std: 18.456465
Actor loss: 35.766399
Action reg: 0.003980
  l1.weight: grad_norm = 0.000413
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.001195
Total gradient norm: 0.003355
=== Actor Training Debug (Iteration 1454) ===
Q mean: -38.898941
Q std: 19.460129
Actor loss: 38.902908
Action reg: 0.003966
  l1.weight: grad_norm = 0.000428
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.001873
Total gradient norm: 0.005616
=== Actor Training Debug (Iteration 1455) ===
Q mean: -42.760017
Q std: 20.626072
Actor loss: 42.764004
Action reg: 0.003988
  l1.weight: grad_norm = 0.000289
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.000844
Total gradient norm: 0.002377
=== Actor Training Debug (Iteration 1456) ===
Q mean: -43.258236
Q std: 19.261620
Actor loss: 43.262230
Action reg: 0.003993
  l1.weight: grad_norm = 0.000207
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.000551
Total gradient norm: 0.001316
=== Actor Training Debug (Iteration 1457) ===
Q mean: -40.065903
Q std: 19.658739
Actor loss: 40.069897
Action reg: 0.003993
  l1.weight: grad_norm = 0.000228
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.000510
Total gradient norm: 0.001217
=== Actor Training Debug (Iteration 1458) ===
Q mean: -39.086819
Q std: 20.503759
Actor loss: 39.090809
Action reg: 0.003991
  l1.weight: grad_norm = 0.000364
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.001008
Total gradient norm: 0.002744
=== Actor Training Debug (Iteration 1459) ===
Q mean: -39.550758
Q std: 21.172485
Actor loss: 39.554760
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1460) ===
Q mean: -42.623314
Q std: 22.094265
Actor loss: 42.627296
Action reg: 0.003981
  l1.weight: grad_norm = 0.000348
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.001186
Total gradient norm: 0.003574
=== Actor Training Debug (Iteration 1461) ===
Q mean: -45.051235
Q std: 22.485250
Actor loss: 45.055206
Action reg: 0.003973
  l1.weight: grad_norm = 0.000461
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.001516
Total gradient norm: 0.004458
=== Actor Training Debug (Iteration 1462) ===
Q mean: -38.236744
Q std: 21.535904
Actor loss: 38.240719
Action reg: 0.003974
  l1.weight: grad_norm = 0.000359
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.001445
Total gradient norm: 0.004398
=== Actor Training Debug (Iteration 1463) ===
Q mean: -39.674400
Q std: 20.428225
Actor loss: 39.678387
Action reg: 0.003985
  l1.weight: grad_norm = 0.000286
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.000980
Total gradient norm: 0.002929
=== Actor Training Debug (Iteration 1464) ===
Q mean: -38.743675
Q std: 19.808170
Actor loss: 38.747658
Action reg: 0.003982
  l1.weight: grad_norm = 0.000352
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.001113
Total gradient norm: 0.003161
=== Actor Training Debug (Iteration 1465) ===
Q mean: -43.450294
Q std: 20.096479
Actor loss: 43.454273
Action reg: 0.003979
  l1.weight: grad_norm = 0.000338
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.001058
Total gradient norm: 0.003060
=== Actor Training Debug (Iteration 1466) ===
Q mean: -44.604034
Q std: 21.296871
Actor loss: 44.608036
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1467) ===
Q mean: -39.558693
Q std: 19.466879
Actor loss: 39.562664
Action reg: 0.003973
  l1.weight: grad_norm = 0.000481
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.001741
Total gradient norm: 0.005093
=== Actor Training Debug (Iteration 1468) ===
Q mean: -38.191174
Q std: 20.261509
Actor loss: 38.195148
Action reg: 0.003974
  l1.weight: grad_norm = 0.000488
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.001427
Total gradient norm: 0.003909
=== Actor Training Debug (Iteration 1469) ===
Q mean: -43.150940
Q std: 21.395517
Actor loss: 43.154915
Action reg: 0.003977
  l1.weight: grad_norm = 0.000590
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.001847
Total gradient norm: 0.005245
=== Actor Training Debug (Iteration 1470) ===
Q mean: -43.373306
Q std: 21.711809
Actor loss: 43.377270
Action reg: 0.003962
  l1.weight: grad_norm = 0.000562
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.002144
Total gradient norm: 0.006511
=== Actor Training Debug (Iteration 1471) ===
Q mean: -39.951889
Q std: 21.807224
Actor loss: 39.955872
Action reg: 0.003981
  l1.weight: grad_norm = 0.000421
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.001297
Total gradient norm: 0.003711
=== Actor Training Debug (Iteration 1472) ===
Q mean: -41.020020
Q std: 21.498919
Actor loss: 41.024014
Action reg: 0.003994
  l1.weight: grad_norm = 0.000182
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.000513
Total gradient norm: 0.001350
=== Actor Training Debug (Iteration 1473) ===
Q mean: -36.163338
Q std: 19.065119
Actor loss: 36.167309
Action reg: 0.003973
  l1.weight: grad_norm = 0.000527
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.001640
Total gradient norm: 0.004535
=== Actor Training Debug (Iteration 1474) ===
Q mean: -41.175598
Q std: 19.597097
Actor loss: 41.179592
Action reg: 0.003993
  l1.weight: grad_norm = 0.000219
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.000497
Total gradient norm: 0.001177
=== Actor Training Debug (Iteration 1475) ===
Q mean: -38.262268
Q std: 20.561453
Actor loss: 38.266262
Action reg: 0.003993
  l1.weight: grad_norm = 0.000194
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.000480
Total gradient norm: 0.001193
=== Actor Training Debug (Iteration 1476) ===
Q mean: -39.507381
Q std: 20.501987
Actor loss: 39.511368
Action reg: 0.003986
  l1.weight: grad_norm = 0.000287
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.000863
Total gradient norm: 0.002444
=== Actor Training Debug (Iteration 1477) ===
Q mean: -40.855469
Q std: 18.873404
Actor loss: 40.859463
Action reg: 0.003994
  l1.weight: grad_norm = 0.000131
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.000376
Total gradient norm: 0.001037
=== Actor Training Debug (Iteration 1478) ===
Q mean: -40.365906
Q std: 21.484440
Actor loss: 40.369865
Action reg: 0.003960
  l1.weight: grad_norm = 0.000627
  l1.bias: grad_norm = 0.000578
  l2.weight: grad_norm = 0.002614
Total gradient norm: 0.008136
=== Actor Training Debug (Iteration 1479) ===
Q mean: -39.507153
Q std: 19.433828
Actor loss: 39.511143
Action reg: 0.003989
  l1.weight: grad_norm = 0.000275
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.000891
Total gradient norm: 0.002524
=== Actor Training Debug (Iteration 1480) ===
Q mean: -40.436523
Q std: 20.181290
Actor loss: 40.440510
Action reg: 0.003985
  l1.weight: grad_norm = 0.000353
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.000893
Total gradient norm: 0.002357
=== Actor Training Debug (Iteration 1481) ===
Q mean: -42.587837
Q std: 20.853645
Actor loss: 42.591820
Action reg: 0.003981
  l1.weight: grad_norm = 0.000380
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.001337
Total gradient norm: 0.003947
=== Actor Training Debug (Iteration 1482) ===
Q mean: -41.166714
Q std: 20.656031
Actor loss: 41.170704
Action reg: 0.003990
  l1.weight: grad_norm = 0.000321
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.000835
Total gradient norm: 0.002359
=== Actor Training Debug (Iteration 1483) ===
Q mean: -37.970074
Q std: 18.690069
Actor loss: 37.974060
Action reg: 0.003988
  l1.weight: grad_norm = 0.000409
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.001159
Total gradient norm: 0.003020
=== Actor Training Debug (Iteration 1484) ===
Q mean: -39.219326
Q std: 20.525961
Actor loss: 39.223309
Action reg: 0.003982
  l1.weight: grad_norm = 0.000343
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.001208
Total gradient norm: 0.003600
=== Actor Training Debug (Iteration 1485) ===
Q mean: -41.782440
Q std: 20.787682
Actor loss: 41.786427
Action reg: 0.003985
  l1.weight: grad_norm = 0.000409
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.001230
Total gradient norm: 0.003645
=== Actor Training Debug (Iteration 1486) ===
Q mean: -41.833015
Q std: 20.813568
Actor loss: 41.837009
Action reg: 0.003993
  l1.weight: grad_norm = 0.000279
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.000589
Total gradient norm: 0.001450
=== Actor Training Debug (Iteration 1487) ===
Q mean: -40.868187
Q std: 20.155210
Actor loss: 40.872181
Action reg: 0.003994
  l1.weight: grad_norm = 0.000135
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.000389
Total gradient norm: 0.001073
=== Actor Training Debug (Iteration 1488) ===
Q mean: -39.975594
Q std: 22.083704
Actor loss: 39.979561
Action reg: 0.003968
  l1.weight: grad_norm = 0.000481
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.001749
Total gradient norm: 0.005254
=== Actor Training Debug (Iteration 1489) ===
Q mean: -40.260468
Q std: 20.734608
Actor loss: 40.264450
Action reg: 0.003981
  l1.weight: grad_norm = 0.000434
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.001362
Total gradient norm: 0.003856
=== Actor Training Debug (Iteration 1490) ===
Q mean: -40.453186
Q std: 22.752539
Actor loss: 40.457161
Action reg: 0.003975
  l1.weight: grad_norm = 0.000415
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.001426
Total gradient norm: 0.004098
=== Actor Training Debug (Iteration 1491) ===
Q mean: -39.805302
Q std: 19.433043
Actor loss: 39.809296
Action reg: 0.003994
  l1.weight: grad_norm = 0.000189
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.000433
Total gradient norm: 0.001073
=== Actor Training Debug (Iteration 1492) ===
Q mean: -38.603077
Q std: 21.117363
Actor loss: 38.607037
Action reg: 0.003959
  l1.weight: grad_norm = 0.000461
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.002073
Total gradient norm: 0.006405
=== Actor Training Debug (Iteration 1493) ===
Q mean: -42.726547
Q std: 19.559238
Actor loss: 42.730534
Action reg: 0.003988
  l1.weight: grad_norm = 0.000260
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.000672
Total gradient norm: 0.001824
=== Actor Training Debug (Iteration 1494) ===
Q mean: -40.465714
Q std: 19.630505
Actor loss: 40.469715
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1495) ===
Q mean: -37.568356
Q std: 20.209009
Actor loss: 37.572342
Action reg: 0.003987
  l1.weight: grad_norm = 0.000294
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.000863
Total gradient norm: 0.002482
=== Actor Training Debug (Iteration 1496) ===
Q mean: -39.241016
Q std: 20.945057
Actor loss: 39.245010
Action reg: 0.003994
  l1.weight: grad_norm = 0.000230
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.000542
Total gradient norm: 0.001426
=== Actor Training Debug (Iteration 1497) ===
Q mean: -43.697556
Q std: 19.791918
Actor loss: 43.701550
Action reg: 0.003993
  l1.weight: grad_norm = 0.000238
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.000554
Total gradient norm: 0.001343
=== Actor Training Debug (Iteration 1498) ===
Q mean: -39.754639
Q std: 21.120602
Actor loss: 39.758614
Action reg: 0.003974
  l1.weight: grad_norm = 0.000530
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.001734
Total gradient norm: 0.004823
=== Actor Training Debug (Iteration 1499) ===
Q mean: -42.060753
Q std: 21.347120
Actor loss: 42.064728
Action reg: 0.003976
  l1.weight: grad_norm = 0.000378
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.001391
Total gradient norm: 0.004254
=== Actor Training Debug (Iteration 1500) ===
Q mean: -40.310158
Q std: 21.986673
Actor loss: 40.314137
Action reg: 0.003977
  l1.weight: grad_norm = 0.000425
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.001573
Total gradient norm: 0.004750
  Average reward: -361.618 | Average length: 100.0
Evaluation at episode 65: -361.618
=== Actor Training Debug (Iteration 1501) ===
Q mean: -41.895523
Q std: 22.566334
Actor loss: 41.899506
Action reg: 0.003982
  l1.weight: grad_norm = 0.000346
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.001150
Total gradient norm: 0.003343
=== Actor Training Debug (Iteration 1502) ===
Q mean: -40.631687
Q std: 20.460251
Actor loss: 40.635670
Action reg: 0.003981
  l1.weight: grad_norm = 0.000372
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.001021
Total gradient norm: 0.002849
=== Actor Training Debug (Iteration 1503) ===
Q mean: -40.999374
Q std: 20.900961
Actor loss: 41.003342
Action reg: 0.003969
  l1.weight: grad_norm = 0.000400
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.001679
Total gradient norm: 0.005214
=== Actor Training Debug (Iteration 1504) ===
Q mean: -36.671066
Q std: 20.302197
Actor loss: 36.675060
Action reg: 0.003994
  l1.weight: grad_norm = 0.000225
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.000568
Total gradient norm: 0.001467
=== Actor Training Debug (Iteration 1505) ===
Q mean: -37.504082
Q std: 17.802933
Actor loss: 37.508076
Action reg: 0.003993
  l1.weight: grad_norm = 0.000254
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.000508
Total gradient norm: 0.001204
=== Actor Training Debug (Iteration 1506) ===
Q mean: -40.191681
Q std: 19.574835
Actor loss: 40.195667
Action reg: 0.003988
  l1.weight: grad_norm = 0.000240
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.000670
Total gradient norm: 0.001877
=== Actor Training Debug (Iteration 1507) ===
Q mean: -44.794670
Q std: 20.439531
Actor loss: 44.798656
Action reg: 0.003988
  l1.weight: grad_norm = 0.000259
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.000671
Total gradient norm: 0.001705
=== Actor Training Debug (Iteration 1508) ===
Q mean: -43.228146
Q std: 21.288023
Actor loss: 43.232136
Action reg: 0.003989
  l1.weight: grad_norm = 0.000293
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.000803
Total gradient norm: 0.002161
=== Actor Training Debug (Iteration 1509) ===
Q mean: -38.023899
Q std: 19.740932
Actor loss: 38.027889
Action reg: 0.003989
  l1.weight: grad_norm = 0.000321
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.000794
Total gradient norm: 0.002209
=== Actor Training Debug (Iteration 1510) ===
Q mean: -35.655380
Q std: 19.059881
Actor loss: 35.659359
Action reg: 0.003980
  l1.weight: grad_norm = 0.000322
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.001036
Total gradient norm: 0.002884
=== Actor Training Debug (Iteration 1511) ===
Q mean: -36.282162
Q std: 18.936808
Actor loss: 36.286140
Action reg: 0.003980
  l1.weight: grad_norm = 0.000399
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.001282
Total gradient norm: 0.003529
=== Actor Training Debug (Iteration 1512) ===
Q mean: -38.865715
Q std: 21.323977
Actor loss: 38.869690
Action reg: 0.003976
  l1.weight: grad_norm = 0.000343
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.001216
Total gradient norm: 0.003547
=== Actor Training Debug (Iteration 1513) ===
Q mean: -45.890816
Q std: 20.802996
Actor loss: 45.894810
Action reg: 0.003993
  l1.weight: grad_norm = 0.000238
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.000503
Total gradient norm: 0.001090
=== Actor Training Debug (Iteration 1514) ===
Q mean: -45.048973
Q std: 20.930702
Actor loss: 45.052975
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1515) ===
Q mean: -42.681465
Q std: 22.523914
Actor loss: 42.685444
Action reg: 0.003980
  l1.weight: grad_norm = 0.000276
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.001028
Total gradient norm: 0.003158
=== Actor Training Debug (Iteration 1516) ===
Q mean: -40.513275
Q std: 20.808840
Actor loss: 40.517239
Action reg: 0.003962
  l1.weight: grad_norm = 0.000504
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.001954
Total gradient norm: 0.006054
=== Actor Training Debug (Iteration 1517) ===
Q mean: -38.542946
Q std: 20.224844
Actor loss: 38.546917
Action reg: 0.003971
  l1.weight: grad_norm = 0.000442
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.001523
Total gradient norm: 0.004558
=== Actor Training Debug (Iteration 1518) ===
Q mean: -40.296410
Q std: 23.100122
Actor loss: 40.300396
Action reg: 0.003987
  l1.weight: grad_norm = 0.000355
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.000933
Total gradient norm: 0.002458
=== Actor Training Debug (Iteration 1519) ===
Q mean: -41.883892
Q std: 22.849560
Actor loss: 41.887878
Action reg: 0.003987
  l1.weight: grad_norm = 0.000277
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.000788
Total gradient norm: 0.002227
=== Actor Training Debug (Iteration 1520) ===
Q mean: -41.563736
Q std: 18.247599
Actor loss: 41.567726
Action reg: 0.003989
  l1.weight: grad_norm = 0.000288
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.000865
Total gradient norm: 0.002452
=== Actor Training Debug (Iteration 1521) ===
Q mean: -37.618507
Q std: 20.200020
Actor loss: 37.622509
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1522) ===
Q mean: -38.724297
Q std: 19.583458
Actor loss: 38.728287
Action reg: 0.003991
  l1.weight: grad_norm = 0.000370
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.001020
Total gradient norm: 0.002668
=== Actor Training Debug (Iteration 1523) ===
Q mean: -39.886879
Q std: 20.072088
Actor loss: 39.890869
Action reg: 0.003989
  l1.weight: grad_norm = 0.000297
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.000829
Total gradient norm: 0.002262
=== Actor Training Debug (Iteration 1524) ===
Q mean: -41.596088
Q std: 20.800039
Actor loss: 41.600082
Action reg: 0.003993
  l1.weight: grad_norm = 0.000270
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.000574
Total gradient norm: 0.001428
=== Actor Training Debug (Iteration 1525) ===
Q mean: -41.113487
Q std: 23.236126
Actor loss: 41.117462
Action reg: 0.003975
  l1.weight: grad_norm = 0.000489
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.001590
Total gradient norm: 0.004619
=== Actor Training Debug (Iteration 1526) ===
Q mean: -39.208588
Q std: 19.929592
Actor loss: 39.212566
Action reg: 0.003979
  l1.weight: grad_norm = 0.000364
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.001156
Total gradient norm: 0.003468
=== Actor Training Debug (Iteration 1527) ===
Q mean: -42.652084
Q std: 22.461586
Actor loss: 42.656059
Action reg: 0.003975
  l1.weight: grad_norm = 0.000395
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.001440
Total gradient norm: 0.004221
=== Actor Training Debug (Iteration 1528) ===
Q mean: -42.297649
Q std: 20.602999
Actor loss: 42.301643
Action reg: 0.003994
  l1.weight: grad_norm = 0.000186
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.000415
Total gradient norm: 0.001034
=== Actor Training Debug (Iteration 1529) ===
Q mean: -39.597893
Q std: 18.097719
Actor loss: 39.601875
Action reg: 0.003983
  l1.weight: grad_norm = 0.000311
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.000961
Total gradient norm: 0.002783
=== Actor Training Debug (Iteration 1530) ===
Q mean: -38.879562
Q std: 19.974468
Actor loss: 38.883556
Action reg: 0.003993
  l1.weight: grad_norm = 0.000142
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.000439
Total gradient norm: 0.001109
=== Actor Training Debug (Iteration 1531) ===
Q mean: -43.327576
Q std: 22.377478
Actor loss: 43.331570
Action reg: 0.003994
  l1.weight: grad_norm = 0.000170
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.000485
Total gradient norm: 0.001289
=== Actor Training Debug (Iteration 1532) ===
Q mean: -40.061893
Q std: 20.750597
Actor loss: 40.065887
Action reg: 0.003994
  l1.weight: grad_norm = 0.000160
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.000429
Total gradient norm: 0.001099
=== Actor Training Debug (Iteration 1533) ===
Q mean: -40.001114
Q std: 19.281645
Actor loss: 40.005100
Action reg: 0.003987
  l1.weight: grad_norm = 0.000407
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.001024
Total gradient norm: 0.002498
=== Actor Training Debug (Iteration 1534) ===
Q mean: -40.166615
Q std: 20.312984
Actor loss: 40.170589
Action reg: 0.003976
  l1.weight: grad_norm = 0.000437
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.001521
Total gradient norm: 0.004569
=== Actor Training Debug (Iteration 1535) ===
Q mean: -43.952881
Q std: 21.556320
Actor loss: 43.956863
Action reg: 0.003981
  l1.weight: grad_norm = 0.000293
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.001125
Total gradient norm: 0.003427
=== Actor Training Debug (Iteration 1536) ===
Q mean: -42.620171
Q std: 20.367310
Actor loss: 42.624157
Action reg: 0.003987
  l1.weight: grad_norm = 0.000323
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.000815
Total gradient norm: 0.002153
=== Actor Training Debug (Iteration 1537) ===
Q mean: -42.185253
Q std: 21.713100
Actor loss: 42.189240
Action reg: 0.003987
  l1.weight: grad_norm = 0.000302
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.000886
Total gradient norm: 0.002371
=== Actor Training Debug (Iteration 1538) ===
Q mean: -43.790615
Q std: 22.811651
Actor loss: 43.794609
Action reg: 0.003994
  l1.weight: grad_norm = 0.000120
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.000328
Total gradient norm: 0.000827
=== Actor Training Debug (Iteration 1539) ===
Q mean: -40.663773
Q std: 19.956598
Actor loss: 40.667751
Action reg: 0.003981
  l1.weight: grad_norm = 0.000347
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.001196
Total gradient norm: 0.003356
=== Actor Training Debug (Iteration 1540) ===
Q mean: -41.328346
Q std: 20.309355
Actor loss: 41.332348
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1541) ===
Q mean: -43.406715
Q std: 23.692104
Actor loss: 43.410702
Action reg: 0.003987
  l1.weight: grad_norm = 0.000259
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.000750
Total gradient norm: 0.002152
=== Actor Training Debug (Iteration 1542) ===
Q mean: -37.495445
Q std: 20.230667
Actor loss: 37.499439
Action reg: 0.003993
  l1.weight: grad_norm = 0.000184
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.000458
Total gradient norm: 0.001155
=== Actor Training Debug (Iteration 1543) ===
Q mean: -41.787437
Q std: 20.073645
Actor loss: 41.791435
Action reg: 0.003996
  l1.weight: grad_norm = 0.000192
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.000518
Total gradient norm: 0.001353
=== Actor Training Debug (Iteration 1544) ===
Q mean: -42.652760
Q std: 23.087076
Actor loss: 42.656746
Action reg: 0.003988
  l1.weight: grad_norm = 0.000256
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.000695
Total gradient norm: 0.001982
=== Actor Training Debug (Iteration 1545) ===
Q mean: -43.173500
Q std: 21.083059
Actor loss: 43.177494
Action reg: 0.003993
  l1.weight: grad_norm = 0.000214
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.000500
Total gradient norm: 0.001306
=== Actor Training Debug (Iteration 1546) ===
Q mean: -38.724319
Q std: 20.300137
Actor loss: 38.728313
Action reg: 0.003993
  l1.weight: grad_norm = 0.000238
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.000573
Total gradient norm: 0.001413
=== Actor Training Debug (Iteration 1547) ===
Q mean: -42.750504
Q std: 21.313768
Actor loss: 42.754505
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1548) ===
Q mean: -46.725014
Q std: 22.854612
Actor loss: 46.729000
Action reg: 0.003988
  l1.weight: grad_norm = 0.000262
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.000771
Total gradient norm: 0.002139
=== Actor Training Debug (Iteration 1549) ===
Q mean: -42.851555
Q std: 21.649025
Actor loss: 42.855549
Action reg: 0.003994
  l1.weight: grad_norm = 0.000252
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.000493
Total gradient norm: 0.001161
=== Actor Training Debug (Iteration 1550) ===
Q mean: -41.388237
Q std: 20.321522
Actor loss: 41.392223
Action reg: 0.003988
  l1.weight: grad_norm = 0.000244
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.000678
Total gradient norm: 0.002005
=== Actor Training Debug (Iteration 1551) ===
Q mean: -37.837105
Q std: 19.809671
Actor loss: 37.841095
Action reg: 0.003992
  l1.weight: grad_norm = 0.000235
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.000556
Total gradient norm: 0.001459
=== Actor Training Debug (Iteration 1552) ===
Q mean: -40.443085
Q std: 18.344509
Actor loss: 40.447086
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1553) ===
Q mean: -41.776829
Q std: 22.250586
Actor loss: 41.780819
Action reg: 0.003991
  l1.weight: grad_norm = 0.000337
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.000886
Total gradient norm: 0.002493
=== Actor Training Debug (Iteration 1554) ===
Q mean: -41.338482
Q std: 20.911104
Actor loss: 41.342476
Action reg: 0.003994
  l1.weight: grad_norm = 0.000198
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.000490
Total gradient norm: 0.001213
=== Actor Training Debug (Iteration 1555) ===
Q mean: -41.282742
Q std: 20.347290
Actor loss: 41.286728
Action reg: 0.003987
  l1.weight: grad_norm = 0.000339
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.000911
Total gradient norm: 0.002487
=== Actor Training Debug (Iteration 1556) ===
Q mean: -41.671406
Q std: 22.696421
Actor loss: 41.675400
Action reg: 0.003994
  l1.weight: grad_norm = 0.000191
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.000440
Total gradient norm: 0.001106
=== Actor Training Debug (Iteration 1557) ===
Q mean: -40.522881
Q std: 22.514139
Actor loss: 40.526836
Action reg: 0.003955
  l1.weight: grad_norm = 0.000585
  l1.bias: grad_norm = 0.000526
  l2.weight: grad_norm = 0.002627
Total gradient norm: 0.008203
=== Actor Training Debug (Iteration 1558) ===
Q mean: -42.243351
Q std: 20.418983
Actor loss: 42.247345
Action reg: 0.003994
  l1.weight: grad_norm = 0.000190
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.000430
Total gradient norm: 0.001080
=== Actor Training Debug (Iteration 1559) ===
Q mean: -42.189934
Q std: 22.330980
Actor loss: 42.193920
Action reg: 0.003986
  l1.weight: grad_norm = 0.000382
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.000916
Total gradient norm: 0.002234
=== Actor Training Debug (Iteration 1560) ===
Q mean: -42.608109
Q std: 21.519133
Actor loss: 42.612091
Action reg: 0.003981
  l1.weight: grad_norm = 0.000350
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.001009
Total gradient norm: 0.002896
=== Actor Training Debug (Iteration 1561) ===
Q mean: -43.663750
Q std: 21.196388
Actor loss: 43.667732
Action reg: 0.003982
  l1.weight: grad_norm = 0.000355
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.001175
Total gradient norm: 0.003264
=== Actor Training Debug (Iteration 1562) ===
Q mean: -39.365669
Q std: 21.426407
Actor loss: 39.369652
Action reg: 0.003984
  l1.weight: grad_norm = 0.000280
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.001124
Total gradient norm: 0.003388
=== Actor Training Debug (Iteration 1563) ===
Q mean: -39.217957
Q std: 20.802237
Actor loss: 39.221935
Action reg: 0.003977
  l1.weight: grad_norm = 0.000372
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.001368
Total gradient norm: 0.004137
=== Actor Training Debug (Iteration 1564) ===
Q mean: -43.190712
Q std: 19.548336
Actor loss: 43.194683
Action reg: 0.003969
  l1.weight: grad_norm = 0.000472
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.001783
Total gradient norm: 0.005527
=== Actor Training Debug (Iteration 1565) ===
Q mean: -42.512520
Q std: 20.778507
Actor loss: 42.516495
Action reg: 0.003974
  l1.weight: grad_norm = 0.000331
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.001276
Total gradient norm: 0.003853
=== Actor Training Debug (Iteration 1566) ===
Q mean: -39.479816
Q std: 19.317703
Actor loss: 39.483814
Action reg: 0.003996
  l1.weight: grad_norm = 0.000193
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.000466
Total gradient norm: 0.001215
=== Actor Training Debug (Iteration 1567) ===
Q mean: -39.620033
Q std: 19.765829
Actor loss: 39.624027
Action reg: 0.003995
  l1.weight: grad_norm = 0.000161
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.000404
Total gradient norm: 0.001074
=== Actor Training Debug (Iteration 1568) ===
Q mean: -42.396191
Q std: 20.056583
Actor loss: 42.400177
Action reg: 0.003987
  l1.weight: grad_norm = 0.000337
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.000809
Total gradient norm: 0.002151
=== Actor Training Debug (Iteration 1569) ===
Q mean: -39.848106
Q std: 21.206362
Actor loss: 39.852089
Action reg: 0.003982
  l1.weight: grad_norm = 0.000519
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.001378
Total gradient norm: 0.003849
=== Actor Training Debug (Iteration 1570) ===
Q mean: -42.022957
Q std: 21.539524
Actor loss: 42.026958
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1571) ===
Q mean: -43.241364
Q std: 22.446140
Actor loss: 43.245358
Action reg: 0.003994
  l1.weight: grad_norm = 0.000210
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.000503
Total gradient norm: 0.001327
=== Actor Training Debug (Iteration 1572) ===
Q mean: -39.552605
Q std: 19.791382
Actor loss: 39.556587
Action reg: 0.003984
  l1.weight: grad_norm = 0.000360
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.001128
Total gradient norm: 0.003347
=== Actor Training Debug (Iteration 1573) ===
Q mean: -41.268185
Q std: 19.995836
Actor loss: 41.272171
Action reg: 0.003985
  l1.weight: grad_norm = 0.000328
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.001079
Total gradient norm: 0.003364
=== Actor Training Debug (Iteration 1574) ===
Q mean: -44.191681
Q std: 21.898806
Actor loss: 44.195663
Action reg: 0.003982
  l1.weight: grad_norm = 0.000412
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.001179
Total gradient norm: 0.003295
=== Actor Training Debug (Iteration 1575) ===
Q mean: -43.884438
Q std: 22.649355
Actor loss: 43.888397
Action reg: 0.003961
  l1.weight: grad_norm = 0.000544
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.002103
Total gradient norm: 0.006419
=== Actor Training Debug (Iteration 1576) ===
Q mean: -38.478622
Q std: 21.960592
Actor loss: 38.482594
Action reg: 0.003971
  l1.weight: grad_norm = 0.000460
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.001705
Total gradient norm: 0.005343
=== Actor Training Debug (Iteration 1577) ===
Q mean: -39.632507
Q std: 20.464870
Actor loss: 39.636490
Action reg: 0.003981
  l1.weight: grad_norm = 0.000420
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.001090
Total gradient norm: 0.002971
=== Actor Training Debug (Iteration 1578) ===
Q mean: -41.811638
Q std: 20.716490
Actor loss: 41.815632
Action reg: 0.003994
  l1.weight: grad_norm = 0.000265
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.000564
Total gradient norm: 0.001413
=== Actor Training Debug (Iteration 1579) ===
Q mean: -42.747448
Q std: 19.581408
Actor loss: 42.751442
Action reg: 0.003993
  l1.weight: grad_norm = 0.000200
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.000421
Total gradient norm: 0.000962
=== Actor Training Debug (Iteration 1580) ===
Q mean: -41.587040
Q std: 20.234030
Actor loss: 41.591019
Action reg: 0.003980
  l1.weight: grad_norm = 0.000316
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.001078
Total gradient norm: 0.003115
=== Actor Training Debug (Iteration 1581) ===
Q mean: -41.423389
Q std: 21.634430
Actor loss: 41.427364
Action reg: 0.003976
  l1.weight: grad_norm = 0.000467
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.001459
Total gradient norm: 0.004309
=== Actor Training Debug (Iteration 1582) ===
Q mean: -42.330803
Q std: 23.566294
Actor loss: 42.334774
Action reg: 0.003970
  l1.weight: grad_norm = 0.000382
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.001518
Total gradient norm: 0.004571
=== Actor Training Debug (Iteration 1583) ===
Q mean: -44.196381
Q std: 22.008448
Actor loss: 44.200371
Action reg: 0.003990
  l1.weight: grad_norm = 0.000232
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.000696
Total gradient norm: 0.002033
=== Actor Training Debug (Iteration 1584) ===
Q mean: -43.712273
Q std: 21.987366
Actor loss: 43.716267
Action reg: 0.003994
  l1.weight: grad_norm = 0.000224
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.000493
Total gradient norm: 0.001293
=== Actor Training Debug (Iteration 1585) ===
Q mean: -42.044193
Q std: 19.551163
Actor loss: 42.048187
Action reg: 0.003995
  l1.weight: grad_norm = 0.000140
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.000382
Total gradient norm: 0.001074
=== Actor Training Debug (Iteration 1586) ===
Q mean: -40.981655
Q std: 20.418575
Actor loss: 40.985641
Action reg: 0.003988
  l1.weight: grad_norm = 0.000405
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.001095
Total gradient norm: 0.002850
=== Actor Training Debug (Iteration 1587) ===
Q mean: -42.634300
Q std: 21.925705
Actor loss: 42.638279
Action reg: 0.003980
  l1.weight: grad_norm = 0.000384
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.001186
Total gradient norm: 0.003415
=== Actor Training Debug (Iteration 1588) ===
Q mean: -43.894417
Q std: 21.460089
Actor loss: 43.898396
Action reg: 0.003980
  l1.weight: grad_norm = 0.000231
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.000944
Total gradient norm: 0.002869
=== Actor Training Debug (Iteration 1589) ===
Q mean: -40.634106
Q std: 19.026201
Actor loss: 40.638100
Action reg: 0.003993
  l1.weight: grad_norm = 0.000258
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.000644
Total gradient norm: 0.001457
=== Actor Training Debug (Iteration 1590) ===
Q mean: -40.300991
Q std: 20.046497
Actor loss: 40.304977
Action reg: 0.003987
  l1.weight: grad_norm = 0.000304
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.000781
Total gradient norm: 0.002108
=== Actor Training Debug (Iteration 1591) ===
Q mean: -45.345718
Q std: 22.594711
Actor loss: 45.349712
Action reg: 0.003994
  l1.weight: grad_norm = 0.000244
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.000481
Total gradient norm: 0.001142
=== Actor Training Debug (Iteration 1592) ===
Q mean: -45.907990
Q std: 22.212378
Actor loss: 45.911991
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1593) ===
Q mean: -43.136631
Q std: 20.291700
Actor loss: 43.140625
Action reg: 0.003993
  l1.weight: grad_norm = 0.000201
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.000426
Total gradient norm: 0.000979
=== Actor Training Debug (Iteration 1594) ===
Q mean: -42.997208
Q std: 19.703320
Actor loss: 43.001190
Action reg: 0.003982
  l1.weight: grad_norm = 0.000283
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.001028
Total gradient norm: 0.003223
=== Actor Training Debug (Iteration 1595) ===
Q mean: -44.334167
Q std: 23.860855
Actor loss: 44.338150
Action reg: 0.003983
  l1.weight: grad_norm = 0.000383
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.001153
Total gradient norm: 0.003429
=== Actor Training Debug (Iteration 1596) ===
Q mean: -42.692902
Q std: 22.344284
Actor loss: 42.696888
Action reg: 0.003985
  l1.weight: grad_norm = 0.000288
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.000879
Total gradient norm: 0.002612
=== Actor Training Debug (Iteration 1597) ===
Q mean: -40.503345
Q std: 20.412474
Actor loss: 40.507339
Action reg: 0.003995
  l1.weight: grad_norm = 0.000139
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.000341
Total gradient norm: 0.000885
=== Actor Training Debug (Iteration 1598) ===
Q mean: -41.162701
Q std: 20.570894
Actor loss: 41.166683
Action reg: 0.003984
  l1.weight: grad_norm = 0.000322
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.001009
Total gradient norm: 0.002965
=== Actor Training Debug (Iteration 1599) ===
Q mean: -40.327988
Q std: 19.979290
Actor loss: 40.331982
Action reg: 0.003994
  l1.weight: grad_norm = 0.000143
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.000438
Total gradient norm: 0.001111
=== Actor Training Debug (Iteration 1600) ===
Q mean: -44.417877
Q std: 21.423990
Actor loss: 44.421856
Action reg: 0.003979
  l1.weight: grad_norm = 0.000336
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.001206
Total gradient norm: 0.003544
=== Actor Training Debug (Iteration 1601) ===
Q mean: -44.654175
Q std: 22.904890
Actor loss: 44.658169
Action reg: 0.003995
  l1.weight: grad_norm = 0.000137
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.000338
Total gradient norm: 0.000881
=== Actor Training Debug (Iteration 1602) ===
Q mean: -39.001499
Q std: 21.484089
Actor loss: 39.005463
Action reg: 0.003962
  l1.weight: grad_norm = 0.000596
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.002441
Total gradient norm: 0.007689
=== Actor Training Debug (Iteration 1603) ===
Q mean: -38.888092
Q std: 21.097586
Actor loss: 38.892082
Action reg: 0.003990
  l1.weight: grad_norm = 0.000240
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.000760
Total gradient norm: 0.002177
=== Actor Training Debug (Iteration 1604) ===
Q mean: -43.446724
Q std: 21.284405
Actor loss: 43.450718
Action reg: 0.003996
  l1.weight: grad_norm = 0.000174
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.000471
Total gradient norm: 0.001164
=== Actor Training Debug (Iteration 1605) ===
Q mean: -43.202686
Q std: 21.438702
Actor loss: 43.206650
Action reg: 0.003962
  l1.weight: grad_norm = 0.000454
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.001949
Total gradient norm: 0.005914
=== Actor Training Debug (Iteration 1606) ===
Q mean: -41.650257
Q std: 21.384613
Actor loss: 41.654247
Action reg: 0.003990
  l1.weight: grad_norm = 0.000344
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.000969
Total gradient norm: 0.002587
=== Actor Training Debug (Iteration 1607) ===
Q mean: -38.981419
Q std: 20.484365
Actor loss: 38.985401
Action reg: 0.003981
  l1.weight: grad_norm = 0.000403
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.001165
Total gradient norm: 0.003298
=== Actor Training Debug (Iteration 1608) ===
Q mean: -38.552677
Q std: 19.447416
Actor loss: 38.556664
Action reg: 0.003988
  l1.weight: grad_norm = 0.000319
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.000862
Total gradient norm: 0.002386
=== Actor Training Debug (Iteration 1609) ===
Q mean: -42.179787
Q std: 21.459517
Actor loss: 42.183781
Action reg: 0.003994
  l1.weight: grad_norm = 0.000193
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.000440
Total gradient norm: 0.001147
=== Actor Training Debug (Iteration 1610) ===
Q mean: -46.353046
Q std: 22.091740
Actor loss: 46.357025
Action reg: 0.003977
  l1.weight: grad_norm = 0.000364
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.001350
Total gradient norm: 0.004109
=== Actor Training Debug (Iteration 1611) ===
Q mean: -43.743301
Q std: 20.738354
Actor loss: 43.747295
Action reg: 0.003994
  l1.weight: grad_norm = 0.000170
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.000358
Total gradient norm: 0.000965
=== Actor Training Debug (Iteration 1612) ===
Q mean: -40.073425
Q std: 20.861713
Actor loss: 40.077408
Action reg: 0.003983
  l1.weight: grad_norm = 0.000376
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.001030
Total gradient norm: 0.002983
=== Actor Training Debug (Iteration 1613) ===
Q mean: -38.983009
Q std: 20.622904
Actor loss: 38.986980
Action reg: 0.003969
  l1.weight: grad_norm = 0.000390
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.001587
Total gradient norm: 0.005005
=== Actor Training Debug (Iteration 1614) ===
Q mean: -45.346581
Q std: 23.771671
Actor loss: 45.350567
Action reg: 0.003988
  l1.weight: grad_norm = 0.000203
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.000663
Total gradient norm: 0.001990
=== Actor Training Debug (Iteration 1615) ===
Q mean: -44.356812
Q std: 20.181538
Actor loss: 44.360783
Action reg: 0.003972
  l1.weight: grad_norm = 0.000522
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.001747
Total gradient norm: 0.005114
=== Actor Training Debug (Iteration 1616) ===
Q mean: -40.290249
Q std: 20.469345
Actor loss: 40.294220
Action reg: 0.003972
  l1.weight: grad_norm = 0.000456
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.001691
Total gradient norm: 0.005237
=== Actor Training Debug (Iteration 1617) ===
Q mean: -42.511341
Q std: 20.617350
Actor loss: 42.515343
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1618) ===
Q mean: -41.808853
Q std: 22.107731
Actor loss: 41.812840
Action reg: 0.003987
  l1.weight: grad_norm = 0.000331
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.000872
Total gradient norm: 0.002405
=== Actor Training Debug (Iteration 1619) ===
Q mean: -45.754745
Q std: 23.329445
Actor loss: 45.758736
Action reg: 0.003989
  l1.weight: grad_norm = 0.000336
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.000873
Total gradient norm: 0.002371
=== Actor Training Debug (Iteration 1620) ===
Q mean: -45.350903
Q std: 21.558357
Actor loss: 45.354885
Action reg: 0.003984
  l1.weight: grad_norm = 0.000293
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.001039
Total gradient norm: 0.003225
=== Actor Training Debug (Iteration 1621) ===
Q mean: -40.070332
Q std: 20.819784
Actor loss: 40.074322
Action reg: 0.003989
  l1.weight: grad_norm = 0.000272
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.000738
Total gradient norm: 0.002004
=== Actor Training Debug (Iteration 1622) ===
Q mean: -39.746487
Q std: 19.872959
Actor loss: 39.750458
Action reg: 0.003969
  l1.weight: grad_norm = 0.000378
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.001593
Total gradient norm: 0.004942
=== Actor Training Debug (Iteration 1623) ===
Q mean: -43.079910
Q std: 22.135506
Actor loss: 43.083881
Action reg: 0.003970
  l1.weight: grad_norm = 0.000516
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.001787
Total gradient norm: 0.004961
=== Actor Training Debug (Iteration 1624) ===
Q mean: -42.990089
Q std: 19.637062
Actor loss: 42.994072
Action reg: 0.003981
  l1.weight: grad_norm = 0.000304
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.001116
Total gradient norm: 0.003415
=== Actor Training Debug (Iteration 1625) ===
Q mean: -41.091457
Q std: 20.730536
Actor loss: 41.095440
Action reg: 0.003982
  l1.weight: grad_norm = 0.000338
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.001068
Total gradient norm: 0.003002
=== Actor Training Debug (Iteration 1626) ===
Q mean: -39.739517
Q std: 20.876728
Actor loss: 39.743504
Action reg: 0.003988
  l1.weight: grad_norm = 0.000257
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.000764
Total gradient norm: 0.002221
=== Actor Training Debug (Iteration 1627) ===
Q mean: -43.198193
Q std: 20.928698
Actor loss: 43.202194
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1628) ===
Q mean: -41.331535
Q std: 19.412054
Actor loss: 41.335522
Action reg: 0.003988
  l1.weight: grad_norm = 0.000222
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.000731
Total gradient norm: 0.002239
=== Actor Training Debug (Iteration 1629) ===
Q mean: -42.066299
Q std: 23.050198
Actor loss: 42.070290
Action reg: 0.003989
  l1.weight: grad_norm = 0.000266
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.000731
Total gradient norm: 0.001989
=== Actor Training Debug (Iteration 1630) ===
Q mean: -41.448044
Q std: 20.543739
Actor loss: 41.452045
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1631) ===
Q mean: -43.604137
Q std: 22.097092
Actor loss: 43.608109
Action reg: 0.003973
  l1.weight: grad_norm = 0.000405
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.001783
Total gradient norm: 0.005646
=== Actor Training Debug (Iteration 1632) ===
Q mean: -44.380821
Q std: 20.901236
Actor loss: 44.384823
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1633) ===
Q mean: -41.601761
Q std: 21.593805
Actor loss: 41.605751
Action reg: 0.003990
  l1.weight: grad_norm = 0.000230
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.000713
Total gradient norm: 0.001906
=== Actor Training Debug (Iteration 1634) ===
Q mean: -43.113068
Q std: 20.397879
Actor loss: 43.117054
Action reg: 0.003988
  l1.weight: grad_norm = 0.000327
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.000870
Total gradient norm: 0.002288
=== Actor Training Debug (Iteration 1635) ===
Q mean: -40.422852
Q std: 18.662596
Actor loss: 40.426842
Action reg: 0.003989
  l1.weight: grad_norm = 0.000285
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.000845
Total gradient norm: 0.002419
=== Actor Training Debug (Iteration 1636) ===
Q mean: -41.354988
Q std: 19.586576
Actor loss: 41.358982
Action reg: 0.003994
  l1.weight: grad_norm = 0.000133
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.000391
Total gradient norm: 0.001140
=== Actor Training Debug (Iteration 1637) ===
Q mean: -42.618408
Q std: 22.080055
Actor loss: 42.622379
Action reg: 0.003971
  l1.weight: grad_norm = 0.000410
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.001492
Total gradient norm: 0.004468
=== Actor Training Debug (Iteration 1638) ===
Q mean: -43.287941
Q std: 22.784315
Actor loss: 43.291924
Action reg: 0.003981
  l1.weight: grad_norm = 0.000316
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.001045
Total gradient norm: 0.003040
=== Actor Training Debug (Iteration 1639) ===
Q mean: -42.237320
Q std: 20.945578
Actor loss: 42.241302
Action reg: 0.003982
  l1.weight: grad_norm = 0.000356
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.001177
Total gradient norm: 0.003432
=== Actor Training Debug (Iteration 1640) ===
Q mean: -43.062210
Q std: 21.371656
Actor loss: 43.066193
Action reg: 0.003981
  l1.weight: grad_norm = 0.000330
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.001037
Total gradient norm: 0.003077
=== Actor Training Debug (Iteration 1641) ===
Q mean: -41.555042
Q std: 20.535686
Actor loss: 41.559032
Action reg: 0.003990
  l1.weight: grad_norm = 0.000213
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.000518
Total gradient norm: 0.001396
=== Actor Training Debug (Iteration 1642) ===
Q mean: -43.219749
Q std: 21.690943
Actor loss: 43.223743
Action reg: 0.003994
  l1.weight: grad_norm = 0.000170
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.000387
Total gradient norm: 0.000996
=== Actor Training Debug (Iteration 1643) ===
Q mean: -43.368355
Q std: 22.168949
Actor loss: 43.372330
Action reg: 0.003977
  l1.weight: grad_norm = 0.000464
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.001382
Total gradient norm: 0.003903
=== Actor Training Debug (Iteration 1644) ===
Q mean: -40.957794
Q std: 21.031105
Actor loss: 40.961754
Action reg: 0.003959
  l1.weight: grad_norm = 0.000561
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.002360
Total gradient norm: 0.007080
=== Actor Training Debug (Iteration 1645) ===
Q mean: -44.139183
Q std: 21.543747
Actor loss: 44.143185
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1646) ===
Q mean: -41.229992
Q std: 21.481480
Actor loss: 41.233982
Action reg: 0.003989
  l1.weight: grad_norm = 0.000218
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.000599
Total gradient norm: 0.001671
=== Actor Training Debug (Iteration 1647) ===
Q mean: -40.583858
Q std: 23.582520
Actor loss: 40.587845
Action reg: 0.003985
  l1.weight: grad_norm = 0.000317
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.000973
Total gradient norm: 0.002912
=== Actor Training Debug (Iteration 1648) ===
Q mean: -45.427555
Q std: 22.477011
Actor loss: 45.431549
Action reg: 0.003994
  l1.weight: grad_norm = 0.000133
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.000415
Total gradient norm: 0.001134
=== Actor Training Debug (Iteration 1649) ===
Q mean: -42.457737
Q std: 20.544600
Actor loss: 42.461716
Action reg: 0.003980
  l1.weight: grad_norm = 0.000321
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.001070
Total gradient norm: 0.003260
=== Actor Training Debug (Iteration 1650) ===
Q mean: -39.511063
Q std: 20.104719
Actor loss: 39.515041
Action reg: 0.003978
  l1.weight: grad_norm = 0.000403
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.001453
Total gradient norm: 0.004450
=== Actor Training Debug (Iteration 1651) ===
Q mean: -40.642605
Q std: 19.132072
Actor loss: 40.646606
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1652) ===
Q mean: -45.184505
Q std: 20.691193
Actor loss: 45.188507
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1653) ===
Q mean: -43.878380
Q std: 22.514275
Actor loss: 43.882362
Action reg: 0.003984
  l1.weight: grad_norm = 0.000336
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.001170
Total gradient norm: 0.003650
=== Actor Training Debug (Iteration 1654) ===
Q mean: -41.809189
Q std: 20.283884
Actor loss: 41.813190
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1655) ===
Q mean: -40.369175
Q std: 20.122169
Actor loss: 40.373158
Action reg: 0.003981
  l1.weight: grad_norm = 0.000379
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.001174
Total gradient norm: 0.003462
=== Actor Training Debug (Iteration 1656) ===
Q mean: -47.003796
Q std: 20.875198
Actor loss: 47.007778
Action reg: 0.003981
  l1.weight: grad_norm = 0.000295
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.001088
Total gradient norm: 0.003278
=== Actor Training Debug (Iteration 1657) ===
Q mean: -41.605534
Q std: 21.237312
Actor loss: 41.609516
Action reg: 0.003983
  l1.weight: grad_norm = 0.000287
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.001000
Total gradient norm: 0.002988
=== Actor Training Debug (Iteration 1658) ===
Q mean: -40.189583
Q std: 20.465689
Actor loss: 40.193584
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1659) ===
Q mean: -41.160713
Q std: 20.825176
Actor loss: 41.164707
Action reg: 0.003994
  l1.weight: grad_norm = 0.000210
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.000484
Total gradient norm: 0.001119
=== Actor Training Debug (Iteration 1660) ===
Q mean: -43.977058
Q std: 22.388672
Actor loss: 43.981045
Action reg: 0.003985
  l1.weight: grad_norm = 0.000340
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.001087
Total gradient norm: 0.003216
=== Actor Training Debug (Iteration 1661) ===
Q mean: -44.639008
Q std: 22.104679
Actor loss: 44.642979
Action reg: 0.003973
  l1.weight: grad_norm = 0.000452
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.001578
Total gradient norm: 0.004969
=== Actor Training Debug (Iteration 1662) ===
Q mean: -41.312145
Q std: 22.854357
Actor loss: 41.316135
Action reg: 0.003989
  l1.weight: grad_norm = 0.000326
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.000810
Total gradient norm: 0.002265
=== Actor Training Debug (Iteration 1663) ===
Q mean: -41.146172
Q std: 21.911434
Actor loss: 41.150131
Action reg: 0.003960
  l1.weight: grad_norm = 0.000545
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.002225
Total gradient norm: 0.007255
=== Actor Training Debug (Iteration 1664) ===
Q mean: -44.281769
Q std: 21.334024
Actor loss: 44.285759
Action reg: 0.003990
  l1.weight: grad_norm = 0.000202
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.000638
Total gradient norm: 0.001874
=== Actor Training Debug (Iteration 1665) ===
Q mean: -42.481522
Q std: 21.252159
Actor loss: 42.485485
Action reg: 0.003964
  l1.weight: grad_norm = 0.000657
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.002173
Total gradient norm: 0.006624
=== Actor Training Debug (Iteration 1666) ===
Q mean: -42.753090
Q std: 21.387274
Actor loss: 42.757080
Action reg: 0.003990
  l1.weight: grad_norm = 0.000280
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.000775
Total gradient norm: 0.002244
=== Actor Training Debug (Iteration 1667) ===
Q mean: -40.063236
Q std: 20.721207
Actor loss: 40.067226
Action reg: 0.003989
  l1.weight: grad_norm = 0.000398
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.001045
Total gradient norm: 0.002689
=== Actor Training Debug (Iteration 1668) ===
Q mean: -44.950508
Q std: 23.600914
Actor loss: 44.954502
Action reg: 0.003995
  l1.weight: grad_norm = 0.000152
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.000411
Total gradient norm: 0.001051
=== Actor Training Debug (Iteration 1669) ===
Q mean: -45.425903
Q std: 22.680540
Actor loss: 45.429893
Action reg: 0.003989
  l1.weight: grad_norm = 0.000175
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.000621
Total gradient norm: 0.001963
=== Actor Training Debug (Iteration 1670) ===
Q mean: -40.383953
Q std: 21.652746
Actor loss: 40.387943
Action reg: 0.003989
  l1.weight: grad_norm = 0.000255
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.000733
Total gradient norm: 0.001980
=== Actor Training Debug (Iteration 1671) ===
Q mean: -41.477703
Q std: 20.869293
Actor loss: 41.481697
Action reg: 0.003995
  l1.weight: grad_norm = 0.000117
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.000300
Total gradient norm: 0.000790
=== Actor Training Debug (Iteration 1672) ===
Q mean: -45.171280
Q std: 21.689617
Actor loss: 45.175270
Action reg: 0.003990
  l1.weight: grad_norm = 0.000271
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.000685
Total gradient norm: 0.001932
=== Actor Training Debug (Iteration 1673) ===
Q mean: -44.999176
Q std: 21.223021
Actor loss: 45.003170
Action reg: 0.003995
  l1.weight: grad_norm = 0.000143
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.000452
Total gradient norm: 0.001183
=== Actor Training Debug (Iteration 1674) ===
Q mean: -40.640408
Q std: 19.224939
Actor loss: 40.644402
Action reg: 0.003993
  l1.weight: grad_norm = 0.000280
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.000806
Total gradient norm: 0.002155
=== Actor Training Debug (Iteration 1675) ===
Q mean: -42.458473
Q std: 19.243477
Actor loss: 42.462460
Action reg: 0.003988
  l1.weight: grad_norm = 0.000201
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.000693
Total gradient norm: 0.002140
=== Actor Training Debug (Iteration 1676) ===
Q mean: -44.192741
Q std: 21.405609
Actor loss: 44.196732
Action reg: 0.003989
  l1.weight: grad_norm = 0.000310
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.000853
Total gradient norm: 0.002405
=== Actor Training Debug (Iteration 1677) ===
Q mean: -45.469261
Q std: 21.596848
Actor loss: 45.473263
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1678) ===
Q mean: -41.932163
Q std: 21.755138
Actor loss: 41.936165
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1679) ===
Q mean: -43.183960
Q std: 22.320686
Actor loss: 43.187946
Action reg: 0.003988
  l1.weight: grad_norm = 0.000228
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.000645
Total gradient norm: 0.001822
=== Actor Training Debug (Iteration 1680) ===
Q mean: -45.697361
Q std: 22.981451
Actor loss: 45.701351
Action reg: 0.003989
  l1.weight: grad_norm = 0.000240
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.000773
Total gradient norm: 0.002106
=== Actor Training Debug (Iteration 1681) ===
Q mean: -49.905640
Q std: 23.825405
Actor loss: 49.909622
Action reg: 0.003983
  l1.weight: grad_norm = 0.000305
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.001034
Total gradient norm: 0.003103
=== Actor Training Debug (Iteration 1682) ===
Q mean: -41.823845
Q std: 22.736103
Actor loss: 41.827808
Action reg: 0.003963
  l1.weight: grad_norm = 0.000394
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.002060
Total gradient norm: 0.006768
=== Actor Training Debug (Iteration 1683) ===
Q mean: -39.595108
Q std: 20.521555
Actor loss: 39.599087
Action reg: 0.003978
  l1.weight: grad_norm = 0.000322
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.001274
Total gradient norm: 0.003996
=== Actor Training Debug (Iteration 1684) ===
Q mean: -42.587715
Q std: 23.490414
Actor loss: 42.591702
Action reg: 0.003986
  l1.weight: grad_norm = 0.000278
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.001007
Total gradient norm: 0.003007
=== Actor Training Debug (Iteration 1685) ===
Q mean: -43.606289
Q std: 21.263086
Actor loss: 43.610279
Action reg: 0.003992
  l1.weight: grad_norm = 0.000261
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.000728
Total gradient norm: 0.002001
=== Actor Training Debug (Iteration 1686) ===
Q mean: -44.862007
Q std: 21.662800
Actor loss: 44.865990
Action reg: 0.003984
  l1.weight: grad_norm = 0.000263
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.001005
Total gradient norm: 0.003055
=== Actor Training Debug (Iteration 1687) ===
Q mean: -41.963047
Q std: 21.381939
Actor loss: 41.967022
Action reg: 0.003976
  l1.weight: grad_norm = 0.000476
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.001675
Total gradient norm: 0.005319
=== Actor Training Debug (Iteration 1688) ===
Q mean: -41.252010
Q std: 20.962824
Actor loss: 41.255989
Action reg: 0.003978
  l1.weight: grad_norm = 0.000328
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.001249
Total gradient norm: 0.003917
=== Actor Training Debug (Iteration 1689) ===
Q mean: -40.485420
Q std: 21.488461
Actor loss: 40.489418
Action reg: 0.003996
  l1.weight: grad_norm = 0.000146
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.000400
Total gradient norm: 0.000985
=== Actor Training Debug (Iteration 1690) ===
Q mean: -46.523006
Q std: 22.015533
Actor loss: 46.527000
Action reg: 0.003995
  l1.weight: grad_norm = 0.000123
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.000275
Total gradient norm: 0.000727
=== Actor Training Debug (Iteration 1691) ===
Q mean: -44.737717
Q std: 23.853069
Actor loss: 44.741703
Action reg: 0.003986
  l1.weight: grad_norm = 0.000374
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.001126
Total gradient norm: 0.003260
=== Actor Training Debug (Iteration 1692) ===
Q mean: -43.816032
Q std: 21.084814
Actor loss: 43.820026
Action reg: 0.003995
  l1.weight: grad_norm = 0.000202
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.000526
Total gradient norm: 0.001391
=== Actor Training Debug (Iteration 1693) ===
Q mean: -43.643654
Q std: 21.568165
Actor loss: 43.647644
Action reg: 0.003990
  l1.weight: grad_norm = 0.000237
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.000701
Total gradient norm: 0.002042
=== Actor Training Debug (Iteration 1694) ===
Q mean: -42.967377
Q std: 21.913694
Actor loss: 42.971359
Action reg: 0.003981
  l1.weight: grad_norm = 0.000367
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.001346
Total gradient norm: 0.004118
=== Actor Training Debug (Iteration 1695) ===
Q mean: -44.658501
Q std: 23.358038
Actor loss: 44.662483
Action reg: 0.003984
  l1.weight: grad_norm = 0.000293
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.001000
Total gradient norm: 0.002952
=== Actor Training Debug (Iteration 1696) ===
Q mean: -46.591164
Q std: 21.981377
Actor loss: 46.595154
Action reg: 0.003990
  l1.weight: grad_norm = 0.000216
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.000676
Total gradient norm: 0.001808
=== Actor Training Debug (Iteration 1697) ===
Q mean: -41.969292
Q std: 20.402922
Actor loss: 41.973278
Action reg: 0.003988
  l1.weight: grad_norm = 0.000316
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.000842
Total gradient norm: 0.002141
=== Actor Training Debug (Iteration 1698) ===
Q mean: -40.802494
Q std: 20.917225
Actor loss: 40.806469
Action reg: 0.003973
  l1.weight: grad_norm = 0.000448
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.001606
Total gradient norm: 0.004942
=== Actor Training Debug (Iteration 1699) ===
Q mean: -40.207085
Q std: 19.976633
Actor loss: 40.211079
Action reg: 0.003994
  l1.weight: grad_norm = 0.000181
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.000437
Total gradient norm: 0.001103
=== Actor Training Debug (Iteration 1700) ===
Q mean: -46.105942
Q std: 20.865154
Actor loss: 46.109936
Action reg: 0.003994
  l1.weight: grad_norm = 0.000118
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.000363
Total gradient norm: 0.000987
=== Actor Training Debug (Iteration 1701) ===
Q mean: -44.755177
Q std: 22.364162
Actor loss: 44.759159
Action reg: 0.003984
  l1.weight: grad_norm = 0.000267
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.000903
Total gradient norm: 0.002839
=== Actor Training Debug (Iteration 1702) ===
Q mean: -42.736015
Q std: 22.575169
Actor loss: 42.740002
Action reg: 0.003986
  l1.weight: grad_norm = 0.000348
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.000985
Total gradient norm: 0.002801
=== Actor Training Debug (Iteration 1703) ===
Q mean: -40.884365
Q std: 20.733904
Actor loss: 40.888355
Action reg: 0.003989
  l1.weight: grad_norm = 0.000368
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.001048
Total gradient norm: 0.002976
=== Actor Training Debug (Iteration 1704) ===
Q mean: -42.217148
Q std: 20.022018
Actor loss: 42.221123
Action reg: 0.003975
  l1.weight: grad_norm = 0.000385
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.001326
Total gradient norm: 0.003918
=== Actor Training Debug (Iteration 1705) ===
Q mean: -43.458164
Q std: 20.792494
Actor loss: 43.462154
Action reg: 0.003990
  l1.weight: grad_norm = 0.000268
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.000674
Total gradient norm: 0.001823
=== Actor Training Debug (Iteration 1706) ===
Q mean: -39.955502
Q std: 19.595371
Actor loss: 39.959492
Action reg: 0.003990
  l1.weight: grad_norm = 0.000213
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.000626
Total gradient norm: 0.001685
=== Actor Training Debug (Iteration 1707) ===
Q mean: -41.911041
Q std: 21.173042
Actor loss: 41.915031
Action reg: 0.003989
  l1.weight: grad_norm = 0.000273
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.000714
Total gradient norm: 0.002040
=== Actor Training Debug (Iteration 1708) ===
Q mean: -42.665291
Q std: 20.807678
Actor loss: 42.669281
Action reg: 0.003991
  l1.weight: grad_norm = 0.000288
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.000813
Total gradient norm: 0.002162
=== Actor Training Debug (Iteration 1709) ===
Q mean: -43.642982
Q std: 23.499628
Actor loss: 43.646984
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1710) ===
Q mean: -42.766426
Q std: 20.184677
Actor loss: 42.770412
Action reg: 0.003987
  l1.weight: grad_norm = 0.000225
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.000728
Total gradient norm: 0.002219
=== Actor Training Debug (Iteration 1711) ===
Q mean: -43.714199
Q std: 21.813475
Actor loss: 43.718201
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1712) ===
Q mean: -43.923729
Q std: 20.544188
Actor loss: 43.927723
Action reg: 0.003995
  l1.weight: grad_norm = 0.000161
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.000377
Total gradient norm: 0.000985
=== Actor Training Debug (Iteration 1713) ===
Q mean: -44.685879
Q std: 20.037365
Actor loss: 44.689880
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1714) ===
Q mean: -44.147781
Q std: 23.122051
Actor loss: 44.151772
Action reg: 0.003989
  l1.weight: grad_norm = 0.000276
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.000821
Total gradient norm: 0.002311
=== Actor Training Debug (Iteration 1715) ===
Q mean: -40.413574
Q std: 22.901535
Actor loss: 40.417561
Action reg: 0.003987
  l1.weight: grad_norm = 0.000267
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.000903
Total gradient norm: 0.002743
=== Actor Training Debug (Iteration 1716) ===
Q mean: -44.330231
Q std: 22.775681
Actor loss: 44.334232
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1717) ===
Q mean: -44.940506
Q std: 23.849722
Actor loss: 44.944489
Action reg: 0.003983
  l1.weight: grad_norm = 0.000230
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.000884
Total gradient norm: 0.002660
=== Actor Training Debug (Iteration 1718) ===
Q mean: -44.221867
Q std: 21.607845
Actor loss: 44.225853
Action reg: 0.003985
  l1.weight: grad_norm = 0.000316
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.000942
Total gradient norm: 0.002873
=== Actor Training Debug (Iteration 1719) ===
Q mean: -45.748871
Q std: 21.904520
Actor loss: 45.752872
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1720) ===
Q mean: -41.918144
Q std: 21.622250
Actor loss: 41.922146
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1721) ===
Q mean: -38.647842
Q std: 18.667461
Actor loss: 38.651833
Action reg: 0.003989
  l1.weight: grad_norm = 0.000293
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.000693
Total gradient norm: 0.001940
=== Actor Training Debug (Iteration 1722) ===
Q mean: -41.068359
Q std: 20.621796
Actor loss: 41.072330
Action reg: 0.003970
  l1.weight: grad_norm = 0.000413
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.001798
Total gradient norm: 0.005779
=== Actor Training Debug (Iteration 1723) ===
Q mean: -43.591431
Q std: 21.772490
Actor loss: 43.595402
Action reg: 0.003972
  l1.weight: grad_norm = 0.000264
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.001408
Total gradient norm: 0.004751
=== Actor Training Debug (Iteration 1724) ===
Q mean: -43.499168
Q std: 22.189928
Actor loss: 43.503151
Action reg: 0.003982
  l1.weight: grad_norm = 0.000288
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.001098
Total gradient norm: 0.003489
=== Actor Training Debug (Iteration 1725) ===
Q mean: -38.690575
Q std: 19.625788
Actor loss: 38.694569
Action reg: 0.003992
  l1.weight: grad_norm = 0.000185
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.000664
Total gradient norm: 0.001945
=== Actor Training Debug (Iteration 1726) ===
Q mean: -42.479416
Q std: 23.144558
Actor loss: 42.483398
Action reg: 0.003984
  l1.weight: grad_norm = 0.000318
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.001035
Total gradient norm: 0.002967
=== Actor Training Debug (Iteration 1727) ===
Q mean: -43.907909
Q std: 21.256433
Actor loss: 43.911903
Action reg: 0.003994
  l1.weight: grad_norm = 0.000167
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.000405
Total gradient norm: 0.001010
=== Actor Training Debug (Iteration 1728) ===
Q mean: -45.258659
Q std: 21.245127
Actor loss: 45.262650
Action reg: 0.003990
  l1.weight: grad_norm = 0.000205
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.000649
Total gradient norm: 0.001902
=== Actor Training Debug (Iteration 1729) ===
Q mean: -44.216446
Q std: 22.131788
Actor loss: 44.220432
Action reg: 0.003986
  l1.weight: grad_norm = 0.000192
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.000751
Total gradient norm: 0.002351
=== Actor Training Debug (Iteration 1730) ===
Q mean: -43.193295
Q std: 21.643127
Actor loss: 43.197277
Action reg: 0.003981
  l1.weight: grad_norm = 0.000338
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.001017
Total gradient norm: 0.003056
=== Actor Training Debug (Iteration 1731) ===
Q mean: -41.283211
Q std: 21.485748
Actor loss: 41.287197
Action reg: 0.003988
  l1.weight: grad_norm = 0.000283
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.000765
Total gradient norm: 0.002069
=== Actor Training Debug (Iteration 1732) ===
Q mean: -40.763676
Q std: 20.957275
Actor loss: 40.767662
Action reg: 0.003985
  l1.weight: grad_norm = 0.000241
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.000855
Total gradient norm: 0.002658
=== Actor Training Debug (Iteration 1733) ===
Q mean: -44.008484
Q std: 21.602859
Actor loss: 44.012455
Action reg: 0.003973
  l1.weight: grad_norm = 0.000384
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.001573
Total gradient norm: 0.005016
=== Actor Training Debug (Iteration 1734) ===
Q mean: -46.337788
Q std: 24.072582
Actor loss: 46.341755
Action reg: 0.003968
  l1.weight: grad_norm = 0.000394
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.001531
Total gradient norm: 0.004860
=== Actor Training Debug (Iteration 1735) ===
Q mean: -44.745972
Q std: 21.797735
Actor loss: 44.749958
Action reg: 0.003988
  l1.weight: grad_norm = 0.000252
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.000716
Total gradient norm: 0.002072
=== Actor Training Debug (Iteration 1736) ===
Q mean: -41.183800
Q std: 20.165623
Actor loss: 41.187790
Action reg: 0.003989
  l1.weight: grad_norm = 0.000298
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.000832
Total gradient norm: 0.002213
=== Actor Training Debug (Iteration 1737) ===
Q mean: -43.654785
Q std: 23.752550
Actor loss: 43.658779
Action reg: 0.003995
  l1.weight: grad_norm = 0.000182
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.000454
Total gradient norm: 0.001163
=== Actor Training Debug (Iteration 1738) ===
Q mean: -46.023617
Q std: 23.043615
Actor loss: 46.027618
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1739) ===
Q mean: -45.521149
Q std: 20.732862
Actor loss: 45.525143
Action reg: 0.003995
  l1.weight: grad_norm = 0.000211
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.000529
Total gradient norm: 0.001390
=== Actor Training Debug (Iteration 1740) ===
Q mean: -41.181179
Q std: 21.212322
Actor loss: 41.185162
Action reg: 0.003984
  l1.weight: grad_norm = 0.000296
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.001022
Total gradient norm: 0.003235
=== Actor Training Debug (Iteration 1741) ===
Q mean: -41.374931
Q std: 20.545046
Actor loss: 41.378914
Action reg: 0.003983
  l1.weight: grad_norm = 0.000311
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.001136
Total gradient norm: 0.003476
=== Actor Training Debug (Iteration 1742) ===
Q mean: -45.635933
Q std: 23.279778
Actor loss: 45.639908
Action reg: 0.003973
  l1.weight: grad_norm = 0.000389
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.001504
Total gradient norm: 0.004497
=== Actor Training Debug (Iteration 1743) ===
Q mean: -45.566444
Q std: 21.684870
Actor loss: 45.570431
Action reg: 0.003985
  l1.weight: grad_norm = 0.000271
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.000870
Total gradient norm: 0.002625
=== Actor Training Debug (Iteration 1744) ===
Q mean: -45.126366
Q std: 20.518082
Actor loss: 45.130356
Action reg: 0.003989
  l1.weight: grad_norm = 0.000253
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.000721
Total gradient norm: 0.002132
=== Actor Training Debug (Iteration 1745) ===
Q mean: -40.118050
Q std: 22.185343
Actor loss: 40.122025
Action reg: 0.003976
  l1.weight: grad_norm = 0.000333
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.001394
Total gradient norm: 0.004600
=== Actor Training Debug (Iteration 1746) ===
Q mean: -38.068565
Q std: 20.461813
Actor loss: 38.072544
Action reg: 0.003980
  l1.weight: grad_norm = 0.000366
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.001318
Total gradient norm: 0.003986
=== Actor Training Debug (Iteration 1747) ===
Q mean: -44.088593
Q std: 20.056845
Actor loss: 44.092583
Action reg: 0.003991
  l1.weight: grad_norm = 0.000182
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.000624
Total gradient norm: 0.001839
=== Actor Training Debug (Iteration 1748) ===
Q mean: -46.101257
Q std: 21.878603
Actor loss: 46.105221
Action reg: 0.003962
  l1.weight: grad_norm = 0.000487
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.002214
Total gradient norm: 0.007482
=== Actor Training Debug (Iteration 1749) ===
Q mean: -41.096169
Q std: 21.225056
Actor loss: 41.100163
Action reg: 0.003994
  l1.weight: grad_norm = 0.000149
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.000379
Total gradient norm: 0.001035
=== Actor Training Debug (Iteration 1750) ===
Q mean: -42.910378
Q std: 23.774635
Actor loss: 42.914364
Action reg: 0.003986
  l1.weight: grad_norm = 0.000176
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.000718
Total gradient norm: 0.002230
=== Actor Training Debug (Iteration 1751) ===
Q mean: -45.359692
Q std: 23.332954
Actor loss: 45.363682
Action reg: 0.003990
  l1.weight: grad_norm = 0.000200
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.000604
Total gradient norm: 0.001804
=== Actor Training Debug (Iteration 1752) ===
Q mean: -43.794067
Q std: 20.867153
Actor loss: 43.798050
Action reg: 0.003982
  l1.weight: grad_norm = 0.000344
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.001092
Total gradient norm: 0.003265
=== Actor Training Debug (Iteration 1753) ===
Q mean: -41.370945
Q std: 22.045620
Actor loss: 41.374931
Action reg: 0.003987
  l1.weight: grad_norm = 0.000221
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.000812
Total gradient norm: 0.002543
=== Actor Training Debug (Iteration 1754) ===
Q mean: -45.417412
Q std: 20.485367
Actor loss: 45.421410
Action reg: 0.003997
  l1.weight: grad_norm = 0.000128
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.000394
Total gradient norm: 0.001008
=== Actor Training Debug (Iteration 1755) ===
Q mean: -46.051910
Q std: 20.129719
Actor loss: 46.055908
Action reg: 0.003997
  l1.weight: grad_norm = 0.000153
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.000410
Total gradient norm: 0.001070
=== Actor Training Debug (Iteration 1756) ===
Q mean: -41.026485
Q std: 23.935087
Actor loss: 41.030453
Action reg: 0.003966
  l1.weight: grad_norm = 0.000415
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.001910
Total gradient norm: 0.006331
=== Actor Training Debug (Iteration 1757) ===
Q mean: -43.872055
Q std: 23.363186
Actor loss: 43.876053
Action reg: 0.003997
  l1.weight: grad_norm = 0.000158
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.000403
Total gradient norm: 0.001044
=== Actor Training Debug (Iteration 1758) ===
Q mean: -41.754333
Q std: 22.624580
Actor loss: 41.758308
Action reg: 0.003976
  l1.weight: grad_norm = 0.000418
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.001499
Total gradient norm: 0.004721
=== Actor Training Debug (Iteration 1759) ===
Q mean: -46.797829
Q std: 19.753502
Actor loss: 46.801823
Action reg: 0.003995
  l1.weight: grad_norm = 0.000222
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.000492
Total gradient norm: 0.001291
=== Actor Training Debug (Iteration 1760) ===
Q mean: -46.212170
Q std: 19.122387
Actor loss: 46.216164
Action reg: 0.003995
  l1.weight: grad_norm = 0.000112
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.000254
Total gradient norm: 0.000643
=== Actor Training Debug (Iteration 1761) ===
Q mean: -44.211037
Q std: 20.253162
Actor loss: 44.215027
Action reg: 0.003989
  l1.weight: grad_norm = 0.000206
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.000622
Total gradient norm: 0.001866
=== Actor Training Debug (Iteration 1762) ===
Q mean: -42.329350
Q std: 21.053484
Actor loss: 42.333324
Action reg: 0.003977
  l1.weight: grad_norm = 0.000398
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.001552
Total gradient norm: 0.005072
=== Actor Training Debug (Iteration 1763) ===
Q mean: -44.104019
Q std: 21.853802
Actor loss: 44.107998
Action reg: 0.003978
  l1.weight: grad_norm = 0.000316
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.001249
Total gradient norm: 0.003832
=== Actor Training Debug (Iteration 1764) ===
Q mean: -45.313904
Q std: 23.589184
Actor loss: 45.317879
Action reg: 0.003976
  l1.weight: grad_norm = 0.000364
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.001201
Total gradient norm: 0.003514
=== Actor Training Debug (Iteration 1765) ===
Q mean: -43.021935
Q std: 22.004259
Actor loss: 43.025906
Action reg: 0.003972
  l1.weight: grad_norm = 0.000515
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.001756
Total gradient norm: 0.005420
=== Actor Training Debug (Iteration 1766) ===
Q mean: -45.265018
Q std: 23.710072
Actor loss: 45.269001
Action reg: 0.003983
  l1.weight: grad_norm = 0.000373
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.000967
Total gradient norm: 0.002635
=== Actor Training Debug (Iteration 1767) ===
Q mean: -44.196419
Q std: 21.691828
Actor loss: 44.200397
Action reg: 0.003980
  l1.weight: grad_norm = 0.000269
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.001117
Total gradient norm: 0.003542
=== Actor Training Debug (Iteration 1768) ===
Q mean: -43.730267
Q std: 20.667515
Actor loss: 43.734268
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1769) ===
Q mean: -41.897415
Q std: 21.773825
Actor loss: 41.901382
Action reg: 0.003969
  l1.weight: grad_norm = 0.000356
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.001650
Total gradient norm: 0.005396
=== Actor Training Debug (Iteration 1770) ===
Q mean: -45.099823
Q std: 19.791304
Actor loss: 45.103809
Action reg: 0.003988
  l1.weight: grad_norm = 0.000232
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.000856
Total gradient norm: 0.002657
=== Actor Training Debug (Iteration 1771) ===
Q mean: -43.755783
Q std: 22.120333
Actor loss: 43.759743
Action reg: 0.003959
  l1.weight: grad_norm = 0.000492
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.002211
Total gradient norm: 0.007054
=== Actor Training Debug (Iteration 1772) ===
Q mean: -44.767826
Q std: 22.000650
Actor loss: 44.771820
Action reg: 0.003995
  l1.weight: grad_norm = 0.000136
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.000337
Total gradient norm: 0.000898
=== Actor Training Debug (Iteration 1773) ===
Q mean: -47.302486
Q std: 23.009026
Actor loss: 47.306488
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1774) ===
Q mean: -44.611607
Q std: 22.515467
Actor loss: 44.615597
Action reg: 0.003989
  l1.weight: grad_norm = 0.000217
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.000710
Total gradient norm: 0.002178
=== Actor Training Debug (Iteration 1775) ===
Q mean: -42.267075
Q std: 20.279730
Actor loss: 42.271076
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1776) ===
Q mean: -44.469086
Q std: 22.237450
Actor loss: 44.473080
Action reg: 0.003995
  l1.weight: grad_norm = 0.000189
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.000521
Total gradient norm: 0.001401
=== Actor Training Debug (Iteration 1777) ===
Q mean: -45.063793
Q std: 21.028391
Actor loss: 45.067780
Action reg: 0.003987
  l1.weight: grad_norm = 0.000256
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.000917
Total gradient norm: 0.002930
=== Actor Training Debug (Iteration 1778) ===
Q mean: -46.273121
Q std: 23.294392
Actor loss: 46.277107
Action reg: 0.003986
  l1.weight: grad_norm = 0.000215
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.000750
Total gradient norm: 0.002355
=== Actor Training Debug (Iteration 1779) ===
Q mean: -42.364208
Q std: 22.230280
Actor loss: 42.368191
Action reg: 0.003981
  l1.weight: grad_norm = 0.000218
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.000992
Total gradient norm: 0.003175
=== Actor Training Debug (Iteration 1780) ===
Q mean: -44.906437
Q std: 21.383087
Actor loss: 44.910427
Action reg: 0.003989
  l1.weight: grad_norm = 0.000296
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.000726
Total gradient norm: 0.001975
=== Actor Training Debug (Iteration 1781) ===
Q mean: -45.638077
Q std: 22.633965
Actor loss: 45.642063
Action reg: 0.003987
  l1.weight: grad_norm = 0.000272
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.000775
Total gradient norm: 0.002267
=== Actor Training Debug (Iteration 1782) ===
Q mean: -45.914001
Q std: 22.507772
Actor loss: 45.917995
Action reg: 0.003994
  l1.weight: grad_norm = 0.000145
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.000367
Total gradient norm: 0.001007
=== Actor Training Debug (Iteration 1783) ===
Q mean: -41.371399
Q std: 21.350378
Actor loss: 41.375385
Action reg: 0.003986
  l1.weight: grad_norm = 0.000273
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.000904
Total gradient norm: 0.002773
=== Actor Training Debug (Iteration 1784) ===
Q mean: -43.023384
Q std: 22.619162
Actor loss: 43.027378
Action reg: 0.003996
  l1.weight: grad_norm = 0.000097
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.000264
Total gradient norm: 0.000711
=== Actor Training Debug (Iteration 1785) ===
Q mean: -48.555588
Q std: 23.495255
Actor loss: 48.559578
Action reg: 0.003988
  l1.weight: grad_norm = 0.000205
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.000633
Total gradient norm: 0.001834
=== Actor Training Debug (Iteration 1786) ===
Q mean: -50.772308
Q std: 23.081079
Actor loss: 50.776302
Action reg: 0.003995
  l1.weight: grad_norm = 0.000124
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.000403
Total gradient norm: 0.001074
=== Actor Training Debug (Iteration 1787) ===
Q mean: -39.483635
Q std: 22.326452
Actor loss: 39.487637
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1788) ===
Q mean: -41.283043
Q std: 21.820032
Actor loss: 41.287025
Action reg: 0.003984
  l1.weight: grad_norm = 0.000347
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.001035
Total gradient norm: 0.003087
=== Actor Training Debug (Iteration 1789) ===
Q mean: -43.315475
Q std: 21.581907
Actor loss: 43.319469
Action reg: 0.003996
  l1.weight: grad_norm = 0.000179
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.000406
Total gradient norm: 0.000973
=== Actor Training Debug (Iteration 1790) ===
Q mean: -48.325386
Q std: 21.167967
Actor loss: 48.329376
Action reg: 0.003990
  l1.weight: grad_norm = 0.000207
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.000624
Total gradient norm: 0.001818
=== Actor Training Debug (Iteration 1791) ===
Q mean: -46.695515
Q std: 23.139956
Actor loss: 46.699497
Action reg: 0.003983
  l1.weight: grad_norm = 0.000290
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.000955
Total gradient norm: 0.002747
=== Actor Training Debug (Iteration 1792) ===
Q mean: -47.249344
Q std: 21.964838
Actor loss: 47.253334
Action reg: 0.003991
  l1.weight: grad_norm = 0.000196
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.000603
Total gradient norm: 0.001742
=== Actor Training Debug (Iteration 1793) ===
Q mean: -42.367714
Q std: 21.011614
Actor loss: 42.371708
Action reg: 0.003995
  l1.weight: grad_norm = 0.000171
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.000418
Total gradient norm: 0.001060
=== Actor Training Debug (Iteration 1794) ===
Q mean: -40.156723
Q std: 22.266134
Actor loss: 40.160713
Action reg: 0.003989
  l1.weight: grad_norm = 0.000179
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.000591
Total gradient norm: 0.001817
=== Actor Training Debug (Iteration 1795) ===
Q mean: -43.441963
Q std: 20.107143
Actor loss: 43.445942
Action reg: 0.003978
  l1.weight: grad_norm = 0.000382
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.001340
Total gradient norm: 0.003965
=== Actor Training Debug (Iteration 1796) ===
Q mean: -48.467484
Q std: 24.962339
Actor loss: 48.471474
Action reg: 0.003991
  l1.weight: grad_norm = 0.000213
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.000627
Total gradient norm: 0.001839
=== Actor Training Debug (Iteration 1797) ===
Q mean: -50.697418
Q std: 22.966812
Actor loss: 50.701405
Action reg: 0.003987
  l1.weight: grad_norm = 0.000271
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.000734
Total gradient norm: 0.002148
=== Actor Training Debug (Iteration 1798) ===
Q mean: -46.268456
Q std: 20.749125
Actor loss: 46.272457
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1799) ===
Q mean: -37.977188
Q std: 19.088844
Actor loss: 37.981174
Action reg: 0.003987
  l1.weight: grad_norm = 0.000302
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.001040
Total gradient norm: 0.003244
=== Actor Training Debug (Iteration 1800) ===
Q mean: -36.795586
Q std: 19.503792
Actor loss: 36.799568
Action reg: 0.003981
  l1.weight: grad_norm = 0.000240
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.000980
Total gradient norm: 0.003090
=== Actor Training Debug (Iteration 1801) ===
Q mean: -40.621651
Q std: 18.668127
Actor loss: 40.625645
Action reg: 0.003993
  l1.weight: grad_norm = 0.000236
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.000553
Total gradient norm: 0.001385
=== Actor Training Debug (Iteration 1802) ===
Q mean: -45.565311
Q std: 21.476419
Actor loss: 45.569302
Action reg: 0.003991
  l1.weight: grad_norm = 0.000204
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.000524
Total gradient norm: 0.001400
=== Actor Training Debug (Iteration 1803) ===
Q mean: -48.923172
Q std: 22.923536
Actor loss: 48.927158
Action reg: 0.003988
  l1.weight: grad_norm = 0.000264
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.000888
Total gradient norm: 0.002738
=== Actor Training Debug (Iteration 1804) ===
Q mean: -45.405655
Q std: 22.065338
Actor loss: 45.409630
Action reg: 0.003974
  l1.weight: grad_norm = 0.000316
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.001445
Total gradient norm: 0.004773
=== Actor Training Debug (Iteration 1805) ===
Q mean: -42.906746
Q std: 22.885363
Actor loss: 42.910736
Action reg: 0.003991
  l1.weight: grad_norm = 0.000137
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.000503
Total gradient norm: 0.001521
=== Actor Training Debug (Iteration 1806) ===
Q mean: -40.460346
Q std: 21.787767
Actor loss: 40.464333
Action reg: 0.003987
  l1.weight: grad_norm = 0.000280
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.000976
Total gradient norm: 0.003055
=== Actor Training Debug (Iteration 1807) ===
Q mean: -42.546974
Q std: 20.560741
Actor loss: 42.550961
Action reg: 0.003986
  l1.weight: grad_norm = 0.000266
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.000936
Total gradient norm: 0.002722
=== Actor Training Debug (Iteration 1808) ===
Q mean: -46.226418
Q std: 22.262190
Actor loss: 46.230412
Action reg: 0.003995
  l1.weight: grad_norm = 0.000152
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.000420
Total gradient norm: 0.001129
=== Actor Training Debug (Iteration 1809) ===
Q mean: -47.571751
Q std: 22.942133
Actor loss: 47.575741
Action reg: 0.003992
  l1.weight: grad_norm = 0.000237
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.000707
Total gradient norm: 0.001949
=== Actor Training Debug (Iteration 1810) ===
Q mean: -47.889179
Q std: 21.643909
Actor loss: 47.893169
Action reg: 0.003990
  l1.weight: grad_norm = 0.000271
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.000738
Total gradient norm: 0.002140
=== Actor Training Debug (Iteration 1811) ===
Q mean: -41.787380
Q std: 21.845217
Actor loss: 41.791359
Action reg: 0.003977
  l1.weight: grad_norm = 0.000277
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.001166
Total gradient norm: 0.003705
=== Actor Training Debug (Iteration 1812) ===
Q mean: -41.317337
Q std: 20.502663
Actor loss: 41.321320
Action reg: 0.003981
  l1.weight: grad_norm = 0.000362
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.001250
Total gradient norm: 0.003863
=== Actor Training Debug (Iteration 1813) ===
Q mean: -41.798878
Q std: 20.723106
Actor loss: 41.802864
Action reg: 0.003986
  l1.weight: grad_norm = 0.000295
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.000925
Total gradient norm: 0.002831
=== Actor Training Debug (Iteration 1814) ===
Q mean: -47.209671
Q std: 21.456154
Actor loss: 47.213654
Action reg: 0.003984
  l1.weight: grad_norm = 0.000291
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.000925
Total gradient norm: 0.002754
=== Actor Training Debug (Iteration 1815) ===
Q mean: -49.029808
Q std: 21.650301
Actor loss: 49.033794
Action reg: 0.003986
  l1.weight: grad_norm = 0.000225
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.000862
Total gradient norm: 0.002706
=== Actor Training Debug (Iteration 1816) ===
Q mean: -49.667965
Q std: 21.940418
Actor loss: 49.671955
Action reg: 0.003990
  l1.weight: grad_norm = 0.000224
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.000700
Total gradient norm: 0.002045
=== Actor Training Debug (Iteration 1817) ===
Q mean: -43.220345
Q std: 20.611855
Actor loss: 43.224323
Action reg: 0.003978
  l1.weight: grad_norm = 0.000278
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.001130
Total gradient norm: 0.003654
=== Actor Training Debug (Iteration 1818) ===
Q mean: -41.289833
Q std: 20.037096
Actor loss: 41.293816
Action reg: 0.003982
  l1.weight: grad_norm = 0.000275
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.001082
Total gradient norm: 0.003518
=== Actor Training Debug (Iteration 1819) ===
Q mean: -42.060585
Q std: 20.263769
Actor loss: 42.064568
Action reg: 0.003983
  l1.weight: grad_norm = 0.000241
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.001013
Total gradient norm: 0.003195
=== Actor Training Debug (Iteration 1820) ===
Q mean: -47.926945
Q std: 21.239258
Actor loss: 47.930920
Action reg: 0.003974
  l1.weight: grad_norm = 0.000380
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.001324
Total gradient norm: 0.004167
=== Actor Training Debug (Iteration 1821) ===
Q mean: -48.757942
Q std: 23.283611
Actor loss: 48.761929
Action reg: 0.003986
  l1.weight: grad_norm = 0.000224
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.000834
Total gradient norm: 0.002640
=== Actor Training Debug (Iteration 1822) ===
Q mean: -45.348358
Q std: 21.821968
Actor loss: 45.352348
Action reg: 0.003989
  l1.weight: grad_norm = 0.000224
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.000855
Total gradient norm: 0.002564
=== Actor Training Debug (Iteration 1823) ===
Q mean: -41.651535
Q std: 22.140917
Actor loss: 41.655521
Action reg: 0.003985
  l1.weight: grad_norm = 0.000243
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.000878
Total gradient norm: 0.002642
=== Actor Training Debug (Iteration 1824) ===
Q mean: -41.566330
Q std: 20.454872
Actor loss: 41.570324
Action reg: 0.003996
  l1.weight: grad_norm = 0.000093
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.000304
Total gradient norm: 0.000822
=== Actor Training Debug (Iteration 1825) ===
Q mean: -42.322536
Q std: 20.999838
Actor loss: 42.326534
Action reg: 0.003996
  l1.weight: grad_norm = 0.000197
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.000495
Total gradient norm: 0.001242
=== Actor Training Debug (Iteration 1826) ===
Q mean: -47.047421
Q std: 22.663130
Actor loss: 47.051411
Action reg: 0.003990
  l1.weight: grad_norm = 0.000153
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.000644
Total gradient norm: 0.001901
=== Actor Training Debug (Iteration 1827) ===
Q mean: -44.770073
Q std: 20.201021
Actor loss: 44.774059
Action reg: 0.003986
  l1.weight: grad_norm = 0.000194
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.000756
Total gradient norm: 0.002314
=== Actor Training Debug (Iteration 1828) ===
Q mean: -43.275913
Q std: 19.602661
Actor loss: 43.279915
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1829) ===
Q mean: -42.489624
Q std: 21.191196
Actor loss: 42.493603
Action reg: 0.003978
  l1.weight: grad_norm = 0.000381
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.001340
Total gradient norm: 0.004215
=== Actor Training Debug (Iteration 1830) ===
Q mean: -44.874573
Q std: 22.386942
Actor loss: 44.878567
Action reg: 0.003994
  l1.weight: grad_norm = 0.000210
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.000504
Total gradient norm: 0.001253
=== Actor Training Debug (Iteration 1831) ===
Q mean: -47.757450
Q std: 23.499187
Actor loss: 47.761421
Action reg: 0.003972
  l1.weight: grad_norm = 0.000341
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.001540
Total gradient norm: 0.005070
=== Actor Training Debug (Iteration 1832) ===
Q mean: -46.231163
Q std: 23.103651
Actor loss: 46.235149
Action reg: 0.003987
  l1.weight: grad_norm = 0.000225
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.000795
Total gradient norm: 0.002516
=== Actor Training Debug (Iteration 1833) ===
Q mean: -43.097832
Q std: 24.511953
Actor loss: 43.101807
Action reg: 0.003975
  l1.weight: grad_norm = 0.000390
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.001532
Total gradient norm: 0.004940
=== Actor Training Debug (Iteration 1834) ===
Q mean: -42.175995
Q std: 21.966352
Actor loss: 42.179989
Action reg: 0.003995
  l1.weight: grad_norm = 0.000153
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.000412
Total gradient norm: 0.001106
=== Actor Training Debug (Iteration 1835) ===
Q mean: -45.015621
Q std: 22.643900
Actor loss: 45.019608
Action reg: 0.003986
  l1.weight: grad_norm = 0.000271
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.000974
Total gradient norm: 0.003069
=== Actor Training Debug (Iteration 1836) ===
Q mean: -47.352867
Q std: 20.194094
Actor loss: 47.356846
Action reg: 0.003980
  l1.weight: grad_norm = 0.000301
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.001169
Total gradient norm: 0.003554
=== Actor Training Debug (Iteration 1837) ===
Q mean: -48.274586
Q std: 22.225416
Actor loss: 48.278572
Action reg: 0.003986
  l1.weight: grad_norm = 0.000337
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.001114
Total gradient norm: 0.003395
=== Actor Training Debug (Iteration 1838) ===
Q mean: -45.095169
Q std: 22.093618
Actor loss: 45.099155
Action reg: 0.003986
  l1.weight: grad_norm = 0.000310
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.000954
Total gradient norm: 0.002954
=== Actor Training Debug (Iteration 1839) ===
Q mean: -40.851799
Q std: 21.504505
Actor loss: 40.855785
Action reg: 0.003985
  l1.weight: grad_norm = 0.000253
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.000828
Total gradient norm: 0.002494
=== Actor Training Debug (Iteration 1840) ===
Q mean: -39.634289
Q std: 21.788109
Actor loss: 39.638271
Action reg: 0.003984
  l1.weight: grad_norm = 0.000304
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.000969
Total gradient norm: 0.002875
=== Actor Training Debug (Iteration 1841) ===
Q mean: -43.497829
Q std: 23.192970
Actor loss: 43.501812
Action reg: 0.003981
  l1.weight: grad_norm = 0.000273
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.001032
Total gradient norm: 0.003300
=== Actor Training Debug (Iteration 1842) ===
Q mean: -46.850204
Q std: 21.907057
Actor loss: 46.854191
Action reg: 0.003985
  l1.weight: grad_norm = 0.000216
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.000746
Total gradient norm: 0.002328
=== Actor Training Debug (Iteration 1843) ===
Q mean: -47.541153
Q std: 22.138351
Actor loss: 47.545132
Action reg: 0.003979
  l1.weight: grad_norm = 0.000361
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.001132
Total gradient norm: 0.003427
=== Actor Training Debug (Iteration 1844) ===
Q mean: -44.537170
Q std: 20.714642
Actor loss: 44.541153
Action reg: 0.003984
  l1.weight: grad_norm = 0.000318
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.001137
Total gradient norm: 0.003503
=== Actor Training Debug (Iteration 1845) ===
Q mean: -41.871544
Q std: 20.156380
Actor loss: 41.875530
Action reg: 0.003988
  l1.weight: grad_norm = 0.000145
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.000552
Total gradient norm: 0.001747
=== Actor Training Debug (Iteration 1846) ===
Q mean: -41.376068
Q std: 21.496778
Actor loss: 41.380058
Action reg: 0.003990
  l1.weight: grad_norm = 0.000155
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.000545
Total gradient norm: 0.001719
=== Actor Training Debug (Iteration 1847) ===
Q mean: -43.459690
Q std: 21.961012
Actor loss: 43.463676
Action reg: 0.003986
  l1.weight: grad_norm = 0.000243
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.000862
Total gradient norm: 0.002756
=== Actor Training Debug (Iteration 1848) ===
Q mean: -47.324894
Q std: 23.598415
Actor loss: 47.328869
Action reg: 0.003975
  l1.weight: grad_norm = 0.000359
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.001464
Total gradient norm: 0.004749
=== Actor Training Debug (Iteration 1849) ===
Q mean: -48.146660
Q std: 22.699894
Actor loss: 48.150650
Action reg: 0.003991
  l1.weight: grad_norm = 0.000190
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.000598
Total gradient norm: 0.001804
=== Actor Training Debug (Iteration 1850) ===
Q mean: -42.619949
Q std: 21.443209
Actor loss: 42.623951
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1851) ===
Q mean: -43.671707
Q std: 21.348999
Actor loss: 43.675701
Action reg: 0.003993
  l1.weight: grad_norm = 0.000207
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.000475
Total gradient norm: 0.001180
=== Actor Training Debug (Iteration 1852) ===
Q mean: -41.504429
Q std: 21.760094
Actor loss: 41.508404
Action reg: 0.003976
  l1.weight: grad_norm = 0.000445
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.001753
Total gradient norm: 0.005476
=== Actor Training Debug (Iteration 1853) ===
Q mean: -49.985023
Q std: 24.397371
Actor loss: 49.989017
Action reg: 0.003995
  l1.weight: grad_norm = 0.000162
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.000421
Total gradient norm: 0.001107
=== Actor Training Debug (Iteration 1854) ===
Q mean: -48.721642
Q std: 24.180893
Actor loss: 48.725632
Action reg: 0.003991
  l1.weight: grad_norm = 0.000163
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.000555
Total gradient norm: 0.001711
=== Actor Training Debug (Iteration 1855) ===
Q mean: -45.651421
Q std: 23.809092
Actor loss: 45.655415
Action reg: 0.003995
  l1.weight: grad_norm = 0.000156
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.000406
Total gradient norm: 0.001074
=== Actor Training Debug (Iteration 1856) ===
Q mean: -42.519424
Q std: 23.494116
Actor loss: 42.523415
Action reg: 0.003992
  l1.weight: grad_norm = 0.000205
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.000664
Total gradient norm: 0.002071
=== Actor Training Debug (Iteration 1857) ===
Q mean: -43.798317
Q std: 23.948013
Actor loss: 43.802307
Action reg: 0.003991
  l1.weight: grad_norm = 0.000196
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.000563
Total gradient norm: 0.001689
=== Actor Training Debug (Iteration 1858) ===
Q mean: -42.610340
Q std: 21.297224
Actor loss: 42.614326
Action reg: 0.003986
  l1.weight: grad_norm = 0.000307
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.000934
Total gradient norm: 0.002913
=== Actor Training Debug (Iteration 1859) ===
Q mean: -49.048233
Q std: 24.617968
Actor loss: 49.052212
Action reg: 0.003979
  l1.weight: grad_norm = 0.000312
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.001124
Total gradient norm: 0.003581
=== Actor Training Debug (Iteration 1860) ===
Q mean: -50.609486
Q std: 23.074217
Actor loss: 50.613480
Action reg: 0.003994
  l1.weight: grad_norm = 0.000206
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.000495
Total gradient norm: 0.001259
=== Actor Training Debug (Iteration 1861) ===
Q mean: -49.340416
Q std: 23.771355
Actor loss: 49.344406
Action reg: 0.003989
  l1.weight: grad_norm = 0.000199
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.000611
Total gradient norm: 0.001787
=== Actor Training Debug (Iteration 1862) ===
Q mean: -44.589588
Q std: 21.390656
Actor loss: 44.593578
Action reg: 0.003989
  l1.weight: grad_norm = 0.000287
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.000904
Total gradient norm: 0.002711
=== Actor Training Debug (Iteration 1863) ===
Q mean: -42.810677
Q std: 22.136761
Actor loss: 42.814671
Action reg: 0.003992
  l1.weight: grad_norm = 0.000218
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.000573
Total gradient norm: 0.001682
=== Actor Training Debug (Iteration 1864) ===
Q mean: -45.438293
Q std: 23.192324
Actor loss: 45.442284
Action reg: 0.003990
  l1.weight: grad_norm = 0.000227
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.000680
Total gradient norm: 0.001994
=== Actor Training Debug (Iteration 1865) ===
Q mean: -47.197918
Q std: 23.003563
Actor loss: 47.201904
Action reg: 0.003986
  l1.weight: grad_norm = 0.000226
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.000828
Total gradient norm: 0.002680
=== Actor Training Debug (Iteration 1866) ===
Q mean: -48.285660
Q std: 20.956427
Actor loss: 48.289661
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1867) ===
Q mean: -46.434559
Q std: 22.889112
Actor loss: 46.438553
Action reg: 0.003994
  l1.weight: grad_norm = 0.000166
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.000466
Total gradient norm: 0.001266
=== Actor Training Debug (Iteration 1868) ===
Q mean: -42.449959
Q std: 21.144899
Actor loss: 42.453930
Action reg: 0.003969
  l1.weight: grad_norm = 0.000381
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.001814
Total gradient norm: 0.006080
=== Actor Training Debug (Iteration 1869) ===
Q mean: -42.393135
Q std: 20.589931
Actor loss: 42.397125
Action reg: 0.003992
  l1.weight: grad_norm = 0.000230
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.000663
Total gradient norm: 0.002005
=== Actor Training Debug (Iteration 1870) ===
Q mean: -45.579765
Q std: 23.533947
Actor loss: 45.583740
Action reg: 0.003975
  l1.weight: grad_norm = 0.000225
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.001224
Total gradient norm: 0.004224
=== Actor Training Debug (Iteration 1871) ===
Q mean: -47.572056
Q std: 23.356316
Actor loss: 47.576050
Action reg: 0.003995
  l1.weight: grad_norm = 0.000146
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.000392
Total gradient norm: 0.001066
=== Actor Training Debug (Iteration 1872) ===
Q mean: -42.370838
Q std: 22.370012
Actor loss: 42.374817
Action reg: 0.003979
  l1.weight: grad_norm = 0.000322
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.001225
Total gradient norm: 0.003720
=== Actor Training Debug (Iteration 1873) ===
Q mean: -44.485909
Q std: 21.561047
Actor loss: 44.489887
Action reg: 0.003980
  l1.weight: grad_norm = 0.000291
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.001168
Total gradient norm: 0.003839
=== Actor Training Debug (Iteration 1874) ===
Q mean: -44.259731
Q std: 21.786873
Actor loss: 44.263721
Action reg: 0.003990
  l1.weight: grad_norm = 0.000179
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.000621
Total gradient norm: 0.001862
=== Actor Training Debug (Iteration 1875) ===
Q mean: -45.707298
Q std: 21.921335
Actor loss: 45.711269
Action reg: 0.003970
  l1.weight: grad_norm = 0.000358
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.001688
Total gradient norm: 0.005574
=== Actor Training Debug (Iteration 1876) ===
Q mean: -47.959831
Q std: 23.021416
Actor loss: 47.963821
Action reg: 0.003990
  l1.weight: grad_norm = 0.000176
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.000650
Total gradient norm: 0.001885
=== Actor Training Debug (Iteration 1877) ===
Q mean: -46.202400
Q std: 23.384584
Actor loss: 46.206390
Action reg: 0.003991
  l1.weight: grad_norm = 0.000220
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.000619
Total gradient norm: 0.001870
=== Actor Training Debug (Iteration 1878) ===
Q mean: -44.146858
Q std: 22.010962
Actor loss: 44.150852
Action reg: 0.003996
  l1.weight: grad_norm = 0.000141
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.000353
Total gradient norm: 0.000989
=== Actor Training Debug (Iteration 1879) ===
Q mean: -44.718353
Q std: 23.049559
Actor loss: 44.722347
Action reg: 0.003993
  l1.weight: grad_norm = 0.000189
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.000631
Total gradient norm: 0.001958
=== Actor Training Debug (Iteration 1880) ===
Q mean: -49.848091
Q std: 23.718702
Actor loss: 49.852081
Action reg: 0.003991
  l1.weight: grad_norm = 0.000162
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.000575
Total gradient norm: 0.001783
=== Actor Training Debug (Iteration 1881) ===
Q mean: -47.029938
Q std: 23.533695
Actor loss: 47.033916
Action reg: 0.003979
  l1.weight: grad_norm = 0.000282
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.001288
Total gradient norm: 0.004177
=== Actor Training Debug (Iteration 1882) ===
Q mean: -44.616589
Q std: 21.113281
Actor loss: 44.620586
Action reg: 0.003996
  l1.weight: grad_norm = 0.000087
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.000219
Total gradient norm: 0.000614
=== Actor Training Debug (Iteration 1883) ===
Q mean: -42.632347
Q std: 21.026726
Actor loss: 42.636322
Action reg: 0.003976
  l1.weight: grad_norm = 0.000304
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.001344
Total gradient norm: 0.004373
=== Actor Training Debug (Iteration 1884) ===
Q mean: -44.010033
Q std: 22.020979
Actor loss: 44.014015
Action reg: 0.003983
  l1.weight: grad_norm = 0.000338
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.001125
Total gradient norm: 0.003327
=== Actor Training Debug (Iteration 1885) ===
Q mean: -45.992512
Q std: 20.912718
Actor loss: 45.996502
Action reg: 0.003990
  l1.weight: grad_norm = 0.000181
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.000687
Total gradient norm: 0.002052
=== Actor Training Debug (Iteration 1886) ===
Q mean: -45.407913
Q std: 23.044348
Actor loss: 45.411896
Action reg: 0.003982
  l1.weight: grad_norm = 0.000271
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.001079
Total gradient norm: 0.003380
=== Actor Training Debug (Iteration 1887) ===
Q mean: -46.291336
Q std: 22.782711
Actor loss: 46.295322
Action reg: 0.003986
  l1.weight: grad_norm = 0.000249
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.000903
Total gradient norm: 0.002883
=== Actor Training Debug (Iteration 1888) ===
Q mean: -46.074905
Q std: 24.329939
Actor loss: 46.078888
Action reg: 0.003984
  l1.weight: grad_norm = 0.000283
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.000883
Total gradient norm: 0.002700
=== Actor Training Debug (Iteration 1889) ===
Q mean: -46.165462
Q std: 22.478647
Actor loss: 46.169445
Action reg: 0.003984
  l1.weight: grad_norm = 0.000222
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.000911
Total gradient norm: 0.002917
=== Actor Training Debug (Iteration 1890) ===
Q mean: -45.583035
Q std: 19.553833
Actor loss: 45.587029
Action reg: 0.003994
  l1.weight: grad_norm = 0.000165
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.000472
Total gradient norm: 0.001284
=== Actor Training Debug (Iteration 1891) ===
Q mean: -45.578049
Q std: 23.047369
Actor loss: 45.582043
Action reg: 0.003995
  l1.weight: grad_norm = 0.000156
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.000436
Total gradient norm: 0.001099
=== Actor Training Debug (Iteration 1892) ===
Q mean: -46.664909
Q std: 21.773678
Actor loss: 46.668896
Action reg: 0.003987
  l1.weight: grad_norm = 0.000201
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.000831
Total gradient norm: 0.002787
=== Actor Training Debug (Iteration 1893) ===
Q mean: -46.200539
Q std: 21.841049
Actor loss: 46.204540
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1894) ===
Q mean: -43.063168
Q std: 21.367302
Actor loss: 43.067146
Action reg: 0.003980
  l1.weight: grad_norm = 0.000227
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.001019
Total gradient norm: 0.003348
=== Actor Training Debug (Iteration 1895) ===
Q mean: -43.359173
Q std: 20.291950
Actor loss: 43.363159
Action reg: 0.003986
  l1.weight: grad_norm = 0.000222
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.000821
Total gradient norm: 0.002615
=== Actor Training Debug (Iteration 1896) ===
Q mean: -44.482719
Q std: 21.775633
Actor loss: 44.486710
Action reg: 0.003991
  l1.weight: grad_norm = 0.000190
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.000589
Total gradient norm: 0.001791
=== Actor Training Debug (Iteration 1897) ===
Q mean: -45.141823
Q std: 22.977098
Actor loss: 45.145805
Action reg: 0.003984
  l1.weight: grad_norm = 0.000194
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.000819
Total gradient norm: 0.002760
=== Actor Training Debug (Iteration 1898) ===
Q mean: -45.305496
Q std: 23.456692
Actor loss: 45.309479
Action reg: 0.003982
  l1.weight: grad_norm = 0.000291
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.001189
Total gradient norm: 0.003775
=== Actor Training Debug (Iteration 1899) ===
Q mean: -47.259598
Q std: 22.271910
Actor loss: 47.263599
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1900) ===
Q mean: -46.402390
Q std: 23.147940
Actor loss: 46.406372
Action reg: 0.003983
  l1.weight: grad_norm = 0.000297
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.001169
Total gradient norm: 0.003835
=== Actor Training Debug (Iteration 1901) ===
Q mean: -43.901764
Q std: 22.311457
Actor loss: 43.905750
Action reg: 0.003986
  l1.weight: grad_norm = 0.000328
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.000986
Total gradient norm: 0.002955
=== Actor Training Debug (Iteration 1902) ===
Q mean: -45.875305
Q std: 23.269339
Actor loss: 45.879292
Action reg: 0.003987
  l1.weight: grad_norm = 0.000182
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.000742
Total gradient norm: 0.002414
=== Actor Training Debug (Iteration 1903) ===
Q mean: -45.133114
Q std: 20.963398
Actor loss: 45.137115
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1904) ===
Q mean: -47.780277
Q std: 20.948853
Actor loss: 47.784264
Action reg: 0.003986
  l1.weight: grad_norm = 0.000255
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.000970
Total gradient norm: 0.003176
=== Actor Training Debug (Iteration 1905) ===
Q mean: -47.984547
Q std: 21.619181
Actor loss: 47.988548
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1906) ===
Q mean: -45.869701
Q std: 21.846106
Actor loss: 45.873695
Action reg: 0.003993
  l1.weight: grad_norm = 0.000178
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.000508
Total gradient norm: 0.001490
=== Actor Training Debug (Iteration 1907) ===
Q mean: -47.884983
Q std: 23.049465
Actor loss: 47.888973
Action reg: 0.003992
  l1.weight: grad_norm = 0.000195
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.000692
Total gradient norm: 0.002103
=== Actor Training Debug (Iteration 1908) ===
Q mean: -45.251583
Q std: 23.590858
Actor loss: 45.255573
Action reg: 0.003991
  l1.weight: grad_norm = 0.000182
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.000602
Total gradient norm: 0.001832
=== Actor Training Debug (Iteration 1909) ===
Q mean: -44.450150
Q std: 22.381084
Actor loss: 44.454136
Action reg: 0.003985
  l1.weight: grad_norm = 0.000194
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.000899
Total gradient norm: 0.003072
=== Actor Training Debug (Iteration 1910) ===
Q mean: -43.363762
Q std: 20.323008
Actor loss: 43.367764
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1911) ===
Q mean: -49.293716
Q std: 21.472441
Actor loss: 49.297718
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1912) ===
Q mean: -47.844494
Q std: 23.251518
Actor loss: 47.848473
Action reg: 0.003978
  l1.weight: grad_norm = 0.000331
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.001322
Total gradient norm: 0.004344
=== Actor Training Debug (Iteration 1913) ===
Q mean: -43.620613
Q std: 21.863045
Actor loss: 43.624596
Action reg: 0.003983
  l1.weight: grad_norm = 0.000319
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.001178
Total gradient norm: 0.003839
=== Actor Training Debug (Iteration 1914) ===
Q mean: -45.782150
Q std: 22.653858
Actor loss: 45.786129
Action reg: 0.003980
  l1.weight: grad_norm = 0.000361
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.001328
Total gradient norm: 0.004338
=== Actor Training Debug (Iteration 1915) ===
Q mean: -47.997162
Q std: 24.013090
Actor loss: 48.001148
Action reg: 0.003986
  l1.weight: grad_norm = 0.000213
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.000858
Total gradient norm: 0.002677
=== Actor Training Debug (Iteration 1916) ===
Q mean: -46.633347
Q std: 24.345783
Actor loss: 46.637333
Action reg: 0.003986
  l1.weight: grad_norm = 0.000241
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.001040
Total gradient norm: 0.003351
=== Actor Training Debug (Iteration 1917) ===
Q mean: -45.924900
Q std: 22.053516
Actor loss: 45.928898
Action reg: 0.003997
  l1.weight: grad_norm = 0.000205
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.000556
Total gradient norm: 0.001412
=== Actor Training Debug (Iteration 1918) ===
Q mean: -45.917183
Q std: 22.915764
Actor loss: 45.921173
Action reg: 0.003991
  l1.weight: grad_norm = 0.000168
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.000646
Total gradient norm: 0.002080
=== Actor Training Debug (Iteration 1919) ===
Q mean: -47.240349
Q std: 22.824930
Actor loss: 47.244335
Action reg: 0.003986
  l1.weight: grad_norm = 0.000219
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.000878
Total gradient norm: 0.002874
=== Actor Training Debug (Iteration 1920) ===
Q mean: -47.954632
Q std: 20.983742
Actor loss: 47.958622
Action reg: 0.003989
  l1.weight: grad_norm = 0.000245
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.000739
Total gradient norm: 0.002020
=== Actor Training Debug (Iteration 1921) ===
Q mean: -44.788376
Q std: 22.627743
Actor loss: 44.792366
Action reg: 0.003991
  l1.weight: grad_norm = 0.000200
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.000706
Total gradient norm: 0.002229
=== Actor Training Debug (Iteration 1922) ===
Q mean: -48.888084
Q std: 23.691637
Actor loss: 48.892078
Action reg: 0.003995
  l1.weight: grad_norm = 0.000168
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.000457
Total gradient norm: 0.001210
=== Actor Training Debug (Iteration 1923) ===
Q mean: -47.386131
Q std: 24.234205
Actor loss: 47.390118
Action reg: 0.003987
  l1.weight: grad_norm = 0.000219
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.000834
Total gradient norm: 0.002753
=== Actor Training Debug (Iteration 1924) ===
Q mean: -45.680531
Q std: 22.802582
Actor loss: 45.684509
Action reg: 0.003979
  l1.weight: grad_norm = 0.000353
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.001250
Total gradient norm: 0.003982
=== Actor Training Debug (Iteration 1925) ===
Q mean: -45.997795
Q std: 22.410782
Actor loss: 46.001785
Action reg: 0.003992
  l1.weight: grad_norm = 0.000245
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.000787
Total gradient norm: 0.002299
=== Actor Training Debug (Iteration 1926) ===
Q mean: -47.857590
Q std: 23.391617
Actor loss: 47.861591
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1927) ===
Q mean: -44.245079
Q std: 21.672430
Actor loss: 44.249058
Action reg: 0.003979
  l1.weight: grad_norm = 0.000314
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.001434
Total gradient norm: 0.004795
=== Actor Training Debug (Iteration 1928) ===
Q mean: -43.802876
Q std: 20.804852
Actor loss: 43.806862
Action reg: 0.003985
  l1.weight: grad_norm = 0.000226
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.000863
Total gradient norm: 0.002789
=== Actor Training Debug (Iteration 1929) ===
Q mean: -45.783295
Q std: 22.407593
Actor loss: 45.787277
Action reg: 0.003984
  l1.weight: grad_norm = 0.000346
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.001196
Total gradient norm: 0.003663
=== Actor Training Debug (Iteration 1930) ===
Q mean: -46.630909
Q std: 22.479568
Actor loss: 46.634892
Action reg: 0.003982
  l1.weight: grad_norm = 0.000293
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.001050
Total gradient norm: 0.003475
=== Actor Training Debug (Iteration 1931) ===
Q mean: -46.502014
Q std: 23.526505
Actor loss: 46.506004
Action reg: 0.003991
  l1.weight: grad_norm = 0.000182
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.000675
Total gradient norm: 0.002125
=== Actor Training Debug (Iteration 1932) ===
Q mean: -43.136368
Q std: 21.919611
Actor loss: 43.140362
Action reg: 0.003994
  l1.weight: grad_norm = 0.000165
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.000426
Total gradient norm: 0.001167
=== Actor Training Debug (Iteration 1933) ===
Q mean: -45.888351
Q std: 22.104858
Actor loss: 45.892334
Action reg: 0.003982
  l1.weight: grad_norm = 0.000341
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.001219
Total gradient norm: 0.003928
=== Actor Training Debug (Iteration 1934) ===
Q mean: -49.338554
Q std: 21.670387
Actor loss: 49.342541
Action reg: 0.003986
  l1.weight: grad_norm = 0.000189
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.000797
Total gradient norm: 0.002648
=== Actor Training Debug (Iteration 1935) ===
Q mean: -52.208233
Q std: 22.765888
Actor loss: 52.212227
Action reg: 0.003995
  l1.weight: grad_norm = 0.000111
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.000437
Total gradient norm: 0.001261
=== Actor Training Debug (Iteration 1936) ===
Q mean: -45.728317
Q std: 20.730106
Actor loss: 45.732300
Action reg: 0.003983
  l1.weight: grad_norm = 0.000246
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.001112
Total gradient norm: 0.003719
=== Actor Training Debug (Iteration 1937) ===
Q mean: -45.192589
Q std: 23.033293
Actor loss: 45.196579
Action reg: 0.003990
  l1.weight: grad_norm = 0.000261
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.000780
Total gradient norm: 0.002279
=== Actor Training Debug (Iteration 1938) ===
Q mean: -46.365925
Q std: 21.856812
Actor loss: 46.369919
Action reg: 0.003992
  l1.weight: grad_norm = 0.000192
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.000602
Total gradient norm: 0.001862
=== Actor Training Debug (Iteration 1939) ===
Q mean: -46.869469
Q std: 21.615372
Actor loss: 46.873459
Action reg: 0.003990
  l1.weight: grad_norm = 0.000226
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.000610
Total gradient norm: 0.001831
=== Actor Training Debug (Iteration 1940) ===
Q mean: -45.301117
Q std: 22.890759
Actor loss: 45.305103
Action reg: 0.003987
  l1.weight: grad_norm = 0.000320
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.001064
Total gradient norm: 0.003315
=== Actor Training Debug (Iteration 1941) ===
Q mean: -47.875977
Q std: 22.368975
Actor loss: 47.879967
Action reg: 0.003992
  l1.weight: grad_norm = 0.000177
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.000514
Total gradient norm: 0.001442
=== Actor Training Debug (Iteration 1942) ===
Q mean: -48.742783
Q std: 22.178043
Actor loss: 48.746769
Action reg: 0.003987
  l1.weight: grad_norm = 0.000236
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.000901
Total gradient norm: 0.002908
=== Actor Training Debug (Iteration 1943) ===
Q mean: -47.773361
Q std: 21.598665
Actor loss: 47.777351
Action reg: 0.003992
  l1.weight: grad_norm = 0.000200
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.000661
Total gradient norm: 0.001988
=== Actor Training Debug (Iteration 1944) ===
Q mean: -43.905312
Q std: 21.878901
Actor loss: 43.909294
Action reg: 0.003981
  l1.weight: grad_norm = 0.000351
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.001385
Total gradient norm: 0.004361
=== Actor Training Debug (Iteration 1945) ===
Q mean: -45.129402
Q std: 22.075212
Actor loss: 45.133396
Action reg: 0.003996
  l1.weight: grad_norm = 0.000086
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.000344
Total gradient norm: 0.000989
=== Actor Training Debug (Iteration 1946) ===
Q mean: -49.821442
Q std: 23.177708
Actor loss: 49.825420
Action reg: 0.003980
  l1.weight: grad_norm = 0.000294
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.001369
Total gradient norm: 0.004575
=== Actor Training Debug (Iteration 1947) ===
Q mean: -47.555653
Q std: 23.638830
Actor loss: 47.559639
Action reg: 0.003985
  l1.weight: grad_norm = 0.000269
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.001049
Total gradient norm: 0.003432
=== Actor Training Debug (Iteration 1948) ===
Q mean: -42.590679
Q std: 19.884487
Actor loss: 42.594673
Action reg: 0.003995
  l1.weight: grad_norm = 0.000194
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.000451
Total gradient norm: 0.001215
=== Actor Training Debug (Iteration 1949) ===
Q mean: -46.775322
Q std: 22.844616
Actor loss: 46.779316
Action reg: 0.003995
  l1.weight: grad_norm = 0.000193
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.000451
Total gradient norm: 0.001216
=== Actor Training Debug (Iteration 1950) ===
Q mean: -47.559490
Q std: 21.125120
Actor loss: 47.563492
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1951) ===
Q mean: -46.708073
Q std: 22.536972
Actor loss: 46.712055
Action reg: 0.003982
  l1.weight: grad_norm = 0.000245
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.001175
Total gradient norm: 0.003977
=== Actor Training Debug (Iteration 1952) ===
Q mean: -46.562752
Q std: 20.892859
Actor loss: 46.566734
Action reg: 0.003981
  l1.weight: grad_norm = 0.000263
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.001020
Total gradient norm: 0.003360
=== Actor Training Debug (Iteration 1953) ===
Q mean: -45.497414
Q std: 22.604832
Actor loss: 45.501408
Action reg: 0.003995
  l1.weight: grad_norm = 0.000121
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.000417
Total gradient norm: 0.001180
=== Actor Training Debug (Iteration 1954) ===
Q mean: -42.180882
Q std: 20.786112
Actor loss: 42.184879
Action reg: 0.003998
  l1.weight: grad_norm = 0.000103
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.000324
Total gradient norm: 0.000823
=== Actor Training Debug (Iteration 1955) ===
Q mean: -46.274746
Q std: 23.279757
Actor loss: 46.278725
Action reg: 0.003979
  l1.weight: grad_norm = 0.000331
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.001267
Total gradient norm: 0.003992
=== Actor Training Debug (Iteration 1956) ===
Q mean: -47.812393
Q std: 23.206211
Actor loss: 47.816364
Action reg: 0.003973
  l1.weight: grad_norm = 0.000318
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.001703
Total gradient norm: 0.005905
=== Actor Training Debug (Iteration 1957) ===
Q mean: -43.628494
Q std: 20.394211
Actor loss: 43.632481
Action reg: 0.003987
  l1.weight: grad_norm = 0.000179
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.000795
Total gradient norm: 0.002687
=== Actor Training Debug (Iteration 1958) ===
Q mean: -45.304813
Q std: 22.530506
Actor loss: 45.308807
Action reg: 0.003996
  l1.weight: grad_norm = 0.000084
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.000349
Total gradient norm: 0.001009
=== Actor Training Debug (Iteration 1959) ===
Q mean: -49.475563
Q std: 23.710459
Actor loss: 49.479553
Action reg: 0.003991
  l1.weight: grad_norm = 0.000208
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.000655
Total gradient norm: 0.002055
=== Actor Training Debug (Iteration 1960) ===
Q mean: -48.682495
Q std: 23.629877
Actor loss: 48.686481
Action reg: 0.003986
  l1.weight: grad_norm = 0.000204
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.000930
Total gradient norm: 0.003205
=== Actor Training Debug (Iteration 1961) ===
Q mean: -45.322166
Q std: 23.602304
Actor loss: 45.326153
Action reg: 0.003987
  l1.weight: grad_norm = 0.000232
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.000942
Total gradient norm: 0.002921
=== Actor Training Debug (Iteration 1962) ===
Q mean: -41.236580
Q std: 20.623278
Actor loss: 41.240570
Action reg: 0.003992
  l1.weight: grad_norm = 0.000206
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.000629
Total gradient norm: 0.001941
=== Actor Training Debug (Iteration 1963) ===
Q mean: -47.087524
Q std: 22.990524
Actor loss: 47.091522
Action reg: 0.003996
  l1.weight: grad_norm = 0.000083
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.000259
Total gradient norm: 0.000727
=== Actor Training Debug (Iteration 1964) ===
Q mean: -47.675308
Q std: 23.718561
Actor loss: 47.679295
Action reg: 0.003987
  l1.weight: grad_norm = 0.000256
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.000957
Total gradient norm: 0.003019
=== Actor Training Debug (Iteration 1965) ===
Q mean: -47.153606
Q std: 23.179018
Actor loss: 47.157600
Action reg: 0.003993
  l1.weight: grad_norm = 0.000143
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.000522
Total gradient norm: 0.001663
=== Actor Training Debug (Iteration 1966) ===
Q mean: -44.374550
Q std: 21.322407
Actor loss: 44.378532
Action reg: 0.003981
  l1.weight: grad_norm = 0.000258
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.001182
Total gradient norm: 0.004129
=== Actor Training Debug (Iteration 1967) ===
Q mean: -41.701294
Q std: 19.855391
Actor loss: 41.705280
Action reg: 0.003987
  l1.weight: grad_norm = 0.000271
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.000980
Total gradient norm: 0.003152
=== Actor Training Debug (Iteration 1968) ===
Q mean: -46.425102
Q std: 20.574070
Actor loss: 46.429089
Action reg: 0.003988
  l1.weight: grad_norm = 0.000212
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.000894
Total gradient norm: 0.002825
=== Actor Training Debug (Iteration 1969) ===
Q mean: -48.772446
Q std: 20.910349
Actor loss: 48.776428
Action reg: 0.003983
  l1.weight: grad_norm = 0.000256
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.001194
Total gradient norm: 0.004180
=== Actor Training Debug (Iteration 1970) ===
Q mean: -45.288803
Q std: 22.461287
Actor loss: 45.292793
Action reg: 0.003988
  l1.weight: grad_norm = 0.000265
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.000890
Total gradient norm: 0.002680
=== Actor Training Debug (Iteration 1971) ===
Q mean: -44.108902
Q std: 22.747843
Actor loss: 44.112896
Action reg: 0.003995
  l1.weight: grad_norm = 0.000155
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.000437
Total gradient norm: 0.001219
=== Actor Training Debug (Iteration 1972) ===
Q mean: -48.225475
Q std: 21.580973
Actor loss: 48.229462
Action reg: 0.003985
  l1.weight: grad_norm = 0.000270
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.000973
Total gradient norm: 0.002979
=== Actor Training Debug (Iteration 1973) ===
Q mean: -49.913338
Q std: 23.827469
Actor loss: 49.917324
Action reg: 0.003987
  l1.weight: grad_norm = 0.000190
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.000847
Total gradient norm: 0.002850
=== Actor Training Debug (Iteration 1974) ===
Q mean: -45.883904
Q std: 21.597519
Actor loss: 45.887882
Action reg: 0.003978
  l1.weight: grad_norm = 0.000264
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.001411
Total gradient norm: 0.004942
=== Actor Training Debug (Iteration 1975) ===
Q mean: -42.519779
Q std: 21.917889
Actor loss: 42.523762
Action reg: 0.003981
  l1.weight: grad_norm = 0.000274
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.001202
Total gradient norm: 0.003957
=== Actor Training Debug (Iteration 1976) ===
Q mean: -47.756523
Q std: 23.286526
Actor loss: 47.760498
Action reg: 0.003976
  l1.weight: grad_norm = 0.000357
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.001669
Total gradient norm: 0.005727
=== Actor Training Debug (Iteration 1977) ===
Q mean: -48.004410
Q std: 22.271652
Actor loss: 48.008385
Action reg: 0.003976
  l1.weight: grad_norm = 0.000270
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.001433
Total gradient norm: 0.005002
=== Actor Training Debug (Iteration 1978) ===
Q mean: -44.638321
Q std: 20.354225
Actor loss: 44.642311
Action reg: 0.003991
  l1.weight: grad_norm = 0.000111
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.000609
Total gradient norm: 0.002091
=== Actor Training Debug (Iteration 1979) ===
Q mean: -45.838348
Q std: 23.546427
Actor loss: 45.842342
Action reg: 0.003993
  l1.weight: grad_norm = 0.000208
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.000646
Total gradient norm: 0.001973
=== Actor Training Debug (Iteration 1980) ===
Q mean: -48.020714
Q std: 22.832912
Actor loss: 48.024715
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1981) ===
Q mean: -45.998024
Q std: 21.809383
Actor loss: 46.002018
Action reg: 0.003995
  l1.weight: grad_norm = 0.000170
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.000494
Total gradient norm: 0.001352
=== Actor Training Debug (Iteration 1982) ===
Q mean: -45.864876
Q std: 21.604235
Actor loss: 45.868877
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1983) ===
Q mean: -46.853462
Q std: 23.215759
Actor loss: 46.857433
Action reg: 0.003970
  l1.weight: grad_norm = 0.000378
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.001734
Total gradient norm: 0.005944
=== Actor Training Debug (Iteration 1984) ===
Q mean: -46.647266
Q std: 22.706720
Actor loss: 46.651257
Action reg: 0.003991
  l1.weight: grad_norm = 0.000118
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.000594
Total gradient norm: 0.002045
=== Actor Training Debug (Iteration 1985) ===
Q mean: -44.757324
Q std: 21.053841
Actor loss: 44.761326
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1986) ===
Q mean: -43.929947
Q std: 21.345083
Actor loss: 43.933933
Action reg: 0.003987
  l1.weight: grad_norm = 0.000292
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.001061
Total gradient norm: 0.003372
=== Actor Training Debug (Iteration 1987) ===
Q mean: -45.828033
Q std: 21.279509
Actor loss: 45.832035
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1988) ===
Q mean: -49.513550
Q std: 24.411449
Actor loss: 49.517544
Action reg: 0.003996
  l1.weight: grad_norm = 0.000159
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.000434
Total gradient norm: 0.001241
=== Actor Training Debug (Iteration 1989) ===
Q mean: -51.767433
Q std: 24.511248
Actor loss: 51.771431
Action reg: 0.003996
  l1.weight: grad_norm = 0.000107
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.000308
Total gradient norm: 0.000907
=== Actor Training Debug (Iteration 1990) ===
Q mean: -48.330643
Q std: 23.959106
Actor loss: 48.334625
Action reg: 0.003983
  l1.weight: grad_norm = 0.000295
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.001259
Total gradient norm: 0.004378
=== Actor Training Debug (Iteration 1991) ===
Q mean: -46.541824
Q std: 21.694731
Actor loss: 46.545826
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1992) ===
Q mean: -45.565430
Q std: 21.871090
Actor loss: 45.569408
Action reg: 0.003981
  l1.weight: grad_norm = 0.000262
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.001136
Total gradient norm: 0.003763
=== Actor Training Debug (Iteration 1993) ===
Q mean: -48.430614
Q std: 21.263042
Actor loss: 48.434605
Action reg: 0.003990
  l1.weight: grad_norm = 0.000222
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.000824
Total gradient norm: 0.002552
=== Actor Training Debug (Iteration 1994) ===
Q mean: -47.789291
Q std: 21.282778
Actor loss: 47.793278
Action reg: 0.003987
  l1.weight: grad_norm = 0.000233
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.000888
Total gradient norm: 0.002995
=== Actor Training Debug (Iteration 1995) ===
Q mean: -44.764618
Q std: 24.419317
Actor loss: 44.768600
Action reg: 0.003983
  l1.weight: grad_norm = 0.000260
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.001292
Total gradient norm: 0.004568
=== Actor Training Debug (Iteration 1996) ===
Q mean: -44.163818
Q std: 24.707148
Actor loss: 44.167805
Action reg: 0.003986
  l1.weight: grad_norm = 0.000238
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.000938
Total gradient norm: 0.003030
=== Actor Training Debug (Iteration 1997) ===
Q mean: -48.064915
Q std: 23.355055
Actor loss: 48.068893
Action reg: 0.003979
  l1.weight: grad_norm = 0.000247
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.001307
Total gradient norm: 0.004608
=== Actor Training Debug (Iteration 1998) ===
Q mean: -44.268677
Q std: 22.007793
Actor loss: 44.272659
Action reg: 0.003981
  l1.weight: grad_norm = 0.000290
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.001306
Total gradient norm: 0.004388
=== Actor Training Debug (Iteration 1999) ===
Q mean: -49.261314
Q std: 22.122486
Actor loss: 49.265293
Action reg: 0.003979
  l1.weight: grad_norm = 0.000250
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.001281
Total gradient norm: 0.004519
=== Actor Training Debug (Iteration 2000) ===
Q mean: -44.752724
Q std: 20.211189
Actor loss: 44.756710
Action reg: 0.003988
  l1.weight: grad_norm = 0.000252
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.000881
Total gradient norm: 0.002808
Step 7000: Critic Loss: 6.9706, Actor Loss: 44.7567, Q Value: -44.7527
  Average reward: -361.971 | Average length: 100.0
Evaluation at episode 70: -361.971
=== Actor Training Debug (Iteration 2001) ===
Q mean: -45.818375
Q std: 21.115858
Actor loss: 45.822357
Action reg: 0.003982
  l1.weight: grad_norm = 0.000271
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.001065
Total gradient norm: 0.003455
=== Actor Training Debug (Iteration 2002) ===
Q mean: -49.381641
Q std: 24.839592
Actor loss: 49.385624
Action reg: 0.003983
  l1.weight: grad_norm = 0.000244
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.001082
Total gradient norm: 0.003740
=== Actor Training Debug (Iteration 2003) ===
Q mean: -48.306126
Q std: 24.558245
Actor loss: 48.310108
Action reg: 0.003983
  l1.weight: grad_norm = 0.000229
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.001138
Total gradient norm: 0.003936
=== Actor Training Debug (Iteration 2004) ===
Q mean: -45.996193
Q std: 22.507771
Actor loss: 46.000187
Action reg: 0.003994
  l1.weight: grad_norm = 0.000178
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.000556
Total gradient norm: 0.001678
=== Actor Training Debug (Iteration 2005) ===
Q mean: -45.455162
Q std: 23.501030
Actor loss: 45.459145
Action reg: 0.003981
  l1.weight: grad_norm = 0.000258
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.001190
Total gradient norm: 0.003880
=== Actor Training Debug (Iteration 2006) ===
Q mean: -45.969627
Q std: 23.763136
Actor loss: 45.973606
Action reg: 0.003979
  l1.weight: grad_norm = 0.000275
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.001419
Total gradient norm: 0.005008
=== Actor Training Debug (Iteration 2007) ===
Q mean: -50.269180
Q std: 25.287518
Actor loss: 50.273167
Action reg: 0.003985
  l1.weight: grad_norm = 0.000216
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.000997
Total gradient norm: 0.003392
=== Actor Training Debug (Iteration 2008) ===
Q mean: -48.127392
Q std: 21.263493
Actor loss: 48.131386
Action reg: 0.003995
  l1.weight: grad_norm = 0.000103
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.000425
Total gradient norm: 0.001194
=== Actor Training Debug (Iteration 2009) ===
Q mean: -48.019581
Q std: 21.602726
Actor loss: 48.023567
Action reg: 0.003987
  l1.weight: grad_norm = 0.000197
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.000948
Total gradient norm: 0.003249
=== Actor Training Debug (Iteration 2010) ===
Q mean: -49.251431
Q std: 24.059071
Actor loss: 49.255421
Action reg: 0.003989
  l1.weight: grad_norm = 0.000197
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.000699
Total gradient norm: 0.002209
=== Actor Training Debug (Iteration 2011) ===
Q mean: -47.903393
Q std: 23.562939
Actor loss: 47.907383
Action reg: 0.003992
  l1.weight: grad_norm = 0.000162
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.000598
Total gradient norm: 0.001998
=== Actor Training Debug (Iteration 2012) ===
Q mean: -44.854237
Q std: 24.833799
Actor loss: 44.858227
Action reg: 0.003989
  l1.weight: grad_norm = 0.000144
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.000701
Total gradient norm: 0.002357
=== Actor Training Debug (Iteration 2013) ===
Q mean: -47.924397
Q std: 23.741243
Actor loss: 47.928379
Action reg: 0.003981
  l1.weight: grad_norm = 0.000282
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.001155
Total gradient norm: 0.003902
=== Actor Training Debug (Iteration 2014) ===
Q mean: -48.626938
Q std: 21.720314
Actor loss: 48.630932
Action reg: 0.003992
  l1.weight: grad_norm = 0.000176
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.000690
Total gradient norm: 0.002151
=== Actor Training Debug (Iteration 2015) ===
Q mean: -49.215759
Q std: 22.736618
Actor loss: 49.219738
Action reg: 0.003978
  l1.weight: grad_norm = 0.000248
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.001311
Total gradient norm: 0.004596
=== Actor Training Debug (Iteration 2016) ===
Q mean: -50.538231
Q std: 24.163933
Actor loss: 50.542206
Action reg: 0.003975
  l1.weight: grad_norm = 0.000252
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.001524
Total gradient norm: 0.005573
=== Actor Training Debug (Iteration 2017) ===
Q mean: -44.502995
Q std: 21.405304
Actor loss: 44.506981
Action reg: 0.003986
  l1.weight: grad_norm = 0.000260
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.001169
Total gradient norm: 0.003738
=== Actor Training Debug (Iteration 2018) ===
Q mean: -47.838383
Q std: 22.332342
Actor loss: 47.842377
Action reg: 0.003995
  l1.weight: grad_norm = 0.000175
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.000429
Total gradient norm: 0.001186
=== Actor Training Debug (Iteration 2019) ===
Q mean: -44.913559
Q std: 24.237600
Actor loss: 44.917530
Action reg: 0.003972
  l1.weight: grad_norm = 0.000337
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.001745
Total gradient norm: 0.006120
=== Actor Training Debug (Iteration 2020) ===
Q mean: -46.593201
Q std: 22.870193
Actor loss: 46.597187
Action reg: 0.003986
  l1.weight: grad_norm = 0.000180
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.000860
Total gradient norm: 0.002892
=== Actor Training Debug (Iteration 2021) ===
Q mean: -49.385979
Q std: 24.326866
Actor loss: 49.389957
Action reg: 0.003978
  l1.weight: grad_norm = 0.000366
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.001675
Total gradient norm: 0.005748
=== Actor Training Debug (Iteration 2022) ===
Q mean: -48.987320
Q std: 23.611345
Actor loss: 48.991314
Action reg: 0.003995
  l1.weight: grad_norm = 0.000143
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.000428
Total gradient norm: 0.001167
=== Actor Training Debug (Iteration 2023) ===
Q mean: -47.553585
Q std: 24.470154
Actor loss: 47.557568
Action reg: 0.003983
  l1.weight: grad_norm = 0.000225
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.001137
Total gradient norm: 0.003954
=== Actor Training Debug (Iteration 2024) ===
Q mean: -43.570374
Q std: 20.957355
Actor loss: 43.574360
Action reg: 0.003987
  l1.weight: grad_norm = 0.000160
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.000832
Total gradient norm: 0.002916
=== Actor Training Debug (Iteration 2025) ===
Q mean: -45.679504
Q std: 22.296806
Actor loss: 45.683483
Action reg: 0.003980
  l1.weight: grad_norm = 0.000224
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.001285
Total gradient norm: 0.004633
=== Actor Training Debug (Iteration 2026) ===
Q mean: -47.200783
Q std: 22.485538
Actor loss: 47.204769
Action reg: 0.003986
  l1.weight: grad_norm = 0.000174
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.000904
Total gradient norm: 0.003083
=== Actor Training Debug (Iteration 2027) ===
Q mean: -44.557053
Q std: 23.382156
Actor loss: 44.561043
Action reg: 0.003990
  l1.weight: grad_norm = 0.000211
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.000911
Total gradient norm: 0.002870
=== Actor Training Debug (Iteration 2028) ===
Q mean: -46.867203
Q std: 23.087446
Actor loss: 46.871197
Action reg: 0.003996
  l1.weight: grad_norm = 0.000152
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.000449
Total gradient norm: 0.001322
=== Actor Training Debug (Iteration 2029) ===
Q mean: -48.404549
Q std: 23.340292
Actor loss: 48.408531
Action reg: 0.003983
  l1.weight: grad_norm = 0.000243
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.001217
Total gradient norm: 0.004332
=== Actor Training Debug (Iteration 2030) ===
Q mean: -45.537441
Q std: 22.645924
Actor loss: 45.541424
Action reg: 0.003984
  l1.weight: grad_norm = 0.000378
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.001314
Total gradient norm: 0.004152
=== Actor Training Debug (Iteration 2031) ===
Q mean: -45.621044
Q std: 23.207331
Actor loss: 45.625046
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2032) ===
Q mean: -48.348930
Q std: 22.961927
Actor loss: 48.352921
Action reg: 0.003991
  l1.weight: grad_norm = 0.000117
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.000550
Total gradient norm: 0.001842
=== Actor Training Debug (Iteration 2033) ===
Q mean: -46.192451
Q std: 23.669199
Actor loss: 46.196438
Action reg: 0.003985
  l1.weight: grad_norm = 0.000230
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.001259
Total gradient norm: 0.004318
=== Actor Training Debug (Iteration 2034) ===
Q mean: -43.309258
Q std: 21.050058
Actor loss: 43.313225
Action reg: 0.003967
  l1.weight: grad_norm = 0.000344
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.002096
Total gradient norm: 0.007744
=== Actor Training Debug (Iteration 2035) ===
Q mean: -45.601303
Q std: 22.824728
Actor loss: 45.605297
Action reg: 0.003995
  l1.weight: grad_norm = 0.000106
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.000422
Total gradient norm: 0.001243
=== Actor Training Debug (Iteration 2036) ===
Q mean: -49.679760
Q std: 23.856688
Actor loss: 49.683743
Action reg: 0.003982
  l1.weight: grad_norm = 0.000243
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.001119
Total gradient norm: 0.003991
=== Actor Training Debug (Iteration 2037) ===
Q mean: -48.329506
Q std: 23.001478
Actor loss: 48.333496
Action reg: 0.003991
  l1.weight: grad_norm = 0.000126
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.000522
Total gradient norm: 0.001688
=== Actor Training Debug (Iteration 2038) ===
Q mean: -46.464668
Q std: 22.671379
Actor loss: 46.468658
Action reg: 0.003989
  l1.weight: grad_norm = 0.000225
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.000721
Total gradient norm: 0.002271
=== Actor Training Debug (Iteration 2039) ===
Q mean: -44.951218
Q std: 23.228504
Actor loss: 44.955204
Action reg: 0.003986
  l1.weight: grad_norm = 0.000258
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.000949
Total gradient norm: 0.003144
=== Actor Training Debug (Iteration 2040) ===
Q mean: -49.904247
Q std: 23.102165
Actor loss: 49.908222
Action reg: 0.003974
  l1.weight: grad_norm = 0.000280
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.001599
Total gradient norm: 0.005815
=== Actor Training Debug (Iteration 2041) ===
Q mean: -49.619473
Q std: 23.659538
Actor loss: 49.623451
Action reg: 0.003978
  l1.weight: grad_norm = 0.000321
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.001385
Total gradient norm: 0.004901
=== Actor Training Debug (Iteration 2042) ===
Q mean: -48.666306
Q std: 21.700533
Actor loss: 48.670307
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2043) ===
Q mean: -44.617195
Q std: 22.908566
Actor loss: 44.621166
Action reg: 0.003970
  l1.weight: grad_norm = 0.000401
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.002279
Total gradient norm: 0.008242
=== Actor Training Debug (Iteration 2044) ===
Q mean: -46.039330
Q std: 22.969337
Actor loss: 46.043324
Action reg: 0.003993
  l1.weight: grad_norm = 0.000151
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.000672
Total gradient norm: 0.002226
=== Actor Training Debug (Iteration 2045) ===
Q mean: -47.187927
Q std: 22.339394
Actor loss: 47.191914
Action reg: 0.003987
  l1.weight: grad_norm = 0.000209
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.000849
Total gradient norm: 0.002864
=== Actor Training Debug (Iteration 2046) ===
Q mean: -44.215324
Q std: 20.303503
Actor loss: 44.219315
Action reg: 0.003989
  l1.weight: grad_norm = 0.000177
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.000745
Total gradient norm: 0.002436
=== Actor Training Debug (Iteration 2047) ===
Q mean: -47.074947
Q std: 22.657564
Actor loss: 47.078934
Action reg: 0.003987
  l1.weight: grad_norm = 0.000181
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.001030
Total gradient norm: 0.003483
=== Actor Training Debug (Iteration 2048) ===
Q mean: -49.205139
Q std: 23.707430
Actor loss: 49.209126
Action reg: 0.003986
  l1.weight: grad_norm = 0.000193
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.001007
Total gradient norm: 0.003528
=== Actor Training Debug (Iteration 2049) ===
Q mean: -51.504364
Q std: 25.375502
Actor loss: 51.508354
Action reg: 0.003990
  l1.weight: grad_norm = 0.000146
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.000665
Total gradient norm: 0.002235
=== Actor Training Debug (Iteration 2050) ===
Q mean: -45.032372
Q std: 23.074184
Actor loss: 45.036366
Action reg: 0.003995
  l1.weight: grad_norm = 0.000115
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.000344
Total gradient norm: 0.001028
=== Actor Training Debug (Iteration 2051) ===
Q mean: -47.057571
Q std: 23.483442
Actor loss: 47.061565
Action reg: 0.003995
  l1.weight: grad_norm = 0.000093
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.000447
Total gradient norm: 0.001332
=== Actor Training Debug (Iteration 2052) ===
Q mean: -47.344120
Q std: 21.638847
Actor loss: 47.348118
Action reg: 0.003996
  l1.weight: grad_norm = 0.000076
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.000334
Total gradient norm: 0.001034
=== Actor Training Debug (Iteration 2053) ===
Q mean: -49.730412
Q std: 23.491989
Actor loss: 49.734390
Action reg: 0.003977
  l1.weight: grad_norm = 0.000258
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.001368
Total gradient norm: 0.004823
=== Actor Training Debug (Iteration 2054) ===
Q mean: -46.370956
Q std: 20.315132
Actor loss: 46.374947
Action reg: 0.003990
  l1.weight: grad_norm = 0.000167
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.000736
Total gradient norm: 0.002453
=== Actor Training Debug (Iteration 2055) ===
Q mean: -44.646690
Q std: 19.528337
Actor loss: 44.650673
Action reg: 0.003984
  l1.weight: grad_norm = 0.000187
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.000919
Total gradient norm: 0.003281
=== Actor Training Debug (Iteration 2056) ===
Q mean: -44.069458
Q std: 21.115282
Actor loss: 44.073433
Action reg: 0.003976
  l1.weight: grad_norm = 0.000340
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.001649
Total gradient norm: 0.005600
=== Actor Training Debug (Iteration 2057) ===
Q mean: -50.166992
Q std: 24.292473
Actor loss: 50.170975
Action reg: 0.003984
  l1.weight: grad_norm = 0.000279
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.001331
Total gradient norm: 0.004490
=== Actor Training Debug (Iteration 2058) ===
Q mean: -49.676308
Q std: 23.239832
Actor loss: 49.680302
Action reg: 0.003992
  l1.weight: grad_norm = 0.000159
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.000685
Total gradient norm: 0.002250
=== Actor Training Debug (Iteration 2059) ===
Q mean: -47.787392
Q std: 23.598888
Actor loss: 47.791393
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2060) ===
Q mean: -45.673298
Q std: 21.035793
Actor loss: 45.677296
Action reg: 0.003996
  l1.weight: grad_norm = 0.000110
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.000313
Total gradient norm: 0.000828
=== Actor Training Debug (Iteration 2061) ===
Q mean: -47.383350
Q std: 21.028992
Actor loss: 47.387352
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2062) ===
Q mean: -48.132374
Q std: 23.132792
Actor loss: 48.136360
Action reg: 0.003987
  l1.weight: grad_norm = 0.000251
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.001085
Total gradient norm: 0.003572
=== Actor Training Debug (Iteration 2063) ===
Q mean: -47.726807
Q std: 24.248692
Actor loss: 47.730797
Action reg: 0.003992
  l1.weight: grad_norm = 0.000144
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.000765
Total gradient norm: 0.002554
=== Actor Training Debug (Iteration 2064) ===
Q mean: -48.330830
Q std: 22.401031
Actor loss: 48.334820
Action reg: 0.003992
  l1.weight: grad_norm = 0.000163
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.000662
Total gradient norm: 0.002032
=== Actor Training Debug (Iteration 2065) ===
Q mean: -48.350182
Q std: 23.232187
Actor loss: 48.354164
Action reg: 0.003981
  l1.weight: grad_norm = 0.000211
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.001212
Total gradient norm: 0.004334
=== Actor Training Debug (Iteration 2066) ===
Q mean: -45.687469
Q std: 22.635447
Actor loss: 45.691467
Action reg: 0.003996
  l1.weight: grad_norm = 0.000079
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.000293
Total gradient norm: 0.000864
=== Actor Training Debug (Iteration 2067) ===
Q mean: -47.911373
Q std: 20.689173
Actor loss: 47.915367
Action reg: 0.003994
  l1.weight: grad_norm = 0.000161
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.000676
Total gradient norm: 0.001948
=== Actor Training Debug (Iteration 2068) ===
Q mean: -46.687225
Q std: 23.229294
Actor loss: 46.691208
Action reg: 0.003982
  l1.weight: grad_norm = 0.000204
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.001199
Total gradient norm: 0.004370
=== Actor Training Debug (Iteration 2069) ===
Q mean: -47.568214
Q std: 24.226557
Actor loss: 47.572205
Action reg: 0.003990
  l1.weight: grad_norm = 0.000156
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.000569
Total gradient norm: 0.001721
=== Actor Training Debug (Iteration 2070) ===
Q mean: -46.877823
Q std: 22.188667
Actor loss: 46.881809
Action reg: 0.003986
  l1.weight: grad_norm = 0.000248
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.001009
Total gradient norm: 0.003443
=== Actor Training Debug (Iteration 2071) ===
Q mean: -46.716408
Q std: 23.811905
Actor loss: 46.720402
Action reg: 0.003995
  l1.weight: grad_norm = 0.000136
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.000366
Total gradient norm: 0.000905
=== Actor Training Debug (Iteration 2072) ===
Q mean: -49.841763
Q std: 25.473362
Actor loss: 49.845753
Action reg: 0.003989
  l1.weight: grad_norm = 0.000231
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.001052
Total gradient norm: 0.003442
=== Actor Training Debug (Iteration 2073) ===
Q mean: -45.548691
Q std: 24.118641
Actor loss: 45.552673
Action reg: 0.003981
  l1.weight: grad_norm = 0.000275
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.001402
Total gradient norm: 0.004757
=== Actor Training Debug (Iteration 2074) ===
Q mean: -46.893185
Q std: 23.396688
Actor loss: 46.897182
Action reg: 0.003997
  l1.weight: grad_norm = 0.000115
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.000447
Total gradient norm: 0.001222
=== Actor Training Debug (Iteration 2075) ===
Q mean: -49.259636
Q std: 23.274290
Actor loss: 49.263630
Action reg: 0.003995
  l1.weight: grad_norm = 0.000159
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.000440
Total gradient norm: 0.001255
=== Actor Training Debug (Iteration 2076) ===
Q mean: -48.504601
Q std: 22.399712
Actor loss: 48.508595
Action reg: 0.003995
  l1.weight: grad_norm = 0.000101
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.000288
Total gradient norm: 0.000779
=== Actor Training Debug (Iteration 2077) ===
Q mean: -48.742390
Q std: 22.804922
Actor loss: 48.746368
Action reg: 0.003979
  l1.weight: grad_norm = 0.000365
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.001595
Total gradient norm: 0.005567
=== Actor Training Debug (Iteration 2078) ===
Q mean: -47.813454
Q std: 22.588737
Actor loss: 47.817432
Action reg: 0.003977
  l1.weight: grad_norm = 0.000299
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.001873
Total gradient norm: 0.006954
=== Actor Training Debug (Iteration 2079) ===
Q mean: -46.752563
Q std: 25.040266
Actor loss: 46.756550
Action reg: 0.003988
  l1.weight: grad_norm = 0.000182
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.000924
Total gradient norm: 0.003366
=== Actor Training Debug (Iteration 2080) ===
Q mean: -48.794899
Q std: 25.263845
Actor loss: 48.798882
Action reg: 0.003983
  l1.weight: grad_norm = 0.000231
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.001205
Total gradient norm: 0.004281
=== Actor Training Debug (Iteration 2081) ===
Q mean: -48.400490
Q std: 23.117599
Actor loss: 48.404465
Action reg: 0.003976
  l1.weight: grad_norm = 0.000278
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.001624
Total gradient norm: 0.005771
=== Actor Training Debug (Iteration 2082) ===
Q mean: -50.320141
Q std: 23.314550
Actor loss: 50.324127
Action reg: 0.003988
  l1.weight: grad_norm = 0.000300
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.001354
Total gradient norm: 0.004184
=== Actor Training Debug (Iteration 2083) ===
Q mean: -48.682957
Q std: 23.112562
Actor loss: 48.686958
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2084) ===
Q mean: -43.393036
Q std: 22.344788
Actor loss: 43.397007
Action reg: 0.003971
  l1.weight: grad_norm = 0.000317
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.002000
Total gradient norm: 0.007253
=== Actor Training Debug (Iteration 2085) ===
Q mean: -44.148994
Q std: 23.694767
Actor loss: 44.152969
Action reg: 0.003975
  l1.weight: grad_norm = 0.000286
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.001880
Total gradient norm: 0.006857
=== Actor Training Debug (Iteration 2086) ===
Q mean: -47.571739
Q std: 24.583607
Actor loss: 47.575726
Action reg: 0.003986
  l1.weight: grad_norm = 0.000284
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.001214
Total gradient norm: 0.004039
=== Actor Training Debug (Iteration 2087) ===
Q mean: -47.422485
Q std: 23.543222
Actor loss: 47.426468
Action reg: 0.003984
  l1.weight: grad_norm = 0.000223
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.001208
Total gradient norm: 0.004283
=== Actor Training Debug (Iteration 2088) ===
Q mean: -47.184902
Q std: 22.907768
Actor loss: 47.188885
Action reg: 0.003982
  l1.weight: grad_norm = 0.000284
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.001600
Total gradient norm: 0.005527
=== Actor Training Debug (Iteration 2089) ===
Q mean: -47.335312
Q std: 22.518999
Actor loss: 47.339302
Action reg: 0.003990
  l1.weight: grad_norm = 0.000172
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.000663
Total gradient norm: 0.002178
=== Actor Training Debug (Iteration 2090) ===
Q mean: -48.120697
Q std: 24.395430
Actor loss: 48.124676
Action reg: 0.003980
  l1.weight: grad_norm = 0.000280
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.001765
Total gradient norm: 0.006467
=== Actor Training Debug (Iteration 2091) ===
Q mean: -49.658180
Q std: 24.587324
Actor loss: 49.662178
Action reg: 0.003997
  l1.weight: grad_norm = 0.000096
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.000374
Total gradient norm: 0.001098
=== Actor Training Debug (Iteration 2092) ===
Q mean: -48.792431
Q std: 25.644075
Actor loss: 48.796417
Action reg: 0.003985
  l1.weight: grad_norm = 0.000225
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.001187
Total gradient norm: 0.004158
=== Actor Training Debug (Iteration 2093) ===
Q mean: -49.062943
Q std: 23.553177
Actor loss: 49.066921
Action reg: 0.003980
  l1.weight: grad_norm = 0.000293
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.001418
Total gradient norm: 0.005010
=== Actor Training Debug (Iteration 2094) ===
Q mean: -45.892418
Q std: 24.124731
Actor loss: 45.896404
Action reg: 0.003986
  l1.weight: grad_norm = 0.000181
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.000927
Total gradient norm: 0.003253
=== Actor Training Debug (Iteration 2095) ===
Q mean: -47.253838
Q std: 22.920763
Actor loss: 47.257832
Action reg: 0.003994
  l1.weight: grad_norm = 0.000119
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.000591
Total gradient norm: 0.001951
=== Actor Training Debug (Iteration 2096) ===
Q mean: -50.655964
Q std: 25.026554
Actor loss: 50.659946
Action reg: 0.003983
  l1.weight: grad_norm = 0.000240
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.001339
Total gradient norm: 0.004791
=== Actor Training Debug (Iteration 2097) ===
Q mean: -51.718510
Q std: 22.450289
Actor loss: 51.722500
Action reg: 0.003991
  l1.weight: grad_norm = 0.000181
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.000939
Total gradient norm: 0.002892
=== Actor Training Debug (Iteration 2098) ===
Q mean: -45.314243
Q std: 22.862318
Actor loss: 45.318230
Action reg: 0.003987
  l1.weight: grad_norm = 0.000198
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.001113
Total gradient norm: 0.003700
=== Actor Training Debug (Iteration 2099) ===
Q mean: -41.758385
Q std: 21.215775
Actor loss: 41.762367
Action reg: 0.003981
  l1.weight: grad_norm = 0.000239
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.001194
Total gradient norm: 0.004282
=== Actor Training Debug (Iteration 2100) ===
Q mean: -46.036423
Q std: 22.123056
Actor loss: 46.040417
Action reg: 0.003996
  l1.weight: grad_norm = 0.000106
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.000516
Total gradient norm: 0.001573
Episode 71: Steps=100, Reward=-308.324, Buffer_size=7100
=== Actor Training Debug (Iteration 2101) ===
Q mean: -55.048965
Q std: 25.923777
Actor loss: 55.052952
Action reg: 0.003988
  l1.weight: grad_norm = 0.000172
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.000999
Total gradient norm: 0.003384
=== Actor Training Debug (Iteration 2102) ===
Q mean: -54.972057
Q std: 23.407578
Actor loss: 54.976048
Action reg: 0.003991
  l1.weight: grad_norm = 0.000143
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.000866
Total gradient norm: 0.002870
=== Actor Training Debug (Iteration 2103) ===
Q mean: -50.001785
Q std: 23.987949
Actor loss: 50.005768
Action reg: 0.003982
  l1.weight: grad_norm = 0.000231
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.001326
Total gradient norm: 0.004924
=== Actor Training Debug (Iteration 2104) ===
Q mean: -44.092953
Q std: 23.527025
Actor loss: 44.096935
Action reg: 0.003982
  l1.weight: grad_norm = 0.000243
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.001535
Total gradient norm: 0.005450
=== Actor Training Debug (Iteration 2105) ===
Q mean: -44.791412
Q std: 20.224823
Actor loss: 44.795414
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2106) ===
Q mean: -46.104298
Q std: 23.077110
Actor loss: 46.108284
Action reg: 0.003987
  l1.weight: grad_norm = 0.000210
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.001101
Total gradient norm: 0.003735
=== Actor Training Debug (Iteration 2107) ===
Q mean: -51.154388
Q std: 24.911922
Actor loss: 51.158375
Action reg: 0.003986
  l1.weight: grad_norm = 0.000263
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.001451
Total gradient norm: 0.004704
=== Actor Training Debug (Iteration 2108) ===
Q mean: -53.579613
Q std: 22.207045
Actor loss: 53.583614
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2109) ===
Q mean: -49.310345
Q std: 22.987932
Actor loss: 49.314335
Action reg: 0.003991
  l1.weight: grad_norm = 0.000136
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.000599
Total gradient norm: 0.002029
=== Actor Training Debug (Iteration 2110) ===
Q mean: -45.338493
Q std: 22.479530
Actor loss: 45.342480
Action reg: 0.003987
  l1.weight: grad_norm = 0.000152
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.000860
Total gradient norm: 0.003085
=== Actor Training Debug (Iteration 2111) ===
Q mean: -43.917969
Q std: 22.827118
Actor loss: 43.921959
Action reg: 0.003990
  l1.weight: grad_norm = 0.000185
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.000798
Total gradient norm: 0.002645
=== Actor Training Debug (Iteration 2112) ===
Q mean: -48.529278
Q std: 23.548239
Actor loss: 48.533272
Action reg: 0.003994
  l1.weight: grad_norm = 0.000131
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.000620
Total gradient norm: 0.002038
=== Actor Training Debug (Iteration 2113) ===
Q mean: -48.171425
Q std: 23.279680
Actor loss: 48.175411
Action reg: 0.003988
  l1.weight: grad_norm = 0.000274
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.001225
Total gradient norm: 0.003999
=== Actor Training Debug (Iteration 2114) ===
Q mean: -47.069649
Q std: 21.841797
Actor loss: 47.073631
Action reg: 0.003983
  l1.weight: grad_norm = 0.000288
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.001431
Total gradient norm: 0.005125
=== Actor Training Debug (Iteration 2115) ===
Q mean: -47.565971
Q std: 24.012848
Actor loss: 47.569950
Action reg: 0.003978
  l1.weight: grad_norm = 0.000271
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.001753
Total gradient norm: 0.006626
=== Actor Training Debug (Iteration 2116) ===
Q mean: -48.839508
Q std: 22.130301
Actor loss: 48.843494
Action reg: 0.003985
  l1.weight: grad_norm = 0.000223
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.000998
Total gradient norm: 0.003449
=== Actor Training Debug (Iteration 2117) ===
Q mean: -45.936401
Q std: 21.273338
Actor loss: 45.940403
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2118) ===
Q mean: -48.023651
Q std: 23.392374
Actor loss: 48.027641
Action reg: 0.003991
  l1.weight: grad_norm = 0.000118
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.000683
Total gradient norm: 0.002471
=== Actor Training Debug (Iteration 2119) ===
Q mean: -49.119476
Q std: 22.953703
Actor loss: 49.123466
Action reg: 0.003992
  l1.weight: grad_norm = 0.000143
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.000722
Total gradient norm: 0.002474
=== Actor Training Debug (Iteration 2120) ===
Q mean: -48.388504
Q std: 23.103504
Actor loss: 48.392483
Action reg: 0.003977
  l1.weight: grad_norm = 0.000280
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.001847
Total gradient norm: 0.006846
=== Actor Training Debug (Iteration 2121) ===
Q mean: -45.175045
Q std: 21.197269
Actor loss: 45.179039
Action reg: 0.003996
  l1.weight: grad_norm = 0.000108
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.000471
Total gradient norm: 0.001305
=== Actor Training Debug (Iteration 2122) ===
Q mean: -47.427273
Q std: 24.454599
Actor loss: 47.431263
Action reg: 0.003990
  l1.weight: grad_norm = 0.000164
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.000828
Total gradient norm: 0.002882
=== Actor Training Debug (Iteration 2123) ===
Q mean: -48.815971
Q std: 23.541969
Actor loss: 48.819962
Action reg: 0.003991
  l1.weight: grad_norm = 0.000159
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.000791
Total gradient norm: 0.002723
=== Actor Training Debug (Iteration 2124) ===
Q mean: -51.617332
Q std: 24.056171
Actor loss: 51.621326
Action reg: 0.003995
  l1.weight: grad_norm = 0.000142
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.000528
Total gradient norm: 0.001553
=== Actor Training Debug (Iteration 2125) ===
Q mean: -43.292259
Q std: 20.355442
Actor loss: 43.296249
Action reg: 0.003991
  l1.weight: grad_norm = 0.000143
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.000708
Total gradient norm: 0.002224
=== Actor Training Debug (Iteration 2126) ===
Q mean: -47.976547
Q std: 23.010410
Actor loss: 47.980541
Action reg: 0.003995
  l1.weight: grad_norm = 0.000141
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.000543
Total gradient norm: 0.001601
=== Actor Training Debug (Iteration 2127) ===
Q mean: -47.982189
Q std: 22.603897
Actor loss: 47.986160
Action reg: 0.003972
  l1.weight: grad_norm = 0.000320
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.002238
Total gradient norm: 0.008396
=== Actor Training Debug (Iteration 2128) ===
Q mean: -48.410095
Q std: 22.840418
Actor loss: 48.414089
Action reg: 0.003995
  l1.weight: grad_norm = 0.000096
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.000320
Total gradient norm: 0.000900
=== Actor Training Debug (Iteration 2129) ===
Q mean: -48.002865
Q std: 22.977842
Actor loss: 48.006863
Action reg: 0.003998
  l1.weight: grad_norm = 0.000111
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.000463
Total gradient norm: 0.001271
=== Actor Training Debug (Iteration 2130) ===
Q mean: -47.819054
Q std: 23.770157
Actor loss: 47.823029
Action reg: 0.003974
  l1.weight: grad_norm = 0.000330
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.001795
Total gradient norm: 0.006650
=== Actor Training Debug (Iteration 2131) ===
Q mean: -47.704529
Q std: 24.170681
Actor loss: 47.708523
Action reg: 0.003994
  l1.weight: grad_norm = 0.000145
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.000510
Total gradient norm: 0.001522
=== Actor Training Debug (Iteration 2132) ===
Q mean: -47.806702
Q std: 21.914816
Actor loss: 47.810688
Action reg: 0.003988
  l1.weight: grad_norm = 0.000177
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.000851
Total gradient norm: 0.003151
=== Actor Training Debug (Iteration 2133) ===
Q mean: -49.607189
Q std: 22.397524
Actor loss: 49.611179
Action reg: 0.003992
  l1.weight: grad_norm = 0.000146
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.000839
Total gradient norm: 0.002773
=== Actor Training Debug (Iteration 2134) ===
Q mean: -48.658669
Q std: 22.263754
Actor loss: 48.662666
Action reg: 0.003996
  l1.weight: grad_norm = 0.000120
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.000552
Total gradient norm: 0.001712
=== Actor Training Debug (Iteration 2135) ===
Q mean: -46.826347
Q std: 23.590555
Actor loss: 46.830338
Action reg: 0.003991
  l1.weight: grad_norm = 0.000167
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.000866
Total gradient norm: 0.003004
=== Actor Training Debug (Iteration 2136) ===
Q mean: -48.048138
Q std: 24.833004
Actor loss: 48.052120
Action reg: 0.003982
  l1.weight: grad_norm = 0.000217
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.001451
Total gradient norm: 0.005435
=== Actor Training Debug (Iteration 2137) ===
Q mean: -47.246769
Q std: 22.867731
Actor loss: 47.250763
Action reg: 0.003995
  l1.weight: grad_norm = 0.000157
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.000628
Total gradient norm: 0.001736
=== Actor Training Debug (Iteration 2138) ===
Q mean: -50.393066
Q std: 23.968597
Actor loss: 50.397068
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2139) ===
Q mean: -46.548653
Q std: 20.811703
Actor loss: 46.552647
Action reg: 0.003992
  l1.weight: grad_norm = 0.000159
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.000919
Total gradient norm: 0.003099
=== Actor Training Debug (Iteration 2140) ===
Q mean: -47.368340
Q std: 20.024740
Actor loss: 47.372330
Action reg: 0.003991
  l1.weight: grad_norm = 0.000192
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.000757
Total gradient norm: 0.002550
=== Actor Training Debug (Iteration 2141) ===
Q mean: -49.681740
Q std: 24.426788
Actor loss: 49.685703
Action reg: 0.003964
  l1.weight: grad_norm = 0.000371
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.002753
Total gradient norm: 0.010764
=== Actor Training Debug (Iteration 2142) ===
Q mean: -50.230030
Q std: 24.893471
Actor loss: 50.234013
Action reg: 0.003982
  l1.weight: grad_norm = 0.000253
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.001533
Total gradient norm: 0.005557
=== Actor Training Debug (Iteration 2143) ===
Q mean: -49.625172
Q std: 23.321079
Actor loss: 49.629166
Action reg: 0.003996
  l1.weight: grad_norm = 0.000107
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.000401
Total gradient norm: 0.001293
=== Actor Training Debug (Iteration 2144) ===
Q mean: -48.616554
Q std: 23.390652
Actor loss: 48.620544
Action reg: 0.003989
  l1.weight: grad_norm = 0.000156
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.000902
Total gradient norm: 0.003321
=== Actor Training Debug (Iteration 2145) ===
Q mean: -50.497650
Q std: 23.274174
Actor loss: 50.501640
Action reg: 0.003991
  l1.weight: grad_norm = 0.000173
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.000952
Total gradient norm: 0.003381
=== Actor Training Debug (Iteration 2146) ===
Q mean: -48.505207
Q std: 22.949953
Actor loss: 48.509201
Action reg: 0.003996
  l1.weight: grad_norm = 0.000107
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.000404
Total gradient norm: 0.001302
=== Actor Training Debug (Iteration 2147) ===
Q mean: -46.618927
Q std: 20.995201
Actor loss: 46.622929
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2148) ===
Q mean: -50.200394
Q std: 22.896351
Actor loss: 50.204388
Action reg: 0.003994
  l1.weight: grad_norm = 0.000144
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.000517
Total gradient norm: 0.001547
=== Actor Training Debug (Iteration 2149) ===
Q mean: -48.400703
Q std: 22.961432
Actor loss: 48.404690
Action reg: 0.003987
  l1.weight: grad_norm = 0.000167
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.000970
Total gradient norm: 0.003590
=== Actor Training Debug (Iteration 2150) ===
Q mean: -48.885796
Q std: 21.912336
Actor loss: 48.889797
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2151) ===
Q mean: -49.052452
Q std: 22.860151
Actor loss: 49.056446
Action reg: 0.003992
  l1.weight: grad_norm = 0.000146
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.000724
Total gradient norm: 0.002523
=== Actor Training Debug (Iteration 2152) ===
Q mean: -50.263176
Q std: 25.513865
Actor loss: 50.267166
Action reg: 0.003990
  l1.weight: grad_norm = 0.000258
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.001084
Total gradient norm: 0.003342
=== Actor Training Debug (Iteration 2153) ===
Q mean: -48.577652
Q std: 23.139120
Actor loss: 48.581638
Action reg: 0.003987
  l1.weight: grad_norm = 0.000190
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.001065
Total gradient norm: 0.004010
=== Actor Training Debug (Iteration 2154) ===
Q mean: -49.957993
Q std: 24.731106
Actor loss: 49.961979
Action reg: 0.003987
  l1.weight: grad_norm = 0.000285
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.001479
Total gradient norm: 0.005171
=== Actor Training Debug (Iteration 2155) ===
Q mean: -45.168388
Q std: 21.290884
Actor loss: 45.172382
Action reg: 0.003993
  l1.weight: grad_norm = 0.000152
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.000718
Total gradient norm: 0.002413
=== Actor Training Debug (Iteration 2156) ===
Q mean: -49.037476
Q std: 21.776455
Actor loss: 49.041447
Action reg: 0.003972
  l1.weight: grad_norm = 0.000386
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.002571
Total gradient norm: 0.009863
=== Actor Training Debug (Iteration 2157) ===
Q mean: -46.985134
Q std: 20.529591
Actor loss: 46.989117
Action reg: 0.003984
  l1.weight: grad_norm = 0.000250
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.001418
Total gradient norm: 0.005037
=== Actor Training Debug (Iteration 2158) ===
Q mean: -51.928684
Q std: 26.110912
Actor loss: 51.932671
Action reg: 0.003986
  l1.weight: grad_norm = 0.000186
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.001183
Total gradient norm: 0.004399
=== Actor Training Debug (Iteration 2159) ===
Q mean: -49.772846
Q std: 26.212873
Actor loss: 49.776833
Action reg: 0.003987
  l1.weight: grad_norm = 0.000235
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.001333
Total gradient norm: 0.005002
=== Actor Training Debug (Iteration 2160) ===
Q mean: -47.751945
Q std: 23.937429
Actor loss: 47.755939
Action reg: 0.003996
  l1.weight: grad_norm = 0.000096
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.000559
Total gradient norm: 0.001761
=== Actor Training Debug (Iteration 2161) ===
Q mean: -49.708679
Q std: 24.310192
Actor loss: 49.712666
Action reg: 0.003986
  l1.weight: grad_norm = 0.000284
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.001704
Total gradient norm: 0.005466
=== Actor Training Debug (Iteration 2162) ===
Q mean: -51.350697
Q std: 23.594376
Actor loss: 51.354683
Action reg: 0.003986
  l1.weight: grad_norm = 0.000251
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.001337
Total gradient norm: 0.004424
=== Actor Training Debug (Iteration 2163) ===
Q mean: -50.134708
Q std: 23.728912
Actor loss: 50.138695
Action reg: 0.003988
  l1.weight: grad_norm = 0.000260
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.001459
Total gradient norm: 0.004968
=== Actor Training Debug (Iteration 2164) ===
Q mean: -46.495483
Q std: 20.820114
Actor loss: 46.499470
Action reg: 0.003985
  l1.weight: grad_norm = 0.000215
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.001104
Total gradient norm: 0.004046
=== Actor Training Debug (Iteration 2165) ===
Q mean: -47.794319
Q std: 24.723356
Actor loss: 47.798286
Action reg: 0.003968
  l1.weight: grad_norm = 0.000337
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.002469
Total gradient norm: 0.009449
=== Actor Training Debug (Iteration 2166) ===
Q mean: -47.837708
Q std: 24.715094
Actor loss: 47.841682
Action reg: 0.003975
  l1.weight: grad_norm = 0.000336
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.002189
Total gradient norm: 0.008436
=== Actor Training Debug (Iteration 2167) ===
Q mean: -50.088638
Q std: 24.651321
Actor loss: 50.092613
Action reg: 0.003977
  l1.weight: grad_norm = 0.000330
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.002378
Total gradient norm: 0.008833
=== Actor Training Debug (Iteration 2168) ===
Q mean: -50.197105
Q std: 24.729139
Actor loss: 50.201099
Action reg: 0.003994
  l1.weight: grad_norm = 0.000132
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.000506
Total gradient norm: 0.001435
=== Actor Training Debug (Iteration 2169) ===
Q mean: -48.164059
Q std: 25.066086
Actor loss: 48.168037
Action reg: 0.003979
  l1.weight: grad_norm = 0.000264
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.001793
Total gradient norm: 0.006957
=== Actor Training Debug (Iteration 2170) ===
Q mean: -48.667885
Q std: 22.727337
Actor loss: 48.671879
Action reg: 0.003992
  l1.weight: grad_norm = 0.000231
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.001056
Total gradient norm: 0.003615
=== Actor Training Debug (Iteration 2171) ===
Q mean: -50.469749
Q std: 20.591255
Actor loss: 50.473747
Action reg: 0.003996
  l1.weight: grad_norm = 0.000251
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.001060
Total gradient norm: 0.002981
=== Actor Training Debug (Iteration 2172) ===
Q mean: -47.563328
Q std: 23.225100
Actor loss: 47.567314
Action reg: 0.003987
  l1.weight: grad_norm = 0.000223
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.001181
Total gradient norm: 0.004222
=== Actor Training Debug (Iteration 2173) ===
Q mean: -50.324924
Q std: 25.611652
Actor loss: 50.328903
Action reg: 0.003980
  l1.weight: grad_norm = 0.000272
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.001601
Total gradient norm: 0.005897
=== Actor Training Debug (Iteration 2174) ===
Q mean: -47.143356
Q std: 23.821802
Actor loss: 47.147350
Action reg: 0.003996
  l1.weight: grad_norm = 0.000096
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.000562
Total gradient norm: 0.001777
=== Actor Training Debug (Iteration 2175) ===
Q mean: -48.498058
Q std: 25.728113
Actor loss: 48.502045
Action reg: 0.003986
  l1.weight: grad_norm = 0.000239
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.001471
Total gradient norm: 0.005358
=== Actor Training Debug (Iteration 2176) ===
Q mean: -49.146439
Q std: 23.941198
Actor loss: 49.150429
Action reg: 0.003991
  l1.weight: grad_norm = 0.000286
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.001259
Total gradient norm: 0.004171
=== Actor Training Debug (Iteration 2177) ===
Q mean: -48.623837
Q std: 22.836376
Actor loss: 48.627838
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2178) ===
Q mean: -51.905128
Q std: 23.445086
Actor loss: 51.909103
Action reg: 0.003976
  l1.weight: grad_norm = 0.000400
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.002434
Total gradient norm: 0.009011
=== Actor Training Debug (Iteration 2179) ===
Q mean: -48.215324
Q std: 24.807981
Actor loss: 48.219311
Action reg: 0.003988
  l1.weight: grad_norm = 0.000264
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.001592
Total gradient norm: 0.005617
=== Actor Training Debug (Iteration 2180) ===
Q mean: -47.096687
Q std: 26.023911
Actor loss: 47.100658
Action reg: 0.003971
  l1.weight: grad_norm = 0.000407
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.002800
Total gradient norm: 0.010283
=== Actor Training Debug (Iteration 2181) ===
Q mean: -48.062836
Q std: 25.205084
Actor loss: 48.066826
Action reg: 0.003989
  l1.weight: grad_norm = 0.000198
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.001000
Total gradient norm: 0.003524
=== Actor Training Debug (Iteration 2182) ===
Q mean: -51.328720
Q std: 26.005312
Actor loss: 51.332699
Action reg: 0.003980
  l1.weight: grad_norm = 0.000250
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.001584
Total gradient norm: 0.006301
=== Actor Training Debug (Iteration 2183) ===
Q mean: -51.070129
Q std: 24.576620
Actor loss: 51.074127
Action reg: 0.003997
  l1.weight: grad_norm = 0.000166
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.000690
Total gradient norm: 0.002108
=== Actor Training Debug (Iteration 2184) ===
Q mean: -46.927399
Q std: 21.984184
Actor loss: 46.931393
Action reg: 0.003995
  l1.weight: grad_norm = 0.000144
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.000612
Total gradient norm: 0.001992
=== Actor Training Debug (Iteration 2185) ===
Q mean: -46.936661
Q std: 21.730774
Actor loss: 46.940655
Action reg: 0.003995
  l1.weight: grad_norm = 0.000178
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.000777
Total gradient norm: 0.002438
=== Actor Training Debug (Iteration 2186) ===
Q mean: -49.472523
Q std: 25.994253
Actor loss: 49.476490
Action reg: 0.003969
  l1.weight: grad_norm = 0.000333
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.002371
Total gradient norm: 0.009369
=== Actor Training Debug (Iteration 2187) ===
Q mean: -50.579025
Q std: 25.233713
Actor loss: 50.583008
Action reg: 0.003984
  l1.weight: grad_norm = 0.000231
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.001232
Total gradient norm: 0.004656
=== Actor Training Debug (Iteration 2188) ===
Q mean: -49.075867
Q std: 23.748989
Actor loss: 49.079849
Action reg: 0.003981
  l1.weight: grad_norm = 0.000339
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.002164
Total gradient norm: 0.008053
=== Actor Training Debug (Iteration 2189) ===
Q mean: -50.578423
Q std: 22.473646
Actor loss: 50.582397
Action reg: 0.003974
  l1.weight: grad_norm = 0.000353
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.002438
Total gradient norm: 0.008983
=== Actor Training Debug (Iteration 2190) ===
Q mean: -52.334507
Q std: 22.585878
Actor loss: 52.338497
Action reg: 0.003990
  l1.weight: grad_norm = 0.000208
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.001033
Total gradient norm: 0.003019
=== Actor Training Debug (Iteration 2191) ===
Q mean: -52.526371
Q std: 24.051434
Actor loss: 52.530361
Action reg: 0.003990
  l1.weight: grad_norm = 0.000145
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.000859
Total gradient norm: 0.003338
=== Actor Training Debug (Iteration 2192) ===
Q mean: -47.646389
Q std: 24.575411
Actor loss: 47.650368
Action reg: 0.003978
  l1.weight: grad_norm = 0.000267
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.001704
Total gradient norm: 0.006346
=== Actor Training Debug (Iteration 2193) ===
Q mean: -46.700203
Q std: 23.327093
Actor loss: 46.704193
Action reg: 0.003990
  l1.weight: grad_norm = 0.000221
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.001122
Total gradient norm: 0.003832
=== Actor Training Debug (Iteration 2194) ===
Q mean: -47.935707
Q std: 22.704601
Actor loss: 47.939693
Action reg: 0.003985
  l1.weight: grad_norm = 0.000293
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.001598
Total gradient norm: 0.005405
=== Actor Training Debug (Iteration 2195) ===
Q mean: -48.940815
Q std: 24.165537
Actor loss: 48.944805
Action reg: 0.003990
  l1.weight: grad_norm = 0.000184
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.000973
Total gradient norm: 0.003532
=== Actor Training Debug (Iteration 2196) ===
Q mean: -49.458122
Q std: 25.073792
Actor loss: 49.462093
Action reg: 0.003973
  l1.weight: grad_norm = 0.000357
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.002212
Total gradient norm: 0.008119
=== Actor Training Debug (Iteration 2197) ===
Q mean: -47.842075
Q std: 23.352062
Actor loss: 47.846077
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2198) ===
Q mean: -45.042763
Q std: 24.115931
Actor loss: 45.046730
Action reg: 0.003968
  l1.weight: grad_norm = 0.000348
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.002740
Total gradient norm: 0.010899
=== Actor Training Debug (Iteration 2199) ===
Q mean: -49.349026
Q std: 22.270266
Actor loss: 49.353024
Action reg: 0.003997
  l1.weight: grad_norm = 0.000148
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.000613
Total gradient norm: 0.001866
=== Actor Training Debug (Iteration 2200) ===
Q mean: -46.348412
Q std: 22.180424
Actor loss: 46.352390
Action reg: 0.003978
  l1.weight: grad_norm = 0.000398
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.002512
Total gradient norm: 0.009430
=== Actor Training Debug (Iteration 2201) ===
Q mean: -48.811527
Q std: 22.296415
Actor loss: 48.815510
Action reg: 0.003981
  l1.weight: grad_norm = 0.000298
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.002054
Total gradient norm: 0.007983
=== Actor Training Debug (Iteration 2202) ===
Q mean: -46.850231
Q std: 22.336544
Actor loss: 46.854221
Action reg: 0.003990
  l1.weight: grad_norm = 0.000211
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.001180
Total gradient norm: 0.004223
=== Actor Training Debug (Iteration 2203) ===
Q mean: -50.592293
Q std: 25.259813
Actor loss: 50.596275
Action reg: 0.003984
  l1.weight: grad_norm = 0.000287
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.001398
Total gradient norm: 0.004834
=== Actor Training Debug (Iteration 2204) ===
Q mean: -50.920990
Q std: 23.978615
Actor loss: 50.924980
Action reg: 0.003991
  l1.weight: grad_norm = 0.000265
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.001191
Total gradient norm: 0.004092
=== Actor Training Debug (Iteration 2205) ===
Q mean: -50.595100
Q std: 22.163614
Actor loss: 50.599094
Action reg: 0.003995
  l1.weight: grad_norm = 0.000176
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.000709
Total gradient norm: 0.002116
=== Actor Training Debug (Iteration 2206) ===
Q mean: -48.010876
Q std: 22.668346
Actor loss: 48.014858
Action reg: 0.003983
  l1.weight: grad_norm = 0.000281
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.001559
Total gradient norm: 0.005806
=== Actor Training Debug (Iteration 2207) ===
Q mean: -52.241123
Q std: 24.761740
Actor loss: 52.245125
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2208) ===
Q mean: -53.094822
Q std: 25.419228
Actor loss: 53.098804
Action reg: 0.003983
  l1.weight: grad_norm = 0.000212
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.001556
Total gradient norm: 0.006023
=== Actor Training Debug (Iteration 2209) ===
Q mean: -45.229660
Q std: 23.124983
Actor loss: 45.233654
Action reg: 0.003994
  l1.weight: grad_norm = 0.000196
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.000805
Total gradient norm: 0.002419
=== Actor Training Debug (Iteration 2210) ===
Q mean: -52.220345
Q std: 24.669163
Actor loss: 52.224339
Action reg: 0.003995
  l1.weight: grad_norm = 0.000176
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.000730
Total gradient norm: 0.002262
=== Actor Training Debug (Iteration 2211) ===
Q mean: -50.283104
Q std: 23.367363
Actor loss: 50.287098
Action reg: 0.003995
  l1.weight: grad_norm = 0.000156
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.000663
Total gradient norm: 0.002044
=== Actor Training Debug (Iteration 2212) ===
Q mean: -51.130249
Q std: 25.368845
Actor loss: 51.134224
Action reg: 0.003974
  l1.weight: grad_norm = 0.000411
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.003011
Total gradient norm: 0.011513
=== Actor Training Debug (Iteration 2213) ===
Q mean: -48.493088
Q std: 23.861681
Actor loss: 48.497078
Action reg: 0.003991
  l1.weight: grad_norm = 0.000295
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.001467
Total gradient norm: 0.005145
=== Actor Training Debug (Iteration 2214) ===
Q mean: -49.855148
Q std: 22.957302
Actor loss: 49.859138
Action reg: 0.003990
  l1.weight: grad_norm = 0.000232
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.001296
Total gradient norm: 0.004680
=== Actor Training Debug (Iteration 2215) ===
Q mean: -48.128242
Q std: 20.784494
Actor loss: 48.132233
Action reg: 0.003990
  l1.weight: grad_norm = 0.000208
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.001139
Total gradient norm: 0.003534
=== Actor Training Debug (Iteration 2216) ===
Q mean: -47.130508
Q std: 22.928476
Actor loss: 47.134491
Action reg: 0.003984
  l1.weight: grad_norm = 0.000295
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.001679
Total gradient norm: 0.006300
=== Actor Training Debug (Iteration 2217) ===
Q mean: -47.792351
Q std: 23.240103
Actor loss: 47.796333
Action reg: 0.003982
  l1.weight: grad_norm = 0.000442
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.002811
Total gradient norm: 0.010644
=== Actor Training Debug (Iteration 2218) ===
Q mean: -51.194504
Q std: 25.440496
Actor loss: 51.198490
Action reg: 0.003985
  l1.weight: grad_norm = 0.000262
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.001458
Total gradient norm: 0.005529
=== Actor Training Debug (Iteration 2219) ===
Q mean: -49.144318
Q std: 25.258204
Actor loss: 49.148315
Action reg: 0.003996
  l1.weight: grad_norm = 0.000275
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.001134
Total gradient norm: 0.003351
=== Actor Training Debug (Iteration 2220) ===
Q mean: -44.271996
Q std: 23.208433
Actor loss: 44.275970
Action reg: 0.003974
  l1.weight: grad_norm = 0.000317
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.002755
Total gradient norm: 0.011081
=== Actor Training Debug (Iteration 2221) ===
Q mean: -47.079575
Q std: 23.202576
Actor loss: 47.083561
Action reg: 0.003985
  l1.weight: grad_norm = 0.000272
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.001688
Total gradient norm: 0.006479
=== Actor Training Debug (Iteration 2222) ===
Q mean: -49.193085
Q std: 23.059290
Actor loss: 49.197071
Action reg: 0.003985
  l1.weight: grad_norm = 0.000422
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.002480
Total gradient norm: 0.009110
=== Actor Training Debug (Iteration 2223) ===
Q mean: -50.158279
Q std: 22.752218
Actor loss: 50.162273
Action reg: 0.003995
  l1.weight: grad_norm = 0.000156
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.000723
Total gradient norm: 0.002113
=== Actor Training Debug (Iteration 2224) ===
Q mean: -49.138374
Q std: 24.504620
Actor loss: 49.142361
Action reg: 0.003985
  l1.weight: grad_norm = 0.000465
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.002473
Total gradient norm: 0.008819
=== Actor Training Debug (Iteration 2225) ===
Q mean: -47.985146
Q std: 24.793167
Actor loss: 47.989140
Action reg: 0.003995
  l1.weight: grad_norm = 0.000181
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.000823
Total gradient norm: 0.002603
=== Actor Training Debug (Iteration 2226) ===
Q mean: -48.684105
Q std: 23.913538
Actor loss: 48.688099
Action reg: 0.003994
  l1.weight: grad_norm = 0.000176
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.000792
Total gradient norm: 0.002401
=== Actor Training Debug (Iteration 2227) ===
Q mean: -50.245621
Q std: 25.761358
Actor loss: 50.249603
Action reg: 0.003983
  l1.weight: grad_norm = 0.000325
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.001994
Total gradient norm: 0.007392
=== Actor Training Debug (Iteration 2228) ===
Q mean: -51.118431
Q std: 23.883324
Actor loss: 51.122410
Action reg: 0.003979
  l1.weight: grad_norm = 0.000330
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.002296
Total gradient norm: 0.008737
=== Actor Training Debug (Iteration 2229) ===
Q mean: -49.266163
Q std: 22.626442
Actor loss: 49.270145
Action reg: 0.003983
  l1.weight: grad_norm = 0.000320
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.001982
Total gradient norm: 0.007370
=== Actor Training Debug (Iteration 2230) ===
Q mean: -46.743267
Q std: 22.462135
Actor loss: 46.747253
Action reg: 0.003988
  l1.weight: grad_norm = 0.000277
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.001461
Total gradient norm: 0.005147
=== Actor Training Debug (Iteration 2231) ===
Q mean: -49.374725
Q std: 21.456757
Actor loss: 49.378716
Action reg: 0.003990
  l1.weight: grad_norm = 0.000349
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.001737
Total gradient norm: 0.005747
=== Actor Training Debug (Iteration 2232) ===
Q mean: -48.716759
Q std: 23.838297
Actor loss: 48.720749
Action reg: 0.003989
  l1.weight: grad_norm = 0.000246
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.001396
Total gradient norm: 0.004877
=== Actor Training Debug (Iteration 2233) ===
Q mean: -49.093506
Q std: 22.244709
Actor loss: 49.097496
Action reg: 0.003990
  l1.weight: grad_norm = 0.000304
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.001441
Total gradient norm: 0.005185
=== Actor Training Debug (Iteration 2234) ===
Q mean: -52.839149
Q std: 24.550112
Actor loss: 52.843132
Action reg: 0.003983
  l1.weight: grad_norm = 0.000403
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.002173
Total gradient norm: 0.007618
=== Actor Training Debug (Iteration 2235) ===
Q mean: -50.820461
Q std: 23.953323
Actor loss: 50.824444
Action reg: 0.003981
  l1.weight: grad_norm = 0.000310
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.002095
Total gradient norm: 0.008321
=== Actor Training Debug (Iteration 2236) ===
Q mean: -48.259384
Q std: 23.989052
Actor loss: 48.263374
Action reg: 0.003992
  l1.weight: grad_norm = 0.000514
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.002498
Total gradient norm: 0.007931
=== Actor Training Debug (Iteration 2237) ===
Q mean: -48.401077
Q std: 23.879311
Actor loss: 48.405056
Action reg: 0.003980
  l1.weight: grad_norm = 0.000360
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.002179
Total gradient norm: 0.008493
=== Actor Training Debug (Iteration 2238) ===
Q mean: -48.984924
Q std: 23.585142
Actor loss: 48.988918
Action reg: 0.003994
  l1.weight: grad_norm = 0.000247
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.000959
Total gradient norm: 0.002984
=== Actor Training Debug (Iteration 2239) ===
Q mean: -51.436913
Q std: 24.297285
Actor loss: 51.440899
Action reg: 0.003987
  l1.weight: grad_norm = 0.000271
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.001408
Total gradient norm: 0.004625
=== Actor Training Debug (Iteration 2240) ===
Q mean: -49.875076
Q std: 24.242310
Actor loss: 49.879055
Action reg: 0.003978
  l1.weight: grad_norm = 0.004913
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.016507
Total gradient norm: 0.041089
=== Actor Training Debug (Iteration 2241) ===
Q mean: -48.316723
Q std: 23.894241
Actor loss: 48.320683
Action reg: 0.003961
  l1.weight: grad_norm = 0.000591
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.004647
Total gradient norm: 0.019150
=== Actor Training Debug (Iteration 2242) ===
Q mean: -49.930729
Q std: 23.074949
Actor loss: 49.934715
Action reg: 0.003985
  l1.weight: grad_norm = 0.000294
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.001751
Total gradient norm: 0.006698
=== Actor Training Debug (Iteration 2243) ===
Q mean: -48.907063
Q std: 21.573162
Actor loss: 48.911053
Action reg: 0.003990
  l1.weight: grad_norm = 0.000268
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.001475
Total gradient norm: 0.005158
=== Actor Training Debug (Iteration 2244) ===
Q mean: -51.901196
Q std: 26.456968
Actor loss: 51.905178
Action reg: 0.003984
  l1.weight: grad_norm = 0.001145
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.004847
Total gradient norm: 0.014705
=== Actor Training Debug (Iteration 2245) ===
Q mean: -47.617744
Q std: 22.012932
Actor loss: 47.621735
Action reg: 0.003989
  l1.weight: grad_norm = 0.000210
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.001073
Total gradient norm: 0.003647
=== Actor Training Debug (Iteration 2246) ===
Q mean: -51.255669
Q std: 23.615026
Actor loss: 51.259647
Action reg: 0.003980
  l1.weight: grad_norm = 0.000374
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.002390
Total gradient norm: 0.009257
=== Actor Training Debug (Iteration 2247) ===
Q mean: -48.116402
Q std: 24.787064
Actor loss: 48.120388
Action reg: 0.003985
  l1.weight: grad_norm = 0.000307
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.002222
Total gradient norm: 0.008397
=== Actor Training Debug (Iteration 2248) ===
Q mean: -48.023293
Q std: 24.020716
Actor loss: 48.027275
Action reg: 0.003984
  l1.weight: grad_norm = 0.000337
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.002209
