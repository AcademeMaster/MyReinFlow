开始纯在线训练...
Episode 1: Steps=100, Reward=-149.471, Buffer_size=100
Episode 2: Steps=100, Reward=-142.494, Buffer_size=200
Episode 3: Steps=100, Reward=-143.826, Buffer_size=300
Episode 4: Steps=100, Reward=-157.482, Buffer_size=400
Episode 5: Steps=100, Reward=-146.118, Buffer_size=500
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 5: -315.713
Episode 6: Steps=100, Reward=-151.695, Buffer_size=600
Episode 7: Steps=100, Reward=-148.549, Buffer_size=700
Episode 8: Steps=100, Reward=-167.761, Buffer_size=800
Episode 9: Steps=100, Reward=-148.501, Buffer_size=900
Episode 10: Steps=100, Reward=-156.330, Buffer_size=1000
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 10: -315.713
Episode 11: Steps=100, Reward=-141.209, Buffer_size=1100
Episode 12: Steps=100, Reward=-152.217, Buffer_size=1200
Episode 13: Steps=100, Reward=-144.766, Buffer_size=1300
Episode 14: Steps=100, Reward=-147.579, Buffer_size=1400
Episode 15: Steps=100, Reward=-157.972, Buffer_size=1500
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 15: -315.713
Episode 16: Steps=100, Reward=-155.101, Buffer_size=1600
Episode 17: Steps=100, Reward=-141.713, Buffer_size=1700
Episode 18: Steps=100, Reward=-151.201, Buffer_size=1800
Episode 19: Steps=100, Reward=-154.716, Buffer_size=1900
Episode 20: Steps=100, Reward=-158.726, Buffer_size=2000
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 20: -315.713
Episode 21: Steps=100, Reward=-141.685, Buffer_size=2100
Episode 22: Steps=100, Reward=-141.259, Buffer_size=2200
Episode 23: Steps=100, Reward=-161.691, Buffer_size=2300
Episode 24: Steps=100, Reward=-153.117, Buffer_size=2400
Episode 25: Steps=100, Reward=-135.707, Buffer_size=2500
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 25: -315.713
Episode 26: Steps=100, Reward=-164.596, Buffer_size=2600
Episode 27: Steps=100, Reward=-154.677, Buffer_size=2700
Episode 28: Steps=100, Reward=-135.760, Buffer_size=2800
Episode 29: Steps=100, Reward=-151.594, Buffer_size=2900
Episode 30: Steps=100, Reward=-134.280, Buffer_size=3000
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 30: -315.713
Episode 31: Steps=100, Reward=-143.650, Buffer_size=3100
Episode 32: Steps=100, Reward=-146.230, Buffer_size=3200
Episode 33: Steps=100, Reward=-140.426, Buffer_size=3300
Episode 34: Steps=100, Reward=-147.120, Buffer_size=3400
Episode 35: Steps=100, Reward=-151.646, Buffer_size=3500
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 35: -315.713
Episode 36: Steps=100, Reward=-159.299, Buffer_size=3600
Episode 37: Steps=100, Reward=-149.568, Buffer_size=3700
Episode 38: Steps=100, Reward=-138.443, Buffer_size=3800
Episode 39: Steps=100, Reward=-140.575, Buffer_size=3900
Episode 40: Steps=100, Reward=-144.622, Buffer_size=4000
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 40: -315.713
Episode 41: Steps=100, Reward=-144.798, Buffer_size=4100
Episode 42: Steps=100, Reward=-166.272, Buffer_size=4200
Episode 43: Steps=100, Reward=-142.231, Buffer_size=4300
Episode 44: Steps=100, Reward=-150.833, Buffer_size=4400
Episode 45: Steps=100, Reward=-147.763, Buffer_size=4500
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 45: -315.713
Episode 46: Steps=100, Reward=-157.905, Buffer_size=4600
Episode 47: Steps=100, Reward=-161.028, Buffer_size=4700
Episode 48: Steps=100, Reward=-149.096, Buffer_size=4800
Episode 49: Steps=100, Reward=-145.423, Buffer_size=4900
Episode 50: Steps=100, Reward=-138.099, Buffer_size=5000
  Average reward: -315.713 | Average length: 100.0
Evaluation at episode 50: -315.713
=== Actor Training Debug (Iteration 1) ===
Q mean: -48.123592
Q std: 30.647867
Actor loss: 48.127453
Action reg: 0.003860
  l1.weight: grad_norm = 0.311125
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 1.248284
Total gradient norm: 1.954013
=== Actor Training Debug (Iteration 2) ===
Q mean: -26.194059
Q std: 24.177767
Actor loss: 26.197977
Action reg: 0.003918
  l1.weight: grad_norm = 0.317435
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 1.165227
Total gradient norm: 1.822261
=== Actor Training Debug (Iteration 3) ===
Q mean: -1.002833
Q std: 20.428360
Actor loss: 1.006695
Action reg: 0.003862
  l1.weight: grad_norm = 0.321538
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 1.320958
Total gradient norm: 2.177871
=== Actor Training Debug (Iteration 4) ===
Q mean: -0.450791
Q std: 20.376270
Actor loss: 0.454711
Action reg: 0.003921
  l1.weight: grad_norm = 0.394002
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 1.711419
Total gradient norm: 2.919887
=== Actor Training Debug (Iteration 5) ===
Q mean: -11.240426
Q std: 21.928839
Actor loss: 11.244316
Action reg: 0.003890
  l1.weight: grad_norm = 0.370411
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 1.489578
Total gradient norm: 2.455375
=== Actor Training Debug (Iteration 6) ===
Q mean: -29.218086
Q std: 23.848534
Actor loss: 29.222004
Action reg: 0.003917
  l1.weight: grad_norm = 0.303749
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 1.318721
Total gradient norm: 2.232824
=== Actor Training Debug (Iteration 7) ===
Q mean: -40.839287
Q std: 23.501768
Actor loss: 40.843212
Action reg: 0.003925
  l1.weight: grad_norm = 0.247591
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 1.098376
Total gradient norm: 1.870729
=== Actor Training Debug (Iteration 8) ===
Q mean: -35.394653
Q std: 22.472464
Actor loss: 35.398567
Action reg: 0.003912
  l1.weight: grad_norm = 0.299341
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 1.433648
Total gradient norm: 2.369671
=== Actor Training Debug (Iteration 9) ===
Q mean: -29.274513
Q std: 24.313225
Actor loss: 29.278410
Action reg: 0.003896
  l1.weight: grad_norm = 0.232421
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.940915
Total gradient norm: 1.463348
=== Actor Training Debug (Iteration 10) ===
Q mean: -20.429850
Q std: 21.111492
Actor loss: 20.433792
Action reg: 0.003942
  l1.weight: grad_norm = 0.234396
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.800990
Total gradient norm: 1.229234
=== Actor Training Debug (Iteration 11) ===
Q mean: -13.285136
Q std: 22.051785
Actor loss: 13.289102
Action reg: 0.003966
  l1.weight: grad_norm = 0.195235
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.662293
Total gradient norm: 1.071599
=== Actor Training Debug (Iteration 12) ===
Q mean: -16.313381
Q std: 20.866371
Actor loss: 16.317303
Action reg: 0.003921
  l1.weight: grad_norm = 0.189410
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.588971
Total gradient norm: 0.861801
=== Actor Training Debug (Iteration 13) ===
Q mean: -19.854719
Q std: 24.933346
Actor loss: 19.858658
Action reg: 0.003938
  l1.weight: grad_norm = 0.151017
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.509511
Total gradient norm: 0.771184
=== Actor Training Debug (Iteration 14) ===
Q mean: -30.693657
Q std: 24.702751
Actor loss: 30.697590
Action reg: 0.003932
  l1.weight: grad_norm = 0.123724
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.425453
Total gradient norm: 0.604134
=== Actor Training Debug (Iteration 15) ===
Q mean: -34.910671
Q std: 24.258410
Actor loss: 34.914600
Action reg: 0.003930
  l1.weight: grad_norm = 0.113432
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.392177
Total gradient norm: 0.573761
=== Actor Training Debug (Iteration 16) ===
Q mean: -28.824762
Q std: 22.936541
Actor loss: 28.828697
Action reg: 0.003935
  l1.weight: grad_norm = 0.118556
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.411123
Total gradient norm: 0.599922
=== Actor Training Debug (Iteration 17) ===
Q mean: -27.210930
Q std: 25.354589
Actor loss: 27.214886
Action reg: 0.003956
  l1.weight: grad_norm = 0.125025
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.391248
Total gradient norm: 0.559993
=== Actor Training Debug (Iteration 18) ===
Q mean: -17.318962
Q std: 23.374308
Actor loss: 17.322903
Action reg: 0.003940
  l1.weight: grad_norm = 0.095125
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.327551
Total gradient norm: 0.474545
=== Actor Training Debug (Iteration 19) ===
Q mean: -15.074368
Q std: 23.093058
Actor loss: 15.078296
Action reg: 0.003928
  l1.weight: grad_norm = 0.071146
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.233222
Total gradient norm: 0.360498
=== Actor Training Debug (Iteration 20) ===
Q mean: -14.447392
Q std: 23.176945
Actor loss: 14.451328
Action reg: 0.003937
  l1.weight: grad_norm = 0.145733
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.479872
Total gradient norm: 0.688613
=== Actor Training Debug (Iteration 21) ===
Q mean: -18.718597
Q std: 22.344896
Actor loss: 18.722542
Action reg: 0.003945
  l1.weight: grad_norm = 0.050766
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.177061
Total gradient norm: 0.262325
=== Actor Training Debug (Iteration 22) ===
Q mean: -31.351337
Q std: 24.802694
Actor loss: 31.355267
Action reg: 0.003930
  l1.weight: grad_norm = 0.069765
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.231228
Total gradient norm: 0.302377
=== Actor Training Debug (Iteration 23) ===
Q mean: -29.572247
Q std: 24.243952
Actor loss: 29.576212
Action reg: 0.003965
  l1.weight: grad_norm = 0.145657
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.583744
Total gradient norm: 0.932708
=== Actor Training Debug (Iteration 24) ===
Q mean: -30.744286
Q std: 24.429243
Actor loss: 30.748226
Action reg: 0.003941
  l1.weight: grad_norm = 0.137622
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.470720
Total gradient norm: 0.679959
=== Actor Training Debug (Iteration 25) ===
Q mean: -25.960369
Q std: 22.684700
Actor loss: 25.964310
Action reg: 0.003941
  l1.weight: grad_norm = 0.118985
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.373013
Total gradient norm: 0.547362
=== Actor Training Debug (Iteration 26) ===
Q mean: -18.496136
Q std: 20.741817
Actor loss: 18.500069
Action reg: 0.003933
  l1.weight: grad_norm = 0.129426
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.442342
Total gradient norm: 0.686429
=== Actor Training Debug (Iteration 27) ===
Q mean: -15.111032
Q std: 21.810610
Actor loss: 15.114993
Action reg: 0.003960
  l1.weight: grad_norm = 0.133036
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.426778
Total gradient norm: 0.603505
=== Actor Training Debug (Iteration 28) ===
Q mean: -16.361231
Q std: 21.381069
Actor loss: 16.365162
Action reg: 0.003931
  l1.weight: grad_norm = 0.126815
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.453912
Total gradient norm: 0.661093
=== Actor Training Debug (Iteration 29) ===
Q mean: -22.069462
Q std: 21.245592
Actor loss: 22.073429
Action reg: 0.003967
  l1.weight: grad_norm = 0.163217
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.549791
Total gradient norm: 0.835617
=== Actor Training Debug (Iteration 30) ===
Q mean: -30.232174
Q std: 22.648321
Actor loss: 30.236128
Action reg: 0.003954
  l1.weight: grad_norm = 0.072232
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.263242
Total gradient norm: 0.398966
=== Actor Training Debug (Iteration 31) ===
Q mean: -35.087845
Q std: 24.789921
Actor loss: 35.091785
Action reg: 0.003941
  l1.weight: grad_norm = 0.055350
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.163889
Total gradient norm: 0.224682
=== Actor Training Debug (Iteration 32) ===
Q mean: -31.316885
Q std: 23.154562
Actor loss: 31.320812
Action reg: 0.003927
  l1.weight: grad_norm = 0.070988
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.273047
Total gradient norm: 0.427631
=== Actor Training Debug (Iteration 33) ===
Q mean: -27.019562
Q std: 20.767462
Actor loss: 27.023407
Action reg: 0.003844
  l1.weight: grad_norm = 0.145054
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.497081
Total gradient norm: 0.726540
=== Actor Training Debug (Iteration 34) ===
Q mean: -17.701096
Q std: 19.874985
Actor loss: 17.705004
Action reg: 0.003907
  l1.weight: grad_norm = 0.107972
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.349314
Total gradient norm: 0.473237
=== Actor Training Debug (Iteration 35) ===
Q mean: -19.758959
Q std: 21.223352
Actor loss: 19.762863
Action reg: 0.003905
  l1.weight: grad_norm = 0.076849
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.260452
Total gradient norm: 0.372167
=== Actor Training Debug (Iteration 36) ===
Q mean: -21.911041
Q std: 23.694647
Actor loss: 21.914976
Action reg: 0.003934
  l1.weight: grad_norm = 0.101759
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.314263
Total gradient norm: 0.436763
=== Actor Training Debug (Iteration 37) ===
Q mean: -24.261332
Q std: 22.572062
Actor loss: 24.265276
Action reg: 0.003944
  l1.weight: grad_norm = 0.096909
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.330409
Total gradient norm: 0.476473
=== Actor Training Debug (Iteration 38) ===
Q mean: -28.624851
Q std: 22.159740
Actor loss: 28.628788
Action reg: 0.003936
  l1.weight: grad_norm = 0.059734
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.214355
Total gradient norm: 0.342117
=== Actor Training Debug (Iteration 39) ===
Q mean: -26.761841
Q std: 23.784294
Actor loss: 26.765724
Action reg: 0.003883
  l1.weight: grad_norm = 0.091359
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.338342
Total gradient norm: 0.564991
=== Actor Training Debug (Iteration 40) ===
Q mean: -22.692245
Q std: 21.667601
Actor loss: 22.696203
Action reg: 0.003958
  l1.weight: grad_norm = 0.127635
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.441531
Total gradient norm: 0.721511
=== Actor Training Debug (Iteration 41) ===
Q mean: -21.984791
Q std: 22.320971
Actor loss: 21.988747
Action reg: 0.003956
  l1.weight: grad_norm = 0.102362
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.308380
Total gradient norm: 0.408210
=== Actor Training Debug (Iteration 42) ===
Q mean: -22.568834
Q std: 20.250214
Actor loss: 22.572805
Action reg: 0.003970
  l1.weight: grad_norm = 0.140631
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.450633
Total gradient norm: 0.693556
=== Actor Training Debug (Iteration 43) ===
Q mean: -30.314709
Q std: 23.231325
Actor loss: 30.318663
Action reg: 0.003953
  l1.weight: grad_norm = 0.045662
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.145227
Total gradient norm: 0.218331
=== Actor Training Debug (Iteration 44) ===
Q mean: -26.167454
Q std: 20.956797
Actor loss: 26.171419
Action reg: 0.003966
  l1.weight: grad_norm = 0.059040
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.174490
Total gradient norm: 0.230182
=== Actor Training Debug (Iteration 45) ===
Q mean: -23.122219
Q std: 20.784597
Actor loss: 23.126104
Action reg: 0.003886
  l1.weight: grad_norm = 0.034367
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.117993
Total gradient norm: 0.183651
=== Actor Training Debug (Iteration 46) ===
Q mean: -23.707897
Q std: 24.153563
Actor loss: 23.711853
Action reg: 0.003955
  l1.weight: grad_norm = 0.083285
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.273067
Total gradient norm: 0.405794
=== Actor Training Debug (Iteration 47) ===
Q mean: -20.267033
Q std: 22.800058
Actor loss: 20.270967
Action reg: 0.003936
  l1.weight: grad_norm = 0.050296
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.188998
Total gradient norm: 0.325187
=== Actor Training Debug (Iteration 48) ===
Q mean: -22.078840
Q std: 23.991426
Actor loss: 22.082767
Action reg: 0.003928
  l1.weight: grad_norm = 0.145687
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.507258
Total gradient norm: 0.786858
=== Actor Training Debug (Iteration 49) ===
Q mean: -23.078236
Q std: 21.696646
Actor loss: 23.082197
Action reg: 0.003962
  l1.weight: grad_norm = 0.082494
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.263738
Total gradient norm: 0.367261
=== Actor Training Debug (Iteration 50) ===
Q mean: -26.409412
Q std: 25.008818
Actor loss: 26.413380
Action reg: 0.003968
  l1.weight: grad_norm = 0.054174
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.173148
Total gradient norm: 0.260632
=== Actor Training Debug (Iteration 51) ===
Q mean: -27.357010
Q std: 23.791740
Actor loss: 27.360947
Action reg: 0.003938
  l1.weight: grad_norm = 0.075018
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.254004
Total gradient norm: 0.415260
=== Actor Training Debug (Iteration 52) ===
Q mean: -25.340290
Q std: 23.994425
Actor loss: 25.344263
Action reg: 0.003974
  l1.weight: grad_norm = 0.066828
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.251383
Total gradient norm: 0.379574
=== Actor Training Debug (Iteration 53) ===
Q mean: -22.097757
Q std: 24.488611
Actor loss: 22.101698
Action reg: 0.003941
  l1.weight: grad_norm = 0.029188
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.098531
Total gradient norm: 0.141162
=== Actor Training Debug (Iteration 54) ===
Q mean: -25.701551
Q std: 23.823898
Actor loss: 25.705477
Action reg: 0.003925
  l1.weight: grad_norm = 0.066946
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.213256
Total gradient norm: 0.295522
=== Actor Training Debug (Iteration 55) ===
Q mean: -23.923141
Q std: 23.204105
Actor loss: 23.927101
Action reg: 0.003959
  l1.weight: grad_norm = 0.078986
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.265935
Total gradient norm: 0.414170
=== Actor Training Debug (Iteration 56) ===
Q mean: -27.995077
Q std: 22.483690
Actor loss: 27.999037
Action reg: 0.003960
  l1.weight: grad_norm = 0.065156
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.236877
Total gradient norm: 0.388559
=== Actor Training Debug (Iteration 57) ===
Q mean: -25.943293
Q std: 20.222044
Actor loss: 25.947235
Action reg: 0.003942
  l1.weight: grad_norm = 0.066457
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.222108
Total gradient norm: 0.342812
=== Actor Training Debug (Iteration 58) ===
Q mean: -21.674488
Q std: 19.294582
Actor loss: 21.678438
Action reg: 0.003951
  l1.weight: grad_norm = 0.017875
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.058863
Total gradient norm: 0.090114
=== Actor Training Debug (Iteration 59) ===
Q mean: -18.787468
Q std: 17.830748
Actor loss: 18.791441
Action reg: 0.003974
  l1.weight: grad_norm = 0.051137
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.162041
Total gradient norm: 0.237939
=== Actor Training Debug (Iteration 60) ===
Q mean: -22.498848
Q std: 20.506634
Actor loss: 22.502806
Action reg: 0.003958
  l1.weight: grad_norm = 0.022974
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.090737
Total gradient norm: 0.129407
=== Actor Training Debug (Iteration 61) ===
Q mean: -28.026665
Q std: 25.168974
Actor loss: 28.030594
Action reg: 0.003930
  l1.weight: grad_norm = 0.119708
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.362392
Total gradient norm: 0.516057
=== Actor Training Debug (Iteration 62) ===
Q mean: -25.485250
Q std: 24.268883
Actor loss: 25.489214
Action reg: 0.003964
  l1.weight: grad_norm = 0.032295
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.107702
Total gradient norm: 0.160455
=== Actor Training Debug (Iteration 63) ===
Q mean: -23.570988
Q std: 24.344940
Actor loss: 23.574966
Action reg: 0.003979
  l1.weight: grad_norm = 0.018772
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.066307
Total gradient norm: 0.101763
=== Actor Training Debug (Iteration 64) ===
Q mean: -20.649046
Q std: 21.095713
Actor loss: 20.653027
Action reg: 0.003981
  l1.weight: grad_norm = 0.009598
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.034347
Total gradient norm: 0.053720
=== Actor Training Debug (Iteration 65) ===
Q mean: -27.735559
Q std: 23.048170
Actor loss: 27.739550
Action reg: 0.003989
  l1.weight: grad_norm = 0.046557
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.147842
Total gradient norm: 0.205929
=== Actor Training Debug (Iteration 66) ===
Q mean: -23.715607
Q std: 21.412903
Actor loss: 23.719587
Action reg: 0.003981
  l1.weight: grad_norm = 0.041845
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.140719
Total gradient norm: 0.211126
=== Actor Training Debug (Iteration 67) ===
Q mean: -22.817284
Q std: 21.297487
Actor loss: 22.821259
Action reg: 0.003975
  l1.weight: grad_norm = 0.064334
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.225194
Total gradient norm: 0.316586
=== Actor Training Debug (Iteration 68) ===
Q mean: -23.932234
Q std: 21.481625
Actor loss: 23.936195
Action reg: 0.003962
  l1.weight: grad_norm = 0.027009
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.087566
Total gradient norm: 0.131136
=== Actor Training Debug (Iteration 69) ===
Q mean: -24.299828
Q std: 21.117079
Actor loss: 24.303799
Action reg: 0.003971
  l1.weight: grad_norm = 0.080096
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.288240
Total gradient norm: 0.448733
=== Actor Training Debug (Iteration 70) ===
Q mean: -23.110741
Q std: 22.686453
Actor loss: 23.114723
Action reg: 0.003982
  l1.weight: grad_norm = 0.036313
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.120234
Total gradient norm: 0.178215
=== Actor Training Debug (Iteration 71) ===
Q mean: -26.204243
Q std: 25.006798
Actor loss: 26.208191
Action reg: 0.003949
  l1.weight: grad_norm = 0.049533
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.168472
Total gradient norm: 0.224188
=== Actor Training Debug (Iteration 72) ===
Q mean: -23.854610
Q std: 23.580469
Actor loss: 23.858526
Action reg: 0.003916
  l1.weight: grad_norm = 0.047405
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.186671
Total gradient norm: 0.301483
=== Actor Training Debug (Iteration 73) ===
Q mean: -25.428465
Q std: 24.823700
Actor loss: 25.432430
Action reg: 0.003965
  l1.weight: grad_norm = 0.025149
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.076801
Total gradient norm: 0.102547
=== Actor Training Debug (Iteration 74) ===
Q mean: -24.235077
Q std: 23.483221
Actor loss: 24.239040
Action reg: 0.003963
  l1.weight: grad_norm = 0.048857
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.172225
Total gradient norm: 0.247746
=== Actor Training Debug (Iteration 75) ===
Q mean: -24.154158
Q std: 22.326618
Actor loss: 24.158106
Action reg: 0.003948
  l1.weight: grad_norm = 0.062099
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.200433
Total gradient norm: 0.294592
=== Actor Training Debug (Iteration 76) ===
Q mean: -25.592667
Q std: 21.742931
Actor loss: 25.596647
Action reg: 0.003980
  l1.weight: grad_norm = 0.023000
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.084221
Total gradient norm: 0.119498
=== Actor Training Debug (Iteration 77) ===
Q mean: -28.940565
Q std: 22.663553
Actor loss: 28.944500
Action reg: 0.003934
  l1.weight: grad_norm = 0.048644
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.156490
Total gradient norm: 0.226389
=== Actor Training Debug (Iteration 78) ===
Q mean: -22.328651
Q std: 20.038750
Actor loss: 22.332630
Action reg: 0.003978
  l1.weight: grad_norm = 0.035444
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.146427
Total gradient norm: 0.212256
=== Actor Training Debug (Iteration 79) ===
Q mean: -21.141949
Q std: 18.700716
Actor loss: 21.145910
Action reg: 0.003962
  l1.weight: grad_norm = 0.036962
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.135803
Total gradient norm: 0.209453
=== Actor Training Debug (Iteration 80) ===
Q mean: -25.457729
Q std: 23.444132
Actor loss: 25.461706
Action reg: 0.003977
  l1.weight: grad_norm = 0.033937
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.113890
Total gradient norm: 0.166078
=== Actor Training Debug (Iteration 81) ===
Q mean: -22.686422
Q std: 23.722486
Actor loss: 22.690357
Action reg: 0.003936
  l1.weight: grad_norm = 0.064576
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.213527
Total gradient norm: 0.338175
=== Actor Training Debug (Iteration 82) ===
Q mean: -24.619379
Q std: 22.596386
Actor loss: 24.623343
Action reg: 0.003964
  l1.weight: grad_norm = 0.007546
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.027988
Total gradient norm: 0.040786
=== Actor Training Debug (Iteration 83) ===
Q mean: -22.395325
Q std: 23.356529
Actor loss: 22.399275
Action reg: 0.003950
  l1.weight: grad_norm = 0.020221
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.065096
Total gradient norm: 0.098761
=== Actor Training Debug (Iteration 84) ===
Q mean: -22.986822
Q std: 20.352177
Actor loss: 22.990742
Action reg: 0.003920
  l1.weight: grad_norm = 0.015935
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.055594
Total gradient norm: 0.086697
=== Actor Training Debug (Iteration 85) ===
Q mean: -26.119846
Q std: 21.856010
Actor loss: 26.123810
Action reg: 0.003964
  l1.weight: grad_norm = 0.028182
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.088071
Total gradient norm: 0.132128
=== Actor Training Debug (Iteration 86) ===
Q mean: -27.367476
Q std: 22.631393
Actor loss: 27.371458
Action reg: 0.003982
  l1.weight: grad_norm = 0.030298
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.103633
Total gradient norm: 0.143997
=== Actor Training Debug (Iteration 87) ===
Q mean: -21.536831
Q std: 21.733297
Actor loss: 21.540794
Action reg: 0.003964
  l1.weight: grad_norm = 0.038157
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.125561
Total gradient norm: 0.190215
=== Actor Training Debug (Iteration 88) ===
Q mean: -17.509579
Q std: 22.344183
Actor loss: 17.513477
Action reg: 0.003899
  l1.weight: grad_norm = 0.019618
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.073544
Total gradient norm: 0.103823
=== Actor Training Debug (Iteration 89) ===
Q mean: -19.924166
Q std: 23.985184
Actor loss: 19.928120
Action reg: 0.003953
  l1.weight: grad_norm = 0.008839
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.029785
Total gradient norm: 0.043065
=== Actor Training Debug (Iteration 90) ===
Q mean: -21.925579
Q std: 24.867977
Actor loss: 21.929579
Action reg: 0.003999
  l1.weight: grad_norm = 0.007636
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.026974
Total gradient norm: 0.038576
=== Actor Training Debug (Iteration 91) ===
Q mean: -24.838440
Q std: 22.223869
Actor loss: 24.842422
Action reg: 0.003983
  l1.weight: grad_norm = 0.013790
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.048363
Total gradient norm: 0.070093
=== Actor Training Debug (Iteration 92) ===
Q mean: -28.902798
Q std: 22.057842
Actor loss: 28.906792
Action reg: 0.003994
  l1.weight: grad_norm = 0.040920
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.133044
Total gradient norm: 0.187492
=== Actor Training Debug (Iteration 93) ===
Q mean: -29.894981
Q std: 22.524902
Actor loss: 29.898949
Action reg: 0.003968
  l1.weight: grad_norm = 0.024335
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.089021
Total gradient norm: 0.143334
=== Actor Training Debug (Iteration 94) ===
Q mean: -26.382446
Q std: 22.707106
Actor loss: 26.386353
Action reg: 0.003906
  l1.weight: grad_norm = 0.028636
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.094876
Total gradient norm: 0.139242
=== Actor Training Debug (Iteration 95) ===
Q mean: -21.872490
Q std: 23.140505
Actor loss: 21.876471
Action reg: 0.003980
  l1.weight: grad_norm = 0.044343
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.132017
Total gradient norm: 0.191517
=== Actor Training Debug (Iteration 96) ===
Q mean: -19.077755
Q std: 19.913857
Actor loss: 19.081703
Action reg: 0.003948
  l1.weight: grad_norm = 0.056175
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.176245
Total gradient norm: 0.270214
=== Actor Training Debug (Iteration 97) ===
Q mean: -26.324219
Q std: 21.095793
Actor loss: 26.328182
Action reg: 0.003963
  l1.weight: grad_norm = 0.025608
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.079948
Total gradient norm: 0.115021
=== Actor Training Debug (Iteration 98) ===
Q mean: -23.700996
Q std: 21.071234
Actor loss: 23.704929
Action reg: 0.003932
  l1.weight: grad_norm = 0.057384
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.202863
Total gradient norm: 0.305337
=== Actor Training Debug (Iteration 99) ===
Q mean: -26.037323
Q std: 21.271061
Actor loss: 26.041273
Action reg: 0.003951
  l1.weight: grad_norm = 0.014684
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.045888
Total gradient norm: 0.072218
=== Actor Training Debug (Iteration 100) ===
Q mean: -24.682678
Q std: 24.826618
Actor loss: 24.686642
Action reg: 0.003963
  l1.weight: grad_norm = 0.060012
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.195972
Total gradient norm: 0.297548
Episode 51: Steps=100, Reward=-259.793, Buffer_size=5100
=== Actor Training Debug (Iteration 101) ===
Q mean: -22.266989
Q std: 22.980333
Actor loss: 22.270939
Action reg: 0.003950
  l1.weight: grad_norm = 0.014671
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.048070
Total gradient norm: 0.074831
=== Actor Training Debug (Iteration 102) ===
Q mean: -22.472445
Q std: 22.965616
Actor loss: 22.476374
Action reg: 0.003928
  l1.weight: grad_norm = 0.047854
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.180186
Total gradient norm: 0.297626
=== Actor Training Debug (Iteration 103) ===
Q mean: -22.291294
Q std: 21.884163
Actor loss: 22.295197
Action reg: 0.003903
  l1.weight: grad_norm = 0.039557
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.134356
Total gradient norm: 0.203750
=== Actor Training Debug (Iteration 104) ===
Q mean: -24.064047
Q std: 21.025118
Actor loss: 24.067997
Action reg: 0.003950
  l1.weight: grad_norm = 0.022649
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.087797
Total gradient norm: 0.144043
=== Actor Training Debug (Iteration 105) ===
Q mean: -26.157366
Q std: 19.066483
Actor loss: 26.161234
Action reg: 0.003868
  l1.weight: grad_norm = 0.048982
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.152297
Total gradient norm: 0.228744
=== Actor Training Debug (Iteration 106) ===
Q mean: -25.911802
Q std: 21.947432
Actor loss: 25.915735
Action reg: 0.003933
  l1.weight: grad_norm = 0.011662
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.043327
Total gradient norm: 0.066676
=== Actor Training Debug (Iteration 107) ===
Q mean: -25.702826
Q std: 22.210323
Actor loss: 25.706814
Action reg: 0.003989
  l1.weight: grad_norm = 0.022784
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.087359
Total gradient norm: 0.134890
=== Actor Training Debug (Iteration 108) ===
Q mean: -22.701302
Q std: 19.776947
Actor loss: 22.705250
Action reg: 0.003949
  l1.weight: grad_norm = 0.075022
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.287138
Total gradient norm: 0.435323
=== Actor Training Debug (Iteration 109) ===
Q mean: -24.405405
Q std: 20.743362
Actor loss: 24.409348
Action reg: 0.003943
  l1.weight: grad_norm = 0.095779
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.345118
Total gradient norm: 0.518233
=== Actor Training Debug (Iteration 110) ===
Q mean: -24.372101
Q std: 22.830956
Actor loss: 24.376040
Action reg: 0.003939
  l1.weight: grad_norm = 0.071276
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.279984
Total gradient norm: 0.471336
=== Actor Training Debug (Iteration 111) ===
Q mean: -25.538067
Q std: 21.249123
Actor loss: 25.541965
Action reg: 0.003899
  l1.weight: grad_norm = 0.059218
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.216958
Total gradient norm: 0.351074
=== Actor Training Debug (Iteration 112) ===
Q mean: -28.137280
Q std: 22.898235
Actor loss: 28.141203
Action reg: 0.003924
  l1.weight: grad_norm = 0.119138
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.481270
Total gradient norm: 0.807840
=== Actor Training Debug (Iteration 113) ===
Q mean: -24.804220
Q std: 22.899817
Actor loss: 24.808201
Action reg: 0.003981
  l1.weight: grad_norm = 0.163249
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.772571
Total gradient norm: 1.330213
=== Actor Training Debug (Iteration 114) ===
Q mean: -21.684946
Q std: 20.720104
Actor loss: 21.688915
Action reg: 0.003968
  l1.weight: grad_norm = 0.136529
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.496582
Total gradient norm: 0.769762
=== Actor Training Debug (Iteration 115) ===
Q mean: -23.068367
Q std: 21.612471
Actor loss: 23.072290
Action reg: 0.003923
  l1.weight: grad_norm = 0.079195
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.302876
Total gradient norm: 0.450666
=== Actor Training Debug (Iteration 116) ===
Q mean: -25.738720
Q std: 22.464928
Actor loss: 25.742672
Action reg: 0.003952
  l1.weight: grad_norm = 0.078586
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.277921
Total gradient norm: 0.371409
=== Actor Training Debug (Iteration 117) ===
Q mean: -28.600534
Q std: 21.113520
Actor loss: 28.604492
Action reg: 0.003957
  l1.weight: grad_norm = 0.088386
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.351234
Total gradient norm: 0.551714
=== Actor Training Debug (Iteration 118) ===
Q mean: -29.312500
Q std: 19.202730
Actor loss: 29.316442
Action reg: 0.003943
  l1.weight: grad_norm = 0.095766
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.343685
Total gradient norm: 0.497337
=== Actor Training Debug (Iteration 119) ===
Q mean: -28.456387
Q std: 21.327108
Actor loss: 28.460323
Action reg: 0.003937
  l1.weight: grad_norm = 0.006408
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.019478
Total gradient norm: 0.029124
=== Actor Training Debug (Iteration 120) ===
Q mean: -21.981321
Q std: 22.175804
Actor loss: 21.985256
Action reg: 0.003936
  l1.weight: grad_norm = 0.019061
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.059350
Total gradient norm: 0.084938
=== Actor Training Debug (Iteration 121) ===
Q mean: -20.439301
Q std: 21.719547
Actor loss: 20.443281
Action reg: 0.003981
  l1.weight: grad_norm = 0.060688
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.202519
Total gradient norm: 0.280125
=== Actor Training Debug (Iteration 122) ===
Q mean: -21.133568
Q std: 19.619001
Actor loss: 21.137506
Action reg: 0.003938
  l1.weight: grad_norm = 0.006958
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.023204
Total gradient norm: 0.033702
=== Actor Training Debug (Iteration 123) ===
Q mean: -24.962351
Q std: 22.867701
Actor loss: 24.966286
Action reg: 0.003934
  l1.weight: grad_norm = 0.009612
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.031467
Total gradient norm: 0.046996
=== Actor Training Debug (Iteration 124) ===
Q mean: -27.522734
Q std: 23.608423
Actor loss: 27.526699
Action reg: 0.003966
  l1.weight: grad_norm = 0.015372
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.051711
Total gradient norm: 0.075360
=== Actor Training Debug (Iteration 125) ===
Q mean: -23.538960
Q std: 23.438351
Actor loss: 23.542925
Action reg: 0.003966
  l1.weight: grad_norm = 0.021354
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.067964
Total gradient norm: 0.098507
=== Actor Training Debug (Iteration 126) ===
Q mean: -22.546143
Q std: 20.473688
Actor loss: 22.550097
Action reg: 0.003955
  l1.weight: grad_norm = 0.000476
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.001854
Total gradient norm: 0.003496
=== Actor Training Debug (Iteration 127) ===
Q mean: -24.712543
Q std: 21.604969
Actor loss: 24.716528
Action reg: 0.003984
  l1.weight: grad_norm = 0.013279
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.037991
Total gradient norm: 0.054470
=== Actor Training Debug (Iteration 128) ===
Q mean: -25.218048
Q std: 20.533289
Actor loss: 25.222015
Action reg: 0.003968
  l1.weight: grad_norm = 0.012149
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.044536
Total gradient norm: 0.076666
=== Actor Training Debug (Iteration 129) ===
Q mean: -28.627460
Q std: 24.127083
Actor loss: 28.631443
Action reg: 0.003982
  l1.weight: grad_norm = 0.022316
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.070163
Total gradient norm: 0.101467
=== Actor Training Debug (Iteration 130) ===
Q mean: -23.854710
Q std: 19.937965
Actor loss: 23.858692
Action reg: 0.003983
  l1.weight: grad_norm = 0.019169
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.072092
Total gradient norm: 0.111548
=== Actor Training Debug (Iteration 131) ===
Q mean: -24.771156
Q std: 20.946342
Actor loss: 24.775110
Action reg: 0.003954
  l1.weight: grad_norm = 0.001188
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.004311
Total gradient norm: 0.006987
=== Actor Training Debug (Iteration 132) ===
Q mean: -26.788301
Q std: 23.177399
Actor loss: 26.792282
Action reg: 0.003981
  l1.weight: grad_norm = 0.103144
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.362845
Total gradient norm: 0.575817
=== Actor Training Debug (Iteration 133) ===
Q mean: -25.381123
Q std: 22.985107
Actor loss: 25.385120
Action reg: 0.003999
  l1.weight: grad_norm = 0.026889
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.074812
Total gradient norm: 0.110613
=== Actor Training Debug (Iteration 134) ===
Q mean: -21.643126
Q std: 20.445993
Actor loss: 21.647030
Action reg: 0.003904
  l1.weight: grad_norm = 0.014709
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.047495
Total gradient norm: 0.066990
=== Actor Training Debug (Iteration 135) ===
Q mean: -22.394524
Q std: 21.860847
Actor loss: 22.398491
Action reg: 0.003968
  l1.weight: grad_norm = 0.003597
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.012544
Total gradient norm: 0.018283
=== Actor Training Debug (Iteration 136) ===
Q mean: -27.784466
Q std: 23.609745
Actor loss: 27.788374
Action reg: 0.003908
  l1.weight: grad_norm = 0.004984
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.016659
Total gradient norm: 0.024538
=== Actor Training Debug (Iteration 137) ===
Q mean: -27.476276
Q std: 23.507118
Actor loss: 27.480261
Action reg: 0.003984
  l1.weight: grad_norm = 0.002628
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.008963
Total gradient norm: 0.014246
=== Actor Training Debug (Iteration 138) ===
Q mean: -25.486118
Q std: 22.349237
Actor loss: 25.490070
Action reg: 0.003952
  l1.weight: grad_norm = 0.006033
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.018647
Total gradient norm: 0.025891
=== Actor Training Debug (Iteration 139) ===
Q mean: -24.679146
Q std: 22.576191
Actor loss: 24.683100
Action reg: 0.003954
  l1.weight: grad_norm = 0.004959
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.016205
Total gradient norm: 0.023453
=== Actor Training Debug (Iteration 140) ===
Q mean: -21.543907
Q std: 20.974712
Actor loss: 21.547874
Action reg: 0.003967
  l1.weight: grad_norm = 0.068530
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.247386
Total gradient norm: 0.377885
=== Actor Training Debug (Iteration 141) ===
Q mean: -24.709211
Q std: 20.693977
Actor loss: 24.713148
Action reg: 0.003936
  l1.weight: grad_norm = 0.020673
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.069373
Total gradient norm: 0.110551
=== Actor Training Debug (Iteration 142) ===
Q mean: -29.574848
Q std: 22.721907
Actor loss: 29.578798
Action reg: 0.003950
  l1.weight: grad_norm = 0.057397
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.184522
Total gradient norm: 0.274413
=== Actor Training Debug (Iteration 143) ===
Q mean: -29.560179
Q std: 23.105204
Actor loss: 29.564117
Action reg: 0.003938
  l1.weight: grad_norm = 0.004276
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.014853
Total gradient norm: 0.022218
=== Actor Training Debug (Iteration 144) ===
Q mean: -26.822304
Q std: 23.781950
Actor loss: 26.826286
Action reg: 0.003982
  l1.weight: grad_norm = 0.016858
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.055549
Total gradient norm: 0.081649
=== Actor Training Debug (Iteration 145) ===
Q mean: -23.925470
Q std: 22.017849
Actor loss: 23.929350
Action reg: 0.003879
  l1.weight: grad_norm = 0.000710
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.003435
Total gradient norm: 0.007115
=== Actor Training Debug (Iteration 146) ===
Q mean: -21.729357
Q std: 22.828981
Actor loss: 21.733339
Action reg: 0.003982
  l1.weight: grad_norm = 0.028950
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.084695
Total gradient norm: 0.110726
=== Actor Training Debug (Iteration 147) ===
Q mean: -23.319136
Q std: 19.586651
Actor loss: 23.323135
Action reg: 0.004000
  l1.weight: grad_norm = 0.010996
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.040271
Total gradient norm: 0.059911
=== Actor Training Debug (Iteration 148) ===
Q mean: -27.383583
Q std: 21.178293
Actor loss: 27.387568
Action reg: 0.003984
  l1.weight: grad_norm = 0.003584
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.013620
Total gradient norm: 0.020151
=== Actor Training Debug (Iteration 149) ===
Q mean: -29.325724
Q std: 22.613934
Actor loss: 29.329693
Action reg: 0.003969
  l1.weight: grad_norm = 0.014804
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.049616
Total gradient norm: 0.077100
=== Actor Training Debug (Iteration 150) ===
Q mean: -26.782162
Q std: 21.849293
Actor loss: 26.786146
Action reg: 0.003984
  l1.weight: grad_norm = 0.025030
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.082657
Total gradient norm: 0.126770
=== Actor Training Debug (Iteration 151) ===
Q mean: -24.215420
Q std: 22.349955
Actor loss: 24.219374
Action reg: 0.003955
  l1.weight: grad_norm = 0.000558
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.002003
Total gradient norm: 0.003601
=== Actor Training Debug (Iteration 152) ===
Q mean: -19.981655
Q std: 21.674034
Actor loss: 19.985609
Action reg: 0.003955
  l1.weight: grad_norm = 0.000435
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.001853
Total gradient norm: 0.003470
=== Actor Training Debug (Iteration 153) ===
Q mean: -23.331947
Q std: 22.383064
Actor loss: 23.335886
Action reg: 0.003938
  l1.weight: grad_norm = 0.006839
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.025923
Total gradient norm: 0.038746
=== Actor Training Debug (Iteration 154) ===
Q mean: -26.093483
Q std: 22.414516
Actor loss: 26.097424
Action reg: 0.003940
  l1.weight: grad_norm = 0.000504
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.001988
Total gradient norm: 0.004006
=== Actor Training Debug (Iteration 155) ===
Q mean: -27.248617
Q std: 22.428947
Actor loss: 27.252571
Action reg: 0.003955
  l1.weight: grad_norm = 0.003182
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.010119
Total gradient norm: 0.015153
=== Actor Training Debug (Iteration 156) ===
Q mean: -24.240067
Q std: 20.472702
Actor loss: 24.244036
Action reg: 0.003970
  l1.weight: grad_norm = 0.000371
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.001476
Total gradient norm: 0.002720
=== Actor Training Debug (Iteration 157) ===
Q mean: -22.916126
Q std: 19.867838
Actor loss: 22.920078
Action reg: 0.003952
  l1.weight: grad_norm = 0.033678
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.108717
Total gradient norm: 0.167178
=== Actor Training Debug (Iteration 158) ===
Q mean: -23.543274
Q std: 20.672241
Actor loss: 23.547258
Action reg: 0.003985
  l1.weight: grad_norm = 0.000543
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.001893
Total gradient norm: 0.003091
=== Actor Training Debug (Iteration 159) ===
Q mean: -25.064159
Q std: 20.209007
Actor loss: 25.068130
Action reg: 0.003970
  l1.weight: grad_norm = 0.000255
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.000906
Total gradient norm: 0.001574
=== Actor Training Debug (Iteration 160) ===
Q mean: -30.519539
Q std: 21.920822
Actor loss: 30.523508
Action reg: 0.003970
  l1.weight: grad_norm = 0.000744
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.002308
Total gradient norm: 0.003465
=== Actor Training Debug (Iteration 161) ===
Q mean: -28.348026
Q std: 22.779104
Actor loss: 28.352011
Action reg: 0.003985
  l1.weight: grad_norm = 0.000647
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.002042
Total gradient norm: 0.003373
=== Actor Training Debug (Iteration 162) ===
Q mean: -27.929865
Q std: 21.875957
Actor loss: 27.933834
Action reg: 0.003970
  l1.weight: grad_norm = 0.000378
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.001482
Total gradient norm: 0.002514
=== Actor Training Debug (Iteration 163) ===
Q mean: -19.218397
Q std: 20.195650
Actor loss: 19.222321
Action reg: 0.003923
  l1.weight: grad_norm = 0.028532
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.094895
Total gradient norm: 0.144744
=== Actor Training Debug (Iteration 164) ===
Q mean: -20.603138
Q std: 22.619307
Actor loss: 20.607092
Action reg: 0.003955
  l1.weight: grad_norm = 0.000428
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.001738
Total gradient norm: 0.003642
=== Actor Training Debug (Iteration 165) ===
Q mean: -22.769087
Q std: 21.334064
Actor loss: 22.773012
Action reg: 0.003924
  l1.weight: grad_norm = 0.000518
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.002284
Total gradient norm: 0.004623
=== Actor Training Debug (Iteration 166) ===
Q mean: -30.536932
Q std: 23.219242
Actor loss: 30.540888
Action reg: 0.003955
  l1.weight: grad_norm = 0.000334
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.001277
Total gradient norm: 0.002399
=== Actor Training Debug (Iteration 167) ===
Q mean: -31.375422
Q std: 22.243519
Actor loss: 31.379393
Action reg: 0.003971
  l1.weight: grad_norm = 0.000298
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.001048
Total gradient norm: 0.001852
=== Actor Training Debug (Iteration 168) ===
Q mean: -30.176224
Q std: 20.867710
Actor loss: 30.180178
Action reg: 0.003955
  l1.weight: grad_norm = 0.000384
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.001339
Total gradient norm: 0.002407
=== Actor Training Debug (Iteration 169) ===
Q mean: -23.740292
Q std: 21.061703
Actor loss: 23.744291
Action reg: 0.004000
  l1.weight: grad_norm = 0.002788
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.010147
Total gradient norm: 0.015162
=== Actor Training Debug (Iteration 170) ===
Q mean: -21.551430
Q std: 20.575279
Actor loss: 21.555368
Action reg: 0.003939
  l1.weight: grad_norm = 0.002356
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.006974
Total gradient norm: 0.010846
=== Actor Training Debug (Iteration 171) ===
Q mean: -26.849981
Q std: 20.664631
Actor loss: 26.853920
Action reg: 0.003939
  l1.weight: grad_norm = 0.005350
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.018363
Total gradient norm: 0.027836
=== Actor Training Debug (Iteration 172) ===
Q mean: -31.548882
Q std: 23.113287
Actor loss: 31.552866
Action reg: 0.003985
  l1.weight: grad_norm = 0.001059
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.003877
Total gradient norm: 0.005542
=== Actor Training Debug (Iteration 173) ===
Q mean: -30.809238
Q std: 20.201126
Actor loss: 30.813208
Action reg: 0.003970
  l1.weight: grad_norm = 0.000636
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.002205
Total gradient norm: 0.003750
=== Actor Training Debug (Iteration 174) ===
Q mean: -26.274685
Q std: 21.817499
Actor loss: 26.278595
Action reg: 0.003910
  l1.weight: grad_norm = 0.000571
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.002741
Total gradient norm: 0.005865
=== Actor Training Debug (Iteration 175) ===
Q mean: -21.336212
Q std: 21.899321
Actor loss: 21.340212
Action reg: 0.003999
  l1.weight: grad_norm = 0.003783
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.013728
Total gradient norm: 0.020267
=== Actor Training Debug (Iteration 176) ===
Q mean: -19.300543
Q std: 19.937237
Actor loss: 19.304514
Action reg: 0.003970
  l1.weight: grad_norm = 0.000391
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.001439
Total gradient norm: 0.002637
=== Actor Training Debug (Iteration 177) ===
Q mean: -20.998379
Q std: 20.485563
Actor loss: 21.002331
Action reg: 0.003952
  l1.weight: grad_norm = 0.038259
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.127742
Total gradient norm: 0.188224
=== Actor Training Debug (Iteration 178) ===
Q mean: -27.489351
Q std: 22.128111
Actor loss: 27.493277
Action reg: 0.003924
  l1.weight: grad_norm = 0.001798
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.006483
Total gradient norm: 0.010599
=== Actor Training Debug (Iteration 179) ===
Q mean: -29.756159
Q std: 21.755383
Actor loss: 29.760113
Action reg: 0.003955
  l1.weight: grad_norm = 0.000382
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.001549
Total gradient norm: 0.002906
=== Actor Training Debug (Iteration 180) ===
Q mean: -28.144112
Q std: 20.125984
Actor loss: 28.148067
Action reg: 0.003955
  l1.weight: grad_norm = 0.000343
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.001516
Total gradient norm: 0.002910
=== Actor Training Debug (Iteration 181) ===
Q mean: -26.312353
Q std: 20.189470
Actor loss: 26.316320
Action reg: 0.003967
  l1.weight: grad_norm = 0.009248
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.033495
Total gradient norm: 0.049470
=== Actor Training Debug (Iteration 182) ===
Q mean: -22.159519
Q std: 22.225439
Actor loss: 22.163504
Action reg: 0.003985
  l1.weight: grad_norm = 0.001695
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.006507
Total gradient norm: 0.009627
=== Actor Training Debug (Iteration 183) ===
Q mean: -20.827248
Q std: 20.069944
Actor loss: 20.831245
Action reg: 0.003998
  l1.weight: grad_norm = 0.047218
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.157295
Total gradient norm: 0.239100
=== Actor Training Debug (Iteration 184) ===
Q mean: -24.664497
Q std: 22.286451
Actor loss: 24.668497
Action reg: 0.004000
  l1.weight: grad_norm = 0.001847
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.007039
Total gradient norm: 0.010358
=== Actor Training Debug (Iteration 185) ===
Q mean: -29.153996
Q std: 22.102327
Actor loss: 29.157980
Action reg: 0.003985
  l1.weight: grad_norm = 0.000294
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.000981
Total gradient norm: 0.001581
=== Actor Training Debug (Iteration 186) ===
Q mean: -31.260164
Q std: 21.863533
Actor loss: 31.264149
Action reg: 0.003985
  l1.weight: grad_norm = 0.000798
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.002927
Total gradient norm: 0.004339
=== Actor Training Debug (Iteration 187) ===
Q mean: -29.464081
Q std: 21.462307
Actor loss: 29.468035
Action reg: 0.003955
  l1.weight: grad_norm = 0.000398
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.001764
Total gradient norm: 0.003270
=== Actor Training Debug (Iteration 188) ===
Q mean: -23.197952
Q std: 23.193052
Actor loss: 23.201908
Action reg: 0.003955
  l1.weight: grad_norm = 0.003313
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.011977
Total gradient norm: 0.018068
=== Actor Training Debug (Iteration 189) ===
Q mean: -23.012554
Q std: 21.607914
Actor loss: 23.016552
Action reg: 0.003998
  l1.weight: grad_norm = 0.005814
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.021934
Total gradient norm: 0.031724
=== Actor Training Debug (Iteration 190) ===
Q mean: -23.217274
Q std: 19.532618
Actor loss: 23.221243
Action reg: 0.003969
  l1.weight: grad_norm = 0.009443
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.032309
Total gradient norm: 0.050182
=== Actor Training Debug (Iteration 191) ===
Q mean: -29.749353
Q std: 21.888218
Actor loss: 29.753292
Action reg: 0.003938
  l1.weight: grad_norm = 0.004440
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.016446
Total gradient norm: 0.023674
=== Actor Training Debug (Iteration 192) ===
Q mean: -30.850296
Q std: 22.479927
Actor loss: 30.854267
Action reg: 0.003971
  l1.weight: grad_norm = 0.000438
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.001417
Total gradient norm: 0.002707
=== Actor Training Debug (Iteration 193) ===
Q mean: -30.036396
Q std: 22.110352
Actor loss: 30.040337
Action reg: 0.003940
  l1.weight: grad_norm = 0.000551
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.002043
Total gradient norm: 0.004067
=== Actor Training Debug (Iteration 194) ===
Q mean: -21.529213
Q std: 20.146984
Actor loss: 21.533167
Action reg: 0.003954
  l1.weight: grad_norm = 0.009506
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.029851
Total gradient norm: 0.045116
=== Actor Training Debug (Iteration 195) ===
Q mean: -22.598372
Q std: 22.128181
Actor loss: 22.602297
Action reg: 0.003925
  l1.weight: grad_norm = 0.000687
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.003236
Total gradient norm: 0.006493
=== Actor Training Debug (Iteration 196) ===
Q mean: -23.522226
Q std: 21.960850
Actor loss: 23.526211
Action reg: 0.003985
  l1.weight: grad_norm = 0.000396
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.001348
Total gradient norm: 0.002236
=== Actor Training Debug (Iteration 197) ===
Q mean: -27.130316
Q std: 21.245708
Actor loss: 27.134256
Action reg: 0.003940
  l1.weight: grad_norm = 0.000667
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.002670
Total gradient norm: 0.005148
=== Actor Training Debug (Iteration 198) ===
Q mean: -31.654449
Q std: 22.972101
Actor loss: 31.658403
Action reg: 0.003955
  l1.weight: grad_norm = 0.000506
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.002012
Total gradient norm: 0.004032
=== Actor Training Debug (Iteration 199) ===
Q mean: -26.414967
Q std: 21.153124
Actor loss: 26.418938
Action reg: 0.003970
  l1.weight: grad_norm = 0.000386
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.001366
Total gradient norm: 0.002610
=== Actor Training Debug (Iteration 200) ===
Q mean: -21.751492
Q std: 20.787689
Actor loss: 21.755461
Action reg: 0.003970
  l1.weight: grad_norm = 0.000343
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.001339
Total gradient norm: 0.002625
=== Actor Training Debug (Iteration 201) ===
Q mean: -20.530109
Q std: 21.792974
Actor loss: 20.534050
Action reg: 0.003940
  l1.weight: grad_norm = 0.000983
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.003005
Total gradient norm: 0.005181
=== Actor Training Debug (Iteration 202) ===
Q mean: -25.135189
Q std: 21.752285
Actor loss: 25.139158
Action reg: 0.003970
  l1.weight: grad_norm = 0.000445
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.001492
Total gradient norm: 0.002768
=== Actor Training Debug (Iteration 203) ===
Q mean: -29.973949
Q std: 22.355841
Actor loss: 29.977905
Action reg: 0.003955
  l1.weight: grad_norm = 0.000605
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.002615
Total gradient norm: 0.004860
=== Actor Training Debug (Iteration 204) ===
Q mean: -31.744152
Q std: 23.422855
Actor loss: 31.748121
Action reg: 0.003970
  l1.weight: grad_norm = 0.000358
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.001396
Total gradient norm: 0.002476
=== Actor Training Debug (Iteration 205) ===
Q mean: -30.082987
Q std: 19.755367
Actor loss: 30.086971
Action reg: 0.003985
  l1.weight: grad_norm = 0.000291
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.000990
Total gradient norm: 0.001532
=== Actor Training Debug (Iteration 206) ===
Q mean: -22.187916
Q std: 20.376751
Actor loss: 22.191916
Action reg: 0.004000
  l1.weight: grad_norm = 0.001329
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.004207
Total gradient norm: 0.006546
=== Actor Training Debug (Iteration 207) ===
Q mean: -20.614231
Q std: 21.536671
Actor loss: 20.618183
Action reg: 0.003952
  l1.weight: grad_norm = 0.003471
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.012078
Total gradient norm: 0.018157
=== Actor Training Debug (Iteration 208) ===
Q mean: -23.927143
Q std: 21.582960
Actor loss: 23.931084
Action reg: 0.003940
  l1.weight: grad_norm = 0.000562
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.002440
Total gradient norm: 0.004713
=== Actor Training Debug (Iteration 209) ===
Q mean: -27.427982
Q std: 20.996546
Actor loss: 27.431967
Action reg: 0.003985
  l1.weight: grad_norm = 0.000303
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.000948
Total gradient norm: 0.001645
=== Actor Training Debug (Iteration 210) ===
Q mean: -29.727676
Q std: 19.909166
Actor loss: 29.731646
Action reg: 0.003970
  l1.weight: grad_norm = 0.000450
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.001834
Total gradient norm: 0.003391
=== Actor Training Debug (Iteration 211) ===
Q mean: -27.217001
Q std: 22.733181
Actor loss: 27.220940
Action reg: 0.003938
  l1.weight: grad_norm = 0.004313
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.015949
Total gradient norm: 0.023995
=== Actor Training Debug (Iteration 212) ===
Q mean: -25.011444
Q std: 20.933159
Actor loss: 25.015429
Action reg: 0.003985
  l1.weight: grad_norm = 0.000303
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.000811
Total gradient norm: 0.001538
=== Actor Training Debug (Iteration 213) ===
Q mean: -24.163273
Q std: 21.585468
Actor loss: 24.167229
Action reg: 0.003955
  l1.weight: grad_norm = 0.000616
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.002648
Total gradient norm: 0.005186
=== Actor Training Debug (Iteration 214) ===
Q mean: -24.064812
Q std: 19.958008
Actor loss: 24.068781
Action reg: 0.003970
  l1.weight: grad_norm = 0.000453
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.001723
Total gradient norm: 0.003003
=== Actor Training Debug (Iteration 215) ===
Q mean: -28.012552
Q std: 22.609081
Actor loss: 28.016535
Action reg: 0.003983
  l1.weight: grad_norm = 0.006899
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.021754
Total gradient norm: 0.033007
=== Actor Training Debug (Iteration 216) ===
Q mean: -30.637291
Q std: 19.759665
Actor loss: 30.641291
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000001
Total gradient norm: 0.000002
=== Actor Training Debug (Iteration 217) ===
Q mean: -26.890205
Q std: 19.655188
Actor loss: 26.894176
Action reg: 0.003970
  l1.weight: grad_norm = 0.000348
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.001148
Total gradient norm: 0.002305
=== Actor Training Debug (Iteration 218) ===
Q mean: -21.885111
Q std: 20.538113
Actor loss: 21.889067
Action reg: 0.003956
  l1.weight: grad_norm = 0.000540
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.001986
Total gradient norm: 0.003494
=== Actor Training Debug (Iteration 219) ===
Q mean: -22.757244
Q std: 23.919657
Actor loss: 22.761185
Action reg: 0.003940
  l1.weight: grad_norm = 0.000541
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.002487
Total gradient norm: 0.004894
=== Actor Training Debug (Iteration 220) ===
Q mean: -26.140007
Q std: 20.983210
Actor loss: 26.143976
Action reg: 0.003970
  l1.weight: grad_norm = 0.000396
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.001581
Total gradient norm: 0.002928
=== Actor Training Debug (Iteration 221) ===
Q mean: -28.204773
Q std: 22.483568
Actor loss: 28.208744
Action reg: 0.003970
  l1.weight: grad_norm = 0.000397
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.001212
Total gradient norm: 0.002017
=== Actor Training Debug (Iteration 222) ===
Q mean: -26.428562
Q std: 21.471716
Actor loss: 26.432518
Action reg: 0.003956
  l1.weight: grad_norm = 0.000566
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.001967
Total gradient norm: 0.003676
=== Actor Training Debug (Iteration 223) ===
Q mean: -26.988461
Q std: 20.713036
Actor loss: 26.992430
Action reg: 0.003970
  l1.weight: grad_norm = 0.000369
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.001238
Total gradient norm: 0.002323
=== Actor Training Debug (Iteration 224) ===
Q mean: -28.657242
Q std: 22.081469
Actor loss: 28.661226
Action reg: 0.003984
  l1.weight: grad_norm = 0.001693
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.005522
Total gradient norm: 0.008687
=== Actor Training Debug (Iteration 225) ===
Q mean: -24.068668
Q std: 20.980057
Actor loss: 24.072624
Action reg: 0.003955
  l1.weight: grad_norm = 0.000493
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.001842
Total gradient norm: 0.003425
=== Actor Training Debug (Iteration 226) ===
Q mean: -23.789951
Q std: 20.081596
Actor loss: 23.793905
Action reg: 0.003953
  l1.weight: grad_norm = 0.003709
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.011900
Total gradient norm: 0.017995
=== Actor Training Debug (Iteration 227) ===
Q mean: -25.771730
Q std: 21.365557
Actor loss: 25.775686
Action reg: 0.003956
  l1.weight: grad_norm = 0.000569
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.002238
Total gradient norm: 0.004342
=== Actor Training Debug (Iteration 228) ===
Q mean: -26.251987
Q std: 22.032543
Actor loss: 26.255987
Action reg: 0.003999
  l1.weight: grad_norm = 0.000783
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.002811
Total gradient norm: 0.004181
=== Actor Training Debug (Iteration 229) ===
Q mean: -25.703707
Q std: 21.343399
Actor loss: 25.707663
Action reg: 0.003956
  l1.weight: grad_norm = 0.000523
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.002121
Total gradient norm: 0.004031
=== Actor Training Debug (Iteration 230) ===
Q mean: -24.909006
Q std: 21.096249
Actor loss: 24.912991
Action reg: 0.003985
  l1.weight: grad_norm = 0.000442
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.001257
Total gradient norm: 0.002064
=== Actor Training Debug (Iteration 231) ===
Q mean: -24.586712
Q std: 18.606976
Actor loss: 24.590668
Action reg: 0.003955
  l1.weight: grad_norm = 0.000477
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.001768
Total gradient norm: 0.003494
=== Actor Training Debug (Iteration 232) ===
Q mean: -26.240580
Q std: 20.181902
Actor loss: 26.244564
Action reg: 0.003985
  l1.weight: grad_norm = 0.000518
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.001572
Total gradient norm: 0.002323
=== Actor Training Debug (Iteration 233) ===
Q mean: -26.663212
Q std: 21.581633
Actor loss: 26.667137
Action reg: 0.003926
  l1.weight: grad_norm = 0.000702
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.003083
Total gradient norm: 0.006317
=== Actor Training Debug (Iteration 234) ===
Q mean: -23.799084
Q std: 21.126856
Actor loss: 23.803068
Action reg: 0.003985
  l1.weight: grad_norm = 0.000195
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.000691
Total gradient norm: 0.001256
=== Actor Training Debug (Iteration 235) ===
Q mean: -21.997429
Q std: 19.528946
Actor loss: 22.001400
Action reg: 0.003970
  l1.weight: grad_norm = 0.000652
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.002003
Total gradient norm: 0.003137
=== Actor Training Debug (Iteration 236) ===
Q mean: -27.109917
Q std: 20.567320
Actor loss: 27.113871
Action reg: 0.003954
  l1.weight: grad_norm = 0.024933
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.071553
Total gradient norm: 0.104316
=== Actor Training Debug (Iteration 237) ===
Q mean: -26.326519
Q std: 20.282015
Actor loss: 26.330460
Action reg: 0.003941
  l1.weight: grad_norm = 0.001546
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.004879
Total gradient norm: 0.007582
=== Actor Training Debug (Iteration 238) ===
Q mean: -23.643454
Q std: 19.201221
Actor loss: 23.647394
Action reg: 0.003941
  l1.weight: grad_norm = 0.000546
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.002368
Total gradient norm: 0.004729
=== Actor Training Debug (Iteration 239) ===
Q mean: -21.252083
Q std: 20.570662
Actor loss: 21.256023
Action reg: 0.003941
  l1.weight: grad_norm = 0.000554
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.002345
Total gradient norm: 0.004402
=== Actor Training Debug (Iteration 240) ===
Q mean: -26.443422
Q std: 19.835787
Actor loss: 26.447363
Action reg: 0.003940
  l1.weight: grad_norm = 0.000510
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.002290
Total gradient norm: 0.004729
=== Actor Training Debug (Iteration 241) ===
Q mean: -31.531784
Q std: 19.930311
Actor loss: 31.535770
Action reg: 0.003985
  l1.weight: grad_norm = 0.000177
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.000640
Total gradient norm: 0.001103
=== Actor Training Debug (Iteration 242) ===
Q mean: -27.308689
Q std: 23.136053
Actor loss: 27.312614
Action reg: 0.003926
  l1.weight: grad_norm = 0.000605
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.002600
Total gradient norm: 0.005549
=== Actor Training Debug (Iteration 243) ===
Q mean: -28.325699
Q std: 20.365019
Actor loss: 28.329681
Action reg: 0.003983
  l1.weight: grad_norm = 0.024640
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.072881
Total gradient norm: 0.113677
=== Actor Training Debug (Iteration 244) ===
Q mean: -23.005375
Q std: 20.409456
Actor loss: 23.009344
Action reg: 0.003970
  l1.weight: grad_norm = 0.000562
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.002107
Total gradient norm: 0.004138
=== Actor Training Debug (Iteration 245) ===
Q mean: -20.347385
Q std: 20.617176
Actor loss: 20.351341
Action reg: 0.003956
  l1.weight: grad_norm = 0.000562
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.002000
Total gradient norm: 0.003884
=== Actor Training Debug (Iteration 246) ===
Q mean: -26.138460
Q std: 20.390421
Actor loss: 26.142429
Action reg: 0.003970
  l1.weight: grad_norm = 0.000363
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.001412
Total gradient norm: 0.002392
=== Actor Training Debug (Iteration 247) ===
Q mean: -27.938272
Q std: 20.855806
Actor loss: 27.942183
Action reg: 0.003911
  l1.weight: grad_norm = 0.000794
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.003416
Total gradient norm: 0.007085
=== Actor Training Debug (Iteration 248) ===
Q mean: -25.751722
Q std: 20.213009
Actor loss: 25.755678
Action reg: 0.003956
  l1.weight: grad_norm = 0.000608
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.002067
Total gradient norm: 0.003531
=== Actor Training Debug (Iteration 249) ===
Q mean: -22.617407
Q std: 19.212669
Actor loss: 22.621363
Action reg: 0.003955
  l1.weight: grad_norm = 0.000415
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.001848
Total gradient norm: 0.003884
=== Actor Training Debug (Iteration 250) ===
Q mean: -24.088890
Q std: 17.629559
Actor loss: 24.092875
Action reg: 0.003985
  l1.weight: grad_norm = 0.002786
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.008513
Total gradient norm: 0.012365
=== Actor Training Debug (Iteration 251) ===
Q mean: -27.042398
Q std: 20.187895
Actor loss: 27.046352
Action reg: 0.003953
  l1.weight: grad_norm = 0.012983
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.041694
Total gradient norm: 0.060842
=== Actor Training Debug (Iteration 252) ===
Q mean: -28.408825
Q std: 23.751022
Actor loss: 28.412766
Action reg: 0.003940
  l1.weight: grad_norm = 0.001531
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.005635
Total gradient norm: 0.009652
=== Actor Training Debug (Iteration 253) ===
Q mean: -24.001995
Q std: 21.005253
Actor loss: 24.005980
Action reg: 0.003985
  l1.weight: grad_norm = 0.002897
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.008701
Total gradient norm: 0.012526
=== Actor Training Debug (Iteration 254) ===
Q mean: -24.821583
Q std: 21.652899
Actor loss: 24.825554
Action reg: 0.003970
  l1.weight: grad_norm = 0.000413
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.001529
Total gradient norm: 0.002912
=== Actor Training Debug (Iteration 255) ===
Q mean: -23.127649
Q std: 21.036858
Actor loss: 23.131605
Action reg: 0.003955
  l1.weight: grad_norm = 0.000782
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.002832
Total gradient norm: 0.005695
=== Actor Training Debug (Iteration 256) ===
Q mean: -25.807264
Q std: 22.923641
Actor loss: 25.811205
Action reg: 0.003940
  l1.weight: grad_norm = 0.001557
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.005715
Total gradient norm: 0.008741
=== Actor Training Debug (Iteration 257) ===
Q mean: -26.056787
Q std: 20.747189
Actor loss: 26.060713
Action reg: 0.003925
  l1.weight: grad_norm = 0.000837
  l1.bias: grad_norm = 0.000563
  l2.weight: grad_norm = 0.003455
Total gradient norm: 0.007386
=== Actor Training Debug (Iteration 258) ===
Q mean: -24.110085
Q std: 19.353842
Actor loss: 24.114056
Action reg: 0.003970
  l1.weight: grad_norm = 0.000534
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.001947
Total gradient norm: 0.003809
=== Actor Training Debug (Iteration 259) ===
Q mean: -25.551750
Q std: 21.182409
Actor loss: 25.555721
Action reg: 0.003970
  l1.weight: grad_norm = 0.000348
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.001287
Total gradient norm: 0.002342
=== Actor Training Debug (Iteration 260) ===
Q mean: -27.081997
Q std: 20.805555
Actor loss: 27.085953
Action reg: 0.003956
  l1.weight: grad_norm = 0.000647
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.002494
Total gradient norm: 0.005035
=== Actor Training Debug (Iteration 261) ===
Q mean: -29.505777
Q std: 21.630894
Actor loss: 29.509762
Action reg: 0.003985
  l1.weight: grad_norm = 0.000405
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.001269
Total gradient norm: 0.002215
=== Actor Training Debug (Iteration 262) ===
Q mean: -28.152769
Q std: 19.858362
Actor loss: 28.156754
Action reg: 0.003985
  l1.weight: grad_norm = 0.000297
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.000928
Total gradient norm: 0.001588
=== Actor Training Debug (Iteration 263) ===
Q mean: -25.920914
Q std: 19.914112
Actor loss: 25.924900
Action reg: 0.003985
  l1.weight: grad_norm = 0.000328
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.000950
Total gradient norm: 0.001758
=== Actor Training Debug (Iteration 264) ===
Q mean: -23.456490
Q std: 20.159666
Actor loss: 23.460461
Action reg: 0.003971
  l1.weight: grad_norm = 0.000440
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.001623
Total gradient norm: 0.002948
=== Actor Training Debug (Iteration 265) ===
Q mean: -24.562958
Q std: 22.113974
Actor loss: 24.566898
Action reg: 0.003940
  l1.weight: grad_norm = 0.008988
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.032292
Total gradient norm: 0.046429
=== Actor Training Debug (Iteration 266) ===
Q mean: -23.091358
Q std: 22.646402
Actor loss: 23.095329
Action reg: 0.003971
  l1.weight: grad_norm = 0.000550
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.001892
Total gradient norm: 0.003704
=== Actor Training Debug (Iteration 267) ===
Q mean: -26.754683
Q std: 21.841204
Actor loss: 26.758652
Action reg: 0.003970
  l1.weight: grad_norm = 0.000309
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.001194
Total gradient norm: 0.002311
=== Actor Training Debug (Iteration 268) ===
Q mean: -25.825642
Q std: 19.760900
Actor loss: 25.829641
Action reg: 0.003999
  l1.weight: grad_norm = 0.004984
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.018040
Total gradient norm: 0.026235
=== Actor Training Debug (Iteration 269) ===
Q mean: -23.928356
Q std: 19.231474
Actor loss: 23.932312
Action reg: 0.003956
  l1.weight: grad_norm = 0.000567
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.001985
Total gradient norm: 0.003906
=== Actor Training Debug (Iteration 270) ===
Q mean: -22.845947
Q std: 19.622292
Actor loss: 22.849916
Action reg: 0.003970
  l1.weight: grad_norm = 0.000444
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.001767
Total gradient norm: 0.003203
=== Actor Training Debug (Iteration 271) ===
Q mean: -28.260166
Q std: 21.951357
Actor loss: 28.264166
Action reg: 0.004000
  l1.weight: grad_norm = 0.000616
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001997
Total gradient norm: 0.002856
=== Actor Training Debug (Iteration 272) ===
Q mean: -27.577503
Q std: 20.401003
Actor loss: 27.581444
Action reg: 0.003941
  l1.weight: grad_norm = 0.000674
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.003035
Total gradient norm: 0.006342
=== Actor Training Debug (Iteration 273) ===
Q mean: -24.965923
Q std: 20.963915
Actor loss: 24.969908
Action reg: 0.003985
  l1.weight: grad_norm = 0.000680
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.002030
Total gradient norm: 0.002954
=== Actor Training Debug (Iteration 274) ===
Q mean: -23.873623
Q std: 19.532736
Actor loss: 23.877592
Action reg: 0.003970
  l1.weight: grad_norm = 0.002877
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.008985
Total gradient norm: 0.013223
=== Actor Training Debug (Iteration 275) ===
Q mean: -22.684864
Q std: 20.788651
Actor loss: 22.688848
Action reg: 0.003985
  l1.weight: grad_norm = 0.000987
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.003535
Total gradient norm: 0.005903
=== Actor Training Debug (Iteration 276) ===
Q mean: -26.893917
Q std: 21.876677
Actor loss: 26.897842
Action reg: 0.003926
  l1.weight: grad_norm = 0.000867
  l1.bias: grad_norm = 0.000528
  l2.weight: grad_norm = 0.003703
Total gradient norm: 0.007713
=== Actor Training Debug (Iteration 277) ===
Q mean: -28.086422
Q std: 22.094118
Actor loss: 28.090422
Action reg: 0.004000
  l1.weight: grad_norm = 0.004835
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.015058
Total gradient norm: 0.022235
=== Actor Training Debug (Iteration 278) ===
Q mean: -23.629528
Q std: 23.191238
Actor loss: 23.633512
Action reg: 0.003985
  l1.weight: grad_norm = 0.000445
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.001553
Total gradient norm: 0.002595
=== Actor Training Debug (Iteration 279) ===
Q mean: -25.374744
Q std: 21.720392
Actor loss: 25.378729
Action reg: 0.003985
  l1.weight: grad_norm = 0.000365
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.001319
Total gradient norm: 0.002098
=== Actor Training Debug (Iteration 280) ===
Q mean: -25.889919
Q std: 19.833395
Actor loss: 25.893843
Action reg: 0.003924
  l1.weight: grad_norm = 0.015507
  l1.bias: grad_norm = 0.000518
  l2.weight: grad_norm = 0.054014
Total gradient norm: 0.080379
=== Actor Training Debug (Iteration 281) ===
Q mean: -27.546024
Q std: 19.942251
Actor loss: 27.549965
Action reg: 0.003941
  l1.weight: grad_norm = 0.000882
  l1.bias: grad_norm = 0.000535
  l2.weight: grad_norm = 0.003570
Total gradient norm: 0.007072
=== Actor Training Debug (Iteration 282) ===
Q mean: -27.690006
Q std: 21.542730
Actor loss: 27.693989
Action reg: 0.003983
  l1.weight: grad_norm = 0.041982
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.131754
Total gradient norm: 0.207570
=== Actor Training Debug (Iteration 283) ===
Q mean: -25.474468
Q std: 22.775909
Actor loss: 25.478409
Action reg: 0.003941
  l1.weight: grad_norm = 0.000786
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.002968
Total gradient norm: 0.005787
=== Actor Training Debug (Iteration 284) ===
Q mean: -27.176832
Q std: 21.844456
Actor loss: 27.180773
Action reg: 0.003940
  l1.weight: grad_norm = 0.000666
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.002781
Total gradient norm: 0.005585
=== Actor Training Debug (Iteration 285) ===
Q mean: -26.809443
Q std: 18.400461
Actor loss: 26.813368
Action reg: 0.003926
  l1.weight: grad_norm = 0.000661
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.003319
Total gradient norm: 0.006791
=== Actor Training Debug (Iteration 286) ===
Q mean: -26.455235
Q std: 20.640686
Actor loss: 26.459221
Action reg: 0.003986
  l1.weight: grad_norm = 0.000511
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.001498
Total gradient norm: 0.002355
=== Actor Training Debug (Iteration 287) ===
Q mean: -27.946562
Q std: 20.303528
Actor loss: 27.950516
Action reg: 0.003953
  l1.weight: grad_norm = 0.011804
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.041586
Total gradient norm: 0.062007
=== Actor Training Debug (Iteration 288) ===
Q mean: -27.839184
Q std: 20.222006
Actor loss: 27.843124
Action reg: 0.003940
  l1.weight: grad_norm = 0.000789
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.003206
Total gradient norm: 0.006257
=== Actor Training Debug (Iteration 289) ===
Q mean: -26.498034
Q std: 21.090477
Actor loss: 26.501989
Action reg: 0.003956
  l1.weight: grad_norm = 0.000769
  l1.bias: grad_norm = 0.000414
  l2.weight: grad_norm = 0.002963
Total gradient norm: 0.005700
=== Actor Training Debug (Iteration 290) ===
Q mean: -25.245731
Q std: 20.319088
Actor loss: 25.249641
Action reg: 0.003910
  l1.weight: grad_norm = 0.003842
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.014288
Total gradient norm: 0.021054
=== Actor Training Debug (Iteration 291) ===
Q mean: -28.256817
Q std: 20.178452
Actor loss: 28.260759
Action reg: 0.003942
  l1.weight: grad_norm = 0.000770
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.003308
Total gradient norm: 0.006950
=== Actor Training Debug (Iteration 292) ===
Q mean: -26.376612
Q std: 18.399418
Actor loss: 26.380568
Action reg: 0.003956
  l1.weight: grad_norm = 0.000718
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.002824
Total gradient norm: 0.005347
=== Actor Training Debug (Iteration 293) ===
Q mean: -27.096384
Q std: 21.282492
Actor loss: 27.100353
Action reg: 0.003970
  l1.weight: grad_norm = 0.000471
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.001793
Total gradient norm: 0.003231
=== Actor Training Debug (Iteration 294) ===
Q mean: -22.828974
Q std: 20.234583
Actor loss: 22.832901
Action reg: 0.003926
  l1.weight: grad_norm = 0.000694
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.003306
Total gradient norm: 0.007081
=== Actor Training Debug (Iteration 295) ===
Q mean: -24.650322
Q std: 20.119669
Actor loss: 24.654263
Action reg: 0.003940
  l1.weight: grad_norm = 0.001767
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.006451
Total gradient norm: 0.011128
=== Actor Training Debug (Iteration 296) ===
Q mean: -26.811260
Q std: 23.497377
Actor loss: 26.815245
Action reg: 0.003985
  l1.weight: grad_norm = 0.000325
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.001045
Total gradient norm: 0.001808
=== Actor Training Debug (Iteration 297) ===
Q mean: -26.350410
Q std: 19.578806
Actor loss: 26.354410
Action reg: 0.004000
  l1.weight: grad_norm = 0.001159
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.004068
Total gradient norm: 0.006088
=== Actor Training Debug (Iteration 298) ===
Q mean: -25.208996
Q std: 22.051510
Actor loss: 25.212952
Action reg: 0.003955
  l1.weight: grad_norm = 0.000632
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.002570
Total gradient norm: 0.005066
=== Actor Training Debug (Iteration 299) ===
Q mean: -26.490131
Q std: 18.794464
Actor loss: 26.494102
Action reg: 0.003970
  l1.weight: grad_norm = 0.000460
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.001598
Total gradient norm: 0.003037
=== Actor Training Debug (Iteration 300) ===
Q mean: -23.341251
Q std: 20.829712
Actor loss: 23.345221
Action reg: 0.003969
  l1.weight: grad_norm = 0.022542
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.076257
Total gradient norm: 0.111225
=== Actor Training Debug (Iteration 301) ===
Q mean: -25.989887
Q std: 20.829054
Actor loss: 25.993828
Action reg: 0.003941
  l1.weight: grad_norm = 0.000658
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.002765
Total gradient norm: 0.005949
=== Actor Training Debug (Iteration 302) ===
Q mean: -33.625038
Q std: 22.957655
Actor loss: 33.628994
Action reg: 0.003956
  l1.weight: grad_norm = 0.007434
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.026663
Total gradient norm: 0.041394
=== Actor Training Debug (Iteration 303) ===
Q mean: -28.724812
Q std: 19.760063
Actor loss: 28.728796
Action reg: 0.003985
  l1.weight: grad_norm = 0.000309
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.000970
Total gradient norm: 0.001670
=== Actor Training Debug (Iteration 304) ===
Q mean: -27.223322
Q std: 21.794230
Actor loss: 27.227322
Action reg: 0.004000
  l1.weight: grad_norm = 0.000022
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000079
Total gradient norm: 0.000114
=== Actor Training Debug (Iteration 305) ===
Q mean: -24.323853
Q std: 19.475876
Actor loss: 24.327852
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000001
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 306) ===
Q mean: -26.454878
Q std: 19.338396
Actor loss: 26.458862
Action reg: 0.003985
  l1.weight: grad_norm = 0.000358
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.001073
Total gradient norm: 0.001955
=== Actor Training Debug (Iteration 307) ===
Q mean: -23.982315
Q std: 20.223642
Actor loss: 23.986271
Action reg: 0.003956
  l1.weight: grad_norm = 0.000505
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.002218
Total gradient norm: 0.004294
=== Actor Training Debug (Iteration 308) ===
Q mean: -26.254246
Q std: 19.755602
Actor loss: 26.258202
Action reg: 0.003956
  l1.weight: grad_norm = 0.000494
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.002245
Total gradient norm: 0.004545
=== Actor Training Debug (Iteration 309) ===
Q mean: -26.563618
Q std: 21.030130
Actor loss: 26.567602
Action reg: 0.003985
  l1.weight: grad_norm = 0.000293
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.000964
Total gradient norm: 0.001576
=== Actor Training Debug (Iteration 310) ===
Q mean: -27.007299
Q std: 19.261410
Actor loss: 27.011253
Action reg: 0.003954
  l1.weight: grad_norm = 0.012728
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.048128
Total gradient norm: 0.069363
=== Actor Training Debug (Iteration 311) ===
Q mean: -24.034218
Q std: 20.449512
Actor loss: 24.038189
Action reg: 0.003971
  l1.weight: grad_norm = 0.000638
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.002262
Total gradient norm: 0.003740
=== Actor Training Debug (Iteration 312) ===
Q mean: -25.150108
Q std: 19.962616
Actor loss: 25.154095
Action reg: 0.003986
  l1.weight: grad_norm = 0.000451
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.001439
Total gradient norm: 0.002456
=== Actor Training Debug (Iteration 313) ===
Q mean: -27.726711
Q std: 22.205776
Actor loss: 27.730698
Action reg: 0.003986
  l1.weight: grad_norm = 0.000419
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.001226
Total gradient norm: 0.002063
=== Actor Training Debug (Iteration 314) ===
Q mean: -27.550049
Q std: 21.625681
Actor loss: 27.554018
Action reg: 0.003970
  l1.weight: grad_norm = 0.003307
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.012842
Total gradient norm: 0.019147
=== Actor Training Debug (Iteration 315) ===
Q mean: -24.795805
Q std: 18.085590
Actor loss: 24.799788
Action reg: 0.003983
  l1.weight: grad_norm = 0.016394
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.058957
Total gradient norm: 0.085234
=== Actor Training Debug (Iteration 316) ===
Q mean: -25.141199
Q std: 21.483072
Actor loss: 25.145096
Action reg: 0.003898
  l1.weight: grad_norm = 0.021800
  l1.bias: grad_norm = 0.000814
  l2.weight: grad_norm = 0.079503
Total gradient norm: 0.120587
=== Actor Training Debug (Iteration 317) ===
Q mean: -25.757782
Q std: 20.296879
Actor loss: 25.761738
Action reg: 0.003955
  l1.weight: grad_norm = 0.000556
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.002439
Total gradient norm: 0.004708
=== Actor Training Debug (Iteration 318) ===
Q mean: -23.008842
Q std: 19.967154
Actor loss: 23.012842
Action reg: 0.004000
  l1.weight: grad_norm = 0.001294
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003996
Total gradient norm: 0.005913
=== Actor Training Debug (Iteration 319) ===
Q mean: -25.390125
Q std: 21.407593
Actor loss: 25.394081
Action reg: 0.003956
  l1.weight: grad_norm = 0.000682
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.002723
Total gradient norm: 0.004983
=== Actor Training Debug (Iteration 320) ===
Q mean: -24.677139
Q std: 19.916847
Actor loss: 24.681095
Action reg: 0.003956
  l1.weight: grad_norm = 0.002272
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.006702
Total gradient norm: 0.009884
=== Actor Training Debug (Iteration 321) ===
Q mean: -24.677439
Q std: 19.319206
Actor loss: 24.681379
Action reg: 0.003941
  l1.weight: grad_norm = 0.000627
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.003031
Total gradient norm: 0.006275
=== Actor Training Debug (Iteration 322) ===
Q mean: -23.360474
Q std: 18.245419
Actor loss: 23.364414
Action reg: 0.003941
  l1.weight: grad_norm = 0.000859
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.003420
Total gradient norm: 0.007231
=== Actor Training Debug (Iteration 323) ===
Q mean: -26.188919
Q std: 19.671162
Actor loss: 26.192904
Action reg: 0.003985
  l1.weight: grad_norm = 0.000333
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.001212
Total gradient norm: 0.002105
=== Actor Training Debug (Iteration 324) ===
Q mean: -27.601742
Q std: 21.314634
Actor loss: 27.605726
Action reg: 0.003985
  l1.weight: grad_norm = 0.000345
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.001127
Total gradient norm: 0.002062
=== Actor Training Debug (Iteration 325) ===
Q mean: -26.001619
Q std: 20.299345
Actor loss: 26.005619
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 326) ===
Q mean: -26.057358
Q std: 20.744999
Actor loss: 26.061298
Action reg: 0.003940
  l1.weight: grad_norm = 0.013536
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.049104
Total gradient norm: 0.079529
=== Actor Training Debug (Iteration 327) ===
Q mean: -25.471394
Q std: 19.719351
Actor loss: 25.475321
Action reg: 0.003927
  l1.weight: grad_norm = 0.000802
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.003480
Total gradient norm: 0.006928
=== Actor Training Debug (Iteration 328) ===
Q mean: -26.107235
Q std: 20.007973
Actor loss: 26.111162
Action reg: 0.003927
  l1.weight: grad_norm = 0.000885
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.003695
Total gradient norm: 0.007250
=== Actor Training Debug (Iteration 329) ===
Q mean: -27.135960
Q std: 22.195406
Actor loss: 27.139931
Action reg: 0.003971
  l1.weight: grad_norm = 0.000585
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.002160
Total gradient norm: 0.003830
=== Actor Training Debug (Iteration 330) ===
Q mean: -27.261757
Q std: 19.745195
Actor loss: 27.265697
Action reg: 0.003941
  l1.weight: grad_norm = 0.005864
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.018620
Total gradient norm: 0.028421
=== Actor Training Debug (Iteration 331) ===
Q mean: -28.050419
Q std: 21.644054
Actor loss: 28.054388
Action reg: 0.003970
  l1.weight: grad_norm = 0.000516
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.002059
Total gradient norm: 0.004058
=== Actor Training Debug (Iteration 332) ===
Q mean: -26.893133
Q std: 20.387548
Actor loss: 26.897102
Action reg: 0.003970
  l1.weight: grad_norm = 0.015119
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.045309
Total gradient norm: 0.065586
=== Actor Training Debug (Iteration 333) ===
Q mean: -24.650574
Q std: 19.042936
Actor loss: 24.654530
Action reg: 0.003957
  l1.weight: grad_norm = 0.000706
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.002640
Total gradient norm: 0.004835
=== Actor Training Debug (Iteration 334) ===
Q mean: -26.549858
Q std: 19.829235
Actor loss: 26.553858
Action reg: 0.004000
  l1.weight: grad_norm = 0.004183
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.014419
Total gradient norm: 0.021481
=== Actor Training Debug (Iteration 335) ===
Q mean: -27.446041
Q std: 21.247427
Actor loss: 27.449982
Action reg: 0.003941
  l1.weight: grad_norm = 0.000808
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.003602
Total gradient norm: 0.007339
=== Actor Training Debug (Iteration 336) ===
Q mean: -27.265120
Q std: 18.684952
Actor loss: 27.269075
Action reg: 0.003956
  l1.weight: grad_norm = 0.002286
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.008043
Total gradient norm: 0.013264
=== Actor Training Debug (Iteration 337) ===
Q mean: -25.819798
Q std: 20.419241
Actor loss: 25.823780
Action reg: 0.003982
  l1.weight: grad_norm = 0.012890
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.045935
Total gradient norm: 0.072789
=== Actor Training Debug (Iteration 338) ===
Q mean: -27.412289
Q std: 22.132740
Actor loss: 27.416260
Action reg: 0.003971
  l1.weight: grad_norm = 0.000546
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.002039
Total gradient norm: 0.003901
=== Actor Training Debug (Iteration 339) ===
Q mean: -25.570301
Q std: 21.964869
Actor loss: 25.574272
Action reg: 0.003971
  l1.weight: grad_norm = 0.000600
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.002149
Total gradient norm: 0.004073
=== Actor Training Debug (Iteration 340) ===
Q mean: -28.200367
Q std: 20.722092
Actor loss: 28.204323
Action reg: 0.003956
  l1.weight: grad_norm = 0.000629
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.002614
Total gradient norm: 0.005268
=== Actor Training Debug (Iteration 341) ===
Q mean: -27.450207
Q std: 21.267626
Actor loss: 27.454191
Action reg: 0.003985
  l1.weight: grad_norm = 0.010526
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.031365
Total gradient norm: 0.049756
=== Actor Training Debug (Iteration 342) ===
Q mean: -27.495087
Q std: 21.375246
Actor loss: 27.499014
Action reg: 0.003927
  l1.weight: grad_norm = 0.000817
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.003749
Total gradient norm: 0.007750
=== Actor Training Debug (Iteration 343) ===
Q mean: -28.299812
Q std: 21.134848
Actor loss: 28.303783
Action reg: 0.003971
  l1.weight: grad_norm = 0.003127
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.009032
Total gradient norm: 0.014179
=== Actor Training Debug (Iteration 344) ===
Q mean: -25.376278
Q std: 20.970232
Actor loss: 25.380234
Action reg: 0.003957
  l1.weight: grad_norm = 0.000794
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.002903
Total gradient norm: 0.005378
=== Actor Training Debug (Iteration 345) ===
Q mean: -27.560747
Q std: 20.197405
Actor loss: 27.564676
Action reg: 0.003928
  l1.weight: grad_norm = 0.000874
  l1.bias: grad_norm = 0.000538
  l2.weight: grad_norm = 0.003715
Total gradient norm: 0.007884
=== Actor Training Debug (Iteration 346) ===
Q mean: -26.886395
Q std: 20.871939
Actor loss: 26.890366
Action reg: 0.003971
  l1.weight: grad_norm = 0.000476
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.001829
Total gradient norm: 0.003412
=== Actor Training Debug (Iteration 347) ===
Q mean: -27.490421
Q std: 20.385843
Actor loss: 27.494406
Action reg: 0.003985
  l1.weight: grad_norm = 0.000330
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.001164
Total gradient norm: 0.002011
=== Actor Training Debug (Iteration 348) ===
Q mean: -27.871357
Q std: 22.898003
Actor loss: 27.875328
Action reg: 0.003971
  l1.weight: grad_norm = 0.002873
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.009931
Total gradient norm: 0.015831
=== Actor Training Debug (Iteration 349) ===
Q mean: -24.590183
Q std: 21.216990
Actor loss: 24.594154
Action reg: 0.003971
  l1.weight: grad_norm = 0.001806
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.006382
Total gradient norm: 0.010801
=== Actor Training Debug (Iteration 350) ===
Q mean: -28.365730
Q std: 21.731836
Actor loss: 28.369715
Action reg: 0.003985
  l1.weight: grad_norm = 0.006143
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.018989
Total gradient norm: 0.027709
=== Actor Training Debug (Iteration 351) ===
Q mean: -29.596893
Q std: 21.250719
Actor loss: 29.600880
Action reg: 0.003986
  l1.weight: grad_norm = 0.000292
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.000932
Total gradient norm: 0.001588
=== Actor Training Debug (Iteration 352) ===
Q mean: -27.093311
Q std: 22.931530
Actor loss: 27.097296
Action reg: 0.003985
  l1.weight: grad_norm = 0.003251
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.009640
Total gradient norm: 0.013424
=== Actor Training Debug (Iteration 353) ===
Q mean: -25.344147
Q std: 20.412231
Actor loss: 25.348131
Action reg: 0.003985
  l1.weight: grad_norm = 0.001517
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.005178
Total gradient norm: 0.007880
=== Actor Training Debug (Iteration 354) ===
Q mean: -24.875059
Q std: 19.024956
Actor loss: 24.879030
Action reg: 0.003972
  l1.weight: grad_norm = 0.000649
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.002271
Total gradient norm: 0.004281
=== Actor Training Debug (Iteration 355) ===
Q mean: -27.565983
Q std: 20.123362
Actor loss: 27.569880
Action reg: 0.003897
  l1.weight: grad_norm = 0.009700
  l1.bias: grad_norm = 0.000913
  l2.weight: grad_norm = 0.031729
Total gradient norm: 0.048080
=== Actor Training Debug (Iteration 356) ===
Q mean: -31.848560
Q std: 21.677460
Actor loss: 31.852518
Action reg: 0.003957
  l1.weight: grad_norm = 0.001405
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.005206
Total gradient norm: 0.008468
=== Actor Training Debug (Iteration 357) ===
Q mean: -28.955133
Q std: 20.555750
Actor loss: 28.959061
Action reg: 0.003928
  l1.weight: grad_norm = 0.000975
  l1.bias: grad_norm = 0.000660
  l2.weight: grad_norm = 0.004426
Total gradient norm: 0.009211
=== Actor Training Debug (Iteration 358) ===
Q mean: -23.494492
Q std: 22.556265
Actor loss: 23.498404
Action reg: 0.003912
  l1.weight: grad_norm = 0.012038
  l1.bias: grad_norm = 0.000685
  l2.weight: grad_norm = 0.041771
Total gradient norm: 0.062246
=== Actor Training Debug (Iteration 359) ===
Q mean: -22.729073
Q std: 21.372505
Actor loss: 22.733059
Action reg: 0.003985
  l1.weight: grad_norm = 0.000361
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.001144
Total gradient norm: 0.002040
=== Actor Training Debug (Iteration 360) ===
Q mean: -26.115978
Q std: 21.045330
Actor loss: 26.119905
Action reg: 0.003927
  l1.weight: grad_norm = 0.000838
  l1.bias: grad_norm = 0.000655
  l2.weight: grad_norm = 0.004160
Total gradient norm: 0.008478
=== Actor Training Debug (Iteration 361) ===
Q mean: -29.095852
Q std: 20.828196
Actor loss: 29.099792
Action reg: 0.003941
  l1.weight: grad_norm = 0.000725
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.002911
Total gradient norm: 0.006227
=== Actor Training Debug (Iteration 362) ===
Q mean: -31.013052
Q std: 20.669849
Actor loss: 31.016993
Action reg: 0.003941
  l1.weight: grad_norm = 0.000599
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.002848
Total gradient norm: 0.006051
=== Actor Training Debug (Iteration 363) ===
Q mean: -25.504204
Q std: 20.972580
Actor loss: 25.508175
Action reg: 0.003971
  l1.weight: grad_norm = 0.000559
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.001898
Total gradient norm: 0.003709
=== Actor Training Debug (Iteration 364) ===
Q mean: -25.759554
Q std: 20.408972
Actor loss: 25.763538
Action reg: 0.003985
  l1.weight: grad_norm = 0.000312
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.000882
Total gradient norm: 0.001608
=== Actor Training Debug (Iteration 365) ===
Q mean: -27.322777
Q std: 21.267099
Actor loss: 27.326719
Action reg: 0.003942
  l1.weight: grad_norm = 0.000812
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.003423
Total gradient norm: 0.006772
=== Actor Training Debug (Iteration 366) ===
Q mean: -25.917429
Q std: 20.537664
Actor loss: 25.921385
Action reg: 0.003956
  l1.weight: grad_norm = 0.000741
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.003151
Total gradient norm: 0.006066
=== Actor Training Debug (Iteration 367) ===
Q mean: -26.830257
Q std: 19.933067
Actor loss: 26.834200
Action reg: 0.003942
  l1.weight: grad_norm = 0.000843
  l1.bias: grad_norm = 0.000553
  l2.weight: grad_norm = 0.003716
Total gradient norm: 0.007381
=== Actor Training Debug (Iteration 368) ===
Q mean: -28.362282
Q std: 19.779144
Actor loss: 28.366268
Action reg: 0.003986
  l1.weight: grad_norm = 0.000309
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.000993
Total gradient norm: 0.001702
=== Actor Training Debug (Iteration 369) ===
Q mean: -27.313892
Q std: 18.371361
Actor loss: 27.317863
Action reg: 0.003972
  l1.weight: grad_norm = 0.000702
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.002393
Total gradient norm: 0.004445
=== Actor Training Debug (Iteration 370) ===
Q mean: -27.442085
Q std: 19.746475
Actor loss: 27.446085
Action reg: 0.004000
  l1.weight: grad_norm = 0.000117
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000384
Total gradient norm: 0.000533
=== Actor Training Debug (Iteration 371) ===
Q mean: -28.545494
Q std: 19.757532
Actor loss: 28.549450
Action reg: 0.003956
  l1.weight: grad_norm = 0.000661
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.002557
Total gradient norm: 0.004973
=== Actor Training Debug (Iteration 372) ===
Q mean: -23.541607
Q std: 20.818792
Actor loss: 23.545521
Action reg: 0.003915
  l1.weight: grad_norm = 0.001210
  l1.bias: grad_norm = 0.000783
  l2.weight: grad_norm = 0.005501
Total gradient norm: 0.011291
=== Actor Training Debug (Iteration 373) ===
Q mean: -24.731083
Q std: 18.638470
Actor loss: 24.735054
Action reg: 0.003971
  l1.weight: grad_norm = 0.000617
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.002284
Total gradient norm: 0.004316
=== Actor Training Debug (Iteration 374) ===
Q mean: -28.213850
Q std: 20.647860
Actor loss: 28.217850
Action reg: 0.004000
  l1.weight: grad_norm = 0.000024
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000086
Total gradient norm: 0.000126
=== Actor Training Debug (Iteration 375) ===
Q mean: -24.020088
Q std: 19.207691
Actor loss: 24.024044
Action reg: 0.003956
  l1.weight: grad_norm = 0.007866
  l1.bias: grad_norm = 0.000428
  l2.weight: grad_norm = 0.024121
Total gradient norm: 0.034560
=== Actor Training Debug (Iteration 376) ===
Q mean: -25.066460
Q std: 19.954674
Actor loss: 25.070385
Action reg: 0.003925
  l1.weight: grad_norm = 0.014786
  l1.bias: grad_norm = 0.000651
  l2.weight: grad_norm = 0.052614
Total gradient norm: 0.082466
=== Actor Training Debug (Iteration 377) ===
Q mean: -26.848671
Q std: 21.574146
Actor loss: 26.852640
Action reg: 0.003970
  l1.weight: grad_norm = 0.005843
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.020731
Total gradient norm: 0.032474
=== Actor Training Debug (Iteration 378) ===
Q mean: -27.414711
Q std: 21.127701
Actor loss: 27.418682
Action reg: 0.003971
  l1.weight: grad_norm = 0.000565
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.001956
Total gradient norm: 0.003867
=== Actor Training Debug (Iteration 379) ===
Q mean: -27.673702
Q std: 22.094027
Actor loss: 27.677702
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 380) ===
Q mean: -26.096195
Q std: 20.320461
Actor loss: 26.100138
Action reg: 0.003942
  l1.weight: grad_norm = 0.000778
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.003533
Total gradient norm: 0.007334
=== Actor Training Debug (Iteration 381) ===
Q mean: -25.999512
Q std: 18.617088
Actor loss: 26.003510
Action reg: 0.003998
  l1.weight: grad_norm = 0.007446
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.025081
Total gradient norm: 0.039399
=== Actor Training Debug (Iteration 382) ===
Q mean: -28.857807
Q std: 20.527893
Actor loss: 28.861794
Action reg: 0.003986
  l1.weight: grad_norm = 0.000382
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.001129
Total gradient norm: 0.002030
=== Actor Training Debug (Iteration 383) ===
Q mean: -28.697321
Q std: 20.335001
Actor loss: 28.701307
Action reg: 0.003985
  l1.weight: grad_norm = 0.003175
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.009715
Total gradient norm: 0.014117
=== Actor Training Debug (Iteration 384) ===
Q mean: -25.169546
Q std: 18.032913
Actor loss: 25.173544
Action reg: 0.003998
  l1.weight: grad_norm = 0.006166
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.019928
Total gradient norm: 0.029820
=== Actor Training Debug (Iteration 385) ===
Q mean: -26.835226
Q std: 19.488846
Actor loss: 26.839195
Action reg: 0.003969
  l1.weight: grad_norm = 0.000794
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.002720
Total gradient norm: 0.004335
=== Actor Training Debug (Iteration 386) ===
Q mean: -24.740866
Q std: 20.238344
Actor loss: 24.744865
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 387) ===
Q mean: -24.783043
Q std: 21.180902
Actor loss: 24.786999
Action reg: 0.003956
  l1.weight: grad_norm = 0.000632
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.002485
Total gradient norm: 0.004608
=== Actor Training Debug (Iteration 388) ===
Q mean: -25.957436
Q std: 21.050180
Actor loss: 25.961435
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 389) ===
Q mean: -26.998367
Q std: 19.860636
Actor loss: 27.002325
Action reg: 0.003957
  l1.weight: grad_norm = 0.000739
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.002897
Total gradient norm: 0.005747
=== Actor Training Debug (Iteration 390) ===
Q mean: -27.975792
Q std: 19.334682
Actor loss: 27.979763
Action reg: 0.003971
  l1.weight: grad_norm = 0.000757
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.002662
Total gradient norm: 0.004752
=== Actor Training Debug (Iteration 391) ===
Q mean: -27.200119
Q std: 19.126928
Actor loss: 27.204090
Action reg: 0.003971
  l1.weight: grad_norm = 0.001198
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.004349
Total gradient norm: 0.007042
=== Actor Training Debug (Iteration 392) ===
Q mean: -27.325249
Q std: 21.947834
Actor loss: 27.329178
Action reg: 0.003929
  l1.weight: grad_norm = 0.001240
  l1.bias: grad_norm = 0.000631
  l2.weight: grad_norm = 0.004891
Total gradient norm: 0.009702
=== Actor Training Debug (Iteration 393) ===
Q mean: -25.913681
Q std: 23.089359
Actor loss: 25.917610
Action reg: 0.003929
  l1.weight: grad_norm = 0.000978
  l1.bias: grad_norm = 0.000600
  l2.weight: grad_norm = 0.004312
Total gradient norm: 0.008993
=== Actor Training Debug (Iteration 394) ===
Q mean: -26.362476
Q std: 21.393362
Actor loss: 26.366447
Action reg: 0.003971
  l1.weight: grad_norm = 0.000659
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.002258
Total gradient norm: 0.003950
=== Actor Training Debug (Iteration 395) ===
Q mean: -26.562593
Q std: 22.979761
Actor loss: 26.566507
Action reg: 0.003913
  l1.weight: grad_norm = 0.001173
  l1.bias: grad_norm = 0.000808
  l2.weight: grad_norm = 0.005865
Total gradient norm: 0.012132
=== Actor Training Debug (Iteration 396) ===
Q mean: -29.218609
Q std: 20.829430
Actor loss: 29.222595
Action reg: 0.003986
  l1.weight: grad_norm = 0.000366
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.001165
Total gradient norm: 0.002098
=== Actor Training Debug (Iteration 397) ===
Q mean: -28.885990
Q std: 18.926540
Actor loss: 28.889977
Action reg: 0.003986
  l1.weight: grad_norm = 0.000564
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.001775
Total gradient norm: 0.002881
=== Actor Training Debug (Iteration 398) ===
Q mean: -26.821997
Q std: 20.363535
Actor loss: 26.825970
Action reg: 0.003973
  l1.weight: grad_norm = 0.000891
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.002614
Total gradient norm: 0.004575
=== Actor Training Debug (Iteration 399) ===
Q mean: -25.279442
Q std: 19.829466
Actor loss: 25.283426
Action reg: 0.003985
  l1.weight: grad_norm = 0.000330
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.001031
Total gradient norm: 0.001822
=== Actor Training Debug (Iteration 400) ===
Q mean: -27.167828
Q std: 20.068056
Actor loss: 27.171814
Action reg: 0.003986
  l1.weight: grad_norm = 0.000365
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.001283
Total gradient norm: 0.002168
=== Actor Training Debug (Iteration 401) ===
Q mean: -30.576767
Q std: 19.595013
Actor loss: 30.580738
Action reg: 0.003972
  l1.weight: grad_norm = 0.000678
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.002430
Total gradient norm: 0.004325
=== Actor Training Debug (Iteration 402) ===
Q mean: -26.376968
Q std: 20.396574
Actor loss: 26.380896
Action reg: 0.003928
  l1.weight: grad_norm = 0.000911
  l1.bias: grad_norm = 0.000626
  l2.weight: grad_norm = 0.004159
Total gradient norm: 0.008561
=== Actor Training Debug (Iteration 403) ===
Q mean: -24.166826
Q std: 18.642647
Actor loss: 24.170811
Action reg: 0.003985
  l1.weight: grad_norm = 0.000380
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.001281
Total gradient norm: 0.002196
=== Actor Training Debug (Iteration 404) ===
Q mean: -27.650816
Q std: 20.203127
Actor loss: 27.654774
Action reg: 0.003957
  l1.weight: grad_norm = 0.000937
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.003212
Total gradient norm: 0.005764
=== Actor Training Debug (Iteration 405) ===
Q mean: -29.315540
Q std: 20.205904
Actor loss: 29.319538
Action reg: 0.003999
  l1.weight: grad_norm = 0.004007
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.015148
Total gradient norm: 0.022177
=== Actor Training Debug (Iteration 406) ===
Q mean: -28.099735
Q std: 19.625996
Actor loss: 28.103706
Action reg: 0.003971
  l1.weight: grad_norm = 0.000590
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.002218
Total gradient norm: 0.004507
=== Actor Training Debug (Iteration 407) ===
Q mean: -25.947973
Q std: 22.540585
Actor loss: 25.951973
Action reg: 0.004000
  l1.weight: grad_norm = 0.000599
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001900
Total gradient norm: 0.002911
=== Actor Training Debug (Iteration 408) ===
Q mean: -23.322763
Q std: 19.289965
Actor loss: 23.326735
Action reg: 0.003972
  l1.weight: grad_norm = 0.000612
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.002191
Total gradient norm: 0.004450
=== Actor Training Debug (Iteration 409) ===
Q mean: -27.124105
Q std: 20.449114
Actor loss: 27.128048
Action reg: 0.003943
  l1.weight: grad_norm = 0.001054
  l1.bias: grad_norm = 0.000665
  l2.weight: grad_norm = 0.004354
Total gradient norm: 0.008901
=== Actor Training Debug (Iteration 410) ===
Q mean: -30.888523
Q std: 19.828821
Actor loss: 30.892466
Action reg: 0.003943
  l1.weight: grad_norm = 0.000838
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.003737
Total gradient norm: 0.007887
=== Actor Training Debug (Iteration 411) ===
Q mean: -25.952175
Q std: 20.211851
Actor loss: 25.956160
Action reg: 0.003985
  l1.weight: grad_norm = 0.000402
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.001262
Total gradient norm: 0.002221
=== Actor Training Debug (Iteration 412) ===
Q mean: -24.976711
Q std: 17.958242
Actor loss: 24.980698
Action reg: 0.003987
  l1.weight: grad_norm = 0.000627
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.001956
Total gradient norm: 0.003369
=== Actor Training Debug (Iteration 413) ===
Q mean: -29.737686
Q std: 20.801577
Actor loss: 29.741671
Action reg: 0.003985
  l1.weight: grad_norm = 0.002066
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.006392
Total gradient norm: 0.009688
=== Actor Training Debug (Iteration 414) ===
Q mean: -28.421087
Q std: 22.658037
Actor loss: 28.425045
Action reg: 0.003957
  l1.weight: grad_norm = 0.000862
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.003162
Total gradient norm: 0.006741
=== Actor Training Debug (Iteration 415) ===
Q mean: -25.567711
Q std: 20.678017
Actor loss: 25.571682
Action reg: 0.003971
  l1.weight: grad_norm = 0.000544
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.001849
Total gradient norm: 0.003566
=== Actor Training Debug (Iteration 416) ===
Q mean: -24.487919
Q std: 21.170776
Actor loss: 24.491848
Action reg: 0.003930
  l1.weight: grad_norm = 0.001012
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.004408
Total gradient norm: 0.009432
=== Actor Training Debug (Iteration 417) ===
Q mean: -26.521271
Q std: 22.434269
Actor loss: 26.525242
Action reg: 0.003972
  l1.weight: grad_norm = 0.000670
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.002338
Total gradient norm: 0.004769
=== Actor Training Debug (Iteration 418) ===
Q mean: -28.972933
Q std: 20.655712
Actor loss: 28.976904
Action reg: 0.003971
  l1.weight: grad_norm = 0.000664
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.002302
Total gradient norm: 0.004273
=== Actor Training Debug (Iteration 419) ===
Q mean: -28.770567
Q std: 20.964907
Actor loss: 28.774525
Action reg: 0.003958
  l1.weight: grad_norm = 0.000714
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.002854
Total gradient norm: 0.005745
=== Actor Training Debug (Iteration 420) ===
Q mean: -22.570255
Q std: 19.445450
Actor loss: 22.574213
Action reg: 0.003957
  l1.weight: grad_norm = 0.000626
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.002741
Total gradient norm: 0.006078
=== Actor Training Debug (Iteration 421) ===
Q mean: -22.957106
Q std: 18.730618
Actor loss: 22.961077
Action reg: 0.003971
  l1.weight: grad_norm = 0.009300
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.029089
Total gradient norm: 0.044520
=== Actor Training Debug (Iteration 422) ===
Q mean: -24.802290
Q std: 20.826494
Actor loss: 24.806231
Action reg: 0.003941
  l1.weight: grad_norm = 0.031073
  l1.bias: grad_norm = 0.000494
  l2.weight: grad_norm = 0.097043
Total gradient norm: 0.148084
=== Actor Training Debug (Iteration 423) ===
Q mean: -29.849113
Q std: 19.777557
Actor loss: 29.853056
Action reg: 0.003943
  l1.weight: grad_norm = 0.001219
  l1.bias: grad_norm = 0.000684
  l2.weight: grad_norm = 0.004763
Total gradient norm: 0.009380
=== Actor Training Debug (Iteration 424) ===
Q mean: -32.960709
Q std: 20.302170
Actor loss: 32.964680
Action reg: 0.003971
  l1.weight: grad_norm = 0.000662
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.002421
Total gradient norm: 0.004464
=== Actor Training Debug (Iteration 425) ===
Q mean: -29.137039
Q std: 20.567553
Actor loss: 29.141010
Action reg: 0.003971
  l1.weight: grad_norm = 0.005973
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.019976
Total gradient norm: 0.031288
=== Actor Training Debug (Iteration 426) ===
Q mean: -24.863369
Q std: 18.275852
Actor loss: 24.867327
Action reg: 0.003958
  l1.weight: grad_norm = 0.000773
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.002760
Total gradient norm: 0.005644
=== Actor Training Debug (Iteration 427) ===
Q mean: -23.501806
Q std: 21.231604
Actor loss: 23.505764
Action reg: 0.003957
  l1.weight: grad_norm = 0.008285
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.029818
Total gradient norm: 0.044040
=== Actor Training Debug (Iteration 428) ===
Q mean: -24.176161
Q std: 19.362247
Actor loss: 24.180145
Action reg: 0.003985
  l1.weight: grad_norm = 0.007308
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.026069
Total gradient norm: 0.038209
=== Actor Training Debug (Iteration 429) ===
Q mean: -30.205875
Q std: 21.657545
Actor loss: 30.209860
Action reg: 0.003984
  l1.weight: grad_norm = 0.011142
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.033371
Total gradient norm: 0.050856
=== Actor Training Debug (Iteration 430) ===
Q mean: -31.269140
Q std: 20.276089
Actor loss: 31.273083
Action reg: 0.003942
  l1.weight: grad_norm = 0.020674
  l1.bias: grad_norm = 0.000534
  l2.weight: grad_norm = 0.055287
Total gradient norm: 0.081646
=== Actor Training Debug (Iteration 431) ===
Q mean: -27.305782
Q std: 19.834923
Actor loss: 27.309710
Action reg: 0.003928
  l1.weight: grad_norm = 0.013798
  l1.bias: grad_norm = 0.000776
  l2.weight: grad_norm = 0.047014
Total gradient norm: 0.074100
=== Actor Training Debug (Iteration 432) ===
Q mean: -23.062801
Q std: 20.811832
Actor loss: 23.066772
Action reg: 0.003971
  l1.weight: grad_norm = 0.001144
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.003621
Total gradient norm: 0.005767
=== Actor Training Debug (Iteration 433) ===
Q mean: -25.400095
Q std: 22.606558
Actor loss: 25.404079
Action reg: 0.003985
  l1.weight: grad_norm = 0.000846
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.002373
Total gradient norm: 0.003566
=== Actor Training Debug (Iteration 434) ===
Q mean: -27.816216
Q std: 22.032228
Actor loss: 27.820202
Action reg: 0.003986
  l1.weight: grad_norm = 0.000414
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.001424
Total gradient norm: 0.002409
=== Actor Training Debug (Iteration 435) ===
Q mean: -27.516621
Q std: 22.252472
Actor loss: 27.520565
Action reg: 0.003944
  l1.weight: grad_norm = 0.001163
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.003997
Total gradient norm: 0.007736
=== Actor Training Debug (Iteration 436) ===
Q mean: -28.404873
Q std: 18.010141
Actor loss: 28.408844
Action reg: 0.003971
  l1.weight: grad_norm = 0.021201
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.076809
Total gradient norm: 0.117794
=== Actor Training Debug (Iteration 437) ===
Q mean: -25.379520
Q std: 18.673080
Actor loss: 25.383507
Action reg: 0.003986
  l1.weight: grad_norm = 0.000397
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.001253
Total gradient norm: 0.002373
=== Actor Training Debug (Iteration 438) ===
Q mean: -27.252045
Q std: 18.776321
Actor loss: 27.256016
Action reg: 0.003972
  l1.weight: grad_norm = 0.000618
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.002255
Total gradient norm: 0.004257
=== Actor Training Debug (Iteration 439) ===
Q mean: -26.409889
Q std: 17.744455
Actor loss: 26.413860
Action reg: 0.003971
  l1.weight: grad_norm = 0.000542
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.001929
Total gradient norm: 0.003732
=== Actor Training Debug (Iteration 440) ===
Q mean: -29.190695
Q std: 20.135317
Actor loss: 29.194681
Action reg: 0.003986
  l1.weight: grad_norm = 0.000469
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.001360
Total gradient norm: 0.002137
=== Actor Training Debug (Iteration 441) ===
Q mean: -28.319088
Q std: 20.863031
Actor loss: 28.323074
Action reg: 0.003986
  l1.weight: grad_norm = 0.000465
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.001496
Total gradient norm: 0.002708
=== Actor Training Debug (Iteration 442) ===
Q mean: -27.471195
Q std: 20.185186
Actor loss: 27.475153
Action reg: 0.003958
  l1.weight: grad_norm = 0.000869
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.003062
Total gradient norm: 0.006034
=== Actor Training Debug (Iteration 443) ===
Q mean: -27.242146
Q std: 19.076643
Actor loss: 27.246103
Action reg: 0.003958
  l1.weight: grad_norm = 0.000749
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.002828
Total gradient norm: 0.005718
=== Actor Training Debug (Iteration 444) ===
Q mean: -24.457436
Q std: 19.223690
Actor loss: 24.461418
Action reg: 0.003983
  l1.weight: grad_norm = 0.009983
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.033053
Total gradient norm: 0.049588
=== Actor Training Debug (Iteration 445) ===
Q mean: -26.659424
Q std: 18.851080
Actor loss: 26.663366
Action reg: 0.003943
  l1.weight: grad_norm = 0.000833
  l1.bias: grad_norm = 0.000565
  l2.weight: grad_norm = 0.003596
Total gradient norm: 0.007855
=== Actor Training Debug (Iteration 446) ===
Q mean: -29.286798
Q std: 20.191605
Actor loss: 29.290783
Action reg: 0.003984
  l1.weight: grad_norm = 0.010222
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.035623
Total gradient norm: 0.054475
=== Actor Training Debug (Iteration 447) ===
Q mean: -24.833250
Q std: 19.664070
Actor loss: 24.837236
Action reg: 0.003985
  l1.weight: grad_norm = 0.000465
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.001378
Total gradient norm: 0.002426
=== Actor Training Debug (Iteration 448) ===
Q mean: -26.545609
Q std: 22.438614
Actor loss: 26.549566
Action reg: 0.003958
  l1.weight: grad_norm = 0.007431
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.024289
Total gradient norm: 0.036743
=== Actor Training Debug (Iteration 449) ===
Q mean: -26.920147
Q std: 19.648571
Actor loss: 26.924105
Action reg: 0.003958
  l1.weight: grad_norm = 0.000795
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.003210
Total gradient norm: 0.006523
=== Actor Training Debug (Iteration 450) ===
Q mean: -26.473152
Q std: 20.528860
Actor loss: 26.477125
Action reg: 0.003972
  l1.weight: grad_norm = 0.000647
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.002470
Total gradient norm: 0.004628
=== Actor Training Debug (Iteration 451) ===
Q mean: -29.856750
Q std: 21.641430
Actor loss: 29.860723
Action reg: 0.003973
  l1.weight: grad_norm = 0.001420
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.004287
Total gradient norm: 0.006694
=== Actor Training Debug (Iteration 452) ===
Q mean: -28.536997
Q std: 20.621691
Actor loss: 28.540968
Action reg: 0.003971
  l1.weight: grad_norm = 0.000630
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.002217
Total gradient norm: 0.004367
=== Actor Training Debug (Iteration 453) ===
Q mean: -26.912386
Q std: 17.504297
Actor loss: 26.916344
Action reg: 0.003958
  l1.weight: grad_norm = 0.008091
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.026749
Total gradient norm: 0.038687
=== Actor Training Debug (Iteration 454) ===
Q mean: -25.475634
Q std: 19.359415
Actor loss: 25.479578
Action reg: 0.003944
  l1.weight: grad_norm = 0.000626
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.003132
Total gradient norm: 0.006804
=== Actor Training Debug (Iteration 455) ===
Q mean: -25.599669
Q std: 19.849518
Actor loss: 25.603613
Action reg: 0.003944
  l1.weight: grad_norm = 0.000999
  l1.bias: grad_norm = 0.000618
  l2.weight: grad_norm = 0.004142
Total gradient norm: 0.008664
=== Actor Training Debug (Iteration 456) ===
Q mean: -28.410513
Q std: 23.058489
Actor loss: 28.414484
Action reg: 0.003972
  l1.weight: grad_norm = 0.001490
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.005597
Total gradient norm: 0.009287
=== Actor Training Debug (Iteration 457) ===
Q mean: -28.127811
Q std: 19.047962
Actor loss: 28.131796
Action reg: 0.003984
  l1.weight: grad_norm = 0.043556
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.147729
Total gradient norm: 0.226693
=== Actor Training Debug (Iteration 458) ===
Q mean: -29.507286
Q std: 22.328892
Actor loss: 29.511229
Action reg: 0.003943
  l1.weight: grad_norm = 0.000838
  l1.bias: grad_norm = 0.000631
  l2.weight: grad_norm = 0.003900
Total gradient norm: 0.008134
=== Actor Training Debug (Iteration 459) ===
Q mean: -28.153194
Q std: 19.361574
Actor loss: 28.157167
Action reg: 0.003972
  l1.weight: grad_norm = 0.000657
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.002298
Total gradient norm: 0.004495
=== Actor Training Debug (Iteration 460) ===
Q mean: -25.260937
Q std: 18.786793
Actor loss: 25.264879
Action reg: 0.003943
  l1.weight: grad_norm = 0.000926
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.003979
Total gradient norm: 0.007866
=== Actor Training Debug (Iteration 461) ===
Q mean: -27.047049
Q std: 20.824644
Actor loss: 27.051020
Action reg: 0.003971
  l1.weight: grad_norm = 0.016422
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.050422
Total gradient norm: 0.078619
=== Actor Training Debug (Iteration 462) ===
Q mean: -27.960630
Q std: 19.765831
Actor loss: 27.964630
Action reg: 0.004000
  l1.weight: grad_norm = 0.000125
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000414
Total gradient norm: 0.000631
=== Actor Training Debug (Iteration 463) ===
Q mean: -27.523289
Q std: 20.607170
Actor loss: 27.527260
Action reg: 0.003972
  l1.weight: grad_norm = 0.000544
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.002020
Total gradient norm: 0.003985
=== Actor Training Debug (Iteration 464) ===
Q mean: -27.925419
Q std: 20.541599
Actor loss: 27.929419
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 465) ===
Q mean: -28.653315
Q std: 20.528486
Actor loss: 28.657272
Action reg: 0.003959
  l1.weight: grad_norm = 0.000878
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.003391
Total gradient norm: 0.006726
=== Actor Training Debug (Iteration 466) ===
Q mean: -28.387108
Q std: 21.188347
Actor loss: 28.391094
Action reg: 0.003987
  l1.weight: grad_norm = 0.000545
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.001712
Total gradient norm: 0.002928
=== Actor Training Debug (Iteration 467) ===
Q mean: -25.657597
Q std: 19.050404
Actor loss: 25.661570
Action reg: 0.003973
  l1.weight: grad_norm = 0.000712
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.002341
Total gradient norm: 0.004529
=== Actor Training Debug (Iteration 468) ===
Q mean: -26.367825
Q std: 20.964025
Actor loss: 26.371769
Action reg: 0.003944
  l1.weight: grad_norm = 0.001048
  l1.bias: grad_norm = 0.000585
  l2.weight: grad_norm = 0.004085
Total gradient norm: 0.008243
=== Actor Training Debug (Iteration 469) ===
Q mean: -26.537575
Q std: 20.764267
Actor loss: 26.541519
Action reg: 0.003944
  l1.weight: grad_norm = 0.001120
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.004218
Total gradient norm: 0.008090
=== Actor Training Debug (Iteration 470) ===
Q mean: -26.885796
Q std: 18.386171
Actor loss: 26.889769
Action reg: 0.003973
  l1.weight: grad_norm = 0.000854
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.002955
Total gradient norm: 0.005436
=== Actor Training Debug (Iteration 471) ===
Q mean: -29.760193
Q std: 19.225367
Actor loss: 29.764137
Action reg: 0.003945
  l1.weight: grad_norm = 0.001008
  l1.bias: grad_norm = 0.000553
  l2.weight: grad_norm = 0.004207
Total gradient norm: 0.008234
=== Actor Training Debug (Iteration 472) ===
Q mean: -28.401148
Q std: 20.134874
Actor loss: 28.405134
Action reg: 0.003986
  l1.weight: grad_norm = 0.000409
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.001262
Total gradient norm: 0.002393
=== Actor Training Debug (Iteration 473) ===
Q mean: -25.147480
Q std: 18.856430
Actor loss: 25.151451
Action reg: 0.003971
  l1.weight: grad_norm = 0.000637
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.002150
Total gradient norm: 0.004278
=== Actor Training Debug (Iteration 474) ===
Q mean: -25.921261
Q std: 19.792112
Actor loss: 25.925219
Action reg: 0.003958
  l1.weight: grad_norm = 0.000783
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.002972
Total gradient norm: 0.006138
=== Actor Training Debug (Iteration 475) ===
Q mean: -25.820932
Q std: 21.101604
Actor loss: 25.824890
Action reg: 0.003958
  l1.weight: grad_norm = 0.000794
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.002988
Total gradient norm: 0.005799
=== Actor Training Debug (Iteration 476) ===
Q mean: -28.687216
Q std: 22.170956
Actor loss: 28.691202
Action reg: 0.003986
  l1.weight: grad_norm = 0.000460
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.001562
Total gradient norm: 0.002626
=== Actor Training Debug (Iteration 477) ===
Q mean: -27.828831
Q std: 22.989765
Actor loss: 27.832790
Action reg: 0.003960
  l1.weight: grad_norm = 0.000745
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.003202
Total gradient norm: 0.006510
=== Actor Training Debug (Iteration 478) ===
Q mean: -29.397728
Q std: 22.165077
Actor loss: 29.401714
Action reg: 0.003986
  l1.weight: grad_norm = 0.000664
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.001812
Total gradient norm: 0.002843
=== Actor Training Debug (Iteration 479) ===
Q mean: -26.079323
Q std: 19.995207
Actor loss: 26.083281
Action reg: 0.003958
  l1.weight: grad_norm = 0.000704
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.002979
Total gradient norm: 0.006233
=== Actor Training Debug (Iteration 480) ===
Q mean: -27.032822
Q std: 19.390060
Actor loss: 27.036808
Action reg: 0.003987
  l1.weight: grad_norm = 0.000463
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.001409
Total gradient norm: 0.002399
=== Actor Training Debug (Iteration 481) ===
Q mean: -30.505838
Q std: 20.013359
Actor loss: 30.509825
Action reg: 0.003986
  l1.weight: grad_norm = 0.000382
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.001125
Total gradient norm: 0.002004
=== Actor Training Debug (Iteration 482) ===
Q mean: -28.610186
Q std: 20.061941
Actor loss: 28.614130
Action reg: 0.003944
  l1.weight: grad_norm = 0.000909
  l1.bias: grad_norm = 0.000653
  l2.weight: grad_norm = 0.003922
Total gradient norm: 0.008150
=== Actor Training Debug (Iteration 483) ===
Q mean: -28.822935
Q std: 19.313068
Actor loss: 28.826921
Action reg: 0.003987
  l1.weight: grad_norm = 0.000596
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.001803
Total gradient norm: 0.003070
=== Actor Training Debug (Iteration 484) ===
Q mean: -26.633827
Q std: 20.051985
Actor loss: 26.637800
Action reg: 0.003974
  l1.weight: grad_norm = 0.000921
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.002958
Total gradient norm: 0.005298
=== Actor Training Debug (Iteration 485) ===
Q mean: -28.643284
Q std: 20.830889
Actor loss: 28.647268
Action reg: 0.003985
  l1.weight: grad_norm = 0.001536
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.004832
Total gradient norm: 0.007234
=== Actor Training Debug (Iteration 486) ===
Q mean: -28.925690
Q std: 20.515015
Actor loss: 28.929649
Action reg: 0.003960
  l1.weight: grad_norm = 0.000838
  l1.bias: grad_norm = 0.000484
  l2.weight: grad_norm = 0.003189
Total gradient norm: 0.006622
=== Actor Training Debug (Iteration 487) ===
Q mean: -27.487045
Q std: 19.950430
Actor loss: 27.491003
Action reg: 0.003958
  l1.weight: grad_norm = 0.000886
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.003037
Total gradient norm: 0.006092
=== Actor Training Debug (Iteration 488) ===
Q mean: -26.648619
Q std: 18.899405
Actor loss: 26.652550
Action reg: 0.003931
  l1.weight: grad_norm = 0.001244
  l1.bias: grad_norm = 0.000788
  l2.weight: grad_norm = 0.004906
Total gradient norm: 0.010253
=== Actor Training Debug (Iteration 489) ===
Q mean: -26.003078
Q std: 18.870262
Actor loss: 26.007010
Action reg: 0.003932
  l1.weight: grad_norm = 0.001347
  l1.bias: grad_norm = 0.000799
  l2.weight: grad_norm = 0.005669
Total gradient norm: 0.011685
=== Actor Training Debug (Iteration 490) ===
Q mean: -24.759922
Q std: 19.976665
Actor loss: 24.763895
Action reg: 0.003973
  l1.weight: grad_norm = 0.000739
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.002543
Total gradient norm: 0.004786
=== Actor Training Debug (Iteration 491) ===
Q mean: -30.763142
Q std: 20.072914
Actor loss: 30.767115
Action reg: 0.003972
  l1.weight: grad_norm = 0.000522
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.002011
Total gradient norm: 0.004200
=== Actor Training Debug (Iteration 492) ===
Q mean: -28.620949
Q std: 19.674534
Actor loss: 28.624893
Action reg: 0.003944
  l1.weight: grad_norm = 0.001064
  l1.bias: grad_norm = 0.000752
  l2.weight: grad_norm = 0.004480
Total gradient norm: 0.009478
=== Actor Training Debug (Iteration 493) ===
Q mean: -27.959019
Q std: 20.103077
Actor loss: 27.962978
Action reg: 0.003961
  l1.weight: grad_norm = 0.001047
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.003472
Total gradient norm: 0.006635
=== Actor Training Debug (Iteration 494) ===
Q mean: -24.079168
Q std: 19.023697
Actor loss: 24.083086
Action reg: 0.003918
  l1.weight: grad_norm = 0.001232
  l1.bias: grad_norm = 0.000999
  l2.weight: grad_norm = 0.006055
Total gradient norm: 0.012745
=== Actor Training Debug (Iteration 495) ===
Q mean: -26.229982
Q std: 17.555763
Actor loss: 26.233940
Action reg: 0.003957
  l1.weight: grad_norm = 0.020126
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.063056
Total gradient norm: 0.092880
=== Actor Training Debug (Iteration 496) ===
Q mean: -29.440439
Q std: 19.833260
Actor loss: 29.444439
Action reg: 0.004000
  l1.weight: grad_norm = 0.000833
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.002801
Total gradient norm: 0.004191
=== Actor Training Debug (Iteration 497) ===
Q mean: -30.954353
Q std: 21.127216
Actor loss: 30.958300
Action reg: 0.003946
  l1.weight: grad_norm = 0.001164
  l1.bias: grad_norm = 0.000617
  l2.weight: grad_norm = 0.004471
Total gradient norm: 0.008672
=== Actor Training Debug (Iteration 498) ===
Q mean: -27.593561
Q std: 19.191792
Actor loss: 27.597509
Action reg: 0.003948
  l1.weight: grad_norm = 0.001145
  l1.bias: grad_norm = 0.000672
  l2.weight: grad_norm = 0.004392
Total gradient norm: 0.009261
=== Actor Training Debug (Iteration 499) ===
Q mean: -26.610844
Q std: 21.223160
Actor loss: 26.614817
Action reg: 0.003972
  l1.weight: grad_norm = 0.000674
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.002278
Total gradient norm: 0.004605
=== Actor Training Debug (Iteration 500) ===
Q mean: -31.148733
Q std: 17.994793
Actor loss: 31.152733
Action reg: 0.004000
  l1.weight: grad_norm = 0.000116
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000406
Total gradient norm: 0.000621
  Average reward: -341.197 | Average length: 100.0
Evaluation at episode 55: -341.197
=== Actor Training Debug (Iteration 501) ===
Q mean: -27.181866
Q std: 18.250053
Actor loss: 27.185837
Action reg: 0.003972
  l1.weight: grad_norm = 0.000642
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.002232
Total gradient norm: 0.004474
=== Actor Training Debug (Iteration 502) ===
Q mean: -25.911678
Q std: 19.574362
Actor loss: 25.915651
Action reg: 0.003974
  l1.weight: grad_norm = 0.000656
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.002258
Total gradient norm: 0.004495
=== Actor Training Debug (Iteration 503) ===
Q mean: -25.047235
Q std: 18.541439
Actor loss: 25.051208
Action reg: 0.003973
  l1.weight: grad_norm = 0.000667
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.002069
Total gradient norm: 0.003927
=== Actor Training Debug (Iteration 504) ===
Q mean: -30.795929
Q std: 19.741402
Actor loss: 30.799891
Action reg: 0.003961
  l1.weight: grad_norm = 0.001154
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.003756
Total gradient norm: 0.007089
=== Actor Training Debug (Iteration 505) ===
Q mean: -28.745293
Q std: 19.795103
Actor loss: 28.749252
Action reg: 0.003960
  l1.weight: grad_norm = 0.000925
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.003129
Total gradient norm: 0.006439
=== Actor Training Debug (Iteration 506) ===
Q mean: -27.058678
Q std: 19.890810
Actor loss: 27.062649
Action reg: 0.003972
  l1.weight: grad_norm = 0.000642
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.002238
Total gradient norm: 0.004541
=== Actor Training Debug (Iteration 507) ===
Q mean: -27.243347
Q std: 20.223949
Actor loss: 27.247334
Action reg: 0.003987
  l1.weight: grad_norm = 0.000485
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.001440
Total gradient norm: 0.002684
=== Actor Training Debug (Iteration 508) ===
Q mean: -29.268667
Q std: 21.184729
Actor loss: 29.272629
Action reg: 0.003961
  l1.weight: grad_norm = 0.000924
  l1.bias: grad_norm = 0.000478
  l2.weight: grad_norm = 0.003334
Total gradient norm: 0.006606
=== Actor Training Debug (Iteration 509) ===
Q mean: -27.351673
Q std: 20.109936
Actor loss: 27.355635
Action reg: 0.003961
  l1.weight: grad_norm = 0.000982
  l1.bias: grad_norm = 0.000510
  l2.weight: grad_norm = 0.003441
Total gradient norm: 0.006842
=== Actor Training Debug (Iteration 510) ===
Q mean: -26.198586
Q std: 18.928396
Actor loss: 26.202545
Action reg: 0.003959
  l1.weight: grad_norm = 0.000903
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.003229
Total gradient norm: 0.006384
=== Actor Training Debug (Iteration 511) ===
Q mean: -23.717648
Q std: 18.838869
Actor loss: 23.721596
Action reg: 0.003949
  l1.weight: grad_norm = 0.001741
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.005403
Total gradient norm: 0.009950
=== Actor Training Debug (Iteration 512) ===
Q mean: -27.908861
Q std: 19.261362
Actor loss: 27.912821
Action reg: 0.003959
  l1.weight: grad_norm = 0.000777
  l1.bias: grad_norm = 0.000544
  l2.weight: grad_norm = 0.003164
Total gradient norm: 0.006555
=== Actor Training Debug (Iteration 513) ===
Q mean: -30.032719
Q std: 17.411203
Actor loss: 30.036667
Action reg: 0.003947
  l1.weight: grad_norm = 0.001007
  l1.bias: grad_norm = 0.000655
  l2.weight: grad_norm = 0.004253
Total gradient norm: 0.008903
=== Actor Training Debug (Iteration 514) ===
Q mean: -29.910580
Q std: 18.545839
Actor loss: 29.914566
Action reg: 0.003986
  l1.weight: grad_norm = 0.000537
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.001597
Total gradient norm: 0.002847
=== Actor Training Debug (Iteration 515) ===
Q mean: -27.212708
Q std: 19.130436
Actor loss: 27.216652
Action reg: 0.003945
  l1.weight: grad_norm = 0.005680
  l1.bias: grad_norm = 0.000682
  l2.weight: grad_norm = 0.019661
Total gradient norm: 0.030794
=== Actor Training Debug (Iteration 516) ===
Q mean: -26.555590
Q std: 18.635084
Actor loss: 26.559589
Action reg: 0.004000
  l1.weight: grad_norm = 0.000003
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000008
Total gradient norm: 0.000013
=== Actor Training Debug (Iteration 517) ===
Q mean: -24.697098
Q std: 19.131477
Actor loss: 24.701084
Action reg: 0.003986
  l1.weight: grad_norm = 0.000503
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.001402
Total gradient norm: 0.002502
=== Actor Training Debug (Iteration 518) ===
Q mean: -29.401293
Q std: 20.563114
Actor loss: 29.405279
Action reg: 0.003987
  l1.weight: grad_norm = 0.000450
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.001406
Total gradient norm: 0.002551
=== Actor Training Debug (Iteration 519) ===
Q mean: -31.945213
Q std: 22.367580
Actor loss: 31.949186
Action reg: 0.003974
  l1.weight: grad_norm = 0.000770
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.002661
Total gradient norm: 0.005091
=== Actor Training Debug (Iteration 520) ===
Q mean: -26.639658
Q std: 18.932793
Actor loss: 26.643644
Action reg: 0.003986
  l1.weight: grad_norm = 0.000425
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.001323
Total gradient norm: 0.002302
=== Actor Training Debug (Iteration 521) ===
Q mean: -27.637804
Q std: 17.531164
Actor loss: 27.641750
Action reg: 0.003946
  l1.weight: grad_norm = 0.052944
  l1.bias: grad_norm = 0.000629
  l2.weight: grad_norm = 0.189335
Total gradient norm: 0.292732
=== Actor Training Debug (Iteration 522) ===
Q mean: -29.716274
Q std: 19.569080
Actor loss: 29.720247
Action reg: 0.003973
  l1.weight: grad_norm = 0.000832
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.002900
Total gradient norm: 0.005302
=== Actor Training Debug (Iteration 523) ===
Q mean: -28.986425
Q std: 20.997246
Actor loss: 28.990398
Action reg: 0.003973
  l1.weight: grad_norm = 0.000787
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.002671
Total gradient norm: 0.004955
=== Actor Training Debug (Iteration 524) ===
Q mean: -25.685677
Q std: 16.970314
Actor loss: 25.689610
Action reg: 0.003932
  l1.weight: grad_norm = 0.001224
  l1.bias: grad_norm = 0.000877
  l2.weight: grad_norm = 0.005320
Total gradient norm: 0.011521
=== Actor Training Debug (Iteration 525) ===
Q mean: -24.919521
Q std: 19.639158
Actor loss: 24.923466
Action reg: 0.003945
  l1.weight: grad_norm = 0.002977
  l1.bias: grad_norm = 0.000718
  l2.weight: grad_norm = 0.010740
Total gradient norm: 0.017960
=== Actor Training Debug (Iteration 526) ===
Q mean: -27.936014
Q std: 19.841627
Actor loss: 27.939957
Action reg: 0.003943
  l1.weight: grad_norm = 0.025595
  l1.bias: grad_norm = 0.000750
  l2.weight: grad_norm = 0.091950
Total gradient norm: 0.144049
=== Actor Training Debug (Iteration 527) ===
Q mean: -31.075512
Q std: 19.026115
Actor loss: 31.079458
Action reg: 0.003946
  l1.weight: grad_norm = 0.002907
  l1.bias: grad_norm = 0.000715
  l2.weight: grad_norm = 0.008807
Total gradient norm: 0.014768
=== Actor Training Debug (Iteration 528) ===
Q mean: -28.076645
Q std: 18.502684
Actor loss: 28.080603
Action reg: 0.003958
  l1.weight: grad_norm = 0.004862
  l1.bias: grad_norm = 0.000618
  l2.weight: grad_norm = 0.014200
Total gradient norm: 0.019057
=== Actor Training Debug (Iteration 529) ===
Q mean: -25.753090
Q std: 17.474834
Actor loss: 25.757051
Action reg: 0.003962
  l1.weight: grad_norm = 0.000787
  l1.bias: grad_norm = 0.000492
  l2.weight: grad_norm = 0.003028
Total gradient norm: 0.006572
=== Actor Training Debug (Iteration 530) ===
Q mean: -33.182327
Q std: 20.276600
Actor loss: 33.186317
Action reg: 0.003988
  l1.weight: grad_norm = 0.001305
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.003946
Total gradient norm: 0.006474
=== Actor Training Debug (Iteration 531) ===
Q mean: -30.758305
Q std: 19.713457
Actor loss: 30.762289
Action reg: 0.003985
  l1.weight: grad_norm = 0.008519
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.030332
Total gradient norm: 0.046953
=== Actor Training Debug (Iteration 532) ===
Q mean: -25.660767
Q std: 19.108791
Actor loss: 25.664713
Action reg: 0.003946
  l1.weight: grad_norm = 0.001037
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.004187
Total gradient norm: 0.008572
=== Actor Training Debug (Iteration 533) ===
Q mean: -26.500546
Q std: 20.325377
Actor loss: 26.504501
Action reg: 0.003956
  l1.weight: grad_norm = 0.057730
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.186376
Total gradient norm: 0.281234
=== Actor Training Debug (Iteration 534) ===
Q mean: -29.923214
Q std: 20.203836
Actor loss: 29.927176
Action reg: 0.003961
  l1.weight: grad_norm = 0.000859
  l1.bias: grad_norm = 0.000494
  l2.weight: grad_norm = 0.003035
Total gradient norm: 0.006454
=== Actor Training Debug (Iteration 535) ===
Q mean: -29.554626
Q std: 20.247410
Actor loss: 29.558575
Action reg: 0.003948
  l1.weight: grad_norm = 0.001198
  l1.bias: grad_norm = 0.000721
  l2.weight: grad_norm = 0.004620
Total gradient norm: 0.009402
=== Actor Training Debug (Iteration 536) ===
Q mean: -24.872036
Q std: 20.071819
Actor loss: 24.875988
Action reg: 0.003953
  l1.weight: grad_norm = 0.090868
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.332322
Total gradient norm: 0.548959
=== Actor Training Debug (Iteration 537) ===
Q mean: -26.185146
Q std: 18.111174
Actor loss: 26.189119
Action reg: 0.003972
  l1.weight: grad_norm = 0.006315
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.021248
Total gradient norm: 0.032747
=== Actor Training Debug (Iteration 538) ===
Q mean: -29.839447
Q std: 19.233757
Actor loss: 29.843393
Action reg: 0.003947
  l1.weight: grad_norm = 0.001143
  l1.bias: grad_norm = 0.000734
  l2.weight: grad_norm = 0.004761
Total gradient norm: 0.009841
=== Actor Training Debug (Iteration 539) ===
Q mean: -31.138784
Q std: 19.161983
Actor loss: 31.142784
Action reg: 0.004000
  l1.weight: grad_norm = 0.000015
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000053
Total gradient norm: 0.000086
=== Actor Training Debug (Iteration 540) ===
Q mean: -26.888824
Q std: 19.489679
Actor loss: 26.892799
Action reg: 0.003974
  l1.weight: grad_norm = 0.000787
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.002513
Total gradient norm: 0.004809
=== Actor Training Debug (Iteration 541) ===
Q mean: -25.853590
Q std: 19.073160
Actor loss: 25.857576
Action reg: 0.003987
  l1.weight: grad_norm = 0.010825
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.033000
Total gradient norm: 0.052623
=== Actor Training Debug (Iteration 542) ===
Q mean: -28.891899
Q std: 17.653883
Actor loss: 28.895870
Action reg: 0.003971
  l1.weight: grad_norm = 0.014237
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.048894
Total gradient norm: 0.073545
=== Actor Training Debug (Iteration 543) ===
Q mean: -29.211956
Q std: 18.143795
Actor loss: 29.215914
Action reg: 0.003957
  l1.weight: grad_norm = 0.011635
  l1.bias: grad_norm = 0.000566
  l2.weight: grad_norm = 0.037797
Total gradient norm: 0.060408
=== Actor Training Debug (Iteration 544) ===
Q mean: -27.703979
Q std: 19.622271
Actor loss: 27.707968
Action reg: 0.003988
  l1.weight: grad_norm = 0.000660
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.001616
Total gradient norm: 0.002871
=== Actor Training Debug (Iteration 545) ===
Q mean: -28.677979
Q std: 21.675398
Actor loss: 28.681915
Action reg: 0.003936
  l1.weight: grad_norm = 0.001195
  l1.bias: grad_norm = 0.000822
  l2.weight: grad_norm = 0.004603
Total gradient norm: 0.010122
=== Actor Training Debug (Iteration 546) ===
Q mean: -30.198891
Q std: 20.781685
Actor loss: 30.202864
Action reg: 0.003973
  l1.weight: grad_norm = 0.030737
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.103979
Total gradient norm: 0.159194
=== Actor Training Debug (Iteration 547) ===
Q mean: -27.135401
Q std: 20.056461
Actor loss: 27.139399
Action reg: 0.003998
  l1.weight: grad_norm = 0.020856
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.062397
Total gradient norm: 0.092534
=== Actor Training Debug (Iteration 548) ===
Q mean: -26.802612
Q std: 18.736143
Actor loss: 26.806587
Action reg: 0.003975
  l1.weight: grad_norm = 0.001401
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.004760
Total gradient norm: 0.008022
=== Actor Training Debug (Iteration 549) ===
Q mean: -30.920704
Q std: 18.629650
Actor loss: 30.924667
Action reg: 0.003963
  l1.weight: grad_norm = 0.001085
  l1.bias: grad_norm = 0.000588
  l2.weight: grad_norm = 0.003874
Total gradient norm: 0.007708
=== Actor Training Debug (Iteration 550) ===
Q mean: -29.977766
Q std: 18.621132
Actor loss: 29.981726
Action reg: 0.003960
  l1.weight: grad_norm = 0.000781
  l1.bias: grad_norm = 0.000557
  l2.weight: grad_norm = 0.003104
Total gradient norm: 0.006585
=== Actor Training Debug (Iteration 551) ===
Q mean: -26.746078
Q std: 18.722631
Actor loss: 26.750063
Action reg: 0.003985
  l1.weight: grad_norm = 0.032121
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.101933
Total gradient norm: 0.152799
=== Actor Training Debug (Iteration 552) ===
Q mean: -25.926060
Q std: 21.760317
Actor loss: 25.929953
Action reg: 0.003893
  l1.weight: grad_norm = 0.001271
  l1.bias: grad_norm = 0.001344
  l2.weight: grad_norm = 0.006975
Total gradient norm: 0.016050
=== Actor Training Debug (Iteration 553) ===
Q mean: -28.431894
Q std: 22.265615
Actor loss: 28.435881
Action reg: 0.003987
  l1.weight: grad_norm = 0.000487
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.001334
Total gradient norm: 0.002462
=== Actor Training Debug (Iteration 554) ===
Q mean: -29.328739
Q std: 20.886374
Actor loss: 29.332674
Action reg: 0.003934
  l1.weight: grad_norm = 0.006208
  l1.bias: grad_norm = 0.000796
  l2.weight: grad_norm = 0.017663
Total gradient norm: 0.026668
=== Actor Training Debug (Iteration 555) ===
Q mean: -26.732424
Q std: 17.446449
Actor loss: 26.736423
Action reg: 0.003999
  l1.weight: grad_norm = 0.001090
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.003613
Total gradient norm: 0.005763
=== Actor Training Debug (Iteration 556) ===
Q mean: -26.163969
Q std: 17.969385
Actor loss: 26.167942
Action reg: 0.003973
  l1.weight: grad_norm = 0.003345
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.010685
Total gradient norm: 0.016748
=== Actor Training Debug (Iteration 557) ===
Q mean: -29.978228
Q std: 19.220615
Actor loss: 29.982212
Action reg: 0.003985
  l1.weight: grad_norm = 0.018737
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.068438
Total gradient norm: 0.107635
=== Actor Training Debug (Iteration 558) ===
Q mean: -29.597336
Q std: 18.271912
Actor loss: 29.601334
Action reg: 0.003998
  l1.weight: grad_norm = 0.028192
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.086514
Total gradient norm: 0.133981
=== Actor Training Debug (Iteration 559) ===
Q mean: -28.225792
Q std: 22.076006
Actor loss: 28.229765
Action reg: 0.003973
  l1.weight: grad_norm = 0.001227
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.003567
Total gradient norm: 0.006087
=== Actor Training Debug (Iteration 560) ===
Q mean: -24.576065
Q std: 19.932920
Actor loss: 24.580040
Action reg: 0.003974
  l1.weight: grad_norm = 0.005511
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.015112
Total gradient norm: 0.020241
=== Actor Training Debug (Iteration 561) ===
Q mean: -27.162811
Q std: 21.748549
Actor loss: 27.166769
Action reg: 0.003958
  l1.weight: grad_norm = 0.073945
  l1.bias: grad_norm = 0.000565
  l2.weight: grad_norm = 0.235921
Total gradient norm: 0.385804
=== Actor Training Debug (Iteration 562) ===
Q mean: -31.074165
Q std: 19.562014
Actor loss: 31.078152
Action reg: 0.003986
  l1.weight: grad_norm = 0.000500
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.001379
Total gradient norm: 0.002476
=== Actor Training Debug (Iteration 563) ===
Q mean: -31.531574
Q std: 19.698786
Actor loss: 31.535509
Action reg: 0.003935
  l1.weight: grad_norm = 0.055991
  l1.bias: grad_norm = 0.000805
  l2.weight: grad_norm = 0.180187
Total gradient norm: 0.291172
=== Actor Training Debug (Iteration 564) ===
Q mean: -25.918699
Q std: 19.405207
Actor loss: 25.922686
Action reg: 0.003987
  l1.weight: grad_norm = 0.001033
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.003367
Total gradient norm: 0.005456
=== Actor Training Debug (Iteration 565) ===
Q mean: -27.394794
Q std: 19.966612
Actor loss: 27.398766
Action reg: 0.003972
  l1.weight: grad_norm = 0.036523
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.129565
Total gradient norm: 0.195914
=== Actor Training Debug (Iteration 566) ===
Q mean: -29.251804
Q std: 21.650484
Actor loss: 29.255762
Action reg: 0.003958
  l1.weight: grad_norm = 0.009092
  l1.bias: grad_norm = 0.000572
  l2.weight: grad_norm = 0.030009
Total gradient norm: 0.045896
=== Actor Training Debug (Iteration 567) ===
Q mean: -30.776951
Q std: 19.436689
Actor loss: 30.780912
Action reg: 0.003961
  l1.weight: grad_norm = 0.001870
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.005633
Total gradient norm: 0.009131
=== Actor Training Debug (Iteration 568) ===
Q mean: -25.604101
Q std: 19.018528
Actor loss: 25.608101
Action reg: 0.004000
  l1.weight: grad_norm = 0.000302
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001119
Total gradient norm: 0.001747
=== Actor Training Debug (Iteration 569) ===
Q mean: -26.633902
Q std: 21.466417
Actor loss: 26.637863
Action reg: 0.003962
  l1.weight: grad_norm = 0.001260
  l1.bias: grad_norm = 0.000536
  l2.weight: grad_norm = 0.003960
Total gradient norm: 0.007105
=== Actor Training Debug (Iteration 570) ===
Q mean: -30.754887
Q std: 19.431402
Actor loss: 30.758860
Action reg: 0.003973
  l1.weight: grad_norm = 0.039449
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.124959
Total gradient norm: 0.193707
=== Actor Training Debug (Iteration 571) ===
Q mean: -31.890339
Q std: 19.836151
Actor loss: 31.894300
Action reg: 0.003961
  l1.weight: grad_norm = 0.006671
  l1.bias: grad_norm = 0.000509
  l2.weight: grad_norm = 0.020902
Total gradient norm: 0.031748
=== Actor Training Debug (Iteration 572) ===
Q mean: -27.972954
Q std: 19.503801
Actor loss: 27.976912
Action reg: 0.003958
  l1.weight: grad_norm = 0.016305
  l1.bias: grad_norm = 0.000595
  l2.weight: grad_norm = 0.044619
Total gradient norm: 0.059054
=== Actor Training Debug (Iteration 573) ===
Q mean: -26.541899
Q std: 19.755283
Actor loss: 26.545843
Action reg: 0.003945
  l1.weight: grad_norm = 0.028808
  l1.bias: grad_norm = 0.000647
  l2.weight: grad_norm = 0.076612
Total gradient norm: 0.101170
=== Actor Training Debug (Iteration 574) ===
Q mean: -29.009842
Q std: 21.294966
Actor loss: 29.013803
Action reg: 0.003961
  l1.weight: grad_norm = 0.000827
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.003147
Total gradient norm: 0.006568
=== Actor Training Debug (Iteration 575) ===
Q mean: -29.670805
Q std: 20.567450
Actor loss: 29.674791
Action reg: 0.003987
  l1.weight: grad_norm = 0.000697
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.002123
Total gradient norm: 0.003405
=== Actor Training Debug (Iteration 576) ===
Q mean: -26.933123
Q std: 19.940559
Actor loss: 26.937059
Action reg: 0.003938
  l1.weight: grad_norm = 0.024197
  l1.bias: grad_norm = 0.000809
  l2.weight: grad_norm = 0.078875
Total gradient norm: 0.123984
=== Actor Training Debug (Iteration 577) ===
Q mean: -27.968216
Q std: 20.195520
Actor loss: 27.972185
Action reg: 0.003969
  l1.weight: grad_norm = 0.076717
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.241632
Total gradient norm: 0.393958
=== Actor Training Debug (Iteration 578) ===
Q mean: -29.021301
Q std: 18.873564
Actor loss: 29.025248
Action reg: 0.003947
  l1.weight: grad_norm = 0.014227
  l1.bias: grad_norm = 0.000665
  l2.weight: grad_norm = 0.043949
Total gradient norm: 0.067600
=== Actor Training Debug (Iteration 579) ===
Q mean: -25.081146
Q std: 20.120247
Actor loss: 25.085091
Action reg: 0.003944
  l1.weight: grad_norm = 0.018655
  l1.bias: grad_norm = 0.000667
  l2.weight: grad_norm = 0.060644
Total gradient norm: 0.091177
=== Actor Training Debug (Iteration 580) ===
Q mean: -27.087227
Q std: 19.164516
Actor loss: 27.091202
Action reg: 0.003976
  l1.weight: grad_norm = 0.000656
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.002078
Total gradient norm: 0.004229
=== Actor Training Debug (Iteration 581) ===
Q mean: -31.326111
Q std: 18.312458
Actor loss: 31.330097
Action reg: 0.003986
  l1.weight: grad_norm = 0.007488
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.021975
Total gradient norm: 0.033184
=== Actor Training Debug (Iteration 582) ===
Q mean: -32.765083
Q std: 20.637783
Actor loss: 32.769043
Action reg: 0.003959
  l1.weight: grad_norm = 0.032856
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.093759
Total gradient norm: 0.147811
=== Actor Training Debug (Iteration 583) ===
Q mean: -28.650959
Q std: 19.017773
Actor loss: 28.654896
Action reg: 0.003937
  l1.weight: grad_norm = 0.001303
  l1.bias: grad_norm = 0.000878
  l2.weight: grad_norm = 0.005632
Total gradient norm: 0.011777
=== Actor Training Debug (Iteration 584) ===
Q mean: -26.216251
Q std: 18.838764
Actor loss: 26.220184
Action reg: 0.003934
  l1.weight: grad_norm = 0.001131
  l1.bias: grad_norm = 0.000921
  l2.weight: grad_norm = 0.004804
Total gradient norm: 0.010572
=== Actor Training Debug (Iteration 585) ===
Q mean: -30.046562
Q std: 19.554602
Actor loss: 30.050524
Action reg: 0.003962
  l1.weight: grad_norm = 0.001274
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.004399
Total gradient norm: 0.008283
=== Actor Training Debug (Iteration 586) ===
Q mean: -31.624847
Q std: 18.959427
Actor loss: 31.628847
Action reg: 0.004000
  l1.weight: grad_norm = 0.000016
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000054
Total gradient norm: 0.000084
=== Actor Training Debug (Iteration 587) ===
Q mean: -23.900616
Q std: 18.540983
Actor loss: 23.904560
Action reg: 0.003944
  l1.weight: grad_norm = 0.054222
  l1.bias: grad_norm = 0.000602
  l2.weight: grad_norm = 0.180984
Total gradient norm: 0.299779
=== Actor Training Debug (Iteration 588) ===
Q mean: -25.777826
Q std: 18.790407
Actor loss: 25.781790
Action reg: 0.003963
  l1.weight: grad_norm = 0.005112
  l1.bias: grad_norm = 0.000498
  l2.weight: grad_norm = 0.012273
Total gradient norm: 0.016491
=== Actor Training Debug (Iteration 589) ===
Q mean: -28.386124
Q std: 19.953913
Actor loss: 28.390097
Action reg: 0.003974
  l1.weight: grad_norm = 0.000755
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.002533
Total gradient norm: 0.005094
=== Actor Training Debug (Iteration 590) ===
Q mean: -29.672878
Q std: 20.141699
Actor loss: 29.676851
Action reg: 0.003972
  l1.weight: grad_norm = 0.048547
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.136617
Total gradient norm: 0.178474
=== Actor Training Debug (Iteration 591) ===
Q mean: -26.464951
Q std: 19.319950
Actor loss: 26.468931
Action reg: 0.003981
  l1.weight: grad_norm = 0.061710
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.216302
Total gradient norm: 0.345955
=== Actor Training Debug (Iteration 592) ===
Q mean: -28.015427
Q std: 19.035282
Actor loss: 28.019402
Action reg: 0.003975
  l1.weight: grad_norm = 0.003238
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.010735
Total gradient norm: 0.018042
=== Actor Training Debug (Iteration 593) ===
Q mean: -24.678602
Q std: 18.658024
Actor loss: 24.682575
Action reg: 0.003972
  l1.weight: grad_norm = 0.029984
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.077968
Total gradient norm: 0.103016
=== Actor Training Debug (Iteration 594) ===
Q mean: -30.303122
Q std: 18.134020
Actor loss: 30.307110
Action reg: 0.003988
  l1.weight: grad_norm = 0.001322
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.003791
Total gradient norm: 0.006219
=== Actor Training Debug (Iteration 595) ===
Q mean: -28.320068
Q std: 19.013651
Actor loss: 28.324055
Action reg: 0.003987
  l1.weight: grad_norm = 0.007507
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.021412
Total gradient norm: 0.032870
=== Actor Training Debug (Iteration 596) ===
Q mean: -27.654091
Q std: 19.416410
Actor loss: 27.658054
Action reg: 0.003964
  l1.weight: grad_norm = 0.001117
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.003567
Total gradient norm: 0.006578
=== Actor Training Debug (Iteration 597) ===
Q mean: -27.099691
Q std: 18.989353
Actor loss: 27.103659
Action reg: 0.003968
  l1.weight: grad_norm = 0.068661
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.211955
Total gradient norm: 0.346003
=== Actor Training Debug (Iteration 598) ===
Q mean: -27.994940
Q std: 18.418243
Actor loss: 27.998901
Action reg: 0.003962
  l1.weight: grad_norm = 0.006453
  l1.bias: grad_norm = 0.000578
  l2.weight: grad_norm = 0.021096
Total gradient norm: 0.031909
=== Actor Training Debug (Iteration 599) ===
Q mean: -31.086651
Q std: 19.504601
Actor loss: 31.090651
Action reg: 0.004000
  l1.weight: grad_norm = 0.000081
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000271
Total gradient norm: 0.000432
=== Actor Training Debug (Iteration 600) ===
Q mean: -30.483677
Q std: 20.953253
Actor loss: 30.487663
Action reg: 0.003987
  l1.weight: grad_norm = 0.027813
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.079566
Total gradient norm: 0.117567
=== Actor Training Debug (Iteration 601) ===
Q mean: -26.073875
Q std: 21.035181
Actor loss: 26.077850
Action reg: 0.003975
  l1.weight: grad_norm = 0.001748
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.005385
Total gradient norm: 0.009172
=== Actor Training Debug (Iteration 602) ===
Q mean: -26.418335
Q std: 21.626366
Actor loss: 26.422285
Action reg: 0.003950
  l1.weight: grad_norm = 0.000936
  l1.bias: grad_norm = 0.000703
  l2.weight: grad_norm = 0.003586
Total gradient norm: 0.008138
=== Actor Training Debug (Iteration 603) ===
Q mean: -30.064959
Q std: 18.580629
Actor loss: 30.068922
Action reg: 0.003964
  l1.weight: grad_norm = 0.000924
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.002978
Total gradient norm: 0.006094
=== Actor Training Debug (Iteration 604) ===
Q mean: -29.433842
Q std: 18.486002
Actor loss: 29.437815
Action reg: 0.003973
  l1.weight: grad_norm = 0.012518
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.038658
Total gradient norm: 0.064576
=== Actor Training Debug (Iteration 605) ===
Q mean: -26.355785
Q std: 18.345280
Actor loss: 26.359747
Action reg: 0.003961
  l1.weight: grad_norm = 0.000878
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.002893
Total gradient norm: 0.006095
=== Actor Training Debug (Iteration 606) ===
Q mean: -30.551147
Q std: 20.260515
Actor loss: 30.555130
Action reg: 0.003983
  l1.weight: grad_norm = 0.092769
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.288990
Total gradient norm: 0.457454
=== Actor Training Debug (Iteration 607) ===
Q mean: -26.441217
Q std: 18.769720
Actor loss: 26.445206
Action reg: 0.003987
  l1.weight: grad_norm = 0.000907
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.002800
Total gradient norm: 0.004809
=== Actor Training Debug (Iteration 608) ===
Q mean: -29.417225
Q std: 19.896944
Actor loss: 29.421190
Action reg: 0.003965
  l1.weight: grad_norm = 0.015478
  l1.bias: grad_norm = 0.000544
  l2.weight: grad_norm = 0.052493
Total gradient norm: 0.085736
=== Actor Training Debug (Iteration 609) ===
Q mean: -30.511614
Q std: 19.829977
Actor loss: 30.515549
Action reg: 0.003935
  l1.weight: grad_norm = 0.021039
  l1.bias: grad_norm = 0.000963
  l2.weight: grad_norm = 0.063162
Total gradient norm: 0.101326
=== Actor Training Debug (Iteration 610) ===
Q mean: -27.930992
Q std: 17.871660
Actor loss: 27.934940
Action reg: 0.003947
  l1.weight: grad_norm = 0.047162
  l1.bias: grad_norm = 0.000734
  l2.weight: grad_norm = 0.144461
Total gradient norm: 0.228102
=== Actor Training Debug (Iteration 611) ===
Q mean: -29.723900
Q std: 19.291182
Actor loss: 29.727873
Action reg: 0.003973
  l1.weight: grad_norm = 0.051095
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.148316
Total gradient norm: 0.229514
=== Actor Training Debug (Iteration 612) ===
Q mean: -29.521389
Q std: 19.618475
Actor loss: 29.525335
Action reg: 0.003946
  l1.weight: grad_norm = 0.049673
  l1.bias: grad_norm = 0.000780
  l2.weight: grad_norm = 0.122339
Total gradient norm: 0.162294
=== Actor Training Debug (Iteration 613) ===
Q mean: -29.849808
Q std: 18.993811
Actor loss: 29.853806
Action reg: 0.003997
  l1.weight: grad_norm = 0.046681
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.164289
Total gradient norm: 0.272208
=== Actor Training Debug (Iteration 614) ===
Q mean: -27.978039
Q std: 20.551836
Actor loss: 27.982000
Action reg: 0.003961
  l1.weight: grad_norm = 0.003938
  l1.bias: grad_norm = 0.000528
  l2.weight: grad_norm = 0.011507
Total gradient norm: 0.018264
=== Actor Training Debug (Iteration 615) ===
Q mean: -27.512089
Q std: 18.647512
Actor loss: 27.516048
Action reg: 0.003961
  l1.weight: grad_norm = 0.011386
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.032908
Total gradient norm: 0.051979
=== Actor Training Debug (Iteration 616) ===
Q mean: -31.780098
Q std: 19.883192
Actor loss: 31.784048
Action reg: 0.003950
  l1.weight: grad_norm = 0.043001
  l1.bias: grad_norm = 0.000707
  l2.weight: grad_norm = 0.137032
Total gradient norm: 0.217205
=== Actor Training Debug (Iteration 617) ===
Q mean: -25.301838
Q std: 17.946966
Actor loss: 25.305838
Action reg: 0.003999
  l1.weight: grad_norm = 0.021282
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.064793
Total gradient norm: 0.098917
=== Actor Training Debug (Iteration 618) ===
Q mean: -26.088436
Q std: 20.646355
Actor loss: 26.092398
Action reg: 0.003963
  l1.weight: grad_norm = 0.005098
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.014969
Total gradient norm: 0.024741
=== Actor Training Debug (Iteration 619) ===
Q mean: -29.079716
Q std: 21.722237
Actor loss: 29.083656
Action reg: 0.003941
  l1.weight: grad_norm = 0.001628
  l1.bias: grad_norm = 0.000893
  l2.weight: grad_norm = 0.005333
Total gradient norm: 0.011007
=== Actor Training Debug (Iteration 620) ===
Q mean: -29.709108
Q std: 19.620615
Actor loss: 29.713095
Action reg: 0.003986
  l1.weight: grad_norm = 0.028151
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.085895
Total gradient norm: 0.130085
=== Actor Training Debug (Iteration 621) ===
Q mean: -26.413887
Q std: 20.270859
Actor loss: 26.417887
Action reg: 0.004000
  l1.weight: grad_norm = 0.000109
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000349
Total gradient norm: 0.000505
=== Actor Training Debug (Iteration 622) ===
Q mean: -28.110928
Q std: 19.937103
Actor loss: 28.114901
Action reg: 0.003974
  l1.weight: grad_norm = 0.044733
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.142109
Total gradient norm: 0.238768
=== Actor Training Debug (Iteration 623) ===
Q mean: -30.052803
Q std: 20.039915
Actor loss: 30.056780
Action reg: 0.003977
  l1.weight: grad_norm = 0.000730
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.002333
Total gradient norm: 0.004702
=== Actor Training Debug (Iteration 624) ===
Q mean: -30.882408
Q std: 19.564951
Actor loss: 30.886366
Action reg: 0.003958
  l1.weight: grad_norm = 0.046106
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.146503
Total gradient norm: 0.237361
=== Actor Training Debug (Iteration 625) ===
Q mean: -31.516636
Q std: 19.403004
Actor loss: 31.520611
Action reg: 0.003976
  l1.weight: grad_norm = 0.001004
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.002877
Total gradient norm: 0.005072
=== Actor Training Debug (Iteration 626) ===
Q mean: -27.607710
Q std: 18.899519
Actor loss: 27.611673
Action reg: 0.003964
  l1.weight: grad_norm = 0.007714
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.023071
Total gradient norm: 0.037199
=== Actor Training Debug (Iteration 627) ===
Q mean: -25.402365
Q std: 18.811169
Actor loss: 25.406330
Action reg: 0.003966
  l1.weight: grad_norm = 0.003393
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.008938
Total gradient norm: 0.013529
=== Actor Training Debug (Iteration 628) ===
Q mean: -30.014727
Q std: 19.166786
Actor loss: 30.018726
Action reg: 0.004000
  l1.weight: grad_norm = 0.000224
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000721
Total gradient norm: 0.001101
=== Actor Training Debug (Iteration 629) ===
Q mean: -27.859467
Q std: 16.952055
Actor loss: 27.863398
Action reg: 0.003930
  l1.weight: grad_norm = 0.002049
  l1.bias: grad_norm = 0.001003
  l2.weight: grad_norm = 0.006266
Total gradient norm: 0.012305
=== Actor Training Debug (Iteration 630) ===
Q mean: -28.874844
Q std: 18.518364
Actor loss: 28.878819
Action reg: 0.003975
  l1.weight: grad_norm = 0.001109
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.003161
Total gradient norm: 0.005231
=== Actor Training Debug (Iteration 631) ===
Q mean: -28.192961
Q std: 20.368069
Actor loss: 28.196922
Action reg: 0.003961
  l1.weight: grad_norm = 0.002850
  l1.bias: grad_norm = 0.000636
  l2.weight: grad_norm = 0.008598
Total gradient norm: 0.013390
=== Actor Training Debug (Iteration 632) ===
Q mean: -29.132898
Q std: 19.435850
Actor loss: 29.136860
Action reg: 0.003962
  l1.weight: grad_norm = 0.000887
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.003042
Total gradient norm: 0.006440
Q mean: -28.847755
Q std: 20.172987
Actor loss: 28.851719
Action reg: 0.003963
  l1.weight: grad_norm = 0.001097
  l1.bias: grad_norm = 0.000654
  l2.weight: grad_norm = 0.003478
Total gradient norm: 0.006716
=== Actor Training Debug (Iteration 651) ===
Q mean: -28.448244
Q std: 20.600986
Actor loss: 28.452185
Action reg: 0.003941
  l1.weight: grad_norm = 0.001610
  l1.bias: grad_norm = 0.000980
  l2.weight: grad_norm = 0.005698
Total gradient norm: 0.011799
=== Actor Training Debug (Iteration 652) ===
Q mean: -32.715290
Q std: 20.697077
Actor loss: 32.719254
Action reg: 0.003963
  l1.weight: grad_norm = 0.009019
  l1.bias: grad_norm = 0.000582
  l2.weight: grad_norm = 0.030478
Total gradient norm: 0.047396
=== Actor Training Debug (Iteration 653) ===
Q mean: -27.600647
Q std: 19.333166
Actor loss: 27.604580
Action reg: 0.003932
  l1.weight: grad_norm = 0.001178
  l1.bias: grad_norm = 0.000995
  l2.weight: grad_norm = 0.005490
Total gradient norm: 0.012306
=== Actor Training Debug (Iteration 654) ===
Q mean: -26.579180
Q std: 18.928144
Actor loss: 26.583168
Action reg: 0.003989
  l1.weight: grad_norm = 0.001729
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.005120
Total gradient norm: 0.007660
=== Actor Training Debug (Iteration 655) ===
Q mean: -32.839191
Q std: 19.119225
Actor loss: 32.843121
Action reg: 0.003929
  l1.weight: grad_norm = 0.032340
  l1.bias: grad_norm = 0.001090
  l2.weight: grad_norm = 0.091676
Total gradient norm: 0.140593
=== Actor Training Debug (Iteration 656) ===
Q mean: -29.181339
Q std: 17.900286
Actor loss: 29.185295
Action reg: 0.003956
  l1.weight: grad_norm = 0.001378
  l1.bias: grad_norm = 0.000661
  l2.weight: grad_norm = 0.004154
Total gradient norm: 0.008229
=== Actor Training Debug (Iteration 657) ===
Q mean: -28.341213
Q std: 20.977470
Actor loss: 28.345175
Action reg: 0.003962
  l1.weight: grad_norm = 0.000966
  l1.bias: grad_norm = 0.000636
  l2.weight: grad_norm = 0.003032
Total gradient norm: 0.006497
=== Actor Training Debug (Iteration 658) ===
Q mean: -27.313595
Q std: 19.755150
Actor loss: 27.317572
Action reg: 0.003977
  l1.weight: grad_norm = 0.000684
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.002217
Total gradient norm: 0.004415
=== Actor Training Debug (Iteration 659) ===
Q mean: -30.326694
Q std: 18.503183
Actor loss: 30.330679
Action reg: 0.003985
  l1.weight: grad_norm = 0.010969
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.032709
Total gradient norm: 0.053574
=== Actor Training Debug (Iteration 660) ===
Q mean: -32.787598
Q std: 20.266272
Actor loss: 32.791595
Action reg: 0.003998
  l1.weight: grad_norm = 0.017558
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.048699
Total gradient norm: 0.076332
=== Actor Training Debug (Iteration 661) ===
Q mean: -29.142933
Q std: 19.214668
Actor loss: 29.146896
Action reg: 0.003963
  l1.weight: grad_norm = 0.023815
  l1.bias: grad_norm = 0.000548
  l2.weight: grad_norm = 0.066835
Total gradient norm: 0.100595
=== Actor Training Debug (Iteration 662) ===
Q mean: -28.495171
Q std: 20.696030
Actor loss: 28.499144
Action reg: 0.003973
  l1.weight: grad_norm = 0.032426
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.090438
Total gradient norm: 0.122450
=== Actor Training Debug (Iteration 663) ===
Q mean: -30.569447
Q std: 18.132559
Actor loss: 30.573425
Action reg: 0.003978
  l1.weight: grad_norm = 0.006831
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.018874
Total gradient norm: 0.030773
=== Actor Training Debug (Iteration 664) ===
Q mean: -30.614132
Q std: 16.885187
Actor loss: 30.618107
Action reg: 0.003976
  l1.weight: grad_norm = 0.007045
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.021211
Total gradient norm: 0.032633
=== Actor Training Debug (Iteration 665) ===
Q mean: -26.989161
Q std: 18.104414
Actor loss: 26.993137
Action reg: 0.003977
  l1.weight: grad_norm = 0.001977
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.005795
Total gradient norm: 0.009046
=== Actor Training Debug (Iteration 666) ===
Q mean: -28.016300
Q std: 19.092772
Actor loss: 28.020266
Action reg: 0.003965
  l1.weight: grad_norm = 0.000774
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.002729
Total gradient norm: 0.005824
=== Actor Training Debug (Iteration 667) ===
Q mean: -31.635912
Q std: 19.204472
Actor loss: 31.639868
Action reg: 0.003956
  l1.weight: grad_norm = 0.001218
  l1.bias: grad_norm = 0.000761
  l2.weight: grad_norm = 0.004045
Total gradient norm: 0.008587
=== Actor Training Debug (Iteration 668) ===
Q mean: -31.053858
Q std: 21.279581
Actor loss: 31.057846
Action reg: 0.003988
  l1.weight: grad_norm = 0.000629
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.001534
Total gradient norm: 0.002642
=== Actor Training Debug (Iteration 669) ===
Q mean: -28.539425
Q std: 19.594681
Actor loss: 28.543335
Action reg: 0.003909
  l1.weight: grad_norm = 0.016944
  l1.bias: grad_norm = 0.001393
  l2.weight: grad_norm = 0.049862
Total gradient norm: 0.078764
=== Actor Training Debug (Iteration 670) ===
Q mean: -29.371647
Q std: 18.039257
Actor loss: 29.375599
Action reg: 0.003952
  l1.weight: grad_norm = 0.001458
  l1.bias: grad_norm = 0.000790
  l2.weight: grad_norm = 0.004911
Total gradient norm: 0.008997
=== Actor Training Debug (Iteration 671) ===
Q mean: -30.725954
Q std: 17.442326
Actor loss: 30.729919
Action reg: 0.003966
  l1.weight: grad_norm = 0.000992
  l1.bias: grad_norm = 0.000507
  l2.weight: grad_norm = 0.002994
Total gradient norm: 0.006005
=== Actor Training Debug (Iteration 672) ===
Q mean: -28.412510
Q std: 19.192673
Actor loss: 28.416452
Action reg: 0.003943
  l1.weight: grad_norm = 0.001549
  l1.bias: grad_norm = 0.000797
  l2.weight: grad_norm = 0.006016
Total gradient norm: 0.012148
=== Actor Training Debug (Iteration 673) ===
Q mean: -28.573524
Q std: 17.277613
Actor loss: 28.577524
Action reg: 0.004000
  l1.weight: grad_norm = 0.000260
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000832
Total gradient norm: 0.001375
=== Actor Training Debug (Iteration 674) ===
Q mean: -30.602182
Q std: 20.115633
Actor loss: 30.606125
Action reg: 0.003943
  l1.weight: grad_norm = 0.001382
  l1.bias: grad_norm = 0.000976
  l2.weight: grad_norm = 0.004748
Total gradient norm: 0.010232
=== Actor Training Debug (Iteration 675) ===
Q mean: -32.788895
Q std: 18.547863
Actor loss: 32.792858
Action reg: 0.003963
  l1.weight: grad_norm = 0.014866
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.044051
Total gradient norm: 0.067726
=== Actor Training Debug (Iteration 676) ===
Q mean: -27.590664
Q std: 18.742901
Actor loss: 27.594662
Action reg: 0.003999
  l1.weight: grad_norm = 0.034794
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.098693
Total gradient norm: 0.154765
=== Actor Training Debug (Iteration 677) ===
Q mean: -28.846550
Q std: 17.550014
Actor loss: 28.850515
Action reg: 0.003965
  l1.weight: grad_norm = 0.001056
  l1.bias: grad_norm = 0.000694
  l2.weight: grad_norm = 0.003456
Total gradient norm: 0.007066
=== Actor Training Debug (Iteration 678) ===
Q mean: -30.251581
Q std: 19.583008
Actor loss: 30.255558
Action reg: 0.003977
  l1.weight: grad_norm = 0.005188
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.015933
Total gradient norm: 0.024688
=== Actor Training Debug (Iteration 679) ===
Q mean: -27.241888
Q std: 18.585052
Actor loss: 27.245853
Action reg: 0.003966
  l1.weight: grad_norm = 0.041171
  l1.bias: grad_norm = 0.000505
  l2.weight: grad_norm = 0.114361
Total gradient norm: 0.178938
=== Actor Training Debug (Iteration 680) ===
Q mean: -29.018368
Q std: 19.180740
Actor loss: 29.022367
Action reg: 0.004000
  l1.weight: grad_norm = 0.001430
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003804
Total gradient norm: 0.005978
=== Actor Training Debug (Iteration 681) ===
Q mean: -31.153088
Q std: 20.922876
Actor loss: 31.157047
Action reg: 0.003960
  l1.weight: grad_norm = 0.086476
  l1.bias: grad_norm = 0.000626
  l2.weight: grad_norm = 0.264034
Total gradient norm: 0.432307
=== Actor Training Debug (Iteration 682) ===
Q mean: -31.963097
Q std: 20.731663
Actor loss: 31.967051
Action reg: 0.003954
  l1.weight: grad_norm = 0.001087
  l1.bias: grad_norm = 0.000797
  l2.weight: grad_norm = 0.003879
Total gradient norm: 0.008292
=== Actor Training Debug (Iteration 683) ===
Q mean: -31.531609
Q std: 19.023485
Actor loss: 31.535559
Action reg: 0.003951
  l1.weight: grad_norm = 0.038127
  l1.bias: grad_norm = 0.000699
  l2.weight: grad_norm = 0.097474
Total gradient norm: 0.126370
=== Actor Training Debug (Iteration 684) ===
Q mean: -26.639713
Q std: 18.351103
Actor loss: 26.643665
Action reg: 0.003953
  l1.weight: grad_norm = 0.008339
  l1.bias: grad_norm = 0.000765
  l2.weight: grad_norm = 0.023410
Total gradient norm: 0.034585
=== Actor Training Debug (Iteration 685) ===
Q mean: -26.222496
Q std: 19.295612
Actor loss: 26.226440
Action reg: 0.003943
  l1.weight: grad_norm = 0.001021
  l1.bias: grad_norm = 0.000905
  l2.weight: grad_norm = 0.004462
Total gradient norm: 0.010236
=== Actor Training Debug (Iteration 686) ===
Q mean: -28.817070
Q std: 18.318398
Actor loss: 28.821058
Action reg: 0.003988
  l1.weight: grad_norm = 0.000498
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.001092
Total gradient norm: 0.002174
=== Actor Training Debug (Iteration 687) ===
Q mean: -31.446327
Q std: 20.229746
Actor loss: 31.450296
Action reg: 0.003969
  l1.weight: grad_norm = 0.000915
  l1.bias: grad_norm = 0.000494
  l2.weight: grad_norm = 0.002657
Total gradient norm: 0.005375
=== Actor Training Debug (Iteration 688) ===
Q mean: -32.023453
Q std: 19.183842
Actor loss: 32.027420
Action reg: 0.003966
  l1.weight: grad_norm = 0.000929
  l1.bias: grad_norm = 0.000639
  l2.weight: grad_norm = 0.003143
Total gradient norm: 0.006619
=== Actor Training Debug (Iteration 689) ===
Q mean: -26.224407
Q std: 17.602581
Actor loss: 26.228348
Action reg: 0.003941
  l1.weight: grad_norm = 0.001274
  l1.bias: grad_norm = 0.001042
  l2.weight: grad_norm = 0.004435
Total gradient norm: 0.010107
=== Actor Training Debug (Iteration 690) ===
Q mean: -28.422329
Q std: 19.617319
Actor loss: 28.426308
Action reg: 0.003978
  l1.weight: grad_norm = 0.000879
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.002203
Total gradient norm: 0.004559
=== Actor Training Debug (Iteration 691) ===
Q mean: -32.381912
Q std: 17.888391
Actor loss: 32.385891
Action reg: 0.003980
  l1.weight: grad_norm = 0.000639
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.001745
Total gradient norm: 0.003710
=== Actor Training Debug (Iteration 692) ===
Q mean: -30.296186
Q std: 20.151001
Actor loss: 30.300142
Action reg: 0.003955
  l1.weight: grad_norm = 0.001021
  l1.bias: grad_norm = 0.000756
  l2.weight: grad_norm = 0.003560
Total gradient norm: 0.007767
=== Actor Training Debug (Iteration 693) ===
Q mean: -29.064472
Q std: 19.539600
Actor loss: 29.068439
Action reg: 0.003967
  l1.weight: grad_norm = 0.027439
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.079449
Total gradient norm: 0.130845
=== Actor Training Debug (Iteration 694) ===
Q mean: -26.590179
Q std: 20.109859
Actor loss: 26.594133
Action reg: 0.003954
  l1.weight: grad_norm = 0.000948
  l1.bias: grad_norm = 0.000730
  l2.weight: grad_norm = 0.003400
Total gradient norm: 0.007509
=== Actor Training Debug (Iteration 695) ===
Q mean: -30.742077
Q std: 19.595121
Actor loss: 30.746019
Action reg: 0.003943
  l1.weight: grad_norm = 0.025822
  l1.bias: grad_norm = 0.000883
  l2.weight: grad_norm = 0.082898
Total gradient norm: 0.126006
=== Actor Training Debug (Iteration 696) ===
Q mean: -29.439287
Q std: 20.400959
Actor loss: 29.443233
Action reg: 0.003946
  l1.weight: grad_norm = 0.006258
  l1.bias: grad_norm = 0.000835
  l2.weight: grad_norm = 0.020999
Total gradient norm: 0.033392
=== Actor Training Debug (Iteration 697) ===
Q mean: -30.954540
Q std: 17.346651
Actor loss: 30.958494
Action reg: 0.003954
  l1.weight: grad_norm = 0.001072
  l1.bias: grad_norm = 0.000811
  l2.weight: grad_norm = 0.003722
Total gradient norm: 0.008167
=== Actor Training Debug (Iteration 698) ===
Q mean: -29.607483
Q std: 20.042881
Actor loss: 29.611471
Action reg: 0.003988
  l1.weight: grad_norm = 0.003486
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.011002
Total gradient norm: 0.016238
=== Actor Training Debug (Iteration 699) ===
Q mean: -31.550093
Q std: 18.800810
Actor loss: 31.554081
Action reg: 0.003989
  l1.weight: grad_norm = 0.017755
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.049482
Total gradient norm: 0.074379
=== Actor Training Debug (Iteration 700) ===
Q mean: -29.123428
Q std: 19.301435
Actor loss: 29.127417
Action reg: 0.003989
  l1.weight: grad_norm = 0.000490
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.001186
Total gradient norm: 0.002249
=== Actor Training Debug (Iteration 701) ===
Q mean: -28.055632
Q std: 18.397408
Actor loss: 28.059631
Action reg: 0.004000
  l1.weight: grad_norm = 0.000617
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001838
Total gradient norm: 0.002852
=== Actor Training Debug (Iteration 702) ===
Q mean: -31.104538
Q std: 20.284651
Actor loss: 31.108501
Action reg: 0.003964
  l1.weight: grad_norm = 0.038847
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.124348
Total gradient norm: 0.187898
=== Actor Training Debug (Iteration 703) ===
Q mean: -30.939297
Q std: 20.596819
Actor loss: 30.943274
Action reg: 0.003977
  l1.weight: grad_norm = 0.036634
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.116478
Total gradient norm: 0.182043
=== Actor Training Debug (Iteration 704) ===
Q mean: -29.786720
Q std: 18.637632
Actor loss: 29.790668
Action reg: 0.003949
  l1.weight: grad_norm = 0.001062
  l1.bias: grad_norm = 0.000744
  l2.weight: grad_norm = 0.003965
Total gradient norm: 0.008953
=== Actor Training Debug (Iteration 705) ===
Q mean: -30.462921
Q std: 18.823387
Actor loss: 30.466909
Action reg: 0.003989
  l1.weight: grad_norm = 0.000523
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.001241
Total gradient norm: 0.002353
=== Actor Training Debug (Iteration 706) ===
Q mean: -32.860031
Q std: 18.029808
Actor loss: 32.864021
Action reg: 0.003990
  l1.weight: grad_norm = 0.001280
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.004166
Total gradient norm: 0.006671
=== Actor Training Debug (Iteration 707) ===
Q mean: -29.299370
Q std: 18.836582
Actor loss: 29.303303
Action reg: 0.003934
  l1.weight: grad_norm = 0.053636
  l1.bias: grad_norm = 0.001170
  l2.weight: grad_norm = 0.153652
Total gradient norm: 0.220393
=== Actor Training Debug (Iteration 708) ===
Q mean: -29.314857
Q std: 19.521330
Actor loss: 29.318827
Action reg: 0.003970
  l1.weight: grad_norm = 0.006123
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.019336
Total gradient norm: 0.030816
=== Actor Training Debug (Iteration 709) ===
Q mean: -27.271130
Q std: 20.041264
Actor loss: 27.275097
Action reg: 0.003967
  l1.weight: grad_norm = 0.001623
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.004612
Total gradient norm: 0.007923
=== Actor Training Debug (Iteration 710) ===
Q mean: -31.557936
Q std: 19.163084
Actor loss: 31.561913
Action reg: 0.003977
  l1.weight: grad_norm = 0.001396
  l1.bias: grad_norm = 0.000378
  l2.weight: grad_norm = 0.003928
Total gradient norm: 0.007068
=== Actor Training Debug (Iteration 711) ===
Q mean: -30.930477
Q std: 20.964384
Actor loss: 30.934433
Action reg: 0.003956
  l1.weight: grad_norm = 0.006225
  l1.bias: grad_norm = 0.000747
  l2.weight: grad_norm = 0.017876
Total gradient norm: 0.026723
=== Actor Training Debug (Iteration 712) ===
Q mean: -28.822603
Q std: 18.702414
Actor loss: 28.826593
Action reg: 0.003991
  l1.weight: grad_norm = 0.008766
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.023038
Total gradient norm: 0.033410
=== Actor Training Debug (Iteration 713) ===
Q mean: -30.754871
Q std: 18.435007
Actor loss: 30.758841
Action reg: 0.003968
  l1.weight: grad_norm = 0.042703
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.143389
Total gradient norm: 0.223367
=== Actor Training Debug (Iteration 714) ===
Q mean: -29.182884
Q std: 20.908514
Actor loss: 29.186840
Action reg: 0.003956
  l1.weight: grad_norm = 0.021576
  l1.bias: grad_norm = 0.000786
  l2.weight: grad_norm = 0.063383
Total gradient norm: 0.096893
=== Actor Training Debug (Iteration 715) ===
Q mean: -29.081596
Q std: 20.043505
Actor loss: 29.085558
Action reg: 0.003962
  l1.weight: grad_norm = 0.001692
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.004935
Total gradient norm: 0.008258
=== Actor Training Debug (Iteration 716) ===
Q mean: -30.305758
Q std: 17.493742
Actor loss: 30.309708
Action reg: 0.003950
  l1.weight: grad_norm = 0.005025
  l1.bias: grad_norm = 0.000808
  l2.weight: grad_norm = 0.014303
Total gradient norm: 0.023526
=== Actor Training Debug (Iteration 717) ===
Q mean: -32.813461
Q std: 18.650066
Actor loss: 32.817421
Action reg: 0.003959
  l1.weight: grad_norm = 0.000945
  l1.bias: grad_norm = 0.000661
  l2.weight: grad_norm = 0.003337
Total gradient norm: 0.007249
=== Actor Training Debug (Iteration 718) ===
Q mean: -28.812803
Q std: 18.546509
Actor loss: 28.816792
Action reg: 0.003988
  l1.weight: grad_norm = 0.021777
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.067797
Total gradient norm: 0.107133
=== Actor Training Debug (Iteration 719) ===
Q mean: -26.562176
Q std: 18.381199
Actor loss: 26.566139
Action reg: 0.003964
  l1.weight: grad_norm = 0.049785
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.120734
Total gradient norm: 0.190078
=== Actor Training Debug (Iteration 720) ===
Q mean: -32.575062
Q std: 20.818693
Actor loss: 32.579063
Action reg: 0.004000
  l1.weight: grad_norm = 0.000003
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000007
Total gradient norm: 0.000012
=== Actor Training Debug (Iteration 721) ===
Q mean: -30.311255
Q std: 20.423355
Actor loss: 30.315210
Action reg: 0.003956
  l1.weight: grad_norm = 0.019317
  l1.bias: grad_norm = 0.000817
  l2.weight: grad_norm = 0.061351
Total gradient norm: 0.093261
=== Actor Training Debug (Iteration 722) ===
Q mean: -27.671602
Q std: 18.714277
Actor loss: 27.675591
Action reg: 0.003988
  l1.weight: grad_norm = 0.075052
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.204756
Total gradient norm: 0.329804
=== Actor Training Debug (Iteration 723) ===
Q mean: -28.300251
Q std: 16.962614
Actor loss: 28.304241
Action reg: 0.003989
  l1.weight: grad_norm = 0.001257
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.003455
Total gradient norm: 0.005197
=== Actor Training Debug (Iteration 724) ===
Q mean: -32.040691
Q std: 18.318497
Actor loss: 32.044659
Action reg: 0.003967
  l1.weight: grad_norm = 0.008501
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.023916
Total gradient norm: 0.039630
=== Actor Training Debug (Iteration 725) ===
Q mean: -33.951508
Q std: 16.296560
Actor loss: 33.955498
Action reg: 0.003990
  l1.weight: grad_norm = 0.000474
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.001057
Total gradient norm: 0.002074
=== Actor Training Debug (Iteration 726) ===
Q mean: -30.924213
Q std: 19.946968
Actor loss: 30.928181
Action reg: 0.003968
  l1.weight: grad_norm = 0.001114
  l1.bias: grad_norm = 0.000612
  l2.weight: grad_norm = 0.002931
Total gradient norm: 0.005901
=== Actor Training Debug (Iteration 727) ===
Q mean: -28.827208
Q std: 18.581432
Actor loss: 28.831207
Action reg: 0.004000
  l1.weight: grad_norm = 0.008428
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.024031
Total gradient norm: 0.040201
=== Actor Training Debug (Iteration 728) ===
Q mean: -29.344236
Q std: 18.840433
Actor loss: 29.348204
Action reg: 0.003967
  l1.weight: grad_norm = 0.014612
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.039252
Total gradient norm: 0.062257
=== Actor Training Debug (Iteration 729) ===
Q mean: -29.435099
Q std: 20.501806
Actor loss: 29.439056
Action reg: 0.003958
  l1.weight: grad_norm = 0.001415
  l1.bias: grad_norm = 0.000639
  l2.weight: grad_norm = 0.004701
Total gradient norm: 0.009268
=== Actor Training Debug (Iteration 730) ===
Q mean: -31.712370
Q std: 20.605757
Actor loss: 31.716351
Action reg: 0.003980
  l1.weight: grad_norm = 0.001652
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.005130
Total gradient norm: 0.008694
=== Actor Training Debug (Iteration 731) ===
Q mean: -29.101101
Q std: 19.670940
Actor loss: 29.105068
Action reg: 0.003967
  l1.weight: grad_norm = 0.031454
  l1.bias: grad_norm = 0.000561
  l2.weight: grad_norm = 0.085415
Total gradient norm: 0.133939
=== Actor Training Debug (Iteration 732) ===
Q mean: -30.979053
Q std: 19.752768
Actor loss: 30.983000
Action reg: 0.003947
  l1.weight: grad_norm = 0.088820
  l1.bias: grad_norm = 0.000715
  l2.weight: grad_norm = 0.260685
Total gradient norm: 0.416148
=== Actor Training Debug (Iteration 733) ===
Q mean: -30.079378
Q std: 18.541929
Actor loss: 30.083338
Action reg: 0.003960
  l1.weight: grad_norm = 0.005311
  l1.bias: grad_norm = 0.000639
  l2.weight: grad_norm = 0.017128
Total gradient norm: 0.028327
=== Actor Training Debug (Iteration 734) ===
Q mean: -30.429129
Q std: 19.801968
Actor loss: 30.433054
Action reg: 0.003925
  l1.weight: grad_norm = 0.002251
  l1.bias: grad_norm = 0.001230
  l2.weight: grad_norm = 0.007066
Total gradient norm: 0.013516
=== Actor Training Debug (Iteration 735) ===
Q mean: -27.839140
Q std: 19.616753
Actor loss: 27.843128
Action reg: 0.003987
  l1.weight: grad_norm = 0.003822
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.009135
Total gradient norm: 0.014167
=== Actor Training Debug (Iteration 736) ===
Q mean: -28.575722
Q std: 18.983395
Actor loss: 28.579691
Action reg: 0.003968
  l1.weight: grad_norm = 0.012637
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.037288
Total gradient norm: 0.053851
=== Actor Training Debug (Iteration 737) ===
Q mean: -31.055176
Q std: 20.476105
Actor loss: 31.059126
Action reg: 0.003950
  l1.weight: grad_norm = 0.078448
  l1.bias: grad_norm = 0.000760
  l2.weight: grad_norm = 0.276714
Total gradient norm: 0.485815
=== Actor Training Debug (Iteration 738) ===
Q mean: -31.884399
Q std: 20.457535
Actor loss: 31.888355
Action reg: 0.003956
  l1.weight: grad_norm = 0.001585
  l1.bias: grad_norm = 0.000653
  l2.weight: grad_norm = 0.004716
Total gradient norm: 0.008999
=== Actor Training Debug (Iteration 739) ===
Q mean: -29.241110
Q std: 17.581917
Actor loss: 29.245100
Action reg: 0.003990
  l1.weight: grad_norm = 0.005873
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.014462
Total gradient norm: 0.024205
=== Actor Training Debug (Iteration 740) ===
Q mean: -28.329580
Q std: 16.552347
Actor loss: 28.333565
Action reg: 0.003984
  l1.weight: grad_norm = 0.000846
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.002264
Total gradient norm: 0.003909
=== Actor Training Debug (Iteration 741) ===
Q mean: -32.754070
Q std: 20.770924
Actor loss: 32.758026
Action reg: 0.003957
  l1.weight: grad_norm = 0.001088
  l1.bias: grad_norm = 0.000735
  l2.weight: grad_norm = 0.004062
Total gradient norm: 0.008546
=== Actor Training Debug (Iteration 742) ===
Q mean: -27.822975
Q std: 16.199314
Actor loss: 27.826921
Action reg: 0.003947
  l1.weight: grad_norm = 0.002364
  l1.bias: grad_norm = 0.000825
  l2.weight: grad_norm = 0.006605
Total gradient norm: 0.011760
=== Actor Training Debug (Iteration 743) ===
Q mean: -27.751553
Q std: 18.453728
Actor loss: 27.755539
Action reg: 0.003987
  l1.weight: grad_norm = 0.008717
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.025510
Total gradient norm: 0.039280
=== Actor Training Debug (Iteration 744) ===
Q mean: -28.864380
Q std: 16.723047
Actor loss: 28.868378
Action reg: 0.003998
  l1.weight: grad_norm = 0.024768
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.077986
Total gradient norm: 0.122080
=== Actor Training Debug (Iteration 745) ===
Q mean: -32.046303
Q std: 18.367317
Actor loss: 32.050282
Action reg: 0.003979
  l1.weight: grad_norm = 0.026831
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.079777
Total gradient norm: 0.125985
=== Actor Training Debug (Iteration 746) ===
Q mean: -30.578773
Q std: 20.209513
Actor loss: 30.582720
Action reg: 0.003946
  l1.weight: grad_norm = 0.020361
  l1.bias: grad_norm = 0.000950
  l2.weight: grad_norm = 0.059399
Total gradient norm: 0.097490
=== Actor Training Debug (Iteration 747) ===
Q mean: -26.331949
Q std: 18.360739
Actor loss: 26.335920
Action reg: 0.003972
  l1.weight: grad_norm = 0.003786
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.012318
Total gradient norm: 0.021812
=== Actor Training Debug (Iteration 748) ===
Q mean: -27.652336
Q std: 17.350063
Actor loss: 27.656282
Action reg: 0.003947
  l1.weight: grad_norm = 0.002118
  l1.bias: grad_norm = 0.000874
  l2.weight: grad_norm = 0.005485
Total gradient norm: 0.010352
=== Actor Training Debug (Iteration 749) ===
Q mean: -30.967640
Q std: 17.772585
Actor loss: 30.971628
Action reg: 0.003988
  l1.weight: grad_norm = 0.004636
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.013813
Total gradient norm: 0.022270
=== Actor Training Debug (Iteration 750) ===
Q mean: -33.952713
Q std: 19.192604
Actor loss: 33.956703
Action reg: 0.003989
  l1.weight: grad_norm = 0.002673
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.007587
Total gradient norm: 0.012596
=== Actor Training Debug (Iteration 751) ===
Q mean: -30.781664
Q std: 19.969641
Actor loss: 30.785635
Action reg: 0.003971
  l1.weight: grad_norm = 0.025457
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.068336
Total gradient norm: 0.089436
=== Actor Training Debug (Iteration 752) ===
Q mean: -28.704344
Q std: 18.806976
Actor loss: 28.708313
Action reg: 0.003968
  l1.weight: grad_norm = 0.000794
  l1.bias: grad_norm = 0.000550
  l2.weight: grad_norm = 0.002511
Total gradient norm: 0.005555
=== Actor Training Debug (Iteration 753) ===
Q mean: -31.532125
Q std: 21.152529
Actor loss: 31.536116
Action reg: 0.003990
  l1.weight: grad_norm = 0.003252
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.010127
Total gradient norm: 0.016471
=== Actor Training Debug (Iteration 754) ===
Q mean: -31.036474
Q std: 17.745716
Actor loss: 31.040472
Action reg: 0.003998
  l1.weight: grad_norm = 0.006194
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.019857
Total gradient norm: 0.032824
=== Actor Training Debug (Iteration 755) ===
Q mean: -29.366287
Q std: 18.663975
Actor loss: 29.370256
Action reg: 0.003968
  l1.weight: grad_norm = 0.000813
  l1.bias: grad_norm = 0.000517
  l2.weight: grad_norm = 0.002716
Total gradient norm: 0.005834
=== Actor Training Debug (Iteration 756) ===
Q mean: -29.584953
Q std: 18.404797
Actor loss: 29.588905
Action reg: 0.003953
  l1.weight: grad_norm = 0.039375
  l1.bias: grad_norm = 0.000754
  l2.weight: grad_norm = 0.120984
Total gradient norm: 0.187981
=== Actor Training Debug (Iteration 757) ===
Q mean: -30.402040
Q std: 16.591206
Actor loss: 30.406031
Action reg: 0.003989
  l1.weight: grad_norm = 0.000467
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.000958
Total gradient norm: 0.001896
=== Actor Training Debug (Iteration 758) ===
Q mean: -32.640015
Q std: 20.243607
Actor loss: 32.643990
Action reg: 0.003976
  l1.weight: grad_norm = 0.003502
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.010961
Total gradient norm: 0.018608
=== Actor Training Debug (Iteration 759) ===
Q mean: -29.603418
Q std: 18.766943
Actor loss: 29.607416
Action reg: 0.003998
  l1.weight: grad_norm = 0.007637
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.022797
Total gradient norm: 0.035955
=== Actor Training Debug (Iteration 760) ===
Q mean: -28.488873
Q std: 17.954706
Actor loss: 28.492861
Action reg: 0.003989
  l1.weight: grad_norm = 0.005884
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.016233
Total gradient norm: 0.025274
=== Actor Training Debug (Iteration 761) ===
Q mean: -27.917236
Q std: 18.082457
Actor loss: 27.921206
Action reg: 0.003969
  l1.weight: grad_norm = 0.001453
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.004026
Total gradient norm: 0.006778
=== Actor Training Debug (Iteration 762) ===
Q mean: -30.336536
Q std: 17.972778
Actor loss: 30.340536
Action reg: 0.004000
  l1.weight: grad_norm = 0.001164
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003734
Total gradient norm: 0.005984
=== Actor Training Debug (Iteration 763) ===
Q mean: -29.360004
Q std: 17.784801
Actor loss: 29.363985
Action reg: 0.003981
  l1.weight: grad_norm = 0.022404
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.062913
Total gradient norm: 0.098290
=== Actor Training Debug (Iteration 764) ===
Q mean: -30.555481
Q std: 21.203093
Actor loss: 30.559458
Action reg: 0.003978
  l1.weight: grad_norm = 0.027825
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.082504
Total gradient norm: 0.119278
=== Actor Training Debug (Iteration 765) ===
Q mean: -34.725105
Q std: 20.918425
Actor loss: 34.729034
Action reg: 0.003928
  l1.weight: grad_norm = 0.001452
  l1.bias: grad_norm = 0.001204
  l2.weight: grad_norm = 0.005292
Total gradient norm: 0.011679
=== Actor Training Debug (Iteration 766) ===
Q mean: -31.897432
Q std: 18.889105
Actor loss: 31.901379
Action reg: 0.003947
  l1.weight: grad_norm = 0.001166
  l1.bias: grad_norm = 0.000975
  l2.weight: grad_norm = 0.004410
Total gradient norm: 0.009619
=== Actor Training Debug (Iteration 767) ===
Q mean: -28.471436
Q std: 18.974928
Actor loss: 28.475395
Action reg: 0.003960
  l1.weight: grad_norm = 0.000969
  l1.bias: grad_norm = 0.000669
  l2.weight: grad_norm = 0.003102
Total gradient norm: 0.006611
=== Actor Training Debug (Iteration 768) ===
Q mean: -28.719936
Q std: 18.231831
Actor loss: 28.723934
Action reg: 0.003998
  l1.weight: grad_norm = 0.006811
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.017148
Total gradient norm: 0.026421
=== Actor Training Debug (Iteration 769) ===
Q mean: -30.688751
Q std: 18.866537
Actor loss: 30.692726
Action reg: 0.003975
  l1.weight: grad_norm = 0.032336
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.078173
Total gradient norm: 0.123503
=== Actor Training Debug (Iteration 770) ===
Q mean: -32.744514
Q std: 19.222820
Actor loss: 32.748474
Action reg: 0.003960
  l1.weight: grad_norm = 0.000991
  l1.bias: grad_norm = 0.000733
  l2.weight: grad_norm = 0.003455
Total gradient norm: 0.007620
=== Actor Training Debug (Iteration 771) ===
Q mean: -30.538101
Q std: 19.528652
Actor loss: 30.542080
Action reg: 0.003980
  l1.weight: grad_norm = 0.000647
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.001901
Total gradient norm: 0.004005
=== Actor Training Debug (Iteration 772) ===
Q mean: -26.769058
Q std: 19.017185
Actor loss: 26.773016
Action reg: 0.003958
  l1.weight: grad_norm = 0.015432
  l1.bias: grad_norm = 0.000693
  l2.weight: grad_norm = 0.042127
Total gradient norm: 0.065854
=== Actor Training Debug (Iteration 773) ===
Q mean: -28.303328
Q std: 20.085251
Actor loss: 28.307283
Action reg: 0.003956
  l1.weight: grad_norm = 0.019797
  l1.bias: grad_norm = 0.000684
  l2.weight: grad_norm = 0.050881
Total gradient norm: 0.068539
=== Actor Training Debug (Iteration 774) ===
Q mean: -29.476452
Q std: 19.866894
Actor loss: 29.480404
Action reg: 0.003952
  l1.weight: grad_norm = 0.015218
  l1.bias: grad_norm = 0.000745
  l2.weight: grad_norm = 0.040136
Total gradient norm: 0.067238
=== Actor Training Debug (Iteration 775) ===
Q mean: -34.919167
Q std: 19.283693
Actor loss: 34.923157
Action reg: 0.003989
  l1.weight: grad_norm = 0.017589
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.045577
Total gradient norm: 0.074567
=== Actor Training Debug (Iteration 776) ===
Q mean: -34.671265
Q std: 19.970551
Actor loss: 34.675220
Action reg: 0.003957
  l1.weight: grad_norm = 0.001093
  l1.bias: grad_norm = 0.000663
  l2.weight: grad_norm = 0.003820
Total gradient norm: 0.008382
=== Actor Training Debug (Iteration 777) ===
Q mean: -31.350256
Q std: 18.795931
Actor loss: 31.354246
Action reg: 0.003989
  l1.weight: grad_norm = 0.005402
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.015754
Total gradient norm: 0.024489
=== Actor Training Debug (Iteration 778) ===
Q mean: -27.240898
Q std: 18.507065
Actor loss: 27.244886
Action reg: 0.003989
  l1.weight: grad_norm = 0.028141
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.074408
Total gradient norm: 0.119104
=== Actor Training Debug (Iteration 779) ===
Q mean: -27.085567
Q std: 16.557089
Actor loss: 27.089546
Action reg: 0.003979
  l1.weight: grad_norm = 0.001425
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.004278
Total gradient norm: 0.007603
=== Actor Training Debug (Iteration 780) ===
Q mean: -26.235897
Q std: 18.057623
Actor loss: 26.239861
Action reg: 0.003964
  l1.weight: grad_norm = 0.001026
  l1.bias: grad_norm = 0.000648
  l2.weight: grad_norm = 0.003210
Total gradient norm: 0.006827
=== Actor Training Debug (Iteration 781) ===
Q mean: -33.236355
Q std: 20.072958
Actor loss: 33.240337
Action reg: 0.003982
  l1.weight: grad_norm = 0.000518
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.001452
Total gradient norm: 0.003044
=== Actor Training Debug (Iteration 782) ===
Q mean: -32.638378
Q std: 19.416058
Actor loss: 32.642357
Action reg: 0.003980
  l1.weight: grad_norm = 0.000533
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.001701
Total gradient norm: 0.003544
=== Actor Training Debug (Iteration 783) ===
Q mean: -33.134441
Q std: 18.980787
Actor loss: 33.138409
Action reg: 0.003968
  l1.weight: grad_norm = 0.000973
  l1.bias: grad_norm = 0.000560
  l2.weight: grad_norm = 0.002744
Total gradient norm: 0.005554
=== Actor Training Debug (Iteration 784) ===
Q mean: -30.799692
Q std: 19.300426
Actor loss: 30.803659
Action reg: 0.003967
  l1.weight: grad_norm = 0.003205
  l1.bias: grad_norm = 0.000571
  l2.weight: grad_norm = 0.008910
Total gradient norm: 0.014470
=== Actor Training Debug (Iteration 785) ===
Q mean: -27.780010
Q std: 17.979631
Actor loss: 27.783989
Action reg: 0.003979
  l1.weight: grad_norm = 0.000724
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.001707
Total gradient norm: 0.003417
=== Actor Training Debug (Iteration 786) ===
Q mean: -28.239290
Q std: 17.975868
Actor loss: 28.243263
Action reg: 0.003973
  l1.weight: grad_norm = 0.000981
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.002656
Total gradient norm: 0.004789
=== Actor Training Debug (Iteration 787) ===
Q mean: -32.113396
Q std: 20.540976
Actor loss: 32.117367
Action reg: 0.003970
  l1.weight: grad_norm = 0.000672
  l1.bias: grad_norm = 0.000524
  l2.weight: grad_norm = 0.002303
Total gradient norm: 0.004997
=== Actor Training Debug (Iteration 788) ===
Q mean: -32.521652
Q std: 18.603449
Actor loss: 32.525639
Action reg: 0.003987
  l1.weight: grad_norm = 0.012127
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.035442
Total gradient norm: 0.056060
=== Actor Training Debug (Iteration 789) ===
Q mean: -29.278767
Q std: 18.688627
Actor loss: 29.282728
Action reg: 0.003962
  l1.weight: grad_norm = 0.005393
  l1.bias: grad_norm = 0.000694
  l2.weight: grad_norm = 0.014947
Total gradient norm: 0.024192
=== Actor Training Debug (Iteration 790) ===
Q mean: -30.598591
Q std: 20.840607
Actor loss: 30.602573
Action reg: 0.003982
  l1.weight: grad_norm = 0.000589
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.001606
Total gradient norm: 0.003394
=== Actor Training Debug (Iteration 791) ===
Q mean: -31.305878
Q std: 19.242994
Actor loss: 31.309847
Action reg: 0.003969
  l1.weight: grad_norm = 0.000718
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.002357
Total gradient norm: 0.005108
=== Actor Training Debug (Iteration 792) ===
Q mean: -30.350574
Q std: 20.104448
Actor loss: 30.354549
Action reg: 0.003976
  l1.weight: grad_norm = 0.019620
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.057708
Total gradient norm: 0.090735
=== Actor Training Debug (Iteration 793) ===
Q mean: -28.251411
Q std: 17.624172
Actor loss: 28.255344
Action reg: 0.003933
  l1.weight: grad_norm = 0.001467
  l1.bias: grad_norm = 0.001120
  l2.weight: grad_norm = 0.004897
Total gradient norm: 0.011144
=== Actor Training Debug (Iteration 794) ===
Q mean: -29.059322
Q std: 15.825533
Actor loss: 29.063305
Action reg: 0.003983
  l1.weight: grad_norm = 0.000674
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.001699
Total gradient norm: 0.003194
=== Actor Training Debug (Iteration 795) ===
Q mean: -32.554474
Q std: 18.107134
Actor loss: 32.558449
Action reg: 0.003974
  l1.weight: grad_norm = 0.000883
  l1.bias: grad_norm = 0.000432
  l2.weight: grad_norm = 0.002743
Total gradient norm: 0.005806
=== Actor Training Debug (Iteration 796) ===
Q mean: -31.362638
Q std: 19.430077
Actor loss: 31.366617
Action reg: 0.003980
  l1.weight: grad_norm = 0.000672
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.001928
Total gradient norm: 0.003883
=== Actor Training Debug (Iteration 797) ===
Q mean: -30.664402
Q std: 18.771406
Actor loss: 30.668375
Action reg: 0.003972
  l1.weight: grad_norm = 0.001073
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.002671
Total gradient norm: 0.005432
=== Actor Training Debug (Iteration 798) ===
Q mean: -28.369469
Q std: 19.354755
Actor loss: 28.373425
Action reg: 0.003956
  l1.weight: grad_norm = 0.013832
  l1.bias: grad_norm = 0.000701
  l2.weight: grad_norm = 0.037444
Total gradient norm: 0.053900
=== Actor Training Debug (Iteration 799) ===
Q mean: -29.569805
Q std: 18.323696
Actor loss: 29.573757
Action reg: 0.003953
  l1.weight: grad_norm = 0.026978
  l1.bias: grad_norm = 0.000753
  l2.weight: grad_norm = 0.078737
Total gradient norm: 0.124018
=== Actor Training Debug (Iteration 800) ===
Q mean: -31.817577
Q std: 16.673824
Actor loss: 31.821569
Action reg: 0.003991
  l1.weight: grad_norm = 0.010028
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.027481
Total gradient norm: 0.044177
=== Actor Training Debug (Iteration 801) ===
Q mean: -32.514984
Q std: 19.486038
Actor loss: 32.518982
Action reg: 0.004000
  l1.weight: grad_norm = 0.004843
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.013004
Total gradient norm: 0.020096
=== Actor Training Debug (Iteration 802) ===
Q mean: -31.187077
Q std: 18.926567
Actor loss: 31.191067
Action reg: 0.003991
  l1.weight: grad_norm = 0.000605
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.001228
Total gradient norm: 0.002101
=== Actor Training Debug (Iteration 803) ===
Q mean: -30.944016
Q std: 21.022532
Actor loss: 30.948015
Action reg: 0.004000
  l1.weight: grad_norm = 0.000297
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000804
Total gradient norm: 0.001175
=== Actor Training Debug (Iteration 804) ===
Q mean: -33.020782
Q std: 18.589027
Actor loss: 33.024761
Action reg: 0.003978
  l1.weight: grad_norm = 0.000783
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.001760
Total gradient norm: 0.003408
=== Actor Training Debug (Iteration 805) ===
Q mean: -33.392731
Q std: 18.569746
Actor loss: 33.396721
Action reg: 0.003989
  l1.weight: grad_norm = 0.027206
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.066526
Total gradient norm: 0.106064
=== Actor Training Debug (Iteration 806) ===
Q mean: -31.135389
Q std: 20.647602
Actor loss: 31.139360
Action reg: 0.003971
  l1.weight: grad_norm = 0.000815
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.002551
Total gradient norm: 0.005197
=== Actor Training Debug (Iteration 807) ===
Q mean: -30.492989
Q std: 19.977682
Actor loss: 30.496950
Action reg: 0.003961
  l1.weight: grad_norm = 0.005196
  l1.bias: grad_norm = 0.000722
  l2.weight: grad_norm = 0.013602
Total gradient norm: 0.023121
=== Actor Training Debug (Iteration 808) ===
Q mean: -27.576698
Q std: 17.773972
Actor loss: 27.580687
Action reg: 0.003989
  l1.weight: grad_norm = 0.001473
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.003704
Total gradient norm: 0.006012
=== Actor Training Debug (Iteration 809) ===
Q mean: -28.899845
Q std: 17.503757
Actor loss: 28.903833
Action reg: 0.003988
  l1.weight: grad_norm = 0.066484
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.203099
Total gradient norm: 0.311454
=== Actor Training Debug (Iteration 810) ===
Q mean: -35.269157
Q std: 18.016077
Actor loss: 35.273132
Action reg: 0.003977
  l1.weight: grad_norm = 0.004583
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.014433
Total gradient norm: 0.022989
=== Actor Training Debug (Iteration 811) ===
Q mean: -31.953350
Q std: 19.144966
Actor loss: 31.957350
Action reg: 0.003999
  l1.weight: grad_norm = 0.010535
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.030457
Total gradient norm: 0.048614
=== Actor Training Debug (Iteration 812) ===
Q mean: -30.287422
Q std: 20.208975
Actor loss: 30.291380
Action reg: 0.003957
  l1.weight: grad_norm = 0.029294
  l1.bias: grad_norm = 0.000713
  l2.weight: grad_norm = 0.084715
Total gradient norm: 0.141382
=== Actor Training Debug (Iteration 813) ===
Q mean: -28.816849
Q std: 19.920523
Actor loss: 28.820774
Action reg: 0.003924
  l1.weight: grad_norm = 0.028393
  l1.bias: grad_norm = 0.001234
  l2.weight: grad_norm = 0.089816
Total gradient norm: 0.134250
=== Actor Training Debug (Iteration 814) ===
Q mean: -33.141464
Q std: 17.393772
Actor loss: 33.145454
Action reg: 0.003989
  l1.weight: grad_norm = 0.004809
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.013085
Total gradient norm: 0.017712
=== Actor Training Debug (Iteration 815) ===
Q mean: -34.403397
Q std: 18.613688
Actor loss: 34.407356
Action reg: 0.003961
  l1.weight: grad_norm = 0.012018
  l1.bias: grad_norm = 0.000668
  l2.weight: grad_norm = 0.030829
Total gradient norm: 0.047404
=== Actor Training Debug (Iteration 816) ===
Q mean: -29.561020
Q std: 19.068300
Actor loss: 29.565008
Action reg: 0.003989
  l1.weight: grad_norm = 0.000831
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.002358
Total gradient norm: 0.003716
=== Actor Training Debug (Iteration 817) ===
Q mean: -28.244537
Q std: 19.055279
Actor loss: 28.248493
Action reg: 0.003956
  l1.weight: grad_norm = 0.039533
  l1.bias: grad_norm = 0.000711
  l2.weight: grad_norm = 0.113024
Total gradient norm: 0.148969
=== Actor Training Debug (Iteration 818) ===
Q mean: -28.301979
Q std: 21.951763
Actor loss: 28.305920
Action reg: 0.003940
  l1.weight: grad_norm = 0.039096
  l1.bias: grad_norm = 0.001004
  l2.weight: grad_norm = 0.122605
Total gradient norm: 0.201253
=== Actor Training Debug (Iteration 819) ===
Q mean: -33.374458
Q std: 19.316362
Actor loss: 33.378429
Action reg: 0.003973
  l1.weight: grad_norm = 0.004588
  l1.bias: grad_norm = 0.000481
  l2.weight: grad_norm = 0.014201
Total gradient norm: 0.021134
=== Actor Training Debug (Iteration 820) ===
Q mean: -33.245750
Q std: 18.899118
Actor loss: 33.249729
Action reg: 0.003980
  l1.weight: grad_norm = 0.001187
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.002904
Total gradient norm: 0.004658
=== Actor Training Debug (Iteration 821) ===
Q mean: -30.654587
Q std: 19.682545
Actor loss: 30.658550
Action reg: 0.003963
  l1.weight: grad_norm = 0.000891
  l1.bias: grad_norm = 0.000655
  l2.weight: grad_norm = 0.003065
Total gradient norm: 0.006598
=== Actor Training Debug (Iteration 822) ===
Q mean: -27.852526
Q std: 17.072241
Actor loss: 27.856508
Action reg: 0.003982
  l1.weight: grad_norm = 0.007746
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.022644
Total gradient norm: 0.039971
=== Actor Training Debug (Iteration 823) ===
Q mean: -30.697277
Q std: 18.421011
Actor loss: 30.701244
Action reg: 0.003966
  l1.weight: grad_norm = 0.019252
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.052943
Total gradient norm: 0.085579
=== Actor Training Debug (Iteration 824) ===
Q mean: -32.423172
Q std: 19.922878
Actor loss: 32.427139
Action reg: 0.003968
  l1.weight: grad_norm = 0.036859
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.097495
Total gradient norm: 0.151789
=== Actor Training Debug (Iteration 825) ===
Q mean: -31.699987
Q std: 19.832731
Actor loss: 31.703964
Action reg: 0.003977
  l1.weight: grad_norm = 0.012570
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.034987
Total gradient norm: 0.048269
=== Actor Training Debug (Iteration 826) ===
Q mean: -28.561203
Q std: 21.620409
Actor loss: 28.565170
Action reg: 0.003967
  l1.weight: grad_norm = 0.044757
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.142516
Total gradient norm: 0.240018
=== Actor Training Debug (Iteration 827) ===
Q mean: -31.709843
Q std: 19.155716
Actor loss: 31.713804
Action reg: 0.003962
  l1.weight: grad_norm = 0.006101
  l1.bias: grad_norm = 0.000670
  l2.weight: grad_norm = 0.018677
Total gradient norm: 0.030576
=== Actor Training Debug (Iteration 828) ===
Q mean: -33.067883
Q std: 19.030851
Actor loss: 33.071873
Action reg: 0.003991
  l1.weight: grad_norm = 0.022884
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.074870
Total gradient norm: 0.121617
=== Actor Training Debug (Iteration 829) ===
Q mean: -32.587193
Q std: 18.902092
Actor loss: 32.591183
Action reg: 0.003989
  l1.weight: grad_norm = 0.011949
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.033851
Total gradient norm: 0.051412
=== Actor Training Debug (Iteration 830) ===
Q mean: -27.160105
Q std: 18.732462
Actor loss: 27.164043
Action reg: 0.003939
  l1.weight: grad_norm = 0.024464
  l1.bias: grad_norm = 0.000953
  l2.weight: grad_norm = 0.068250
Total gradient norm: 0.097949
=== Actor Training Debug (Iteration 831) ===
Q mean: -29.514309
Q std: 18.458946
Actor loss: 29.518253
Action reg: 0.003945
  l1.weight: grad_norm = 0.010485
  l1.bias: grad_norm = 0.000900
  l2.weight: grad_norm = 0.031202
Total gradient norm: 0.047530
=== Actor Training Debug (Iteration 832) ===
Q mean: -32.562508
Q std: 19.161537
Actor loss: 32.566475
Action reg: 0.003969
  l1.weight: grad_norm = 0.019033
  l1.bias: grad_norm = 0.000582
  l2.weight: grad_norm = 0.053846
Total gradient norm: 0.084633
=== Actor Training Debug (Iteration 833) ===
Q mean: -30.395517
Q std: 18.303070
Actor loss: 30.399508
Action reg: 0.003990
  l1.weight: grad_norm = 0.041110
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.139906
Total gradient norm: 0.219959
=== Actor Training Debug (Iteration 834) ===
Q mean: -29.400631
Q std: 18.634966
Actor loss: 29.404619
Action reg: 0.003988
  l1.weight: grad_norm = 0.059115
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.151953
Total gradient norm: 0.254785
=== Actor Training Debug (Iteration 835) ===
Q mean: -30.133476
Q std: 20.124660
Actor loss: 30.137428
Action reg: 0.003952
  l1.weight: grad_norm = 0.014437
  l1.bias: grad_norm = 0.000799
  l2.weight: grad_norm = 0.045960
Total gradient norm: 0.072174
=== Actor Training Debug (Iteration 836) ===
Q mean: -32.311352
Q std: 20.065155
Actor loss: 32.315304
Action reg: 0.003951
  l1.weight: grad_norm = 0.001534
  l1.bias: grad_norm = 0.000900
  l2.weight: grad_norm = 0.004329
Total gradient norm: 0.008509
=== Actor Training Debug (Iteration 837) ===
Q mean: -36.034409
Q std: 19.042349
Actor loss: 36.038399
Action reg: 0.003990
  l1.weight: grad_norm = 0.000728
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.001685
Total gradient norm: 0.002712
=== Actor Training Debug (Iteration 838) ===
Q mean: -32.119129
Q std: 18.074381
Actor loss: 32.123112
Action reg: 0.003984
  l1.weight: grad_norm = 0.003642
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.009174
Total gradient norm: 0.015450
=== Actor Training Debug (Iteration 839) ===
Q mean: -32.066036
Q std: 20.148129
Actor loss: 32.070026
Action reg: 0.003991
  l1.weight: grad_norm = 0.000393
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.000970
Total gradient norm: 0.001869
=== Actor Training Debug (Iteration 840) ===
Q mean: -30.854860
Q std: 20.382559
Actor loss: 30.858839
Action reg: 0.003978
  l1.weight: grad_norm = 0.006011
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.018302
Total gradient norm: 0.027097
=== Actor Training Debug (Iteration 841) ===
Q mean: -31.607258
Q std: 18.757374
Actor loss: 31.611250
Action reg: 0.003991
  l1.weight: grad_norm = 0.001211
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.002968
Total gradient norm: 0.004394
=== Actor Training Debug (Iteration 842) ===
Q mean: -34.348282
Q std: 17.532064
Actor loss: 34.352245
Action reg: 0.003965
  l1.weight: grad_norm = 0.000638
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.002374
Total gradient norm: 0.005348
=== Actor Training Debug (Iteration 843) ===
Q mean: -33.435909
Q std: 18.539062
Actor loss: 33.439884
Action reg: 0.003974
  l1.weight: grad_norm = 0.001027
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.002385
Total gradient norm: 0.004590
=== Actor Training Debug (Iteration 844) ===
Q mean: -28.810261
Q std: 18.334612
Actor loss: 28.814234
Action reg: 0.003972
  l1.weight: grad_norm = 0.000788
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.002288
Total gradient norm: 0.004737
=== Actor Training Debug (Iteration 845) ===
Q mean: -31.006989
Q std: 21.040287
Actor loss: 31.010971
Action reg: 0.003982
  l1.weight: grad_norm = 0.007372
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.019299
Total gradient norm: 0.032005
=== Actor Training Debug (Iteration 846) ===
Q mean: -31.675318
Q std: 18.772299
Actor loss: 31.679300
Action reg: 0.003983
  l1.weight: grad_norm = 0.012099
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.040851
Total gradient norm: 0.064108
=== Actor Training Debug (Iteration 847) ===
Q mean: -33.354889
Q std: 21.014746
Actor loss: 33.358852
Action reg: 0.003965
  l1.weight: grad_norm = 0.025848
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.063128
Total gradient norm: 0.100599
=== Actor Training Debug (Iteration 848) ===
Q mean: -30.088425
Q std: 20.711143
Actor loss: 30.092405
Action reg: 0.003982
  l1.weight: grad_norm = 0.014159
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.044734
Total gradient norm: 0.076775
=== Actor Training Debug (Iteration 849) ===
Q mean: -29.741711
Q std: 18.170376
Actor loss: 29.745699
Action reg: 0.003989
  l1.weight: grad_norm = 0.136662
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.356338
Total gradient norm: 0.561328
=== Actor Training Debug (Iteration 850) ===
Q mean: -34.116749
Q std: 20.236397
Actor loss: 34.120712
Action reg: 0.003963
  l1.weight: grad_norm = 0.001420
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.003732
Total gradient norm: 0.006545
=== Actor Training Debug (Iteration 851) ===
Q mean: -34.080978
Q std: 21.477297
Actor loss: 34.084957
Action reg: 0.003980
  l1.weight: grad_norm = 0.012445
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.036000
Total gradient norm: 0.057188
=== Actor Training Debug (Iteration 852) ===
Q mean: -33.292107
Q std: 18.916929
Actor loss: 33.296097
Action reg: 0.003990
  l1.weight: grad_norm = 0.001822
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.005231
Total gradient norm: 0.008210
=== Actor Training Debug (Iteration 853) ===
Q mean: -30.202250
Q std: 18.634554
Actor loss: 30.206213
Action reg: 0.003963
  l1.weight: grad_norm = 0.009149
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.028989
Total gradient norm: 0.043455
=== Actor Training Debug (Iteration 854) ===
Q mean: -30.879665
Q std: 19.878233
Actor loss: 30.883656
Action reg: 0.003991
  l1.weight: grad_norm = 0.056826
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.177451
Total gradient norm: 0.272217
=== Actor Training Debug (Iteration 855) ===
Q mean: -31.564508
Q std: 20.171371
Actor loss: 31.568470
Action reg: 0.003962
  l1.weight: grad_norm = 0.042278
  l1.bias: grad_norm = 0.000560
  l2.weight: grad_norm = 0.130237
Total gradient norm: 0.214729
=== Actor Training Debug (Iteration 856) ===
Q mean: -31.609509
Q std: 19.251442
Actor loss: 31.613472
Action reg: 0.003964
  l1.weight: grad_norm = 0.029725
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.092207
Total gradient norm: 0.144994
=== Actor Training Debug (Iteration 857) ===
Q mean: -28.469543
Q std: 20.158794
Actor loss: 28.473499
Action reg: 0.003957
  l1.weight: grad_norm = 0.005079
  l1.bias: grad_norm = 0.000674
  l2.weight: grad_norm = 0.012671
Total gradient norm: 0.018457
=== Actor Training Debug (Iteration 858) ===
Q mean: -30.539886
Q std: 20.892054
Actor loss: 30.543858
Action reg: 0.003972
  l1.weight: grad_norm = 0.017960
  l1.bias: grad_norm = 0.000548
  l2.weight: grad_norm = 0.049197
Total gradient norm: 0.078717
=== Actor Training Debug (Iteration 859) ===
Q mean: -35.271648
Q std: 18.053614
Actor loss: 35.275631
Action reg: 0.003982
  l1.weight: grad_norm = 0.000913
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.002337
Total gradient norm: 0.003911
=== Actor Training Debug (Iteration 860) ===
Q mean: -30.809252
Q std: 18.037977
Actor loss: 30.813229
Action reg: 0.003977
  l1.weight: grad_norm = 0.064007
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.187883
Total gradient norm: 0.308998
=== Actor Training Debug (Iteration 861) ===
Q mean: -28.371092
Q std: 16.906548
Actor loss: 28.375090
Action reg: 0.003998
  l1.weight: grad_norm = 0.020200
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.058526
Total gradient norm: 0.095453
=== Actor Training Debug (Iteration 862) ===
Q mean: -26.807886
Q std: 15.515826
Actor loss: 26.811865
Action reg: 0.003978
  l1.weight: grad_norm = 0.103312
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.301453
Total gradient norm: 0.522818
=== Actor Training Debug (Iteration 863) ===
Q mean: -33.943760
Q std: 19.117672
Actor loss: 33.947739
Action reg: 0.003978
  l1.weight: grad_norm = 0.020682
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.050718
Total gradient norm: 0.076615
=== Actor Training Debug (Iteration 864) ===
Q mean: -34.274139
Q std: 18.635279
Actor loss: 34.278111
Action reg: 0.003972
  l1.weight: grad_norm = 0.007933
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.024749
Total gradient norm: 0.044500
=== Actor Training Debug (Iteration 865) ===
Q mean: -31.723881
Q std: 17.996180
Actor loss: 31.727835
Action reg: 0.003954
  l1.weight: grad_norm = 0.018761
  l1.bias: grad_norm = 0.000744
  l2.weight: grad_norm = 0.054440
Total gradient norm: 0.086067
=== Actor Training Debug (Iteration 866) ===
Q mean: -28.185387
Q std: 19.911488
Actor loss: 28.189356
Action reg: 0.003970
  l1.weight: grad_norm = 0.066081
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.205268
Total gradient norm: 0.329313
=== Actor Training Debug (Iteration 867) ===
Q mean: -27.038416
Q std: 17.607935
Actor loss: 27.042393
Action reg: 0.003978
  l1.weight: grad_norm = 0.127636
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.388173
Total gradient norm: 0.675271
=== Actor Training Debug (Iteration 868) ===
Q mean: -29.342108
Q std: 18.518898
Actor loss: 29.346094
Action reg: 0.003987
  l1.weight: grad_norm = 0.000945
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.002885
Total gradient norm: 0.004822
=== Actor Training Debug (Iteration 869) ===
Q mean: -34.107338
Q std: 16.680449
Actor loss: 34.111328
Action reg: 0.003988
  l1.weight: grad_norm = 0.021835
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.069486
Total gradient norm: 0.120921
=== Actor Training Debug (Iteration 870) ===
Q mean: -39.010994
Q std: 19.784344
Actor loss: 39.014992
Action reg: 0.004000
  l1.weight: grad_norm = 0.014188
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.039995
Total gradient norm: 0.068098
=== Actor Training Debug (Iteration 871) ===
Q mean: -35.045376
Q std: 18.948786
Actor loss: 35.049362
Action reg: 0.003986
  l1.weight: grad_norm = 0.009083
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.024341
Total gradient norm: 0.036102
=== Actor Training Debug (Iteration 872) ===
Q mean: -30.528368
Q std: 18.498077
Actor loss: 30.532341
Action reg: 0.003974
  l1.weight: grad_norm = 0.000637
  l1.bias: grad_norm = 0.000454
  l2.weight: grad_norm = 0.002026
Total gradient norm: 0.004435
=== Actor Training Debug (Iteration 873) ===
Q mean: -27.299297
Q std: 19.021822
Actor loss: 27.303278
Action reg: 0.003980
  l1.weight: grad_norm = 0.022041
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.056709
Total gradient norm: 0.090896
=== Actor Training Debug (Iteration 874) ===
Q mean: -30.178577
Q std: 18.804853
Actor loss: 30.182570
Action reg: 0.003991
  l1.weight: grad_norm = 0.023425
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.063956
Total gradient norm: 0.104409
=== Actor Training Debug (Iteration 875) ===
Q mean: -33.771721
Q std: 18.685991
Actor loss: 33.775711
Action reg: 0.003990
  l1.weight: grad_norm = 0.059364
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.147761
Total gradient norm: 0.242818
=== Actor Training Debug (Iteration 876) ===
Q mean: -33.611374
Q std: 18.910702
Actor loss: 33.615356
Action reg: 0.003981
  l1.weight: grad_norm = 0.026497
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.070740
Total gradient norm: 0.111678
=== Actor Training Debug (Iteration 877) ===
Q mean: -29.421749
Q std: 18.029058
Actor loss: 29.425732
Action reg: 0.003983
  l1.weight: grad_norm = 0.123040
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.329907
Total gradient norm: 0.569727
=== Actor Training Debug (Iteration 878) ===
Q mean: -32.890125
Q std: 18.967058
Actor loss: 32.894112
Action reg: 0.003987
  l1.weight: grad_norm = 0.000513
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.001216
Total gradient norm: 0.002464
=== Actor Training Debug (Iteration 879) ===
Q mean: -30.038616
Q std: 17.752420
Actor loss: 30.042597
Action reg: 0.003980
  l1.weight: grad_norm = 0.038348
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.092911
Total gradient norm: 0.132998
=== Actor Training Debug (Iteration 880) ===
Q mean: -33.345547
Q std: 18.887354
Actor loss: 33.349518
Action reg: 0.003972
  l1.weight: grad_norm = 0.012713
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.030431
Total gradient norm: 0.045569
=== Actor Training Debug (Iteration 881) ===
Q mean: -34.767384
Q std: 20.060762
Actor loss: 34.771374
Action reg: 0.003990
  l1.weight: grad_norm = 0.028459
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.070913
Total gradient norm: 0.115072
=== Actor Training Debug (Iteration 882) ===
Q mean: -31.312017
Q std: 21.315855
Actor loss: 31.315975
Action reg: 0.003958
  l1.weight: grad_norm = 0.012653
  l1.bias: grad_norm = 0.000681
  l2.weight: grad_norm = 0.032978
Total gradient norm: 0.055021
=== Actor Training Debug (Iteration 883) ===
Q mean: -32.180229
Q std: 18.169285
Actor loss: 32.184219
Action reg: 0.003990
  l1.weight: grad_norm = 0.008357
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.023262
Total gradient norm: 0.037346
=== Actor Training Debug (Iteration 884) ===
Q mean: -29.665989
Q std: 18.488268
Actor loss: 29.669971
Action reg: 0.003983
  l1.weight: grad_norm = 0.029210
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.081738
Total gradient norm: 0.136289
=== Actor Training Debug (Iteration 885) ===
Q mean: -32.054657
Q std: 16.865526
Actor loss: 32.058655
Action reg: 0.003998
  l1.weight: grad_norm = 0.040651
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.120361
Total gradient norm: 0.194093
=== Actor Training Debug (Iteration 886) ===
Q mean: -34.407135
Q std: 19.682833
Actor loss: 34.411095
Action reg: 0.003959
  l1.weight: grad_norm = 0.010085
  l1.bias: grad_norm = 0.000646
  l2.weight: grad_norm = 0.029407
Total gradient norm: 0.046824
=== Actor Training Debug (Iteration 887) ===
Q mean: -35.597794
Q std: 17.176594
Actor loss: 35.601768
Action reg: 0.003975
  l1.weight: grad_norm = 0.003150
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.010184
Total gradient norm: 0.018307
=== Actor Training Debug (Iteration 888) ===
Q mean: -30.402435
Q std: 20.723843
Actor loss: 30.406410
Action reg: 0.003975
  l1.weight: grad_norm = 0.001160
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.003508
Total gradient norm: 0.007223
=== Actor Training Debug (Iteration 889) ===
Q mean: -29.137833
Q std: 19.734774
Actor loss: 29.141819
Action reg: 0.003987
  l1.weight: grad_norm = 0.027592
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.078142
Total gradient norm: 0.142894
=== Actor Training Debug (Iteration 890) ===
Q mean: -28.947845
Q std: 19.571833
Actor loss: 28.951826
Action reg: 0.003981
  l1.weight: grad_norm = 0.001735
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.005447
Total gradient norm: 0.010081
=== Actor Training Debug (Iteration 891) ===
Q mean: -31.679068
Q std: 19.493649
Actor loss: 31.683064
Action reg: 0.003996
  l1.weight: grad_norm = 0.025669
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.068863
Total gradient norm: 0.110250
=== Actor Training Debug (Iteration 892) ===
Q mean: -33.343307
Q std: 16.987215
Actor loss: 33.347298
Action reg: 0.003990
  l1.weight: grad_norm = 0.006116
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.019647
Total gradient norm: 0.035324
=== Actor Training Debug (Iteration 893) ===
Q mean: -32.552830
Q std: 19.336535
Actor loss: 32.556812
Action reg: 0.003984
  l1.weight: grad_norm = 0.000497
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.001223
Total gradient norm: 0.002549
=== Actor Training Debug (Iteration 894) ===
Q mean: -29.864325
Q std: 16.232300
Actor loss: 29.868307
Action reg: 0.003983
  l1.weight: grad_norm = 0.001044
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.002423
Total gradient norm: 0.004008
=== Actor Training Debug (Iteration 895) ===
Q mean: -33.929722
Q std: 18.691185
Actor loss: 33.933712
Action reg: 0.003990
  l1.weight: grad_norm = 0.022267
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.068477
Total gradient norm: 0.111708
=== Actor Training Debug (Iteration 896) ===
Q mean: -29.393423
Q std: 18.658369
Actor loss: 29.397394
Action reg: 0.003972
  l1.weight: grad_norm = 0.009207
  l1.bias: grad_norm = 0.000518
  l2.weight: grad_norm = 0.028235
Total gradient norm: 0.046593
=== Actor Training Debug (Iteration 897) ===
Q mean: -30.667589
Q std: 19.231550
Actor loss: 30.671572
Action reg: 0.003983
  l1.weight: grad_norm = 0.021291
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.059185
Total gradient norm: 0.090891
=== Actor Training Debug (Iteration 898) ===
Q mean: -31.706585
Q std: 18.699448
Actor loss: 31.710577
Action reg: 0.003992
  l1.weight: grad_norm = 0.000399
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.000861
Total gradient norm: 0.001752
=== Actor Training Debug (Iteration 899) ===
Q mean: -33.046879
Q std: 15.419273
Actor loss: 33.050861
Action reg: 0.003984
  l1.weight: grad_norm = 0.000353
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.001124
Total gradient norm: 0.002589
=== Actor Training Debug (Iteration 900) ===
Q mean: -33.483208
Q std: 20.127522
Actor loss: 33.487167
Action reg: 0.003961
  l1.weight: grad_norm = 0.043937
  l1.bias: grad_norm = 0.000618
  l2.weight: grad_norm = 0.115074
Total gradient norm: 0.178469
=== Actor Training Debug (Iteration 901) ===
Q mean: -32.146370
Q std: 18.688198
Actor loss: 32.150337
Action reg: 0.003969
  l1.weight: grad_norm = 0.047888
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.126455
Total gradient norm: 0.201084
=== Actor Training Debug (Iteration 902) ===
Q mean: -29.363844
Q std: 17.582407
Actor loss: 29.367828
Action reg: 0.003984
  l1.weight: grad_norm = 0.008690
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.025507
Total gradient norm: 0.042432
=== Actor Training Debug (Iteration 903) ===
Q mean: -28.976414
Q std: 18.283146
Actor loss: 28.980413
Action reg: 0.003999
  l1.weight: grad_norm = 0.019078
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.049668
Total gradient norm: 0.077896
=== Actor Training Debug (Iteration 904) ===
Q mean: -30.764999
Q std: 19.556753
Actor loss: 30.768980
Action reg: 0.003980
  l1.weight: grad_norm = 0.031638
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.075402
Total gradient norm: 0.118594
=== Actor Training Debug (Iteration 905) ===
Q mean: -33.749397
Q std: 19.346140
Actor loss: 33.753395
Action reg: 0.003999
  l1.weight: grad_norm = 0.038627
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.098461
Total gradient norm: 0.155083
=== Actor Training Debug (Iteration 906) ===
Q mean: -30.745743
Q std: 18.345716
Actor loss: 30.749733
Action reg: 0.003990
  l1.weight: grad_norm = 0.020701
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.053582
Total gradient norm: 0.076248
=== Actor Training Debug (Iteration 907) ===
Q mean: -30.816311
Q std: 17.152958
Actor loss: 30.820272
Action reg: 0.003962
  l1.weight: grad_norm = 0.002886
  l1.bias: grad_norm = 0.000602
  l2.weight: grad_norm = 0.008286
Total gradient norm: 0.012948
=== Actor Training Debug (Iteration 908) ===
Q mean: -32.992718
Q std: 18.878998
Actor loss: 32.996708
Action reg: 0.003989
  l1.weight: grad_norm = 0.030440
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.077007
Total gradient norm: 0.117572
=== Actor Training Debug (Iteration 909) ===
Q mean: -36.448700
Q std: 21.968197
Actor loss: 36.452682
Action reg: 0.003981
  l1.weight: grad_norm = 0.011464
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.035730
Total gradient norm: 0.057437
=== Actor Training Debug (Iteration 910) ===
Q mean: -29.237652
Q std: 19.090956
Actor loss: 29.241627
Action reg: 0.003975
  l1.weight: grad_norm = 0.001296
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.003592
Total gradient norm: 0.007181
=== Actor Training Debug (Iteration 911) ===
Q mean: -29.435547
Q std: 20.653959
Actor loss: 29.439522
Action reg: 0.003974
  l1.weight: grad_norm = 0.000602
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.001953
Total gradient norm: 0.004382
=== Actor Training Debug (Iteration 912) ===
Q mean: -31.485054
Q std: 21.030529
Actor loss: 31.488981
Action reg: 0.003926
  l1.weight: grad_norm = 0.060020
  l1.bias: grad_norm = 0.000961
  l2.weight: grad_norm = 0.191381
Total gradient norm: 0.341909
=== Actor Training Debug (Iteration 913) ===
Q mean: -35.820911
Q std: 20.741978
Actor loss: 35.824883
Action reg: 0.003972
  l1.weight: grad_norm = 0.000781
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.001949
Total gradient norm: 0.004248
=== Actor Training Debug (Iteration 914) ===
Q mean: -31.143093
Q std: 18.156237
Actor loss: 31.147083
Action reg: 0.003991
  l1.weight: grad_norm = 0.004314
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.011231
Total gradient norm: 0.018836
=== Actor Training Debug (Iteration 915) ===
Q mean: -33.481140
Q std: 18.020128
Actor loss: 33.485107
Action reg: 0.003966
  l1.weight: grad_norm = 0.023621
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.071115
Total gradient norm: 0.116491
=== Actor Training Debug (Iteration 916) ===
Q mean: -29.928232
Q std: 18.595974
Actor loss: 29.932192
Action reg: 0.003959
  l1.weight: grad_norm = 0.001224
  l1.bias: grad_norm = 0.000688
  l2.weight: grad_norm = 0.003371
Total gradient norm: 0.006600
=== Actor Training Debug (Iteration 917) ===
Q mean: -32.747318
Q std: 20.098804
Actor loss: 32.751263
Action reg: 0.003945
  l1.weight: grad_norm = 0.002194
  l1.bias: grad_norm = 0.000836
  l2.weight: grad_norm = 0.005828
Total gradient norm: 0.010096
=== Actor Training Debug (Iteration 918) ===
Q mean: -29.121140
Q std: 19.218143
Actor loss: 29.125113
Action reg: 0.003972
  l1.weight: grad_norm = 0.052358
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.116685
Total gradient norm: 0.171541
=== Actor Training Debug (Iteration 919) ===
Q mean: -30.774437
Q std: 17.027456
Actor loss: 30.778416
Action reg: 0.003979
  l1.weight: grad_norm = 0.000718
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.001933
Total gradient norm: 0.004121
=== Actor Training Debug (Iteration 920) ===
Q mean: -33.367634
Q std: 18.580687
Actor loss: 33.371601
Action reg: 0.003966
  l1.weight: grad_norm = 0.061442
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.144563
Total gradient norm: 0.202107
=== Actor Training Debug (Iteration 921) ===
Q mean: -34.038643
Q std: 20.415972
Actor loss: 34.042622
Action reg: 0.003980
  l1.weight: grad_norm = 0.022838
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.061037
Total gradient norm: 0.101512
=== Actor Training Debug (Iteration 922) ===
Q mean: -32.203808
Q std: 16.907604
Actor loss: 32.207794
Action reg: 0.003987
  l1.weight: grad_norm = 0.024523
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.072628
Total gradient norm: 0.128421
=== Actor Training Debug (Iteration 923) ===
Q mean: -30.839813
Q std: 18.297672
Actor loss: 30.843800
Action reg: 0.003986
  l1.weight: grad_norm = 0.039648
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.113385
Total gradient norm: 0.187875
=== Actor Training Debug (Iteration 924) ===
Q mean: -32.857292
Q std: 18.876514
Actor loss: 32.861282
Action reg: 0.003992
  l1.weight: grad_norm = 0.007321
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.019968
Total gradient norm: 0.033502
=== Actor Training Debug (Iteration 925) ===
Q mean: -32.109894
Q std: 18.812159
Actor loss: 32.113865
Action reg: 0.003972
  l1.weight: grad_norm = 0.104423
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.312151
Total gradient norm: 0.497572
=== Actor Training Debug (Iteration 926) ===
Q mean: -32.613697
Q std: 19.126907
Actor loss: 32.617680
Action reg: 0.003984
  l1.weight: grad_norm = 0.000967
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.003101
Total gradient norm: 0.005652
=== Actor Training Debug (Iteration 927) ===
Q mean: -30.734793
Q std: 17.275900
Actor loss: 30.738733
Action reg: 0.003940
  l1.weight: grad_norm = 0.000881
  l1.bias: grad_norm = 0.000921
  l2.weight: grad_norm = 0.003724
Total gradient norm: 0.008569
=== Actor Training Debug (Iteration 928) ===
Q mean: -30.406940
Q std: 17.896145
Actor loss: 30.410925
Action reg: 0.003985
  l1.weight: grad_norm = 0.000768
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.002088
Total gradient norm: 0.003527
=== Actor Training Debug (Iteration 929) ===
Q mean: -29.263018
Q std: 16.974428
Actor loss: 29.266987
Action reg: 0.003970
  l1.weight: grad_norm = 0.026656
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.075813
Total gradient norm: 0.134780
=== Actor Training Debug (Iteration 930) ===
Q mean: -31.899717
Q std: 18.115973
Actor loss: 31.903709
Action reg: 0.003992
  l1.weight: grad_norm = 0.027895
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.068218
Total gradient norm: 0.087107
=== Actor Training Debug (Iteration 931) ===
Q mean: -33.138515
Q std: 19.075541
Actor loss: 33.142490
Action reg: 0.003975
  l1.weight: grad_norm = 0.022837
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.055128
Total gradient norm: 0.087179
=== Actor Training Debug (Iteration 932) ===
Q mean: -30.731976
Q std: 19.612902
Actor loss: 30.735949
Action reg: 0.003972
  l1.weight: grad_norm = 0.039638
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.104379
Total gradient norm: 0.170660
=== Actor Training Debug (Iteration 933) ===
Q mean: -31.197453
Q std: 18.991983
Actor loss: 31.201427
Action reg: 0.003974
  l1.weight: grad_norm = 0.001760
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.004534
Total gradient norm: 0.007429
=== Actor Training Debug (Iteration 934) ===
Q mean: -30.844685
Q std: 17.840082
Actor loss: 30.848663
Action reg: 0.003979
  l1.weight: grad_norm = 0.043878
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.109902
Total gradient norm: 0.179128
=== Actor Training Debug (Iteration 935) ===
Q mean: -34.708481
Q std: 16.458624
Actor loss: 34.712479
Action reg: 0.003996
  l1.weight: grad_norm = 0.046665
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.115574
Total gradient norm: 0.176765
=== Actor Training Debug (Iteration 936) ===
Q mean: -33.784607
Q std: 18.063288
Actor loss: 33.788582
Action reg: 0.003977
  l1.weight: grad_norm = 0.030366
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.081568
Total gradient norm: 0.117805
=== Actor Training Debug (Iteration 937) ===
Q mean: -33.754463
Q std: 15.815337
Actor loss: 33.758453
Action reg: 0.003989
  l1.weight: grad_norm = 0.033169
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.089589
Total gradient norm: 0.143924
=== Actor Training Debug (Iteration 938) ===
Q mean: -29.936337
Q std: 17.949427
Actor loss: 29.940319
Action reg: 0.003982
  l1.weight: grad_norm = 0.015508
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.039955
Total gradient norm: 0.065829
=== Actor Training Debug (Iteration 939) ===
Q mean: -30.845795
Q std: 17.402264
Actor loss: 30.849771
Action reg: 0.003977
  l1.weight: grad_norm = 0.000739
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.002163
Total gradient norm: 0.004474
=== Actor Training Debug (Iteration 940) ===
Q mean: -34.193699
Q std: 18.013176
Actor loss: 34.197681
Action reg: 0.003983
  l1.weight: grad_norm = 0.054222
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.129793
Total gradient norm: 0.170873
=== Actor Training Debug (Iteration 941) ===
Q mean: -34.712849
Q std: 18.336422
Actor loss: 34.716835
Action reg: 0.003985
  l1.weight: grad_norm = 0.079244
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.210616
Total gradient norm: 0.327926
=== Actor Training Debug (Iteration 942) ===
Q mean: -33.258801
Q std: 17.254349
Actor loss: 33.262791
Action reg: 0.003990
  l1.weight: grad_norm = 0.001225
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.003631
Total gradient norm: 0.005836
=== Actor Training Debug (Iteration 943) ===
Q mean: -31.994858
Q std: 16.434847
Actor loss: 31.998814
Action reg: 0.003956
  l1.weight: grad_norm = 0.000941
  l1.bias: grad_norm = 0.000741
  l2.weight: grad_norm = 0.002828
Total gradient norm: 0.006393
=== Actor Training Debug (Iteration 944) ===
Q mean: -33.105762
Q std: 16.968699
Actor loss: 33.109764
Action reg: 0.004000
  l1.weight: grad_norm = 0.000042
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000111
Total gradient norm: 0.000190
=== Actor Training Debug (Iteration 945) ===
Q mean: -32.245750
Q std: 17.669949
Actor loss: 32.249722
Action reg: 0.003972
  l1.weight: grad_norm = 0.044397
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.105374
Total gradient norm: 0.145657
=== Actor Training Debug (Iteration 946) ===
Q mean: -32.545807
Q std: 17.353628
Actor loss: 32.549797
Action reg: 0.003989
  l1.weight: grad_norm = 0.010846
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.036017
Total gradient norm: 0.066852
=== Actor Training Debug (Iteration 947) ===
Q mean: -33.127483
Q std: 17.790476
Actor loss: 33.131466
Action reg: 0.003983
  l1.weight: grad_norm = 0.029454
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.087465
Total gradient norm: 0.146027
=== Actor Training Debug (Iteration 948) ===
Q mean: -32.852066
Q std: 18.016371
Actor loss: 32.856045
Action reg: 0.003978
  l1.weight: grad_norm = 0.002966
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.006897
Total gradient norm: 0.009493
=== Actor Training Debug (Iteration 949) ===
Q mean: -31.882828
Q std: 20.080517
Actor loss: 31.886814
Action reg: 0.003987
  l1.weight: grad_norm = 0.001819
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.004965
Total gradient norm: 0.007556
=== Actor Training Debug (Iteration 950) ===
Q mean: -28.736824
Q std: 18.305384
Actor loss: 28.740759
Action reg: 0.003935
  l1.weight: grad_norm = 0.059107
  l1.bias: grad_norm = 0.000927
  l2.weight: grad_norm = 0.169241
Total gradient norm: 0.251245
=== Actor Training Debug (Iteration 951) ===
Q mean: -28.736254
Q std: 17.521893
Actor loss: 28.740211
Action reg: 0.003958
  l1.weight: grad_norm = 0.024985
  l1.bias: grad_norm = 0.000761
  l2.weight: grad_norm = 0.077898
Total gradient norm: 0.120104
=== Actor Training Debug (Iteration 952) ===
Q mean: -33.182312
Q std: 19.121408
Actor loss: 33.186260
Action reg: 0.003949
  l1.weight: grad_norm = 0.001902
  l1.bias: grad_norm = 0.000737
  l2.weight: grad_norm = 0.005092
Total gradient norm: 0.008871
=== Actor Training Debug (Iteration 953) ===
Q mean: -34.991371
Q std: 18.895885
Actor loss: 34.995350
Action reg: 0.003977
  l1.weight: grad_norm = 0.004880
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.012335
Total gradient norm: 0.020788
=== Actor Training Debug (Iteration 954) ===
Q mean: -33.500946
Q std: 18.665962
Actor loss: 33.504913
Action reg: 0.003967
  l1.weight: grad_norm = 0.000691
  l1.bias: grad_norm = 0.000541
  l2.weight: grad_norm = 0.002155
Total gradient norm: 0.004882
=== Actor Training Debug (Iteration 955) ===
Q mean: -31.619801
Q std: 18.117363
Actor loss: 31.623800
Action reg: 0.003999
  l1.weight: grad_norm = 0.010711
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.031280
Total gradient norm: 0.052730
=== Actor Training Debug (Iteration 956) ===
Q mean: -33.205338
Q std: 19.955055
Actor loss: 33.209290
Action reg: 0.003951
  l1.weight: grad_norm = 0.006320
  l1.bias: grad_norm = 0.000770
  l2.weight: grad_norm = 0.014742
Total gradient norm: 0.022139
=== Actor Training Debug (Iteration 957) ===
Q mean: -33.418800
Q std: 20.110256
Actor loss: 33.422775
Action reg: 0.003973
  l1.weight: grad_norm = 0.023444
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.065247
Total gradient norm: 0.107905
=== Actor Training Debug (Iteration 958) ===
Q mean: -35.009739
Q std: 19.381111
Actor loss: 35.013718
Action reg: 0.003977
  l1.weight: grad_norm = 0.002111
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.005364
Total gradient norm: 0.008279
=== Actor Training Debug (Iteration 959) ===
Q mean: -32.929523
Q std: 21.382469
Actor loss: 32.933514
Action reg: 0.003990
  l1.weight: grad_norm = 0.021059
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.056009
Total gradient norm: 0.076493
=== Actor Training Debug (Iteration 960) ===
Q mean: -29.264534
Q std: 17.674799
Actor loss: 29.268524
Action reg: 0.003990
  l1.weight: grad_norm = 0.012678
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.033685
Total gradient norm: 0.059754
=== Actor Training Debug (Iteration 961) ===
Q mean: -32.421051
Q std: 17.229515
Actor loss: 32.425041
Action reg: 0.003992
  l1.weight: grad_norm = 0.002824
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.007165
Total gradient norm: 0.012042
=== Actor Training Debug (Iteration 962) ===
Q mean: -32.682541
Q std: 17.909290
Actor loss: 32.686527
Action reg: 0.003986
  l1.weight: grad_norm = 0.000408
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.001058
Total gradient norm: 0.002207
=== Actor Training Debug (Iteration 963) ===
Q mean: -34.471397
Q std: 19.218958
Actor loss: 34.475388
Action reg: 0.003990
  l1.weight: grad_norm = 0.052330
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.155548
Total gradient norm: 0.273793
=== Actor Training Debug (Iteration 964) ===
Q mean: -31.889702
Q std: 18.551769
Actor loss: 31.893690
Action reg: 0.003989
  l1.weight: grad_norm = 0.017638
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.046295
Total gradient norm: 0.074670
=== Actor Training Debug (Iteration 965) ===
Q mean: -32.013229
Q std: 18.214098
Actor loss: 32.017220
Action reg: 0.003991
  l1.weight: grad_norm = 0.024916
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.070237
Total gradient norm: 0.113106
=== Actor Training Debug (Iteration 966) ===
Q mean: -35.218925
Q std: 18.650734
Actor loss: 35.222912
Action reg: 0.003985
  l1.weight: grad_norm = 0.000314
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.000928
Total gradient norm: 0.002144
=== Actor Training Debug (Iteration 967) ===
Q mean: -32.975136
Q std: 18.874273
Actor loss: 32.979103
Action reg: 0.003969
  l1.weight: grad_norm = 0.000632
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.002078
Total gradient norm: 0.004782
=== Actor Training Debug (Iteration 968) ===
Q mean: -32.841766
Q std: 20.810547
Actor loss: 32.845737
Action reg: 0.003972
  l1.weight: grad_norm = 0.003079
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.007782
Total gradient norm: 0.014004
=== Actor Training Debug (Iteration 969) ===
Q mean: -31.952068
Q std: 19.354235
Actor loss: 31.956049
Action reg: 0.003980
  l1.weight: grad_norm = 0.051185
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.168495
Total gradient norm: 0.289162
=== Actor Training Debug (Iteration 970) ===
Q mean: -33.776306
Q std: 17.463259
Actor loss: 33.780308
Action reg: 0.004000
  l1.weight: grad_norm = 0.000017
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000043
Total gradient norm: 0.000073
=== Actor Training Debug (Iteration 971) ===
Q mean: -34.185566
Q std: 21.382477
Actor loss: 34.189526
Action reg: 0.003960
  l1.weight: grad_norm = 0.048444
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.127935
Total gradient norm: 0.202758
=== Actor Training Debug (Iteration 972) ===
Q mean: -31.859781
Q std: 19.846836
Actor loss: 31.863773
Action reg: 0.003993
  l1.weight: grad_norm = 0.000308
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.000528
Total gradient norm: 0.001063
=== Actor Training Debug (Iteration 973) ===
Q mean: -32.752182
Q std: 20.340752
Actor loss: 32.756145
Action reg: 0.003963
  l1.weight: grad_norm = 0.024798
  l1.bias: grad_norm = 0.000571
  l2.weight: grad_norm = 0.067513
Total gradient norm: 0.103767
=== Actor Training Debug (Iteration 974) ===
Q mean: -34.455555
Q std: 17.329372
Actor loss: 34.459515
Action reg: 0.003959
  l1.weight: grad_norm = 0.074112
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.186060
Total gradient norm: 0.298378
=== Actor Training Debug (Iteration 975) ===
Q mean: -32.625813
Q std: 18.002699
Actor loss: 32.629776
Action reg: 0.003963
  l1.weight: grad_norm = 0.014892
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.039897
Total gradient norm: 0.064909
=== Actor Training Debug (Iteration 976) ===
Q mean: -32.234245
Q std: 18.276220
Actor loss: 32.238216
Action reg: 0.003971
  l1.weight: grad_norm = 0.006102
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.019341
Total gradient norm: 0.032271
=== Actor Training Debug (Iteration 977) ===
Q mean: -31.912994
Q std: 21.139528
Actor loss: 31.916952
Action reg: 0.003957
  l1.weight: grad_norm = 0.039039
  l1.bias: grad_norm = 0.000665
  l2.weight: grad_norm = 0.099970
Total gradient norm: 0.160666
=== Actor Training Debug (Iteration 978) ===
Q mean: -34.715324
Q std: 19.040764
Actor loss: 34.719303
Action reg: 0.003977
  l1.weight: grad_norm = 0.048484
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.142655
Total gradient norm: 0.243112
=== Actor Training Debug (Iteration 979) ===
Q mean: -33.124462
Q std: 20.312675
Actor loss: 33.128433
Action reg: 0.003970
  l1.weight: grad_norm = 0.026933
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.076918
Total gradient norm: 0.127095
=== Actor Training Debug (Iteration 980) ===
Q mean: -32.282566
Q std: 19.142979
Actor loss: 32.286552
Action reg: 0.003986
  l1.weight: grad_norm = 0.000348
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.000965
Total gradient norm: 0.002167
=== Actor Training Debug (Iteration 981) ===
Q mean: -30.205309
Q std: 18.755503
Actor loss: 30.209265
Action reg: 0.003957
  l1.weight: grad_norm = 0.021325
  l1.bias: grad_norm = 0.000669
  l2.weight: grad_norm = 0.065583
Total gradient norm: 0.104468
=== Actor Training Debug (Iteration 982) ===
Q mean: -35.135254
Q std: 19.365730
Actor loss: 35.139236
Action reg: 0.003982
  l1.weight: grad_norm = 0.026578
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.079594
Total gradient norm: 0.134838
=== Actor Training Debug (Iteration 983) ===
Q mean: -33.455315
Q std: 17.493277
Actor loss: 33.459290
Action reg: 0.003975
  l1.weight: grad_norm = 0.000712
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.002057
Total gradient norm: 0.004033
=== Actor Training Debug (Iteration 984) ===
Q mean: -31.563446
Q std: 18.228367
Actor loss: 31.567427
Action reg: 0.003981
  l1.weight: grad_norm = 0.049473
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.133068
Total gradient norm: 0.206965
=== Actor Training Debug (Iteration 985) ===
Q mean: -30.410580
Q std: 19.896296
Actor loss: 30.414511
Action reg: 0.003932
  l1.weight: grad_norm = 0.011247
  l1.bias: grad_norm = 0.000917
  l2.weight: grad_norm = 0.031674
Total gradient norm: 0.055204
=== Actor Training Debug (Iteration 986) ===
Q mean: -36.302895
Q std: 20.625685
Actor loss: 36.306889
Action reg: 0.003995
  l1.weight: grad_norm = 0.002358
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.006009
Total gradient norm: 0.009872
=== Actor Training Debug (Iteration 987) ===
Q mean: -33.794228
Q std: 18.429749
Actor loss: 33.798225
Action reg: 0.003999
  l1.weight: grad_norm = 0.011806
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.035152
Total gradient norm: 0.066263
=== Actor Training Debug (Iteration 988) ===
Q mean: -31.002522
Q std: 18.524443
Actor loss: 31.006500
Action reg: 0.003978
  l1.weight: grad_norm = 0.052981
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.155533
Total gradient norm: 0.247614
=== Actor Training Debug (Iteration 989) ===
Q mean: -31.992975
Q std: 17.438864
Actor loss: 31.996958
Action reg: 0.003982
  l1.weight: grad_norm = 0.002616
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.006012
Total gradient norm: 0.008406
=== Actor Training Debug (Iteration 990) ===
Q mean: -34.590828
Q std: 18.446072
Actor loss: 34.594795
Action reg: 0.003966
  l1.weight: grad_norm = 0.001640
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.004401
Total gradient norm: 0.007585
=== Actor Training Debug (Iteration 991) ===
Q mean: -31.828434
Q std: 18.498846
Actor loss: 31.832394
Action reg: 0.003960
  l1.weight: grad_norm = 0.000752
  l1.bias: grad_norm = 0.000628
  l2.weight: grad_norm = 0.002446
Total gradient norm: 0.005426
=== Actor Training Debug (Iteration 992) ===
Q mean: -31.282835
Q std: 20.250988
Actor loss: 31.286808
Action reg: 0.003974
  l1.weight: grad_norm = 0.030702
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.090032
Total gradient norm: 0.151590
=== Actor Training Debug (Iteration 993) ===
Q mean: -30.375605
Q std: 17.464487
Actor loss: 30.379581
Action reg: 0.003978
  l1.weight: grad_norm = 0.046778
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.136979
Total gradient norm: 0.237629
=== Actor Training Debug (Iteration 994) ===
Q mean: -34.205223
Q std: 20.723803
Actor loss: 34.209213
Action reg: 0.003990
  l1.weight: grad_norm = 0.020686
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.054960
Total gradient norm: 0.088901
=== Actor Training Debug (Iteration 995) ===
Q mean: -34.586281
Q std: 17.565077
Actor loss: 34.590263
Action reg: 0.003983
  l1.weight: grad_norm = 0.089169
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.260186
Total gradient norm: 0.412161
=== Actor Training Debug (Iteration 996) ===
Q mean: -34.326702
Q std: 20.074100
Actor loss: 34.330662
Action reg: 0.003960
  l1.weight: grad_norm = 0.039420
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.097764
Total gradient norm: 0.148072
=== Actor Training Debug (Iteration 997) ===
Q mean: -30.287258
Q std: 19.466576
Actor loss: 30.291233
Action reg: 0.003976
  l1.weight: grad_norm = 0.000654
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.002078
Total gradient norm: 0.004099
=== Actor Training Debug (Iteration 998) ===
Q mean: -29.132517
Q std: 17.822275
Actor loss: 29.136505
Action reg: 0.003989
  l1.weight: grad_norm = 0.042122
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.115905
Total gradient norm: 0.185573
=== Actor Training Debug (Iteration 999) ===
Q mean: -30.576078
Q std: 17.999834
Actor loss: 30.580061
Action reg: 0.003983
  l1.weight: grad_norm = 0.048150
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.146159
Total gradient norm: 0.246599
=== Actor Training Debug (Iteration 1000) ===
Q mean: -35.301434
Q std: 19.449907
Actor loss: 35.305412
Action reg: 0.003979
  l1.weight: grad_norm = 0.042667
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.116414
Total gradient norm: 0.178066
Step 6000: Critic Loss: 6.8972, Actor Loss: 35.3054, Q Value: -35.3014
  Average reward: -342.000 | Average length: 100.0
Evaluation at episode 60: -342.000
=== Actor Training Debug (Iteration 1001) ===
Q mean: -36.729408
Q std: 18.854048
Actor loss: 36.733402
Action reg: 0.003992
  l1.weight: grad_norm = 0.000663
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.001702
Total gradient norm: 0.002822
=== Actor Training Debug (Iteration 1002) ===
Q mean: -33.306618
Q std: 18.777754
Actor loss: 33.310589
Action reg: 0.003971
  l1.weight: grad_norm = 0.106785
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.308829
Total gradient norm: 0.534014
=== Actor Training Debug (Iteration 1003) ===
Q mean: -29.707802
Q std: 17.614311
Actor loss: 29.711786
Action reg: 0.003984
  l1.weight: grad_norm = 0.001379
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.003602
Total gradient norm: 0.006304
=== Actor Training Debug (Iteration 1004) ===
Q mean: -29.354521
Q std: 18.959396
Actor loss: 29.358501
Action reg: 0.003981
  l1.weight: grad_norm = 0.078577
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.211584
Total gradient norm: 0.351378
=== Actor Training Debug (Iteration 1005) ===
Q mean: -33.778336
Q std: 19.754957
Actor loss: 33.782314
Action reg: 0.003977
  l1.weight: grad_norm = 0.001495
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.004154
Total gradient norm: 0.007519
=== Actor Training Debug (Iteration 1006) ===
Q mean: -34.124790
Q std: 20.629839
Actor loss: 34.128765
Action reg: 0.003976
  l1.weight: grad_norm = 0.003065
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.008342
Total gradient norm: 0.012148
=== Actor Training Debug (Iteration 1007) ===
Q mean: -35.339203
Q std: 19.332117
Actor loss: 35.343197
Action reg: 0.003993
  l1.weight: grad_norm = 0.002698
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.007825
Total gradient norm: 0.012736
=== Actor Training Debug (Iteration 1008) ===
Q mean: -31.829088
Q std: 18.016979
Actor loss: 31.833067
Action reg: 0.003979
  l1.weight: grad_norm = 0.045961
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.111263
Total gradient norm: 0.166278
=== Actor Training Debug (Iteration 1009) ===
Q mean: -30.591660
Q std: 17.157331
Actor loss: 30.595634
Action reg: 0.003975
  l1.weight: grad_norm = 0.000619
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.001858
Total gradient norm: 0.003804
=== Actor Training Debug (Iteration 1010) ===
Q mean: -35.185051
Q std: 17.648287
Actor loss: 35.189026
Action reg: 0.003975
  l1.weight: grad_norm = 0.000546
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.001525
Total gradient norm: 0.003322
=== Actor Training Debug (Iteration 1011) ===
Q mean: -32.273853
Q std: 17.333725
Actor loss: 32.277843
Action reg: 0.003991
  l1.weight: grad_norm = 0.005374
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.014546
Total gradient norm: 0.025070
=== Actor Training Debug (Iteration 1012) ===
Q mean: -34.456718
Q std: 19.183191
Actor loss: 34.460712
Action reg: 0.003993
  l1.weight: grad_norm = 0.000268
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.000566
Total gradient norm: 0.001005
=== Actor Training Debug (Iteration 1013) ===
Q mean: -29.284512
Q std: 17.812449
Actor loss: 29.288502
Action reg: 0.003991
  l1.weight: grad_norm = 0.005961
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.016952
Total gradient norm: 0.028808
=== Actor Training Debug (Iteration 1014) ===
Q mean: -32.508335
Q std: 18.953022
Actor loss: 32.512287
Action reg: 0.003951
  l1.weight: grad_norm = 0.032715
  l1.bias: grad_norm = 0.000716
  l2.weight: grad_norm = 0.084897
Total gradient norm: 0.141364
=== Actor Training Debug (Iteration 1015) ===
Q mean: -34.569492
Q std: 17.744593
Actor loss: 34.573471
Action reg: 0.003979
  l1.weight: grad_norm = 0.006472
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.019200
Total gradient norm: 0.034787
=== Actor Training Debug (Iteration 1016) ===
Q mean: -35.510139
Q std: 19.513680
Actor loss: 35.514114
Action reg: 0.003974
  l1.weight: grad_norm = 0.015380
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.045995
Total gradient norm: 0.075247
=== Actor Training Debug (Iteration 1017) ===
Q mean: -32.379395
Q std: 17.301374
Actor loss: 32.383396
Action reg: 0.004000
  l1.weight: grad_norm = 0.002150
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.006622
Total gradient norm: 0.010677
=== Actor Training Debug (Iteration 1018) ===
Q mean: -30.305973
Q std: 17.285110
Actor loss: 30.309931
Action reg: 0.003958
  l1.weight: grad_norm = 0.010283
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.027370
Total gradient norm: 0.046325
=== Actor Training Debug (Iteration 1019) ===
Q mean: -31.510475
Q std: 17.399925
Actor loss: 31.514454
Action reg: 0.003979
  l1.weight: grad_norm = 0.084682
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.239895
Total gradient norm: 0.453751
=== Actor Training Debug (Iteration 1020) ===
Q mean: -35.238327
Q std: 19.841732
Actor loss: 35.242310
Action reg: 0.003984
  l1.weight: grad_norm = 0.003684
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.009708
Total gradient norm: 0.015470
=== Actor Training Debug (Iteration 1021) ===
Q mean: -36.381439
Q std: 19.700060
Actor loss: 36.385426
Action reg: 0.003986
  l1.weight: grad_norm = 0.025503
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.069028
Total gradient norm: 0.112718
=== Actor Training Debug (Iteration 1022) ===
Q mean: -33.294998
Q std: 18.311808
Actor loss: 33.298962
Action reg: 0.003962
  l1.weight: grad_norm = 0.004980
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.013796
Total gradient norm: 0.023454
=== Actor Training Debug (Iteration 1023) ===
Q mean: -28.286766
Q std: 17.476524
Actor loss: 28.290756
Action reg: 0.003990
  l1.weight: grad_norm = 0.013463
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.040845
Total gradient norm: 0.065751
=== Actor Training Debug (Iteration 1024) ===
Q mean: -30.920847
Q std: 18.078482
Actor loss: 30.924812
Action reg: 0.003965
  l1.weight: grad_norm = 0.000571
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.002013
Total gradient norm: 0.004562
=== Actor Training Debug (Iteration 1025) ===
Q mean: -31.173576
Q std: 19.247923
Actor loss: 31.177542
Action reg: 0.003965
  l1.weight: grad_norm = 0.002787
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.008376
Total gradient norm: 0.016044
=== Actor Training Debug (Iteration 1026) ===
Q mean: -37.577412
Q std: 22.371887
Actor loss: 37.581379
Action reg: 0.003967
  l1.weight: grad_norm = 0.023748
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.067652
Total gradient norm: 0.105738
=== Actor Training Debug (Iteration 1027) ===
Q mean: -39.466564
Q std: 18.517099
Actor loss: 39.470551
Action reg: 0.003985
  l1.weight: grad_norm = 0.000368
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.000930
Total gradient norm: 0.001953
=== Actor Training Debug (Iteration 1028) ===
Q mean: -36.443352
Q std: 20.534407
Actor loss: 36.447327
Action reg: 0.003977
  l1.weight: grad_norm = 0.000439
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.001291
Total gradient norm: 0.002768
=== Actor Training Debug (Iteration 1029) ===
Q mean: -30.261700
Q std: 17.845364
Actor loss: 30.265676
Action reg: 0.003978
  l1.weight: grad_norm = 0.000471
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.001333
Total gradient norm: 0.002762
=== Actor Training Debug (Iteration 1030) ===
Q mean: -28.235447
Q std: 17.199987
Actor loss: 28.239437
Action reg: 0.003991
  l1.weight: grad_norm = 0.023139
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.063180
Total gradient norm: 0.104479
=== Actor Training Debug (Iteration 1031) ===
Q mean: -31.628435
Q std: 17.097738
Actor loss: 31.632418
Action reg: 0.003983
  l1.weight: grad_norm = 0.048291
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.117285
Total gradient norm: 0.164945
=== Actor Training Debug (Iteration 1032) ===
Q mean: -34.130081
Q std: 17.608345
Actor loss: 34.134071
Action reg: 0.003991
  l1.weight: grad_norm = 0.048514
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.112432
Total gradient norm: 0.169250
=== Actor Training Debug (Iteration 1033) ===
Q mean: -35.300941
Q std: 19.792618
Actor loss: 35.304932
Action reg: 0.003991
  l1.weight: grad_norm = 0.002651
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.006170
Total gradient norm: 0.008181
=== Actor Training Debug (Iteration 1034) ===
Q mean: -32.870808
Q std: 19.424425
Actor loss: 32.874794
Action reg: 0.003985
  l1.weight: grad_norm = 0.000714
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.002103
Total gradient norm: 0.004271
=== Actor Training Debug (Iteration 1035) ===
Q mean: -29.149397
Q std: 17.682455
Actor loss: 29.153381
Action reg: 0.003984
  l1.weight: grad_norm = 0.076323
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.219350
Total gradient norm: 0.383408
=== Actor Training Debug (Iteration 1036) ===
Q mean: -29.893242
Q std: 17.682402
Actor loss: 29.897224
Action reg: 0.003983
  l1.weight: grad_norm = 0.014610
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.040958
Total gradient norm: 0.065594
=== Actor Training Debug (Iteration 1037) ===
Q mean: -31.878675
Q std: 18.720957
Actor loss: 31.882656
Action reg: 0.003981
  l1.weight: grad_norm = 0.036012
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.092271
Total gradient norm: 0.158789
=== Actor Training Debug (Iteration 1038) ===
Q mean: -34.210701
Q std: 18.463358
Actor loss: 34.214680
Action reg: 0.003979
  l1.weight: grad_norm = 0.001475
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.004384
Total gradient norm: 0.008600
=== Actor Training Debug (Iteration 1039) ===
Q mean: -36.290833
Q std: 20.291243
Actor loss: 36.294800
Action reg: 0.003967
  l1.weight: grad_norm = 0.011961
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.033288
Total gradient norm: 0.055302
=== Actor Training Debug (Iteration 1040) ===
Q mean: -34.855320
Q std: 17.511885
Actor loss: 34.859310
Action reg: 0.003991
  l1.weight: grad_norm = 0.010077
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.030167
Total gradient norm: 0.056485
=== Actor Training Debug (Iteration 1041) ===
Q mean: -28.225689
Q std: 16.780811
Actor loss: 28.229647
Action reg: 0.003959
  l1.weight: grad_norm = 0.003145
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.007438
Total gradient norm: 0.011099
=== Actor Training Debug (Iteration 1042) ===
Q mean: -27.859638
Q std: 17.099623
Actor loss: 27.863632
Action reg: 0.003994
  l1.weight: grad_norm = 0.019240
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.049180
Total gradient norm: 0.069212
=== Actor Training Debug (Iteration 1043) ===
Q mean: -30.570807
Q std: 16.787819
Actor loss: 30.574797
Action reg: 0.003991
  l1.weight: grad_norm = 0.000360
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.000811
Total gradient norm: 0.001443
=== Actor Training Debug (Iteration 1044) ===
Q mean: -34.213684
Q std: 19.568270
Actor loss: 34.217644
Action reg: 0.003961
  l1.weight: grad_norm = 0.011046
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.028663
Total gradient norm: 0.052619
=== Actor Training Debug (Iteration 1045) ===
Q mean: -37.810497
Q std: 18.610016
Actor loss: 37.814487
Action reg: 0.003990
  l1.weight: grad_norm = 0.028963
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.080061
Total gradient norm: 0.133990
=== Actor Training Debug (Iteration 1046) ===
Q mean: -33.704464
Q std: 17.949406
Actor loss: 33.708447
Action reg: 0.003982
  l1.weight: grad_norm = 0.028238
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.068429
Total gradient norm: 0.099891
=== Actor Training Debug (Iteration 1047) ===
Q mean: -29.427067
Q std: 18.468199
Actor loss: 29.431055
Action reg: 0.003988
  l1.weight: grad_norm = 0.021076
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.050391
Total gradient norm: 0.083449
=== Actor Training Debug (Iteration 1048) ===
Q mean: -31.206059
Q std: 18.195419
Actor loss: 31.210052
Action reg: 0.003994
  l1.weight: grad_norm = 0.000172
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.000375
Total gradient norm: 0.000702
=== Actor Training Debug (Iteration 1049) ===
Q mean: -30.550037
Q std: 17.351652
Actor loss: 30.553989
Action reg: 0.003952
  l1.weight: grad_norm = 0.050636
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.140256
Total gradient norm: 0.230140
=== Actor Training Debug (Iteration 1050) ===
Q mean: -39.250454
Q std: 20.984314
Actor loss: 39.254436
Action reg: 0.003984
  l1.weight: grad_norm = 0.002152
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.004920
Total gradient norm: 0.007078
=== Actor Training Debug (Iteration 1051) ===
Q mean: -38.661915
Q std: 19.222454
Actor loss: 38.665909
Action reg: 0.003995
  l1.weight: grad_norm = 0.026329
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.061420
Total gradient norm: 0.096732
=== Actor Training Debug (Iteration 1052) ===
Q mean: -34.592361
Q std: 20.256468
Actor loss: 34.596313
Action reg: 0.003954
  l1.weight: grad_norm = 0.048053
  l1.bias: grad_norm = 0.000594
  l2.weight: grad_norm = 0.124939
Total gradient norm: 0.203070
=== Actor Training Debug (Iteration 1053) ===
Q mean: -31.139351
Q std: 18.237713
Actor loss: 31.143343
Action reg: 0.003992
  l1.weight: grad_norm = 0.000550
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.001538
Total gradient norm: 0.002425
=== Actor Training Debug (Iteration 1054) ===
Q mean: -28.920601
Q std: 18.254919
Actor loss: 28.924576
Action reg: 0.003975
  l1.weight: grad_norm = 0.012343
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.028528
Total gradient norm: 0.043652
=== Actor Training Debug (Iteration 1055) ===
Q mean: -30.121552
Q std: 16.957270
Actor loss: 30.125530
Action reg: 0.003978
  l1.weight: grad_norm = 0.003418
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.010842
Total gradient norm: 0.019859
=== Actor Training Debug (Iteration 1056) ===
Q mean: -33.448845
Q std: 18.284000
Actor loss: 33.452824
Action reg: 0.003978
  l1.weight: grad_norm = 0.032919
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.084019
Total gradient norm: 0.133019
=== Actor Training Debug (Iteration 1057) ===
Q mean: -36.748924
Q std: 19.267456
Actor loss: 36.752892
Action reg: 0.003968
  l1.weight: grad_norm = 0.029673
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.088077
Total gradient norm: 0.146384
=== Actor Training Debug (Iteration 1058) ===
Q mean: -36.233059
Q std: 18.238096
Actor loss: 36.237049
Action reg: 0.003990
  l1.weight: grad_norm = 0.033747
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.093989
Total gradient norm: 0.154893
=== Actor Training Debug (Iteration 1059) ===
Q mean: -31.445656
Q std: 18.133858
Actor loss: 31.449621
Action reg: 0.003965
  l1.weight: grad_norm = 0.001113
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.003181
Total gradient norm: 0.005876
=== Actor Training Debug (Iteration 1060) ===
Q mean: -29.601725
Q std: 20.454187
Actor loss: 29.605688
Action reg: 0.003964
  l1.weight: grad_norm = 0.024179
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.058087
Total gradient norm: 0.077858
=== Actor Training Debug (Iteration 1061) ===
Q mean: -30.982601
Q std: 19.092638
Actor loss: 30.986580
Action reg: 0.003978
  l1.weight: grad_norm = 0.027424
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.078888
Total gradient norm: 0.149828
=== Actor Training Debug (Iteration 1062) ===
Q mean: -32.088127
Q std: 17.547440
Actor loss: 32.092113
Action reg: 0.003986
  l1.weight: grad_norm = 0.000766
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.002143
Total gradient norm: 0.004105
=== Actor Training Debug (Iteration 1063) ===
Q mean: -36.132927
Q std: 20.023756
Actor loss: 36.136898
Action reg: 0.003972
  l1.weight: grad_norm = 0.021187
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.052797
Total gradient norm: 0.083667
=== Actor Training Debug (Iteration 1064) ===
Q mean: -37.456875
Q std: 20.738167
Actor loss: 37.460857
Action reg: 0.003983
  l1.weight: grad_norm = 0.006420
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.016596
Total gradient norm: 0.027102
=== Actor Training Debug (Iteration 1065) ===
Q mean: -34.071289
Q std: 19.040720
Actor loss: 34.075272
Action reg: 0.003982
  l1.weight: grad_norm = 0.046861
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.131218
Total gradient norm: 0.228692
=== Actor Training Debug (Iteration 1066) ===
Q mean: -31.187120
Q std: 18.807467
Actor loss: 31.191092
Action reg: 0.003972
  l1.weight: grad_norm = 0.034349
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.079436
Total gradient norm: 0.133316
=== Actor Training Debug (Iteration 1067) ===
Q mean: -31.810753
Q std: 19.091469
Actor loss: 31.814732
Action reg: 0.003978
  l1.weight: grad_norm = 0.009862
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.027702
Total gradient norm: 0.048160
=== Actor Training Debug (Iteration 1068) ===
Q mean: -34.083237
Q std: 18.260143
Actor loss: 34.087223
Action reg: 0.003986
  l1.weight: grad_norm = 0.000347
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.001036
Total gradient norm: 0.002255
=== Actor Training Debug (Iteration 1069) ===
Q mean: -37.295189
Q std: 19.341883
Actor loss: 37.299160
Action reg: 0.003970
  l1.weight: grad_norm = 0.064366
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.166023
Total gradient norm: 0.265730
=== Actor Training Debug (Iteration 1070) ===
Q mean: -31.731453
Q std: 17.642443
Actor loss: 31.735449
Action reg: 0.003996
  l1.weight: grad_norm = 0.029057
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.073301
Total gradient norm: 0.115799
=== Actor Training Debug (Iteration 1071) ===
Q mean: -31.781752
Q std: 18.514656
Actor loss: 31.785748
Action reg: 0.003996
  l1.weight: grad_norm = 0.049455
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.123475
Total gradient norm: 0.188144
=== Actor Training Debug (Iteration 1072) ===
Q mean: -32.932518
Q std: 18.751720
Actor loss: 32.936497
Action reg: 0.003979
  l1.weight: grad_norm = 0.009827
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.028374
Total gradient norm: 0.054198
=== Actor Training Debug (Iteration 1073) ===
Q mean: -35.083221
Q std: 19.566338
Actor loss: 35.087204
Action reg: 0.003983
  l1.weight: grad_norm = 0.025537
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.063244
Total gradient norm: 0.103977
=== Actor Training Debug (Iteration 1074) ===
Q mean: -34.763260
Q std: 17.421530
Actor loss: 34.767246
Action reg: 0.003986
  l1.weight: grad_norm = 0.001517
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.004407
Total gradient norm: 0.007588
=== Actor Training Debug (Iteration 1075) ===
Q mean: -32.125900
Q std: 19.234922
Actor loss: 32.129883
Action reg: 0.003984
  l1.weight: grad_norm = 0.022037
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.068615
Total gradient norm: 0.120581
=== Actor Training Debug (Iteration 1076) ===
Q mean: -30.854103
Q std: 17.422852
Actor loss: 30.858101
Action reg: 0.003997
  l1.weight: grad_norm = 0.011750
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.031252
Total gradient norm: 0.052982
=== Actor Training Debug (Iteration 1077) ===
Q mean: -30.986580
Q std: 17.165573
Actor loss: 30.990541
Action reg: 0.003961
  l1.weight: grad_norm = 0.004987
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.013345
Total gradient norm: 0.021632
=== Actor Training Debug (Iteration 1078) ===
Q mean: -32.949535
Q std: 17.971466
Actor loss: 32.953506
Action reg: 0.003969
  l1.weight: grad_norm = 0.060744
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.144663
Total gradient norm: 0.208393
=== Actor Training Debug (Iteration 1079) ===
Q mean: -34.584801
Q std: 19.716419
Actor loss: 34.588795
Action reg: 0.003992
  l1.weight: grad_norm = 0.000265
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.000583
Total gradient norm: 0.001075
=== Actor Training Debug (Iteration 1080) ===
Q mean: -35.509663
Q std: 16.629818
Actor loss: 35.513645
Action reg: 0.003984
  l1.weight: grad_norm = 0.033085
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.085399
Total gradient norm: 0.141436
=== Actor Training Debug (Iteration 1081) ===
Q mean: -33.158947
Q std: 18.858595
Actor loss: 33.162933
Action reg: 0.003986
  l1.weight: grad_norm = 0.000989
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.002465
Total gradient norm: 0.004026
=== Actor Training Debug (Iteration 1082) ===
Q mean: -31.874643
Q std: 18.576027
Actor loss: 31.878635
Action reg: 0.003993
  l1.weight: grad_norm = 0.005246
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.013627
Total gradient norm: 0.022156
=== Actor Training Debug (Iteration 1083) ===
Q mean: -34.139801
Q std: 18.006121
Actor loss: 34.143787
Action reg: 0.003986
  l1.weight: grad_norm = 0.001292
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.003032
Total gradient norm: 0.004553
=== Actor Training Debug (Iteration 1084) ===
Q mean: -34.419182
Q std: 17.144932
Actor loss: 34.423164
Action reg: 0.003984
  l1.weight: grad_norm = 0.003144
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.008994
Total gradient norm: 0.015395
=== Actor Training Debug (Iteration 1085) ===
Q mean: -32.789047
Q std: 18.590841
Actor loss: 32.793030
Action reg: 0.003984
  l1.weight: grad_norm = 0.003204
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.009102
Total gradient norm: 0.016125
=== Actor Training Debug (Iteration 1086) ===
Q mean: -34.190231
Q std: 18.571173
Actor loss: 34.194202
Action reg: 0.003972
  l1.weight: grad_norm = 0.021643
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.069645
Total gradient norm: 0.123172
=== Actor Training Debug (Iteration 1087) ===
Q mean: -34.198486
Q std: 19.323805
Actor loss: 34.202477
Action reg: 0.003990
  l1.weight: grad_norm = 0.065832
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.173365
Total gradient norm: 0.312532
=== Actor Training Debug (Iteration 1088) ===
Q mean: -34.720131
Q std: 18.561687
Actor loss: 34.724117
Action reg: 0.003985
  l1.weight: grad_norm = 0.032400
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.081200
Total gradient norm: 0.130028
=== Actor Training Debug (Iteration 1089) ===
Q mean: -33.437317
Q std: 17.734428
Actor loss: 33.441299
Action reg: 0.003984
  l1.weight: grad_norm = 0.003071
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.008481
Total gradient norm: 0.014103
=== Actor Training Debug (Iteration 1090) ===
Q mean: -32.625267
Q std: 19.128082
Actor loss: 32.629223
Action reg: 0.003957
  l1.weight: grad_norm = 0.000487
  l1.bias: grad_norm = 0.000505
  l2.weight: grad_norm = 0.002080
Total gradient norm: 0.004578
=== Actor Training Debug (Iteration 1091) ===
Q mean: -35.807114
Q std: 19.321133
Actor loss: 35.811092
Action reg: 0.003980
  l1.weight: grad_norm = 0.002551
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.006204
Total gradient norm: 0.008421
=== Actor Training Debug (Iteration 1092) ===
Q mean: -34.861088
Q std: 16.996483
Actor loss: 34.865055
Action reg: 0.003966
  l1.weight: grad_norm = 0.088894
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.261255
Total gradient norm: 0.442404
=== Actor Training Debug (Iteration 1093) ===
Q mean: -32.338371
Q std: 18.905947
Actor loss: 32.342369
Action reg: 0.003999
  l1.weight: grad_norm = 0.080433
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.214985
Total gradient norm: 0.366238
=== Actor Training Debug (Iteration 1094) ===
Q mean: -30.529314
Q std: 19.514061
Actor loss: 30.533312
Action reg: 0.003998
  l1.weight: grad_norm = 0.015120
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.040234
Total gradient norm: 0.065235
=== Actor Training Debug (Iteration 1095) ===
Q mean: -37.202450
Q std: 16.843941
Actor loss: 37.206425
Action reg: 0.003974
  l1.weight: grad_norm = 0.017042
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.043931
Total gradient norm: 0.064656
=== Actor Training Debug (Iteration 1096) ===
Q mean: -34.905815
Q std: 20.254923
Actor loss: 34.909790
Action reg: 0.003976
  l1.weight: grad_norm = 0.000383
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.001186
Total gradient norm: 0.002794
=== Actor Training Debug (Iteration 1097) ===
Q mean: -32.818115
Q std: 18.926855
Actor loss: 32.822098
Action reg: 0.003982
  l1.weight: grad_norm = 0.023946
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.069161
Total gradient norm: 0.116398
=== Actor Training Debug (Iteration 1098) ===
Q mean: -32.486671
Q std: 17.314707
Actor loss: 32.490662
Action reg: 0.003992
  l1.weight: grad_norm = 0.016897
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.047439
Total gradient norm: 0.080591
=== Actor Training Debug (Iteration 1099) ===
Q mean: -35.810410
Q std: 19.763361
Actor loss: 35.814392
Action reg: 0.003982
  l1.weight: grad_norm = 0.011590
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.028438
Total gradient norm: 0.038417
=== Actor Training Debug (Iteration 1100) ===
Q mean: -33.933716
Q std: 16.367706
Actor loss: 33.937702
Action reg: 0.003987
  l1.weight: grad_norm = 0.000244
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.000596
Total gradient norm: 0.001301
Episode 61: Steps=100, Reward=-291.563, Buffer_size=6100
=== Actor Training Debug (Iteration 1101) ===
Q mean: -32.515594
Q std: 16.683609
Actor loss: 32.519585
Action reg: 0.003991
  l1.weight: grad_norm = 0.018227
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.051856
Total gradient norm: 0.081151
=== Actor Training Debug (Iteration 1102) ===
Q mean: -33.580196
Q std: 18.497507
Actor loss: 33.584167
Action reg: 0.003971
  l1.weight: grad_norm = 0.018111
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.050364
Total gradient norm: 0.068940
=== Actor Training Debug (Iteration 1103) ===
Q mean: -32.549774
Q std: 18.667353
Actor loss: 32.553753
Action reg: 0.003979
  l1.weight: grad_norm = 0.000873
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.002362
Total gradient norm: 0.004343
=== Actor Training Debug (Iteration 1104) ===
Q mean: -33.062599
Q std: 18.074385
Actor loss: 33.066593
Action reg: 0.003993
  l1.weight: grad_norm = 0.067689
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.162931
Total gradient norm: 0.275775
=== Actor Training Debug (Iteration 1105) ===
Q mean: -37.001041
Q std: 19.795883
Actor loss: 37.005016
Action reg: 0.003975
  l1.weight: grad_norm = 0.039150
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.107410
Total gradient norm: 0.162859
=== Actor Training Debug (Iteration 1106) ===
Q mean: -37.921341
Q std: 19.656393
Actor loss: 37.925323
Action reg: 0.003984
  l1.weight: grad_norm = 0.099430
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.282138
Total gradient norm: 0.540831
=== Actor Training Debug (Iteration 1107) ===
Q mean: -33.536995
Q std: 17.542425
Actor loss: 33.540985
Action reg: 0.003991
  l1.weight: grad_norm = 0.034859
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.086038
Total gradient norm: 0.144585
=== Actor Training Debug (Iteration 1108) ===
Q mean: -33.113491
Q std: 16.924274
Actor loss: 33.117477
Action reg: 0.003985
  l1.weight: grad_norm = 0.002645
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.007426
Total gradient norm: 0.012965
=== Actor Training Debug (Iteration 1109) ===
Q mean: -35.929111
Q std: 20.203833
Actor loss: 35.933098
Action reg: 0.003987
  l1.weight: grad_norm = 0.001531
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.004368
Total gradient norm: 0.006692
=== Actor Training Debug (Iteration 1110) ===
Q mean: -36.039505
Q std: 18.395580
Actor loss: 36.043495
Action reg: 0.003990
  l1.weight: grad_norm = 0.038658
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.103193
Total gradient norm: 0.168701
=== Actor Training Debug (Iteration 1111) ===
Q mean: -33.906334
Q std: 19.632343
Actor loss: 33.910305
Action reg: 0.003970
  l1.weight: grad_norm = 0.000503
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.001623
Total gradient norm: 0.003437
=== Actor Training Debug (Iteration 1112) ===
Q mean: -34.945526
Q std: 18.958454
Actor loss: 34.949520
Action reg: 0.003993
  l1.weight: grad_norm = 0.034216
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.090482
Total gradient norm: 0.122334
=== Actor Training Debug (Iteration 1113) ===
Q mean: -36.087570
Q std: 20.648125
Actor loss: 36.091553
Action reg: 0.003982
  l1.weight: grad_norm = 0.002057
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.005563
Total gradient norm: 0.009302
=== Actor Training Debug (Iteration 1114) ===
Q mean: -34.654297
Q std: 18.069641
Actor loss: 34.658264
Action reg: 0.003967
  l1.weight: grad_norm = 0.005258
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.013033
Total gradient norm: 0.018003
=== Actor Training Debug (Iteration 1115) ===
Q mean: -35.846275
Q std: 18.384478
Actor loss: 35.850262
Action reg: 0.003986
  l1.weight: grad_norm = 0.000767
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.002236
Total gradient norm: 0.003988
=== Actor Training Debug (Iteration 1116) ===
Q mean: -31.868767
Q std: 17.439703
Actor loss: 31.872747
Action reg: 0.003980
  l1.weight: grad_norm = 0.004124
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.012970
Total gradient norm: 0.022221
=== Actor Training Debug (Iteration 1117) ===
Q mean: -35.411713
Q std: 18.884554
Actor loss: 35.415688
Action reg: 0.003977
  l1.weight: grad_norm = 0.101159
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.253372
Total gradient norm: 0.411817
=== Actor Training Debug (Iteration 1118) ===
Q mean: -30.851599
Q std: 16.318039
Actor loss: 30.855589
Action reg: 0.003990
  l1.weight: grad_norm = 0.045595
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.114370
Total gradient norm: 0.172478
=== Actor Training Debug (Iteration 1119) ===
Q mean: -34.328300
Q std: 17.785215
Actor loss: 34.332275
Action reg: 0.003974
  l1.weight: grad_norm = 0.031206
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.080097
Total gradient norm: 0.133692
=== Actor Training Debug (Iteration 1120) ===
Q mean: -36.398140
Q std: 18.423615
Actor loss: 36.402122
Action reg: 0.003982
  l1.weight: grad_norm = 0.040888
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.110090
Total gradient norm: 0.169949
=== Actor Training Debug (Iteration 1121) ===
Q mean: -34.499344
Q std: 19.497272
Actor loss: 34.503326
Action reg: 0.003982
  l1.weight: grad_norm = 0.008308
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.022988
Total gradient norm: 0.042198
=== Actor Training Debug (Iteration 1122) ===
Q mean: -31.154629
Q std: 18.142296
Actor loss: 31.158628
Action reg: 0.004000
  l1.weight: grad_norm = 0.000222
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000541
Total gradient norm: 0.000704
=== Actor Training Debug (Iteration 1123) ===
Q mean: -33.578201
Q std: 18.546654
Actor loss: 33.582191
Action reg: 0.003990
  l1.weight: grad_norm = 0.010457
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.026021
Total gradient norm: 0.043113
=== Actor Training Debug (Iteration 1124) ===
Q mean: -36.580311
Q std: 19.114754
Actor loss: 36.584305
Action reg: 0.003993
  l1.weight: grad_norm = 0.003873
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.010907
Total gradient norm: 0.017676
=== Actor Training Debug (Iteration 1125) ===
Q mean: -34.544880
Q std: 16.993565
Actor loss: 34.548855
Action reg: 0.003977
  l1.weight: grad_norm = 0.004564
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.012719
Total gradient norm: 0.023554
=== Actor Training Debug (Iteration 1126) ===
Q mean: -35.813210
Q std: 18.244146
Actor loss: 35.817188
Action reg: 0.003979
  l1.weight: grad_norm = 0.049928
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.135991
Total gradient norm: 0.227444
=== Actor Training Debug (Iteration 1127) ===
Q mean: -33.812168
Q std: 19.271437
Actor loss: 33.816135
Action reg: 0.003966
  l1.weight: grad_norm = 0.032718
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.082905
Total gradient norm: 0.141899
=== Actor Training Debug (Iteration 1128) ===
Q mean: -32.252678
Q std: 17.474720
Actor loss: 32.256668
Action reg: 0.003991
  l1.weight: grad_norm = 0.034970
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.085204
Total gradient norm: 0.140867
=== Actor Training Debug (Iteration 1129) ===
Q mean: -32.025818
Q std: 16.588385
Actor loss: 32.029816
Action reg: 0.004000
  l1.weight: grad_norm = 0.003797
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010597
Total gradient norm: 0.020982
=== Actor Training Debug (Iteration 1130) ===
Q mean: -33.267467
Q std: 17.964262
Actor loss: 33.271435
Action reg: 0.003969
  l1.weight: grad_norm = 0.045184
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.129589
Total gradient norm: 0.205610
=== Actor Training Debug (Iteration 1131) ===
Q mean: -34.647991
Q std: 18.111572
Actor loss: 34.651958
Action reg: 0.003969
  l1.weight: grad_norm = 0.002708
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.007671
Total gradient norm: 0.013649
=== Actor Training Debug (Iteration 1132) ===
Q mean: -35.397263
Q std: 18.299717
Actor loss: 35.401241
Action reg: 0.003980
  l1.weight: grad_norm = 0.010270
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.028305
Total gradient norm: 0.042717
=== Actor Training Debug (Iteration 1133) ===
Q mean: -31.965721
Q std: 16.751995
Actor loss: 31.969707
Action reg: 0.003986
  l1.weight: grad_norm = 0.005520
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.013272
Total gradient norm: 0.017685
=== Actor Training Debug (Iteration 1134) ===
Q mean: -36.423042
Q std: 18.616327
Actor loss: 36.427025
Action reg: 0.003983
  l1.weight: grad_norm = 0.085912
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.225786
Total gradient norm: 0.392956
=== Actor Training Debug (Iteration 1135) ===
Q mean: -33.574348
Q std: 18.785715
Actor loss: 33.578297
Action reg: 0.003947
  l1.weight: grad_norm = 0.011372
  l1.bias: grad_norm = 0.000588
  l2.weight: grad_norm = 0.028113
Total gradient norm: 0.037920
=== Actor Training Debug (Iteration 1136) ===
Q mean: -31.656052
Q std: 18.035587
Actor loss: 31.660038
Action reg: 0.003986
  l1.weight: grad_norm = 0.042152
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.106737
Total gradient norm: 0.181008
=== Actor Training Debug (Iteration 1137) ===
Q mean: -35.011703
Q std: 18.154865
Actor loss: 35.015690
Action reg: 0.003987
  l1.weight: grad_norm = 0.017517
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.049760
Total gradient norm: 0.085406
=== Actor Training Debug (Iteration 1138) ===
Q mean: -34.297291
Q std: 17.200727
Actor loss: 34.301250
Action reg: 0.003961
  l1.weight: grad_norm = 0.012569
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.034493
Total gradient norm: 0.060130
=== Actor Training Debug (Iteration 1139) ===
Q mean: -33.454041
Q std: 15.787217
Actor loss: 33.458023
Action reg: 0.003983
  l1.weight: grad_norm = 0.072085
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.225984
Total gradient norm: 0.404781
=== Actor Training Debug (Iteration 1140) ===
Q mean: -31.950834
Q std: 17.381905
Actor loss: 31.954805
Action reg: 0.003971
  l1.weight: grad_norm = 0.011688
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.030331
Total gradient norm: 0.054267
=== Actor Training Debug (Iteration 1141) ===
Q mean: -33.048630
Q std: 19.163982
Actor loss: 33.052601
Action reg: 0.003970
  l1.weight: grad_norm = 0.006060
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.013712
Total gradient norm: 0.024620
=== Actor Training Debug (Iteration 1142) ===
Q mean: -35.336403
Q std: 19.366838
Actor loss: 35.340389
Action reg: 0.003985
  l1.weight: grad_norm = 0.023803
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.064733
Total gradient norm: 0.105530
=== Actor Training Debug (Iteration 1143) ===
Q mean: -34.859165
Q std: 18.960705
Actor loss: 34.863140
Action reg: 0.003976
  l1.weight: grad_norm = 0.000315
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.000953
Total gradient norm: 0.002128
=== Actor Training Debug (Iteration 1144) ===
Q mean: -36.513546
Q std: 18.058367
Actor loss: 36.517517
Action reg: 0.003973
  l1.weight: grad_norm = 0.063715
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.170053
Total gradient norm: 0.280113
=== Actor Training Debug (Iteration 1145) ===
Q mean: -36.274757
Q std: 19.355616
Actor loss: 36.278748
Action reg: 0.003991
  l1.weight: grad_norm = 0.012494
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.037201
Total gradient norm: 0.065398
=== Actor Training Debug (Iteration 1146) ===
Q mean: -33.445202
Q std: 18.729782
Actor loss: 33.449192
Action reg: 0.003991
  l1.weight: grad_norm = 0.018906
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.055741
Total gradient norm: 0.097448
=== Actor Training Debug (Iteration 1147) ===
Q mean: -32.329010
Q std: 18.055292
Actor loss: 32.332996
Action reg: 0.003987
  l1.weight: grad_norm = 0.007554
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.017458
Total gradient norm: 0.029643
=== Actor Training Debug (Iteration 1148) ===
Q mean: -35.137009
Q std: 19.486439
Actor loss: 35.140976
Action reg: 0.003965
  l1.weight: grad_norm = 0.037361
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.109593
Total gradient norm: 0.181284
=== Actor Training Debug (Iteration 1149) ===
Q mean: -34.876175
Q std: 18.503922
Actor loss: 34.880138
Action reg: 0.003963
  l1.weight: grad_norm = 0.002363
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.006282
Total gradient norm: 0.010845
=== Actor Training Debug (Iteration 1150) ===
Q mean: -32.467476
Q std: 17.145586
Actor loss: 32.471447
Action reg: 0.003969
  l1.weight: grad_norm = 0.008419
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.022462
Total gradient norm: 0.040865
=== Actor Training Debug (Iteration 1151) ===
Q mean: -31.642534
Q std: 16.355379
Actor loss: 31.646505
Action reg: 0.003971
  l1.weight: grad_norm = 0.008955
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.021914
Total gradient norm: 0.029394
=== Actor Training Debug (Iteration 1152) ===
Q mean: -35.110687
Q std: 16.229000
Actor loss: 35.114689
Action reg: 0.004000
  l1.weight: grad_norm = 0.004328
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.010926
Total gradient norm: 0.016401
=== Actor Training Debug (Iteration 1153) ===
Q mean: -35.313614
Q std: 18.526491
Actor loss: 35.317585
Action reg: 0.003972
  l1.weight: grad_norm = 0.001282
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.003654
Total gradient norm: 0.006564
=== Actor Training Debug (Iteration 1154) ===
Q mean: -32.885223
Q std: 16.202307
Actor loss: 32.889210
Action reg: 0.003988
  l1.weight: grad_norm = 0.001259
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.003035
Total gradient norm: 0.005313
=== Actor Training Debug (Iteration 1155) ===
Q mean: -32.496700
Q std: 18.726154
Actor loss: 32.500687
Action reg: 0.003985
  l1.weight: grad_norm = 0.021795
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.053715
Total gradient norm: 0.086856
=== Actor Training Debug (Iteration 1156) ===
Q mean: -36.141258
Q std: 18.253990
Actor loss: 36.145241
Action reg: 0.003984
  l1.weight: grad_norm = 0.008399
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.023459
Total gradient norm: 0.036715
=== Actor Training Debug (Iteration 1157) ===
Q mean: -35.627800
Q std: 18.167112
Actor loss: 35.631783
Action reg: 0.003984
  l1.weight: grad_norm = 0.000289
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.000758
Total gradient norm: 0.001654
=== Actor Training Debug (Iteration 1158) ===
Q mean: -33.106327
Q std: 17.104856
Actor loss: 33.110306
Action reg: 0.003981
  l1.weight: grad_norm = 0.004951
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.012505
Total gradient norm: 0.021301
=== Actor Training Debug (Iteration 1159) ===
Q mean: -34.642853
Q std: 17.100344
Actor loss: 34.646851
Action reg: 0.003997
  l1.weight: grad_norm = 0.033651
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.087592
Total gradient norm: 0.138340
=== Actor Training Debug (Iteration 1160) ===
Q mean: -36.302353
Q std: 19.727345
Actor loss: 36.306335
Action reg: 0.003984
  l1.weight: grad_norm = 0.019753
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.051332
Total gradient norm: 0.090327
=== Actor Training Debug (Iteration 1161) ===
Q mean: -35.186760
Q std: 18.264521
Actor loss: 35.190739
Action reg: 0.003979
  l1.weight: grad_norm = 0.001068
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.002958
Total gradient norm: 0.005739
=== Actor Training Debug (Iteration 1162) ===
Q mean: -32.852406
Q std: 19.432213
Actor loss: 32.856365
Action reg: 0.003958
  l1.weight: grad_norm = 0.041665
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.101374
Total gradient norm: 0.156420
=== Actor Training Debug (Iteration 1163) ===
Q mean: -32.405861
Q std: 17.801579
Actor loss: 32.409832
Action reg: 0.003969
  l1.weight: grad_norm = 0.015378
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.045553
Total gradient norm: 0.080503
=== Actor Training Debug (Iteration 1164) ===
Q mean: -34.152496
Q std: 19.931738
Actor loss: 34.156456
Action reg: 0.003960
  l1.weight: grad_norm = 0.000492
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.001840
Total gradient norm: 0.004535
=== Actor Training Debug (Iteration 1165) ===
Q mean: -34.803566
Q std: 17.877644
Actor loss: 34.807545
Action reg: 0.003980
  l1.weight: grad_norm = 0.045597
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.117681
Total gradient norm: 0.193515
=== Actor Training Debug (Iteration 1166) ===
Q mean: -30.943573
Q std: 17.222219
Actor loss: 30.947536
Action reg: 0.003964
  l1.weight: grad_norm = 0.002514
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.007566
Total gradient norm: 0.014147
=== Actor Training Debug (Iteration 1167) ===
Q mean: -32.371582
Q std: 17.735989
Actor loss: 32.375557
Action reg: 0.003974
  l1.weight: grad_norm = 0.078779
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.196941
Total gradient norm: 0.318426
=== Actor Training Debug (Iteration 1168) ===
Q mean: -35.655441
Q std: 18.929308
Actor loss: 35.659424
Action reg: 0.003984
  l1.weight: grad_norm = 0.001624
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.004623
Total gradient norm: 0.007069
=== Actor Training Debug (Iteration 1169) ===
Q mean: -35.499084
Q std: 21.192802
Actor loss: 35.503059
Action reg: 0.003976
  l1.weight: grad_norm = 0.028716
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.086019
Total gradient norm: 0.170882
=== Actor Training Debug (Iteration 1170) ===
Q mean: -34.237736
Q std: 19.424202
Actor loss: 34.241714
Action reg: 0.003977
  l1.weight: grad_norm = 0.003065
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.008323
Total gradient norm: 0.015648
=== Actor Training Debug (Iteration 1171) ===
Q mean: -33.098373
Q std: 18.827358
Actor loss: 33.102360
Action reg: 0.003988
  l1.weight: grad_norm = 0.038691
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.121368
Total gradient norm: 0.232435
=== Actor Training Debug (Iteration 1172) ===
Q mean: -35.992348
Q std: 19.648218
Actor loss: 35.996330
Action reg: 0.003984
  l1.weight: grad_norm = 0.005460
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.013067
Total gradient norm: 0.017294
=== Actor Training Debug (Iteration 1173) ===
Q mean: -34.829903
Q std: 16.856823
Actor loss: 34.833889
Action reg: 0.003986
  l1.weight: grad_norm = 0.007453
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.019083
Total gradient norm: 0.030807
=== Actor Training Debug (Iteration 1174) ===
Q mean: -32.479355
Q std: 16.065140
Actor loss: 32.483330
Action reg: 0.003974
  l1.weight: grad_norm = 0.066420
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.159560
Total gradient norm: 0.222629
=== Actor Training Debug (Iteration 1175) ===
Q mean: -34.223862
Q std: 17.564697
Actor loss: 34.227848
Action reg: 0.003986
  l1.weight: grad_norm = 0.001167
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.003266
Total gradient norm: 0.005332
=== Actor Training Debug (Iteration 1176) ===
Q mean: -36.602699
Q std: 19.519226
Actor loss: 36.606678
Action reg: 0.003980
  l1.weight: grad_norm = 0.042921
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.110612
Total gradient norm: 0.171558
=== Actor Training Debug (Iteration 1177) ===
Q mean: -35.842422
Q std: 17.270987
Actor loss: 35.846405
Action reg: 0.003982
  l1.weight: grad_norm = 0.027884
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.074806
Total gradient norm: 0.119724
=== Actor Training Debug (Iteration 1178) ===
Q mean: -32.655075
Q std: 17.065010
Actor loss: 32.659069
Action reg: 0.003994
  l1.weight: grad_norm = 0.039095
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.116129
Total gradient norm: 0.229054
=== Actor Training Debug (Iteration 1179) ===
Q mean: -34.243580
Q std: 19.583193
Actor loss: 34.247555
Action reg: 0.003974
  l1.weight: grad_norm = 0.014144
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.041148
Total gradient norm: 0.081669
=== Actor Training Debug (Iteration 1180) ===
Q mean: -36.537407
Q std: 19.489233
Actor loss: 36.541397
Action reg: 0.003991
  l1.weight: grad_norm = 0.044414
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.120487
Total gradient norm: 0.207097
=== Actor Training Debug (Iteration 1181) ===
Q mean: -35.468643
Q std: 18.626249
Actor loss: 35.472614
Action reg: 0.003973
  l1.weight: grad_norm = 0.003083
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.009347
Total gradient norm: 0.017021
=== Actor Training Debug (Iteration 1182) ===
Q mean: -33.515339
Q std: 19.158361
Actor loss: 33.519310
Action reg: 0.003970
  l1.weight: grad_norm = 0.026307
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.080138
Total gradient norm: 0.161211
=== Actor Training Debug (Iteration 1183) ===
Q mean: -30.888700
Q std: 16.784836
Actor loss: 30.892694
Action reg: 0.003993
  l1.weight: grad_norm = 0.002863
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.007398
Total gradient norm: 0.011525
=== Actor Training Debug (Iteration 1184) ===
Q mean: -35.109108
Q std: 18.837019
Actor loss: 35.113094
Action reg: 0.003985
  l1.weight: grad_norm = 0.034913
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.095070
Total gradient norm: 0.174672
=== Actor Training Debug (Iteration 1185) ===
Q mean: -37.671005
Q std: 17.096670
Actor loss: 37.674988
Action reg: 0.003984
  l1.weight: grad_norm = 0.044333
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.119500
Total gradient norm: 0.165664
=== Actor Training Debug (Iteration 1186) ===
Q mean: -36.119492
Q std: 19.973898
Actor loss: 36.123474
Action reg: 0.003982
  l1.weight: grad_norm = 0.050275
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.138757
Total gradient norm: 0.224399
=== Actor Training Debug (Iteration 1187) ===
Q mean: -33.600716
Q std: 17.435020
Actor loss: 33.604702
Action reg: 0.003984
  l1.weight: grad_norm = 0.061777
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.160552
Total gradient norm: 0.257212
=== Actor Training Debug (Iteration 1188) ===
Q mean: -32.357769
Q std: 18.617907
Actor loss: 32.361732
Action reg: 0.003964
  l1.weight: grad_norm = 0.010429
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.026452
Total gradient norm: 0.045110
=== Actor Training Debug (Iteration 1189) ===
Q mean: -33.483719
Q std: 16.138811
Actor loss: 33.487709
Action reg: 0.003991
  l1.weight: grad_norm = 0.004102
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.011607
Total gradient norm: 0.019811
=== Actor Training Debug (Iteration 1190) ===
Q mean: -34.243332
Q std: 19.394379
Actor loss: 34.247303
Action reg: 0.003972
  l1.weight: grad_norm = 0.003023
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.009339
Total gradient norm: 0.016298
=== Actor Training Debug (Iteration 1191) ===
Q mean: -34.816101
Q std: 17.775591
Actor loss: 34.820095
Action reg: 0.003992
  l1.weight: grad_norm = 0.005555
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.016455
Total gradient norm: 0.027902
=== Actor Training Debug (Iteration 1192) ===
Q mean: -33.808640
Q std: 17.170099
Actor loss: 33.812614
Action reg: 0.003975
  l1.weight: grad_norm = 0.018579
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.061153
Total gradient norm: 0.127447
=== Actor Training Debug (Iteration 1193) ===
Q mean: -35.382019
Q std: 17.925688
Actor loss: 35.386013
Action reg: 0.003993
  l1.weight: grad_norm = 0.006925
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.020034
Total gradient norm: 0.033911
=== Actor Training Debug (Iteration 1194) ===
Q mean: -33.516491
Q std: 20.439470
Actor loss: 33.520481
Action reg: 0.003988
  l1.weight: grad_norm = 0.002764
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.006437
Total gradient norm: 0.009069
=== Actor Training Debug (Iteration 1195) ===
Q mean: -33.240276
Q std: 17.911978
Actor loss: 33.244267
Action reg: 0.003990
  l1.weight: grad_norm = 0.023753
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.066341
Total gradient norm: 0.117428
=== Actor Training Debug (Iteration 1196) ===
Q mean: -36.095070
Q std: 18.399385
Actor loss: 36.099064
Action reg: 0.003996
  l1.weight: grad_norm = 0.013952
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.044056
Total gradient norm: 0.081068
=== Actor Training Debug (Iteration 1197) ===
Q mean: -36.116222
Q std: 18.008192
Actor loss: 36.120209
Action reg: 0.003986
  l1.weight: grad_norm = 0.000404
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.001038
Total gradient norm: 0.002096
=== Actor Training Debug (Iteration 1198) ===
Q mean: -34.931419
Q std: 17.176517
Actor loss: 34.935402
Action reg: 0.003983
  l1.weight: grad_norm = 0.068996
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.189067
Total gradient norm: 0.341737
=== Actor Training Debug (Iteration 1199) ===
Q mean: -34.209251
Q std: 18.148476
Actor loss: 34.213238
Action reg: 0.003985
  l1.weight: grad_norm = 0.017265
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.049490
Total gradient norm: 0.092797
=== Actor Training Debug (Iteration 1200) ===
Q mean: -36.419857
Q std: 17.387579
Actor loss: 36.423840
Action reg: 0.003984
  l1.weight: grad_norm = 0.020728
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.050554
Total gradient norm: 0.079316
=== Actor Training Debug (Iteration 1201) ===
Q mean: -37.234245
Q std: 18.992487
Actor loss: 37.238243
Action reg: 0.003999
  l1.weight: grad_norm = 0.021586
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.059864
Total gradient norm: 0.095000
=== Actor Training Debug (Iteration 1202) ===
Q mean: -33.503517
Q std: 16.771721
Actor loss: 33.507500
Action reg: 0.003982
  l1.weight: grad_norm = 0.012166
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.035312
Total gradient norm: 0.057509
=== Actor Training Debug (Iteration 1203) ===
Q mean: -34.633049
Q std: 17.739607
Actor loss: 34.637043
Action reg: 0.003994
  l1.weight: grad_norm = 0.000301
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.000734
Total gradient norm: 0.001220
=== Actor Training Debug (Iteration 1204) ===
Q mean: -36.792244
Q std: 19.700348
Actor loss: 36.796227
Action reg: 0.003984
  l1.weight: grad_norm = 0.087500
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.245741
Total gradient norm: 0.420677
=== Actor Training Debug (Iteration 1205) ===
Q mean: -38.367165
Q std: 16.682289
Actor loss: 38.371159
Action reg: 0.003992
  l1.weight: grad_norm = 0.009146
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.024638
Total gradient norm: 0.040391
=== Actor Training Debug (Iteration 1206) ===
Q mean: -34.511475
Q std: 18.585468
Actor loss: 34.515434
Action reg: 0.003960
  l1.weight: grad_norm = 0.026498
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.075480
Total gradient norm: 0.133573
=== Actor Training Debug (Iteration 1207) ===
Q mean: -33.320778
Q std: 17.766426
Actor loss: 33.324741
Action reg: 0.003965
  l1.weight: grad_norm = 0.006561
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.018970
Total gradient norm: 0.032637
=== Actor Training Debug (Iteration 1208) ===
Q mean: -32.967072
Q std: 16.904467
Actor loss: 32.971054
Action reg: 0.003984
  l1.weight: grad_norm = 0.016579
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.052690
Total gradient norm: 0.094531
=== Actor Training Debug (Iteration 1209) ===
Q mean: -40.444603
Q std: 20.625444
Actor loss: 40.448589
Action reg: 0.003985
  l1.weight: grad_norm = 0.068841
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.175970
Total gradient norm: 0.294005
=== Actor Training Debug (Iteration 1210) ===
Q mean: -37.722572
Q std: 20.446789
Actor loss: 37.726536
Action reg: 0.003963
  l1.weight: grad_norm = 0.008978
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.025387
Total gradient norm: 0.047861
=== Actor Training Debug (Iteration 1211) ===
Q mean: -35.372097
Q std: 16.671274
Actor loss: 35.376076
Action reg: 0.003979
  l1.weight: grad_norm = 0.018572
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.048244
Total gradient norm: 0.069540
=== Actor Training Debug (Iteration 1212) ===
Q mean: -32.039688
Q std: 16.165747
Actor loss: 32.043678
Action reg: 0.003992
  l1.weight: grad_norm = 0.020230
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.055039
Total gradient norm: 0.091108
=== Actor Training Debug (Iteration 1213) ===
Q mean: -35.618919
Q std: 17.747400
Actor loss: 35.622898
Action reg: 0.003977
  l1.weight: grad_norm = 0.006473
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.018071
Total gradient norm: 0.029316
=== Actor Training Debug (Iteration 1214) ===
Q mean: -36.253876
Q std: 18.364546
Actor loss: 36.257843
Action reg: 0.003969
  l1.weight: grad_norm = 0.004693
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.014403
Total gradient norm: 0.027563
=== Actor Training Debug (Iteration 1215) ===
Q mean: -35.263557
Q std: 17.400009
Actor loss: 35.267521
Action reg: 0.003963
  l1.weight: grad_norm = 0.032976
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.088787
Total gradient norm: 0.149023
=== Actor Training Debug (Iteration 1216) ===
Q mean: -32.399460
Q std: 16.380262
Actor loss: 32.403450
Action reg: 0.003990
  l1.weight: grad_norm = 0.012756
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.030046
Total gradient norm: 0.039128
=== Actor Training Debug (Iteration 1217) ===
Q mean: -37.374092
Q std: 19.893885
Actor loss: 37.378075
Action reg: 0.003984
  l1.weight: grad_norm = 0.055646
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.152910
Total gradient norm: 0.200322
=== Actor Training Debug (Iteration 1218) ===
Q mean: -34.807236
Q std: 19.552631
Actor loss: 34.811199
Action reg: 0.003962
  l1.weight: grad_norm = 0.049812
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.118087
Total gradient norm: 0.205352
=== Actor Training Debug (Iteration 1219) ===
Q mean: -32.195782
Q std: 17.723825
Actor loss: 32.199741
Action reg: 0.003961
  l1.weight: grad_norm = 0.022203
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.058849
Total gradient norm: 0.099381
=== Actor Training Debug (Iteration 1220) ===
Q mean: -35.021156
Q std: 17.960529
Actor loss: 35.025139
Action reg: 0.003981
  l1.weight: grad_norm = 0.092541
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.238106
Total gradient norm: 0.312462
=== Actor Training Debug (Iteration 1221) ===
Q mean: -34.415108
Q std: 18.197796
Actor loss: 34.419064
Action reg: 0.003956
  l1.weight: grad_norm = 0.006245
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.020486
Total gradient norm: 0.040299
=== Actor Training Debug (Iteration 1222) ===
Q mean: -35.196953
Q std: 18.360546
Actor loss: 35.200939
Action reg: 0.003985
  l1.weight: grad_norm = 0.018632
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.056954
Total gradient norm: 0.094933
=== Actor Training Debug (Iteration 1223) ===
Q mean: -33.318520
Q std: 17.857201
Actor loss: 33.322472
Action reg: 0.003954
  l1.weight: grad_norm = 0.061683
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.151149
Total gradient norm: 0.251567
=== Actor Training Debug (Iteration 1224) ===
Q mean: -31.468435
Q std: 18.367245
Actor loss: 31.472424
Action reg: 0.003989
  l1.weight: grad_norm = 0.000276
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.000734
Total gradient norm: 0.001318
=== Actor Training Debug (Iteration 1225) ===
Q mean: -35.301247
Q std: 16.207294
Actor loss: 35.305237
Action reg: 0.003990
  l1.weight: grad_norm = 0.007359
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.021843
Total gradient norm: 0.038842
=== Actor Training Debug (Iteration 1226) ===
Q mean: -35.756664
Q std: 18.117064
Actor loss: 35.760639
Action reg: 0.003975
  l1.weight: grad_norm = 0.000375
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.001073
Total gradient norm: 0.002421
=== Actor Training Debug (Iteration 1227) ===
Q mean: -36.682926
Q std: 17.666883
Actor loss: 36.686901
Action reg: 0.003976
  l1.weight: grad_norm = 0.049580
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.144407
Total gradient norm: 0.255201
=== Actor Training Debug (Iteration 1228) ===
Q mean: -32.844891
Q std: 16.446123
Actor loss: 32.848854
Action reg: 0.003965
  l1.weight: grad_norm = 0.071387
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.187131
Total gradient norm: 0.314205
=== Actor Training Debug (Iteration 1229) ===
Q mean: -33.540024
Q std: 17.133013
Actor loss: 33.544014
Action reg: 0.003990
  l1.weight: grad_norm = 0.018011
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.044030
Total gradient norm: 0.073015
=== Actor Training Debug (Iteration 1230) ===
Q mean: -33.913757
Q std: 17.979725
Actor loss: 33.917717
Action reg: 0.003958
  l1.weight: grad_norm = 0.003534
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.010328
Total gradient norm: 0.020754
=== Actor Training Debug (Iteration 1231) ===
Q mean: -37.878334
Q std: 18.921139
Actor loss: 37.882309
Action reg: 0.003974
  l1.weight: grad_norm = 0.088185
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.215517
Total gradient norm: 0.345667
=== Actor Training Debug (Iteration 1232) ===
Q mean: -36.446678
Q std: 18.186140
Actor loss: 36.450657
Action reg: 0.003978
  l1.weight: grad_norm = 0.000380
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.001045
Total gradient norm: 0.002482
=== Actor Training Debug (Iteration 1233) ===
Q mean: -33.737244
Q std: 17.937048
Actor loss: 33.741234
Action reg: 0.003991
  l1.weight: grad_norm = 0.021665
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.061579
Total gradient norm: 0.107504
=== Actor Training Debug (Iteration 1234) ===
Q mean: -32.916801
Q std: 18.877724
Actor loss: 32.920795
Action reg: 0.003995
  l1.weight: grad_norm = 0.044108
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.119337
Total gradient norm: 0.191461
=== Actor Training Debug (Iteration 1235) ===
Q mean: -36.840744
Q std: 17.755331
Actor loss: 36.844734
Action reg: 0.003992
  l1.weight: grad_norm = 0.004927
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.014030
Total gradient norm: 0.026087
=== Actor Training Debug (Iteration 1236) ===
Q mean: -37.328320
Q std: 16.941141
Actor loss: 37.332302
Action reg: 0.003981
  l1.weight: grad_norm = 0.040164
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.135452
Total gradient norm: 0.237386
=== Actor Training Debug (Iteration 1237) ===
Q mean: -34.789940
Q std: 16.925011
Actor loss: 34.793915
Action reg: 0.003975
  l1.weight: grad_norm = 0.041342
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.103560
Total gradient norm: 0.145295
=== Actor Training Debug (Iteration 1238) ===
Q mean: -35.276886
Q std: 17.669050
Actor loss: 35.280865
Action reg: 0.003978
  l1.weight: grad_norm = 0.007636
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.016171
Total gradient norm: 0.021702
=== Actor Training Debug (Iteration 1239) ===
Q mean: -37.270214
Q std: 16.826410
Actor loss: 37.274212
Action reg: 0.003999
  l1.weight: grad_norm = 0.005822
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.015897
Total gradient norm: 0.026850
=== Actor Training Debug (Iteration 1240) ===
Q mean: -35.873169
Q std: 17.506727
Actor loss: 35.877151
Action reg: 0.003982
  l1.weight: grad_norm = 0.013209
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.035307
Total gradient norm: 0.051611
=== Actor Training Debug (Iteration 1241) ===
Q mean: -34.524059
Q std: 16.929529
Actor loss: 34.528049
Action reg: 0.003990
  l1.weight: grad_norm = 0.009589
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.028903
Total gradient norm: 0.047297
=== Actor Training Debug (Iteration 1242) ===
Q mean: -33.754353
Q std: 17.630325
Actor loss: 33.758339
Action reg: 0.003985
  l1.weight: grad_norm = 0.101757
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.282259
Total gradient norm: 0.573833
=== Actor Training Debug (Iteration 1243) ===
Q mean: -35.547401
Q std: 16.501263
Actor loss: 35.551384
Action reg: 0.003984
  l1.weight: grad_norm = 0.011464
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.034674
Total gradient norm: 0.062164
=== Actor Training Debug (Iteration 1244) ===
Q mean: -35.462845
Q std: 16.964024
Actor loss: 35.466801
Action reg: 0.003957
  l1.weight: grad_norm = 0.190160
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.557864
Total gradient norm: 0.957613
=== Actor Training Debug (Iteration 1245) ===
Q mean: -33.217110
Q std: 17.753733
Actor loss: 33.221096
Action reg: 0.003987
  l1.weight: grad_norm = 0.002358
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.006488
Total gradient norm: 0.011736
=== Actor Training Debug (Iteration 1246) ===
Q mean: -32.666065
Q std: 17.820992
Actor loss: 32.670052
Action reg: 0.003985
  l1.weight: grad_norm = 0.000394
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.001022
Total gradient norm: 0.002391
=== Actor Training Debug (Iteration 1247) ===
Q mean: -35.276009
Q std: 19.767391
Actor loss: 35.279976
Action reg: 0.003968
  l1.weight: grad_norm = 0.038859
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.120689
Total gradient norm: 0.186184
=== Actor Training Debug (Iteration 1248) ===
Q mean: -37.386311
Q std: 20.564219
Actor loss: 37.390289
Action reg: 0.003979
  l1.weight: grad_norm = 0.001037
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.002486
Total gradient norm: 0.004577
=== Actor Training Debug (Iteration 1249) ===
Q mean: -34.356285
Q std: 17.636553
Actor loss: 34.360275
Action reg: 0.003989
  l1.weight: grad_norm = 0.039333
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.106662
Total gradient norm: 0.173135
=== Actor Training Debug (Iteration 1250) ===
Q mean: -36.636406
Q std: 16.673616
Actor loss: 36.640388
Action reg: 0.003984
  l1.weight: grad_norm = 0.050150
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.151728
Total gradient norm: 0.247662
=== Actor Training Debug (Iteration 1251) ===
Q mean: -35.380207
Q std: 18.771038
Actor loss: 35.384186
Action reg: 0.003979
  l1.weight: grad_norm = 0.027313
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.075410
Total gradient norm: 0.128972
=== Actor Training Debug (Iteration 1252) ===
Q mean: -33.081123
Q std: 19.039530
Actor loss: 33.085091
Action reg: 0.003966
  l1.weight: grad_norm = 0.019331
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.058871
Total gradient norm: 0.109990
=== Actor Training Debug (Iteration 1253) ===
Q mean: -34.120125
Q std: 18.863405
Actor loss: 34.124111
Action reg: 0.003985
  l1.weight: grad_norm = 0.045322
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.127448
Total gradient norm: 0.235206
=== Actor Training Debug (Iteration 1254) ===
Q mean: -34.341560
Q std: 17.962292
Actor loss: 34.345543
Action reg: 0.003984
  l1.weight: grad_norm = 0.031690
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.073062
Total gradient norm: 0.118376
=== Actor Training Debug (Iteration 1255) ===
Q mean: -36.833450
Q std: 18.307190
Actor loss: 36.837448
Action reg: 0.003998
  l1.weight: grad_norm = 0.012698
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.035478
Total gradient norm: 0.056871
=== Actor Training Debug (Iteration 1256) ===
Q mean: -36.008881
Q std: 17.041874
Actor loss: 36.012856
Action reg: 0.003977
  l1.weight: grad_norm = 0.100277
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.255304
Total gradient norm: 0.429839
=== Actor Training Debug (Iteration 1257) ===
Q mean: -33.843246
Q std: 17.500019
Actor loss: 33.847221
Action reg: 0.003976
  l1.weight: grad_norm = 0.048640
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.109415
Total gradient norm: 0.187482
=== Actor Training Debug (Iteration 1258) ===
Q mean: -32.878448
Q std: 16.913481
Actor loss: 32.882431
Action reg: 0.003984
  l1.weight: grad_norm = 0.006532
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.016268
Total gradient norm: 0.031286
=== Actor Training Debug (Iteration 1259) ===
Q mean: -37.972382
Q std: 17.595745
Actor loss: 37.976349
Action reg: 0.003966
  l1.weight: grad_norm = 0.041282
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.118058
Total gradient norm: 0.221439
=== Actor Training Debug (Iteration 1260) ===
Q mean: -37.916649
Q std: 17.324476
Actor loss: 37.920635
Action reg: 0.003985
  l1.weight: grad_norm = 0.000774
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.002378
Total gradient norm: 0.004976
=== Actor Training Debug (Iteration 1261) ===
Q mean: -35.476254
Q std: 18.458399
Actor loss: 35.480247
Action reg: 0.003993
  l1.weight: grad_norm = 0.005402
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.014890
Total gradient norm: 0.027388
=== Actor Training Debug (Iteration 1262) ===
Q mean: -32.439644
Q std: 17.605207
Actor loss: 32.443626
Action reg: 0.003983
  l1.weight: grad_norm = 0.028325
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.069887
Total gradient norm: 0.122117
=== Actor Training Debug (Iteration 1263) ===
Q mean: -34.494820
Q std: 17.587851
Actor loss: 34.498806
Action reg: 0.003985
  l1.weight: grad_norm = 0.043771
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.109549
Total gradient norm: 0.198708
=== Actor Training Debug (Iteration 1264) ===
Q mean: -35.774162
Q std: 17.402342
Actor loss: 35.778145
Action reg: 0.003982
  l1.weight: grad_norm = 0.019372
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.049852
Total gradient norm: 0.082266
=== Actor Training Debug (Iteration 1265) ===
Q mean: -34.886189
Q std: 17.497023
Actor loss: 34.890175
Action reg: 0.003987
  l1.weight: grad_norm = 0.131879
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.322029
Total gradient norm: 0.442345
=== Actor Training Debug (Iteration 1266) ===
Q mean: -35.647865
Q std: 16.953184
Actor loss: 35.651859
Action reg: 0.003994
  l1.weight: grad_norm = 0.052704
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.130098
Total gradient norm: 0.216375
=== Actor Training Debug (Iteration 1267) ===
Q mean: -35.278652
Q std: 19.365326
Actor loss: 35.282646
Action reg: 0.003993
  l1.weight: grad_norm = 0.015435
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.040661
Total gradient norm: 0.066812
=== Actor Training Debug (Iteration 1268) ===
Q mean: -33.598236
Q std: 16.961651
Actor loss: 33.602222
Action reg: 0.003988
  l1.weight: grad_norm = 0.000245
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.000704
Total gradient norm: 0.001563
=== Actor Training Debug (Iteration 1269) ===
Q mean: -34.583870
Q std: 16.976419
Actor loss: 34.587852
Action reg: 0.003983
  l1.weight: grad_norm = 0.058222
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.140467
Total gradient norm: 0.199696
=== Actor Training Debug (Iteration 1270) ===
Q mean: -36.979645
Q std: 17.489536
Actor loss: 36.983612
Action reg: 0.003968
  l1.weight: grad_norm = 0.001937
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.005126
Total gradient norm: 0.008671
=== Actor Training Debug (Iteration 1271) ===
Q mean: -34.194267
Q std: 18.520744
Actor loss: 34.198254
Action reg: 0.003985
  l1.weight: grad_norm = 0.008354
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.021857
Total gradient norm: 0.036436
=== Actor Training Debug (Iteration 1272) ===
Q mean: -35.315582
Q std: 18.008755
Actor loss: 35.319572
Action reg: 0.003991
  l1.weight: grad_norm = 0.030821
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.080498
Total gradient norm: 0.136958
=== Actor Training Debug (Iteration 1273) ===
Q mean: -35.927940
Q std: 18.762630
Actor loss: 35.931915
Action reg: 0.003977
  l1.weight: grad_norm = 0.008697
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.022985
Total gradient norm: 0.039006
=== Actor Training Debug (Iteration 1274) ===
Q mean: -36.134899
Q std: 20.376268
Actor loss: 36.138889
Action reg: 0.003990
  l1.weight: grad_norm = 0.011226
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.029598
Total gradient norm: 0.053331
=== Actor Training Debug (Iteration 1275) ===
Q mean: -33.620419
Q std: 18.792511
Actor loss: 33.624401
Action reg: 0.003981
  l1.weight: grad_norm = 0.041782
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.108938
Total gradient norm: 0.186611
=== Actor Training Debug (Iteration 1276) ===
Q mean: -33.927460
Q std: 20.122759
Actor loss: 33.931431
Action reg: 0.003971
  l1.weight: grad_norm = 0.018284
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.058694
Total gradient norm: 0.116725
=== Actor Training Debug (Iteration 1277) ===
Q mean: -36.526505
Q std: 17.462599
Actor loss: 36.530476
Action reg: 0.003971
  l1.weight: grad_norm = 0.026781
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.065931
Total gradient norm: 0.092914
=== Actor Training Debug (Iteration 1278) ===
Q mean: -39.816719
Q std: 20.749863
Actor loss: 39.820683
Action reg: 0.003962
  l1.weight: grad_norm = 0.017711
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.045551
Total gradient norm: 0.075442
=== Actor Training Debug (Iteration 1279) ===
Q mean: -33.781055
Q std: 17.169607
Actor loss: 33.785030
Action reg: 0.003975
  l1.weight: grad_norm = 0.042347
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.123900
Total gradient norm: 0.222137
=== Actor Training Debug (Iteration 1280) ===
Q mean: -33.335678
Q std: 17.000134
Actor loss: 33.339676
Action reg: 0.003998
  l1.weight: grad_norm = 0.013090
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.036311
Total gradient norm: 0.069913
=== Actor Training Debug (Iteration 1281) ===
Q mean: -35.829006
Q std: 16.905481
Actor loss: 35.832993
Action reg: 0.003985
  l1.weight: grad_norm = 0.004816
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.014183
Total gradient norm: 0.027281
=== Actor Training Debug (Iteration 1282) ===
Q mean: -34.588257
Q std: 18.209246
Actor loss: 34.592220
Action reg: 0.003962
  l1.weight: grad_norm = 0.098187
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.266183
Total gradient norm: 0.455590
=== Actor Training Debug (Iteration 1283) ===
Q mean: -35.305649
Q std: 17.191698
Actor loss: 35.309631
Action reg: 0.003983
  l1.weight: grad_norm = 0.153317
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.380527
Total gradient norm: 0.705562
=== Actor Training Debug (Iteration 1284) ===
Q mean: -36.496101
Q std: 17.166006
Actor loss: 36.500088
Action reg: 0.003986
  l1.weight: grad_norm = 0.017790
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.044888
Total gradient norm: 0.076497
=== Actor Training Debug (Iteration 1285) ===
Q mean: -35.029358
Q std: 17.202139
Actor loss: 35.033329
Action reg: 0.003971
  l1.weight: grad_norm = 0.040740
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.107067
Total gradient norm: 0.164093
=== Actor Training Debug (Iteration 1286) ===
Q mean: -36.917618
Q std: 17.900295
Actor loss: 36.921600
Action reg: 0.003983
  l1.weight: grad_norm = 0.040027
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.101133
Total gradient norm: 0.145492
=== Actor Training Debug (Iteration 1287) ===
Q mean: -34.700573
Q std: 17.938761
Actor loss: 34.704540
Action reg: 0.003967
  l1.weight: grad_norm = 0.005130
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.012693
Total gradient norm: 0.020150
=== Actor Training Debug (Iteration 1288) ===
Q mean: -34.626633
Q std: 16.362597
Actor loss: 34.630615
Action reg: 0.003982
  l1.weight: grad_norm = 0.073490
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.202241
Total gradient norm: 0.328547
=== Actor Training Debug (Iteration 1289) ===
Q mean: -35.979530
Q std: 18.457209
Actor loss: 35.983471
Action reg: 0.003941
  l1.weight: grad_norm = 0.018112
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.046248
Total gradient norm: 0.083668
=== Actor Training Debug (Iteration 1290) ===
Q mean: -34.981163
Q std: 17.583290
Actor loss: 34.985146
Action reg: 0.003983
  l1.weight: grad_norm = 0.012400
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.033035
Total gradient norm: 0.060132
=== Actor Training Debug (Iteration 1291) ===
Q mean: -33.686417
Q std: 18.548660
Actor loss: 33.690403
Action reg: 0.003987
  l1.weight: grad_norm = 0.001194
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.003080
Total gradient norm: 0.005862
=== Actor Training Debug (Iteration 1292) ===
Q mean: -36.325733
Q std: 18.128693
Actor loss: 36.329720
Action reg: 0.003987
  l1.weight: grad_norm = 0.046919
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.127388
Total gradient norm: 0.225636
=== Actor Training Debug (Iteration 1293) ===
Q mean: -35.898567
Q std: 19.217442
Actor loss: 35.902546
Action reg: 0.003978
  l1.weight: grad_norm = 0.092486
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.238510
Total gradient norm: 0.386206
=== Actor Training Debug (Iteration 1294) ===
Q mean: -36.814522
Q std: 18.089840
Actor loss: 36.818516
Action reg: 0.003992
  l1.weight: grad_norm = 0.000781
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.002021
Total gradient norm: 0.003418
=== Actor Training Debug (Iteration 1295) ===
Q mean: -33.020699
Q std: 18.507870
Actor loss: 33.024677
Action reg: 0.003980
  l1.weight: grad_norm = 0.000936
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.002598
Total gradient norm: 0.005758
=== Actor Training Debug (Iteration 1296) ===
Q mean: -36.982224
Q std: 18.064587
Actor loss: 36.986210
Action reg: 0.003986
  l1.weight: grad_norm = 0.002072
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.005699
Total gradient norm: 0.011185
=== Actor Training Debug (Iteration 1297) ===
Q mean: -36.962860
Q std: 16.261002
Actor loss: 36.966850
Action reg: 0.003992
  l1.weight: grad_norm = 0.035258
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.093062
Total gradient norm: 0.198787
=== Actor Training Debug (Iteration 1298) ===
Q mean: -37.233402
Q std: 15.816904
Actor loss: 37.237377
Action reg: 0.003975
  l1.weight: grad_norm = 0.034849
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.092361
Total gradient norm: 0.176051
=== Actor Training Debug (Iteration 1299) ===
Q mean: -34.120125
Q std: 15.134285
Actor loss: 34.124104
Action reg: 0.003979
  l1.weight: grad_norm = 0.000732
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.001833
Total gradient norm: 0.003164
=== Actor Training Debug (Iteration 1300) ===
Q mean: -35.717476
Q std: 18.853855
Actor loss: 35.721455
Action reg: 0.003978
  l1.weight: grad_norm = 0.084392
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.247000
Total gradient norm: 0.422367
=== Actor Training Debug (Iteration 1301) ===
Q mean: -33.572868
Q std: 19.227972
Actor loss: 33.576859
Action reg: 0.003990
  l1.weight: grad_norm = 0.011686
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.035827
Total gradient norm: 0.066934
=== Actor Training Debug (Iteration 1302) ===
Q mean: -34.703117
Q std: 17.963114
Actor loss: 34.707100
Action reg: 0.003984
  l1.weight: grad_norm = 0.013859
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.042202
Total gradient norm: 0.076939
=== Actor Training Debug (Iteration 1303) ===
Q mean: -35.039940
Q std: 17.981684
Actor loss: 35.043896
Action reg: 0.003955
  l1.weight: grad_norm = 0.008442
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.025832
Total gradient norm: 0.051778
=== Actor Training Debug (Iteration 1304) ===
Q mean: -37.851448
Q std: 18.955687
Actor loss: 37.855431
Action reg: 0.003982
  l1.weight: grad_norm = 0.026432
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.067487
Total gradient norm: 0.121715
=== Actor Training Debug (Iteration 1305) ===
Q mean: -38.495525
Q std: 17.595015
Actor loss: 38.499508
Action reg: 0.003984
  l1.weight: grad_norm = 0.026530
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.080534
Total gradient norm: 0.145682
=== Actor Training Debug (Iteration 1306) ===
Q mean: -34.816605
Q std: 16.775648
Actor loss: 34.820572
Action reg: 0.003966
  l1.weight: grad_norm = 0.035288
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.098332
Total gradient norm: 0.163500
=== Actor Training Debug (Iteration 1307) ===
Q mean: -31.869158
Q std: 18.755127
Actor loss: 31.873137
Action reg: 0.003978
  l1.weight: grad_norm = 0.001756
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.004551
Total gradient norm: 0.007298
=== Actor Training Debug (Iteration 1308) ===
Q mean: -32.258575
Q std: 17.401167
Actor loss: 32.262554
Action reg: 0.003981
  l1.weight: grad_norm = 0.018447
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.044643
Total gradient norm: 0.059662
=== Actor Training Debug (Iteration 1309) ===
Q mean: -36.735207
Q std: 19.355852
Actor loss: 36.739174
Action reg: 0.003968
  l1.weight: grad_norm = 0.000407
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.001777
Total gradient norm: 0.005344
=== Actor Training Debug (Iteration 1310) ===
Q mean: -35.718906
Q std: 19.370752
Actor loss: 35.722866
Action reg: 0.003959
  l1.weight: grad_norm = 0.012648
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.035460
Total gradient norm: 0.059314
=== Actor Training Debug (Iteration 1311) ===
Q mean: -37.046852
Q std: 18.173609
Actor loss: 37.050812
Action reg: 0.003960
  l1.weight: grad_norm = 0.030921
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.084886
Total gradient norm: 0.141279
=== Actor Training Debug (Iteration 1312) ===
Q mean: -35.547348
Q std: 18.251379
Actor loss: 35.551334
Action reg: 0.003987
  l1.weight: grad_norm = 0.034300
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.097425
Total gradient norm: 0.185617
=== Actor Training Debug (Iteration 1313) ===
Q mean: -33.105301
Q std: 17.044317
Actor loss: 33.109287
Action reg: 0.003988
  l1.weight: grad_norm = 0.005173
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.014051
Total gradient norm: 0.023858
=== Actor Training Debug (Iteration 1314) ===
Q mean: -35.914291
Q std: 17.718277
Actor loss: 35.918262
Action reg: 0.003970
  l1.weight: grad_norm = 0.062588
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.159776
Total gradient norm: 0.238597
=== Actor Training Debug (Iteration 1315) ===
Q mean: -37.529236
Q std: 20.219812
Actor loss: 37.533218
Action reg: 0.003983
  l1.weight: grad_norm = 0.007445
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.022345
Total gradient norm: 0.040238
=== Actor Training Debug (Iteration 1316) ===
Q mean: -37.921989
Q std: 20.267056
Actor loss: 37.925949
Action reg: 0.003961
  l1.weight: grad_norm = 0.006250
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.016662
Total gradient norm: 0.031918
=== Actor Training Debug (Iteration 1317) ===
Q mean: -38.572815
Q std: 19.815313
Actor loss: 38.576813
Action reg: 0.003999
  l1.weight: grad_norm = 0.003934
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.010725
Total gradient norm: 0.018692
=== Actor Training Debug (Iteration 1318) ===
Q mean: -35.098221
Q std: 17.626642
Actor loss: 35.102215
Action reg: 0.003992
  l1.weight: grad_norm = 0.000767
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.002223
Total gradient norm: 0.004928
=== Actor Training Debug (Iteration 1319) ===
Q mean: -32.259293
Q std: 16.520266
Actor loss: 32.263260
Action reg: 0.003966
  l1.weight: grad_norm = 0.056003
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.153905
Total gradient norm: 0.244903
=== Actor Training Debug (Iteration 1320) ===
Q mean: -36.367661
Q std: 17.239174
Actor loss: 36.371651
Action reg: 0.003990
  l1.weight: grad_norm = 0.025699
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.061814
Total gradient norm: 0.090181
=== Actor Training Debug (Iteration 1321) ===
Q mean: -39.172661
Q std: 18.844667
Actor loss: 39.176617
Action reg: 0.003955
  l1.weight: grad_norm = 0.013188
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.034244
Total gradient norm: 0.051259
=== Actor Training Debug (Iteration 1322) ===
Q mean: -35.943565
Q std: 18.731382
Actor loss: 35.947533
Action reg: 0.003969
  l1.weight: grad_norm = 0.046244
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.118690
Total gradient norm: 0.162739
=== Actor Training Debug (Iteration 1323) ===
Q mean: -30.858662
Q std: 19.200197
Actor loss: 30.862648
Action reg: 0.003986
  l1.weight: grad_norm = 0.003711
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.009788
Total gradient norm: 0.015401
=== Actor Training Debug (Iteration 1324) ===
Q mean: -32.460285
Q std: 20.428368
Actor loss: 32.464279
Action reg: 0.003993
  l1.weight: grad_norm = 0.005491
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.015728
Total gradient norm: 0.029452
=== Actor Training Debug (Iteration 1325) ===
Q mean: -36.873161
Q std: 19.911598
Actor loss: 36.877136
Action reg: 0.003974
  l1.weight: grad_norm = 0.040619
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.113260
Total gradient norm: 0.209104
=== Actor Training Debug (Iteration 1326) ===
Q mean: -37.986851
Q std: 19.148092
Actor loss: 37.990826
Action reg: 0.003974
  l1.weight: grad_norm = 0.028466
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.086611
Total gradient norm: 0.176580
=== Actor Training Debug (Iteration 1327) ===
Q mean: -36.616627
Q std: 18.758579
Actor loss: 36.620613
Action reg: 0.003984
  l1.weight: grad_norm = 0.100530
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.261113
Total gradient norm: 0.460146
=== Actor Training Debug (Iteration 1328) ===
Q mean: -35.410057
Q std: 16.865017
Actor loss: 35.414043
Action reg: 0.003985
  l1.weight: grad_norm = 0.001629
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.004420
Total gradient norm: 0.007721
=== Actor Training Debug (Iteration 1329) ===
Q mean: -36.436157
Q std: 17.144798
Actor loss: 36.440151
Action reg: 0.003993
  l1.weight: grad_norm = 0.002247
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.005143
Total gradient norm: 0.007251
=== Actor Training Debug (Iteration 1330) ===
Q mean: -35.756889
Q std: 18.035618
Actor loss: 35.760868
Action reg: 0.003978
  l1.weight: grad_norm = 0.000429
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.001181
Total gradient norm: 0.002866
=== Actor Training Debug (Iteration 1331) ===
Q mean: -33.681847
Q std: 16.246941
Actor loss: 33.685833
Action reg: 0.003988
  l1.weight: grad_norm = 0.125759
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.375620
Total gradient norm: 0.754022
=== Actor Training Debug (Iteration 1332) ===
Q mean: -34.986473
Q std: 17.456837
Actor loss: 34.990452
Action reg: 0.003980
  l1.weight: grad_norm = 0.001403
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.004208
Total gradient norm: 0.007236
=== Actor Training Debug (Iteration 1333) ===
Q mean: -38.143108
Q std: 19.228003
Actor loss: 38.147091
Action reg: 0.003984
  l1.weight: grad_norm = 0.104699
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.295163
Total gradient norm: 0.496070
=== Actor Training Debug (Iteration 1334) ===
Q mean: -36.792603
Q std: 18.031239
Actor loss: 36.796574
Action reg: 0.003971
  l1.weight: grad_norm = 0.026113
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.066167
Total gradient norm: 0.106892
=== Actor Training Debug (Iteration 1335) ===
Q mean: -34.059807
Q std: 16.340464
Actor loss: 34.063786
Action reg: 0.003978
  l1.weight: grad_norm = 0.005961
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.016453
Total gradient norm: 0.027094
=== Actor Training Debug (Iteration 1336) ===
Q mean: -34.133945
Q std: 17.374798
Actor loss: 34.137905
Action reg: 0.003961
  l1.weight: grad_norm = 0.045105
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.121713
Total gradient norm: 0.195274
=== Actor Training Debug (Iteration 1337) ===
Q mean: -36.828400
Q std: 18.751419
Actor loss: 36.832394
Action reg: 0.003993
  l1.weight: grad_norm = 0.001177
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.003218
Total gradient norm: 0.005352
=== Actor Training Debug (Iteration 1338) ===
Q mean: -37.634010
Q std: 19.195255
Actor loss: 37.637974
Action reg: 0.003964
  l1.weight: grad_norm = 0.002001
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.004872
Total gradient norm: 0.007387
=== Actor Training Debug (Iteration 1339) ===
Q mean: -35.170822
Q std: 18.217842
Actor loss: 35.174809
Action reg: 0.003985
  l1.weight: grad_norm = 0.025176
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.061751
Total gradient norm: 0.089880
=== Actor Training Debug (Iteration 1340) ===
Q mean: -35.194038
Q std: 16.041063
Actor loss: 35.198032
Action reg: 0.003994
  l1.weight: grad_norm = 0.044691
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.127203
Total gradient norm: 0.223246
=== Actor Training Debug (Iteration 1341) ===
Q mean: -37.348972
Q std: 16.880009
Actor loss: 37.352955
Action reg: 0.003984
  l1.weight: grad_norm = 0.087246
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.277604
Total gradient norm: 0.559997
=== Actor Training Debug (Iteration 1342) ===
Q mean: -36.900322
Q std: 17.180073
Actor loss: 36.904308
Action reg: 0.003987
  l1.weight: grad_norm = 0.017807
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.040860
Total gradient norm: 0.054470
=== Actor Training Debug (Iteration 1343) ===
Q mean: -35.470795
Q std: 16.843081
Actor loss: 35.474777
Action reg: 0.003984
  l1.weight: grad_norm = 0.023581
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.056353
Total gradient norm: 0.086758
=== Actor Training Debug (Iteration 1344) ===
Q mean: -35.135445
Q std: 16.259462
Actor loss: 35.139416
Action reg: 0.003972
  l1.weight: grad_norm = 0.007836
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.019687
Total gradient norm: 0.037634
=== Actor Training Debug (Iteration 1345) ===
Q mean: -37.402100
Q std: 17.956406
Actor loss: 37.406067
Action reg: 0.003969
  l1.weight: grad_norm = 0.026905
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.084256
Total gradient norm: 0.171415
=== Actor Training Debug (Iteration 1346) ===
Q mean: -37.373901
Q std: 16.584757
Actor loss: 37.377892
Action reg: 0.003991
  l1.weight: grad_norm = 0.036760
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.095102
Total gradient norm: 0.146345
=== Actor Training Debug (Iteration 1347) ===
Q mean: -35.660675
Q std: 17.377319
Actor loss: 35.664665
Action reg: 0.003991
  l1.weight: grad_norm = 0.140189
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.417255
Total gradient norm: 0.856086
=== Actor Training Debug (Iteration 1348) ===
Q mean: -35.797199
Q std: 17.629818
Actor loss: 35.801178
Action reg: 0.003980
  l1.weight: grad_norm = 0.006280
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.017613
Total gradient norm: 0.035619
=== Actor Training Debug (Iteration 1349) ===
Q mean: -37.399227
Q std: 17.430984
Actor loss: 37.403187
Action reg: 0.003959
  l1.weight: grad_norm = 0.033749
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.087364
Total gradient norm: 0.157560
=== Actor Training Debug (Iteration 1350) ===
Q mean: -37.464462
Q std: 17.209742
Actor loss: 37.468452
Action reg: 0.003991
  l1.weight: grad_norm = 0.020326
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.064732
Total gradient norm: 0.136716
=== Actor Training Debug (Iteration 1351) ===
Q mean: -36.219814
Q std: 17.407757
Actor loss: 36.223797
Action reg: 0.003981
  l1.weight: grad_norm = 0.035516
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.092515
Total gradient norm: 0.123561
=== Actor Training Debug (Iteration 1352) ===
Q mean: -36.724937
Q std: 17.760775
Actor loss: 36.728928
Action reg: 0.003991
  l1.weight: grad_norm = 0.014504
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.034109
Total gradient norm: 0.047608
=== Actor Training Debug (Iteration 1353) ===
Q mean: -37.852612
Q std: 17.755997
Actor loss: 37.856571
Action reg: 0.003960
  l1.weight: grad_norm = 0.025456
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.076351
Total gradient norm: 0.122575
Q mean: -37.178917 Debug (Iteration 1203) ===
Q std: 17.548925
Actor loss: 37.182896
Action reg: 0.003979
  l1.weight: grad_norm = 0.032203
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.086956
Total gradient norm: 0.135815
=== Actor Training Debug (Iteration 1360) ===
Q mean: -37.960304
Q std: 19.694983
Actor loss: 37.964306
Action reg: 0.004000
  l1.weight: grad_norm = 0.000766
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002666
Total gradient norm: 0.005577
=== Actor Training Debug (Iteration 1361) ===
Q mean: -37.178089
Q std: 16.922007
Actor loss: 37.182087
Action reg: 0.003999
  l1.weight: grad_norm = 0.005023
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.011452
Total gradient norm: 0.015969
=== Actor Training Debug (Iteration 1362) ===
Q mean: -37.317528
Q std: 21.451569
Actor loss: 37.321518
Action reg: 0.003992
  l1.weight: grad_norm = 0.018407
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.052888
Total gradient norm: 0.097237
=== Actor Training Debug (Iteration 1363) ===
Q mean: -36.703094
Q std: 16.903036
Actor loss: 36.707081
Action reg: 0.003987
  l1.weight: grad_norm = 0.020272
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.054602
Total gradient norm: 0.092792
=== Actor Training Debug (Iteration 1364) ===
Q mean: -38.026978
Q std: 16.786615
Actor loss: 38.030956
Action reg: 0.003979
  l1.weight: grad_norm = 0.014215
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.034697
Total gradient norm: 0.061571
=== Actor Training Debug (Iteration 1365) ===
Q mean: -38.928391
Q std: 17.896252
Actor loss: 38.932365
Action reg: 0.003973
  l1.weight: grad_norm = 0.000348
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.001226
Total gradient norm: 0.003384
=== Actor Training Debug (Iteration 1366) ===
Q mean: -35.963821
Q std: 19.821775
Actor loss: 35.967796
Action reg: 0.003974
  l1.weight: grad_norm = 0.038375
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.095182
Total gradient norm: 0.137410
=== Actor Training Debug (Iteration 1367) ===
Q mean: -35.575813
Q std: 19.343788
Actor loss: 35.579803
Action reg: 0.003991
  l1.weight: grad_norm = 0.004760
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.012339
Total gradient norm: 0.019703
=== Actor Training Debug (Iteration 1368) ===
Q mean: -37.006348
Q std: 17.558668
Actor loss: 37.010319
Action reg: 0.003973
  l1.weight: grad_norm = 0.002702
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.007359
Total gradient norm: 0.012292
=== Actor Training Debug (Iteration 1369) ===
Q mean: -34.783440
Q std: 18.369347
Actor loss: 34.787418
Action reg: 0.003978
  l1.weight: grad_norm = 0.022082
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.051201
Total gradient norm: 0.070999
=== Actor Training Debug (Iteration 1370) ===
Q mean: -34.899818
Q std: 19.230087
Actor loss: 34.903801
Action reg: 0.003983
  l1.weight: grad_norm = 0.092791
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.228962
Total gradient norm: 0.382315
=== Actor Training Debug (Iteration 1371) ===
Q mean: -36.836201
Q std: 17.419958
Actor loss: 36.840187
Action reg: 0.003986
  l1.weight: grad_norm = 0.000643
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.002101
Total gradient norm: 0.003714
=== Actor Training Debug (Iteration 1372) ===
Q mean: -37.130180
Q std: 17.898735
Actor loss: 37.134167
Action reg: 0.003986
  l1.weight: grad_norm = 0.026017
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.077710
Total gradient norm: 0.136374
=== Actor Training Debug (Iteration 1373) ===
Q mean: -34.629604
Q std: 19.619465
Actor loss: 34.633587
Action reg: 0.003984
  l1.weight: grad_norm = 0.015070
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.042735
Total gradient norm: 0.073436
=== Actor Training Debug (Iteration 1374) ===
Q mean: -33.473995
Q std: 18.024126
Actor loss: 33.477989
Action reg: 0.003993
  l1.weight: grad_norm = 0.014895
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.034252
Total gradient norm: 0.046527
=== Actor Training Debug (Iteration 1375) ===
Q mean: -34.158005
Q std: 17.965748
Actor loss: 34.161991
Action reg: 0.003986
  l1.weight: grad_norm = 0.038182
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.100786
Total gradient norm: 0.174486
=== Actor Training Debug (Iteration 1376) ===
Q mean: -37.525742
Q std: 17.290060
Actor loss: 37.529732
Action reg: 0.003991
  l1.weight: grad_norm = 0.014264
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.041172
Total gradient norm: 0.079811
=== Actor Training Debug (Iteration 1377) ===
Q mean: -38.008102
Q std: 15.843251
Actor loss: 38.012070
Action reg: 0.003968
  l1.weight: grad_norm = 0.021009
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.059400
Total gradient norm: 0.098123
=== Actor Training Debug (Iteration 1378) ===
Q mean: -38.645313
Q std: 18.381456
Actor loss: 38.649288
Action reg: 0.003976
  l1.weight: grad_norm = 0.021255
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.054908
Total gradient norm: 0.074288
=== Actor Training Debug (Iteration 1379) ===
Q mean: -34.072910
Q std: 16.425631
Actor loss: 34.076897
Action reg: 0.003986
  l1.weight: grad_norm = 0.005039
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.012612
Total gradient norm: 0.019404
=== Actor Training Debug (Iteration 1380) ===
Q mean: -37.706772
Q std: 19.626919
Actor loss: 37.710754
Action reg: 0.003982
  l1.weight: grad_norm = 0.017029
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.042064
Total gradient norm: 0.062892
=== Actor Training Debug (Iteration 1381) ===
Q mean: -37.929871
Q std: 19.288792
Actor loss: 37.933868
Action reg: 0.003997
  l1.weight: grad_norm = 0.003074
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.007816
Total gradient norm: 0.011618
=== Actor Training Debug (Iteration 1382) ===
Q mean: -38.319946
Q std: 18.314116
Actor loss: 38.323910
Action reg: 0.003962
  l1.weight: grad_norm = 0.037596
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.113917
Total gradient norm: 0.201387
=== Actor Training Debug (Iteration 1383) ===
Q mean: -35.396816
Q std: 17.245535
Actor loss: 35.400787
Action reg: 0.003969
  l1.weight: grad_norm = 0.073535
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.185994
Total gradient norm: 0.274170
=== Actor Training Debug (Iteration 1384) ===
Q mean: -36.122917
Q std: 17.909615
Actor loss: 36.126888
Action reg: 0.003973
  l1.weight: grad_norm = 0.032490
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.093856
Total gradient norm: 0.165106
=== Actor Training Debug (Iteration 1385) ===
Q mean: -38.014194
Q std: 16.738037
Actor loss: 38.018188
Action reg: 0.003993
  l1.weight: grad_norm = 0.004728
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.013624
Total gradient norm: 0.021450
=== Actor Training Debug (Iteration 1386) ===
Q mean: -37.021511
Q std: 17.156338
Actor loss: 37.025501
Action reg: 0.003990
  l1.weight: grad_norm = 0.032048
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.080722
Total gradient norm: 0.150091
=== Actor Training Debug (Iteration 1387) ===
Q mean: -34.583107
Q std: 17.309801
Actor loss: 34.587078
Action reg: 0.003969
  l1.weight: grad_norm = 0.010003
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.032080
Total gradient norm: 0.051899
=== Actor Training Debug (Iteration 1388) ===
Q mean: -36.301109
Q std: 17.462500
Actor loss: 36.305092
Action reg: 0.003983
  l1.weight: grad_norm = 0.043436
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.119288
Total gradient norm: 0.205552
=== Actor Training Debug (Iteration 1389) ===
Q mean: -37.015770
Q std: 17.489410
Actor loss: 37.019737
Action reg: 0.003968
  l1.weight: grad_norm = 0.016766
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.048061
Total gradient norm: 0.088027
=== Actor Training Debug (Iteration 1390) ===
Q mean: -36.470623
Q std: 17.094049
Actor loss: 36.474606
Action reg: 0.003982
  l1.weight: grad_norm = 0.009407
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.025778
Total gradient norm: 0.042010
=== Actor Training Debug (Iteration 1391) ===
Q mean: -36.452560
Q std: 17.842617
Actor loss: 36.456551
Action reg: 0.003990
  l1.weight: grad_norm = 0.025701
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.069918
Total gradient norm: 0.125375
=== Actor Training Debug (Iteration 1392) ===
Q mean: -36.736504
Q std: 18.420870
Actor loss: 36.740482
Action reg: 0.003979
  l1.weight: grad_norm = 0.008211
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.021914
Total gradient norm: 0.042147
=== Actor Training Debug (Iteration 1393) ===
Q mean: -38.300354
Q std: 18.552214
Actor loss: 38.304344
Action reg: 0.003990
  l1.weight: grad_norm = 0.027496
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.084275
Total gradient norm: 0.150728
=== Actor Training Debug (Iteration 1394) ===
Q mean: -36.671024
Q std: 17.949535
Actor loss: 36.675022
Action reg: 0.004000
  l1.weight: grad_norm = 0.012883
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.035750
Total gradient norm: 0.059395
=== Actor Training Debug (Iteration 1395) ===
Q mean: -35.161629
Q std: 17.276304
Actor loss: 35.165611
Action reg: 0.003983
  l1.weight: grad_norm = 0.016165
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.039410
Total gradient norm: 0.065244
=== Actor Training Debug (Iteration 1396) ===
Q mean: -36.864906
Q std: 16.400604
Actor loss: 36.868881
Action reg: 0.003975
  l1.weight: grad_norm = 0.064502
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.152666
Total gradient norm: 0.214517
=== Actor Training Debug (Iteration 1397) ===
Q mean: -38.115280
Q std: 18.384079
Actor loss: 38.119278
Action reg: 0.003999
  l1.weight: grad_norm = 0.003498
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.010593
Total gradient norm: 0.022480
=== Actor Training Debug (Iteration 1398) ===
Q mean: -33.486458
Q std: 17.489954
Actor loss: 33.490444
Action reg: 0.003988
  l1.weight: grad_norm = 0.001612
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.005420
Total gradient norm: 0.012608
=== Actor Training Debug (Iteration 1399) ===
Q mean: -35.768631
Q std: 18.725510
Actor loss: 35.772610
Action reg: 0.003979
  l1.weight: grad_norm = 0.043503
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.107472
Total gradient norm: 0.185301
=== Actor Training Debug (Iteration 1400) ===
Q mean: -37.743912
Q std: 19.019947
Actor loss: 37.747902
Action reg: 0.003989
  l1.weight: grad_norm = 0.071041
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.212682
Total gradient norm: 0.387834
=== Actor Training Debug (Iteration 1401) ===
Q mean: -37.304970
Q std: 19.670490
Actor loss: 37.308918
Action reg: 0.003949
  l1.weight: grad_norm = 0.003264
  l1.bias: grad_norm = 0.000524
  l2.weight: grad_norm = 0.009748
Total gradient norm: 0.022379
=== Actor Training Debug (Iteration 1402) ===
Q mean: -38.178703
Q std: 19.371105
Actor loss: 38.182690
Action reg: 0.003987
  l1.weight: grad_norm = 0.063487
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.182951
Total gradient norm: 0.370612
=== Actor Training Debug (Iteration 1403) ===
Q mean: -36.177483
Q std: 18.459953
Actor loss: 36.181461
Action reg: 0.003979
  l1.weight: grad_norm = 0.044975
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.130312
Total gradient norm: 0.255818
=== Actor Training Debug (Iteration 1404) ===
Q mean: -33.593632
Q std: 16.692394
Actor loss: 33.597630
Action reg: 0.003996
  l1.weight: grad_norm = 0.051065
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.148898
Total gradient norm: 0.268711
=== Actor Training Debug (Iteration 1405) ===
Q mean: -32.721054
Q std: 15.853136
Actor loss: 32.725037
Action reg: 0.003983
  l1.weight: grad_norm = 0.006210
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.016877
Total gradient norm: 0.026519
=== Actor Training Debug (Iteration 1406) ===
Q mean: -37.219906
Q std: 15.992826
Actor loss: 37.223888
Action reg: 0.003984
  l1.weight: grad_norm = 0.013004
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.032536
Total gradient norm: 0.049503
=== Actor Training Debug (Iteration 1407) ===
Q mean: -39.951298
Q std: 15.697980
Actor loss: 39.955296
Action reg: 0.003996
  l1.weight: grad_norm = 0.048055
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.143826
Total gradient norm: 0.244259
=== Actor Training Debug (Iteration 1408) ===
Q mean: -39.722572
Q std: 17.399158
Actor loss: 39.726559
Action reg: 0.003987
  l1.weight: grad_norm = 0.023170
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.063857
Total gradient norm: 0.109081
=== Actor Training Debug (Iteration 1409) ===
Q mean: -35.199707
Q std: 16.913715
Actor loss: 35.203690
Action reg: 0.003983
  l1.weight: grad_norm = 0.005306
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.014468
Total gradient norm: 0.024285
=== Actor Training Debug (Iteration 1410) ===
Q mean: -34.916191
Q std: 19.060047
Actor loss: 34.920158
Action reg: 0.003966
  l1.weight: grad_norm = 0.041497
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.133028
Total gradient norm: 0.228697
=== Actor Training Debug (Iteration 1411) ===
Q mean: -37.110588
Q std: 21.653700
Actor loss: 37.114563
Action reg: 0.003974
  l1.weight: grad_norm = 0.008337
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.019769
Total gradient norm: 0.027276
=== Actor Training Debug (Iteration 1412) ===
Q mean: -37.554371
Q std: 16.876783
Actor loss: 37.558357
Action reg: 0.003985
  l1.weight: grad_norm = 0.016681
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.050269
Total gradient norm: 0.091722
=== Actor Training Debug (Iteration 1413) ===
Q mean: -37.031620
Q std: 16.781761
Actor loss: 37.035595
Action reg: 0.003975
  l1.weight: grad_norm = 0.006643
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.015114
Total gradient norm: 0.020103
=== Actor Training Debug (Iteration 1414) ===
Q mean: -35.991760
Q std: 17.328217
Actor loss: 35.995747
Action reg: 0.003985
  l1.weight: grad_norm = 0.044625
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.134736
Total gradient norm: 0.238069
=== Actor Training Debug (Iteration 1415) ===
Q mean: -36.458534
Q std: 18.098322
Actor loss: 36.462509
Action reg: 0.003976
  l1.weight: grad_norm = 0.344626
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.870301
Total gradient norm: 1.504761
=== Actor Training Debug (Iteration 1416) ===
Q mean: -38.717121
Q std: 18.846098
Actor loss: 38.721100
Action reg: 0.003981
  l1.weight: grad_norm = 0.061295
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.135158
Total gradient norm: 0.193153
=== Actor Training Debug (Iteration 1417) ===
Q mean: -39.262062
Q std: 18.820097
Actor loss: 39.266056
Action reg: 0.003993
  l1.weight: grad_norm = 0.008400
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.020911
Total gradient norm: 0.032879
=== Actor Training Debug (Iteration 1418) ===
Q mean: -35.828888
Q std: 17.444664
Actor loss: 35.832882
Action reg: 0.003992
  l1.weight: grad_norm = 0.008892
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.026871
Total gradient norm: 0.055170
=== Actor Training Debug (Iteration 1419) ===
Q mean: -34.658886
Q std: 16.278900
Actor loss: 34.662865
Action reg: 0.003979
  l1.weight: grad_norm = 0.008266
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.023085
Total gradient norm: 0.043083
=== Actor Training Debug (Iteration 1420) ===
Q mean: -38.913292
Q std: 17.368744
Actor loss: 38.917278
Action reg: 0.003985
  l1.weight: grad_norm = 0.035824
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.090251
Total gradient norm: 0.171530
=== Actor Training Debug (Iteration 1421) ===
Q mean: -39.353359
Q std: 18.994799
Actor loss: 39.357338
Action reg: 0.003979
  l1.weight: grad_norm = 0.005250
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.011489
Total gradient norm: 0.021465
=== Actor Training Debug (Iteration 1422) ===
Q mean: -41.522369
Q std: 17.741037
Actor loss: 41.526344
Action reg: 0.003975
  l1.weight: grad_norm = 0.000631
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.002489
Total gradient norm: 0.007521
=== Actor Training Debug (Iteration 1423) ===
Q mean: -39.684830
Q std: 16.472574
Actor loss: 39.688805
Action reg: 0.003975
  l1.weight: grad_norm = 0.064023
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.145516
Total gradient norm: 0.204269
=== Actor Training Debug (Iteration 1424) ===
Q mean: -36.193726
Q std: 17.581490
Actor loss: 36.197712
Action reg: 0.003985
  l1.weight: grad_norm = 0.030083
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.071252
Total gradient norm: 0.111046
=== Actor Training Debug (Iteration 1425) ===
Q mean: -31.948479
Q std: 17.569710
Actor loss: 31.952454
Action reg: 0.003975
  l1.weight: grad_norm = 0.016653
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.037904
Total gradient norm: 0.051490
=== Actor Training Debug (Iteration 1426) ===
Q mean: -34.139549
Q std: 18.290691
Actor loss: 34.143528
Action reg: 0.003979
  l1.weight: grad_norm = 0.009646
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.027716
Total gradient norm: 0.045052
=== Actor Training Debug (Iteration 1427) ===
Q mean: -41.430603
Q std: 18.354345
Actor loss: 41.434586
Action reg: 0.003982
  l1.weight: grad_norm = 0.056285
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.143000
Total gradient norm: 0.256944
=== Actor Training Debug (Iteration 1428) ===
Q mean: -40.410805
Q std: 18.743526
Actor loss: 40.414768
Action reg: 0.003965
  l1.weight: grad_norm = 0.006144
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.017837
Total gradient norm: 0.036106
=== Actor Training Debug (Iteration 1429) ===
Q mean: -40.155525
Q std: 17.500340
Actor loss: 40.159512
Action reg: 0.003985
  l1.weight: grad_norm = 0.000380
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.001242
Total gradient norm: 0.003423
=== Actor Training Debug (Iteration 1430) ===
Q mean: -35.140228
Q std: 17.515171
Actor loss: 35.144226
Action reg: 0.003998
  l1.weight: grad_norm = 0.063664
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.162220
Total gradient norm: 0.277779
=== Actor Training Debug (Iteration 1431) ===
Q mean: -35.695492
Q std: 17.623844
Actor loss: 35.699490
Action reg: 0.003999
  l1.weight: grad_norm = 0.033466
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.085371
Total gradient norm: 0.147642
=== Actor Training Debug (Iteration 1432) ===
Q mean: -35.567032
Q std: 17.996508
Actor loss: 35.571014
Action reg: 0.003981
  l1.weight: grad_norm = 0.022095
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.052147
Total gradient norm: 0.088238
=== Actor Training Debug (Iteration 1433) ===
Q mean: -38.772381
Q std: 19.644993
Actor loss: 38.776356
Action reg: 0.003976
  l1.weight: grad_norm = 0.010431
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.027044
Total gradient norm: 0.044498
=== Actor Training Debug (Iteration 1434) ===
Q mean: -38.312363
Q std: 20.134600
Actor loss: 38.316357
Action reg: 0.003992
  l1.weight: grad_norm = 0.023795
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.086090
Total gradient norm: 0.160703
=== Actor Training Debug (Iteration 1435) ===
Q mean: -35.834923
Q std: 17.233324
Actor loss: 35.838909
Action reg: 0.003987
  l1.weight: grad_norm = 0.061915
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.182221
Total gradient norm: 0.336275
=== Actor Training Debug (Iteration 1436) ===
Q mean: -32.826347
Q std: 16.016527
Actor loss: 32.830326
Action reg: 0.003980
  l1.weight: grad_norm = 0.155763
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.563253
Total gradient norm: 0.998810
=== Actor Training Debug (Iteration 1437) ===
Q mean: -37.691654
Q std: 18.352922
Actor loss: 37.695629
Action reg: 0.003973
  l1.weight: grad_norm = 0.143392
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.456349
Total gradient norm: 0.848500
=== Actor Training Debug (Iteration 1438) ===
Q mean: -39.104061
Q std: 17.579342
Actor loss: 39.108044
Action reg: 0.003981
  l1.weight: grad_norm = 0.301715
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 1.017774
Total gradient norm: 1.988448
=== Actor Training Debug (Iteration 1439) ===
Q mean: -35.644215
Q std: 16.991835
Actor loss: 35.648170
Action reg: 0.003956
  l1.weight: grad_norm = 0.185423
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.753966
Total gradient norm: 1.473998
=== Actor Training Debug (Iteration 1440) ===
Q mean: -36.312943
Q std: 18.527950
Actor loss: 36.316917
Action reg: 0.003974
  l1.weight: grad_norm = 0.186325
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.709170
Total gradient norm: 1.369159
=== Actor Training Debug (Iteration 1441) ===
Q mean: -41.051750
Q std: 17.738590
Actor loss: 41.055717
Action reg: 0.003967
  l1.weight: grad_norm = 0.072943
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.237583
Total gradient norm: 0.342108
=== Actor Training Debug (Iteration 1442) ===
Q mean: -40.095131
Q std: 19.527052
Actor loss: 40.099087
Action reg: 0.003957
  l1.weight: grad_norm = 0.021531
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.065312
Total gradient norm: 0.091404
=== Actor Training Debug (Iteration 1443) ===
Q mean: -35.983318
Q std: 15.691815
Actor loss: 35.987305
Action reg: 0.003985
  l1.weight: grad_norm = 0.000720
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.002635
Total gradient norm: 0.006723
=== Actor Training Debug (Iteration 1444) ===
Q mean: -35.099915
Q std: 18.168787
Actor loss: 35.103897
Action reg: 0.003984
  l1.weight: grad_norm = 0.015036
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.039873
Total gradient norm: 0.063472
=== Actor Training Debug (Iteration 1445) ===
Q mean: -38.205330
Q std: 17.843679
Actor loss: 38.209316
Action reg: 0.003986
  l1.weight: grad_norm = 0.023095
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.060081
Total gradient norm: 0.117775
=== Actor Training Debug (Iteration 1446) ===
Q mean: -35.701485
Q std: 19.436745
Actor loss: 35.705448
Action reg: 0.003963
  l1.weight: grad_norm = 0.032769
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.097639
Total gradient norm: 0.190120
=== Actor Training Debug (Iteration 1447) ===
Q mean: -37.090393
Q std: 17.384085
Actor loss: 37.094383
Action reg: 0.003990
  l1.weight: grad_norm = 0.041681
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.105903
Total gradient norm: 0.175340
=== Actor Training Debug (Iteration 1448) ===
Q mean: -34.007668
Q std: 17.996792
Actor loss: 34.011620
Action reg: 0.003953
  l1.weight: grad_norm = 0.006661
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.024427
Total gradient norm: 0.054627
=== Actor Training Debug (Iteration 1449) ===
Q mean: -36.453659
Q std: 14.763677
Actor loss: 36.457649
Action reg: 0.003990
  l1.weight: grad_norm = 0.017723
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.047924
Total gradient norm: 0.086643
=== Actor Training Debug (Iteration 1450) ===
Q mean: -37.910896
Q std: 16.760048
Actor loss: 37.914875
Action reg: 0.003979
  l1.weight: grad_norm = 0.075783
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.223699
Total gradient norm: 0.391786
=== Actor Training Debug (Iteration 1451) ===
Q mean: -38.680614
Q std: 16.306534
Actor loss: 38.684570
Action reg: 0.003956
  l1.weight: grad_norm = 0.078291
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.203402
Total gradient norm: 0.335917
=== Actor Training Debug (Iteration 1452) ===
Q mean: -37.528870
Q std: 16.918789
Actor loss: 37.532829
Action reg: 0.003961
  l1.weight: grad_norm = 0.163419
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.446666
Total gradient norm: 0.805075
=== Actor Training Debug (Iteration 1453) ===
Q mean: -34.798576
Q std: 15.762764
Actor loss: 34.802555
Action reg: 0.003977
  l1.weight: grad_norm = 0.042801
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.110215
Total gradient norm: 0.227052
=== Actor Training Debug (Iteration 1454) ===
Q mean: -36.217854
Q std: 18.800276
Actor loss: 36.221821
Action reg: 0.003966
  l1.weight: grad_norm = 0.006244
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.013761
Total gradient norm: 0.018648
=== Actor Training Debug (Iteration 1455) ===
Q mean: -38.117050
Q std: 16.998768
Actor loss: 38.121037
Action reg: 0.003985
  l1.weight: grad_norm = 0.026295
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.065722
Total gradient norm: 0.093235
=== Actor Training Debug (Iteration 1456) ===
Q mean: -40.648952
Q std: 18.031822
Actor loss: 40.652943
Action reg: 0.003992
  l1.weight: grad_norm = 0.014778
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.038483
Total gradient norm: 0.076610
=== Actor Training Debug (Iteration 1457) ===
Q mean: -37.179890
Q std: 17.632284
Actor loss: 37.183880
Action reg: 0.003989
  l1.weight: grad_norm = 0.028570
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.081500
Total gradient norm: 0.149361
=== Actor Training Debug (Iteration 1458) ===
Q mean: -35.356125
Q std: 17.223536
Actor loss: 35.360115
Action reg: 0.003991
  l1.weight: grad_norm = 0.000536
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.001512
Total gradient norm: 0.003515
=== Actor Training Debug (Iteration 1459) ===
Q mean: -36.953106
Q std: 17.772179
Actor loss: 36.957104
Action reg: 0.003998
  l1.weight: grad_norm = 0.053209
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.152390
Total gradient norm: 0.262068
=== Actor Training Debug (Iteration 1460) ===
Q mean: -38.940788
Q std: 18.850800
Actor loss: 38.944767
Action reg: 0.003978
  l1.weight: grad_norm = 0.135139
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.387001
Total gradient norm: 0.768792
=== Actor Training Debug (Iteration 1461) ===
Q mean: -36.736221
Q std: 17.022060
Actor loss: 36.740192
Action reg: 0.003970
  l1.weight: grad_norm = 0.036154
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.098403
Total gradient norm: 0.166943
=== Actor Training Debug (Iteration 1462) ===
Q mean: -36.679577
Q std: 17.352972
Actor loss: 36.683548
Action reg: 0.003972
  l1.weight: grad_norm = 0.084505
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.251421
Total gradient norm: 0.470730
=== Actor Training Debug (Iteration 1463) ===
Q mean: -41.533352
Q std: 19.780230
Actor loss: 41.537334
Action reg: 0.003984
  l1.weight: grad_norm = 0.010502
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.025185
Total gradient norm: 0.041318
=== Actor Training Debug (Iteration 1464) ===
Q mean: -37.907806
Q std: 18.664982
Actor loss: 37.911785
Action reg: 0.003978
  l1.weight: grad_norm = 0.009887
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.023514
Total gradient norm: 0.033636
=== Actor Training Debug (Iteration 1465) ===
Q mean: -34.156868
Q std: 16.609335
Actor loss: 34.160847
Action reg: 0.003981
  l1.weight: grad_norm = 0.000552
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.002154
Total gradient norm: 0.006627
=== Actor Training Debug (Iteration 1466) ===
Q mean: -38.081108
Q std: 17.221754
Actor loss: 38.085102
Action reg: 0.003993
  l1.weight: grad_norm = 0.093182
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.237737
Total gradient norm: 0.346400
=== Actor Training Debug (Iteration 1467) ===
Q mean: -39.351902
Q std: 17.429090
Actor loss: 39.355873
Action reg: 0.003970
  l1.weight: grad_norm = 0.059363
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.168260
Total gradient norm: 0.329324
=== Actor Training Debug (Iteration 1468) ===
Q mean: -35.468651
Q std: 15.837897
Actor loss: 35.472626
Action reg: 0.003974
  l1.weight: grad_norm = 0.000823
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.002334
Total gradient norm: 0.006693
=== Actor Training Debug (Iteration 1469) ===
Q mean: -36.214249
Q std: 16.955942
Actor loss: 36.218224
Action reg: 0.003975
  l1.weight: grad_norm = 0.008174
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.023531
Total gradient norm: 0.049272
=== Actor Training Debug (Iteration 1470) ===
Q mean: -36.970917
Q std: 18.368967
Actor loss: 36.974876
Action reg: 0.003961
  l1.weight: grad_norm = 0.030893
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.078927
Total gradient norm: 0.126749
=== Actor Training Debug (Iteration 1471) ===
Q mean: -38.361217
Q std: 19.053194
Actor loss: 38.365196
Action reg: 0.003979
  l1.weight: grad_norm = 0.042814
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.098559
Total gradient norm: 0.141982
=== Actor Training Debug (Iteration 1472) ===
Q mean: -34.521324
Q std: 17.280405
Actor loss: 34.525314
Action reg: 0.003991
  l1.weight: grad_norm = 0.064459
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.190081
Total gradient norm: 0.343322
=== Actor Training Debug (Iteration 1473) ===
Q mean: -36.434555
Q std: 16.728735
Actor loss: 36.438526
Action reg: 0.003970
  l1.weight: grad_norm = 0.059651
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.130728
Total gradient norm: 0.187460
=== Actor Training Debug (Iteration 1474) ===
Q mean: -40.678261
Q std: 16.418335
Actor loss: 40.682255
Action reg: 0.003993
  l1.weight: grad_norm = 0.000505
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.001623
Total gradient norm: 0.004125
=== Actor Training Debug (Iteration 1475) ===
Q mean: -40.653496
Q std: 19.446362
Actor loss: 40.657490
Action reg: 0.003994
  l1.weight: grad_norm = 0.000345
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.001001
Total gradient norm: 0.002432
Total gradient norm: 0.097687ration 1203) ===
=== Actor Training Debug (Iteration 1486) ===
Q mean: -38.519840
Q std: 18.411217
Actor loss: 38.523830
Action reg: 0.003992
  l1.weight: grad_norm = 0.014880
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.043043
Total gradient norm: 0.076075
=== Actor Training Debug (Iteration 1487) ===
Q mean: -39.303047
Q std: 17.589394
Actor loss: 39.307041
Action reg: 0.003994
  l1.weight: grad_norm = 0.011928
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.032813
Total gradient norm: 0.068196
=== Actor Training Debug (Iteration 1488) ===
Q mean: -36.713165
Q std: 17.402840
Actor loss: 36.717133
Action reg: 0.003968
  l1.weight: grad_norm = 0.000630
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.003055
Total gradient norm: 0.010640
=== Actor Training Debug (Iteration 1489) ===
Q mean: -37.503456
Q std: 18.100483
Actor loss: 37.507439
Action reg: 0.003981
  l1.weight: grad_norm = 0.000428
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.001252
Total gradient norm: 0.003332
=== Actor Training Debug (Iteration 1490) ===
Q mean: -39.366734
Q std: 19.627331
Actor loss: 39.370701
Action reg: 0.003968
  l1.weight: grad_norm = 0.072672
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.191705
Total gradient norm: 0.282843
=== Actor Training Debug (Iteration 1491) ===
Q mean: -38.964310
Q std: 18.845671
Actor loss: 38.968300
Action reg: 0.003991
  l1.weight: grad_norm = 0.051823
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.146439
Total gradient norm: 0.293999
=== Actor Training Debug (Iteration 1492) ===
Q mean: -34.410522
Q std: 19.076803
Actor loss: 34.414482
Action reg: 0.003961
  l1.weight: grad_norm = 0.025724
  l1.bias: grad_norm = 0.000553
  l2.weight: grad_norm = 0.061317
Total gradient norm: 0.099207
=== Actor Training Debug (Iteration 1493) ===
Q mean: -34.125710
Q std: 15.306560
Actor loss: 34.129696
Action reg: 0.003987
  l1.weight: grad_norm = 0.014801
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.042239
Total gradient norm: 0.073228
=== Actor Training Debug (Iteration 1494) ===
Q mean: -38.220310
Q std: 15.594129
Actor loss: 38.224308
Action reg: 0.003997
  l1.weight: grad_norm = 0.069521
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.190426
Total gradient norm: 0.315315
=== Actor Training Debug (Iteration 1495) ===
Q mean: -39.395195
Q std: 18.999252
Actor loss: 39.399181
Action reg: 0.003985
  l1.weight: grad_norm = 0.006804
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.016363
Total gradient norm: 0.024423
=== Actor Training Debug (Iteration 1496) ===
Q mean: -36.237213
Q std: 17.265306
Actor loss: 36.241203
Action reg: 0.003991
  l1.weight: grad_norm = 0.009475
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.027165
Total gradient norm: 0.049161
=== Actor Training Debug (Iteration 1497) ===
Q mean: -33.805115
Q std: 17.434589
Actor loss: 33.809101
Action reg: 0.003988
  l1.weight: grad_norm = 0.014882
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.037462
Total gradient norm: 0.059843
=== Actor Training Debug (Iteration 1498) ===
Q mean: -34.614586
Q std: 16.765493
Actor loss: 34.618553
Action reg: 0.003968
  l1.weight: grad_norm = 0.041640
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.097491
Total gradient norm: 0.133773
=== Actor Training Debug (Iteration 1499) ===
Q mean: -41.151749
Q std: 18.140732
Actor loss: 41.155720
Action reg: 0.003973
  l1.weight: grad_norm = 0.126870
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.362806
Total gradient norm: 0.639344
=== Actor Training Debug (Iteration 1500) ===
Q mean: -40.420647
Q std: 15.960363
Actor loss: 40.424622
Action reg: 0.003974
  l1.weight: grad_norm = 0.020686
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.048362
Total gradient norm: 0.093813
  Average reward: -366.158 | Average length: 100.0
Evaluation at episode 65: -366.158
=== Actor Training Debug (Iteration 1501) ===
Q mean: -36.314842
Q std: 16.164978
Actor loss: 36.318821
Action reg: 0.003978
  l1.weight: grad_norm = 0.014898
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.042764
Total gradient norm: 0.094913
=== Actor Training Debug (Iteration 1502) ===
Q mean: -35.267956
Q std: 17.931211
Actor loss: 35.271935
Action reg: 0.003977
  l1.weight: grad_norm = 0.051437
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.134856
Total gradient norm: 0.271720
=== Actor Training Debug (Iteration 1503) ===
Q mean: -37.754986
Q std: 17.511339
Actor loss: 37.758953
Action reg: 0.003969
  l1.weight: grad_norm = 0.008384
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.019182
Total gradient norm: 0.025903
=== Actor Training Debug (Iteration 1504) ===
Q mean: -37.821373
Q std: 17.362684
Actor loss: 37.825363
Action reg: 0.003990
  l1.weight: grad_norm = 0.023909
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.057984
Total gradient norm: 0.080037
=== Actor Training Debug (Iteration 1505) ===
Q mean: -36.199570
Q std: 15.676155
Actor loss: 36.203556
Action reg: 0.003986
  l1.weight: grad_norm = 0.124436
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.340273
Total gradient norm: 0.665795
=== Actor Training Debug (Iteration 1506) ===
Q mean: -39.075455
Q std: 17.829893
Actor loss: 39.079437
Action reg: 0.003984
  l1.weight: grad_norm = 0.021075
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.051116
Total gradient norm: 0.071542
=== Actor Training Debug (Iteration 1507) ===
Q mean: -39.869709
Q std: 17.713753
Actor loss: 39.873695
Action reg: 0.003987
  l1.weight: grad_norm = 0.011569
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.031431
Total gradient norm: 0.062695
=== Actor Training Debug (Iteration 1508) ===
Q mean: -37.387367
Q std: 19.045851
Actor loss: 37.391350
Action reg: 0.003982
  l1.weight: grad_norm = 0.037671
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.086442
Total gradient norm: 0.118696
=== Actor Training Debug (Iteration 1509) ===
Q mean: -34.112968
Q std: 15.992127
Actor loss: 34.116951
Action reg: 0.003983
  l1.weight: grad_norm = 0.068525
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.196066
Total gradient norm: 0.406875
=== Actor Training Debug (Iteration 1510) ===
Q mean: -34.060204
Q std: 16.911827
Actor loss: 34.064182
Action reg: 0.003980
  l1.weight: grad_norm = 0.005725
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.014953
Total gradient norm: 0.022136
=== Actor Training Debug (Iteration 1511) ===
Q mean: -37.758125
Q std: 18.522449
Actor loss: 37.762104
Action reg: 0.003980
  l1.weight: grad_norm = 0.003935
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.009899
Total gradient norm: 0.013652
=== Actor Training Debug (Iteration 1512) ===
Q mean: -39.460041
Q std: 19.808304
Actor loss: 39.464008
Action reg: 0.003969
  l1.weight: grad_norm = 0.225909
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.623460
Total gradient norm: 1.240953
=== Actor Training Debug (Iteration 1513) ===
Q mean: -40.817463
Q std: 16.683706
Actor loss: 40.821453
Action reg: 0.003990
  l1.weight: grad_norm = 0.048449
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.126216
Total gradient norm: 0.187339
=== Actor Training Debug (Iteration 1514) ===
Q mean: -39.529053
Q std: 17.068163
Actor loss: 39.533051
Action reg: 0.003998
  l1.weight: grad_norm = 0.045731
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.127189
Total gradient norm: 0.235054
=== Actor Training Debug (Iteration 1515) ===
Q mean: -37.204887
Q std: 16.789455
Actor loss: 37.208866
Action reg: 0.003979
  l1.weight: grad_norm = 0.093533
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.255719
Total gradient norm: 0.443261
=== Actor Training Debug (Iteration 1516) ===
Q mean: -37.132889
Q std: 17.873512
Actor loss: 37.136852
Action reg: 0.003962
  l1.weight: grad_norm = 0.012563
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.031617
Total gradient norm: 0.042151
=== Actor Training Debug (Iteration 1517) ===
Q mean: -36.769485
Q std: 15.864982
Actor loss: 36.773449
Action reg: 0.003963
  l1.weight: grad_norm = 0.046305
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.129289
Total gradient norm: 0.261194
=== Actor Training Debug (Iteration 1518) ===
Q mean: -39.573132
Q std: 18.344898
Actor loss: 39.577118
Action reg: 0.003987
  l1.weight: grad_norm = 0.002416
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.006771
Total gradient norm: 0.014131
=== Actor Training Debug (Iteration 1519) ===
Q mean: -38.810196
Q std: 18.163383
Actor loss: 38.814182
Action reg: 0.003986
  l1.weight: grad_norm = 0.009359
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.023912
Total gradient norm: 0.045239
=== Actor Training Debug (Iteration 1520) ===
Q mean: -36.880646
Q std: 16.569311
Actor loss: 36.884632
Action reg: 0.003986
  l1.weight: grad_norm = 0.051793
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.144363
Total gradient norm: 0.256442
=== Actor Training Debug (Iteration 1521) ===
Q mean: -36.317272
Q std: 16.292858
Actor loss: 36.321266
Action reg: 0.003995
  l1.weight: grad_norm = 0.031888
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.081913
Total gradient norm: 0.128543
=== Actor Training Debug (Iteration 1522) ===
Q mean: -40.029205
Q std: 16.709742
Actor loss: 40.033192
Action reg: 0.003988
  l1.weight: grad_norm = 0.008653
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.023235
Total gradient norm: 0.043488
=== Actor Training Debug (Iteration 1523) ===
Q mean: -38.451321
Q std: 17.763115
Actor loss: 38.455307
Action reg: 0.003987
  l1.weight: grad_norm = 0.030465
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.081372
Total gradient norm: 0.129656
=== Actor Training Debug (Iteration 1524) ===
Q mean: -37.118477
Q std: 17.442200
Actor loss: 37.122471
Action reg: 0.003992
  l1.weight: grad_norm = 0.004164
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.009219
Total gradient norm: 0.013265
=== Actor Training Debug (Iteration 1525) ===
Q mean: -35.391159
Q std: 17.628920
Actor loss: 35.395134
Action reg: 0.003973
  l1.weight: grad_norm = 0.065146
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.185931
Total gradient norm: 0.313904
=== Actor Training Debug (Iteration 1526) ===
Q mean: -36.204750
Q std: 17.098576
Actor loss: 36.208729
Action reg: 0.003978
  l1.weight: grad_norm = 0.006345
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.016262
Total gradient norm: 0.025687
=== Actor Training Debug (Iteration 1527) ===
Q mean: -39.797661
Q std: 18.080168
Actor loss: 39.801632
Action reg: 0.003973
  l1.weight: grad_norm = 0.009920
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.023253
Total gradient norm: 0.041454
=== Actor Training Debug (Iteration 1528) ===
Q mean: -40.191181
Q std: 17.916389
Actor loss: 40.195175
Action reg: 0.003993
  l1.weight: grad_norm = 0.004768
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.011793
Total gradient norm: 0.018957
=== Actor Training Debug (Iteration 1529) ===
Q mean: -37.097912
Q std: 18.659256
Actor loss: 37.101891
Action reg: 0.003980
  l1.weight: grad_norm = 0.011301
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.032321
Total gradient norm: 0.053701
=== Actor Training Debug (Iteration 1530) ===
Q mean: -34.899746
Q std: 15.834764
Actor loss: 34.903740
Action reg: 0.003994
  l1.weight: grad_norm = 0.000533
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.001647
Total gradient norm: 0.004494
=== Actor Training Debug (Iteration 1531) ===
Q mean: -37.380375
Q std: 17.829117
Actor loss: 37.384365
Action reg: 0.003990
  l1.weight: grad_norm = 0.029095
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.074778
Total gradient norm: 0.115342
=== Actor Training Debug (Iteration 1532) ===
Q mean: -41.082802
Q std: 17.341448
Actor loss: 41.086796
Action reg: 0.003993
  l1.weight: grad_norm = 0.003558
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.008372
Total gradient norm: 0.011758
=== Actor Training Debug (Iteration 1533) ===
Q mean: -40.765312
Q std: 18.278322
Actor loss: 40.769295
Action reg: 0.003983
  l1.weight: grad_norm = 0.049034
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.129447
Total gradient norm: 0.225872
=== Actor Training Debug (Iteration 1534) ===
Q mean: -37.309151
Q std: 16.662533
Actor loss: 37.313129
Action reg: 0.003977
  l1.weight: grad_norm = 0.001021
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.002697
Total gradient norm: 0.005929
=== Actor Training Debug (Iteration 1535) ===
Q mean: -37.589500
Q std: 15.946404
Actor loss: 37.593483
Action reg: 0.003981
  l1.weight: grad_norm = 0.000687
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.002763
Total gradient norm: 0.009180
=== Actor Training Debug (Iteration 1536) ===
Q mean: -39.875137
Q std: 20.017099
Actor loss: 39.879124
Action reg: 0.003986
  l1.weight: grad_norm = 0.032706
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.093522
Total gradient norm: 0.185452
=== Actor Training Debug (Iteration 1537) ===
Q mean: -38.268837
Q std: 18.426996
Actor loss: 38.272823
Action reg: 0.003988
  l1.weight: grad_norm = 0.003125
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.008552
Total gradient norm: 0.019393
=== Actor Training Debug (Iteration 1538) ===
Q mean: -36.530403
Q std: 17.379166
Actor loss: 36.534393
Action reg: 0.003991
  l1.weight: grad_norm = 0.059406
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.164688
Total gradient norm: 0.344686
=== Actor Training Debug (Iteration 1539) ===
Q mean: -36.866673
Q std: 16.378563
Actor loss: 36.870647
Action reg: 0.003976
  l1.weight: grad_norm = 0.043060
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.101901
Total gradient norm: 0.138877
=== Actor Training Debug (Iteration 1540) ===
Q mean: -40.119854
Q std: 17.643435
Actor loss: 40.123848
Action reg: 0.003995
  l1.weight: grad_norm = 0.021735
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.048809
Total gradient norm: 0.064836
=== Actor Training Debug (Iteration 1541) ===
Q mean: -40.285702
Q std: 20.670349
Actor loss: 40.289688
Action reg: 0.003987
  l1.weight: grad_norm = 0.001298
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.002999
Total gradient norm: 0.005475
=== Actor Training Debug (Iteration 1542) ===
Q mean: -36.588882
Q std: 16.134272
Actor loss: 36.592873
Action reg: 0.003992
  l1.weight: grad_norm = 0.025326
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.069667
Total gradient norm: 0.143138
=== Actor Training Debug (Iteration 1543) ===
Q mean: -36.300865
Q std: 16.949865
Actor loss: 36.304859
Action reg: 0.003995
  l1.weight: grad_norm = 0.001256
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.003027
Total gradient norm: 0.004318
=== Actor Training Debug (Iteration 1544) ===
Q mean: -38.740685
Q std: 19.609499
Actor loss: 38.744667
Action reg: 0.003984
  l1.weight: grad_norm = 0.048235
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.130054
Total gradient norm: 0.226168
=== Actor Training Debug (Iteration 1545) ===
Q mean: -39.932556
Q std: 17.003584
Actor loss: 39.936550
Action reg: 0.003992
  l1.weight: grad_norm = 0.004234
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.010786
Total gradient norm: 0.020908
=== Actor Training Debug (Iteration 1546) ===
Q mean: -35.490952
Q std: 17.115225
Actor loss: 35.494942
Action reg: 0.003988
  l1.weight: grad_norm = 0.040566
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.082452
Total gradient norm: 0.117753
=== Actor Training Debug (Iteration 1547) ===
Q mean: -37.085117
Q std: 14.927522
Actor loss: 37.089115
Action reg: 0.003997
  l1.weight: grad_norm = 0.018583
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.046018
Total gradient norm: 0.074343
=== Actor Training Debug (Iteration 1548) ===
Q mean: -39.584923
Q std: 16.272900
Actor loss: 39.588909
Action reg: 0.003987
  l1.weight: grad_norm = 0.019711
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.052617
Total gradient norm: 0.107694
=== Actor Training Debug (Iteration 1549) ===
Q mean: -37.661098
Q std: 17.848673
Actor loss: 37.665089
Action reg: 0.003991
  l1.weight: grad_norm = 0.009273
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.021364
Total gradient norm: 0.036721
=== Actor Training Debug (Iteration 1550) ===
Q mean: -36.878796
Q std: 17.480898
Actor loss: 36.882782
Action reg: 0.003988
  l1.weight: grad_norm = 0.002413
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.007187
Total gradient norm: 0.014660
=== Actor Training Debug (Iteration 1551) ===
Q mean: -37.474823
Q std: 16.525530
Actor loss: 37.478809
Action reg: 0.003985
  l1.weight: grad_norm = 0.067748
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.185544
Total gradient norm: 0.322560
=== Actor Training Debug (Iteration 1552) ===
Q mean: -38.497860
Q std: 17.623398
Actor loss: 38.501862
Action reg: 0.004000
  l1.weight: grad_norm = 0.000372
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000888
Total gradient norm: 0.001607
=== Actor Training Debug (Iteration 1553) ===
Q mean: -39.479748
Q std: 19.023125
Actor loss: 39.483738
Action reg: 0.003988
  l1.weight: grad_norm = 0.004531
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.011874
Total gradient norm: 0.019172
=== Actor Training Debug (Iteration 1554) ===
Q mean: -38.763634
Q std: 16.127039
Actor loss: 38.767624
Action reg: 0.003991
  l1.weight: grad_norm = 0.028760
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.066423
Total gradient norm: 0.103140
=== Actor Training Debug (Iteration 1555) ===
Q mean: -37.144680
Q std: 19.442995
Actor loss: 37.148663
Action reg: 0.003984
  l1.weight: grad_norm = 0.039943
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.134701
Total gradient norm: 0.276051
=== Actor Training Debug (Iteration 1556) ===
Q mean: -37.274750
Q std: 16.726910
Actor loss: 37.278744
Action reg: 0.003992
  l1.weight: grad_norm = 0.028080
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.077927
Total gradient norm: 0.135080
=== Actor Training Debug (Iteration 1557) ===
Q mean: -37.245705
Q std: 19.172157
Actor loss: 37.249657
Action reg: 0.003952
  l1.weight: grad_norm = 0.014368
  l1.bias: grad_norm = 0.000607
  l2.weight: grad_norm = 0.031735
Total gradient norm: 0.044172
Total gradient norm: 0.028534ration 1203) ===
=== Actor Training Debug (Iteration 1568) ===
Q mean: -38.353737
Q std: 16.507992
Actor loss: 38.357719
Action reg: 0.003984
  l1.weight: grad_norm = 0.026845
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.068403
Total gradient norm: 0.140275
=== Actor Training Debug (Iteration 1569) ===
Q mean: -37.722126
Q std: 19.876394
Actor loss: 37.726109
Action reg: 0.003981
  l1.weight: grad_norm = 0.009108
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.022309
Total gradient norm: 0.038979
=== Actor Training Debug (Iteration 1570) ===
Q mean: -39.896019
Q std: 20.272657
Actor loss: 39.900017
Action reg: 0.003998
  l1.weight: grad_norm = 0.013565
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.036716
Total gradient norm: 0.059610
=== Actor Training Debug (Iteration 1571) ===
Q mean: -42.093945
Q std: 16.740177
Actor loss: 42.097939
Action reg: 0.003994
  l1.weight: grad_norm = 0.027599
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.079217
Total gradient norm: 0.143503
=== Actor Training Debug (Iteration 1572) ===
Q mean: -36.960014
Q std: 16.545294
Actor loss: 36.963989
Action reg: 0.003975
  l1.weight: grad_norm = 0.074511
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.213262
Total gradient norm: 0.405553
=== Actor Training Debug (Iteration 1573) ===
Q mean: -36.421181
Q std: 16.419830
Actor loss: 36.425163
Action reg: 0.003982
  l1.weight: grad_norm = 0.017461
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.038871
Total gradient norm: 0.061320
=== Actor Training Debug (Iteration 1574) ===
Q mean: -37.367500
Q std: 17.169353
Actor loss: 37.371483
Action reg: 0.003982
  l1.weight: grad_norm = 0.001471
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.004255
Total gradient norm: 0.011257
=== Actor Training Debug (Iteration 1575) ===
Q mean: -38.929317
Q std: 17.648970
Actor loss: 38.933281
Action reg: 0.003962
  l1.weight: grad_norm = 0.001088
  l1.bias: grad_norm = 0.000609
  l2.weight: grad_norm = 0.004775
Total gradient norm: 0.016232
=== Actor Training Debug (Iteration 1576) ===
Q mean: -39.135422
Q std: 17.400431
Actor loss: 39.139389
Action reg: 0.003967
  l1.weight: grad_norm = 0.069672
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.186643
Total gradient norm: 0.334833
=== Actor Training Debug (Iteration 1577) ===
Q mean: -36.691669
Q std: 17.028723
Actor loss: 36.695644
Action reg: 0.003977
  l1.weight: grad_norm = 0.049518
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.137343
Total gradient norm: 0.257055
=== Actor Training Debug (Iteration 1578) ===
Q mean: -38.276741
Q std: 16.332636
Actor loss: 38.280731
Action reg: 0.003991
  l1.weight: grad_norm = 0.006307
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.015530
Total gradient norm: 0.024819
=== Actor Training Debug (Iteration 1579) ===
Q mean: -40.410992
Q std: 17.761400
Actor loss: 40.414982
Action reg: 0.003990
  l1.weight: grad_norm = 0.040902
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.102580
Total gradient norm: 0.146284
=== Actor Training Debug (Iteration 1580) ===
Q mean: -40.151848
Q std: 17.373632
Actor loss: 40.155830
Action reg: 0.003981
  l1.weight: grad_norm = 0.001471
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.003870
Total gradient norm: 0.006394
=== Actor Training Debug (Iteration 1581) ===
Q mean: -37.285637
Q std: 17.671791
Actor loss: 37.289608
Action reg: 0.003972
  l1.weight: grad_norm = 0.046289
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.109905
Total gradient norm: 0.148209
=== Actor Training Debug (Iteration 1582) ===
Q mean: -35.867851
Q std: 19.251875
Actor loss: 35.871822
Action reg: 0.003970
  l1.weight: grad_norm = 0.001359
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.003574
Total gradient norm: 0.007267
=== Actor Training Debug (Iteration 1583) ===
Q mean: -38.935032
Q std: 18.743189
Actor loss: 38.939018
Action reg: 0.003987
  l1.weight: grad_norm = 0.027606
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.066507
Total gradient norm: 0.148616
=== Actor Training Debug (Iteration 1584) ===
Q mean: -38.541466
Q std: 17.505840
Actor loss: 38.545456
Action reg: 0.003990
  l1.weight: grad_norm = 0.012919
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.031435
Total gradient norm: 0.043937
=== Actor Training Debug (Iteration 1585) ===
Q mean: -41.174721
Q std: 16.398550
Actor loss: 41.178711
Action reg: 0.003991
  l1.weight: grad_norm = 0.016527
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.048215
Total gradient norm: 0.093775
=== Actor Training Debug (Iteration 1586) ===
Q mean: -39.290363
Q std: 18.186007
Actor loss: 39.294346
Action reg: 0.003983
  l1.weight: grad_norm = 0.017426
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.050560
Total gradient norm: 0.101096
=== Actor Training Debug (Iteration 1587) ===
Q mean: -37.634460
Q std: 18.628738
Actor loss: 37.638428
Action reg: 0.003968
  l1.weight: grad_norm = 0.049819
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.125765
Total gradient norm: 0.210541
=== Actor Training Debug (Iteration 1588) ===
Q mean: -38.481071
Q std: 19.593626
Actor loss: 38.485050
Action reg: 0.003980
  l1.weight: grad_norm = 0.018186
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.042925
Total gradient norm: 0.079266
=== Actor Training Debug (Iteration 1589) ===
Q mean: -38.726273
Q std: 17.261797
Actor loss: 38.730267
Action reg: 0.003992
  l1.weight: grad_norm = 0.011563
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.025262
Total gradient norm: 0.035554
=== Actor Training Debug (Iteration 1590) ===
Q mean: -42.499901
Q std: 17.406857
Actor loss: 42.503887
Action reg: 0.003986
  l1.weight: grad_norm = 0.020936
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.054652
Total gradient norm: 0.091444
=== Actor Training Debug (Iteration 1591) ===
Q mean: -40.178535
Q std: 18.586287
Actor loss: 40.182526
Action reg: 0.003990
  l1.weight: grad_norm = 0.010575
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.026139
Total gradient norm: 0.041361
=== Actor Training Debug (Iteration 1592) ===
Q mean: -40.962219
Q std: 18.106342
Actor loss: 40.966217
Action reg: 0.003998
  l1.weight: grad_norm = 0.021017
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.054333
Total gradient norm: 0.097019
=== Actor Training Debug (Iteration 1593) ===
Q mean: -36.822075
Q std: 17.735609
Actor loss: 36.826065
Action reg: 0.003990
  l1.weight: grad_norm = 0.003209
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.007518
Total gradient norm: 0.010388
=== Actor Training Debug (Iteration 1594) ===
Q mean: -39.169224
Q std: 16.498232
Actor loss: 39.173203
Action reg: 0.003980
  l1.weight: grad_norm = 0.010588
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.028451
Total gradient norm: 0.056658
=== Actor Training Debug (Iteration 1595) ===
Q mean: -37.400810
Q std: 16.608187
Actor loss: 37.404793
Action reg: 0.003981
  l1.weight: grad_norm = 0.036991
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.096205
Total gradient norm: 0.174178
=== Actor Training Debug (Iteration 1596) ===
Q mean: -38.236580
Q std: 16.771067
Actor loss: 38.240562
Action reg: 0.003982
  l1.weight: grad_norm = 0.019253
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.050716
Total gradient norm: 0.096924
=== Actor Training Debug (Iteration 1597) ===
Q mean: -37.025253
Q std: 16.274342
Actor loss: 37.029247
Action reg: 0.003995
  l1.weight: grad_norm = 0.000373
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.001298
Total gradient norm: 0.003771
=== Actor Training Debug (Iteration 1598) ===
Q mean: -38.915825
Q std: 18.646992
Actor loss: 38.919807
Action reg: 0.003982
  l1.weight: grad_norm = 0.011424
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.031900
Total gradient norm: 0.066746
=== Actor Training Debug (Iteration 1599) ===
Q mean: -37.979500
Q std: 17.383497
Actor loss: 37.983494
Action reg: 0.003993
  l1.weight: grad_norm = 0.080251
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.210610
Total gradient norm: 0.360487
=== Actor Training Debug (Iteration 1600) ===
Q mean: -39.009350
Q std: 17.154909
Actor loss: 39.013325
Action reg: 0.003975
  l1.weight: grad_norm = 0.021935
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.061541
Total gradient norm: 0.134253
=== Actor Training Debug (Iteration 1601) ===
Q mean: -40.864487
Q std: 17.679325
Actor loss: 40.868481
Action reg: 0.003995
  l1.weight: grad_norm = 0.001649
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.004097
Total gradient norm: 0.007463
=== Actor Training Debug (Iteration 1602) ===
Q mean: -37.129158
Q std: 19.359329
Actor loss: 37.133121
Action reg: 0.003962
  l1.weight: grad_norm = 0.001053
  l1.bias: grad_norm = 0.000600
  l2.weight: grad_norm = 0.003853
Total gradient norm: 0.012390
=== Actor Training Debug (Iteration 1603) ===
Q mean: -38.981533
Q std: 17.958258
Actor loss: 38.985523
Action reg: 0.003990
  l1.weight: grad_norm = 0.001620
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.003833
Total gradient norm: 0.005995
=== Actor Training Debug (Iteration 1604) ===
Q mean: -40.087627
Q std: 19.102978
Actor loss: 40.091614
Action reg: 0.003988
  l1.weight: grad_norm = 0.138301
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.359601
Total gradient norm: 0.679697
=== Actor Training Debug (Iteration 1605) ===
Q mean: -41.111515
Q std: 17.522615
Actor loss: 41.115475
Action reg: 0.003961
  l1.weight: grad_norm = 0.027937
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.068599
Total gradient norm: 0.128719
=== Actor Training Debug (Iteration 1606) ===
Q mean: -38.895462
Q std: 16.413656
Actor loss: 38.899448
Action reg: 0.003986
  l1.weight: grad_norm = 0.038343
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.093716
Total gradient norm: 0.146154
=== Actor Training Debug (Iteration 1607) ===
Q mean: -36.261086
Q std: 15.953745
Actor loss: 36.265068
Action reg: 0.003981
  l1.weight: grad_norm = 0.004320
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.011142
Total gradient norm: 0.020506
=== Actor Training Debug (Iteration 1608) ===
Q mean: -35.841362
Q std: 16.475653
Actor loss: 35.845352
Action reg: 0.003989
  l1.weight: grad_norm = 0.004253
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.009668
Total gradient norm: 0.013091
=== Actor Training Debug (Iteration 1609) ===
Q mean: -38.407570
Q std: 15.783770
Actor loss: 38.411549
Action reg: 0.003980
  l1.weight: grad_norm = 0.538587
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 1.504182
Total gradient norm: 3.200548
=== Actor Training Debug (Iteration 1610) ===
Q mean: -38.093056
Q std: 18.269791
Actor loss: 38.097034
Action reg: 0.003977
  l1.weight: grad_norm = 0.004673
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.011147
Total gradient norm: 0.017995
=== Actor Training Debug (Iteration 1611) ===
Q mean: -38.541603
Q std: 15.875325
Actor loss: 38.545597
Action reg: 0.003992
  l1.weight: grad_norm = 0.043828
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.125225
Total gradient norm: 0.230998
=== Actor Training Debug (Iteration 1612) ===
Q mean: -39.898945
Q std: 16.443417
Actor loss: 39.902924
Action reg: 0.003978
  l1.weight: grad_norm = 0.041729
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.116899
Total gradient norm: 0.185021
=== Actor Training Debug (Iteration 1613) ===
Q mean: -39.107048
Q std: 17.453194
Actor loss: 39.111015
Action reg: 0.003966
  l1.weight: grad_norm = 0.004926
  l1.bias: grad_norm = 0.000526
  l2.weight: grad_norm = 0.011879
Total gradient norm: 0.018086
=== Actor Training Debug (Iteration 1614) ===
Q mean: -38.942211
Q std: 19.737272
Actor loss: 38.946194
Action reg: 0.003981
  l1.weight: grad_norm = 0.070947
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.188749
Total gradient norm: 0.391942
=== Actor Training Debug (Iteration 1615) ===
Q mean: -37.368118
Q std: 18.509752
Actor loss: 37.372086
Action reg: 0.003968
  l1.weight: grad_norm = 0.023920
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.055384
Total gradient norm: 0.076665
=== Actor Training Debug (Iteration 1616) ===
Q mean: -38.746075
Q std: 17.751068
Actor loss: 38.750042
Action reg: 0.003968
  l1.weight: grad_norm = 0.068291
  l1.bias: grad_norm = 0.000534
  l2.weight: grad_norm = 0.173169
Total gradient norm: 0.335616
=== Actor Training Debug (Iteration 1617) ===
Q mean: -41.813869
Q std: 17.272356
Actor loss: 41.817871
Action reg: 0.004000
  l1.weight: grad_norm = 0.000468
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001258
Total gradient norm: 0.002707
=== Actor Training Debug (Iteration 1618) ===
Q mean: -41.353199
Q std: 17.879744
Actor loss: 41.357178
Action reg: 0.003980
  l1.weight: grad_norm = 0.065064
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.155890
Total gradient norm: 0.251029
=== Actor Training Debug (Iteration 1619) ===
Q mean: -39.066826
Q std: 19.369169
Actor loss: 39.070812
Action reg: 0.003986
  l1.weight: grad_norm = 0.052764
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.134598
Total gradient norm: 0.252907
=== Actor Training Debug (Iteration 1620) ===
Q mean: -37.627598
Q std: 16.290516
Actor loss: 37.631580
Action reg: 0.003983
  l1.weight: grad_norm = 0.000466
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.001976
Total gradient norm: 0.006527
=== Actor Training Debug (Iteration 1621) ===
Q mean: -39.002655
Q std: 17.856594
Actor loss: 39.006641
Action reg: 0.003987
  l1.weight: grad_norm = 0.080160
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.224241
Total gradient norm: 0.438168
=== Actor Training Debug (Iteration 1622) ===
Q mean: -40.650864
Q std: 18.750744
Actor loss: 40.654827
Action reg: 0.003963
  l1.weight: grad_norm = 0.046177
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.113445
Total gradient norm: 0.223496
=== Actor Training Debug (Iteration 1623) ===
Q mean: -36.400848
Q std: 17.689560
Actor loss: 36.404816
Action reg: 0.003967
  l1.weight: grad_norm = 0.023281
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.055063
Total gradient norm: 0.074288
=== Actor Training Debug (Iteration 1624) ===
Q mean: -37.966972
Q std: 18.386965
Actor loss: 37.970951
Action reg: 0.003978
  l1.weight: grad_norm = 0.024828
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.065954
Total gradient norm: 0.107452
=== Actor Training Debug (Iteration 1625) ===
Q mean: -39.401474
Q std: 17.656178
Actor loss: 39.405453
Action reg: 0.003979
  l1.weight: grad_norm = 0.036035
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.093614
Total gradient norm: 0.171462
=== Actor Training Debug (Iteration 1626) ===
Q mean: -37.449612
Q std: 17.670340
Actor loss: 37.453594
Action reg: 0.003984
  l1.weight: grad_norm = 0.031611
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.083716
Total gradient norm: 0.178286
=== Actor Training Debug (Iteration 1627) ===
Q mean: -36.420506
Q std: 15.715972
Actor loss: 36.424500
Action reg: 0.003995
  l1.weight: grad_norm = 0.063297
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.184429
Total gradient norm: 0.412707
=== Actor Training Debug (Iteration 1628) ===
Q mean: -38.140987
Q std: 16.020290
Actor loss: 38.144974
Action reg: 0.003986
  l1.weight: grad_norm = 0.013656
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.034225
Total gradient norm: 0.062796
=== Actor Training Debug (Iteration 1629) ===
Q mean: -43.071663
Q std: 19.111467
Actor loss: 43.075649
Action reg: 0.003985
  l1.weight: grad_norm = 0.056966
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.116095
Total gradient norm: 0.165024
=== Actor Training Debug (Iteration 1630) ===
Q mean: -41.954960
Q std: 17.842615
Actor loss: 41.958958
Action reg: 0.003997
  l1.weight: grad_norm = 0.059482
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.152401
Total gradient norm: 0.262259
=== Actor Training Debug (Iteration 1631) ===
Q mean: -39.528755
Q std: 18.321266
Actor loss: 39.532722
Action reg: 0.003969
  l1.weight: grad_norm = 0.047172
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.127100
Total gradient norm: 0.264881
=== Actor Training Debug (Iteration 1632) ===
Q mean: -37.608334
Q std: 16.899891
Actor loss: 37.612331
Action reg: 0.003998
  l1.weight: grad_norm = 0.024115
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.078703
Total gradient norm: 0.151854
=== Actor Training Debug (Iteration 1633) ===
Q mean: -39.224243
Q std: 18.340475
Actor loss: 39.228230
Action reg: 0.003988
  l1.weight: grad_norm = 0.010684
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.022711
Total gradient norm: 0.033311
=== Actor Training Debug (Iteration 1634) ===
Q mean: -42.341568
Q std: 20.266745
Actor loss: 42.345554
Action reg: 0.003988
  l1.weight: grad_norm = 0.024125
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.065011
Total gradient norm: 0.119281
=== Actor Training Debug (Iteration 1635) ===
Q mean: -41.305344
Q std: 18.416914
Actor loss: 41.309326
Action reg: 0.003984
  l1.weight: grad_norm = 0.048022
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.138740
Total gradient norm: 0.271602
=== Actor Training Debug (Iteration 1636) ===
Q mean: -37.053188
Q std: 16.398239
Actor loss: 37.057178
Action reg: 0.003990
  l1.weight: grad_norm = 0.080057
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.209672
Total gradient norm: 0.373974
=== Actor Training Debug (Iteration 1637) ===
Q mean: -36.475929
Q std: 16.268518
Actor loss: 36.479897
Action reg: 0.003966
  l1.weight: grad_norm = 0.024837
  l1.bias: grad_norm = 0.000552
  l2.weight: grad_norm = 0.072565
Total gradient norm: 0.162603
=== Actor Training Debug (Iteration 1638) ===
Q mean: -38.447006
Q std: 17.316702
Actor loss: 38.450985
Action reg: 0.003979
Action reg: 0.003993 0.028534ration 1203) ===
  l1.weight: grad_norm = 0.007662
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.018048
Total gradient norm: 0.032013
=== Actor Training Debug (Iteration 1649) ===
Q mean: -39.714577
Q std: 17.659348
Actor loss: 39.718555
Action reg: 0.003978
  l1.weight: grad_norm = 0.006315
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.016611
Total gradient norm: 0.024898
=== Actor Training Debug (Iteration 1650) ===
Q mean: -39.253613
Q std: 20.851084
Actor loss: 39.257587
Action reg: 0.003976
  l1.weight: grad_norm = 0.000661
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.002995
Total gradient norm: 0.010062
=== Actor Training Debug (Iteration 1651) ===
Q mean: -39.179859
Q std: 17.827728
Actor loss: 39.183857
Action reg: 0.003998
  l1.weight: grad_norm = 0.041725
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.100823
Total gradient norm: 0.191880
=== Actor Training Debug (Iteration 1652) ===
Q mean: -40.008255
Q std: 16.411037
Actor loss: 40.012253
Action reg: 0.003998
  l1.weight: grad_norm = 0.026636
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.070144
Total gradient norm: 0.136033
=== Actor Training Debug (Iteration 1653) ===
Q mean: -41.088081
Q std: 18.727570
Actor loss: 41.092064
Action reg: 0.003982
  l1.weight: grad_norm = 0.001979
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.006815
Total gradient norm: 0.017156
=== Actor Training Debug (Iteration 1654) ===
Q mean: -39.988220
Q std: 18.013411
Actor loss: 39.992218
Action reg: 0.004000
  l1.weight: grad_norm = 0.006308
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.016878
Total gradient norm: 0.034747
=== Actor Training Debug (Iteration 1655) ===
Q mean: -40.464417
Q std: 16.161596
Actor loss: 40.468395
Action reg: 0.003980
  l1.weight: grad_norm = 0.001823
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.004550
Total gradient norm: 0.007322
=== Actor Training Debug (Iteration 1656) ===
Q mean: -39.828869
Q std: 16.818277
Actor loss: 39.832848
Action reg: 0.003978
  l1.weight: grad_norm = 0.021373
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.054561
Total gradient norm: 0.082704
=== Actor Training Debug (Iteration 1657) ===
Q mean: -38.436630
Q std: 18.210350
Actor loss: 38.440613
Action reg: 0.003982
  l1.weight: grad_norm = 0.001554
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.003844
Total gradient norm: 0.006307
=== Actor Training Debug (Iteration 1658) ===
Q mean: -41.217457
Q std: 17.578634
Actor loss: 41.221455
Action reg: 0.003997
  l1.weight: grad_norm = 0.014985
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.040410
Total gradient norm: 0.075373
=== Actor Training Debug (Iteration 1659) ===
Q mean: -38.144917
Q std: 17.045206
Actor loss: 38.148907
Action reg: 0.003990
  l1.weight: grad_norm = 0.016239
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.041117
Total gradient norm: 0.062300
=== Actor Training Debug (Iteration 1660) ===
Q mean: -39.048172
Q std: 17.346642
Actor loss: 39.052151
Action reg: 0.003979
  l1.weight: grad_norm = 0.026572
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.055226
Total gradient norm: 0.075851
=== Actor Training Debug (Iteration 1661) ===
Q mean: -38.732273
Q std: 17.484058
Actor loss: 38.736244
Action reg: 0.003970
  l1.weight: grad_norm = 0.014207
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 0.035249
Total gradient norm: 0.070731
=== Actor Training Debug (Iteration 1662) ===
Q mean: -41.700897
Q std: 19.568218
Actor loss: 41.704884
Action reg: 0.003985
  l1.weight: grad_norm = 0.045770
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.101577
Total gradient norm: 0.136592
=== Actor Training Debug (Iteration 1663) ===
Q mean: -39.338570
Q std: 19.232450
Actor loss: 39.342529
Action reg: 0.003958
  l1.weight: grad_norm = 0.015333
  l1.bias: grad_norm = 0.000851
  l2.weight: grad_norm = 0.042810
Total gradient norm: 0.083654
=== Actor Training Debug (Iteration 1664) ===
Q mean: -38.323574
Q std: 16.559689
Actor loss: 38.327564
Action reg: 0.003989
  l1.weight: grad_norm = 0.004432
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.012035
Total gradient norm: 0.024631
=== Actor Training Debug (Iteration 1665) ===
Q mean: -38.851265
Q std: 16.191603
Actor loss: 38.855225
Action reg: 0.003958
  l1.weight: grad_norm = 0.018363
  l1.bias: grad_norm = 0.000778
  l2.weight: grad_norm = 0.043858
Total gradient norm: 0.075979
=== Actor Training Debug (Iteration 1666) ===
Q mean: -43.166988
Q std: 18.151115
Actor loss: 43.170971
Action reg: 0.003983
  l1.weight: grad_norm = 0.034291
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.098905
Total gradient norm: 0.176396
=== Actor Training Debug (Iteration 1667) ===
Q mean: -41.226875
Q std: 16.632696
Actor loss: 41.230862
Action reg: 0.003985
  l1.weight: grad_norm = 0.013844
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.029299
Total gradient norm: 0.037795
=== Actor Training Debug (Iteration 1668) ===
Q mean: -38.552048
Q std: 18.130928
Actor loss: 38.556042
Action reg: 0.003995
  l1.weight: grad_norm = 0.001126
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.003651
Total gradient norm: 0.008878
=== Actor Training Debug (Iteration 1669) ===
Q mean: -38.350739
Q std: 17.227591
Actor loss: 38.354729
Action reg: 0.003989
  l1.weight: grad_norm = 0.001091
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.003095
Total gradient norm: 0.008045
=== Actor Training Debug (Iteration 1670) ===
Q mean: -40.023975
Q std: 17.443047
Actor loss: 40.027962
Action reg: 0.003988
  l1.weight: grad_norm = 0.015517
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.039998
Total gradient norm: 0.073862
=== Actor Training Debug (Iteration 1671) ===
Q mean: -42.173103
Q std: 17.185705
Actor loss: 42.177097
Action reg: 0.003993
  l1.weight: grad_norm = 0.017036
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.041846
Total gradient norm: 0.073947
=== Actor Training Debug (Iteration 1672) ===
Q mean: -39.206932
Q std: 17.740517
Actor loss: 39.210922
Action reg: 0.003989
  l1.weight: grad_norm = 0.000444
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.001362
Total gradient norm: 0.003900
=== Actor Training Debug (Iteration 1673) ===
Q mean: -38.803055
Q std: 17.128395
Actor loss: 38.807049
Action reg: 0.003993
  l1.weight: grad_norm = 0.016717
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.043852
Total gradient norm: 0.072543
=== Actor Training Debug (Iteration 1674) ===
Q mean: -38.728161
Q std: 18.592861
Actor loss: 38.732151
Action reg: 0.003989
  l1.weight: grad_norm = 0.040734
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.114239
Total gradient norm: 0.213933
=== Actor Training Debug (Iteration 1675) ===
Q mean: -40.394272
Q std: 17.126040
Actor loss: 40.398254
Action reg: 0.003983
  l1.weight: grad_norm = 0.079237
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.228689
Total gradient norm: 0.467156
=== Actor Training Debug (Iteration 1676) ===
Q mean: -39.438896
Q std: 18.639620
Actor loss: 39.442883
Action reg: 0.003986
  l1.weight: grad_norm = 0.026184
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.067863
Total gradient norm: 0.137131
=== Actor Training Debug (Iteration 1677) ===
Q mean: -40.746811
Q std: 17.607349
Actor loss: 40.750809
Action reg: 0.003999
  l1.weight: grad_norm = 0.005568
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.015386
Total gradient norm: 0.026023
=== Actor Training Debug (Iteration 1678) ===
Q mean: -39.353565
Q std: 17.024441
Actor loss: 39.357563
Action reg: 0.003999
  l1.weight: grad_norm = 0.007049
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.018425
Total gradient norm: 0.038249
=== Actor Training Debug (Iteration 1679) ===
Q mean: -40.370171
Q std: 19.081861
Actor loss: 40.374157
Action reg: 0.003985
  l1.weight: grad_norm = 0.014969
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.042137
Total gradient norm: 0.098474
=== Actor Training Debug (Iteration 1680) ===
Q mean: -40.210831
Q std: 17.350729
Actor loss: 40.214813
Action reg: 0.003983
  l1.weight: grad_norm = 0.025481
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.060717
Total gradient norm: 0.108240
=== Actor Training Debug (Iteration 1681) ===
Q mean: -40.939270
Q std: 17.380428
Actor loss: 40.943253
Action reg: 0.003983
  l1.weight: grad_norm = 0.002193
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.005329
Total gradient norm: 0.008568
=== Actor Training Debug (Iteration 1682) ===
Q mean: -38.331635
Q std: 18.600090
Actor loss: 38.335590
Action reg: 0.003957
  l1.weight: grad_norm = 0.092170
  l1.bias: grad_norm = 0.000708
  l2.weight: grad_norm = 0.266291
Total gradient norm: 0.495866
=== Actor Training Debug (Iteration 1683) ===
Q mean: -38.414246
Q std: 19.021065
Actor loss: 38.418221
Action reg: 0.003974
  l1.weight: grad_norm = 0.043955
  l1.bias: grad_norm = 0.000478
  l2.weight: grad_norm = 0.132984
Total gradient norm: 0.252465
=== Actor Training Debug (Iteration 1684) ===
Q mean: -38.292389
Q std: 18.799595
Actor loss: 38.296371
Action reg: 0.003982
  l1.weight: grad_norm = 0.085238
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.236978
Total gradient norm: 0.428866
=== Actor Training Debug (Iteration 1685) ===
Q mean: -41.014008
Q std: 18.135363
Actor loss: 41.017998
Action reg: 0.003990
  l1.weight: grad_norm = 0.026242
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.063170
Total gradient norm: 0.116509
=== Actor Training Debug (Iteration 1686) ===
Q mean: -40.618515
Q std: 16.552778
Actor loss: 40.622498
Action reg: 0.003983
  l1.weight: grad_norm = 0.000477
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.002081
Total gradient norm: 0.006855
=== Actor Training Debug (Iteration 1687) ===
Q mean: -39.481773
Q std: 17.395048
Actor loss: 39.485748
Action reg: 0.003974
  l1.weight: grad_norm = 0.000820
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.003839
Total gradient norm: 0.012967
=== Actor Training Debug (Iteration 1688) ===
Q mean: -40.282345
Q std: 18.964911
Actor loss: 40.286320
Action reg: 0.003975
  l1.weight: grad_norm = 0.009389
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.022785
Total gradient norm: 0.033438
=== Actor Training Debug (Iteration 1689) ===
Q mean: -41.023788
Q std: 17.339252
Actor loss: 41.027782
Action reg: 0.003995
  l1.weight: grad_norm = 0.001284
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.003821
Total gradient norm: 0.007931
=== Actor Training Debug (Iteration 1690) ===
Q mean: -40.572845
Q std: 16.896614
Actor loss: 40.576839
Action reg: 0.003995
  l1.weight: grad_norm = 0.002376
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.007433
Total gradient norm: 0.013041
=== Actor Training Debug (Iteration 1691) ===
Q mean: -41.176003
Q std: 18.746468
Actor loss: 41.179985
Action reg: 0.003983
  l1.weight: grad_norm = 0.013437
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.038939
Total gradient norm: 0.067519
=== Actor Training Debug (Iteration 1692) ===
Q mean: -40.223412
Q std: 16.175369
Actor loss: 40.227406
Action reg: 0.003994
  l1.weight: grad_norm = 0.000260
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.000819
Total gradient norm: 0.002202
=== Actor Training Debug (Iteration 1693) ===
Q mean: -40.424164
Q std: 18.210627
Actor loss: 40.428150
Action reg: 0.003988
  l1.weight: grad_norm = 0.013677
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.034149
Total gradient norm: 0.066414
=== Actor Training Debug (Iteration 1694) ===
Q mean: -40.650879
Q std: 18.727346
Actor loss: 40.654854
Action reg: 0.003974
  l1.weight: grad_norm = 0.098755
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.308980
Total gradient norm: 0.545027
=== Actor Training Debug (Iteration 1695) ===
Q mean: -39.025932
Q std: 18.942856
Actor loss: 39.029911
Action reg: 0.003980
  l1.weight: grad_norm = 0.032183
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.084993
Total gradient norm: 0.117642
=== Actor Training Debug (Iteration 1696) ===
Q mean: -40.534431
Q std: 18.788517
Actor loss: 40.538422
Action reg: 0.003990
  l1.weight: grad_norm = 0.000579
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.001959
Total gradient norm: 0.005333
=== Actor Training Debug (Iteration 1697) ===
Q mean: -42.186977
Q std: 17.057846
Actor loss: 42.190960
Action reg: 0.003982
  l1.weight: grad_norm = 0.053240
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.146682
Total gradient norm: 0.297050
=== Actor Training Debug (Iteration 1698) ===
Q mean: -40.856461
Q std: 17.314837
Actor loss: 40.860432
Action reg: 0.003970
  l1.weight: grad_norm = 0.092876
  l1.bias: grad_norm = 0.000591
  l2.weight: grad_norm = 0.264667
Total gradient norm: 0.470263
Total gradient norm: 0.027225ration 1203) ===
=== Actor Training Debug (Iteration 1709) ===
Q mean: -41.519810
Q std: 18.629314
Actor loss: 41.523811
Action reg: 0.004000
  l1.weight: grad_norm = 0.001301
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003671
Total gradient norm: 0.007885
=== Actor Training Debug (Iteration 1710) ===
Q mean: -39.081490
Q std: 17.206100
Actor loss: 39.085476
Action reg: 0.003986
  l1.weight: grad_norm = 0.001334
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.003308
Total gradient norm: 0.005152
=== Actor Training Debug (Iteration 1711) ===
Q mean: -40.712463
Q std: 19.067123
Actor loss: 40.716461
Action reg: 0.003998
  l1.weight: grad_norm = 0.009730
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.030010
Total gradient norm: 0.059042
=== Actor Training Debug (Iteration 1712) ===
Q mean: -40.444206
Q std: 18.804443
Actor loss: 40.448196
Action reg: 0.003991
  l1.weight: grad_norm = 0.011790
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.032695
Total gradient norm: 0.067116
=== Actor Training Debug (Iteration 1713) ===
Q mean: -43.003349
Q std: 17.624126
Actor loss: 43.007343
Action reg: 0.003995
  l1.weight: grad_norm = 0.043963
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.098921
Total gradient norm: 0.153078
=== Actor Training Debug (Iteration 1714) ===
Q mean: -42.744751
Q std: 18.266178
Actor loss: 42.748741
Action reg: 0.003989
  l1.weight: grad_norm = 0.000482
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.001980
Total gradient norm: 0.006221
=== Actor Training Debug (Iteration 1715) ===
Q mean: -40.086002
Q std: 18.069569
Actor loss: 40.089989
Action reg: 0.003985
  l1.weight: grad_norm = 0.003505
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.008989
Total gradient norm: 0.016348
=== Actor Training Debug (Iteration 1716) ===
Q mean: -37.182777
Q std: 17.356716
Actor loss: 37.186775
Action reg: 0.003999
  l1.weight: grad_norm = 0.002422
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.005839
Total gradient norm: 0.009598
=== Actor Training Debug (Iteration 1717) ===
Q mean: -39.368397
Q std: 18.245295
Actor loss: 39.372375
Action reg: 0.003980
  l1.weight: grad_norm = 0.006736
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.015915
Total gradient norm: 0.022396
=== Actor Training Debug (Iteration 1718) ===
Q mean: -40.376465
Q std: 18.982426
Actor loss: 40.380447
Action reg: 0.003982
  l1.weight: grad_norm = 0.005883
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.013598
Total gradient norm: 0.020255
=== Actor Training Debug (Iteration 1719) ===
Q mean: -41.014320
Q std: 17.546215
Actor loss: 41.018322
Action reg: 0.004000
  l1.weight: grad_norm = 0.003717
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.009289
Total gradient norm: 0.017321
=== Actor Training Debug (Iteration 1720) ===
Q mean: -43.846283
Q std: 18.146221
Actor loss: 43.850281
Action reg: 0.003998
  l1.weight: grad_norm = 0.040287
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.109114
Total gradient norm: 0.216298
=== Actor Training Debug (Iteration 1721) ===
Q mean: -41.580536
Q std: 16.991905
Actor loss: 41.584522
Action reg: 0.003986
  l1.weight: grad_norm = 0.019151
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.051892
Total gradient norm: 0.085387
=== Actor Training Debug (Iteration 1722) ===
Q mean: -39.959400
Q std: 18.480110
Actor loss: 39.963367
Action reg: 0.003969
  l1.weight: grad_norm = 0.001309
  l1.bias: grad_norm = 0.000627
  l2.weight: grad_norm = 0.004549
Total gradient norm: 0.014635
=== Actor Training Debug (Iteration 1723) ===
Q mean: -39.976196
Q std: 19.223576
Actor loss: 39.980164
Action reg: 0.003968
  l1.weight: grad_norm = 0.082940
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.222448
Total gradient norm: 0.410919
=== Actor Training Debug (Iteration 1724) ===
Q mean: -38.906094
Q std: 19.343832
Actor loss: 38.910072
Action reg: 0.003980
  l1.weight: grad_norm = 0.002294
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.006783
Total gradient norm: 0.009534
=== Actor Training Debug (Iteration 1725) ===
Q mean: -38.816063
Q std: 17.616167
Actor loss: 38.820053
Action reg: 0.003989
  l1.weight: grad_norm = 0.007117
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.017560
Total gradient norm: 0.031198
=== Actor Training Debug (Iteration 1726) ===
Q mean: -41.544083
Q std: 19.153864
Actor loss: 41.548061
Action reg: 0.003980
  l1.weight: grad_norm = 0.060800
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.171028
Total gradient norm: 0.333257
=== Actor Training Debug (Iteration 1727) ===
Q mean: -41.938690
Q std: 17.428520
Actor loss: 41.942680
Action reg: 0.003992
  l1.weight: grad_norm = 0.056146
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.157103
Total gradient norm: 0.276665
=== Actor Training Debug (Iteration 1728) ===
Q mean: -42.989838
Q std: 18.942865
Actor loss: 42.993824
Action reg: 0.003988
  l1.weight: grad_norm = 0.005507
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.014062
Total gradient norm: 0.029537
=== Actor Training Debug (Iteration 1729) ===
Q mean: -39.972954
Q std: 17.986710
Actor loss: 39.976936
Action reg: 0.003982
  l1.weight: grad_norm = 0.007180
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.020353
Total gradient norm: 0.037462
=== Actor Training Debug (Iteration 1730) ===
Q mean: -39.723640
Q std: 17.822264
Actor loss: 39.727619
Action reg: 0.003979
  l1.weight: grad_norm = 0.005576
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.015966
Total gradient norm: 0.029981
=== Actor Training Debug (Iteration 1731) ===
Q mean: -39.972435
Q std: 19.436968
Actor loss: 39.976421
Action reg: 0.003985
  l1.weight: grad_norm = 0.021523
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.060638
Total gradient norm: 0.116670
=== Actor Training Debug (Iteration 1732) ===
Q mean: -42.474712
Q std: 19.643202
Actor loss: 42.478695
Action reg: 0.003982
  l1.weight: grad_norm = 0.003099
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.008910
Total gradient norm: 0.019995
=== Actor Training Debug (Iteration 1733) ===
Q mean: -40.207462
Q std: 18.578806
Actor loss: 40.211430
Action reg: 0.003968
  l1.weight: grad_norm = 0.027760
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.073158
Total gradient norm: 0.139177
=== Actor Training Debug (Iteration 1734) ===
Q mean: -40.724297
Q std: 18.380983
Actor loss: 40.728260
Action reg: 0.003965
  l1.weight: grad_norm = 0.021351
  l1.bias: grad_norm = 0.000725
  l2.weight: grad_norm = 0.056977
Total gradient norm: 0.117042
=== Actor Training Debug (Iteration 1735) ===
Q mean: -38.590607
Q std: 17.870356
Actor loss: 38.594593
Action reg: 0.003987
  l1.weight: grad_norm = 0.012515
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.033953
Total gradient norm: 0.062357
=== Actor Training Debug (Iteration 1736) ===
Q mean: -41.689041
Q std: 17.972820
Actor loss: 41.693024
Action reg: 0.003984
  l1.weight: grad_norm = 0.020510
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.047387
Total gradient norm: 0.068550
=== Actor Training Debug (Iteration 1737) ===
Q mean: -42.397125
Q std: 18.066128
Actor loss: 42.401119
Action reg: 0.003994
  l1.weight: grad_norm = 0.000857
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.002789
Total gradient norm: 0.006847
=== Actor Training Debug (Iteration 1738) ===
Q mean: -38.753040
Q std: 16.623373
Actor loss: 38.757038
Action reg: 0.003997
  l1.weight: grad_norm = 0.038770
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.120745
Total gradient norm: 0.210884
=== Actor Training Debug (Iteration 1739) ===
Q mean: -39.209221
Q std: 16.741842
Actor loss: 39.213215
Action reg: 0.003994
  l1.weight: grad_norm = 0.000787
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.002235
Total gradient norm: 0.005017
=== Actor Training Debug (Iteration 1740) ===
Q mean: -42.630260
Q std: 17.902702
Actor loss: 42.634243
Action reg: 0.003981
  l1.weight: grad_norm = 0.009628
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.027327
Total gradient norm: 0.046962
=== Actor Training Debug (Iteration 1741) ===
Q mean: -42.695168
Q std: 18.528887
Actor loss: 42.699146
Action reg: 0.003978
  l1.weight: grad_norm = 0.037423
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.096764
Total gradient norm: 0.157045
=== Actor Training Debug (Iteration 1742) ===
Q mean: -39.450027
Q std: 20.146090
Actor loss: 39.453999
Action reg: 0.003971
  l1.weight: grad_norm = 0.004687
  l1.bias: grad_norm = 0.000710
  l2.weight: grad_norm = 0.013271
Total gradient norm: 0.025485
=== Actor Training Debug (Iteration 1743) ===
Q mean: -40.445438
Q std: 17.666773
Actor loss: 40.449421
Action reg: 0.003981
  l1.weight: grad_norm = 0.009927
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.028082
Total gradient norm: 0.045400
=== Actor Training Debug (Iteration 1744) ===
Q mean: -41.921463
Q std: 18.153549
Actor loss: 41.925449
Action reg: 0.003987
  l1.weight: grad_norm = 0.020651
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.058936
Total gradient norm: 0.125659
=== Actor Training Debug (Iteration 1745) ===
Q mean: -38.364174
Q std: 16.812195
Actor loss: 38.368145
Action reg: 0.003972
  l1.weight: grad_norm = 0.012821
  l1.bias: grad_norm = 0.000461
  l2.weight: grad_norm = 0.028084
Total gradient norm: 0.046848
=== Actor Training Debug (Iteration 1746) ===
Q mean: -40.077511
Q std: 18.091928
Actor loss: 40.081490
Action reg: 0.003977
  l1.weight: grad_norm = 0.028413
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.066277
Total gradient norm: 0.126150
=== Actor Training Debug (Iteration 1747) ===
Q mean: -40.447227
Q std: 15.720380
Actor loss: 40.451218
Action reg: 0.003989
  l1.weight: grad_norm = 0.011179
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.029367
Total gradient norm: 0.055495
=== Actor Training Debug (Iteration 1748) ===
Q mean: -37.390499
Q std: 17.890129
Actor loss: 37.394459
Action reg: 0.003960
  l1.weight: grad_norm = 0.001063
  l1.bias: grad_norm = 0.000779
  l2.weight: grad_norm = 0.005601
Total gradient norm: 0.020231
=== Actor Training Debug (Iteration 1749) ===
Q mean: -40.421261
Q std: 18.210684
Actor loss: 40.425251
Action reg: 0.003992
  l1.weight: grad_norm = 0.002239
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.006296
Total gradient norm: 0.010734
=== Actor Training Debug (Iteration 1750) ===
Q mean: -43.866234
Q std: 19.365559
Actor loss: 43.870216
Action reg: 0.003982
  l1.weight: grad_norm = 0.007415
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.018310
Total gradient norm: 0.026818
=== Actor Training Debug (Iteration 1751) ===
Q mean: -40.539383
Q std: 19.655441
Actor loss: 40.543369
Action reg: 0.003988
  l1.weight: grad_norm = 0.010924
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.033339
Total gradient norm: 0.057010
=== Actor Training Debug (Iteration 1752) ===
Q mean: -40.130562
Q std: 18.237425
Actor loss: 40.134541
Action reg: 0.003979
  l1.weight: grad_norm = 0.011790
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.032088
Total gradient norm: 0.060694
=== Actor Training Debug (Iteration 1753) ===
Q mean: -39.930222
Q std: 19.805946
Actor loss: 39.934204
Action reg: 0.003982
  l1.weight: grad_norm = 0.005126
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.012668
Total gradient norm: 0.019671
=== Actor Training Debug (Iteration 1754) ===
Q mean: -42.733070
Q std: 18.717962
Actor loss: 42.737064
Action reg: 0.003996
  l1.weight: grad_norm = 0.000386
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.001281
Total gradient norm: 0.003297
Total gradient norm: 0.015401ration 1203) ===
=== Actor Training Debug (Iteration 1765) ===
Q mean: -38.928825
Q std: 17.731852
Actor loss: 38.932796
Action reg: 0.003972
  l1.weight: grad_norm = 0.001348
  l1.bias: grad_norm = 0.000733
  l2.weight: grad_norm = 0.004464
Total gradient norm: 0.013619
=== Actor Training Debug (Iteration 1766) ===
Q mean: -41.152115
Q std: 19.294630
Actor loss: 41.156094
Action reg: 0.003979
  l1.weight: grad_norm = 0.003395
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.008047
Total gradient norm: 0.012000
=== Actor Training Debug (Iteration 1767) ===
Q mean: -40.817024
Q std: 19.254705
Actor loss: 40.820999
Action reg: 0.003974
  l1.weight: grad_norm = 0.022796
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.064290
Total gradient norm: 0.111932
=== Actor Training Debug (Iteration 1768) ===
Q mean: -41.857620
Q std: 18.093800
Actor loss: 41.861618
Action reg: 0.003997
  l1.weight: grad_norm = 0.032647
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.102457
Total gradient norm: 0.176057
=== Actor Training Debug (Iteration 1769) ===
Q mean: -39.539059
Q std: 18.131340
Actor loss: 39.543022
Action reg: 0.003965
  l1.weight: grad_norm = 0.013300
  l1.bias: grad_norm = 0.000673
  l2.weight: grad_norm = 0.034660
Total gradient norm: 0.059538
=== Actor Training Debug (Iteration 1770) ===
Q mean: -43.179047
Q std: 18.128572
Actor loss: 43.183033
Action reg: 0.003986
  l1.weight: grad_norm = 0.014875
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.040372
Total gradient norm: 0.068802
=== Actor Training Debug (Iteration 1771) ===
Q mean: -39.249969
Q std: 17.245014
Actor loss: 39.253925
Action reg: 0.003957
  l1.weight: grad_norm = 0.001050
  l1.bias: grad_norm = 0.000991
  l2.weight: grad_norm = 0.005637
Total gradient norm: 0.019459
=== Actor Training Debug (Iteration 1772) ===
Q mean: -41.991875
Q std: 18.110876
Actor loss: 41.995869
Action reg: 0.003995
  l1.weight: grad_norm = 0.000904
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.002188
Total gradient norm: 0.003916
=== Actor Training Debug (Iteration 1773) ===
Q mean: -44.188576
Q std: 19.266315
Actor loss: 44.192577
Action reg: 0.004000
  l1.weight: grad_norm = 0.000105
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000258
Total gradient norm: 0.000477
=== Actor Training Debug (Iteration 1774) ===
Q mean: -42.568798
Q std: 18.711266
Actor loss: 42.572784
Action reg: 0.003986
  l1.weight: grad_norm = 0.011787
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.034543
Total gradient norm: 0.079588
=== Actor Training Debug (Iteration 1775) ===
Q mean: -39.807449
Q std: 16.498976
Actor loss: 39.811447
Action reg: 0.003998
  l1.weight: grad_norm = 0.019336
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.047695
Total gradient norm: 0.064118
=== Actor Training Debug (Iteration 1776) ===
Q mean: -38.677521
Q std: 19.111729
Actor loss: 38.681515
Action reg: 0.003992
  l1.weight: grad_norm = 0.041723
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.112625
Total gradient norm: 0.196753
=== Actor Training Debug (Iteration 1777) ===
Q mean: -41.287323
Q std: 18.394564
Actor loss: 41.291309
Action reg: 0.003985
  l1.weight: grad_norm = 0.000637
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.002720
Total gradient norm: 0.009047
=== Actor Training Debug (Iteration 1778) ===
Q mean: -43.840992
Q std: 19.804510
Actor loss: 43.844975
Action reg: 0.003984
  l1.weight: grad_norm = 0.020532
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.053623
Total gradient norm: 0.090173
=== Actor Training Debug (Iteration 1779) ===
Q mean: -40.486824
Q std: 16.813623
Actor loss: 40.490799
Action reg: 0.003976
  l1.weight: grad_norm = 0.047643
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.143388
Total gradient norm: 0.264062
=== Actor Training Debug (Iteration 1780) ===
Q mean: -39.898346
Q std: 17.815369
Actor loss: 39.902332
Action reg: 0.003987
  l1.weight: grad_norm = 0.001755
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.004813
Total gradient norm: 0.006940
=== Actor Training Debug (Iteration 1781) ===
Q mean: -44.484783
Q std: 18.819628
Actor loss: 44.488770
Action reg: 0.003985
  l1.weight: grad_norm = 0.055645
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.148440
Total gradient norm: 0.255151
=== Actor Training Debug (Iteration 1782) ===
Q mean: -41.322685
Q std: 16.771763
Actor loss: 41.326679
Action reg: 0.003993
  l1.weight: grad_norm = 0.006362
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.020009
Total gradient norm: 0.039837
=== Actor Training Debug (Iteration 1783) ===
Q mean: -39.888664
Q std: 17.852356
Actor loss: 39.892647
Action reg: 0.003983
  l1.weight: grad_norm = 0.004643
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.012727
Total gradient norm: 0.026571
=== Actor Training Debug (Iteration 1784) ===
Q mean: -41.687920
Q std: 19.309572
Actor loss: 41.691914
Action reg: 0.003996
  l1.weight: grad_norm = 0.004216
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.010261
Total gradient norm: 0.018763
=== Actor Training Debug (Iteration 1785) ===
Q mean: -42.753113
Q std: 18.437590
Actor loss: 42.757099
Action reg: 0.003988
  l1.weight: grad_norm = 0.000467
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.001740
Total gradient norm: 0.005153
=== Actor Training Debug (Iteration 1786) ===
Q mean: -40.413750
Q std: 17.786945
Actor loss: 40.417744
Action reg: 0.003992
  l1.weight: grad_norm = 0.019337
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.055557
Total gradient norm: 0.099018
=== Actor Training Debug (Iteration 1787) ===
Q mean: -40.443016
Q std: 17.163528
Actor loss: 40.447014
Action reg: 0.003997
  l1.weight: grad_norm = 0.087769
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.236695
Total gradient norm: 0.453742
=== Actor Training Debug (Iteration 1788) ===
Q mean: -43.161324
Q std: 18.907705
Actor loss: 43.165306
Action reg: 0.003984
  l1.weight: grad_norm = 0.009437
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.022060
Total gradient norm: 0.034630
=== Actor Training Debug (Iteration 1789) ===
Q mean: -42.877003
Q std: 19.071774
Actor loss: 42.880997
Action reg: 0.003992
  l1.weight: grad_norm = 0.021659
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.055162
Total gradient norm: 0.089229
=== Actor Training Debug (Iteration 1790) ===
Q mean: -39.248802
Q std: 16.884193
Actor loss: 39.252789
Action reg: 0.003987
  l1.weight: grad_norm = 0.000956
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.002495
Total gradient norm: 0.004932
=== Actor Training Debug (Iteration 1791) ===
Q mean: -39.417206
Q std: 16.956440
Actor loss: 39.421188
Action reg: 0.003981
  l1.weight: grad_norm = 0.002461
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.006647
Total gradient norm: 0.010623
=== Actor Training Debug (Iteration 1792) ===
Q mean: -44.838501
Q std: 19.138977
Actor loss: 44.842487
Action reg: 0.003987
  l1.weight: grad_norm = 0.018820
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.053342
Total gradient norm: 0.094993
=== Actor Training Debug (Iteration 1793) ===
Q mean: -43.734756
Q std: 18.548616
Actor loss: 43.738750
Action reg: 0.003994
  l1.weight: grad_norm = 0.001206
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.003330
Total gradient norm: 0.005099
=== Actor Training Debug (Iteration 1794) ===
Q mean: -40.242912
Q std: 18.120083
Actor loss: 40.246899
Action reg: 0.003987
  l1.weight: grad_norm = 0.003824
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.010926
Total gradient norm: 0.025076
=== Actor Training Debug (Iteration 1795) ===
Q mean: -39.636581
Q std: 19.017263
Actor loss: 39.640556
Action reg: 0.003974
  l1.weight: grad_norm = 0.030218
  l1.bias: grad_norm = 0.000524
  l2.weight: grad_norm = 0.082321
Total gradient norm: 0.143114
=== Actor Training Debug (Iteration 1796) ===
Q mean: -42.279350
Q std: 17.034599
Actor loss: 42.283337
Action reg: 0.003986
  l1.weight: grad_norm = 0.032588
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.090734
Total gradient norm: 0.146471
=== Actor Training Debug (Iteration 1797) ===
Q mean: -41.888908
Q std: 18.233040
Actor loss: 41.892895
Action reg: 0.003985
  l1.weight: grad_norm = 0.007450
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.021023
Total gradient norm: 0.044880
=== Actor Training Debug (Iteration 1798) ===
Q mean: -40.529320
Q std: 17.045971
Actor loss: 40.533318
Action reg: 0.003998
  l1.weight: grad_norm = 0.017566
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.047502
Total gradient norm: 0.089401
=== Actor Training Debug (Iteration 1799) ===
Q mean: -39.738491
Q std: 16.596388
Actor loss: 39.742474
Action reg: 0.003984
  l1.weight: grad_norm = 0.000618
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.002676
Total gradient norm: 0.008816
=== Actor Training Debug (Iteration 1800) ===
Q mean: -39.885475
Q std: 18.866461
Actor loss: 39.889454
Action reg: 0.003979
  l1.weight: grad_norm = 0.002002
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.005293
Total gradient norm: 0.008325
=== Actor Training Debug (Iteration 1801) ===
Q mean: -41.903709
Q std: 19.529530
Actor loss: 41.907700
Action reg: 0.003991
  l1.weight: grad_norm = 0.004922
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.013141
Total gradient norm: 0.026101
=== Actor Training Debug (Iteration 1802) ===
Q mean: -41.898243
Q std: 18.563856
Actor loss: 41.902233
Action reg: 0.003988
  l1.weight: grad_norm = 0.025034
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.070386
Total gradient norm: 0.129609
=== Actor Training Debug (Iteration 1803) ===
Q mean: -40.498184
Q std: 19.432066
Actor loss: 40.502167
Action reg: 0.003983
  l1.weight: grad_norm = 0.016361
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.044780
Total gradient norm: 0.085089
=== Actor Training Debug (Iteration 1804) ===
Q mean: -40.824631
Q std: 17.937489
Actor loss: 40.828602
Action reg: 0.003971
  l1.weight: grad_norm = 0.003235
  l1.bias: grad_norm = 0.000604
  l2.weight: grad_norm = 0.007396
Total gradient norm: 0.012416
=== Actor Training Debug (Iteration 1805) ===
Q mean: -41.901623
Q std: 17.038971
Actor loss: 41.905613
Action reg: 0.003990
  l1.weight: grad_norm = 0.000401
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.001547
Total gradient norm: 0.004568
=== Actor Training Debug (Iteration 1806) ===
Q mean: -40.897583
Q std: 18.628950
Actor loss: 40.901566
Action reg: 0.003981
  l1.weight: grad_norm = 0.003829
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.009394
Total gradient norm: 0.015855
=== Actor Training Debug (Iteration 1807) ===
Q mean: -40.700005
Q std: 17.907019
Actor loss: 40.703991
Action reg: 0.003985
  l1.weight: grad_norm = 0.002683
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.007143
Total gradient norm: 0.017254
=== Actor Training Debug (Iteration 1808) ===
Q mean: -40.839108
Q std: 15.858091
Actor loss: 40.843102
Action reg: 0.003992
  l1.weight: grad_norm = 0.004425
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.012688
Total gradient norm: 0.020541
=== Actor Training Debug (Iteration 1809) ===
Q mean: -40.797123
Q std: 17.814535
Actor loss: 40.801113
Action reg: 0.003989
  l1.weight: grad_norm = 0.016200
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.042042
Total gradient norm: 0.085207
=== Actor Training Debug (Iteration 1810) ===
Q mean: -40.842705
Q std: 17.212566
Actor loss: 40.846691
Action reg: 0.003986
  l1.weight: grad_norm = 0.028669
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.064613
Total gradient norm: 0.104668
=== Actor Training Debug (Iteration 1811) ===
Q mean: -40.776554
Q std: 20.889017
Actor loss: 40.780529
Action reg: 0.003974
  l1.weight: grad_norm = 0.010677
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.023804
Total gradient norm: 0.039917
=== Actor Training Debug (Iteration 1812) ===
Q mean: -41.939995
Q std: 20.038662
Actor loss: 41.943974
Action reg: 0.003978
  l1.weight: grad_norm = 0.001548
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.005606
Total gradient norm: 0.014772
=== Actor Training Debug (Iteration 1813) ===
Q mean: -40.343430
Q std: 19.362474
Actor loss: 40.347416
Action reg: 0.003985
  l1.weight: grad_norm = 0.003234
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.010061
Total gradient norm: 0.020339
=== Actor Training Debug (Iteration 1814) ===
Q mean: -40.892075
Q std: 17.377802
Actor loss: 40.896057
Action reg: 0.003981
  l1.weight: grad_norm = 0.000527
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.002179
Total gradient norm: 0.006973
=== Actor Training Debug (Iteration 1815) ===
Q mean: -40.495071
Q std: 17.053864
Actor loss: 40.499054
Action reg: 0.003984
  l1.weight: grad_norm = 0.001612
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.005162
Total gradient norm: 0.013921
=== Actor Training Debug (Iteration 1816) ===
Q mean: -43.699703
Q std: 17.747910
Actor loss: 43.703693
Action reg: 0.003989
  l1.weight: grad_norm = 0.000686
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.002452
Total gradient norm: 0.007095
Total gradient norm: 0.121546ration 1203) ===
=== Actor Training Debug (Iteration 1827) ===
Q mean: -41.377457
Q std: 17.372709
Actor loss: 41.381439
Action reg: 0.003983
  l1.weight: grad_norm = 0.002323
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.007660
Total gradient norm: 0.018709
=== Actor Training Debug (Iteration 1828) ===
Q mean: -42.412586
Q std: 16.872221
Actor loss: 42.416584
Action reg: 0.003999
  l1.weight: grad_norm = 0.000572
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001492
Total gradient norm: 0.003003
=== Actor Training Debug (Iteration 1829) ===
Q mean: -43.299488
Q std: 18.127884
Actor loss: 43.303463
Action reg: 0.003975
  l1.weight: grad_norm = 0.029369
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.092737
Total gradient norm: 0.185273
=== Actor Training Debug (Iteration 1830) ===
Q mean: -41.407166
Q std: 17.175608
Actor loss: 41.411156
Action reg: 0.003991
  l1.weight: grad_norm = 0.085743
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.250290
Total gradient norm: 0.436508
=== Actor Training Debug (Iteration 1831) ===
Q mean: -41.428009
Q std: 19.585304
Actor loss: 41.431976
Action reg: 0.003966
  l1.weight: grad_norm = 0.029588
  l1.bias: grad_norm = 0.000533
  l2.weight: grad_norm = 0.084263
Total gradient norm: 0.158986
=== Actor Training Debug (Iteration 1832) ===
Q mean: -43.122253
Q std: 16.702250
Actor loss: 43.126236
Action reg: 0.003984
  l1.weight: grad_norm = 0.024527
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.069724
Total gradient norm: 0.122674
=== Actor Training Debug (Iteration 1833) ===
Q mean: -39.448509
Q std: 19.433722
Actor loss: 39.452477
Action reg: 0.003968
  l1.weight: grad_norm = 0.054597
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.147254
Total gradient norm: 0.304851
=== Actor Training Debug (Iteration 1834) ===
Q mean: -42.798576
Q std: 18.915524
Actor loss: 42.802567
Action reg: 0.003989
  l1.weight: grad_norm = 0.128771
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.375654
Total gradient norm: 0.742027
=== Actor Training Debug (Iteration 1835) ===
Q mean: -41.464569
Q std: 18.244213
Actor loss: 41.468552
Action reg: 0.003981
  l1.weight: grad_norm = 0.027724
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.067144
Total gradient norm: 0.112084
=== Actor Training Debug (Iteration 1836) ===
Q mean: -41.291107
Q std: 16.743835
Actor loss: 41.295082
Action reg: 0.003977
  l1.weight: grad_norm = 0.007042
  l1.bias: grad_norm = 0.000478
  l2.weight: grad_norm = 0.019499
Total gradient norm: 0.036247
=== Actor Training Debug (Iteration 1837) ===
Q mean: -43.688133
Q std: 18.067738
Actor loss: 43.692108
Action reg: 0.003975
  l1.weight: grad_norm = 0.020755
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.048105
Total gradient norm: 0.089642
=== Actor Training Debug (Iteration 1838) ===
Q mean: -41.413170
Q std: 16.656755
Actor loss: 41.417149
Action reg: 0.003978
  l1.weight: grad_norm = 0.011824
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.025310
Total gradient norm: 0.034394
=== Actor Training Debug (Iteration 1839) ===
Q mean: -39.905418
Q std: 18.429924
Actor loss: 39.909397
Action reg: 0.003979
  l1.weight: grad_norm = 0.006332
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.016646
Total gradient norm: 0.035854
=== Actor Training Debug (Iteration 1840) ===
Q mean: -40.425339
Q std: 19.380051
Actor loss: 40.429317
Action reg: 0.003980
  l1.weight: grad_norm = 0.004705
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.013437
Total gradient norm: 0.026992
=== Actor Training Debug (Iteration 1841) ===
Q mean: -40.000446
Q std: 19.708355
Actor loss: 40.004417
Action reg: 0.003971
  l1.weight: grad_norm = 0.038708
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.112787
Total gradient norm: 0.224335
=== Actor Training Debug (Iteration 1842) ===
Q mean: -43.989296
Q std: 18.471977
Actor loss: 43.993279
Action reg: 0.003982
  l1.weight: grad_norm = 0.005891
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.016429
Total gradient norm: 0.028205
=== Actor Training Debug (Iteration 1843) ===
Q mean: -41.608448
Q std: 18.492462
Actor loss: 41.612423
Action reg: 0.003976
  l1.weight: grad_norm = 0.001187
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.004560
Total gradient norm: 0.013774
=== Actor Training Debug (Iteration 1844) ===
Q mean: -42.757160
Q std: 18.129387
Actor loss: 42.761139
Action reg: 0.003980
  l1.weight: grad_norm = 0.047455
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.122056
Total gradient norm: 0.198796
=== Actor Training Debug (Iteration 1845) ===
Q mean: -42.512672
Q std: 17.895527
Actor loss: 42.516659
Action reg: 0.003986
  l1.weight: grad_norm = 0.000870
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.003401
Total gradient norm: 0.009407
=== Actor Training Debug (Iteration 1846) ===
Q mean: -41.777565
Q std: 20.208158
Actor loss: 41.781555
Action reg: 0.003989
  l1.weight: grad_norm = 0.000457
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.001924
Total gradient norm: 0.006116
=== Actor Training Debug (Iteration 1847) ===
Q mean: -42.373360
Q std: 18.599941
Actor loss: 42.377338
Action reg: 0.003977
  l1.weight: grad_norm = 0.087825
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.277723
Total gradient norm: 0.558433
=== Actor Training Debug (Iteration 1848) ===
Q mean: -41.428337
Q std: 18.680840
Actor loss: 41.432308
Action reg: 0.003970
  l1.weight: grad_norm = 0.000832
  l1.bias: grad_norm = 0.000652
  l2.weight: grad_norm = 0.004146
Total gradient norm: 0.014036
=== Actor Training Debug (Iteration 1849) ===
Q mean: -43.767296
Q std: 16.565483
Actor loss: 43.771282
Action reg: 0.003988
  l1.weight: grad_norm = 0.023545
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.059542
Total gradient norm: 0.119625
=== Actor Training Debug (Iteration 1850) ===
Q mean: -42.854706
Q std: 19.715393
Actor loss: 42.858704
Action reg: 0.003997
  l1.weight: grad_norm = 0.010279
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.026504
Total gradient norm: 0.054356
=== Actor Training Debug (Iteration 1851) ===
Q mean: -42.717918
Q std: 18.443150
Actor loss: 42.721909
Action reg: 0.003990
  l1.weight: grad_norm = 0.019839
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.061776
Total gradient norm: 0.136753
=== Actor Training Debug (Iteration 1852) ===
Q mean: -40.471550
Q std: 18.262163
Actor loss: 40.475517
Action reg: 0.003969
  l1.weight: grad_norm = 0.001922
  l1.bias: grad_norm = 0.000627
  l2.weight: grad_norm = 0.007004
Total gradient norm: 0.020506
=== Actor Training Debug (Iteration 1853) ===
Q mean: -44.207436
Q std: 17.994072
Actor loss: 44.211430
Action reg: 0.003993
  l1.weight: grad_norm = 0.002977
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.009251
Total gradient norm: 0.019428
=== Actor Training Debug (Iteration 1854) ===
Q mean: -43.175125
Q std: 17.793871
Actor loss: 43.179111
Action reg: 0.003986
  l1.weight: grad_norm = 0.015492
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.039614
Total gradient norm: 0.064136
=== Actor Training Debug (Iteration 1855) ===
Q mean: -40.215813
Q std: 17.640001
Actor loss: 40.219807
Action reg: 0.003993
  l1.weight: grad_norm = 0.004160
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.010118
Total gradient norm: 0.018829
=== Actor Training Debug (Iteration 1856) ===
Q mean: -42.587509
Q std: 19.018591
Actor loss: 42.591496
Action reg: 0.003986
  l1.weight: grad_norm = 0.067824
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.208199
Total gradient norm: 0.373904
=== Actor Training Debug (Iteration 1857) ===
Q mean: -46.170071
Q std: 17.262568
Actor loss: 46.174057
Action reg: 0.003988
  l1.weight: grad_norm = 0.014861
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.035120
Total gradient norm: 0.061557
=== Actor Training Debug (Iteration 1858) ===
Q mean: -42.199509
Q std: 18.017193
Actor loss: 42.203491
Action reg: 0.003982
  l1.weight: grad_norm = 0.002763
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.006981
Total gradient norm: 0.012642
=== Actor Training Debug (Iteration 1859) ===
Q mean: -42.848785
Q std: 18.649933
Actor loss: 42.852760
Action reg: 0.003977
  l1.weight: grad_norm = 0.013946
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.034990
Total gradient norm: 0.053254
=== Actor Training Debug (Iteration 1860) ===
Q mean: -40.616272
Q std: 17.937042
Actor loss: 40.620258
Action reg: 0.003988
  l1.weight: grad_norm = 0.123158
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.339286
Total gradient norm: 0.706511
=== Actor Training Debug (Iteration 1861) ===
Q mean: -43.396122
Q std: 19.362240
Actor loss: 43.400108
Action reg: 0.003985
  l1.weight: grad_norm = 0.017762
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.048823
Total gradient norm: 0.086280
=== Actor Training Debug (Iteration 1862) ===
Q mean: -41.581070
Q std: 16.921129
Actor loss: 41.585052
Action reg: 0.003982
  l1.weight: grad_norm = 0.029277
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.063777
Total gradient norm: 0.086127
=== Actor Training Debug (Iteration 1863) ===
Q mean: -41.504974
Q std: 16.547739
Actor loss: 41.508961
Action reg: 0.003988
  l1.weight: grad_norm = 0.032475
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.085039
Total gradient norm: 0.161929
=== Actor Training Debug (Iteration 1864) ===
Q mean: -44.615532
Q std: 20.282778
Actor loss: 44.619518
Action reg: 0.003988
  l1.weight: grad_norm = 0.000508
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.001987
Total gradient norm: 0.006092
=== Actor Training Debug (Iteration 1865) ===
Q mean: -44.226482
Q std: 21.289610
Actor loss: 44.230465
Action reg: 0.003982
  l1.weight: grad_norm = 0.009315
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.024051
Total gradient norm: 0.037644
=== Actor Training Debug (Iteration 1866) ===
Q mean: -41.968735
Q std: 17.746567
Actor loss: 41.972736
Action reg: 0.004000
  l1.weight: grad_norm = 0.000028
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000072
Total gradient norm: 0.000106
=== Actor Training Debug (Iteration 1867) ===
Q mean: -42.860756
Q std: 20.822123
Actor loss: 42.864750
Action reg: 0.003994
  l1.weight: grad_norm = 0.000430
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.001345
Total gradient norm: 0.003451
=== Actor Training Debug (Iteration 1868) ===
Q mean: -41.652367
Q std: 18.559597
Actor loss: 41.656330
Action reg: 0.003962
  l1.weight: grad_norm = 0.000914
  l1.bias: grad_norm = 0.000688
  l2.weight: grad_norm = 0.005195
Total gradient norm: 0.018269
=== Actor Training Debug (Iteration 1869) ===
Q mean: -39.926857
Q std: 17.290472
Actor loss: 39.930847
Action reg: 0.003989
  l1.weight: grad_norm = 0.000486
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.001912
Total gradient norm: 0.005857
=== Actor Training Debug (Iteration 1870) ===
Q mean: -43.406281
Q std: 17.815811
Actor loss: 43.410252
Action reg: 0.003970
  l1.weight: grad_norm = 0.000545
  l1.bias: grad_norm = 0.000543
  l2.weight: grad_norm = 0.003475
Total gradient norm: 0.012234
=== Actor Training Debug (Iteration 1871) ===
Q mean: -43.458908
Q std: 18.644827
Actor loss: 43.462902
Action reg: 0.003995
  l1.weight: grad_norm = 0.000283
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.000989
Total gradient norm: 0.002840
=== Actor Training Debug (Iteration 1872) ===
Q mean: -40.573006
Q std: 20.333506
Actor loss: 40.576977
Action reg: 0.003972
  l1.weight: grad_norm = 0.034835
  l1.bias: grad_norm = 0.000591
  l2.weight: grad_norm = 0.107425
Total gradient norm: 0.194925
=== Actor Training Debug (Iteration 1873) ===
Q mean: -39.449326
Q std: 18.503979
Actor loss: 39.453300
Action reg: 0.003974
  l1.weight: grad_norm = 0.016821
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.044461
Total gradient norm: 0.093888
=== Actor Training Debug (Iteration 1874) ===
Q mean: -40.399364
Q std: 17.733404
Actor loss: 40.403351
Action reg: 0.003986
  l1.weight: grad_norm = 0.060833
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.188636
Total gradient norm: 0.344576
=== Actor Training Debug (Iteration 1875) ===
Q mean: -42.100754
Q std: 18.671503
Actor loss: 42.104717
Action reg: 0.003963
  l1.weight: grad_norm = 0.006975
  l1.bias: grad_norm = 0.000709
  l2.weight: grad_norm = 0.020641
Total gradient norm: 0.046868
=== Actor Training Debug (Iteration 1876) ===
Q mean: -45.897171
Q std: 17.796362
Actor loss: 45.901157
Action reg: 0.003988
  l1.weight: grad_norm = 0.030063
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.080709
Total gradient norm: 0.167911
=== Actor Training Debug (Iteration 1877) ===
Q mean: -44.288406
Q std: 17.869463
Actor loss: 44.292397
Action reg: 0.003990
  l1.weight: grad_norm = 0.003392
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.008698
Total gradient norm: 0.016251
Total gradient norm: 0.007727ration 1203) ===
=== Actor Training Debug (Iteration 1888) ===
Q mean: -42.931633
Q std: 17.913465
Actor loss: 42.935616
Action reg: 0.003982
  l1.weight: grad_norm = 0.000697
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.002945
Total gradient norm: 0.009530
=== Actor Training Debug (Iteration 1889) ===
Q mean: -42.105324
Q std: 16.753189
Actor loss: 42.109303
Action reg: 0.003980
  l1.weight: grad_norm = 0.013596
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.039607
Total gradient norm: 0.071016
=== Actor Training Debug (Iteration 1890) ===
Q mean: -42.075645
Q std: 16.286640
Actor loss: 42.079639
Action reg: 0.003994
  l1.weight: grad_norm = 0.000390
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.001346
Total gradient norm: 0.003652
=== Actor Training Debug (Iteration 1891) ===
Q mean: -45.022652
Q std: 15.998898
Actor loss: 45.026646
Action reg: 0.003994
  l1.weight: grad_norm = 0.000234
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.000975
Total gradient norm: 0.002412
=== Actor Training Debug (Iteration 1892) ===
Q mean: -41.300591
Q std: 17.581615
Actor loss: 41.304573
Action reg: 0.003984
  l1.weight: grad_norm = 0.002335
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.006382
Total gradient norm: 0.013754
=== Actor Training Debug (Iteration 1893) ===
Q mean: -43.010738
Q std: 17.984600
Actor loss: 43.014740
Action reg: 0.004000
  l1.weight: grad_norm = 0.000033
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000093
Total gradient norm: 0.000169
=== Actor Training Debug (Iteration 1894) ===
Q mean: -42.295830
Q std: 19.291025
Actor loss: 42.299805
Action reg: 0.003976
  l1.weight: grad_norm = 0.000640
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.003248
Total gradient norm: 0.010998
=== Actor Training Debug (Iteration 1895) ===
Q mean: -41.791515
Q std: 17.985252
Actor loss: 41.795498
Action reg: 0.003983
  l1.weight: grad_norm = 0.000514
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.002264
Total gradient norm: 0.007698
=== Actor Training Debug (Iteration 1896) ===
Q mean: -42.495384
Q std: 17.252987
Actor loss: 42.499371
Action reg: 0.003987
  l1.weight: grad_norm = 0.034302
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.094359
Total gradient norm: 0.184570
=== Actor Training Debug (Iteration 1897) ===
Q mean: -41.873894
Q std: 16.956768
Actor loss: 41.877872
Action reg: 0.003980
  l1.weight: grad_norm = 0.010783
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.035054
Total gradient norm: 0.057126
=== Actor Training Debug (Iteration 1898) ===
Q mean: -43.226402
Q std: 19.042444
Actor loss: 43.230377
Action reg: 0.003975
  l1.weight: grad_norm = 0.003275
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.009360
Total gradient norm: 0.021114
=== Actor Training Debug (Iteration 1899) ===
Q mean: -42.881935
Q std: 18.119469
Actor loss: 42.885933
Action reg: 0.003998
  l1.weight: grad_norm = 0.029572
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.085849
Total gradient norm: 0.157400
=== Actor Training Debug (Iteration 1900) ===
Q mean: -41.412403
Q std: 19.579134
Actor loss: 41.416382
Action reg: 0.003979
  l1.weight: grad_norm = 0.000874
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.003179
Total gradient norm: 0.008987
=== Actor Training Debug (Iteration 1901) ===
Q mean: -39.757080
Q std: 17.333212
Actor loss: 39.761063
Action reg: 0.003981
  l1.weight: grad_norm = 0.000517
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.001939
Total gradient norm: 0.005752
=== Actor Training Debug (Iteration 1902) ===
Q mean: -42.440804
Q std: 18.618359
Actor loss: 42.444786
Action reg: 0.003983
  l1.weight: grad_norm = 0.000477
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.002250
Total gradient norm: 0.007348
=== Actor Training Debug (Iteration 1903) ===
Q mean: -43.604778
Q std: 17.920301
Actor loss: 43.608776
Action reg: 0.003999
  l1.weight: grad_norm = 0.007579
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.019887
Total gradient norm: 0.039161
=== Actor Training Debug (Iteration 1904) ===
Q mean: -44.761269
Q std: 17.512550
Actor loss: 44.765251
Action reg: 0.003982
  l1.weight: grad_norm = 0.000586
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.002684
Total gradient norm: 0.008872
=== Actor Training Debug (Iteration 1905) ===
Q mean: -46.905529
Q std: 16.667391
Actor loss: 46.909527
Action reg: 0.003997
  l1.weight: grad_norm = 0.021791
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.057752
Total gradient norm: 0.115837
=== Actor Training Debug (Iteration 1906) ===
Q mean: -43.244293
Q std: 17.566109
Actor loss: 43.248287
Action reg: 0.003993
  l1.weight: grad_norm = 0.000386
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.001365
Total gradient norm: 0.004045
=== Actor Training Debug (Iteration 1907) ===
Q mean: -41.256039
Q std: 17.227840
Actor loss: 41.260025
Action reg: 0.003988
  l1.weight: grad_norm = 0.004909
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.013394
Total gradient norm: 0.022565
=== Actor Training Debug (Iteration 1908) ===
Q mean: -43.580612
Q std: 17.496479
Actor loss: 43.584599
Action reg: 0.003985
  l1.weight: grad_norm = 0.008018
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.023903
Total gradient norm: 0.049425
=== Actor Training Debug (Iteration 1909) ===
Q mean: -46.242958
Q std: 18.899353
Actor loss: 46.246941
Action reg: 0.003982
  l1.weight: grad_norm = 0.000476
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.002489
Total gradient norm: 0.008402
=== Actor Training Debug (Iteration 1910) ===
Q mean: -42.166851
Q std: 18.156612
Actor loss: 42.170849
Action reg: 0.003999
  l1.weight: grad_norm = 0.001835
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.005028
Total gradient norm: 0.008697
=== Actor Training Debug (Iteration 1911) ===
Q mean: -42.925201
Q std: 18.769793
Actor loss: 42.929203
Action reg: 0.004000
  l1.weight: grad_norm = 0.000152
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000415
Total gradient norm: 0.000838
=== Actor Training Debug (Iteration 1912) ===
Q mean: -40.028343
Q std: 16.481377
Actor loss: 40.032314
Action reg: 0.003972
  l1.weight: grad_norm = 0.000774
  l1.bias: grad_norm = 0.000534
  l2.weight: grad_norm = 0.003888
Total gradient norm: 0.012952
=== Actor Training Debug (Iteration 1913) ===
Q mean: -40.342499
Q std: 17.018105
Actor loss: 40.346478
Action reg: 0.003978
  l1.weight: grad_norm = 0.000459
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.002111
Total gradient norm: 0.007212
=== Actor Training Debug (Iteration 1914) ===
Q mean: -43.052628
Q std: 18.874214
Actor loss: 43.056602
Action reg: 0.003974
  l1.weight: grad_norm = 0.009313
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.024857
Total gradient norm: 0.046624
=== Actor Training Debug (Iteration 1915) ===
Q mean: -45.316280
Q std: 20.230953
Actor loss: 45.320263
Action reg: 0.003982
  l1.weight: grad_norm = 0.017405
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.046985
Total gradient norm: 0.092062
=== Actor Training Debug (Iteration 1916) ===
Q mean: -45.025917
Q std: 19.944456
Actor loss: 45.029900
Action reg: 0.003981
  l1.weight: grad_norm = 0.022706
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.061285
Total gradient norm: 0.120332
=== Actor Training Debug (Iteration 1917) ===
Q mean: -42.162930
Q std: 18.406715
Actor loss: 42.166924
Action reg: 0.003995
  l1.weight: grad_norm = 0.000244
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.000724
Total gradient norm: 0.001509
=== Actor Training Debug (Iteration 1918) ===
Q mean: -41.835754
Q std: 17.760983
Actor loss: 41.839741
Action reg: 0.003988
  l1.weight: grad_norm = 0.000309
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.001269
Total gradient norm: 0.003851
=== Actor Training Debug (Iteration 1919) ===
Q mean: -43.314552
Q std: 17.984558
Actor loss: 43.318535
Action reg: 0.003981
  l1.weight: grad_norm = 0.000497
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.002629
Total gradient norm: 0.008719
=== Actor Training Debug (Iteration 1920) ===
Q mean: -42.914062
Q std: 17.775673
Actor loss: 42.918045
Action reg: 0.003982
  l1.weight: grad_norm = 0.008563
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.020729
Total gradient norm: 0.032661
=== Actor Training Debug (Iteration 1921) ===
Q mean: -44.646347
Q std: 18.990509
Actor loss: 44.650333
Action reg: 0.003986
  l1.weight: grad_norm = 0.037294
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.095400
Total gradient norm: 0.194719
=== Actor Training Debug (Iteration 1922) ===
Q mean: -45.673454
Q std: 20.240313
Actor loss: 45.677448
Action reg: 0.003994
  l1.weight: grad_norm = 0.000351
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.001274
Total gradient norm: 0.003473
=== Actor Training Debug (Iteration 1923) ===
Q mean: -41.614922
Q std: 19.693274
Actor loss: 41.618904
Action reg: 0.003983
  l1.weight: grad_norm = 0.000803
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.002646
Total gradient norm: 0.006899
=== Actor Training Debug (Iteration 1924) ===
Q mean: -39.627781
Q std: 18.315723
Actor loss: 39.631756
Action reg: 0.003973
  l1.weight: grad_norm = 0.000678
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.003103
Total gradient norm: 0.009701
=== Actor Training Debug (Iteration 1925) ===
Q mean: -43.681889
Q std: 18.091454
Actor loss: 43.685875
Action reg: 0.003986
  l1.weight: grad_norm = 0.020348
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.047831
Total gradient norm: 0.079873
=== Actor Training Debug (Iteration 1926) ===
Q mean: -43.188133
Q std: 17.121143
Actor loss: 43.192135
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 1927) ===
Q mean: -39.973549
Q std: 18.546640
Actor loss: 39.977520
Action reg: 0.003971
  l1.weight: grad_norm = 0.000720
  l1.bias: grad_norm = 0.000481
  l2.weight: grad_norm = 0.003801
Total gradient norm: 0.012608
=== Actor Training Debug (Iteration 1928) ===
Q mean: -42.030899
Q std: 17.805840
Actor loss: 42.034882
Action reg: 0.003982
  l1.weight: grad_norm = 0.002594
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.006561
Total gradient norm: 0.011977
=== Actor Training Debug (Iteration 1929) ===
Q mean: -45.506805
Q std: 19.573441
Actor loss: 45.510784
Action reg: 0.003977
  l1.weight: grad_norm = 0.000684
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.003308
Total gradient norm: 0.010632
=== Actor Training Debug (Iteration 1930) ===
Q mean: -44.974804
Q std: 18.872555
Actor loss: 44.978779
Action reg: 0.003976
  l1.weight: grad_norm = 0.001755
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.004474
Total gradient norm: 0.009578
=== Actor Training Debug (Iteration 1931) ===
Q mean: -38.916058
Q std: 18.849876
Actor loss: 38.920044
Action reg: 0.003988
  l1.weight: grad_norm = 0.000348
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.001692
Total gradient norm: 0.005373
=== Actor Training Debug (Iteration 1932) ===
Q mean: -43.244049
Q std: 18.434343
Actor loss: 43.248039
Action reg: 0.003992
  l1.weight: grad_norm = 0.000428
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.001307
Total gradient norm: 0.003317
=== Actor Training Debug (Iteration 1933) ===
Q mean: -44.058212
Q std: 18.737633
Actor loss: 44.062187
Action reg: 0.003976
  l1.weight: grad_norm = 0.000909
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.003377
Total gradient norm: 0.009863
=== Actor Training Debug (Iteration 1934) ===
Q mean: -45.108299
Q std: 17.730946
Actor loss: 45.112282
Action reg: 0.003981
  l1.weight: grad_norm = 0.000599
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.002732
Total gradient norm: 0.008811
=== Actor Training Debug (Iteration 1935) ===
Q mean: -43.621002
Q std: 18.885569
Actor loss: 43.624996
Action reg: 0.003994
  l1.weight: grad_norm = 0.000385
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.001099
Total gradient norm: 0.002207
=== Actor Training Debug (Iteration 1936) ===
Q mean: -42.512794
Q std: 19.353998
Actor loss: 42.516773
Action reg: 0.003978
  l1.weight: grad_norm = 0.000644
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.003035
Total gradient norm: 0.009820
=== Actor Training Debug (Iteration 1937) ===
Q mean: -45.034584
Q std: 19.351568
Actor loss: 45.038570
Action reg: 0.003987
  l1.weight: grad_norm = 0.001608
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.004174
Total gradient norm: 0.007497
=== Actor Training Debug (Iteration 1938) ===
Q mean: -42.590481
Q std: 17.522188
Actor loss: 42.594467
Action reg: 0.003987
  l1.weight: grad_norm = 0.027603
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.082679
Total gradient norm: 0.161174
=== Actor Training Debug (Iteration 1939) ===
Q mean: -42.209450
Q std: 18.407255
Actor loss: 42.213436
Action reg: 0.003987
  l1.weight: grad_norm = 0.000514
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.001583
Total gradient norm: 0.004140
=== Actor Training Debug (Iteration 1940) ===
Q mean: -44.008198
Q std: 18.868290
Actor loss: 44.012180
Action reg: 0.003982
  l1.weight: grad_norm = 0.000592
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.002487
Total gradient norm: 0.007871
=== Actor Training Debug (Iteration 1941) ===
Q mean: -43.882042
Q std: 18.183191
Actor loss: 43.886032
Action reg: 0.003989
  l1.weight: grad_norm = 0.000653
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.002038
Total gradient norm: 0.004366
=== Actor Training Debug (Iteration 1942) ===
Q mean: -39.116119
Q std: 17.033916
Actor loss: 39.120102
Action reg: 0.003982
  l1.weight: grad_norm = 0.000455
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.002016
Total gradient norm: 0.006523
=== Actor Training Debug (Iteration 1943) ===
Q mean: -43.775650
Q std: 17.817192
Actor loss: 43.779636
Action reg: 0.003988
  l1.weight: grad_norm = 0.000586
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.001916
Total gradient norm: 0.005063
Total gradient norm: 0.003847ration 1203) ===
=== Actor Training Debug (Iteration 1954) ===
Q mean: -44.434551
Q std: 17.054407
Actor loss: 44.438545
Action reg: 0.003994
  l1.weight: grad_norm = 0.040306
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.109271
Total gradient norm: 0.217314
=== Actor Training Debug (Iteration 1955) ===
Q mean: -45.461014
Q std: 18.528305
Actor loss: 45.464989
Action reg: 0.003974
  l1.weight: grad_norm = 0.001006
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.003921
Total gradient norm: 0.011541
=== Actor Training Debug (Iteration 1956) ===
Q mean: -40.786053
Q std: 18.723026
Actor loss: 40.790016
Action reg: 0.003962
  l1.weight: grad_norm = 0.000846
  l1.bias: grad_norm = 0.000673
  l2.weight: grad_norm = 0.005247
Total gradient norm: 0.018056
=== Actor Training Debug (Iteration 1957) ===
Q mean: -42.499771
Q std: 18.187328
Actor loss: 42.503754
Action reg: 0.003981
  l1.weight: grad_norm = 0.004260
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.010555
Total gradient norm: 0.018553
=== Actor Training Debug (Iteration 1958) ===
Q mean: -44.011421
Q std: 17.614664
Actor loss: 44.015415
Action reg: 0.003993
  l1.weight: grad_norm = 0.015822
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.045750
Total gradient norm: 0.086937
=== Actor Training Debug (Iteration 1959) ===
Q mean: -42.554405
Q std: 18.444635
Actor loss: 42.558392
Action reg: 0.003985
  l1.weight: grad_norm = 0.002816
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.008152
Total gradient norm: 0.018903
=== Actor Training Debug (Iteration 1960) ===
Q mean: -44.195511
Q std: 18.230536
Actor loss: 44.199493
Action reg: 0.003981
  l1.weight: grad_norm = 0.024722
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.064334
Total gradient norm: 0.118468
=== Actor Training Debug (Iteration 1961) ===
Q mean: -43.043755
Q std: 19.390173
Actor loss: 43.047737
Action reg: 0.003982
  l1.weight: grad_norm = 0.000633
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.002650
Total gradient norm: 0.007577
=== Actor Training Debug (Iteration 1962) ===
Q mean: -40.894184
Q std: 18.187037
Actor loss: 40.898170
Action reg: 0.003987
  l1.weight: grad_norm = 0.001559
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.004867
Total gradient norm: 0.010904
=== Actor Training Debug (Iteration 1963) ===
Q mean: -42.150726
Q std: 18.182745
Actor loss: 42.154720
Action reg: 0.003995
  l1.weight: grad_norm = 0.000264
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.001021
Total gradient norm: 0.002832
=== Actor Training Debug (Iteration 1964) ===
Q mean: -40.388100
Q std: 18.238688
Actor loss: 40.392082
Action reg: 0.003981
  l1.weight: grad_norm = 0.000550
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.002694
Total gradient norm: 0.008449
=== Actor Training Debug (Iteration 1965) ===
Q mean: -46.799583
Q std: 18.654222
Actor loss: 46.803574
Action reg: 0.003988
  l1.weight: grad_norm = 0.000431
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.001858
Total gradient norm: 0.005612
=== Actor Training Debug (Iteration 1966) ===
Q mean: -43.526157
Q std: 18.231821
Actor loss: 43.530132
Action reg: 0.003974
  l1.weight: grad_norm = 0.001017
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.003468
Total gradient norm: 0.010296
=== Actor Training Debug (Iteration 1967) ===
Q mean: -41.781178
Q std: 16.689310
Actor loss: 41.785156
Action reg: 0.003980
  l1.weight: grad_norm = 0.015447
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.040245
Total gradient norm: 0.078922
=== Actor Training Debug (Iteration 1968) ===
Q mean: -41.231800
Q std: 16.544432
Actor loss: 41.235783
Action reg: 0.003981
  l1.weight: grad_norm = 0.054629
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.142242
Total gradient norm: 0.250724
=== Actor Training Debug (Iteration 1969) ===
Q mean: -44.453796
Q std: 19.100067
Actor loss: 44.457771
Action reg: 0.003973
  l1.weight: grad_norm = 0.003896
  l1.bias: grad_norm = 0.000432
  l2.weight: grad_norm = 0.011210
Total gradient norm: 0.027155
=== Actor Training Debug (Iteration 1970) ===
Q mean: -43.539906
Q std: 18.707682
Actor loss: 43.543888
Action reg: 0.003983
  l1.weight: grad_norm = 0.004071
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.010926
Total gradient norm: 0.023992
=== Actor Training Debug (Iteration 1971) ===
Q mean: -43.222832
Q std: 17.160540
Actor loss: 43.226818
Action reg: 0.003987
  l1.weight: grad_norm = 0.030875
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.075968
Total gradient norm: 0.148292
=== Actor Training Debug (Iteration 1972) ===
Q mean: -40.512943
Q std: 17.178633
Actor loss: 40.516922
Action reg: 0.003978
  l1.weight: grad_norm = 0.012341
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.035234
Total gradient norm: 0.058995
=== Actor Training Debug (Iteration 1973) ===
Q mean: -42.941536
Q std: 17.448034
Actor loss: 42.945515
Action reg: 0.003978
  l1.weight: grad_norm = 0.033557
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.079876
Total gradient norm: 0.120868
=== Actor Training Debug (Iteration 1974) ===
Q mean: -43.610374
Q std: 18.257133
Actor loss: 43.614342
Action reg: 0.003966
  l1.weight: grad_norm = 0.024449
  l1.bias: grad_norm = 0.000598
  l2.weight: grad_norm = 0.070058
Total gradient norm: 0.117842
=== Actor Training Debug (Iteration 1975) ===
Q mean: -43.732128
Q std: 18.477201
Actor loss: 43.736099
Action reg: 0.003972
  l1.weight: grad_norm = 0.000750
  l1.bias: grad_norm = 0.000541
  l2.weight: grad_norm = 0.003546
Total gradient norm: 0.010813
=== Actor Training Debug (Iteration 1976) ===
Q mean: -42.547638
Q std: 18.451551
Actor loss: 42.551601
Action reg: 0.003963
  l1.weight: grad_norm = 0.093746
  l1.bias: grad_norm = 0.000691
  l2.weight: grad_norm = 0.232612
Total gradient norm: 0.358725
=== Actor Training Debug (Iteration 1977) ===
Q mean: -40.482143
Q std: 19.487446
Actor loss: 40.486111
Action reg: 0.003966
  l1.weight: grad_norm = 0.007593
  l1.bias: grad_norm = 0.000690
  l2.weight: grad_norm = 0.022178
Total gradient norm: 0.045680
=== Actor Training Debug (Iteration 1978) ===
Q mean: -44.281921
Q std: 17.636993
Actor loss: 44.285908
Action reg: 0.003985
  l1.weight: grad_norm = 0.056816
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.147476
Total gradient norm: 0.277259
=== Actor Training Debug (Iteration 1979) ===
Q mean: -44.666962
Q std: 18.552584
Actor loss: 44.670948
Action reg: 0.003986
  l1.weight: grad_norm = 0.081734
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.251572
Total gradient norm: 0.427477
=== Actor Training Debug (Iteration 1980) ===
Q mean: -46.368168
Q std: 17.773499
Actor loss: 46.372166
Action reg: 0.003999
  l1.weight: grad_norm = 0.030443
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.080322
Total gradient norm: 0.157473
=== Actor Training Debug (Iteration 1981) ===
Q mean: -42.350571
Q std: 17.150240
Actor loss: 42.354561
Action reg: 0.003992
  l1.weight: grad_norm = 0.006082
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.018952
Total gradient norm: 0.034806
=== Actor Training Debug (Iteration 1982) ===
Q mean: -43.823818
Q std: 18.480429
Actor loss: 43.827816
Action reg: 0.003999
  l1.weight: grad_norm = 0.009283
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.024357
Total gradient norm: 0.047768
=== Actor Training Debug (Iteration 1983) ===
Q mean: -41.766060
Q std: 20.155569
Actor loss: 41.770016
Action reg: 0.003958
  l1.weight: grad_norm = 0.000976
  l1.bias: grad_norm = 0.000748
  l2.weight: grad_norm = 0.005302
Total gradient norm: 0.016926
=== Actor Training Debug (Iteration 1984) ===
Q mean: -42.446362
Q std: 18.770988
Actor loss: 42.450348
Action reg: 0.003986
  l1.weight: grad_norm = 0.000806
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.003098
Total gradient norm: 0.008986
=== Actor Training Debug (Iteration 1985) ===
Q mean: -42.574951
Q std: 17.220652
Actor loss: 42.578953
Action reg: 0.004000
  l1.weight: grad_norm = 0.000284
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000745
Total gradient norm: 0.001505
=== Actor Training Debug (Iteration 1986) ===
Q mean: -44.502403
Q std: 18.975302
Actor loss: 44.506382
Action reg: 0.003979
  l1.weight: grad_norm = 0.002437
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.007370
Total gradient norm: 0.017974
=== Actor Training Debug (Iteration 1987) ===
Q mean: -46.392796
Q std: 17.505362
Actor loss: 46.396797
Action reg: 0.004000
  l1.weight: grad_norm = 0.000322
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000970
Total gradient norm: 0.001693
=== Actor Training Debug (Iteration 1988) ===
Q mean: -44.677303
Q std: 16.855907
Actor loss: 44.681297
Action reg: 0.003994
  l1.weight: grad_norm = 0.000458
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.001574
Total gradient norm: 0.004268
Total gradient norm: 0.011814ration 1203) ===
=== Actor Training Debug (Iteration 1999) ===
Q mean: -45.080353
Q std: 17.978317
Actor loss: 45.084320
Action reg: 0.003968
  l1.weight: grad_norm = 0.002469
  l1.bias: grad_norm = 0.000544
  l2.weight: grad_norm = 0.006364
Total gradient norm: 0.012791
=== Actor Training Debug (Iteration 2000) ===
Q mean: -44.711502
Q std: 16.678972
Actor loss: 44.715481
Action reg: 0.003980
  l1.weight: grad_norm = 0.000632
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.002838
Total gradient norm: 0.008920
Step 7000: Critic Loss: 8.2535, Actor Loss: 44.7155, Q Value: -44.7115
  Average reward: -365.966 | Average length: 100.0
Evaluation at episode 70: -365.966
=== Actor Training Debug (Iteration 2001) ===
Q mean: -43.033730
Q std: 18.136124
Actor loss: 43.037701
Action reg: 0.003970
  l1.weight: grad_norm = 0.006333
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.016907
Total gradient norm: 0.033712
=== Actor Training Debug (Iteration 2002) ===
Q mean: -44.254753
Q std: 19.465864
Actor loss: 44.258728
Action reg: 0.003974
  l1.weight: grad_norm = 0.001261
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.004660
Total gradient norm: 0.013538
=== Actor Training Debug (Iteration 2003) ===
Q mean: -43.658939
Q std: 19.253727
Actor loss: 43.662910
Action reg: 0.003972
  l1.weight: grad_norm = 0.005287
  l1.bias: grad_norm = 0.000510
  l2.weight: grad_norm = 0.019269
Total gradient norm: 0.037922
=== Actor Training Debug (Iteration 2004) ===
Q mean: -46.239178
Q std: 18.538046
Actor loss: 46.243168
Action reg: 0.003989
  l1.weight: grad_norm = 0.048938
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.102885
Total gradient norm: 0.152606
=== Actor Training Debug (Iteration 2005) ===
Q mean: -44.564423
Q std: 19.270321
Actor loss: 44.568394
Action reg: 0.003970
  l1.weight: grad_norm = 0.006854
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.022191
Total gradient norm: 0.039145
=== Actor Training Debug (Iteration 2006) ===
Q mean: -43.025517
Q std: 17.258738
Actor loss: 43.029480
Action reg: 0.003963
  l1.weight: grad_norm = 0.013306
  l1.bias: grad_norm = 0.000656
  l2.weight: grad_norm = 0.037871
Total gradient norm: 0.082949
=== Actor Training Debug (Iteration 2007) ===
Q mean: -42.789768
Q std: 19.533604
Actor loss: 42.793739
Action reg: 0.003973
  l1.weight: grad_norm = 0.003799
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.009423
Total gradient norm: 0.018134
=== Actor Training Debug (Iteration 2008) ===
Q mean: -42.965126
Q std: 16.743206
Actor loss: 42.969120
Action reg: 0.003993
  l1.weight: grad_norm = 0.001265
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.003122
Total gradient norm: 0.005706
=== Actor Training Debug (Iteration 2009) ===
Q mean: -46.693298
Q std: 16.554222
Actor loss: 46.697277
Action reg: 0.003979
  l1.weight: grad_norm = 0.001248
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.004130
Total gradient norm: 0.011250
=== Actor Training Debug (Iteration 2010) ===
Q mean: -45.382595
Q std: 18.141176
Actor loss: 45.386578
Action reg: 0.003983
  l1.weight: grad_norm = 0.033761
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.105566
Total gradient norm: 0.216416
=== Actor Training Debug (Iteration 2011) ===
Q mean: -43.026482
Q std: 19.045612
Actor loss: 43.030468
Action reg: 0.003986
  l1.weight: grad_norm = 0.041319
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.132662
Total gradient norm: 0.311348
=== Actor Training Debug (Iteration 2012) ===
Q mean: -42.960850
Q std: 18.259333
Actor loss: 42.964828
Action reg: 0.003979
  l1.weight: grad_norm = 0.075761
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.194738
Total gradient norm: 0.333977
=== Actor Training Debug (Iteration 2013) ===
Q mean: -45.369095
Q std: 19.360479
Actor loss: 45.373058
Action reg: 0.003963
  l1.weight: grad_norm = 0.108392
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.285275
Total gradient norm: 0.566493
=== Actor Training Debug (Iteration 2014) ===
Q mean: -43.016930
Q std: 18.708929
Actor loss: 43.020912
Action reg: 0.003984
  l1.weight: grad_norm = 0.093632
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.336281
Total gradient norm: 0.797066
=== Actor Training Debug (Iteration 2015) ===
Q mean: -42.630348
Q std: 17.885281
Actor loss: 42.634308
Action reg: 0.003961
  l1.weight: grad_norm = 0.072068
  l1.bias: grad_norm = 0.000513
  l2.weight: grad_norm = 0.274423
Total gradient norm: 0.716250
=== Actor Training Debug (Iteration 2016) ===
Q mean: -45.021126
Q std: 19.510202
Actor loss: 45.025082
Action reg: 0.003954
  l1.weight: grad_norm = 0.081199
  l1.bias: grad_norm = 0.000669
  l2.weight: grad_norm = 0.333779
Total gradient norm: 0.830710
=== Actor Training Debug (Iteration 2017) ===
Q mean: -42.023594
Q std: 17.046110
Actor loss: 42.027565
Action reg: 0.003971
  l1.weight: grad_norm = 0.136714
  l1.bias: grad_norm = 0.000619
  l2.weight: grad_norm = 0.564297
Total gradient norm: 1.276297
=== Actor Training Debug (Iteration 2018) ===
Q mean: -45.949654
Q std: 18.166542
Actor loss: 45.953640
Action reg: 0.003985
  l1.weight: grad_norm = 0.164402
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.632654
Total gradient norm: 1.427222
=== Actor Training Debug (Iteration 2019) ===
Q mean: -43.854347
Q std: 19.084084
Actor loss: 43.858292
Action reg: 0.003943
  l1.weight: grad_norm = 0.595444
  l1.bias: grad_norm = 0.000713
  l2.weight: grad_norm = 2.058475
Total gradient norm: 4.682083
=== Actor Training Debug (Iteration 2020) ===
Q mean: -42.264278
Q std: 17.735275
Actor loss: 42.268250
Action reg: 0.003973
  l1.weight: grad_norm = 0.119817
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.429114
Total gradient norm: 0.919288
=== Actor Training Debug (Iteration 2021) ===
Q mean: -46.307823
Q std: 19.847027
Actor loss: 46.311794
Action reg: 0.003969
  l1.weight: grad_norm = 0.000702
  l1.bias: grad_norm = 0.000533
  l2.weight: grad_norm = 0.003735
Total gradient norm: 0.012045
=== Actor Training Debug (Iteration 2022) ===
Q mean: -46.930466
Q std: 18.945366
Actor loss: 46.934460
Action reg: 0.003993
  l1.weight: grad_norm = 0.000480
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.001753
Total gradient norm: 0.004815
=== Actor Training Debug (Iteration 2023) ===
Q mean: -45.448059
Q std: 19.969227
Actor loss: 45.452034
Action reg: 0.003974
  l1.weight: grad_norm = 0.001323
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.004675
Total gradient norm: 0.013743
=== Actor Training Debug (Iteration 2024) ===
Q mean: -40.696518
Q std: 17.664614
Actor loss: 40.700497
Action reg: 0.003980
  l1.weight: grad_norm = 0.000605
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.003077
Total gradient norm: 0.010117
=== Actor Training Debug (Iteration 2025) ===
Q mean: -43.183388
Q std: 19.317102
Actor loss: 43.187359
Action reg: 0.003971
  l1.weight: grad_norm = 0.000637
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.003311
Total gradient norm: 0.010836
=== Actor Training Debug (Iteration 2026) ===
Q mean: -46.637928
Q std: 18.624826
Actor loss: 46.641907
Action reg: 0.003980
  l1.weight: grad_norm = 0.000631
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.003605
Total gradient norm: 0.011839
=== Actor Training Debug (Iteration 2027) ===
Q mean: -46.452446
Q std: 18.761431
Actor loss: 46.456432
Action reg: 0.003986
  l1.weight: grad_norm = 0.001284
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.003563
Total gradient norm: 0.008436
=== Actor Training Debug (Iteration 2028) ===
Q mean: -43.269508
Q std: 17.942236
Actor loss: 43.273499
Action reg: 0.003991
  l1.weight: grad_norm = 0.020716
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.065026
Total gradient norm: 0.117933
=== Actor Training Debug (Iteration 2029) ===
Q mean: -42.218681
Q std: 18.529573
Actor loss: 42.222656
Action reg: 0.003974
  l1.weight: grad_norm = 0.017507
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.039669
Total gradient norm: 0.071808
=== Actor Training Debug (Iteration 2030) ===
Q mean: -44.713711
Q std: 19.503042
Actor loss: 44.717686
Action reg: 0.003976
  l1.weight: grad_norm = 0.000761
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.003779
Total gradient norm: 0.011824
=== Actor Training Debug (Iteration 2031) ===
Q mean: -46.347923
Q std: 18.994984
Actor loss: 46.351925
Action reg: 0.004000
  l1.weight: grad_norm = 0.000047
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000120
Total gradient norm: 0.000239
=== Actor Training Debug (Iteration 2032) ===
Q mean: -42.041016
Q std: 18.280634
Actor loss: 42.045002
Action reg: 0.003987
  l1.weight: grad_norm = 0.003080
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.008454
Total gradient norm: 0.016807
=== Actor Training Debug (Iteration 2033) ===
Q mean: -41.234646
Q std: 19.132347
Actor loss: 41.238625
Action reg: 0.003978
  l1.weight: grad_norm = 0.000854
  l1.bias: grad_norm = 0.000481
  l2.weight: grad_norm = 0.004587
Total gradient norm: 0.014505
=== Actor Training Debug (Iteration 2034) ===
Q mean: -41.641434
Q std: 18.424965
Actor loss: 41.645386
Action reg: 0.003954
  l1.weight: grad_norm = 0.001029
  l1.bias: grad_norm = 0.000827
  l2.weight: grad_norm = 0.007076
Total gradient norm: 0.025276
=== Actor Training Debug (Iteration 2035) ===
Q mean: -47.351768
Q std: 18.510008
Actor loss: 47.355759
Action reg: 0.003992
  l1.weight: grad_norm = 0.003294
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.008641
Total gradient norm: 0.017762
=== Actor Training Debug (Iteration 2036) ===
Q mean: -46.334743
Q std: 18.876146
Actor loss: 46.338718
Action reg: 0.003975
  l1.weight: grad_norm = 0.000994
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.003170
Total gradient norm: 0.008730
=== Actor Training Debug (Iteration 2037) ===
Q mean: -40.424873
Q std: 17.294262
Actor loss: 40.428860
Action reg: 0.003987
  l1.weight: grad_norm = 0.003421
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.009167
Total gradient norm: 0.018360
=== Actor Training Debug (Iteration 2038) ===
Q mean: -42.227528
Q std: 18.085590
Actor loss: 42.231510
Action reg: 0.003984
  l1.weight: grad_norm = 0.005218
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.014100
Total gradient norm: 0.028049
=== Actor Training Debug (Iteration 2039) ===
Q mean: -43.362926
Q std: 18.342516
Actor loss: 43.366909
Action reg: 0.003981
  l1.weight: grad_norm = 0.001370
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.003783
Total gradient norm: 0.008795
=== Actor Training Debug (Iteration 2040) ===
Q mean: -47.040871
Q std: 20.128294
Actor loss: 47.044830
Action reg: 0.003959
  l1.weight: grad_norm = 0.022892
  l1.bias: grad_norm = 0.000517
  l2.weight: grad_norm = 0.060177
Total gradient norm: 0.111405
=== Actor Training Debug (Iteration 2041) ===
Q mean: -44.569191
Q std: 19.445354
Actor loss: 44.573158
Action reg: 0.003968
  l1.weight: grad_norm = 0.004483
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.012027
Total gradient norm: 0.025201
=== Actor Training Debug (Iteration 2042) ===
Q mean: -44.244339
Q std: 16.591766
Actor loss: 44.248341
Action reg: 0.004000
  l1.weight: grad_norm = 0.002099
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.005709
Total gradient norm: 0.011577
=== Actor Training Debug (Iteration 2043) ===
Q mean: -41.776749
Q std: 18.354023
Actor loss: 41.780704
Action reg: 0.003958
  l1.weight: grad_norm = 0.002709
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.010092
Total gradient norm: 0.027827
=== Actor Training Debug (Iteration 2044) ===
Q mean: -46.091026
Q std: 18.303282
Actor loss: 46.095016
Action reg: 0.003990
  l1.weight: grad_norm = 0.000731
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.002559
Total gradient norm: 0.007218
=== Actor Training Debug (Iteration 2045) ===
Q mean: -44.957634
Q std: 18.985054
Actor loss: 44.961617
Action reg: 0.003981
  l1.weight: grad_norm = 0.001037
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.003567
Total gradient norm: 0.009714
=== Actor Training Debug (Iteration 2046) ===
Q mean: -42.164482
Q std: 17.954239
Actor loss: 42.168468
Action reg: 0.003986
  l1.weight: grad_norm = 0.001236
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.003091
Total gradient norm: 0.006492
=== Actor Training Debug (Iteration 2047) ===
Q mean: -43.050522
Q std: 18.901804
Actor loss: 43.054501
Action reg: 0.003978
  l1.weight: grad_norm = 0.015086
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.039186
Total gradient norm: 0.079318
=== Actor Training Debug (Iteration 2048) ===
Q mean: -44.518333
Q std: 18.380543
Actor loss: 44.522312
Action reg: 0.003980
  l1.weight: grad_norm = 0.001198
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.003534
Total gradient norm: 0.008822
=== Actor Training Debug (Iteration 2049) ===
Q mean: -46.509811
Q std: 18.076023
Actor loss: 46.513798
Action reg: 0.003985
  l1.weight: grad_norm = 0.016057
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.042983
Total gradient norm: 0.087146
=== Actor Training Debug (Iteration 2050) ===
Q mean: -42.524712
Q std: 18.903383
Actor loss: 42.528706
Action reg: 0.003992
  l1.weight: grad_norm = 0.009572
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.023164
Total gradient norm: 0.032196
=== Actor Training Debug (Iteration 2051) ===
Q mean: -43.584278
Q std: 17.990379
Actor loss: 43.588268
Action reg: 0.003991
  l1.weight: grad_norm = 0.002490
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.006474
Total gradient norm: 0.013331
=== Actor Training Debug (Iteration 2052) ===
Q mean: -41.523899
Q std: 19.311764
Actor loss: 41.527889
Action reg: 0.003990
  l1.weight: grad_norm = 0.192579
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.551684
Total gradient norm: 1.154260
=== Actor Training Debug (Iteration 2053) ===
Q mean: -45.172798
Q std: 20.796808
Actor loss: 45.176765
Action reg: 0.003969
  l1.weight: grad_norm = 0.000667
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.003959
Total gradient norm: 0.013690
Total gradient norm: 0.748236ration 1203) ===
=== Actor Training Debug (Iteration 2064) ===
Q mean: -40.446144
Q std: 17.003300
Actor loss: 40.450127
Action reg: 0.003984
  l1.weight: grad_norm = 0.146371
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.477913
Total gradient norm: 0.880126
=== Actor Training Debug (Iteration 2065) ===
Q mean: -40.820076
Q std: 18.198883
Actor loss: 40.824047
Action reg: 0.003971
  l1.weight: grad_norm = 0.028544
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.069615
Total gradient norm: 0.117987
=== Actor Training Debug (Iteration 2066) ===
Q mean: -41.402222
Q std: 17.648424
Actor loss: 41.406216
Action reg: 0.003993
  l1.weight: grad_norm = 0.058246
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.157033
Total gradient norm: 0.275540
=== Actor Training Debug (Iteration 2067) ===
Q mean: -46.312141
Q std: 17.908602
Actor loss: 46.316132
Action reg: 0.003989
  l1.weight: grad_norm = 0.029591
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.082850
Total gradient norm: 0.151197
=== Actor Training Debug (Iteration 2068) ===
Q mean: -42.981091
Q std: 19.126955
Actor loss: 42.985062
Action reg: 0.003973
  l1.weight: grad_norm = 0.012227
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.027624
Total gradient norm: 0.039387
=== Actor Training Debug (Iteration 2069) ===
Q mean: -43.645931
Q std: 18.772293
Actor loss: 43.649918
Action reg: 0.003985
  l1.weight: grad_norm = 0.004709
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.011739
Total gradient norm: 0.020972
=== Actor Training Debug (Iteration 2070) ===
Q mean: -46.282616
Q std: 18.243517
Actor loss: 46.286594
Action reg: 0.003978
  l1.weight: grad_norm = 0.017258
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.044080
Total gradient norm: 0.083166
=== Actor Training Debug (Iteration 2071) ===
Q mean: -44.988979
Q std: 18.631018
Actor loss: 44.992970
Action reg: 0.003990
  l1.weight: grad_norm = 0.054473
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.132091
Total gradient norm: 0.264224
=== Actor Training Debug (Iteration 2072) ===
Q mean: -44.505615
Q std: 18.842333
Actor loss: 44.509602
Action reg: 0.003986
  l1.weight: grad_norm = 0.000594
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.002123
Total gradient norm: 0.006061
=== Actor Training Debug (Iteration 2073) ===
Q mean: -42.822548
Q std: 16.768200
Actor loss: 42.826523
Action reg: 0.003975
  l1.weight: grad_norm = 0.000448
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.001747
Total gradient norm: 0.005225
=== Actor Training Debug (Iteration 2074) ===
Q mean: -44.184250
Q std: 16.374241
Actor loss: 44.188244
Action reg: 0.003996
  l1.weight: grad_norm = 0.000336
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.000988
Total gradient norm: 0.002398
=== Actor Training Debug (Iteration 2075) ===
Q mean: -48.342117
Q std: 18.146692
Actor loss: 48.346111
Action reg: 0.003994
  l1.weight: grad_norm = 0.000209
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.000541
Total gradient norm: 0.001409
=== Actor Training Debug (Iteration 2076) ===
Q mean: -44.970825
Q std: 18.547321
Actor loss: 44.974819
Action reg: 0.003993
  l1.weight: grad_norm = 0.007275
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.016626
Total gradient norm: 0.032972
=== Actor Training Debug (Iteration 2077) ===
Q mean: -41.211929
Q std: 18.351660
Actor loss: 41.215904
Action reg: 0.003974
  l1.weight: grad_norm = 0.003467
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.008417
Total gradient norm: 0.018052
=== Actor Training Debug (Iteration 2078) ===
Q mean: -43.144615
Q std: 17.894840
Actor loss: 43.148586
Action reg: 0.003971
  l1.weight: grad_norm = 0.000528
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.002530
Total gradient norm: 0.008176
=== Actor Training Debug (Iteration 2079) ===
Q mean: -45.438885
Q std: 18.615370
Actor loss: 45.442871
Action reg: 0.003985
  l1.weight: grad_norm = 0.000285
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.001092
Total gradient norm: 0.003426
=== Actor Training Debug (Iteration 2080) ===
Q mean: -45.483894
Q std: 19.330173
Actor loss: 45.487873
Action reg: 0.003979
  l1.weight: grad_norm = 0.000340
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.001339
Total gradient norm: 0.004258
=== Actor Training Debug (Iteration 2081) ===
Q mean: -43.907360
Q std: 18.383152
Actor loss: 43.911331
Action reg: 0.003971
  l1.weight: grad_norm = 0.000522
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.002029
Total gradient norm: 0.006317
=== Actor Training Debug (Iteration 2082) ===
Q mean: -42.932861
Q std: 17.627659
Actor loss: 42.936844
Action reg: 0.003984
  l1.weight: grad_norm = 0.001121
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.003615
Total gradient norm: 0.007465
=== Actor Training Debug (Iteration 2083) ===
Q mean: -43.751129
Q std: 18.106464
Actor loss: 43.755131
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2084) ===
Q mean: -43.515244
Q std: 18.394217
Actor loss: 43.519207
Action reg: 0.003964
  l1.weight: grad_norm = 0.000527
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.002517
Total gradient norm: 0.007992
=== Actor Training Debug (Iteration 2085) ===
Q mean: -45.480129
Q std: 20.373806
Actor loss: 45.484097
Action reg: 0.003969
  l1.weight: grad_norm = 0.000552
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.002603
Total gradient norm: 0.008294
=== Actor Training Debug (Iteration 2086) ===
Q mean: -44.117893
Q std: 18.342432
Actor loss: 44.121876
Action reg: 0.003983
  l1.weight: grad_norm = 0.000437
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.001267
Total gradient norm: 0.003800
=== Actor Training Debug (Iteration 2087) ===
Q mean: -42.590942
Q std: 18.132463
Actor loss: 42.594921
Action reg: 0.003980
  l1.weight: grad_norm = 0.002053
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.006365
Total gradient norm: 0.013090
=== Actor Training Debug (Iteration 2088) ===
Q mean: -41.530647
Q std: 18.277473
Actor loss: 41.534626
Action reg: 0.003980
  l1.weight: grad_norm = 0.000889
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.002586
Total gradient norm: 0.006694
=== Actor Training Debug (Iteration 2089) ===
Q mean: -44.458092
Q std: 16.898417
Actor loss: 44.462078
Action reg: 0.003988
  l1.weight: grad_norm = 0.000262
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.000830
Total gradient norm: 0.002435
=== Actor Training Debug (Iteration 2090) ===
Q mean: -45.453438
Q std: 19.230436
Actor loss: 45.457413
Action reg: 0.003975
  l1.weight: grad_norm = 0.000479
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.001859
Total gradient norm: 0.005748
=== Actor Training Debug (Iteration 2091) ===
Q mean: -49.265686
Q std: 18.049040
Actor loss: 49.269684
Action reg: 0.003997
  l1.weight: grad_norm = 0.000149
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.000378
Total gradient norm: 0.000917
=== Actor Training Debug (Iteration 2092) ===
Q mean: -44.292461
Q std: 19.144815
Actor loss: 44.296444
Action reg: 0.003982
  l1.weight: grad_norm = 0.000345
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.001172
Total gradient norm: 0.003485
=== Actor Training Debug (Iteration 2093) ===
Q mean: -41.767521
Q std: 18.077696
Actor loss: 41.771496
Action reg: 0.003976
  l1.weight: grad_norm = 0.000525
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.001905
Total gradient norm: 0.005939
=== Actor Training Debug (Iteration 2094) ===
Q mean: -40.921761
Q std: 17.445433
Actor loss: 40.925743
Action reg: 0.003983
  l1.weight: grad_norm = 0.009493
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.021551
Total gradient norm: 0.043302
=== Actor Training Debug (Iteration 2095) ===
Q mean: -41.913292
Q std: 16.325089
Actor loss: 41.917286
Action reg: 0.003993
  l1.weight: grad_norm = 0.000260
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.000876
Total gradient norm: 0.002558
=== Actor Training Debug (Iteration 2096) ===
Q mean: -47.811989
Q std: 18.783768
Actor loss: 47.815968
Action reg: 0.003978
  l1.weight: grad_norm = 0.039605
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.088974
Total gradient norm: 0.175676
=== Actor Training Debug (Iteration 2097) ===
Q mean: -48.397636
Q std: 19.274044
Actor loss: 48.401627
Action reg: 0.003990
  l1.weight: grad_norm = 0.001955
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.004637
Total gradient norm: 0.009694
=== Actor Training Debug (Iteration 2098) ===
Q mean: -44.813812
Q std: 18.703379
Actor loss: 44.817799
Action reg: 0.003985
  l1.weight: grad_norm = 0.000382
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.001303
Total gradient norm: 0.003924
=== Actor Training Debug (Iteration 2099) ===
Q mean: -38.880547
Q std: 17.473173
Actor loss: 38.884521
Action reg: 0.003976
  l1.weight: grad_norm = 0.001129
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.003091
Total gradient norm: 0.007637
=== Actor Training Debug (Iteration 2100) ===
Q mean: -42.492371
Q std: 18.689980
Actor loss: 42.496365
Action reg: 0.003995
  l1.weight: grad_norm = 0.000154
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.000404
Total gradient norm: 0.001028
Episode 71: Steps=100, Reward=-310.079, Buffer_size=7100
=== Actor Training Debug (Iteration 2101) ===
Q mean: -46.883930
Q std: 20.365427
Actor loss: 46.887917
Action reg: 0.003985
  l1.weight: grad_norm = 0.003117
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.007578
Total gradient norm: 0.015477
=== Actor Training Debug (Iteration 2102) ===
Q mean: -45.663818
Q std: 19.357985
Actor loss: 45.667809
Action reg: 0.003989
  l1.weight: grad_norm = 0.000247
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.000892
Total gradient norm: 0.002444
=== Actor Training Debug (Iteration 2103) ===
Q mean: -43.708351
Q std: 18.710079
Actor loss: 43.712326
Action reg: 0.003977
  l1.weight: grad_norm = 0.000417
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.001589
Total gradient norm: 0.004889
=== Actor Training Debug (Iteration 2104) ===
Q mean: -42.478622
Q std: 17.129953
Actor loss: 42.482601
Action reg: 0.003979
  l1.weight: grad_norm = 0.001412
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.004028
Total gradient norm: 0.009389
=== Actor Training Debug (Iteration 2105) ===
Q mean: -44.418869
Q std: 18.051580
Actor loss: 44.422871
Action reg: 0.004000
  l1.weight: grad_norm = 0.001822
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.004494
Total gradient norm: 0.008976
=== Actor Training Debug (Iteration 2106) ===
Q mean: -44.346260
Q std: 17.922989
Actor loss: 44.350235
Action reg: 0.003973
  l1.weight: grad_norm = 0.100116
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.246703
Total gradient norm: 0.492192
=== Actor Training Debug (Iteration 2107) ===
Q mean: -43.254219
Q std: 18.225698
Actor loss: 43.258202
Action reg: 0.003984
  l1.weight: grad_norm = 0.000414
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.001172
Total gradient norm: 0.003344
=== Actor Training Debug (Iteration 2108) ===
Q mean: -44.605934
Q std: 18.375528
Actor loss: 44.609936
Action reg: 0.004000
  l1.weight: grad_norm = 0.000068
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000155
Total gradient norm: 0.000304
=== Actor Training Debug (Iteration 2109) ===
Q mean: -46.966171
Q std: 19.140631
Actor loss: 46.970161
Action reg: 0.003989
  l1.weight: grad_norm = 0.010638
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.024766
Total gradient norm: 0.050217
=== Actor Training Debug (Iteration 2110) ===
Q mean: -47.562439
Q std: 19.712065
Actor loss: 47.566425
Action reg: 0.003985
  l1.weight: grad_norm = 0.000610
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.001698
Total gradient norm: 0.003463
=== Actor Training Debug (Iteration 2111) ===
Q mean: -45.459770
Q std: 17.530714
Actor loss: 45.463757
Action reg: 0.003987
  l1.weight: grad_norm = 0.000414
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.001209
Total gradient norm: 0.003251
=== Actor Training Debug (Iteration 2112) ===
Q mean: -44.431618
Q std: 17.792358
Actor loss: 44.435612
Action reg: 0.003992
  l1.weight: grad_norm = 0.000278
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.000820
Total gradient norm: 0.002286
=== Actor Training Debug (Iteration 2113) ===
Q mean: -46.253540
Q std: 19.219980
Actor loss: 46.257523
Action reg: 0.003984
  l1.weight: grad_norm = 0.000416
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.001313
Total gradient norm: 0.003696
=== Actor Training Debug (Iteration 2114) ===
Q mean: -42.776012
Q std: 17.877073
Actor loss: 42.779991
Action reg: 0.003980
  l1.weight: grad_norm = 0.000430
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.001578
Total gradient norm: 0.004838
=== Actor Training Debug (Iteration 2115) ===
Q mean: -42.755569
Q std: 18.343292
Actor loss: 42.759544
Action reg: 0.003974
  l1.weight: grad_norm = 0.000437
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.001837
Total gradient norm: 0.005848
Total gradient norm: 0.007884ration 1203) ===
=== Actor Training Debug (Iteration 2126) ===
Q mean: -45.898861
Q std: 17.066021
Actor loss: 45.902851
Action reg: 0.003992
  l1.weight: grad_norm = 0.016145
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.049770
Total gradient norm: 0.086671
=== Actor Training Debug (Iteration 2127) ===
Q mean: -46.041584
Q std: 20.483849
Actor loss: 46.045551
Action reg: 0.003966
  l1.weight: grad_norm = 0.008748
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.023225
Total gradient norm: 0.043542
=== Actor Training Debug (Iteration 2128) ===
Q mean: -46.010151
Q std: 18.773661
Actor loss: 46.014145
Action reg: 0.003994
  l1.weight: grad_norm = 0.000127
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.000379
Total gradient norm: 0.001027
=== Actor Training Debug (Iteration 2129) ===
Q mean: -44.838928
Q std: 20.684841
Actor loss: 44.842926
Action reg: 0.003997
  l1.weight: grad_norm = 0.000307
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.000825
Total gradient norm: 0.001715
=== Actor Training Debug (Iteration 2130) ===
Q mean: -45.021828
Q std: 19.606619
Actor loss: 45.025795
Action reg: 0.003969
  l1.weight: grad_norm = 0.001273
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.003608
Total gradient norm: 0.009496
=== Actor Training Debug (Iteration 2131) ===
Q mean: -42.153366
Q std: 18.885729
Actor loss: 42.157356
Action reg: 0.003992
  l1.weight: grad_norm = 0.005302
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.011593
Total gradient norm: 0.023180
=== Actor Training Debug (Iteration 2132) ===
Q mean: -44.347740
Q std: 17.848930
Actor loss: 44.351727
Action reg: 0.003985
  l1.weight: grad_norm = 0.000401
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.001374
Total gradient norm: 0.003797
=== Actor Training Debug (Iteration 2133) ===
Q mean: -46.097008
Q std: 16.074116
Actor loss: 46.100998
Action reg: 0.003990
  l1.weight: grad_norm = 0.000299
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.001011
Total gradient norm: 0.002808
=== Actor Training Debug (Iteration 2134) ===
Q mean: -47.053543
Q std: 17.167107
Actor loss: 47.057537
Action reg: 0.003995
  l1.weight: grad_norm = 0.000126
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.000372
Total gradient norm: 0.000990
=== Actor Training Debug (Iteration 2135) ===
Q mean: -46.396484
Q std: 16.913507
Actor loss: 46.400475
Action reg: 0.003989
  l1.weight: grad_norm = 0.063288
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.165553
Total gradient norm: 0.311732
=== Actor Training Debug (Iteration 2136) ===
Q mean: -43.362823
Q std: 18.377783
Actor loss: 43.366802
Action reg: 0.003979
  l1.weight: grad_norm = 0.000326
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.001528
Total gradient norm: 0.004937
=== Actor Training Debug (Iteration 2137) ===
Q mean: -43.355762
Q std: 18.124407
Actor loss: 43.359756
Action reg: 0.003993
  l1.weight: grad_norm = 0.000202
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.000457
Total gradient norm: 0.001050
=== Actor Training Debug (Iteration 2138) ===
Q mean: -45.966076
Q std: 20.077755
Actor loss: 45.970078
Action reg: 0.004000
  l1.weight: grad_norm = 0.000001
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000003
Total gradient norm: 0.000005
=== Actor Training Debug (Iteration 2139) ===
Q mean: -42.873177
Q std: 18.852671
Actor loss: 42.877167
Action reg: 0.003991
  l1.weight: grad_norm = 0.000203
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.000724
Total gradient norm: 0.002084
=== Actor Training Debug (Iteration 2140) ===
Q mean: -46.358482
Q std: 18.270590
Actor loss: 46.362469
Action reg: 0.003988
  l1.weight: grad_norm = 0.008134
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.017820
Total gradient norm: 0.032286
=== Actor Training Debug (Iteration 2141) ===
Q mean: -44.018612
Q std: 19.422937
Actor loss: 44.022572
Action reg: 0.003958
  l1.weight: grad_norm = 0.000645
  l1.bias: grad_norm = 0.000546
  l2.weight: grad_norm = 0.003078
Total gradient norm: 0.010147
=== Actor Training Debug (Iteration 2142) ===
Q mean: -44.997837
Q std: 17.069151
Actor loss: 45.001816
Action reg: 0.003978
  l1.weight: grad_norm = 0.000365
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.001358
Total gradient norm: 0.004201
=== Actor Training Debug (Iteration 2143) ===
Q mean: -44.923256
Q std: 19.137278
Actor loss: 44.927250
Action reg: 0.003995
  l1.weight: grad_norm = 0.000125
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.000310
Total gradient norm: 0.000858
=== Actor Training Debug (Iteration 2144) ===
Q mean: -44.866997
Q std: 17.846704
Actor loss: 44.870983
Action reg: 0.003986
  l1.weight: grad_norm = 0.000229
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.000910
Total gradient norm: 0.002823
=== Actor Training Debug (Iteration 2145) ===
Q mean: -45.321442
Q std: 18.606806
Actor loss: 45.325432
Action reg: 0.003989
  l1.weight: grad_norm = 0.064776
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.137487
Total gradient norm: 0.267931
=== Actor Training Debug (Iteration 2146) ===
Q mean: -43.603031
Q std: 18.257160
Actor loss: 43.607025
Action reg: 0.003995
  l1.weight: grad_norm = 0.000137
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.000349
Total gradient norm: 0.000957
=== Actor Training Debug (Iteration 2147) ===
Q mean: -45.908230
Q std: 18.613377
Actor loss: 45.912228
Action reg: 0.004000
  l1.weight: grad_norm = 0.012968
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.031810
Total gradient norm: 0.060227
=== Actor Training Debug (Iteration 2148) ===
Q mean: -43.300812
Q std: 16.838764
Actor loss: 43.304802
Action reg: 0.003992
  l1.weight: grad_norm = 0.000330
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.000772
Total gradient norm: 0.001928
=== Actor Training Debug (Iteration 2149) ===
Q mean: -45.645660
Q std: 19.420477
Actor loss: 45.649643
Action reg: 0.003984
  l1.weight: grad_norm = 0.000275
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.001056
Total gradient norm: 0.003276
=== Actor Training Debug (Iteration 2150) ===
Q mean: -45.644073
Q std: 18.983288
Actor loss: 45.648075
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2151) ===
Q mean: -45.280647
Q std: 18.475786
Actor loss: 45.284637
Action reg: 0.003991
  l1.weight: grad_norm = 0.000196
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.000599
Total gradient norm: 0.001738
=== Actor Training Debug (Iteration 2152) ===
Q mean: -44.118282
Q std: 17.867163
Actor loss: 44.122269
Action reg: 0.003988
  l1.weight: grad_norm = 0.000416
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.001213
Total gradient norm: 0.003123
=== Actor Training Debug (Iteration 2153) ===
Q mean: -45.643120
Q std: 17.513521
Actor loss: 45.647102
Action reg: 0.003983
  l1.weight: grad_norm = 0.000261
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.000986
Total gradient norm: 0.003120
=== Actor Training Debug (Iteration 2154) ===
Q mean: -46.884598
Q std: 17.232191
Actor loss: 46.888580
Action reg: 0.003983
  l1.weight: grad_norm = 0.102414
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.214698
Total gradient norm: 0.420370
=== Actor Training Debug (Iteration 2155) ===
Q mean: -43.017288
Q std: 17.054358
Actor loss: 43.021278
Action reg: 0.003990
  l1.weight: grad_norm = 0.008763
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.018852
Total gradient norm: 0.037328
=== Actor Training Debug (Iteration 2156) ===
Q mean: -43.168884
Q std: 18.827871
Actor loss: 43.172852
Action reg: 0.003966
  l1.weight: grad_norm = 0.029457
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.063082
Total gradient norm: 0.124775
=== Actor Training Debug (Iteration 2157) ===
Q mean: -46.536171
Q std: 18.025537
Actor loss: 46.540154
Action reg: 0.003982
  l1.weight: grad_norm = 0.000378
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.001540
Total gradient norm: 0.004886
=== Actor Training Debug (Iteration 2158) ===
Q mean: -46.822178
Q std: 18.998766
Actor loss: 46.826160
Action reg: 0.003983
  l1.weight: grad_norm = 0.000273
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.001082
Total gradient norm: 0.003322
=== Actor Training Debug (Iteration 2159) ===
Q mean: -43.560223
Q std: 17.960579
Actor loss: 43.564209
Action reg: 0.003985
  l1.weight: grad_norm = 0.000344
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.001262
Total gradient norm: 0.003968
=== Actor Training Debug (Iteration 2160) ===
Q mean: -43.699196
Q std: 18.428532
Actor loss: 43.703190
Action reg: 0.003995
  l1.weight: grad_norm = 0.000188
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.000649
Total gradient norm: 0.001734
=== Actor Training Debug (Iteration 2161) ===
Q mean: -45.746956
Q std: 18.929619
Actor loss: 45.750942
Action reg: 0.003985
  l1.weight: grad_norm = 0.000441
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.001553
Total gradient norm: 0.004394
=== Actor Training Debug (Iteration 2162) ===
Q mean: -45.172234
Q std: 17.675545
Actor loss: 45.176216
Action reg: 0.003982
  l1.weight: grad_norm = 0.002996
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.007137
Total gradient norm: 0.015115
=== Actor Training Debug (Iteration 2163) ===
Q mean: -42.256073
Q std: 17.263426
Actor loss: 42.260056
Action reg: 0.003984
  l1.weight: grad_norm = 0.008602
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.019653
Total gradient norm: 0.039605
=== Actor Training Debug (Iteration 2164) ===
Q mean: -42.472355
Q std: 16.719717
Actor loss: 42.476337
Action reg: 0.003982
  l1.weight: grad_norm = 0.000448
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.001450
Total gradient norm: 0.004350
=== Actor Training Debug (Iteration 2165) ===
Q mean: -45.758053
Q std: 19.941820
Actor loss: 45.762016
Action reg: 0.003962
  l1.weight: grad_norm = 0.000606
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.002580
Total gradient norm: 0.008168
=== Actor Training Debug (Iteration 2166) ===
Q mean: -45.004234
Q std: 18.623867
Actor loss: 45.008205
Action reg: 0.003971
  l1.weight: grad_norm = 0.000425
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.001769
Total gradient norm: 0.005636
=== Actor Training Debug (Iteration 2167) ===
Q mean: -43.713821
Q std: 19.164150
Actor loss: 43.717796
Action reg: 0.003973
  l1.weight: grad_norm = 0.007187
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.015328
Total gradient norm: 0.026680
=== Actor Training Debug (Iteration 2168) ===
Q mean: -45.009171
Q std: 19.144938
Actor loss: 45.013165
Action reg: 0.003993
  l1.weight: grad_norm = 0.000244
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.000650
Total gradient norm: 0.001659
=== Actor Training Debug (Iteration 2169) ===
Q mean: -46.594894
Q std: 19.094114
Actor loss: 46.598869
Action reg: 0.003975
  l1.weight: grad_norm = 0.000407
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.001844
Total gradient norm: 0.006013
=== Actor Training Debug (Iteration 2170) ===
Q mean: -46.336727
Q std: 18.245468
Actor loss: 46.340717
Action reg: 0.003989
  l1.weight: grad_norm = 0.001357
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.003019
Total gradient norm: 0.005920
=== Actor Training Debug (Iteration 2171) ===
Q mean: -44.516914
Q std: 16.908409
Actor loss: 44.520885
Action reg: 0.003971
  l1.weight: grad_norm = 0.110898
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.259074
Total gradient norm: 0.516614
=== Actor Training Debug (Iteration 2172) ===
Q mean: -42.773758
Q std: 17.961588
Actor loss: 42.777740
Action reg: 0.003983
  l1.weight: grad_norm = 0.132327
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.359599
Total gradient norm: 0.700010
=== Actor Training Debug (Iteration 2173) ===
Q mean: -47.075340
Q std: 19.345556
Actor loss: 47.079315
Action reg: 0.003976
  l1.weight: grad_norm = 0.000402
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.001524
Total gradient norm: 0.004710
=== Actor Training Debug (Iteration 2174) ===
Q mean: -45.072868
Q std: 19.401148
Actor loss: 45.076862
Action reg: 0.003994
  l1.weight: grad_norm = 0.012565
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.032049
Total gradient norm: 0.062677
=== Actor Training Debug (Iteration 2175) ===
Q mean: -45.349216
Q std: 19.398844
Actor loss: 45.353199
Action reg: 0.003983
  l1.weight: grad_norm = 0.000394
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.001499
Total gradient norm: 0.004532
=== Actor Training Debug (Iteration 2176) ===
Q mean: -43.264481
Q std: 18.474550
Actor loss: 43.268471
Action reg: 0.003989
  l1.weight: grad_norm = 0.000276
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.000848
Total gradient norm: 0.002514
=== Actor Training Debug (Iteration 2177) ===
Q mean: -44.408108
Q std: 17.067041
Actor loss: 44.412109
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2178) ===
Q mean: -47.987328
Q std: 18.043953
Actor loss: 47.991299
Action reg: 0.003970
  l1.weight: grad_norm = 0.000599
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.002382
Total gradient norm: 0.007107
=== Actor Training Debug (Iteration 2179) ===
Q mean: -47.719635
Q std: 18.859732
Actor loss: 47.723621
Action reg: 0.003987
  l1.weight: grad_norm = 0.000378
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.001533
Total gradient norm: 0.004618
=== Actor Training Debug (Iteration 2180) ===
Q mean: -42.246117
Q std: 18.711473
Actor loss: 42.250084
Action reg: 0.003967
  l1.weight: grad_norm = 0.000612
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.002539
Total gradient norm: 0.007873
=== Actor Training Debug (Iteration 2181) ===
Q mean: -41.311459
Q std: 17.573328
Actor loss: 41.315445
Action reg: 0.003988
  l1.weight: grad_norm = 0.000260
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.000796
Total gradient norm: 0.002308
Total gradient norm: 0.003099ration 1203) ===
=== Actor Training Debug (Iteration 2192) ===
Q mean: -44.059654
Q std: 18.885748
Actor loss: 44.063625
Action reg: 0.003972
  l1.weight: grad_norm = 0.125003
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.296744
Total gradient norm: 0.564166
=== Actor Training Debug (Iteration 2193) ===
Q mean: -44.013603
Q std: 18.239994
Actor loss: 44.017590
Action reg: 0.003988
  l1.weight: grad_norm = 0.000270
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.000787
Total gradient norm: 0.002275
=== Actor Training Debug (Iteration 2194) ===
Q mean: -45.750412
Q std: 17.431122
Actor loss: 45.754395
Action reg: 0.003983
  l1.weight: grad_norm = 0.000372
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.001138
Total gradient norm: 0.003149
=== Actor Training Debug (Iteration 2195) ===
Q mean: -44.410725
Q std: 17.735111
Actor loss: 44.414715
Action reg: 0.003988
  l1.weight: grad_norm = 0.000231
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.000752
Total gradient norm: 0.002255
=== Actor Training Debug (Iteration 2196) ===
Q mean: -45.145306
Q std: 18.744488
Actor loss: 45.149273
Action reg: 0.003968
  l1.weight: grad_norm = 0.000573
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.002056
Total gradient norm: 0.006295
=== Actor Training Debug (Iteration 2197) ===
Q mean: -44.373360
Q std: 17.760633
Actor loss: 44.377361
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2198) ===
Q mean: -43.844116
Q std: 19.462053
Actor loss: 43.848076
Action reg: 0.003961
  l1.weight: grad_norm = 0.029345
  l1.bias: grad_norm = 0.000594
  l2.weight: grad_norm = 0.065276
Total gradient norm: 0.130438
=== Actor Training Debug (Iteration 2199) ===
Q mean: -46.732529
Q std: 16.979279
Actor loss: 46.736526
Action reg: 0.003996
  l1.weight: grad_norm = 0.000155
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.000507
Total gradient norm: 0.001303
=== Actor Training Debug (Iteration 2200) ===
Q mean: -44.403748
Q std: 18.244827
Actor loss: 44.407722
Action reg: 0.003975
  l1.weight: grad_norm = 0.000498
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.002063
Total gradient norm: 0.006411
=== Actor Training Debug (Iteration 2201) ===
Q mean: -45.589508
Q std: 17.961100
Actor loss: 45.593487
Action reg: 0.003977
  l1.weight: grad_norm = 0.000383
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.001629
Total gradient norm: 0.005137
=== Actor Training Debug (Iteration 2202) ===
Q mean: -45.799873
Q std: 19.154442
Actor loss: 45.803864
Action reg: 0.003989
  l1.weight: grad_norm = 0.001342
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.003249
Total gradient norm: 0.007024
=== Actor Training Debug (Iteration 2203) ===
Q mean: -46.422691
Q std: 18.963125
Actor loss: 46.426670
Action reg: 0.003980
  l1.weight: grad_norm = 0.000464
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.001476
Total gradient norm: 0.004242
=== Actor Training Debug (Iteration 2204) ===
Q mean: -44.754135
Q std: 19.398920
Actor loss: 44.758125
Action reg: 0.003990
  l1.weight: grad_norm = 0.000236
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.000789
Total gradient norm: 0.002250
=== Actor Training Debug (Iteration 2205) ===
Q mean: -45.803276
Q std: 17.542820
Actor loss: 45.807270
Action reg: 0.003994
  l1.weight: grad_norm = 0.000221
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.000601
Total gradient norm: 0.001542
=== Actor Training Debug (Iteration 2206) ===
Q mean: -44.273415
Q std: 18.102562
Actor loss: 44.277397
Action reg: 0.003981
  l1.weight: grad_norm = 0.000430
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.001433
Total gradient norm: 0.004425
=== Actor Training Debug (Iteration 2207) ===
Q mean: -49.363270
Q std: 18.143688
Actor loss: 49.367271
Action reg: 0.004000
  l1.weight: grad_norm = 0.000000
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000000
Total gradient norm: 0.000000
WARNING: Very small gradients detected!
=== Actor Training Debug (Iteration 2208) ===
Q mean: -45.798534
Q std: 17.383461
Actor loss: 45.802517
Action reg: 0.003982
  l1.weight: grad_norm = 0.000282
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.001165
Total gradient norm: 0.003842
=== Actor Training Debug (Iteration 2209) ===
Q mean: -43.114330
Q std: 17.637486
Actor loss: 43.118324
Action reg: 0.003993
  l1.weight: grad_norm = 0.000327
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.000881
Total gradient norm: 0.002172
=== Actor Training Debug (Iteration 2210) ===
Q mean: -48.378616
Q std: 20.647863
Actor loss: 48.382610
Action reg: 0.003993
  l1.weight: grad_norm = 0.011744
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.025713
Total gradient norm: 0.051013
=== Actor Training Debug (Iteration 2211) ===
Q mean: -44.955799
Q std: 19.316673
Actor loss: 44.959793
Action reg: 0.003994
  l1.weight: grad_norm = 0.000192
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.000430
Total gradient norm: 0.001127
=== Actor Training Debug (Iteration 2212) ===
Q mean: -45.695007
Q std: 19.678631
Actor loss: 45.698978
Action reg: 0.003971
  l1.weight: grad_norm = 0.000532
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.002386
Total gradient norm: 0.007622
=== Actor Training Debug (Iteration 2213) ===
Q mean: -44.420177
Q std: 18.154280
Actor loss: 44.424168
Action reg: 0.003990
  l1.weight: grad_norm = 0.000291
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.000955
Total gradient norm: 0.002790
=== Actor Training Debug (Iteration 2214) ===
Q mean: -48.347813
Q std: 18.048573
Actor loss: 48.351799
Action reg: 0.003988
  l1.weight: grad_norm = 0.000220
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.000681
Total gradient norm: 0.001995
=== Actor Training Debug (Iteration 2215) ===
Q mean: -48.288887
Q std: 16.904516
Actor loss: 48.292873
Action reg: 0.003988
  l1.weight: grad_norm = 0.000250
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.000725
Total gradient norm: 0.001911
=== Actor Training Debug (Iteration 2216) ===
Q mean: -43.956928
Q std: 18.816025
Actor loss: 43.960911
Action reg: 0.003981
  l1.weight: grad_norm = 0.001393
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.003266
Total gradient norm: 0.006986
=== Actor Training Debug (Iteration 2217) ===
Q mean: -43.673660
Q std: 18.582222
Actor loss: 43.677639
Action reg: 0.003980
  l1.weight: grad_norm = 0.000432
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.001844
Total gradient norm: 0.005884
=== Actor Training Debug (Iteration 2218) ===
Q mean: -45.327320
Q std: 18.007238
Actor loss: 45.331303
Action reg: 0.003983
  l1.weight: grad_norm = 0.000372
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.001254
Total gradient norm: 0.003772
=== Actor Training Debug (Iteration 2219) ===
Q mean: -47.172050
Q std: 17.871883
Actor loss: 47.176044
Action reg: 0.003996
  l1.weight: grad_norm = 0.000252
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.000722
Total gradient norm: 0.001757
=== Actor Training Debug (Iteration 2220) ===
Q mean: -44.207279
Q std: 18.841961
Actor loss: 44.211250
Action reg: 0.003971
  l1.weight: grad_norm = 0.000626
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.002277
Total gradient norm: 0.006676
=== Actor Training Debug (Iteration 2221) ===
Q mean: -44.596222
Q std: 17.229996
Actor loss: 44.600204
Action reg: 0.003983
  l1.weight: grad_norm = 0.000310
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.001199
Total gradient norm: 0.003715
=== Actor Training Debug (Iteration 2222) ===
Q mean: -47.135483
Q std: 18.495651
Actor loss: 47.139465
Action reg: 0.003984
  l1.weight: grad_norm = 0.000323
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.001375
Total gradient norm: 0.004410
=== Actor Training Debug (Iteration 2223) ===
Q mean: -46.150398
Q std: 18.241199
Actor loss: 46.154392
Action reg: 0.003994
  l1.weight: grad_norm = 0.001360
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.003066
Total gradient norm: 0.006269
=== Actor Training Debug (Iteration 2224) ===
Q mean: -44.251228
Q std: 19.434555
Actor loss: 44.255211
Action reg: 0.003983
  l1.weight: grad_norm = 0.004768
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.011817
Total gradient norm: 0.021343
=== Actor Training Debug (Iteration 2225) ===
Q mean: -46.094101
Q std: 19.231539
Actor loss: 46.098095
Action reg: 0.003994
  l1.weight: grad_norm = 0.000198
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.000551
Total gradient norm: 0.001525
=== Actor Training Debug (Iteration 2226) ===
Q mean: -47.008614
Q std: 18.873346
Actor loss: 47.012608
Action reg: 0.003993
  l1.weight: grad_norm = 0.000227
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.000607
Total gradient norm: 0.001536
=== Actor Training Debug (Iteration 2227) ===
Q mean: -45.630501
Q std: 18.386896
Actor loss: 45.634483
Action reg: 0.003981
  l1.weight: grad_norm = 0.000409
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.001579
Total gradient norm: 0.004974
=== Actor Training Debug (Iteration 2228) ===
Q mean: -43.335728
Q std: 17.017897
Actor loss: 43.339703
Action reg: 0.003975
  l1.weight: grad_norm = 0.017083
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.042478
Total gradient norm: 0.076167
=== Actor Training Debug (Iteration 2229) ===
Q mean: -43.558784
Q std: 16.272957
Actor loss: 43.562763
Action reg: 0.003980
  l1.weight: grad_norm = 0.000440
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.001682
Total gradient norm: 0.005124
=== Actor Training Debug (Iteration 2230) ===
Q mean: -46.431358
Q std: 17.028898
Actor loss: 46.435345
Action reg: 0.003986
  l1.weight: grad_norm = 0.000444
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.001395
Total gradient norm: 0.003912
=== Actor Training Debug (Iteration 2231) ===
Q mean: -45.351574
Q std: 18.883276
Actor loss: 45.355564
Action reg: 0.003989
  l1.weight: grad_norm = 0.000285
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.001028
Total gradient norm: 0.002988
=== Actor Training Debug (Iteration 2232) ===
Q mean: -44.944519
Q std: 18.463640
Actor loss: 44.948509
Action reg: 0.003989
  l1.weight: grad_norm = 0.000258
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.001003
Total gradient norm: 0.002996
=== Actor Training Debug (Iteration 2233) ===
Total gradient norm: 0.003099ration 1203) ===
=== Actor Training Debug (Iteration 2243) ===
Q mean: -44.052654
Q std: 18.063526
Actor loss: 44.056633
Action reg: 0.003980
  l1.weight: grad_norm = 0.150773
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.536800
Total gradient norm: 1.190384
=== Actor Training Debug (Iteration 2244) ===
Q mean: -45.518860
Q std: 19.676067
Actor loss: 45.522839
Action reg: 0.003979
  l1.weight: grad_norm = 0.049024
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.154711
Total gradient norm: 0.254393
=== Actor Training Debug (Iteration 2245) ===
Q mean: -44.327850
Q std: 19.135855
Actor loss: 44.331837
Action reg: 0.003987
  l1.weight: grad_norm = 0.019291
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.055732
Total gradient norm: 0.084838
=== Actor Training Debug (Iteration 2246) ===
Q mean: -46.870010
Q std: 18.626881
Actor loss: 46.873978
Action reg: 0.003969
  l1.weight: grad_norm = 0.106998
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.365008
Total gradient norm: 0.631734
=== Actor Training Debug (Iteration 2247) ===
Q mean: -42.302406
Q std: 16.946720
Actor loss: 42.306389
Action reg: 0.003983
  l1.weight: grad_norm = 0.006762
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.022356
Total gradient norm: 0.043785
=== Actor Training Debug (Iteration 2248) ===
Q mean: -44.900749
Q std: 20.369583
Actor loss: 44.904724
Action reg: 0.003975
  l1.weight: grad_norm = 0.049734
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.186714
Total gradient norm: 0.435844
=== Actor Training Debug (Iteration 2249) ===
Q mean: -46.829792
Q std: 19.730595
Actor loss: 46.833755
Action reg: 0.003963
  l1.weight: grad_norm = 0.067037
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.215202
Total gradient norm: 0.437567
=== Actor Training Debug (Iteration 2250) ===
Q mean: -47.690834
Q std: 18.032629
Actor loss: 47.694828
Action reg: 0.003993
  l1.weight: grad_norm = 0.130742
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.384999
Total gradient norm: 0.631916
=== Actor Training Debug (Iteration 2251) ===
Q mean: -44.679596
Q std: 17.774202
Actor loss: 44.683567
Action reg: 0.003969
  l1.weight: grad_norm = 0.162045
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.529689
Total gradient norm: 1.199270
=== Actor Training Debug (Iteration 2252) ===
Q mean: -44.387276
Q std: 17.004370
Actor loss: 44.391251
Action reg: 0.003974
  l1.weight: grad_norm = 0.037813
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.123075
Total gradient norm: 0.271033
=== Actor Training Debug (Iteration 2253) ===
Q mean: -50.280724
Q std: 19.255348
Actor loss: 50.284702
Action reg: 0.003980
  l1.weight: grad_norm = 0.028356
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.104158
Total gradient norm: 0.215292
=== Actor Training Debug (Iteration 2254) ===
Q mean: -46.998810
Q std: 18.836180
Actor loss: 47.002781
Action reg: 0.003969
  l1.weight: grad_norm = 0.024648
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.055712
Total gradient norm: 0.110483
=== Actor Training Debug (Iteration 2255) ===
Q mean: -42.530037
Q std: 18.866373
Actor loss: 42.534012
Action reg: 0.003974
  l1.weight: grad_norm = 0.037022
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.100999
Total gradient norm: 0.159500
=== Actor Training Debug (Iteration 2256) ===
Q mean: -42.015114
Q std: 18.023998
Actor loss: 42.019100
Action reg: 0.003988
  l1.weight: grad_norm = 0.003132
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.009846
Total gradient norm: 0.021148
=== Actor Training Debug (Iteration 2257) ===
Q mean: -45.329403
Q std: 19.518692
Actor loss: 45.333378
Action reg: 0.003976
  l1.weight: grad_norm = 0.140584
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.412640
Total gradient norm: 0.795853
=== Actor Training Debug (Iteration 2258) ===
Q mean: -46.688507
Q std: 19.489166
Actor loss: 46.692490
Action reg: 0.003981
  l1.weight: grad_norm = 0.006644
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.019691
Total gradient norm: 0.031527
=== Actor Training Debug (Iteration 2259) ===
Q mean: -42.861961
Q std: 17.359722
Actor loss: 42.865944
Action reg: 0.003982
  l1.weight: grad_norm = 0.005122
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.013229
Total gradient norm: 0.022077
=== Actor Training Debug (Iteration 2260) ===
Q mean: -45.007633
Q std: 16.926716
Actor loss: 45.011600
Action reg: 0.003969
  l1.weight: grad_norm = 0.111773
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.277750
Total gradient norm: 0.436200
=== Actor Training Debug (Iteration 2261) ===
Q mean: -48.210869
Q std: 18.031754
Actor loss: 48.214848
Action reg: 0.003979
  l1.weight: grad_norm = 0.011436
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.032399
Total gradient norm: 0.068820
=== Actor Training Debug (Iteration 2262) ===
Q mean: -46.627605
Q std: 18.123989
Actor loss: 46.631584
Action reg: 0.003980
  l1.weight: grad_norm = 0.007153
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.021274
Total gradient norm: 0.047019
=== Actor Training Debug (Iteration 2263) ===
Q mean: -43.786472
Q std: 18.749836
Actor loss: 43.790466
Action reg: 0.003993
  l1.weight: grad_norm = 0.034981
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.117523
Total gradient norm: 0.277425
=== Actor Training Debug (Iteration 2264) ===
Q mean: -43.755753
Q std: 17.646645
Actor loss: 43.759739
Action reg: 0.003987
  l1.weight: grad_norm = 0.248955
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.905465
Total gradient norm: 1.876183
=== Actor Training Debug (Iteration 2265) ===
Q mean: -45.826912
Q std: 17.811834
Actor loss: 45.830902
Action reg: 0.003989
  l1.weight: grad_norm = 0.078921
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.267828
Total gradient norm: 0.635948
=== Actor Training Debug (Iteration 2266) ===
Q mean: -47.382942
Q std: 19.628405
Actor loss: 47.386913
Action reg: 0.003970
  l1.weight: grad_norm = 0.019746
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.063711
Total gradient norm: 0.127016
=== Actor Training Debug (Iteration 2267) ===
Q mean: -43.913174
Q std: 18.505621
Actor loss: 43.917152
Action reg: 0.003979
  l1.weight: grad_norm = 0.050538
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.126025
Total gradient norm: 0.185055
=== Actor Training Debug (Iteration 2268) ===
Q mean: -45.316990
Q std: 17.803524
Actor loss: 45.320976
Action reg: 0.003987
  l1.weight: grad_norm = 0.003885
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.011458
Total gradient norm: 0.021129
=== Actor Training Debug (Iteration 2269) ===
Q mean: -48.259903
Q std: 18.904131
Actor loss: 48.263874
Action reg: 0.003969
  l1.weight: grad_norm = 0.000555
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.001985
Total gradient norm: 0.005803
=== Actor Training Debug (Iteration 2270) ===
Q mean: -47.686760
Q std: 18.945427
Actor loss: 47.690739
Action reg: 0.003980
  l1.weight: grad_norm = 0.042675
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.109337
Total gradient norm: 0.196175
=== Actor Training Debug (Iteration 2271) ===
Q mean: -46.154331
Q std: 18.527809
Actor loss: 46.158302
Action reg: 0.003970
  l1.weight: grad_norm = 0.020086
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.049664
Total gradient norm: 0.098908
=== Actor Training Debug (Iteration 2272) ===
Q mean: -44.599060
Q std: 16.784634
Actor loss: 44.603054
Action reg: 0.003993
  l1.weight: grad_norm = 0.002115
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.006271
Total gradient norm: 0.012249
=== Actor Training Debug (Iteration 2273) ===
Q mean: -45.222271
Q std: 18.328537
Actor loss: 45.226254
Action reg: 0.003984
  l1.weight: grad_norm = 0.105315
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.250498
Total gradient norm: 0.410076
=== Actor Training Debug (Iteration 2274) ===
Q mean: -48.031853
Q std: 18.724415
Actor loss: 48.035831
Action reg: 0.003980
  l1.weight: grad_norm = 0.014448
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.042940
Total gradient norm: 0.091278
=== Actor Training Debug (Iteration 2275) ===
Q mean: -45.904282
Q std: 19.665556
Actor loss: 45.908260
Action reg: 0.003977
  l1.weight: grad_norm = 0.000865
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.002962
Total gradient norm: 0.008555
=== Actor Training Debug (Iteration 2276) ===
Q mean: -44.694469
Q std: 18.013248
Actor loss: 44.698452
Action reg: 0.003983
  l1.weight: grad_norm = 0.000432
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.001556
Total gradient norm: 0.004254
=== Actor Training Debug (Iteration 2277) ===
Q mean: -46.252846
Q std: 18.395000
Actor loss: 46.256821
Action reg: 0.003974
  l1.weight: grad_norm = 0.035055
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.096699
Total gradient norm: 0.191124
=== Actor Training Debug (Iteration 2278) ===
Q mean: -46.009079
Q std: 19.039116
Actor loss: 46.013050
Action reg: 0.003972
  l1.weight: grad_norm = 0.081161
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.228941
Total gradient norm: 0.497032
=== Actor Training Debug (Iteration 2279) ===
Q mean: -45.276321
Q std: 16.890171
Actor loss: 45.280312
Action reg: 0.003990
  l1.weight: grad_norm = 0.059501
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.171062
Total gradient norm: 0.380994
=== Actor Training Debug (Iteration 2280) ===
Q mean: -46.309570
Q std: 18.496309
Actor loss: 46.313534
Action reg: 0.003964
  l1.weight: grad_norm = 0.019377
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.051088
Total gradient norm: 0.096738
=== Actor Training Debug (Iteration 2281) ===
Q mean: -46.142151
Q std: 18.678555
Actor loss: 46.146126
Action reg: 0.003976
  l1.weight: grad_norm = 0.004153
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.011234
Total gradient norm: 0.020543
=== Actor Training Debug (Iteration 2282) ===
Q mean: -47.525055
Q std: 19.060251
Actor loss: 47.529037
Action reg: 0.003983
  l1.weight: grad_norm = 0.016503
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.041920
Total gradient norm: 0.082255
=== Actor Training Debug (Iteration 2283) ===
Q mean: -45.482517
Q std: 18.069265
Actor loss: 45.486504
Action reg: 0.003985
  l1.weight: grad_norm = 0.030303
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.088967
Total gradient norm: 0.191011
=== Actor Training Debug (Iteration 2284) ===
Q mean: -45.271961
Q std: 17.582363
Actor loss: 45.275963
Action reg: 0.004000
  l1.weight: grad_norm = 0.001918
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.005571
Total gradient norm: 0.012032
=== Actor Training Debug (Iteration 2285) ===
Q mean: -48.821896
Q std: 18.898438
Actor loss: 48.825893
Action reg: 0.004000
  l1.weight: grad_norm = 0.000600
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001396
Total gradient norm: 0.002092
=== Actor Training Debug (Iteration 2286) ===
Q mean: -49.016956
Q std: 19.098730
Actor loss: 49.020947
Action reg: 0.003991
  l1.weight: grad_norm = 0.036802
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.095888
Total gradient norm: 0.194315
=== Actor Training Debug (Iteration 2287) ===
Q mean: -46.239952
Q std: 19.593309
Actor loss: 46.243938
Action reg: 0.003986
  l1.weight: grad_norm = 0.032986
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.099086
Total gradient norm: 0.180351
=== Actor Training Debug (Iteration 2288) ===
Q mean: -44.625885
Q std: 17.022949
Actor loss: 44.629879
Action reg: 0.003993
  l1.weight: grad_norm = 0.011221
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.030414
Total gradient norm: 0.047738
=== Actor Training Debug (Iteration 2289) ===
Q mean: -45.763229
Q std: 18.098766
Actor loss: 45.767216
Action reg: 0.003988
  l1.weight: grad_norm = 0.000432
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.001116
Total gradient norm: 0.002643
=== Actor Training Debug (Iteration 2290) ===
Q mean: -46.547905
Q std: 18.334642
Actor loss: 46.551888
Action reg: 0.003983
  l1.weight: grad_norm = 0.000694
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.001928
Total gradient norm: 0.003909
=== Actor Training Debug (Iteration 2291) ===
Q mean: -45.515766
Q std: 18.712828
Actor loss: 45.519745
Action reg: 0.003980
  l1.weight: grad_norm = 0.061849
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.183379
Total gradient norm: 0.306568
=== Actor Training Debug (Iteration 2292) ===
Q mean: -47.740864
Q std: 17.466656
Actor loss: 47.744839
Action reg: 0.003974
  l1.weight: grad_norm = 0.019805
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.060396
Total gradient norm: 0.113923
=== Actor Training Debug (Iteration 2293) ===
Q mean: -45.963749
Q std: 17.870914
Actor loss: 45.967735
Action reg: 0.003987
  l1.weight: grad_norm = 0.100194
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.284644
Total gradient norm: 0.474685
=== Actor Training Debug (Iteration 2294) ===
Q mean: -44.906055
Q std: 17.872280
Actor loss: 44.910038
Action reg: 0.003982
  l1.weight: grad_norm = 0.018771
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.060594
Total gradient norm: 0.111858
=== Actor Training Debug (Iteration 2295) ===
Q mean: -45.290161
Q std: 18.695250
Actor loss: 45.294147
Action reg: 0.003985
  l1.weight: grad_norm = 0.000620
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.001628
Total gradient norm: 0.003690
=== Actor Training Debug (Iteration 2296) ===
Q mean: -46.643490
Q std: 19.746124
Actor loss: 46.647480
Action reg: 0.003990
  l1.weight: grad_norm = 0.010521
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.028522
Total gradient norm: 0.051719
=== Actor Training Debug (Iteration 2297) ===
Q mean: -47.599869
Q std: 19.246883
Actor loss: 47.603844
Action reg: 0.003974
  l1.weight: grad_norm = 0.044833
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.118449
Total gradient norm: 0.209131
Total gradient norm: 0.346185ration 1203) ===
=== Actor Training Debug (Iteration 2308) ===
Q mean: -45.273392
Q std: 18.623341
Actor loss: 45.277363
Action reg: 0.003971
  l1.weight: grad_norm = 0.003047
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.009582
Total gradient norm: 0.018210
=== Actor Training Debug (Iteration 2309) ===
Q mean: -45.701839
Q std: 18.145014
Actor loss: 45.705833
Action reg: 0.003995
  l1.weight: grad_norm = 0.001150
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.002962
Total gradient norm: 0.004786
=== Actor Training Debug (Iteration 2310) ===
Q mean: -43.885406
Q std: 18.987946
Actor loss: 43.889397
Action reg: 0.003988
  l1.weight: grad_norm = 0.000270
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.000825
Total gradient norm: 0.002265
=== Actor Training Debug (Iteration 2311) ===
Q mean: -43.683731
Q std: 19.724955
Actor loss: 43.687702
Action reg: 0.003970
  l1.weight: grad_norm = 0.015150
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.039029
Total gradient norm: 0.057373
=== Actor Training Debug (Iteration 2312) ===
Q mean: -45.581566
Q std: 18.814693
Actor loss: 45.585537
Action reg: 0.003969
  l1.weight: grad_norm = 0.010409
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.026949
Total gradient norm: 0.048224
=== Actor Training Debug (Iteration 2313) ===
Q mean: -48.144089
Q std: 16.878778
Actor loss: 48.148075
Action reg: 0.003988
  l1.weight: grad_norm = 0.010946
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.033722
Total gradient norm: 0.064861
=== Actor Training Debug (Iteration 2314) ===
Q mean: -46.331429
Q std: 16.785652
Actor loss: 46.335423
Action reg: 0.003994
  l1.weight: grad_norm = 0.000549
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.001628
Total gradient norm: 0.003239
=== Actor Training Debug (Iteration 2315) ===
Q mean: -46.609276
Q std: 18.977676
Actor loss: 46.613262
Action reg: 0.003988
  l1.weight: grad_norm = 0.013354
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.037797
Total gradient norm: 0.067698
=== Actor Training Debug (Iteration 2316) ===
Q mean: -44.142715
Q std: 18.499901
Actor loss: 44.146698
Action reg: 0.003981
  l1.weight: grad_norm = 0.000476
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.001380
Total gradient norm: 0.003927
=== Actor Training Debug (Iteration 2317) ===
Q mean: -45.024223
Q std: 18.508444
Actor loss: 45.028214
Action reg: 0.003989
  l1.weight: grad_norm = 0.039520
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.114934
Total gradient norm: 0.197610
=== Actor Training Debug (Iteration 2318) ===
Q mean: -46.699959
Q std: 17.931849
Actor loss: 46.703938
Action reg: 0.003978
  l1.weight: grad_norm = 0.000536
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.002192
Total gradient norm: 0.006954
=== Actor Training Debug (Iteration 2319) ===
Q mean: -46.207352
Q std: 16.700087
Actor loss: 46.211338
Action reg: 0.003984
  l1.weight: grad_norm = 0.006236
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.021776
Total gradient norm: 0.045989
=== Actor Training Debug (Iteration 2320) ===
Q mean: -46.161160
Q std: 18.907856
Actor loss: 46.165134
Action reg: 0.003974
  l1.weight: grad_norm = 0.005213
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.012781
Total gradient norm: 0.020066
=== Actor Training Debug (Iteration 2321) ===
Q mean: -45.513313
Q std: 18.737606
Actor loss: 45.517303
Action reg: 0.003990
  l1.weight: grad_norm = 0.061734
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.157615
Total gradient norm: 0.317505
=== Actor Training Debug (Iteration 2322) ===
Q mean: -45.339676
Q std: 17.952299
Actor loss: 45.343647
Action reg: 0.003971
  l1.weight: grad_norm = 0.003949
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.010882
Total gradient norm: 0.021896
=== Actor Training Debug (Iteration 2323) ===
Q mean: -46.813580
Q std: 17.872946
Actor loss: 46.817566
Action reg: 0.003987
  l1.weight: grad_norm = 0.057651
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.159135
Total gradient norm: 0.333875
=== Actor Training Debug (Iteration 2324) ===
Q mean: -44.573536
Q std: 18.708715
Actor loss: 44.577526
Action reg: 0.003991
  l1.weight: grad_norm = 0.034298
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.102915
Total gradient norm: 0.219850
=== Actor Training Debug (Iteration 2325) ===
Q mean: -47.313683
Q std: 18.551809
Actor loss: 47.317661
Action reg: 0.003980
  l1.weight: grad_norm = 0.018660
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.045642
Total gradient norm: 0.096114
=== Actor Training Debug (Iteration 2326) ===
Q mean: -46.778744
Q std: 19.248198
Actor loss: 46.782719
Action reg: 0.003975
  l1.weight: grad_norm = 0.017777
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.045559
Total gradient norm: 0.099692
=== Actor Training Debug (Iteration 2327) ===
Q mean: -43.865234
Q std: 18.023546
Actor loss: 43.869228
Action reg: 0.003994
  l1.weight: grad_norm = 0.030779
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.078965
Total gradient norm: 0.148854
=== Actor Training Debug (Iteration 2328) ===
Q mean: -41.997524
Q std: 18.615292
Actor loss: 42.001499
Action reg: 0.003974
  l1.weight: grad_norm = 0.031421
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.088515
Total gradient norm: 0.139051
=== Actor Training Debug (Iteration 2329) ===
Q mean: -49.422256
Q std: 20.227520
Actor loss: 49.426250
Action reg: 0.003993
  l1.weight: grad_norm = 0.006129
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.017517
Total gradient norm: 0.033273
=== Actor Training Debug (Iteration 2330) ===
Q mean: -49.537567
Q std: 17.401499
Actor loss: 49.541565
Action reg: 0.003999
  l1.weight: grad_norm = 0.007801
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.018469
Total gradient norm: 0.025382
=== Actor Training Debug (Iteration 2331) ===
Q mean: -47.887337
Q std: 19.800423
Actor loss: 47.891319
Action reg: 0.003982
  l1.weight: grad_norm = 0.007017
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.018140
Total gradient norm: 0.027268
=== Actor Training Debug (Iteration 2332) ===
Q mean: -45.285477
Q std: 17.648430
Actor loss: 45.289471
Action reg: 0.003993
  l1.weight: grad_norm = 0.000255
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.000726
Total gradient norm: 0.001887
=== Actor Training Debug (Iteration 2333) ===
Q mean: -41.201973
Q std: 18.735209
Actor loss: 41.205956
Action reg: 0.003982
  l1.weight: grad_norm = 0.111643
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.318055
Total gradient norm: 0.683353
=== Actor Training Debug (Iteration 2334) ===
Q mean: -43.251381
Q std: 17.524042
Actor loss: 43.255363
Action reg: 0.003984
  l1.weight: grad_norm = 0.008395
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.022010
Total gradient norm: 0.033599
=== Actor Training Debug (Iteration 2335) ===
Q mean: -50.237095
Q std: 19.614145
Actor loss: 50.241070
Action reg: 0.003976
  l1.weight: grad_norm = 0.009082
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.025073
Total gradient norm: 0.058221
=== Actor Training Debug (Iteration 2336) ===
Q mean: -49.084461
Q std: 17.824530
Actor loss: 49.088459
Action reg: 0.003998
  l1.weight: grad_norm = 0.036076
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.098570
Total gradient norm: 0.165185
=== Actor Training Debug (Iteration 2337) ===
Q mean: -47.863468
Q std: 17.658947
Actor loss: 47.867451
Action reg: 0.003982
  l1.weight: grad_norm = 0.000489
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.001714
Total gradient norm: 0.004930
=== Actor Training Debug (Iteration 2338) ===
Q mean: -42.847572
Q std: 17.250708
Actor loss: 42.851547
Action reg: 0.003976
  l1.weight: grad_norm = 0.000520
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.001859
Total gradient norm: 0.005520
=== Actor Training Debug (Iteration 2339) ===
Q mean: -43.940426
Q std: 17.676920
Actor loss: 43.944401
Action reg: 0.003975
  l1.weight: grad_norm = 0.134197
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.413561
Total gradient norm: 0.755844
=== Actor Training Debug (Iteration 2340) ===
Q mean: -49.413445
Q std: 18.348501
Actor loss: 49.417431
Action reg: 0.003987
  l1.weight: grad_norm = 0.026240
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.079901
Total gradient norm: 0.141083
=== Actor Training Debug (Iteration 2341) ===
Q mean: -47.538979
Q std: 18.204771
Actor loss: 47.542965
Action reg: 0.003986
  l1.weight: grad_norm = 0.037103
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.104563
Total gradient norm: 0.225841
=== Actor Training Debug (Iteration 2342) ===
Q mean: -44.074486
Q std: 17.683817
Actor loss: 44.078472
Action reg: 0.003985
  l1.weight: grad_norm = 0.107767
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.312757
Total gradient norm: 0.590683
=== Actor Training Debug (Iteration 2343) ===
Q mean: -44.890358
Q std: 19.281502
Actor loss: 44.894333
Action reg: 0.003975
  l1.weight: grad_norm = 0.036527
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.086510
Total gradient norm: 0.132254
=== Actor Training Debug (Iteration 2344) ===
Q mean: -47.569664
Q std: 18.897308
Actor loss: 47.573643
Action reg: 0.003980
  l1.weight: grad_norm = 0.028373
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.080044
Total gradient norm: 0.149931
=== Actor Training Debug (Iteration 2345) ===
Q mean: -44.755905
Q std: 18.738907
Actor loss: 44.759888
Action reg: 0.003981
  l1.weight: grad_norm = 0.006118
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.013981
Total gradient norm: 0.021145
=== Actor Training Debug (Iteration 2346) ===
Q mean: -45.355789
Q std: 19.343998
Actor loss: 45.359776
Action reg: 0.003986
  l1.weight: grad_norm = 0.005849
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.016617
Total gradient norm: 0.035466
=== Actor Training Debug (Iteration 2347) ===
Q mean: -45.733223
Q std: 18.719559
Actor loss: 45.737186
Action reg: 0.003964
  l1.weight: grad_norm = 0.014147
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.038276
Total gradient norm: 0.070609
=== Actor Training Debug (Iteration 2348) ===
Q mean: -45.694412
Q std: 18.874863
Actor loss: 45.698391
Action reg: 0.003979
  l1.weight: grad_norm = 0.127329
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.354313
Total gradient norm: 0.677820
=== Actor Training Debug (Iteration 2349) ===
Q mean: -46.511978
Q std: 17.861803
Actor loss: 46.515976
Action reg: 0.003999
  l1.weight: grad_norm = 0.002591
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.006446
Total gradient norm: 0.011712
=== Actor Training Debug (Iteration 2350) ===
Q mean: -46.131096
Q std: 18.416096
Actor loss: 46.135090
Action reg: 0.003992
  l1.weight: grad_norm = 0.029524
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.079903
Total gradient norm: 0.149784
=== Actor Training Debug (Iteration 2351) ===
Q mean: -46.144981
Q std: 17.382763
Actor loss: 46.148972
Action reg: 0.003990
  l1.weight: grad_norm = 0.000381
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.001129
Total gradient norm: 0.002754
=== Actor Training Debug (Iteration 2352) ===
Q mean: -46.908966
Q std: 18.934849
Actor loss: 46.912952
Action reg: 0.003988
  l1.weight: grad_norm = 0.001865
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.005371
Total gradient norm: 0.011300
=== Actor Training Debug (Iteration 2353) ===
Q mean: -43.660217
Q std: 19.143858
Actor loss: 43.664185
Action reg: 0.003966
  l1.weight: grad_norm = 0.083147
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.228088
Total gradient norm: 0.406202
=== Actor Training Debug (Iteration 2354) ===
Q mean: -46.645958
Q std: 18.285921
Actor loss: 46.649948
Action reg: 0.003992
  l1.weight: grad_norm = 0.036694
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.094276
Total gradient norm: 0.179626
=== Actor Training Debug (Iteration 2355) ===
Q mean: -48.182350
Q std: 19.591091
Actor loss: 48.186340
Action reg: 0.003989
  l1.weight: grad_norm = 0.001071
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.002714
Total gradient norm: 0.004897
=== Actor Training Debug (Iteration 2356) ===
Q mean: -45.779854
Q std: 19.257790
Actor loss: 45.783817
Action reg: 0.003963
  l1.weight: grad_norm = 0.011650
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.031639
Total gradient norm: 0.058743
=== Actor Training Debug (Iteration 2357) ===
Q mean: -48.688881
Q std: 18.929266
Actor loss: 48.692867
Action reg: 0.003987
  l1.weight: grad_norm = 0.010701
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.028277
Total gradient norm: 0.053052
=== Actor Training Debug (Iteration 2358) ===
Q mean: -45.629280
Q std: 16.512070
Actor loss: 45.633270
Action reg: 0.003988
  l1.weight: grad_norm = 0.000304
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.000924
Total gradient norm: 0.002790
=== Actor Training Debug (Iteration 2359) ===
Q mean: -45.874893
Q std: 18.680883
Actor loss: 45.878864
Action reg: 0.003973
  l1.weight: grad_norm = 0.004542
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.011983
Total gradient norm: 0.020002
=== Actor Training Debug (Iteration 2360) ===
Q mean: -48.877724
Q std: 19.266098
Actor loss: 48.881721
Action reg: 0.003999
  l1.weight: grad_norm = 0.003379
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.009354
Total gradient norm: 0.018709
=== Actor Training Debug (Iteration 2361) ===
Q mean: -48.731388
Q std: 19.396378
Actor loss: 48.735382
Action reg: 0.003992
  l1.weight: grad_norm = 0.015829
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.035519
Total gradient norm: 0.068546
=== Actor Training Debug (Iteration 2362) ===
Q mean: -48.315201
Q std: 17.363911
Actor loss: 48.319199
Action reg: 0.003999
  l1.weight: grad_norm = 0.067225
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.186407
Total gradient norm: 0.334987
=== Actor Training Debug (Iteration 2363) ===
Q mean: -43.247047
Q std: 18.314568
Actor loss: 43.251022
Action reg: 0.003973
  l1.weight: grad_norm = 0.000393
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.001842
Total gradient norm: 0.006002
=== Actor Training Debug (Iteration 2364) ===
Q mean: -48.245773
Q std: 18.248983
Actor loss: 48.249767
Action reg: 0.003995
  l1.weight: grad_norm = 0.000235
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.000517
Total gradient norm: 0.001005
=== Actor Training Debug (Iteration 2365) ===
Q mean: -48.507462
Q std: 17.911180
Actor loss: 48.511448
Action reg: 0.003988
  l1.weight: grad_norm = 0.022961
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.057299
Total gradient norm: 0.092681
=== Actor Training Debug (Iteration 2366) ===
Q mean: -47.522449
Q std: 18.107086
Actor loss: 47.526436
Action reg: 0.003985
  l1.weight: grad_norm = 0.119857
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.272017
Total gradient norm: 0.398244
Total gradient norm: 0.007340ration 1203) ===
=== Actor Training Debug (Iteration 2377) ===
Q mean: -45.250534
Q std: 18.315023
Actor loss: 45.254520
Action reg: 0.003986
  l1.weight: grad_norm = 0.100666
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.274810
Total gradient norm: 0.541999
=== Actor Training Debug (Iteration 2378) ===
Q mean: -47.185810
Q std: 18.565226
Actor loss: 47.189789
Action reg: 0.003979
  l1.weight: grad_norm = 0.078494
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.187616
Total gradient norm: 0.395553
=== Actor Training Debug (Iteration 2379) ===
Q mean: -50.075569
Q std: 16.869324
Actor loss: 50.079559
Action reg: 0.003990
  l1.weight: grad_norm = 0.000198
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.000755
Total gradient norm: 0.002265
=== Actor Training Debug (Iteration 2380) ===
Q mean: -46.058598
Q std: 19.713694
Actor loss: 46.062572
Action reg: 0.003975
  l1.weight: grad_norm = 0.001212
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.003202
Total gradient norm: 0.006850
=== Actor Training Debug (Iteration 2381) ===
Q mean: -45.306915
Q std: 16.622599
Actor loss: 45.310909
Action reg: 0.003992
  l1.weight: grad_norm = 0.040509
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.100435
Total gradient norm: 0.217789
=== Actor Training Debug (Iteration 2382) ===
Q mean: -44.873192
Q std: 18.975122
Actor loss: 44.877167
Action reg: 0.003976
  l1.weight: grad_norm = 0.000318
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.001220
Total gradient norm: 0.003809
=== Actor Training Debug (Iteration 2383) ===
Q mean: -48.776634
Q std: 17.630886
Actor loss: 48.780628
Action reg: 0.003992
  l1.weight: grad_norm = 0.031896
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.090444
Total gradient norm: 0.187338
=== Actor Training Debug (Iteration 2384) ===
Q mean: -48.221596
Q std: 17.012356
Actor loss: 48.225590
Action reg: 0.003993
  l1.weight: grad_norm = 0.016328
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.037434
Total gradient norm: 0.054256
=== Actor Training Debug (Iteration 2385) ===
Q mean: -46.716873
Q std: 18.664625
Actor loss: 46.720867
Action reg: 0.003992
  l1.weight: grad_norm = 0.003012
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.008789
Total gradient norm: 0.017278
=== Actor Training Debug (Iteration 2386) ===
Q mean: -47.375854
Q std: 18.157188
Actor loss: 47.379845
Action reg: 0.003990
  l1.weight: grad_norm = 0.002696
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.007831
Total gradient norm: 0.017681
=== Actor Training Debug (Iteration 2387) ===
Q mean: -46.601257
Q std: 19.072632
Actor loss: 46.605247
Action reg: 0.003989
  l1.weight: grad_norm = 0.016745
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.041738
Total gradient norm: 0.065178
=== Actor Training Debug (Iteration 2388) ===
Q mean: -46.250912
Q std: 17.724136
Actor loss: 46.254890
Action reg: 0.003978
  l1.weight: grad_norm = 0.012755
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.031198
Total gradient norm: 0.067488
=== Actor Training Debug (Iteration 2389) ===
Q mean: -46.667931
Q std: 18.992035
Actor loss: 46.671906
Action reg: 0.003976
  l1.weight: grad_norm = 0.072989
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.167177
Total gradient norm: 0.281255
=== Actor Training Debug (Iteration 2390) ===
Q mean: -44.021881
Q std: 18.615704
Actor loss: 44.025848
Action reg: 0.003969
  l1.weight: grad_norm = 0.009336
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.022681
Total gradient norm: 0.041121
=== Actor Training Debug (Iteration 2391) ===
Q mean: -45.832031
Q std: 20.143007
Actor loss: 45.835995
Action reg: 0.003962
  l1.weight: grad_norm = 0.006689
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.017536
Total gradient norm: 0.036311
=== Actor Training Debug (Iteration 2392) ===
Q mean: -49.496792
Q std: 17.754244
Actor loss: 49.500782
Action reg: 0.003990
  l1.weight: grad_norm = 0.089646
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.255309
Total gradient norm: 0.502925
=== Actor Training Debug (Iteration 2393) ===
Q mean: -47.573891
Q std: 17.866774
Actor loss: 47.577881
Action reg: 0.003989
  l1.weight: grad_norm = 0.001821
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.005186
Total gradient norm: 0.011436
=== Actor Training Debug (Iteration 2394) ===
Q mean: -45.089333
Q std: 16.327118
Actor loss: 45.093319
Action reg: 0.003988
  l1.weight: grad_norm = 0.000308
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.000746
Total gradient norm: 0.002041
=== Actor Training Debug (Iteration 2395) ===
Q mean: -47.096226
Q std: 18.877241
Actor loss: 47.100212
Action reg: 0.003986
  l1.weight: grad_norm = 0.071999
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.186354
Total gradient norm: 0.346673
=== Actor Training Debug (Iteration 2396) ===
Q mean: -46.935135
Q std: 18.204729
Actor loss: 46.939125
Action reg: 0.003989
  l1.weight: grad_norm = 0.004392
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.009862
Total gradient norm: 0.017805
=== Actor Training Debug (Iteration 2397) ===
Q mean: -45.460869
Q std: 18.569492
Actor loss: 45.464863
Action reg: 0.003993
  l1.weight: grad_norm = 0.012169
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.033580
Total gradient norm: 0.063611
=== Actor Training Debug (Iteration 2398) ===
Q mean: -44.651627
Q std: 18.089354
Actor loss: 44.655613
Action reg: 0.003986
  l1.weight: grad_norm = 0.089723
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.219342
Total gradient norm: 0.418123
=== Actor Training Debug (Iteration 2399) ===
Q mean: -47.338646
Q std: 15.831230
Actor loss: 47.342640
Action reg: 0.003996
  l1.weight: grad_norm = 0.002309
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.006924
Total gradient norm: 0.011010
=== Actor Training Debug (Iteration 2400) ===
Q mean: -47.111259
Q std: 16.525406
Actor loss: 47.115253
Action reg: 0.003993
  l1.weight: grad_norm = 0.019521
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.051218
Total gradient norm: 0.099546
=== Actor Training Debug (Iteration 2401) ===
Q mean: -48.569595
Q std: 17.286486
Actor loss: 48.573578
Action reg: 0.003983
  l1.weight: grad_norm = 0.005295
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.014865
Total gradient norm: 0.029348
=== Actor Training Debug (Iteration 2402) ===
Q mean: -46.243786
Q std: 18.125212
Actor loss: 46.247776
Action reg: 0.003992
  l1.weight: grad_norm = 0.004894
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.013740
Total gradient norm: 0.030397
=== Actor Training Debug (Iteration 2403) ===
Q mean: -46.806976
Q std: 20.520237
Actor loss: 46.810955
Action reg: 0.003978
  l1.weight: grad_norm = 0.067161
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.213903
Total gradient norm: 0.446877
=== Actor Training Debug (Iteration 2404) ===
Q mean: -46.821896
Q std: 19.222374
Actor loss: 46.825878
Action reg: 0.003984
  l1.weight: grad_norm = 0.000973
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.002724
Total gradient norm: 0.005284
=== Actor Training Debug (Iteration 2405) ===
Q mean: -47.949665
Q std: 17.056131
Actor loss: 47.953659
Action reg: 0.003993
  l1.weight: grad_norm = 0.007865
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.019361
Total gradient norm: 0.028480
=== Actor Training Debug (Iteration 2406) ===
Q mean: -47.179085
Q std: 17.312237
Actor loss: 47.183086
Action reg: 0.004000
  l1.weight: grad_norm = 0.003422
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.010091
Total gradient norm: 0.019117
=== Actor Training Debug (Iteration 2407) ===
Q mean: -47.208233
Q std: 16.922495
Actor loss: 47.212208
Action reg: 0.003977
  l1.weight: grad_norm = 0.040008
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.109558
Total gradient norm: 0.243255
=== Actor Training Debug (Iteration 2408) ===
Q mean: -45.902424
Q std: 19.066509
Actor loss: 45.906399
Action reg: 0.003976
  l1.weight: grad_norm = 0.020501
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.045376
Total gradient norm: 0.085108
=== Actor Training Debug (Iteration 2409) ===
Q mean: -46.213352
Q std: 18.155418
Actor loss: 46.217327
Action reg: 0.003976
  l1.weight: grad_norm = 0.006398
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.016535
Total gradient norm: 0.031592
=== Actor Training Debug (Iteration 2410) ===
Q mean: -44.016235
Q std: 19.201843
Actor loss: 44.020222
Action reg: 0.003988
  l1.weight: grad_norm = 0.011112
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.031450
Total gradient norm: 0.054449
=== Actor Training Debug (Iteration 2411) ===
Q mean: -47.046871
Q std: 18.815933
Actor loss: 47.050842
Action reg: 0.003970
  l1.weight: grad_norm = 0.018516
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.046379
Total gradient norm: 0.096991
=== Actor Training Debug (Iteration 2412) ===
Q mean: -48.371655
Q std: 18.382353
Actor loss: 48.375633
Action reg: 0.003978
  l1.weight: grad_norm = 0.004786
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.014883
Total gradient norm: 0.028469
=== Actor Training Debug (Iteration 2413) ===
Q mean: -46.430130
Q std: 18.266054
Actor loss: 46.434105
Action reg: 0.003976
  l1.weight: grad_norm = 0.058542
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.144556
Total gradient norm: 0.257745
=== Actor Training Debug (Iteration 2414) ===
Q mean: -46.623798
Q std: 19.440609
Actor loss: 46.627792
Action reg: 0.003994
  l1.weight: grad_norm = 0.000220
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.000561
Total gradient norm: 0.001521
=== Actor Training Debug (Iteration 2415) ===
Q mean: -47.377769
Q std: 18.792593
Actor loss: 47.381763
Action reg: 0.003994
  l1.weight: grad_norm = 0.008896
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.023004
Total gradient norm: 0.046499
=== Actor Training Debug (Iteration 2416) ===
Q mean: -46.362610
Q std: 20.224749
Actor loss: 46.366566
Action reg: 0.003955
  l1.weight: grad_norm = 0.008502
  l1.bias: grad_norm = 0.000623
  l2.weight: grad_norm = 0.020291
Total gradient norm: 0.035355
=== Actor Training Debug (Iteration 2417) ===
Q mean: -46.697853
Q std: 17.393597
Actor loss: 46.701836
Action reg: 0.003984
  l1.weight: grad_norm = 0.005670
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.013957
Total gradient norm: 0.028375
=== Actor Training Debug (Iteration 2418) ===
Q mean: -45.991959
Q std: 16.902891
Actor loss: 45.995953
Action reg: 0.003993
  l1.weight: grad_norm = 0.108697
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.301422
Total gradient norm: 0.564296
=== Actor Training Debug (Iteration 2419) ===
Q mean: -45.809525
Q std: 19.219086
Actor loss: 45.813496
Action reg: 0.003973
  l1.weight: grad_norm = 0.004618
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.012933
Total gradient norm: 0.026263
=== Actor Training Debug (Iteration 2420) ===
Q mean: -47.338081
Q std: 18.060862
Actor loss: 47.342064
Action reg: 0.003981
  l1.weight: grad_norm = 0.000957
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.003295
Total gradient norm: 0.008666
=== Actor Training Debug (Iteration 2421) ===
Q mean: -44.930824
Q std: 17.712034
Actor loss: 44.934799
Action reg: 0.003973
  l1.weight: grad_norm = 0.011688
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.025562
Total gradient norm: 0.034048
=== Actor Training Debug (Iteration 2422) ===
Q mean: -44.390656
Q std: 18.182625
Actor loss: 44.394642
Action reg: 0.003985
  l1.weight: grad_norm = 0.082165
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.198879
Total gradient norm: 0.372847
=== Actor Training Debug (Iteration 2423) ===
Q mean: -48.376503
Q std: 18.606983
Actor loss: 48.380505
Action reg: 0.004000
  l1.weight: grad_norm = 0.003296
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.008939
Total gradient norm: 0.019552
=== Actor Training Debug (Iteration 2424) ===
Q mean: -45.747726
Q std: 18.122782
Actor loss: 45.751698
Action reg: 0.003970
  l1.weight: grad_norm = 0.081221
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.195745
Total gradient norm: 0.408315
=== Actor Training Debug (Iteration 2425) ===
Q mean: -45.713623
Q std: 17.920616
Actor loss: 45.717609
Action reg: 0.003986
  l1.weight: grad_norm = 0.030273
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.076438
Total gradient norm: 0.132020
Total gradient norm: 0.187571ration 1203) ===
=== Actor Training Debug (Iteration 2436) ===
Q mean: -44.598419
Q std: 18.317963
Actor loss: 44.602409
Action reg: 0.003991
  l1.weight: grad_norm = 0.001665
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.004843
Total gradient norm: 0.007478
=== Actor Training Debug (Iteration 2437) ===
Q mean: -44.708565
Q std: 19.634323
Actor loss: 44.712543
Action reg: 0.003980
  l1.weight: grad_norm = 0.000491
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.001765
Total gradient norm: 0.004882
=== Actor Training Debug (Iteration 2438) ===
Q mean: -49.164776
Q std: 18.659172
Actor loss: 49.168755
Action reg: 0.003979
  l1.weight: grad_norm = 0.000351
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.001468
Total gradient norm: 0.004606
=== Actor Training Debug (Iteration 2439) ===
Q mean: -46.298782
Q std: 18.008223
Actor loss: 46.302757
Action reg: 0.003976
  l1.weight: grad_norm = 0.010693
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.028732
Total gradient norm: 0.050344
=== Actor Training Debug (Iteration 2440) ===
Q mean: -45.539219
Q std: 17.312895
Actor loss: 45.543213
Action reg: 0.003995
  l1.weight: grad_norm = 0.009478
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.028004
Total gradient norm: 0.056369
=== Actor Training Debug (Iteration 2441) ===
Q mean: -46.970230
Q std: 18.055874
Actor loss: 46.974216
Action reg: 0.003986
  l1.weight: grad_norm = 0.006851
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.015786
Total gradient norm: 0.027338
=== Actor Training Debug (Iteration 2442) ===
Q mean: -48.180984
Q std: 18.542364
Actor loss: 48.184982
Action reg: 0.003998
  l1.weight: grad_norm = 0.017588
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.038647
Total gradient norm: 0.051288
=== Actor Training Debug (Iteration 2443) ===
Q mean: -45.531498
Q std: 18.897604
Actor loss: 45.535492
Action reg: 0.003994
  l1.weight: grad_norm = 0.000257
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.000723
Total gradient norm: 0.001809
=== Actor Training Debug (Iteration 2444) ===
Q mean: -43.695984
Q std: 17.952101
Actor loss: 43.699959
Action reg: 0.003974
  l1.weight: grad_norm = 0.095002
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.214294
Total gradient norm: 0.441631
=== Actor Training Debug (Iteration 2445) ===
Q mean: -47.377144
Q std: 18.141279
Actor loss: 47.381138
Action reg: 0.003995
  l1.weight: grad_norm = 0.000331
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.000952
Total gradient norm: 0.002243
=== Actor Training Debug (Iteration 2446) ===
Q mean: -46.828541
Q std: 18.155605
Actor loss: 46.832520
Action reg: 0.003980
  l1.weight: grad_norm = 0.019078
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.047188
Total gradient norm: 0.086221
=== Actor Training Debug (Iteration 2447) ===
Q mean: -46.785419
Q std: 19.457350
Actor loss: 46.789387
Action reg: 0.003967
  l1.weight: grad_norm = 0.000523
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.002556
Total gradient norm: 0.008485
=== Actor Training Debug (Iteration 2448) ===
Q mean: -47.848579
Q std: 18.908827
Actor loss: 47.852577
Action reg: 0.003999
  l1.weight: grad_norm = 0.003972
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.010791
Total gradient norm: 0.019916
=== Actor Training Debug (Iteration 2449) ===
Q mean: -46.263767
Q std: 18.850353
Actor loss: 46.267746
Action reg: 0.003980
  l1.weight: grad_norm = 0.015370
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.043743
Total gradient norm: 0.078246
=== Actor Training Debug (Iteration 2450) ===
Q mean: -44.684418
Q std: 18.835093
Actor loss: 44.688400
Action reg: 0.003982
  l1.weight: grad_norm = 0.037203
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.105242
Total gradient norm: 0.184541
=== Actor Training Debug (Iteration 2451) ===
Q mean: -44.866905
Q std: 18.344717
Actor loss: 44.870869
Action reg: 0.003965
  l1.weight: grad_norm = 0.029796
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.076650
Total gradient norm: 0.135106
=== Actor Training Debug (Iteration 2452) ===
Q mean: -47.671696
Q std: 19.212038
Actor loss: 47.675690
Action reg: 0.003994
  l1.weight: grad_norm = 0.002660
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.008269
Total gradient norm: 0.016055
=== Actor Training Debug (Iteration 2453) ===
Q mean: -48.440807
Q std: 17.955536
Actor loss: 48.444801
Action reg: 0.003994
  l1.weight: grad_norm = 0.057140
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.122785
Total gradient norm: 0.239087
=== Actor Training Debug (Iteration 2454) ===
Q mean: -42.929386
Q std: 17.509569
Actor loss: 42.933353
Action reg: 0.003969
  l1.weight: grad_norm = 0.069366
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.174428
Total gradient norm: 0.246809
=== Actor Training Debug (Iteration 2455) ===
Q mean: -44.981365
Q std: 19.067184
Actor loss: 44.985352
Action reg: 0.003988
  l1.weight: grad_norm = 0.048807
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.126360
Total gradient norm: 0.228799
=== Actor Training Debug (Iteration 2456) ===
Q mean: -47.097855
Q std: 19.895084
Actor loss: 47.101837
Action reg: 0.003981
  l1.weight: grad_norm = 0.018315
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.049903
Total gradient norm: 0.096369
=== Actor Training Debug (Iteration 2457) ===
Q mean: -46.246773
Q std: 19.058601
Actor loss: 46.250755
Action reg: 0.003981
  l1.weight: grad_norm = 0.005602
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.014898
Total gradient norm: 0.029516
=== Actor Training Debug (Iteration 2458) ===
Q mean: -45.659668
Q std: 17.388470
Actor loss: 45.663651
Action reg: 0.003983
  l1.weight: grad_norm = 0.071784
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.182257
Total gradient norm: 0.334740
=== Actor Training Debug (Iteration 2459) ===
Q mean: -50.426933
Q std: 19.262709
Actor loss: 50.430912
Action reg: 0.003978
  l1.weight: grad_norm = 0.000537
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.001791
Total gradient norm: 0.005167
=== Actor Training Debug (Iteration 2460) ===
Q mean: -47.273697
Q std: 18.166618
Actor loss: 47.277672
Action reg: 0.003976
  l1.weight: grad_norm = 0.038069
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.093325
Total gradient norm: 0.164408
=== Actor Training Debug (Iteration 2461) ===
Q mean: -43.512436
Q std: 17.870319
Actor loss: 43.516430
Action reg: 0.003994
  l1.weight: grad_norm = 0.017752
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.044699
Total gradient norm: 0.090232
=== Actor Training Debug (Iteration 2462) ===
Q mean: -45.010735
Q std: 18.730145
Actor loss: 45.014717
Action reg: 0.003984
  l1.weight: grad_norm = 0.010126
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.027435
Total gradient norm: 0.052928
=== Actor Training Debug (Iteration 2463) ===
Q mean: -47.398403
Q std: 19.437592
Actor loss: 47.402397
Action reg: 0.003993
  l1.weight: grad_norm = 0.015605
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.046996
Total gradient norm: 0.078362
=== Actor Training Debug (Iteration 2464) ===
Q mean: -47.263065
Q std: 17.499350
Actor loss: 47.267059
Action reg: 0.003993
  l1.weight: grad_norm = 0.001745
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.004597
Total gradient norm: 0.008126
=== Actor Training Debug (Iteration 2465) ===
Q mean: -47.948315
Q std: 16.587198
Actor loss: 47.952305
Action reg: 0.003989
  l1.weight: grad_norm = 0.096430
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.208400
Total gradient norm: 0.372918
=== Actor Training Debug (Iteration 2466) ===
Q mean: -46.466839
Q std: 17.156328
Actor loss: 46.470825
Action reg: 0.003985
  l1.weight: grad_norm = 0.103094
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.280107
Total gradient norm: 0.526077
=== Actor Training Debug (Iteration 2467) ===
Q mean: -48.129711
Q std: 19.686258
Actor loss: 48.133701
Action reg: 0.003990
  l1.weight: grad_norm = 0.047206
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.103531
Total gradient norm: 0.180761
=== Actor Training Debug (Iteration 2468) ===
Q mean: -46.334457
Q std: 17.924522
Actor loss: 46.338448
Action reg: 0.003991
  l1.weight: grad_norm = 0.017956
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.043042
Total gradient norm: 0.082207
=== Actor Training Debug (Iteration 2469) ===
Q mean: -45.550262
Q std: 18.199223
Actor loss: 45.554245
Action reg: 0.003983
  l1.weight: grad_norm = 0.033476
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.085544
Total gradient norm: 0.155474
=== Actor Training Debug (Iteration 2470) ===
Q mean: -47.076012
Q std: 18.971075
Actor loss: 47.079994
Action reg: 0.003982
  l1.weight: grad_norm = 0.003714
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.011798
Total gradient norm: 0.023041
=== Actor Training Debug (Iteration 2471) ===
Q mean: -46.410873
Q std: 19.099163
Actor loss: 46.414829
Action reg: 0.003957
  l1.weight: grad_norm = 0.030717
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.081990
Total gradient norm: 0.161409
=== Actor Training Debug (Iteration 2472) ===
Q mean: -47.809921
Q std: 18.632404
Actor loss: 47.813911
Action reg: 0.003991
  l1.weight: grad_norm = 0.070897
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.193485
Total gradient norm: 0.426312
=== Actor Training Debug (Iteration 2473) ===
Q mean: -48.072746
Q std: 17.803467
Actor loss: 48.076733
Action reg: 0.003986
  l1.weight: grad_norm = 0.004965
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.015059
Total gradient norm: 0.026787
=== Actor Training Debug (Iteration 2474) ===
Q mean: -45.878239
Q std: 19.001907
Actor loss: 45.882217
Action reg: 0.003978
  l1.weight: grad_norm = 0.007829
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.020564
Total gradient norm: 0.036961
=== Actor Training Debug (Iteration 2475) ===
Q mean: -48.978573
Q std: 18.620474
Actor loss: 48.982555
Action reg: 0.003984
  l1.weight: grad_norm = 0.024479
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.071948
Total gradient norm: 0.139399
=== Actor Training Debug (Iteration 2476) ===
Q mean: -46.217785
Q std: 16.667231
Actor loss: 46.221779
Action reg: 0.003994
  l1.weight: grad_norm = 0.005743
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.015002
Total gradient norm: 0.028571
=== Actor Training Debug (Iteration 2477) ===
Q mean: -46.559910
Q std: 17.864864
Actor loss: 46.563896
Action reg: 0.003987
  l1.weight: grad_norm = 0.000770
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.002149
Total gradient norm: 0.003729
=== Actor Training Debug (Iteration 2478) ===
Q mean: -48.315735
Q std: 17.874739
Actor loss: 48.319721
Action reg: 0.003986
  l1.weight: grad_norm = 0.011844
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.036823
Total gradient norm: 0.080895
=== Actor Training Debug (Iteration 2479) ===
Q mean: -47.984482
Q std: 18.530430
Actor loss: 47.988472
Action reg: 0.003989
  l1.weight: grad_norm = 0.003236
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.010356
Total gradient norm: 0.023216
=== Actor Training Debug (Iteration 2480) ===
Q mean: -46.708637
Q std: 20.184980
Actor loss: 46.712620
Action reg: 0.003984
  l1.weight: grad_norm = 0.001193
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.002859
Total gradient norm: 0.004918
=== Actor Training Debug (Iteration 2481) ===
Q mean: -46.627953
Q std: 18.270552
Actor loss: 46.631947
Action reg: 0.003992
  l1.weight: grad_norm = 0.046558
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.107924
Total gradient norm: 0.204680
=== Actor Training Debug (Iteration 2482) ===
Q mean: -46.753693
Q std: 18.929506
Actor loss: 46.757671
Action reg: 0.003978
  l1.weight: grad_norm = 0.002916
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.007158
Total gradient norm: 0.010552
=== Actor Training Debug (Iteration 2483) ===
Q mean: -48.695755
Q std: 16.769800
Actor loss: 48.699734
Action reg: 0.003980
  l1.weight: grad_norm = 0.005600
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.014959
Total gradient norm: 0.028122
=== Actor Training Debug (Iteration 2484) ===
Q mean: -48.627289
Q std: 18.514864
Actor loss: 48.631279
Action reg: 0.003989
  l1.weight: grad_norm = 0.001702
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.004130
Total gradient norm: 0.007453
=== Actor Training Debug (Iteration 2485) ===
Q mean: -47.178562
Q std: 19.916502
Actor loss: 47.182552
Action reg: 0.003991
  l1.weight: grad_norm = 0.000458
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.001234
Total gradient norm: 0.002571
=== Actor Training Debug (Iteration 2486) ===
Q mean: -47.140842
Q std: 18.818485
Actor loss: 47.144836
Action reg: 0.003993
  l1.weight: grad_norm = 0.003240
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.008762
Total gradient norm: 0.016556
=== Actor Training Debug (Iteration 2487) ===
Q mean: -47.206902
Q std: 18.669609
Actor loss: 47.210892
Action reg: 0.003991
  l1.weight: grad_norm = 0.021235
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.054340
Total gradient norm: 0.124873
Total gradient norm: 0.042559ration 1203) ===
=== Actor Training Debug (Iteration 2498) ===
Q mean: -47.293846
Q std: 18.828758
Actor loss: 47.297821
Action reg: 0.003977
  l1.weight: grad_norm = 0.061860
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.160131
Total gradient norm: 0.333451
=== Actor Training Debug (Iteration 2499) ===
Q mean: -44.195007
Q std: 18.686012
Actor loss: 44.198986
Action reg: 0.003977
  l1.weight: grad_norm = 0.000579
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.002420
Total gradient norm: 0.007458
=== Actor Training Debug (Iteration 2500) ===
Q mean: -48.175240
Q std: 19.054659
Actor loss: 48.179207
Action reg: 0.003969
  l1.weight: grad_norm = 0.066885
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.171398
Total gradient norm: 0.292081
  Average reward: -361.173 | Average length: 100.0
Evaluation at episode 75: -361.173
=== Actor Training Debug (Iteration 2501) ===
Q mean: -48.915260
Q std: 18.571629
Actor loss: 48.919243
Action reg: 0.003984
  l1.weight: grad_norm = 0.016228
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.042498
Total gradient norm: 0.079675
=== Actor Training Debug (Iteration 2502) ===
Q mean: -47.565331
Q std: 18.200851
Actor loss: 47.569317
Action reg: 0.003985
  l1.weight: grad_norm = 0.000394
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.001388
Total gradient norm: 0.004196
=== Actor Training Debug (Iteration 2503) ===
Q mean: -46.708733
Q std: 17.075586
Actor loss: 46.712719
Action reg: 0.003988
  l1.weight: grad_norm = 0.079069
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.195489
Total gradient norm: 0.428639
=== Actor Training Debug (Iteration 2504) ===
Q mean: -47.646271
Q std: 18.189533
Actor loss: 47.650257
Action reg: 0.003985
  l1.weight: grad_norm = 0.139389
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.342434
Total gradient norm: 0.586393
=== Actor Training Debug (Iteration 2505) ===
Q mean: -49.444981
Q std: 20.370893
Actor loss: 49.448956
Action reg: 0.003975
  l1.weight: grad_norm = 0.000680
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.001867
Total gradient norm: 0.004561
=== Actor Training Debug (Iteration 2506) ===
Q mean: -47.125938
Q std: 18.866043
Actor loss: 47.129921
Action reg: 0.003982
  l1.weight: grad_norm = 0.089037
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.218134
Total gradient norm: 0.421468
=== Actor Training Debug (Iteration 2507) ===
Q mean: -44.648827
Q std: 17.637291
Actor loss: 44.652813
Action reg: 0.003986
  l1.weight: grad_norm = 0.010218
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.025035
Total gradient norm: 0.051416
=== Actor Training Debug (Iteration 2508) ===
Q mean: -48.011803
Q std: 18.699612
Actor loss: 48.015774
Action reg: 0.003971
  l1.weight: grad_norm = 0.025876
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.071009
Total gradient norm: 0.134300
=== Actor Training Debug (Iteration 2509) ===
Q mean: -47.558075
Q std: 17.122030
Actor loss: 47.562061
Action reg: 0.003988
  l1.weight: grad_norm = 0.000868
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.001945
Total gradient norm: 0.003346
=== Actor Training Debug (Iteration 2510) ===
Q mean: -47.994713
Q std: 18.791397
Actor loss: 47.998695
Action reg: 0.003981
  l1.weight: grad_norm = 0.017061
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.040574
Total gradient norm: 0.087220
=== Actor Training Debug (Iteration 2511) ===
Q mean: -48.345314
Q std: 18.070042
Actor loss: 48.349297
Action reg: 0.003982
  l1.weight: grad_norm = 0.015571
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.037258
Total gradient norm: 0.067741
=== Actor Training Debug (Iteration 2512) ===
Q mean: -50.025131
Q std: 18.990816
Actor loss: 50.029114
Action reg: 0.003984
  l1.weight: grad_norm = 0.000308
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.000877
Total gradient norm: 0.002549
=== Actor Training Debug (Iteration 2513) ===
Q mean: -45.930893
Q std: 19.240784
Actor loss: 45.934856
Action reg: 0.003963
  l1.weight: grad_norm = 0.000732
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.002650
Total gradient norm: 0.007504
=== Actor Training Debug (Iteration 2514) ===
Q mean: -46.745766
Q std: 17.210079
Actor loss: 46.749752
Action reg: 0.003987
  l1.weight: grad_norm = 0.050869
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.115632
Total gradient norm: 0.221302
=== Actor Training Debug (Iteration 2515) ===
Q mean: -47.600563
Q std: 19.463675
Actor loss: 47.604534
Action reg: 0.003972
  l1.weight: grad_norm = 0.012103
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.027103
Total gradient norm: 0.044626
=== Actor Training Debug (Iteration 2516) ===
Q mean: -49.870468
Q std: 16.223951
Actor loss: 49.874470
Action reg: 0.004000
  l1.weight: grad_norm = 0.000355
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000946
Total gradient norm: 0.001832
=== Actor Training Debug (Iteration 2517) ===
Q mean: -44.895050
Q std: 17.136776
Actor loss: 44.899029
Action reg: 0.003978
  l1.weight: grad_norm = 0.009431
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.022779
Total gradient norm: 0.046142
=== Actor Training Debug (Iteration 2518) ===
Q mean: -46.370224
Q std: 20.172985
Actor loss: 46.374203
Action reg: 0.003980
  l1.weight: grad_norm = 0.000440
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.001831
Total gradient norm: 0.005652
=== Actor Training Debug (Iteration 2519) ===
Q mean: -48.694275
Q std: 19.363779
Actor loss: 48.698269
Action reg: 0.003995
  l1.weight: grad_norm = 0.000407
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.001027
Total gradient norm: 0.002354
=== Actor Training Debug (Iteration 2520) ===
Q mean: -46.624828
Q std: 18.919733
Actor loss: 46.628815
Action reg: 0.003988
  l1.weight: grad_norm = 0.000260
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.000785
Total gradient norm: 0.002114
=== Actor Training Debug (Iteration 2521) ===
Q mean: -47.610836
Q std: 19.032322
Actor loss: 47.614822
Action reg: 0.003986
  l1.weight: grad_norm = 0.035256
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.093140
Total gradient norm: 0.190858
=== Actor Training Debug (Iteration 2522) ===
Q mean: -46.027805
Q std: 16.988289
Actor loss: 46.031796
Action reg: 0.003989
  l1.weight: grad_norm = 0.076028
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.192045
Total gradient norm: 0.352547
=== Actor Training Debug (Iteration 2523) ===
Q mean: -45.544907
Q std: 17.687092
Actor loss: 45.548882
Action reg: 0.003973
  l1.weight: grad_norm = 0.004711
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.011335
Total gradient norm: 0.016491
=== Actor Training Debug (Iteration 2524) ===
Q mean: -48.507881
Q std: 17.231647
Actor loss: 48.511864
Action reg: 0.003984
  l1.weight: grad_norm = 0.000388
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.001303
Total gradient norm: 0.003849
=== Actor Training Debug (Iteration 2525) ===
Q mean: -46.762802
Q std: 19.820230
Actor loss: 46.766788
Action reg: 0.003988
  l1.weight: grad_norm = 0.019465
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.052070
Total gradient norm: 0.113582
=== Actor Training Debug (Iteration 2526) ===
Q mean: -45.079891
Q std: 20.278011
Actor loss: 45.083874
Action reg: 0.003983
  l1.weight: grad_norm = 0.022170
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.053027
Total gradient norm: 0.096102
=== Actor Training Debug (Iteration 2527) ===
Q mean: -46.755253
Q std: 17.569395
Actor loss: 46.759247
Action reg: 0.003995
  l1.weight: grad_norm = 0.001603
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.003968
Total gradient norm: 0.007199
=== Actor Training Debug (Iteration 2528) ===
Q mean: -49.499229
Q std: 18.627466
Actor loss: 49.503204
Action reg: 0.003975
  l1.weight: grad_norm = 0.002972
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.007608
Total gradient norm: 0.011937
=== Actor Training Debug (Iteration 2529) ===
Q mean: -50.613037
Q std: 16.751467
Actor loss: 50.617027
Action reg: 0.003991
  l1.weight: grad_norm = 0.005606
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.012641
Total gradient norm: 0.025835
=== Actor Training Debug (Iteration 2530) ===
Q mean: -46.977348
Q std: 16.134369
Actor loss: 46.981335
Action reg: 0.003987
  l1.weight: grad_norm = 0.085270
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.250094
Total gradient norm: 0.440843
=== Actor Training Debug (Iteration 2531) ===
Q mean: -45.580669
Q std: 17.888184
Actor loss: 45.584656
Action reg: 0.003985
  l1.weight: grad_norm = 0.004188
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.009634
Total gradient norm: 0.013878
=== Actor Training Debug (Iteration 2532) ===
Q mean: -42.856441
Q std: 19.028805
Actor loss: 42.860432
Action reg: 0.003990
  l1.weight: grad_norm = 0.000801
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.002426
Total gradient norm: 0.005965
=== Actor Training Debug (Iteration 2533) ===
Q mean: -48.006119
Q std: 19.050369
Actor loss: 48.010109
Action reg: 0.003990
  l1.weight: grad_norm = 0.000699
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.001879
Total gradient norm: 0.004843
=== Actor Training Debug (Iteration 2534) ===
Q mean: -50.073112
Q std: 17.707045
Actor loss: 50.077099
Action reg: 0.003988
  l1.weight: grad_norm = 0.022641
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.062468
Total gradient norm: 0.115490
=== Actor Training Debug (Iteration 2535) ===
Q mean: -46.499146
Q std: 17.325102
Actor loss: 46.503132
Action reg: 0.003988
  l1.weight: grad_norm = 0.016644
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.042113
Total gradient norm: 0.057182
=== Actor Training Debug (Iteration 2536) ===
Q mean: -43.152073
Q std: 17.552835
Actor loss: 43.156063
Action reg: 0.003992
  l1.weight: grad_norm = 0.053802
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.121923
Total gradient norm: 0.213169
=== Actor Training Debug (Iteration 2537) ===
Q mean: -45.711536
Q std: 17.991211
Actor loss: 45.715527
Action reg: 0.003989
  l1.weight: grad_norm = 0.001017
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.002502
Total gradient norm: 0.003923
=== Actor Training Debug (Iteration 2538) ===
Q mean: -46.449287
Q std: 18.150627
Actor loss: 46.453270
Action reg: 0.003982
  l1.weight: grad_norm = 0.003570
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.010685
Total gradient norm: 0.021368
=== Actor Training Debug (Iteration 2539) ===
Q mean: -48.016994
Q std: 18.336546
Q std: 17.864641orm: 0.042559ration 1203) ===
Actor loss: 49.575237
Action reg: 0.003979
  l1.weight: grad_norm = 0.085329
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.214004
Total gradient norm: 0.447603
=== Actor Training Debug (Iteration 2550) ===
Q mean: -47.298519
Q std: 17.610439
Actor loss: 47.302509
Action reg: 0.003992
  l1.weight: grad_norm = 0.029558
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.076730
Total gradient norm: 0.139111
=== Actor Training Debug (Iteration 2551) ===
Q mean: -46.210945
Q std: 17.514036
Actor loss: 46.214939
Action reg: 0.003993
  l1.weight: grad_norm = 0.017524
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.043248
Total gradient norm: 0.078368
=== Actor Training Debug (Iteration 2552) ===
Q mean: -46.756283
Q std: 19.071260
Actor loss: 46.760262
Action reg: 0.003978
  l1.weight: grad_norm = 0.018358
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.038067
Total gradient norm: 0.052419
=== Actor Training Debug (Iteration 2553) ===
Q mean: -48.380070
Q std: 16.916073
Actor loss: 48.384052
Action reg: 0.003983
  l1.weight: grad_norm = 0.013649
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.029932
Total gradient norm: 0.061134
=== Actor Training Debug (Iteration 2554) ===
Q mean: -48.309143
Q std: 17.290205
Actor loss: 48.313122
Action reg: 0.003979
  l1.weight: grad_norm = 0.016014
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.038404
Total gradient norm: 0.086647
=== Actor Training Debug (Iteration 2555) ===
Q mean: -48.719379
Q std: 17.896364
Actor loss: 48.723377
Action reg: 0.003996
  l1.weight: grad_norm = 0.000363
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.000859
Total gradient norm: 0.001561
=== Actor Training Debug (Iteration 2556) ===
Q mean: -45.178799
Q std: 17.594559
Actor loss: 45.182793
Action reg: 0.003994
  l1.weight: grad_norm = 0.001450
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.003766
Total gradient norm: 0.006518
=== Actor Training Debug (Iteration 2557) ===
Q mean: -47.131889
Q std: 17.729565
Actor loss: 47.135868
Action reg: 0.003980
  l1.weight: grad_norm = 0.043894
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.127089
Total gradient norm: 0.225011
=== Actor Training Debug (Iteration 2558) ===
Q mean: -49.586536
Q std: 18.513412
Actor loss: 49.590527
Action reg: 0.003989
  l1.weight: grad_norm = 0.000666
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.001715
Total gradient norm: 0.003255
=== Actor Training Debug (Iteration 2559) ===
Q mean: -46.637642
Q std: 17.665670
Actor loss: 46.641636
Action reg: 0.003993
  l1.weight: grad_norm = 0.006137
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.016581
Total gradient norm: 0.037866
=== Actor Training Debug (Iteration 2560) ===
Q mean: -46.662472
Q std: 17.363256
Actor loss: 46.666462
Action reg: 0.003990
  l1.weight: grad_norm = 0.004080
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.010280
Total gradient norm: 0.022567
=== Actor Training Debug (Iteration 2561) ===
Q mean: -47.922112
Q std: 17.218397
Actor loss: 47.926086
Action reg: 0.003975
  l1.weight: grad_norm = 0.009784
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.022547
Total gradient norm: 0.042370
=== Actor Training Debug (Iteration 2562) ===
Q mean: -47.834530
Q std: 19.098856
Actor loss: 47.838520
Action reg: 0.003991
  l1.weight: grad_norm = 0.047689
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.114806
Total gradient norm: 0.190198
=== Actor Training Debug (Iteration 2563) ===
Q mean: -46.481316
Q std: 18.011053
Actor loss: 46.485313
Action reg: 0.003998
  l1.weight: grad_norm = 0.134925
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.387143
Total gradient norm: 0.689998
=== Actor Training Debug (Iteration 2564) ===
Q mean: -48.320610
Q std: 17.287220
Actor loss: 48.324585
Action reg: 0.003976
  l1.weight: grad_norm = 0.121208
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.346992
Total gradient norm: 0.796541
=== Actor Training Debug (Iteration 2565) ===
Q mean: -48.219440
Q std: 18.969528
Actor loss: 48.223419
Action reg: 0.003981
  l1.weight: grad_norm = 0.000348
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.001318
Total gradient norm: 0.003905
=== Actor Training Debug (Iteration 2566) ===
Q mean: -48.549660
Q std: 17.340172
Actor loss: 48.553642
Action reg: 0.003983
  l1.weight: grad_norm = 0.000280
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.001080
Total gradient norm: 0.003242
=== Actor Training Debug (Iteration 2567) ===
Q mean: -49.086735
Q std: 18.316116
Actor loss: 49.090725
Action reg: 0.003989
  l1.weight: grad_norm = 0.049284
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.118677
Total gradient norm: 0.251483
=== Actor Training Debug (Iteration 2568) ===
Q mean: -47.044891
Q std: 17.472887
Actor loss: 47.048874
Action reg: 0.003982
  l1.weight: grad_norm = 0.022205
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.050296
Total gradient norm: 0.104235
=== Actor Training Debug (Iteration 2569) ===
Q mean: -45.532600
Q std: 17.233683
Actor loss: 45.536579
Action reg: 0.003977
  l1.weight: grad_norm = 0.096634
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.298188
Total gradient norm: 0.610183
=== Actor Training Debug (Iteration 2570) ===
Q mean: -46.749496
Q std: 18.133652
Actor loss: 46.753483
Action reg: 0.003987
  l1.weight: grad_norm = 0.042656
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.115455
Total gradient norm: 0.264867
=== Actor Training Debug (Iteration 2571) ===
Q mean: -47.609810
Q std: 17.424158
Actor loss: 47.613796
Action reg: 0.003986
  l1.weight: grad_norm = 0.000462
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.001874
Total gradient norm: 0.005702
=== Actor Training Debug (Iteration 2572) ===
Q mean: -47.887814
Q std: 18.383965
Actor loss: 47.891800
Action reg: 0.003986
  l1.weight: grad_norm = 0.024044
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.064928
Total gradient norm: 0.112874
=== Actor Training Debug (Iteration 2573) ===
Q mean: -49.367065
Q std: 18.261696
Actor loss: 49.371048
Action reg: 0.003984
  l1.weight: grad_norm = 0.004154
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.011272
Total gradient norm: 0.018655
=== Actor Training Debug (Iteration 2574) ===
Q mean: -49.642105
Q std: 18.503330
Actor loss: 49.646091
Action reg: 0.003985
  l1.weight: grad_norm = 0.002476
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.005803
Total gradient norm: 0.009263
=== Actor Training Debug (Iteration 2575) ===
Q mean: -47.554218
Q std: 16.688831
Actor loss: 47.558212
Action reg: 0.003995
  l1.weight: grad_norm = 0.079533
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.159296
Total gradient norm: 0.245750
=== Actor Training Debug (Iteration 2576) ===
Q mean: -46.116798
Q std: 18.497105
Actor loss: 46.120766
Action reg: 0.003968
  l1.weight: grad_norm = 0.000416
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.001887
Total gradient norm: 0.005820
=== Actor Training Debug (Iteration 2577) ===
Q mean: -48.595142
Q std: 19.771238
Actor loss: 48.599121
Action reg: 0.003979
  l1.weight: grad_norm = 0.027769
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.065043
Total gradient norm: 0.146083
=== Actor Training Debug (Iteration 2578) ===
Q mean: -51.972389
Q std: 19.255444
Actor loss: 51.976376
Action reg: 0.003987
  l1.weight: grad_norm = 0.006505
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.016086
Total gradient norm: 0.034599
=== Actor Training Debug (Iteration 2579) ===
Q mean: -49.277821
Q std: 17.940193
Actor loss: 49.281799
Action reg: 0.003977
  l1.weight: grad_norm = 0.084239
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.226894
Total gradient norm: 0.390373
=== Actor Training Debug (Iteration 2580) ===
Q mean: -45.310860
Q std: 17.939884
Actor loss: 45.314838
Action reg: 0.003978
  l1.weight: grad_norm = 0.080015
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.206102
Total gradient norm: 0.385654
=== Actor Training Debug (Iteration 2581) ===
Q mean: -43.584660
Q std: 17.602015
Actor loss: 43.588638
Action reg: 0.003979
  l1.weight: grad_norm = 0.000656
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.001813
Total gradient norm: 0.003947
=== Actor Training Debug (Iteration 2582) ===
Q mean: -47.733635
Q std: 17.050598
Actor loss: 47.737625
Action reg: 0.003990
  l1.weight: grad_norm = 0.004151
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.010162
Total gradient norm: 0.018826
=== Actor Training Debug (Iteration 2583) ===
Q mean: -49.485039
Q std: 18.739794
Actor loss: 49.489017
Action reg: 0.003977
  l1.weight: grad_norm = 0.020325
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.057650
Total gradient norm: 0.109139
=== Actor Training Debug (Iteration 2584) ===
Q mean: -49.326267
Q std: 17.898493
Actor loss: 49.330265
Action reg: 0.003999
  l1.weight: grad_norm = 0.005419
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.011842
Total gradient norm: 0.017254
=== Actor Training Debug (Iteration 2585) ===
Q mean: -46.675835
Q std: 18.721539
Actor loss: 46.679817
Action reg: 0.003984
  l1.weight: grad_norm = 0.002149
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.005944
Total gradient norm: 0.014294
=== Actor Training Debug (Iteration 2586) ===
Q mean: -44.897648
Q std: 18.227137
Actor loss: 44.901627
Action reg: 0.003977
  l1.weight: grad_norm = 0.039140
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.112725
Total gradient norm: 0.210369
=== Actor Training Debug (Iteration 2587) ===
Q mean: -47.995594
Q std: 19.427759
Actor loss: 47.999592
Action reg: 0.003999
  l1.weight: grad_norm = 0.012359
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.030148
Total gradient norm: 0.066125
=== Actor Training Debug (Iteration 2588) ===
Q mean: -50.099319
Q std: 20.490730
Actor loss: 50.103306
Action reg: 0.003986
  l1.weight: grad_norm = 0.008118
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.019372
Total gradient norm: 0.040410
=== Actor Training Debug (Iteration 2589) ===
Q mean: -49.844444
Q std: 19.077433
Actor loss: 49.848427
Action reg: 0.003984
  l1.weight: grad_norm = 0.023411
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.069127
Total gradient norm: 0.140604
=== Actor Training Debug (Iteration 2590) ===
Q mean: -46.586662
Q std: 18.177048
Actor loss: 46.590637
Action reg: 0.003975
  l1.weight: grad_norm = 0.007051
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.019348
Total gradient norm: 0.039036
=== Actor Training Debug (Iteration 2591) ===
Q mean: -46.571087
Q std: 19.059437
Actor loss: 46.575050
Action reg: 0.003962
  l1.weight: grad_norm = 0.063674
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.198449
Total gradient norm: 0.414455
=== Actor Training Debug (Iteration 2592) ===
Q mean: -46.873135
Q std: 17.871752
Actor loss: 46.877129
Action reg: 0.003996
  l1.weight: grad_norm = 0.003527
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.008570
Total gradient norm: 0.016463
=== Actor Training Debug (Iteration 2593) ===
Q mean: -50.466789
Q std: 19.568466
Actor loss: 50.470776
Action reg: 0.003985
  l1.weight: grad_norm = 0.000327
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.001251
Total gradient norm: 0.003736
=== Actor Training Debug (Iteration 2594) ===
Q mean: -47.704956
Q std: 18.409687
Actor loss: 47.708931
Action reg: 0.003975
  l1.weight: grad_norm = 0.018454
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.044518
Total gradient norm: 0.095588
=== Actor Training Debug (Iteration 2595) ===
Q mean: -47.523132
Q std: 17.174271
Actor loss: 47.527126
Action reg: 0.003995
  l1.weight: grad_norm = 0.001315
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.003754
Total gradient norm: 0.006593
=== Actor Training Debug (Iteration 2596) ===
Q mean: -46.820297
Q std: 18.323879
Actor loss: 46.824265
Action reg: 0.003968
  l1.weight: grad_norm = 0.111906
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.325550
Total gradient norm: 0.583896
=== Actor Training Debug (Iteration 2597) ===
Q mean: -49.554089
Q std: 18.498367
Actor loss: 49.558083
Action reg: 0.003996
  l1.weight: grad_norm = 0.029683
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.071119
Total gradient norm: 0.131111
=== Actor Training Debug (Iteration 2598) ===
Q mean: -47.238831
Q std: 19.544926
Actor loss: 47.242809
Action reg: 0.003980
  l1.weight: grad_norm = 0.005206
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.012997
Total gradient norm: 0.027873
=== Actor Training Debug (Iteration 2599) ===
Q mean: -48.048233
Q std: 18.390345
Actor loss: 48.052223
Action reg: 0.003989
  l1.weight: grad_norm = 0.003489
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.007827
Total gradient norm: 0.016022
=== Actor Training Debug (Iteration 2600) ===
Q mean: -49.030277
Q std: 17.512054
Actor loss: 49.034267
Action reg: 0.003992
  l1.weight: grad_norm = 0.018064
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.039622
Total gradient norm: 0.057159
=== Actor Training Debug (Iteration 2601) ===
Q mean: -48.951706
Q std: 18.833364
Actor loss: 48.955692
Action reg: 0.003985
  l1.weight: grad_norm = 0.016707
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.045963
Total gradient norm: 0.081298
=== Actor Training Debug (Iteration 2602) ===
Q mean: -48.895691
Q std: 19.734133
Actor loss: 48.899666
Action reg: 0.003977
  l1.weight: grad_norm = 0.050563
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.126782
Total gradient norm: 0.209837
=== Actor Training Debug (Iteration 2603) ===
Q mean: -49.834709
Q std: 17.259663
Actor loss: 49.838699
Action reg: 0.003989
  l1.weight: grad_norm = 0.020045
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.054423
Total gradient norm: 0.103179
=== Actor Training Debug (Iteration 2604) ===
Q mean: -45.185997
Q std: 17.717194
Actor loss: 45.189983
Action reg: 0.003988
  l1.weight: grad_norm = 0.005245
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.014332
Total gradient norm: 0.025968
=== Actor Training Debug (Iteration 2605) ===
Q mean: -48.886482
Q std: 19.649689
Actor loss: 48.890465
Action reg: 0.003983
  l1.weight: grad_norm = 0.000360
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.001435
Total gradient norm: 0.004434
Total gradient norm: 0.044154ration 1203) ===
=== Actor Training Debug (Iteration 2616) ===
Q mean: -50.779224
Q std: 19.492796
Actor loss: 50.783207
Action reg: 0.003983
  l1.weight: grad_norm = 0.044447
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.120882
Total gradient norm: 0.188372
=== Actor Training Debug (Iteration 2617) ===
Q mean: -50.568657
Q std: 19.483982
Actor loss: 50.572636
Action reg: 0.003977
  l1.weight: grad_norm = 0.000555
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.002335
Total gradient norm: 0.007088
=== Actor Training Debug (Iteration 2618) ===
Q mean: -45.715012
Q std: 16.765932
Actor loss: 45.719002
Action reg: 0.003990
  l1.weight: grad_norm = 0.000620
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.001412
Total gradient norm: 0.002591
=== Actor Training Debug (Iteration 2619) ===
Q mean: -48.739529
Q std: 19.151779
Actor loss: 48.743492
Action reg: 0.003965
  l1.weight: grad_norm = 0.018874
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.048521
Total gradient norm: 0.090042
=== Actor Training Debug (Iteration 2620) ===
Q mean: -48.930954
Q std: 17.733131
Actor loss: 48.934952
Action reg: 0.003997
  l1.weight: grad_norm = 0.011114
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.029463
Total gradient norm: 0.054271
=== Actor Training Debug (Iteration 2621) ===
Q mean: -46.822327
Q std: 17.584803
Actor loss: 46.826298
Action reg: 0.003971
  l1.weight: grad_norm = 0.152216
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.362739
Total gradient norm: 0.678367
=== Actor Training Debug (Iteration 2622) ===
Q mean: -46.388561
Q std: 18.949768
Actor loss: 46.392536
Action reg: 0.003976
  l1.weight: grad_norm = 0.000831
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.002011
Total gradient norm: 0.004573
=== Actor Training Debug (Iteration 2623) ===
Q mean: -48.401985
Q std: 17.512770
Actor loss: 48.405968
Action reg: 0.003981
  l1.weight: grad_norm = 0.000679
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.001736
Total gradient norm: 0.003503
=== Actor Training Debug (Iteration 2624) ===
Q mean: -49.820091
Q std: 17.076271
Actor loss: 49.824066
Action reg: 0.003976
  l1.weight: grad_norm = 0.018088
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.041799
Total gradient norm: 0.092898
=== Actor Training Debug (Iteration 2625) ===
Q mean: -49.498322
Q std: 16.253630
Actor loss: 49.502316
Action reg: 0.003994
  l1.weight: grad_norm = 0.001126
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.003401
Total gradient norm: 0.006487
=== Actor Training Debug (Iteration 2626) ===
Q mean: -50.015415
Q std: 18.417233
Actor loss: 50.019398
Action reg: 0.003983
  l1.weight: grad_norm = 0.043024
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.124675
Total gradient norm: 0.227843
=== Actor Training Debug (Iteration 2627) ===
Q mean: -47.320366
Q std: 17.922520
Actor loss: 47.324352
Action reg: 0.003985
  l1.weight: grad_norm = 0.007526
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.020644
Total gradient norm: 0.039758
=== Actor Training Debug (Iteration 2628) ===
Q mean: -49.111343
Q std: 19.092543
Actor loss: 49.115311
Action reg: 0.003969
  l1.weight: grad_norm = 0.090763
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.277893
Total gradient norm: 0.568632
=== Actor Training Debug (Iteration 2629) ===
Q mean: -47.784195
Q std: 19.155949
Actor loss: 47.788181
Action reg: 0.003988
  l1.weight: grad_norm = 0.034910
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.074896
Total gradient norm: 0.111564
=== Actor Training Debug (Iteration 2630) ===
Q mean: -49.897942
Q std: 19.141155
Actor loss: 49.901932
Action reg: 0.003990
  l1.weight: grad_norm = 0.002044
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.005113
Total gradient norm: 0.009211
=== Actor Training Debug (Iteration 2631) ===
Q mean: -47.165009
Q std: 18.807297
Actor loss: 47.168972
Action reg: 0.003965
  l1.weight: grad_norm = 0.031803
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.081847
Total gradient norm: 0.160212
=== Actor Training Debug (Iteration 2632) ===
Q mean: -47.292576
Q std: 17.437628
Actor loss: 47.296555
Action reg: 0.003980
  l1.weight: grad_norm = 0.000322
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.001369
Total gradient norm: 0.004229
=== Actor Training Debug (Iteration 2633) ===
Q mean: -49.187557
Q std: 18.934000
Actor loss: 49.191525
Action reg: 0.003968
  l1.weight: grad_norm = 0.019818
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.053646
Total gradient norm: 0.116920
=== Actor Training Debug (Iteration 2634) ===
Q mean: -49.982822
Q std: 19.817127
Actor loss: 49.986813
Action reg: 0.003989
  l1.weight: grad_norm = 0.116382
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.288638
Total gradient norm: 0.522873
=== Actor Training Debug (Iteration 2635) ===
Q mean: -47.293900
Q std: 17.842600
Actor loss: 47.297890
Action reg: 0.003990
  l1.weight: grad_norm = 0.028672
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.073951
Total gradient norm: 0.133098
=== Actor Training Debug (Iteration 2636) ===
Q mean: -49.737350
Q std: 20.949158
Actor loss: 49.741322
Action reg: 0.003971
  l1.weight: grad_norm = 0.010704
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.030628
Total gradient norm: 0.064734
=== Actor Training Debug (Iteration 2637) ===
Q mean: -49.585457
Q std: 19.011662
Actor loss: 49.589439
Action reg: 0.003983
  l1.weight: grad_norm = 0.027444
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.076819
Total gradient norm: 0.166265
=== Actor Training Debug (Iteration 2638) ===
Q mean: -48.718540
Q std: 17.897898
Actor loss: 48.722530
Action reg: 0.003992
  l1.weight: grad_norm = 0.098509
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.269160
Total gradient norm: 0.510928
=== Actor Training Debug (Iteration 2639) ===
Q mean: -48.607975
Q std: 19.868774
Actor loss: 48.611954
Action reg: 0.003980
  l1.weight: grad_norm = 0.002830
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.007197
Total gradient norm: 0.011017
=== Actor Training Debug (Iteration 2640) ===
Q mean: -48.043045
Q std: 18.430435
Actor loss: 48.047031
Action reg: 0.003985
  l1.weight: grad_norm = 0.006161
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.016448
Total gradient norm: 0.025839
=== Actor Training Debug (Iteration 2641) ===
Q mean: -47.521637
Q std: 16.812862
Actor loss: 47.525627
Action reg: 0.003992
  l1.weight: grad_norm = 0.044538
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.129503
Total gradient norm: 0.264571
=== Actor Training Debug (Iteration 2642) ===
Q mean: -47.733929
Q std: 17.791891
Actor loss: 47.737911
Action reg: 0.003983
  l1.weight: grad_norm = 0.002188
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.006238
Total gradient norm: 0.012855
=== Actor Training Debug (Iteration 2643) ===
Q mean: -49.157257
Q std: 18.370693
Actor loss: 49.161243
Action reg: 0.003987
  l1.weight: grad_norm = 0.043240
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.109412
Total gradient norm: 0.207042
=== Actor Training Debug (Iteration 2644) ===
Q mean: -48.302155
Q std: 20.005949
Actor loss: 48.306133
Action reg: 0.003978
  l1.weight: grad_norm = 0.070577
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.193246
Total gradient norm: 0.335104
=== Actor Training Debug (Iteration 2645) ===
Q mean: -47.953609
Q std: 19.258238
Actor loss: 47.957581
Action reg: 0.003970
  l1.weight: grad_norm = 0.003734
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.009442
Total gradient norm: 0.017940
=== Actor Training Debug (Iteration 2646) ===
Q mean: -50.872093
Q std: 19.288584
Actor loss: 50.876076
Action reg: 0.003984
  l1.weight: grad_norm = 0.059593
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.155295
Total gradient norm: 0.342879
=== Actor Training Debug (Iteration 2647) ===
Q mean: -48.537285
Q std: 18.367451
Actor loss: 48.541283
Action reg: 0.003999
  l1.weight: grad_norm = 0.017715
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.041777
Total gradient norm: 0.074620
=== Actor Training Debug (Iteration 2648) ===
Q mean: -51.144577
Q std: 20.012976
Actor loss: 51.148556
Action reg: 0.003978
  l1.weight: grad_norm = 0.000774
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.002139
Total gradient norm: 0.004401
=== Actor Training Debug (Iteration 2649) ===
Q mean: -49.414948
Q std: 18.501247
Actor loss: 49.418941
Action reg: 0.003994
  l1.weight: grad_norm = 0.001690
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.004652
Total gradient norm: 0.007880
=== Actor Training Debug (Iteration 2650) ===
Q mean: -46.659966
Q std: 16.916178
Actor loss: 46.663952
Action reg: 0.003985
  l1.weight: grad_norm = 0.054556
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.124727
Total gradient norm: 0.226490
=== Actor Training Debug (Iteration 2651) ===
Q mean: -49.025623
Q std: 18.038557
Actor loss: 49.029610
Action reg: 0.003987
  l1.weight: grad_norm = 0.012525
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.034712
Total gradient norm: 0.081174
=== Actor Training Debug (Iteration 2652) ===
Q mean: -47.927383
Q std: 18.677881
Actor loss: 47.931358
Action reg: 0.003975
  l1.weight: grad_norm = 0.032397
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.080648
Total gradient norm: 0.156880
=== Actor Training Debug (Iteration 2653) ===
Q mean: -49.951618
Q std: 17.734306
Actor loss: 49.955597
Action reg: 0.003979
  l1.weight: grad_norm = 0.024001
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.052807
Total gradient norm: 0.084970
=== Actor Training Debug (Iteration 2654) ===
Q mean: -48.077801
Q std: 18.702101
Actor loss: 48.081787
Action reg: 0.003987
  l1.weight: grad_norm = 0.007225
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.020098
Total gradient norm: 0.042253
=== Actor Training Debug (Iteration 2655) ===
Q mean: -46.457611
Q std: 17.943302
Actor loss: 46.461594
Action reg: 0.003984
  l1.weight: grad_norm = 0.030560
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.071787
Total gradient norm: 0.134273
=== Actor Training Debug (Iteration 2656) ===
Q mean: -48.420555
Q std: 18.466351
Actor loss: 48.424549
Action reg: 0.003993
  l1.weight: grad_norm = 0.053049
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.122626
Total gradient norm: 0.225894
=== Actor Training Debug (Iteration 2657) ===
Q mean: -50.707333
Q std: 19.630760
Actor loss: 50.711315
Action reg: 0.003981
  l1.weight: grad_norm = 0.029549
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.074069
Total gradient norm: 0.172755
=== Actor Training Debug (Iteration 2658) ===
Q mean: -47.210583
Q std: 18.819403
Actor loss: 47.214550
Action reg: 0.003968
  l1.weight: grad_norm = 0.001867
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.005044
Total gradient norm: 0.010445
=== Actor Training Debug (Iteration 2659) ===
Q mean: -45.156639
Q std: 16.942736
Actor loss: 45.160633
Action reg: 0.003992
  l1.weight: grad_norm = 0.009509
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.022499
Total gradient norm: 0.042418
=== Actor Training Debug (Iteration 2660) ===
Q mean: -46.879318
Q std: 17.964643
Actor loss: 46.883312
Action reg: 0.003994
  l1.weight: grad_norm = 0.029870
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.076196
Total gradient norm: 0.140301
=== Actor Training Debug (Iteration 2661) ===
Q mean: -49.852509
Q std: 17.821037
Actor loss: 49.856495
Action reg: 0.003985
  l1.weight: grad_norm = 0.001168
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.002750
Total gradient norm: 0.004484
=== Actor Training Debug (Iteration 2662) ===
Q mean: -49.231186
Q std: 17.365501
Actor loss: 49.235184
Action reg: 0.003998
  l1.weight: grad_norm = 0.051676
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.126562
Total gradient norm: 0.209386
=== Actor Training Debug (Iteration 2663) ===
Q mean: -48.840714
Q std: 17.903528
Actor loss: 48.844704
Action reg: 0.003990
  l1.weight: grad_norm = 0.002520
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.005716
Total gradient norm: 0.009017
=== Actor Training Debug (Iteration 2664) ===
Q mean: -46.519688
Q std: 18.677370
Actor loss: 46.523663
Action reg: 0.003973
  l1.weight: grad_norm = 0.001555
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.004889
Total gradient norm: 0.012079
=== Actor Training Debug (Iteration 2665) ===
Q mean: -51.786362
Q std: 18.594015
Actor loss: 51.790348
Action reg: 0.003985
  l1.weight: grad_norm = 0.000217
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.000810
Total gradient norm: 0.002568
=== Actor Training Debug (Iteration 2666) ===
Q mean: -49.593987
Q std: 19.601007
Actor loss: 49.597973
Action reg: 0.003988
  l1.weight: grad_norm = 0.104401
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.229667
Total gradient norm: 0.423717
=== Actor Training Debug (Iteration 2667) ===
Q mean: -44.578529
Q std: 16.435387
Actor loss: 44.582523
Action reg: 0.003995
  l1.weight: grad_norm = 0.000530
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.001674
Total gradient norm: 0.004387
=== Actor Training Debug (Iteration 2668) ===
Q mean: -46.840916
Q std: 19.637447
Actor loss: 46.844902
Action reg: 0.003986
  l1.weight: grad_norm = 0.003160
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.008360
Total gradient norm: 0.017650
=== Actor Training Debug (Iteration 2669) ===
Q mean: -49.773071
Q std: 19.743124
Actor loss: 49.777046
Action reg: 0.003974
  l1.weight: grad_norm = 0.020955
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.055217
Total gradient norm: 0.108137
=== Actor Training Debug (Iteration 2670) ===
Q mean: -49.447243
Q std: 19.091604
Actor loss: 49.451225
Action reg: 0.003984
  l1.weight: grad_norm = 0.013964
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.034549
Total gradient norm: 0.062860
=== Actor Training Debug (Iteration 2671) ===
Q mean: -47.668297
Q std: 18.670486
Actor loss: 47.672283
Action reg: 0.003988
  l1.weight: grad_norm = 0.025705
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.060669
Total gradient norm: 0.129923
=== Actor Training Debug (Iteration 2672) ===
Q mean: -46.240017
Q std: 18.491808
Actor loss: 46.243999
Action reg: 0.003983
Action reg: 0.003981 0.044154ration 1203) ===
  l1.weight: grad_norm = 0.100333
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.221344
Total gradient norm: 0.300124
=== Actor Training Debug (Iteration 2683) ===
Q mean: -51.142132
Q std: 18.068186
Actor loss: 51.146122
Action reg: 0.003990
  l1.weight: grad_norm = 0.002223
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.005117
Total gradient norm: 0.010245
=== Actor Training Debug (Iteration 2684) ===
Q mean: -47.961967
Q std: 18.392111
Actor loss: 47.965954
Action reg: 0.003988
  l1.weight: grad_norm = 0.019084
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.044124
Total gradient norm: 0.101071
=== Actor Training Debug (Iteration 2685) ===
Q mean: -46.074577
Q std: 18.888309
Actor loss: 46.078548
Action reg: 0.003970
  l1.weight: grad_norm = 0.017354
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.040012
Total gradient norm: 0.075572
=== Actor Training Debug (Iteration 2686) ===
Q mean: -50.531830
Q std: 17.618195
Actor loss: 50.535831
Action reg: 0.004000
  l1.weight: grad_norm = 0.000603
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001352
Total gradient norm: 0.002444
=== Actor Training Debug (Iteration 2687) ===
Q mean: -50.012081
Q std: 18.460825
Actor loss: 50.016056
Action reg: 0.003975
  l1.weight: grad_norm = 0.008769
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.025934
Total gradient norm: 0.047526
=== Actor Training Debug (Iteration 2688) ===
Q mean: -46.380989
Q std: 17.290783
Actor loss: 46.384983
Action reg: 0.003992
  l1.weight: grad_norm = 0.038741
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.099376
Total gradient norm: 0.192856
=== Actor Training Debug (Iteration 2689) ===
Q mean: -44.654720
Q std: 17.070301
Actor loss: 44.658703
Action reg: 0.003982
  l1.weight: grad_norm = 0.054822
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.141995
Total gradient norm: 0.218432
=== Actor Training Debug (Iteration 2690) ===
Q mean: -50.166748
Q std: 18.017286
Actor loss: 50.170738
Action reg: 0.003990
  l1.weight: grad_norm = 0.000381
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.001063
Total gradient norm: 0.002683
=== Actor Training Debug (Iteration 2691) ===
Q mean: -51.094318
Q std: 17.517164
Actor loss: 51.098305
Action reg: 0.003986
  l1.weight: grad_norm = 0.005111
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.013147
Total gradient norm: 0.024759
=== Actor Training Debug (Iteration 2692) ===
Q mean: -47.619709
Q std: 18.386988
Actor loss: 47.623707
Action reg: 0.003998
  l1.weight: grad_norm = 0.007429
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.019617
Total gradient norm: 0.035402
=== Actor Training Debug (Iteration 2693) ===
Q mean: -49.147804
Q std: 19.400026
Actor loss: 49.151794
Action reg: 0.003990
  l1.weight: grad_norm = 0.000307
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.001052
Total gradient norm: 0.003081
=== Actor Training Debug (Iteration 2694) ===
Q mean: -50.371567
Q std: 19.280325
Actor loss: 50.375557
Action reg: 0.003990
  l1.weight: grad_norm = 0.059201
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.112024
Total gradient norm: 0.180854
=== Actor Training Debug (Iteration 2695) ===
Q mean: -49.427620
Q std: 19.228863
Actor loss: 49.431610
Action reg: 0.003990
  l1.weight: grad_norm = 0.027562
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.074802
Total gradient norm: 0.131347
=== Actor Training Debug (Iteration 2696) ===
Q mean: -47.841137
Q std: 17.937651
Actor loss: 47.845123
Action reg: 0.003985
  l1.weight: grad_norm = 0.063676
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.158243
Total gradient norm: 0.299212
=== Actor Training Debug (Iteration 2697) ===
Q mean: -49.261444
Q std: 18.674286
Actor loss: 49.265434
Action reg: 0.003989
  l1.weight: grad_norm = 0.000319
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.000804
Total gradient norm: 0.001855
=== Actor Training Debug (Iteration 2698) ===
Q mean: -47.939228
Q std: 17.945496
Actor loss: 47.943214
Action reg: 0.003988
  l1.weight: grad_norm = 0.000471
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.001330
Total gradient norm: 0.003266
=== Actor Training Debug (Iteration 2699) ===
Q mean: -47.488060
Q std: 18.569921
Actor loss: 47.492046
Action reg: 0.003985
  l1.weight: grad_norm = 0.000418
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.001255
Total gradient norm: 0.002899
=== Actor Training Debug (Iteration 2700) ===
Q mean: -49.168861
Q std: 18.007889
Actor loss: 49.172852
Action reg: 0.003991
  l1.weight: grad_norm = 0.072480
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.167882
Total gradient norm: 0.240372
=== Actor Training Debug (Iteration 2701) ===
Q mean: -46.139450
Q std: 17.108204
Actor loss: 46.143436
Action reg: 0.003986
  l1.weight: grad_norm = 0.078482
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.176370
Total gradient norm: 0.270873
=== Actor Training Debug (Iteration 2702) ===
Q mean: -48.055954
Q std: 16.648499
Actor loss: 48.059952
Action reg: 0.003996
  l1.weight: grad_norm = 0.004993
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.012463
Total gradient norm: 0.026779
=== Actor Training Debug (Iteration 2703) ===
Q mean: -51.124115
Q std: 17.257494
Actor loss: 51.128109
Action reg: 0.003994
  l1.weight: grad_norm = 0.003192
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.007682
Total gradient norm: 0.014980
=== Actor Training Debug (Iteration 2704) ===
Q mean: -47.486748
Q std: 18.741419
Actor loss: 47.490734
Action reg: 0.003987
  l1.weight: grad_norm = 0.008122
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.020565
Total gradient norm: 0.039127
=== Actor Training Debug (Iteration 2705) ===
Q mean: -46.199364
Q std: 18.027647
Actor loss: 46.203354
Action reg: 0.003990
  l1.weight: grad_norm = 0.009886
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.026678
Total gradient norm: 0.047258
=== Actor Training Debug (Iteration 2706) ===
Q mean: -48.791122
Q std: 18.947880
Actor loss: 48.795109
Action reg: 0.003985
  l1.weight: grad_norm = 0.000447
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.001041
Total gradient norm: 0.002259
=== Actor Training Debug (Iteration 2707) ===
Q mean: -51.443588
Q std: 19.550900
Actor loss: 51.447575
Action reg: 0.003985
  l1.weight: grad_norm = 0.025054
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.056758
Total gradient norm: 0.101336
=== Actor Training Debug (Iteration 2708) ===
Q mean: -52.367954
Q std: 20.288660
Actor loss: 52.371952
Action reg: 0.003996
  l1.weight: grad_norm = 0.000967
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.002266
Total gradient norm: 0.003340
=== Actor Training Debug (Iteration 2709) ===
Q mean: -47.806751
Q std: 17.982283
Actor loss: 47.810741
Action reg: 0.003989
  l1.weight: grad_norm = 0.002164
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.004807
Total gradient norm: 0.008747
=== Actor Training Debug (Iteration 2710) ===
Q mean: -46.143394
Q std: 17.729778
Actor loss: 46.147373
Action reg: 0.003979
  l1.weight: grad_norm = 0.000283
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.001165
Total gradient norm: 0.003578
=== Actor Training Debug (Iteration 2711) ===
Q mean: -48.721500
Q std: 18.543587
Actor loss: 48.725487
Action reg: 0.003987
  l1.weight: grad_norm = 0.000217
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.000843
Total gradient norm: 0.002437
=== Actor Training Debug (Iteration 2712) ===
Q mean: -48.114777
Q std: 18.226843
Actor loss: 48.118763
Action reg: 0.003985
  l1.weight: grad_norm = 0.019305
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.044477
Total gradient norm: 0.080295
=== Actor Training Debug (Iteration 2713) ===
Q mean: -50.152443
Q std: 17.369164
Actor loss: 50.156437
Action reg: 0.003993
  l1.weight: grad_norm = 0.008856
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.022456
Total gradient norm: 0.039346
=== Actor Training Debug (Iteration 2714) ===
Q mean: -50.401581
Q std: 17.667643
Actor loss: 50.405575
Action reg: 0.003994
  l1.weight: grad_norm = 0.081888
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.198836
Total gradient norm: 0.344619
=== Actor Training Debug (Iteration 2715) ===
Q mean: -48.781044
Q std: 17.964262
Actor loss: 48.785030
Action reg: 0.003988
  l1.weight: grad_norm = 0.015607
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.037799
Total gradient norm: 0.075980
=== Actor Training Debug (Iteration 2716) ===
Q mean: -47.471397
Q std: 18.220432
Actor loss: 47.475395
Action reg: 0.003998
  l1.weight: grad_norm = 0.039501
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.104658
Total gradient norm: 0.199447
=== Actor Training Debug (Iteration 2717) ===
Q mean: -48.030094
Q std: 18.455372
Actor loss: 48.034077
Action reg: 0.003984
  l1.weight: grad_norm = 0.014079
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.038195
Total gradient norm: 0.077739
=== Actor Training Debug (Iteration 2718) ===
Q mean: -51.003281
Q std: 18.254843
Actor loss: 51.007263
Action reg: 0.003982
  l1.weight: grad_norm = 0.054312
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.138640
Total gradient norm: 0.278664
=== Actor Training Debug (Iteration 2719) ===
Q mean: -52.269897
Q std: 20.477736
Actor loss: 52.273880
Action reg: 0.003983
  l1.weight: grad_norm = 0.031976
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.086445
Total gradient norm: 0.191417
=== Actor Training Debug (Iteration 2720) ===
Q mean: -51.157898
Q std: 17.855551
Actor loss: 51.161892
Action reg: 0.003992
  l1.weight: grad_norm = 0.063832
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.176972
Total gradient norm: 0.354151
=== Actor Training Debug (Iteration 2721) ===
Q mean: -46.783714
Q std: 16.637186
Actor loss: 46.787697
Action reg: 0.003983
  l1.weight: grad_norm = 0.000263
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.000858
Total gradient norm: 0.002574
=== Actor Training Debug (Iteration 2722) ===
Q mean: -43.927505
Q std: 17.235832
Actor loss: 43.931488
Action reg: 0.003981
  l1.weight: grad_norm = 0.000468
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.001915
Total gradient norm: 0.005509
=== Actor Training Debug (Iteration 2723) ===
Q mean: -46.222832
Q std: 17.575003
Actor loss: 46.226822
Action reg: 0.003989
  l1.weight: grad_norm = 0.001161
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.003493
Total gradient norm: 0.007077
=== Actor Training Debug (Iteration 2724) ===
Q mean: -52.984283
Q std: 20.745895
Actor loss: 52.988285
Action reg: 0.004000
  l1.weight: grad_norm = 0.000028
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000072
Total gradient norm: 0.000166
=== Actor Training Debug (Iteration 2725) ===
Q mean: -51.322769
Q std: 18.394266
Actor loss: 51.326756
Action reg: 0.003985
  l1.weight: grad_norm = 0.035612
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.113067
Total gradient norm: 0.250024
=== Actor Training Debug (Iteration 2726) ===
Q mean: -48.528736
Q std: 19.294788
Actor loss: 48.532703
Action reg: 0.003966
  l1.weight: grad_norm = 0.004796
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.012416
Total gradient norm: 0.023346
=== Actor Training Debug (Iteration 2727) ===
Q mean: -46.358936
Q std: 19.647198
Actor loss: 46.362915
Action reg: 0.003979
  l1.weight: grad_norm = 0.009423
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.019712
Total gradient norm: 0.038700
=== Actor Training Debug (Iteration 2728) ===
Q mean: -45.445221
Q std: 18.623281
Actor loss: 45.449200
Action reg: 0.003978
  l1.weight: grad_norm = 0.063875
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.162263
Total gradient norm: 0.349024
=== Actor Training Debug (Iteration 2729) ===
Q mean: -48.056664
Q std: 18.047403
Actor loss: 48.060650
Action reg: 0.003988
  l1.weight: grad_norm = 0.019359
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.047851
Total gradient norm: 0.101486
=== Actor Training Debug (Iteration 2730) ===
Q mean: -50.766891
Q std: 17.418928
Actor loss: 50.770885
Action reg: 0.003994
  l1.weight: grad_norm = 0.021257
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.052964
Total gradient norm: 0.102280
=== Actor Training Debug (Iteration 2731) ===
Q mean: -49.053528
Q std: 19.318079
Actor loss: 49.057503
Action reg: 0.003976
  l1.weight: grad_norm = 0.065879
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.165249
Total gradient norm: 0.320978
=== Actor Training Debug (Iteration 2732) ===
Q mean: -47.319389
Q std: 18.100742
Actor loss: 47.323364
Action reg: 0.003977
  l1.weight: grad_norm = 0.001382
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.003960
Total gradient norm: 0.009575
=== Actor Training Debug (Iteration 2733) ===
Q mean: -47.038593
Q std: 18.092394
Actor loss: 47.042580
Action reg: 0.003986
  l1.weight: grad_norm = 0.074654
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.176589
Total gradient norm: 0.352899
=== Actor Training Debug (Iteration 2734) ===
Q mean: -50.092171
Q std: 17.283188
Actor loss: 50.096153
Action reg: 0.003981
  l1.weight: grad_norm = 0.002041
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.005133
Total gradient norm: 0.008247
=== Actor Training Debug (Iteration 2735) ===
Q mean: -48.202263
Q std: 18.339581
Actor loss: 48.206245
Action reg: 0.003984
  l1.weight: grad_norm = 0.001003
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.002655
Total gradient norm: 0.004351
=== Actor Training Debug (Iteration 2736) ===
Q mean: -49.563221
Q std: 19.689486
Actor loss: 49.567196
Action reg: 0.003976
  l1.weight: grad_norm = 0.070733
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.168645
Total gradient norm: 0.345065
=== Actor Training Debug (Iteration 2737) ===
Q mean: -48.899979
Q std: 17.888948
Actor loss: 48.903976
Action reg: 0.003997
  l1.weight: grad_norm = 0.025740
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.067758
Total gradient norm: 0.143508
=== Actor Training Debug (Iteration 2738) ===
Q mean: -47.744240
Q std: 19.096781
Actor loss: 47.748238
Action reg: 0.003996
  l1.weight: grad_norm = 0.001115
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.002395
Total gradient norm: 0.005071
=== Actor Training Debug (Iteration 2739) ===
Q mean: -49.409142
Q std: 17.250881
Actor loss: 49.413128
Action reg: 0.003985
  l1.weight: grad_norm = 0.042913
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.105664
Total gradient norm: 0.228774
=== Actor Training Debug (Iteration 2740) ===
Q mean: -51.461571
Q std: 17.943804
Actor loss: 51.465565
Action reg: 0.003992
  l1.weight: grad_norm = 0.006557
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.015519
Total gradient norm: 0.031073
=== Actor Training Debug (Iteration 2741) ===
Q mean: -51.380138
Q std: 18.333281
Actor loss: 51.384132
Action reg: 0.003995
  l1.weight: grad_norm = 0.082944
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.203285
Total gradient norm: 0.351013
Total gradient norm: 0.149554ration 1203) ===
=== Actor Training Debug (Iteration 2752) ===
Q mean: -48.945511
Q std: 18.476561
Actor loss: 48.949478
Action reg: 0.003966
  l1.weight: grad_norm = 0.026206
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.064033
Total gradient norm: 0.120866
=== Actor Training Debug (Iteration 2753) ===
Q mean: -50.307980
Q std: 19.734093
Actor loss: 50.311970
Action reg: 0.003991
  l1.weight: grad_norm = 0.000214
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.000794
Total gradient norm: 0.002408
=== Actor Training Debug (Iteration 2754) ===
Q mean: -47.931404
Q std: 18.482279
Actor loss: 47.935390
Action reg: 0.003988
  l1.weight: grad_norm = 0.000235
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.000655
Total gradient norm: 0.001829
=== Actor Training Debug (Iteration 2755) ===
Q mean: -47.223473
Q std: 17.691105
Actor loss: 47.227463
Action reg: 0.003990
  l1.weight: grad_norm = 0.031068
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.089107
Total gradient norm: 0.178233
=== Actor Training Debug (Iteration 2756) ===
Q mean: -50.658501
Q std: 17.704189
Actor loss: 50.662491
Action reg: 0.003990
  l1.weight: grad_norm = 0.000365
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.000856
Total gradient norm: 0.001699
=== Actor Training Debug (Iteration 2757) ===
Q mean: -49.910625
Q std: 18.983450
Actor loss: 49.914612
Action reg: 0.003988
  l1.weight: grad_norm = 0.003322
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.007603
Total gradient norm: 0.012681
=== Actor Training Debug (Iteration 2758) ===
Q mean: -50.258110
Q std: 19.334301
Actor loss: 50.262100
Action reg: 0.003989
  l1.weight: grad_norm = 0.005549
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.009610
Total gradient norm: 0.013302
=== Actor Training Debug (Iteration 2759) ===
Q mean: -45.393372
Q std: 17.021265
Actor loss: 45.397366
Action reg: 0.003992
  l1.weight: grad_norm = 0.014083
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.039499
Total gradient norm: 0.078256
=== Actor Training Debug (Iteration 2760) ===
Q mean: -47.488506
Q std: 18.975447
Actor loss: 47.492489
Action reg: 0.003982
  l1.weight: grad_norm = 0.000561
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.001440
Total gradient norm: 0.002917
=== Actor Training Debug (Iteration 2761) ===
Q mean: -51.950699
Q std: 18.690081
Actor loss: 51.954689
Action reg: 0.003989
  l1.weight: grad_norm = 0.031144
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.079303
Total gradient norm: 0.133947
=== Actor Training Debug (Iteration 2762) ===
Q mean: -51.099632
Q std: 18.442823
Actor loss: 51.103619
Action reg: 0.003985
  l1.weight: grad_norm = 0.002793
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.006669
Total gradient norm: 0.010451
=== Actor Training Debug (Iteration 2763) ===
Q mean: -48.327171
Q std: 18.430944
Actor loss: 48.331154
Action reg: 0.003984
  l1.weight: grad_norm = 0.002918
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.007665
Total gradient norm: 0.018865
=== Actor Training Debug (Iteration 2764) ===
Q mean: -47.801563
Q std: 20.599413
Actor loss: 47.805546
Action reg: 0.003982
  l1.weight: grad_norm = 0.061872
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.136809
Total gradient norm: 0.225504
=== Actor Training Debug (Iteration 2765) ===
Q mean: -49.379288
Q std: 19.267475
Actor loss: 49.383266
Action reg: 0.003980
  l1.weight: grad_norm = 0.000378
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.001647
Total gradient norm: 0.005047
=== Actor Training Debug (Iteration 2766) ===
Q mean: -51.083988
Q std: 17.504635
Actor loss: 51.087986
Action reg: 0.003998
  l1.weight: grad_norm = 0.018075
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.040822
Total gradient norm: 0.083571
=== Actor Training Debug (Iteration 2767) ===
Q mean: -50.312656
Q std: 20.361889
Actor loss: 50.316650
Action reg: 0.003993
  l1.weight: grad_norm = 0.015679
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.040029
Total gradient norm: 0.083642
=== Actor Training Debug (Iteration 2768) ===
Q mean: -50.808556
Q std: 19.690331
Actor loss: 50.812550
Action reg: 0.003992
  l1.weight: grad_norm = 0.050659
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.121452
Total gradient norm: 0.220217
=== Actor Training Debug (Iteration 2769) ===
Q mean: -47.733627
Q std: 19.392277
Actor loss: 47.737629
Action reg: 0.004000
  l1.weight: grad_norm = 0.000037
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000096
Total gradient norm: 0.000197
=== Actor Training Debug (Iteration 2770) ===
Q mean: -48.920033
Q std: 18.366083
Actor loss: 48.924015
Action reg: 0.003983
  l1.weight: grad_norm = 0.039053
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.101914
Total gradient norm: 0.177159
=== Actor Training Debug (Iteration 2771) ===
Q mean: -51.397430
Q std: 17.416929
Actor loss: 51.401428
Action reg: 0.003997
  l1.weight: grad_norm = 0.048207
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.134813
Total gradient norm: 0.240620
=== Actor Training Debug (Iteration 2772) ===
Q mean: -51.409607
Q std: 18.809822
Actor loss: 51.413593
Action reg: 0.003987
  l1.weight: grad_norm = 0.035910
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.077536
Total gradient norm: 0.122347
=== Actor Training Debug (Iteration 2773) ===
Q mean: -46.844055
Q std: 18.053312
Actor loss: 46.848038
Action reg: 0.003982
  l1.weight: grad_norm = 0.023189
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.051837
Total gradient norm: 0.107130
=== Actor Training Debug (Iteration 2774) ===
Q mean: -48.184067
Q std: 17.916962
Actor loss: 48.188061
Action reg: 0.003995
  l1.weight: grad_norm = 0.032382
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.074980
Total gradient norm: 0.127023
=== Actor Training Debug (Iteration 2775) ===
Q mean: -51.627274
Q std: 17.927599
Actor loss: 51.631260
Action reg: 0.003985
  l1.weight: grad_norm = 0.000252
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.000955
Total gradient norm: 0.002701
=== Actor Training Debug (Iteration 2776) ===
Q mean: -49.371597
Q std: 17.689449
Actor loss: 49.375580
Action reg: 0.003982
  l1.weight: grad_norm = 0.011709
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.031677
Total gradient norm: 0.054329
=== Actor Training Debug (Iteration 2777) ===
Q mean: -47.325294
Q std: 18.128965
Actor loss: 47.329285
Action reg: 0.003989
  l1.weight: grad_norm = 0.000328
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.001186
Total gradient norm: 0.003482
=== Actor Training Debug (Iteration 2778) ===
Q mean: -48.763279
Q std: 18.638845
Actor loss: 48.767258
Action reg: 0.003978
  l1.weight: grad_norm = 0.000248
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.001151
Total gradient norm: 0.003579
=== Actor Training Debug (Iteration 2779) ===
Q mean: -52.077217
Q std: 18.227737
Actor loss: 52.081207
Action reg: 0.003991
  l1.weight: grad_norm = 0.104058
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.225446
Total gradient norm: 0.454230
=== Actor Training Debug (Iteration 2780) ===
Q mean: -50.263645
Q std: 16.841267
Actor loss: 50.267643
Action reg: 0.003998
  l1.weight: grad_norm = 0.003141
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.008478
Total gradient norm: 0.015753
=== Actor Training Debug (Iteration 2781) ===
Q mean: -46.675095
Q std: 17.476032
Actor loss: 46.679081
Action reg: 0.003985
  l1.weight: grad_norm = 0.018648
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.047359
Total gradient norm: 0.084992
=== Actor Training Debug (Iteration 2782) ===
Q mean: -48.459190
Q std: 17.232378
Actor loss: 48.463177
Action reg: 0.003987
  l1.weight: grad_norm = 0.006132
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.014597
Total gradient norm: 0.027324
=== Actor Training Debug (Iteration 2783) ===
Q mean: -49.603058
Q std: 19.256763
Actor loss: 49.607056
Action reg: 0.003999
  l1.weight: grad_norm = 0.004488
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.011164
Total gradient norm: 0.017710
=== Actor Training Debug (Iteration 2784) ===
Q mean: -49.831886
Q std: 18.230925
Actor loss: 49.835865
Action reg: 0.003979
  l1.weight: grad_norm = 0.009235
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.025891
Total gradient norm: 0.047373
=== Actor Training Debug (Iteration 2785) ===
Q mean: -48.683098
Q std: 18.015688
Actor loss: 48.687088
Action reg: 0.003988
  l1.weight: grad_norm = 0.096818
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.246668
Total gradient norm: 0.525162
=== Actor Training Debug (Iteration 2786) ===
Q mean: -49.904186
Q std: 15.835176
Actor loss: 49.908173
Action reg: 0.003985
  l1.weight: grad_norm = 0.069571
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.157521
Total gradient norm: 0.320832
=== Actor Training Debug (Iteration 2787) ===
Q mean: -48.627102
Q std: 18.481630
Actor loss: 48.631084
Action reg: 0.003983
  l1.weight: grad_norm = 0.042713
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.090355
Total gradient norm: 0.132297
=== Actor Training Debug (Iteration 2788) ===
Q mean: -51.850975
Q std: 18.580473
Actor loss: 51.854961
Action reg: 0.003986
  l1.weight: grad_norm = 0.000197
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.000901
Total gradient norm: 0.002792
=== Actor Training Debug (Iteration 2789) ===
Q mean: -48.371017
Q std: 18.859182
Actor loss: 48.375004
Action reg: 0.003986
  l1.weight: grad_norm = 0.000222
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.000668
Total gradient norm: 0.002018
=== Actor Training Debug (Iteration 2790) ===
Q mean: -48.017876
Q std: 17.902708
Actor loss: 48.021854
Action reg: 0.003980
  l1.weight: grad_norm = 0.056453
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.159177
Total gradient norm: 0.293292
=== Actor Training Debug (Iteration 2791) ===
Q mean: -48.028137
Q std: 19.056675
Actor loss: 48.032108
Action reg: 0.003972
  l1.weight: grad_norm = 0.015875
  l1.bias: grad_norm = 0.000378
  l2.weight: grad_norm = 0.040808
Total gradient norm: 0.076010
=== Actor Training Debug (Iteration 2792) ===
Q mean: -52.510933
Q std: 18.294771
Actor loss: 52.514931
Action reg: 0.003998
  l1.weight: grad_norm = 0.002357
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.006259
Total gradient norm: 0.014764
=== Actor Training Debug (Iteration 2793) ===
Q mean: -50.268097
Q std: 18.173378
Actor loss: 50.272079
Action reg: 0.003981
  l1.weight: grad_norm = 0.001270
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.002582
Total gradient norm: 0.004414
=== Actor Training Debug (Iteration 2794) ===
Q mean: -48.511551
Q std: 16.898632
Actor loss: 48.515530
Action reg: 0.003980
  l1.weight: grad_norm = 0.012270
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.030023
Total gradient norm: 0.051908
=== Actor Training Debug (Iteration 2795) ===
Q mean: -47.242645
Q std: 18.259209
Actor loss: 47.246613
Action reg: 0.003968
  l1.weight: grad_norm = 0.003565
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.008431
Total gradient norm: 0.014358
=== Actor Training Debug (Iteration 2796) ===
Q mean: -51.846443
Q std: 19.267300
Actor loss: 51.850422
Action reg: 0.003980
  l1.weight: grad_norm = 0.019599
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.046360
Total gradient norm: 0.085588
=== Actor Training Debug (Iteration 2797) ===
Q mean: -51.194656
Q std: 18.743940
Actor loss: 51.198647
Action reg: 0.003989
  l1.weight: grad_norm = 0.000720
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.001696
Total gradient norm: 0.003685
=== Actor Training Debug (Iteration 2798) ===
Q mean: -49.647209
Q std: 18.460121
Actor loss: 49.651188
Action reg: 0.003980
  l1.weight: grad_norm = 0.001461
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.004105
Total gradient norm: 0.009727
=== Actor Training Debug (Iteration 2799) ===
Q mean: -46.367779
Q std: 15.531060
Actor loss: 46.371773
Action reg: 0.003994
  l1.weight: grad_norm = 0.000149
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.000357
Total gradient norm: 0.000970
=== Actor Training Debug (Iteration 2800) ===
Q mean: -46.030586
Q std: 18.250532
Actor loss: 46.034580
Action reg: 0.003993
  l1.weight: grad_norm = 0.076457
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.185822
Total gradient norm: 0.351724
=== Actor Training Debug (Iteration 2801) ===
Q mean: -49.264267
Q std: 17.311977
Actor loss: 49.268250
Action reg: 0.003983
  l1.weight: grad_norm = 0.032748
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.077781
Total gradient norm: 0.156274
=== Actor Training Debug (Iteration 2802) ===
Q mean: -52.905846
Q std: 17.776005
Actor loss: 52.909840
Action reg: 0.003993
  l1.weight: grad_norm = 0.037655
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.085554
Total gradient norm: 0.190791
=== Actor Training Debug (Iteration 2803) ===
Q mean: -52.747589
Q std: 17.986347
Actor loss: 52.751583
Action reg: 0.003993
  l1.weight: grad_norm = 0.003947
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.009985
Total gradient norm: 0.022321
=== Actor Training Debug (Iteration 2804) ===
Q mean: -50.124592
Q std: 19.413048
Actor loss: 50.128578
Action reg: 0.003988
  l1.weight: grad_norm = 0.042916
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.105745
Total gradient norm: 0.231427
=== Actor Training Debug (Iteration 2805) ===
Q mean: -48.233288
Q std: 17.251715
Actor loss: 48.237282
Action reg: 0.003994
  l1.weight: grad_norm = 0.014615
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.037819
Total gradient norm: 0.074218
=== Actor Training Debug (Iteration 2806) ===
Q mean: -48.180351
Q std: 17.764111
Actor loss: 48.184345
Action reg: 0.003995
  l1.weight: grad_norm = 0.004213
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.011363
Total gradient norm: 0.019731
=== Actor Training Debug (Iteration 2807) ===
Q mean: -51.616158
Q std: 19.272303
Actor loss: 51.620144
Action reg: 0.003985
  l1.weight: grad_norm = 0.006166
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.016087
Total gradient norm: 0.031019
=== Actor Training Debug (Iteration 2808) ===
Q mean: -53.229683
Q std: 19.206451
Actor loss: 53.233665
Action reg: 0.003982
  l1.weight: grad_norm = 0.003017
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.008427
Total gradient norm: 0.019159
=== Actor Training Debug (Iteration 2809) ===
Q mean: -52.224457
Q std: 16.419920
Actor loss: 52.228455
Action reg: 0.003998
  l1.weight: grad_norm = 0.010085
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.023125
Total gradient norm: 0.042471
=== Actor Training Debug (Iteration 2810) ===
Q mean: -47.803902
Q std: 17.911833
Actor loss: 47.807896
Action reg: 0.003994
  l1.weight: grad_norm = 0.004458
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.010704
Total gradient norm: 0.023174
Total gradient norm: 0.004069ration 1203) ===
=== Actor Training Debug (Iteration 2821) ===
Q mean: -51.114243
Q std: 17.892448
Actor loss: 51.118233
Action reg: 0.003990
  l1.weight: grad_norm = 0.000308
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.000773
Total gradient norm: 0.001593
=== Actor Training Debug (Iteration 2822) ===
Q mean: -50.533157
Q std: 17.715998
Actor loss: 50.537136
Action reg: 0.003979
  l1.weight: grad_norm = 0.039055
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.088515
Total gradient norm: 0.174714
=== Actor Training Debug (Iteration 2823) ===
Q mean: -48.147667
Q std: 16.037275
Actor loss: 48.151665
Action reg: 0.003999
  l1.weight: grad_norm = 0.032729
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.077298
Total gradient norm: 0.161949
=== Actor Training Debug (Iteration 2824) ===
Q mean: -49.509289
Q std: 16.993422
Actor loss: 49.513279
Action reg: 0.003991
  l1.weight: grad_norm = 0.035471
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.086532
Total gradient norm: 0.154144
=== Actor Training Debug (Iteration 2825) ===
Q mean: -50.736534
Q std: 20.005812
Actor loss: 50.740517
Action reg: 0.003983
  l1.weight: grad_norm = 0.013635
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.035410
Total gradient norm: 0.064867
=== Actor Training Debug (Iteration 2826) ===
Q mean: -52.777191
Q std: 20.685669
Actor loss: 52.781170
Action reg: 0.003977
  l1.weight: grad_norm = 0.013870
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.034722
Total gradient norm: 0.068430
=== Actor Training Debug (Iteration 2827) ===
Q mean: -50.128761
Q std: 19.234110
Actor loss: 50.132748
Action reg: 0.003986
  l1.weight: grad_norm = 0.005858
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.014011
Total gradient norm: 0.028963
=== Actor Training Debug (Iteration 2828) ===
Q mean: -49.490021
Q std: 15.947066
Actor loss: 49.494019
Action reg: 0.003998
  l1.weight: grad_norm = 0.041319
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.094230
Total gradient norm: 0.190129
=== Actor Training Debug (Iteration 2829) ===
Q mean: -48.490543
Q std: 16.721373
Actor loss: 48.494537
Action reg: 0.003993
  l1.weight: grad_norm = 0.025238
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.068039
Total gradient norm: 0.123988
=== Actor Training Debug (Iteration 2830) ===
Q mean: -50.434662
Q std: 17.917063
Actor loss: 50.438637
Action reg: 0.003975
  l1.weight: grad_norm = 0.003343
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.007094
Total gradient norm: 0.010944
=== Actor Training Debug (Iteration 2831) ===
Q mean: -51.904911
Q std: 17.960926
Actor loss: 51.908897
Action reg: 0.003985
  l1.weight: grad_norm = 0.011064
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.022664
Total gradient norm: 0.041498
=== Actor Training Debug (Iteration 2832) ===
Q mean: -47.717918
Q std: 17.558634
Actor loss: 47.721905
Action reg: 0.003986
  l1.weight: grad_norm = 0.000359
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.000984
Total gradient norm: 0.002390
=== Actor Training Debug (Iteration 2833) ===
Q mean: -48.666740
Q std: 17.885683
Actor loss: 48.670727
Action reg: 0.003988
  l1.weight: grad_norm = 0.053119
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.126961
Total gradient norm: 0.277958
=== Actor Training Debug (Iteration 2834) ===
Q mean: -50.001965
Q std: 18.987753
Actor loss: 50.005959
Action reg: 0.003993
  l1.weight: grad_norm = 0.001928
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.004999
Total gradient norm: 0.008876
=== Actor Training Debug (Iteration 2835) ===
Q mean: -51.929794
Q std: 17.997730
Actor loss: 51.933781
Action reg: 0.003988
  l1.weight: grad_norm = 0.004666
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.014153
Total gradient norm: 0.027700
=== Actor Training Debug (Iteration 2836) ===
Q mean: -49.937332
Q std: 18.786896
Actor loss: 49.941315
Action reg: 0.003983
  l1.weight: grad_norm = 0.036223
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.094773
Total gradient norm: 0.208450
=== Actor Training Debug (Iteration 2837) ===
Q mean: -50.660919
Q std: 16.139900
Actor loss: 50.664913
Action reg: 0.003995
  l1.weight: grad_norm = 0.000370
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.000874
Total gradient norm: 0.001381
=== Actor Training Debug (Iteration 2838) ===
Q mean: -50.535004
Q std: 17.591427
Actor loss: 50.538975
Action reg: 0.003970
  l1.weight: grad_norm = 0.028184
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.063411
Total gradient norm: 0.114925
=== Actor Training Debug (Iteration 2839) ===
Q mean: -50.443626
Q std: 18.722929
Actor loss: 50.447605
Action reg: 0.003980
  l1.weight: grad_norm = 0.008153
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.020164
Total gradient norm: 0.040723
=== Actor Training Debug (Iteration 2840) ===
Q mean: -50.515888
Q std: 18.088285
Actor loss: 50.519875
Action reg: 0.003987
  l1.weight: grad_norm = 0.092411
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.241998
Total gradient norm: 0.541064
=== Actor Training Debug (Iteration 2841) ===
Q mean: -50.640312
Q std: 18.303246
Actor loss: 50.644299
Action reg: 0.003988
  l1.weight: grad_norm = 0.025161
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.054210
Total gradient norm: 0.098089
=== Actor Training Debug (Iteration 2842) ===
Q mean: -47.652081
Q std: 18.270477
Actor loss: 47.656059
Action reg: 0.003977
  l1.weight: grad_norm = 0.016267
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.041647
Total gradient norm: 0.077475
=== Actor Training Debug (Iteration 2843) ===
Q mean: -48.012329
Q std: 18.414179
Actor loss: 48.016319
Action reg: 0.003991
  l1.weight: grad_norm = 0.046213
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.105206
Total gradient norm: 0.229857
=== Actor Training Debug (Iteration 2844) ===
Q mean: -49.276550
Q std: 18.291203
Actor loss: 49.280544
Action reg: 0.003992
  l1.weight: grad_norm = 0.007866
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.020087
Total gradient norm: 0.037071
=== Actor Training Debug (Iteration 2845) ===
Q mean: -48.538132
Q std: 17.143225
Actor loss: 48.542122
Action reg: 0.003990
  l1.weight: grad_norm = 0.016410
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.040697
Total gradient norm: 0.082588
=== Actor Training Debug (Iteration 2846) ===
Q mean: -49.929138
Q std: 19.248390
Actor loss: 49.933090
Action reg: 0.003952
  l1.weight: grad_norm = 0.004440
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.010398
Total gradient norm: 0.021248
=== Actor Training Debug (Iteration 2847) ===
Q mean: -50.649384
Q std: 19.220526
Actor loss: 50.653378
Action reg: 0.003994
  l1.weight: grad_norm = 0.009123
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.025899
Total gradient norm: 0.055435
=== Actor Training Debug (Iteration 2848) ===
Q mean: -48.945202
Q std: 20.003904
Actor loss: 48.949177
Action reg: 0.003974
  l1.weight: grad_norm = 0.024886
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.063703
Total gradient norm: 0.136509
=== Actor Training Debug (Iteration 2849) ===
Q mean: -47.801598
Q std: 20.387632
Actor loss: 47.805576
Action reg: 0.003978
  l1.weight: grad_norm = 0.034148
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.081382
Total gradient norm: 0.151661
=== Actor Training Debug (Iteration 2850) ===
Q mean: -48.787910
Q std: 18.016651
Actor loss: 48.791889
Action reg: 0.003978
  l1.weight: grad_norm = 0.025247
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.052607
Total gradient norm: 0.075105
=== Actor Training Debug (Iteration 2851) ===
Q mean: -53.629314
Q std: 19.240040
Actor loss: 53.633308
Action reg: 0.003995
  l1.weight: grad_norm = 0.002594
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.006392
Total gradient norm: 0.013371
=== Actor Training Debug (Iteration 2852) ===
Q mean: -51.386059
Q std: 16.593777
Actor loss: 51.390053
Action reg: 0.003994
  l1.weight: grad_norm = 0.038791
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.090962
Total gradient norm: 0.184113
=== Actor Training Debug (Iteration 2853) ===
Q mean: -48.300434
Q std: 18.728706
Actor loss: 48.304417
Action reg: 0.003984
  l1.weight: grad_norm = 0.000246
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.000972
Total gradient norm: 0.002655
=== Actor Training Debug (Iteration 2854) ===
Q mean: -49.674850
Q std: 17.821371
Actor loss: 49.678837
Action reg: 0.003987
  l1.weight: grad_norm = 0.065616
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.160367
Total gradient norm: 0.285868
=== Actor Training Debug (Iteration 2855) ===
Q mean: -50.212997
Q std: 18.818989
Actor loss: 50.216976
Action reg: 0.003980
  l1.weight: grad_norm = 0.016012
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.037234
Total gradient norm: 0.062267
=== Actor Training Debug (Iteration 2856) ===
Q mean: -51.858707
Q std: 17.492754
Actor loss: 51.862698
Action reg: 0.003988
  l1.weight: grad_norm = 0.036725
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.105334
Total gradient norm: 0.203933
=== Actor Training Debug (Iteration 2857) ===
Q mean: -47.669563
Q std: 16.036833
Actor loss: 47.673557
Action reg: 0.003995
  l1.weight: grad_norm = 0.000526
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.001277
Total gradient norm: 0.002374
=== Actor Training Debug (Iteration 2858) ===
Q mean: -49.425117
Q std: 17.981947
Actor loss: 49.429100
Action reg: 0.003983
  l1.weight: grad_norm = 0.000212
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.000971
Total gradient norm: 0.002933
=== Actor Training Debug (Iteration 2859) ===
Q mean: -52.144218
Q std: 17.569916
Actor loss: 52.148209
Action reg: 0.003989
  l1.weight: grad_norm = 0.000257
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.000783
Total gradient norm: 0.001938
=== Actor Training Debug (Iteration 2860) ===
Q mean: -46.946110
Q std: 18.258810
Actor loss: 46.950104
Action reg: 0.003994
  l1.weight: grad_norm = 0.000461
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.001139
Total gradient norm: 0.001953
Total gradient norm: 0.002774ration 1203) ===
=== Actor Training Debug (Iteration 2871) ===
Q mean: -50.474205
Q std: 19.853226
Actor loss: 50.478176
Action reg: 0.003971
  l1.weight: grad_norm = 0.000448
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.001643
Total gradient norm: 0.004688
=== Actor Training Debug (Iteration 2872) ===
Q mean: -49.672630
Q std: 16.939571
Actor loss: 49.676617
Action reg: 0.003987
  l1.weight: grad_norm = 0.093141
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.257280
Total gradient norm: 0.547335
=== Actor Training Debug (Iteration 2873) ===
Q mean: -49.973164
Q std: 18.229364
Actor loss: 49.977158
Action reg: 0.003996
  l1.weight: grad_norm = 0.000667
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.001687
Total gradient norm: 0.003033
=== Actor Training Debug (Iteration 2874) ===
Q mean: -48.723930
Q std: 17.881310
Actor loss: 48.727913
Action reg: 0.003983
  l1.weight: grad_norm = 0.000399
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.001211
Total gradient norm: 0.002889
=== Actor Training Debug (Iteration 2875) ===
Q mean: -52.534470
Q std: 18.011093
Actor loss: 52.538448
Action reg: 0.003979
  l1.weight: grad_norm = 0.000260
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.001219
Total gradient norm: 0.003696
=== Actor Training Debug (Iteration 2876) ===
Q mean: -49.306435
Q std: 18.880779
Actor loss: 49.310425
Action reg: 0.003990
  l1.weight: grad_norm = 0.099539
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.223365
Total gradient norm: 0.437235
=== Actor Training Debug (Iteration 2877) ===
Q mean: -49.414993
Q std: 18.536026
Actor loss: 49.418980
Action reg: 0.003986
  l1.weight: grad_norm = 0.018660
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.053464
Total gradient norm: 0.108986
=== Actor Training Debug (Iteration 2878) ===
Q mean: -51.215965
Q std: 19.761307
Actor loss: 51.219944
Action reg: 0.003980
  l1.weight: grad_norm = 0.003956
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.009980
Total gradient norm: 0.017441
=== Actor Training Debug (Iteration 2879) ===
Q mean: -47.629051
Q std: 17.462965
Actor loss: 47.633045
Action reg: 0.003995
  l1.weight: grad_norm = 0.003166
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.007250
Total gradient norm: 0.012910
=== Actor Training Debug (Iteration 2880) ===
Q mean: -48.932617
Q std: 18.953432
Actor loss: 48.936592
Action reg: 0.003976
  l1.weight: grad_norm = 0.082620
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.259940
Total gradient norm: 0.536127
=== Actor Training Debug (Iteration 2881) ===
Q mean: -51.912792
Q std: 17.689831
Actor loss: 51.916786
Action reg: 0.003993
  l1.weight: grad_norm = 0.002981
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.007227
Total gradient norm: 0.010302
=== Actor Training Debug (Iteration 2882) ===
Q mean: -51.128830
Q std: 18.478567
Actor loss: 51.132824
Action reg: 0.003995
  l1.weight: grad_norm = 0.000112
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.000251
Total gradient norm: 0.000557
=== Actor Training Debug (Iteration 2883) ===
Q mean: -50.915863
Q std: 19.196182
Actor loss: 50.919849
Action reg: 0.003985
  l1.weight: grad_norm = 0.081747
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.194338
Total gradient norm: 0.273267
=== Actor Training Debug (Iteration 2884) ===
Q mean: -48.100540
Q std: 18.861832
Actor loss: 48.104515
Action reg: 0.003977
  l1.weight: grad_norm = 0.004222
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.009811
Total gradient norm: 0.016950
=== Actor Training Debug (Iteration 2885) ===
Q mean: -52.875320
Q std: 17.629835
Actor loss: 52.879318
Action reg: 0.003997
  l1.weight: grad_norm = 0.035913
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.096751
Total gradient norm: 0.179254
=== Actor Training Debug (Iteration 2886) ===
Q mean: -51.226425
Q std: 19.515064
Actor loss: 51.230400
Action reg: 0.003976
  l1.weight: grad_norm = 0.019904
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.051196
Total gradient norm: 0.092413
=== Actor Training Debug (Iteration 2887) ===
Q mean: -50.362656
Q std: 19.110603
Actor loss: 50.366631
Action reg: 0.003975
  l1.weight: grad_norm = 0.026802
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.062943
Total gradient norm: 0.108205
=== Actor Training Debug (Iteration 2888) ===
Q mean: -50.489849
Q std: 19.580067
Actor loss: 50.493816
Action reg: 0.003966
  l1.weight: grad_norm = 0.003348
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.007513
Total gradient norm: 0.014200
=== Actor Training Debug (Iteration 2889) ===
Q mean: -50.401215
Q std: 18.891558
Actor loss: 50.405193
Action reg: 0.003980
  l1.weight: grad_norm = 0.034932
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.105525
Total gradient norm: 0.239653
=== Actor Training Debug (Iteration 2890) ===
Q mean: -47.716137
Q std: 19.219988
Actor loss: 47.720112
Action reg: 0.003977
  l1.weight: grad_norm = 0.016415
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.043446
Total gradient norm: 0.078850
=== Actor Training Debug (Iteration 2891) ===
Q mean: -50.963852
Q std: 17.196821
Actor loss: 50.967831
Action reg: 0.003980
  l1.weight: grad_norm = 0.078200
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.185932
Total gradient norm: 0.406069
=== Actor Training Debug (Iteration 2892) ===
Q mean: -52.570961
Q std: 18.708349
Actor loss: 52.574951
Action reg: 0.003989
  l1.weight: grad_norm = 0.000580
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.001393
Total gradient norm: 0.002344
=== Actor Training Debug (Iteration 2893) ===
Q mean: -49.121304
Q std: 19.392525
Actor loss: 49.125271
Action reg: 0.003966
  l1.weight: grad_norm = 0.001645
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.004912
Total gradient norm: 0.010378
=== Actor Training Debug (Iteration 2894) ===
Q mean: -49.786140
Q std: 17.586861
Actor loss: 49.790134
Action reg: 0.003995
  l1.weight: grad_norm = 0.000097
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.000260
Total gradient norm: 0.000686
=== Actor Training Debug (Iteration 2895) ===
Q mean: -49.325638
Q std: 17.781281
Actor loss: 49.329609
Action reg: 0.003970
  l1.weight: grad_norm = 0.002265
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.005477
Total gradient norm: 0.010778
=== Actor Training Debug (Iteration 2896) ===
Q mean: -51.383575
Q std: 16.642059
Actor loss: 51.387562
Action reg: 0.003987
  l1.weight: grad_norm = 0.034234
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.089001
Total gradient norm: 0.154524
=== Actor Training Debug (Iteration 2897) ===
Q mean: -50.069458
Q std: 16.617390
Actor loss: 50.073452
Action reg: 0.003994
  l1.weight: grad_norm = 0.001170
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.003038
Total gradient norm: 0.005924
=== Actor Training Debug (Iteration 2898) ===
Q mean: -46.611259
Q std: 17.899515
Actor loss: 46.615242
Action reg: 0.003982
  l1.weight: grad_norm = 0.036359
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.090017
Total gradient norm: 0.189929
=== Actor Training Debug (Iteration 2899) ===
Q mean: -49.325249
Q std: 17.434370
Actor loss: 49.329231
Action reg: 0.003982
  l1.weight: grad_norm = 0.000334
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.001265
Total gradient norm: 0.003294
=== Actor Training Debug (Iteration 2900) ===
Q mean: -54.047592
Q std: 19.722706
Actor loss: 54.051582
Action reg: 0.003989
  l1.weight: grad_norm = 0.002541
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.006010
Total gradient norm: 0.012892
=== Actor Training Debug (Iteration 2901) ===
Q mean: -52.651039
Q std: 17.911707
Actor loss: 52.655033
Action reg: 0.003994
  l1.weight: grad_norm = 0.052996
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.127616
Total gradient norm: 0.229826
=== Actor Training Debug (Iteration 2902) ===
Q mean: -49.504730
Q std: 19.147900
Actor loss: 49.508686
Action reg: 0.003956
  l1.weight: grad_norm = 0.085367
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.200921
Total gradient norm: 0.400413
=== Actor Training Debug (Iteration 2903) ===
Q mean: -49.264229
Q std: 16.888006
Actor loss: 49.268215
Action reg: 0.003986
  l1.weight: grad_norm = 0.001741
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.004499
Total gradient norm: 0.008512
=== Actor Training Debug (Iteration 2904) ===
Q mean: -48.859196
Q std: 15.719504
Actor loss: 48.863174
Action reg: 0.003980
  l1.weight: grad_norm = 0.016600
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.044225
Total gradient norm: 0.079118
=== Actor Training Debug (Iteration 2905) ===
Q mean: -51.151543
Q std: 17.552933
Actor loss: 51.155525
Action reg: 0.003984
  l1.weight: grad_norm = 0.000359
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.001056
Total gradient norm: 0.002543
=== Actor Training Debug (Iteration 2906) ===
Q mean: -50.283180
Q std: 18.298218
Actor loss: 50.287170
Action reg: 0.003989
  l1.weight: grad_norm = 0.004009
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.009708
Total gradient norm: 0.020488
=== Actor Training Debug (Iteration 2907) ===
Q mean: -49.692879
Q std: 20.120108
Actor loss: 49.696857
Action reg: 0.003981
  l1.weight: grad_norm = 0.148262
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.281656
Total gradient norm: 0.455422
=== Actor Training Debug (Iteration 2908) ===
Q mean: -50.135857
Q std: 19.347353
Actor loss: 50.139832
Action reg: 0.003974
  l1.weight: grad_norm = 0.022289
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.052326
Total gradient norm: 0.102349
=== Actor Training Debug (Iteration 2909) ===
Q mean: -50.909706
Q std: 17.460947
Actor loss: 50.913692
Action reg: 0.003988
  l1.weight: grad_norm = 0.006645
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.015072
Total gradient norm: 0.022010
=== Actor Training Debug (Iteration 2910) ===
Q mean: -49.827179
Q std: 18.157686
Actor loss: 49.831169
Action reg: 0.003989
  l1.weight: grad_norm = 0.008814
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.021229
Total gradient norm: 0.046496
=== Actor Training Debug (Iteration 2911) ===
Q mean: -51.342537
Q std: 17.690348
Actor loss: 51.346531
Action reg: 0.003994
  l1.weight: grad_norm = 0.000183
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.000619
Total gradient norm: 0.001531
=== Actor Training Debug (Iteration 2912) ===
Q mean: -51.325829
Q std: 17.949266
Actor loss: 51.329815
Action reg: 0.003988
  l1.weight: grad_norm = 0.032584
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.082022
Total gradient norm: 0.170269
=== Actor Training Debug (Iteration 2913) ===
Q mean: -50.366508
Q std: 19.282663
Actor loss: 50.370495
Action reg: 0.003987
  l1.weight: grad_norm = 0.019422
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.045352
Total gradient norm: 0.083324
=== Actor Training Debug (Iteration 2914) ===
Q mean: -49.489273
Q std: 18.004122
Actor loss: 49.493256
Action reg: 0.003981
  l1.weight: grad_norm = 0.001263
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.002674
Total gradient norm: 0.004871
=== Actor Training Debug (Iteration 2915) ===
Q mean: -49.653435
Q std: 18.785004
Actor loss: 49.657417
Action reg: 0.003982
  l1.weight: grad_norm = 0.000361
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.001247
Total gradient norm: 0.003461
=== Actor Training Debug (Iteration 2916) ===
Q mean: -49.292694
Q std: 17.894541
Actor loss: 49.296677
Action reg: 0.003983
  l1.weight: grad_norm = 0.002490
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.005502
Total gradient norm: 0.008974
=== Actor Training Debug (Iteration 2917) ===
Q mean: -51.750607
Q std: 19.919195
Actor loss: 51.754578
Action reg: 0.003972
  l1.weight: grad_norm = 0.024472
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.062538
Total gradient norm: 0.114488
=== Actor Training Debug (Iteration 2918) ===
Q mean: -52.197662
Q std: 17.853708
Actor loss: 52.201653
Action reg: 0.003989
  l1.weight: grad_norm = 0.002521
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.006924
Total gradient norm: 0.015912
=== Actor Training Debug (Iteration 2919) ===
Q mean: -51.885429
Q std: 18.699007
Actor loss: 51.889416
Action reg: 0.003987
  l1.weight: grad_norm = 0.002953
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.006970
Total gradient norm: 0.012986
=== Actor Training Debug (Iteration 2920) ===
Q mean: -47.531502
Q std: 17.729311
Actor loss: 47.535458
Action reg: 0.003956
  l1.weight: grad_norm = 0.030519
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.070664
Total gradient norm: 0.132262
=== Actor Training Debug (Iteration 2921) ===
Q mean: -49.633881
Q std: 18.621729
Actor loss: 49.637871
Action reg: 0.003991
  l1.weight: grad_norm = 0.030165
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.072785
Total gradient norm: 0.123829
=== Actor Training Debug (Iteration 2922) ===
Q mean: -50.371025
Q std: 21.020578
Actor loss: 50.374981
Action reg: 0.003957
  l1.weight: grad_norm = 0.007778
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.018770
Total gradient norm: 0.032210
=== Actor Training Debug (Iteration 2923) ===
Q mean: -50.453156
Q std: 18.577103
Actor loss: 50.457130
Action reg: 0.003975
  l1.weight: grad_norm = 0.008221
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.022037
Total gradient norm: 0.045077
=== Actor Training Debug (Iteration 2924) ===
Q mean: -49.138397
Q std: 18.376684
Actor loss: 49.142384
Action reg: 0.003986
  l1.weight: grad_norm = 0.006059
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.016421
Total gradient norm: 0.033656
=== Actor Training Debug (Iteration 2925) ===
Q mean: -48.736267
Q std: 17.858667
Actor loss: 48.740250
Action reg: 0.003983
  l1.weight: grad_norm = 0.005189
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.013450
Total gradient norm: 0.024657
=== Actor Training Debug (Iteration 2926) ===
Q mean: -52.643486
Q std: 19.036543
Actor loss: 52.647472
Action reg: 0.003987
  l1.weight: grad_norm = 0.006237
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.019412
Total gradient norm: 0.044371
=== Actor Training Debug (Iteration 2927) ===
Q mean: -52.353600
Q std: 18.149925
Actor loss: 52.357594
Action reg: 0.003994
  l1.weight: grad_norm = 0.000742
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.002192
Total gradient norm: 0.004668
=== Actor Training Debug (Iteration 2928) ===
Q mean: -48.600246
Q std: 19.495049
Actor loss: 48.604221
Action reg: 0.003976
  l1.weight: grad_norm = 0.005881
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.016058
Total gradient norm: 0.030331
=== Actor Training Debug (Iteration 2929) ===
Q mean: -49.881027
Q std: 19.155539
Actor loss: 49.885002
Action reg: 0.003976
  l1.weight: grad_norm = 0.039739
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.090048
Total gradient norm: 0.177798
=== Actor Training Debug (Iteration 2930) ===
Q mean: -52.002392
Q std: 19.182278
Actor loss: 52.006382
Action reg: 0.003989
  l1.weight: grad_norm = 0.001003
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.002460
Total gradient norm: 0.004409
Total gradient norm: 0.000611ration 1203) ===
=== Actor Training Debug (Iteration 2941) ===
Q mean: -51.016418
Q std: 17.278936
Actor loss: 51.020420
Action reg: 0.004000
  l1.weight: grad_norm = 0.004019
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.009290
Total gradient norm: 0.018710
=== Actor Training Debug (Iteration 2942) ===
Q mean: -49.120697
Q std: 17.837368
Actor loss: 49.124672
Action reg: 0.003975
  l1.weight: grad_norm = 0.000302
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.001011
Total gradient norm: 0.002807
=== Actor Training Debug (Iteration 2943) ===
Q mean: -52.691936
Q std: 18.389246
Actor loss: 52.695923
Action reg: 0.003988
  l1.weight: grad_norm = 0.005925
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.013739
Total gradient norm: 0.029868
=== Actor Training Debug (Iteration 2944) ===
Q mean: -49.823650
Q std: 18.604893
Actor loss: 49.827614
Action reg: 0.003964
  l1.weight: grad_norm = 0.122139
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.325905
Total gradient norm: 0.620697
=== Actor Training Debug (Iteration 2945) ===
Q mean: -48.216312
Q std: 18.084137
Actor loss: 48.220303
Action reg: 0.003992
  l1.weight: grad_norm = 0.020312
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.045086
Total gradient norm: 0.063147
=== Actor Training Debug (Iteration 2946) ===
Q mean: -49.244881
Q std: 17.332432
Actor loss: 49.248882
Action reg: 0.004000
  l1.weight: grad_norm = 0.002190
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.006620
Total gradient norm: 0.012116
=== Actor Training Debug (Iteration 2947) ===
Q mean: -51.527538
Q std: 18.399277
Actor loss: 51.531506
Action reg: 0.003968
  l1.weight: grad_norm = 0.061504
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.169493
Total gradient norm: 0.353846
=== Actor Training Debug (Iteration 2948) ===
Q mean: -52.469246
Q std: 18.880934
Actor loss: 52.473217
Action reg: 0.003971
  l1.weight: grad_norm = 0.014824
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.041836
Total gradient norm: 0.090830
=== Actor Training Debug (Iteration 2949) ===
Q mean: -48.795780
Q std: 17.693655
Actor loss: 48.799767
Action reg: 0.003988
  l1.weight: grad_norm = 0.000505
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.001002
Total gradient norm: 0.001884
=== Actor Training Debug (Iteration 2950) ===
Q mean: -47.346458
Q std: 17.581322
Actor loss: 47.350433
Action reg: 0.003977
  l1.weight: grad_norm = 0.000642
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.001656
Total gradient norm: 0.003393
=== Actor Training Debug (Iteration 2951) ===
Q mean: -49.278984
Q std: 19.286520
Actor loss: 49.282970
Action reg: 0.003988
  l1.weight: grad_norm = 0.000252
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.000701
Total gradient norm: 0.001642
=== Actor Training Debug (Iteration 2952) ===
Q mean: -54.460457
Q std: 17.559292
Actor loss: 54.464447
Action reg: 0.003988
  l1.weight: grad_norm = 0.004869
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.011402
Total gradient norm: 0.020792
=== Actor Training Debug (Iteration 2953) ===
Q mean: -52.441689
Q std: 18.117750
Actor loss: 52.445675
Action reg: 0.003988
  l1.weight: grad_norm = 0.001721
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.003540
Total gradient norm: 0.007304
=== Actor Training Debug (Iteration 2954) ===
Q mean: -50.699741
Q std: 19.247427
Actor loss: 50.703716
Action reg: 0.003974
  l1.weight: grad_norm = 0.032954
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.087420
Total gradient norm: 0.175413
=== Actor Training Debug (Iteration 2955) ===
Q mean: -49.214855
Q std: 16.723957
Actor loss: 49.218842
Action reg: 0.003985
  l1.weight: grad_norm = 0.063230
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.152834
Total gradient norm: 0.256961
=== Actor Training Debug (Iteration 2956) ===
Q mean: -51.351486
Q std: 18.004969
Actor loss: 51.355473
Action reg: 0.003988
  l1.weight: grad_norm = 0.020715
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.056546
Total gradient norm: 0.105791
=== Actor Training Debug (Iteration 2957) ===
Q mean: -52.773682
Q std: 19.803051
Actor loss: 52.777645
Action reg: 0.003963
  l1.weight: grad_norm = 0.014917
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.041478
Total gradient norm: 0.072623
=== Actor Training Debug (Iteration 2958) ===
Q mean: -49.601379
Q std: 17.081520
Actor loss: 49.605373
Action reg: 0.003995
  l1.weight: grad_norm = 0.050846
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.125130
Total gradient norm: 0.186607
=== Actor Training Debug (Iteration 2959) ===
Q mean: -49.308628
Q std: 18.271610
Actor loss: 49.312607
Action reg: 0.003979
  l1.weight: grad_norm = 0.053291
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.106682
Total gradient norm: 0.140413
=== Actor Training Debug (Iteration 2960) ===
Q mean: -51.748035
Q std: 18.468390
Actor loss: 51.752022
Action reg: 0.003985
  l1.weight: grad_norm = 0.058112
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.160423
Total gradient norm: 0.340591
=== Actor Training Debug (Iteration 2961) ===
Q mean: -54.204437
Q std: 19.159056
Actor loss: 54.208427
Action reg: 0.003992
  l1.weight: grad_norm = 0.042093
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.099531
Total gradient norm: 0.196455
=== Actor Training Debug (Iteration 2962) ===
Q mean: -52.086380
Q std: 18.454559
Actor loss: 52.090355
Action reg: 0.003976
  l1.weight: grad_norm = 0.078900
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.220339
Total gradient norm: 0.378048
=== Actor Training Debug (Iteration 2963) ===
Q mean: -48.756226
Q std: 16.485313
Actor loss: 48.760216
Action reg: 0.003992
  l1.weight: grad_norm = 0.071325
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.168677
Total gradient norm: 0.317326
=== Actor Training Debug (Iteration 2964) ===
Q mean: -52.450218
Q std: 16.919889
Actor loss: 52.454212
Action reg: 0.003993
  l1.weight: grad_norm = 0.059389
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.127456
Total gradient norm: 0.238937
=== Actor Training Debug (Iteration 2965) ===
Q mean: -51.706207
Q std: 19.236832
Actor loss: 51.710194
Action reg: 0.003986
  l1.weight: grad_norm = 0.035978
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.083897
Total gradient norm: 0.168666
=== Actor Training Debug (Iteration 2966) ===
Q mean: -53.139439
Q std: 18.018770
Actor loss: 53.143436
Action reg: 0.003998
  l1.weight: grad_norm = 0.008582
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.019950
Total gradient norm: 0.036571
=== Actor Training Debug (Iteration 2967) ===
Q mean: -51.563286
Q std: 18.509899
Actor loss: 51.567268
Action reg: 0.003983
  l1.weight: grad_norm = 0.000673
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.001776
Total gradient norm: 0.003014
=== Actor Training Debug (Iteration 2968) ===
Q mean: -47.742336
Q std: 17.331377
Actor loss: 47.746319
Action reg: 0.003981
  l1.weight: grad_norm = 0.000254
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.001114
Total gradient norm: 0.003216
=== Actor Training Debug (Iteration 2969) ===
Q mean: -52.022903
Q std: 18.478863
Actor loss: 52.026890
Action reg: 0.003986
  l1.weight: grad_norm = 0.007069
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.019829
Total gradient norm: 0.048046
=== Actor Training Debug (Iteration 2970) ===
Q mean: -51.404083
Q std: 19.363422
Actor loss: 51.408051
Action reg: 0.003968
  l1.weight: grad_norm = 0.053994
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.121863
Total gradient norm: 0.218993
=== Actor Training Debug (Iteration 2971) ===
Q mean: -49.808998
Q std: 18.209438
Actor loss: 49.812984
Action reg: 0.003987
  l1.weight: grad_norm = 0.044059
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.106684
Total gradient norm: 0.233644
=== Actor Training Debug (Iteration 2972) ===
Q mean: -52.310867
Q std: 18.281940
Actor loss: 52.314850
Action reg: 0.003981
  l1.weight: grad_norm = 0.035031
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.079312
Total gradient norm: 0.150395
=== Actor Training Debug (Iteration 2973) ===
Q mean: -50.700317
Q std: 19.611055
Actor loss: 50.704285
Action reg: 0.003967
  l1.weight: grad_norm = 0.093578
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.226682
Total gradient norm: 0.423851
=== Actor Training Debug (Iteration 2974) ===
Q mean: -50.399178
Q std: 18.419798
Actor loss: 50.403164
Action reg: 0.003987
  l1.weight: grad_norm = 0.010078
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.026695
Total gradient norm: 0.048772
=== Actor Training Debug (Iteration 2975) ===
Q mean: -50.609016
Q std: 17.664053
Actor loss: 50.612999
Action reg: 0.003983
  l1.weight: grad_norm = 0.000294
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.001321
Total gradient norm: 0.003606
=== Actor Training Debug (Iteration 2976) ===
Q mean: -48.320805
Q std: 16.589115
Actor loss: 48.324783
Action reg: 0.003979
  l1.weight: grad_norm = 0.022185
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.045702
Total gradient norm: 0.070424
=== Actor Training Debug (Iteration 2977) ===
Q mean: -53.379715
Q std: 18.443201
Actor loss: 53.383701
Action reg: 0.003987
  l1.weight: grad_norm = 0.012179
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.031480
Total gradient norm: 0.068000
=== Actor Training Debug (Iteration 2978) ===
Q mean: -51.474915
Q std: 19.634731
Actor loss: 51.478870
Action reg: 0.003955
  l1.weight: grad_norm = 0.005445
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.011256
Total gradient norm: 0.017349
=== Actor Training Debug (Iteration 2979) ===
Q mean: -52.163269
Q std: 18.747707
Actor loss: 52.167244
Action reg: 0.003977
  l1.weight: grad_norm = 0.000242
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.001089
Total gradient norm: 0.003170
=== Actor Training Debug (Iteration 2980) ===
Q mean: -50.103455
Q std: 17.970909
Actor loss: 50.107449
Action reg: 0.003993
  l1.weight: grad_norm = 0.005300
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.014266
Total gradient norm: 0.027299
=== Actor Training Debug (Iteration 2981) ===
Q mean: -50.893475
Q std: 19.368711
Actor loss: 50.897461
Action reg: 0.003986
  l1.weight: grad_norm = 0.010400
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.028367
Total gradient norm: 0.067261
=== Actor Training Debug (Iteration 2982) ===
Q mean: -51.717743
Q std: 17.229712
Actor loss: 51.721737
Action reg: 0.003994
  l1.weight: grad_norm = 0.013333
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.036670
Total gradient norm: 0.076224
=== Actor Training Debug (Iteration 2983) ===
Q mean: -50.289955
Q std: 18.624540
Actor loss: 50.293919
Action reg: 0.003962
  l1.weight: grad_norm = 0.000379
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.001732
Total gradient norm: 0.004987
=== Actor Training Debug (Iteration 2984) ===
Q mean: -47.692146
Q std: 18.987169
Actor loss: 47.696114
Action reg: 0.003966
  l1.weight: grad_norm = 0.107353
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.254173
Total gradient norm: 0.557548
=== Actor Training Debug (Iteration 2985) ===
Q mean: -50.618225
Q std: 18.400034
Actor loss: 50.622211
Action reg: 0.003987
  l1.weight: grad_norm = 0.015077
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.034521
Total gradient norm: 0.073451
=== Actor Training Debug (Iteration 2986) ===
Q mean: -52.007664
Q std: 19.891224
Actor loss: 52.011635
Action reg: 0.003969
  l1.weight: grad_norm = 0.017991
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.046174
Total gradient norm: 0.085543
=== Actor Training Debug (Iteration 2987) ===
Q mean: -53.017982
Q std: 18.806906
Actor loss: 53.021969
Action reg: 0.003988
  l1.weight: grad_norm = 0.020831
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.058475
Total gradient norm: 0.109463
=== Actor Training Debug (Iteration 2988) ===
Q mean: -50.405678
Q std: 19.421581
Actor loss: 50.409653
Action reg: 0.003977
  l1.weight: grad_norm = 0.052742
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.117609
Total gradient norm: 0.173427
=== Actor Training Debug (Iteration 2989) ===
Q mean: -50.906410
Q std: 17.441559
Actor loss: 50.910397
Action reg: 0.003988
  l1.weight: grad_norm = 0.002719
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.006073
Total gradient norm: 0.011524
=== Actor Training Debug (Iteration 2990) ===
Q mean: -52.883373
Q std: 16.935934
Actor loss: 52.887360
Action reg: 0.003985
  l1.weight: grad_norm = 0.014959
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.036343
Total gradient norm: 0.067035
=== Actor Training Debug (Iteration 2991) ===
Q mean: -51.244232
Q std: 19.193592
Actor loss: 51.248215
Action reg: 0.003981
  l1.weight: grad_norm = 0.023176
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.056096
Total gradient norm: 0.095431
=== Actor Training Debug (Iteration 2992) ===
Q mean: -49.775627
Q std: 17.901558
Actor loss: 49.779606
Action reg: 0.003979
  l1.weight: grad_norm = 0.026371
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.077138
Total gradient norm: 0.166275
=== Actor Training Debug (Iteration 2993) ===
Q mean: -50.853180
Q std: 19.073664
Actor loss: 50.857162
Action reg: 0.003981
  l1.weight: grad_norm = 0.045330
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.103022
Total gradient norm: 0.188891
=== Actor Training Debug (Iteration 2994) ===
Q mean: -51.667755
Q std: 18.611567
Actor loss: 51.671753
Action reg: 0.003998
  l1.weight: grad_norm = 0.038621
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.095939
Total gradient norm: 0.212780
=== Actor Training Debug (Iteration 2995) ===
Q mean: -49.905563
Q std: 17.989355
Actor loss: 49.909550
Action reg: 0.003987
  l1.weight: grad_norm = 0.106292
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.228438
Total gradient norm: 0.325967
=== Actor Training Debug (Iteration 2996) ===
Q mean: -52.031509
Q std: 17.325865
Actor loss: 52.035496
Action reg: 0.003988
  l1.weight: grad_norm = 0.027706
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.075549
Total gradient norm: 0.149836
=== Actor Training Debug (Iteration 2997) ===
Q mean: -50.577332
Q std: 17.841515
Actor loss: 50.581322
Action reg: 0.003988
  l1.weight: grad_norm = 0.000613
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.001401
Total gradient norm: 0.003196
=== Actor Training Debug (Iteration 2998) ===
Q mean: -54.377174
Q std: 18.331291
Actor loss: 54.381157
Action reg: 0.003984
  l1.weight: grad_norm = 0.035711
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.095349
Total gradient norm: 0.172681
=== Actor Training Debug (Iteration 2999) ===
Q mean: -50.621719
Q std: 17.790422
Actor loss: 50.625687
Action reg: 0.003968
  l1.weight: grad_norm = 0.000410
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.001896
Total gradient norm: 0.005300
=== Actor Training Debug (Iteration 3000) ===
Q mean: -49.585312
Q std: 18.309128
Actor loss: 49.589287
Action reg: 0.003976
  l1.weight: grad_norm = 0.036244
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.093152
Total gradient norm: 0.165858
Step 8000: Critic Loss: 6.9515, Actor Loss: 49.5893, Q Value: -49.5853
  l1.bias: grad_norm = 0.000192tion 1203) ===
  l2.weight: grad_norm = 0.002046
Total gradient norm: 0.004915
=== Actor Training Debug (Iteration 3011) ===
Q mean: -53.136925
Q std: 18.174215
Actor loss: 53.140903
Action reg: 0.003978
  l1.weight: grad_norm = 0.009160
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.027043
Total gradient norm: 0.059674
=== Actor Training Debug (Iteration 3012) ===
Q mean: -51.656403
Q std: 17.440748
Actor loss: 51.660393
Action reg: 0.003991
  l1.weight: grad_norm = 0.031856
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.104979
Total gradient norm: 0.233313
=== Actor Training Debug (Iteration 3013) ===
Q mean: -51.593174
Q std: 18.952038
Actor loss: 51.597145
Action reg: 0.003973
  l1.weight: grad_norm = 0.019081
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.045190
Total gradient norm: 0.094661
=== Actor Training Debug (Iteration 3014) ===
Q mean: -53.723572
Q std: 19.997425
Actor loss: 53.727566
Action reg: 0.003994
  l1.weight: grad_norm = 0.001129
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.003739
Total gradient norm: 0.008510
=== Actor Training Debug (Iteration 3015) ===
Q mean: -53.985462
Q std: 18.379150
Actor loss: 53.989437
Action reg: 0.003975
  l1.weight: grad_norm = 0.201500
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.587200
Total gradient norm: 1.252594
=== Actor Training Debug (Iteration 3016) ===
Q mean: -50.536663
Q std: 18.847370
Actor loss: 50.540657
Action reg: 0.003992
  l1.weight: grad_norm = 0.122456
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.408600
Total gradient norm: 0.865415
=== Actor Training Debug (Iteration 3017) ===
Q mean: -51.021004
Q std: 19.855461
Actor loss: 51.024952
Action reg: 0.003950
  l1.weight: grad_norm = 0.022809
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.070571
Total gradient norm: 0.152401
=== Actor Training Debug (Iteration 3018) ===
Q mean: -50.798042
Q std: 19.754034
Actor loss: 50.801987
Action reg: 0.003943
  l1.weight: grad_norm = 0.308810
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 1.049686
Total gradient norm: 1.963185
=== Actor Training Debug (Iteration 3019) ===
Q mean: -53.470032
Q std: 17.683674
Actor loss: 53.473991
Action reg: 0.003959
  l1.weight: grad_norm = 0.723884
  l1.bias: grad_norm = 0.000836
  l2.weight: grad_norm = 2.359269
Total gradient norm: 5.062024
=== Actor Training Debug (Iteration 3020) ===
Q mean: -50.514496
Q std: 18.860329
Actor loss: 50.518467
Action reg: 0.003972
  l1.weight: grad_norm = 0.240826
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.621464
Total gradient norm: 0.907837
=== Actor Training Debug (Iteration 3021) ===
Q mean: -52.464737
Q std: 18.887186
Actor loss: 52.468716
Action reg: 0.003977
  l1.weight: grad_norm = 0.098079
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.289939
Total gradient norm: 0.594078
=== Actor Training Debug (Iteration 3022) ===
Q mean: -52.617516
Q std: 19.988132
Actor loss: 52.621483
Action reg: 0.003969
  l1.weight: grad_norm = 1.125663
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 3.194300
Total gradient norm: 5.462937
=== Actor Training Debug (Iteration 3023) ===
Q mean: -51.146328
Q std: 19.513498
Actor loss: 51.150303
Action reg: 0.003974
  l1.weight: grad_norm = 0.276909
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.726122
Total gradient norm: 1.065587
=== Actor Training Debug (Iteration 3024) ===
Q mean: -52.819672
Q std: 19.078318
Actor loss: 52.823650
Action reg: 0.003978
  l1.weight: grad_norm = 0.049470
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.128227
Total gradient norm: 0.176484
=== Actor Training Debug (Iteration 3025) ===
Q mean: -50.972561
Q std: 18.140678
Actor loss: 50.976536
Action reg: 0.003976
  l1.weight: grad_norm = 0.243483
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.667420
Total gradient norm: 1.091780
=== Actor Training Debug (Iteration 3026) ===
Q mean: -52.471481
Q std: 19.253592
Actor loss: 52.475460
Action reg: 0.003977
  l1.weight: grad_norm = 0.291629
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.835995
Total gradient norm: 1.313587
=== Actor Training Debug (Iteration 3027) ===
Q mean: -49.961349
Q std: 18.606556
Actor loss: 49.965332
Action reg: 0.003982
  l1.weight: grad_norm = 0.359201
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 1.251236
Total gradient norm: 2.525695
=== Actor Training Debug (Iteration 3028) ===
Q mean: -51.634750
Q std: 18.881285
Actor loss: 51.638729
Action reg: 0.003977
  l1.weight: grad_norm = 0.251138
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.785318
Total gradient norm: 1.363247
=== Actor Training Debug (Iteration 3029) ===
Q mean: -52.238968
Q std: 18.542131
Actor loss: 52.242954
Action reg: 0.003985
  l1.weight: grad_norm = 0.087855
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.272933
Total gradient norm: 0.413215
=== Actor Training Debug (Iteration 3030) ===
Q mean: -54.033955
Q std: 18.428942
Actor loss: 54.037933
Action reg: 0.003977
  l1.weight: grad_norm = 0.024511
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.066620
Total gradient norm: 0.121984
=== Actor Training Debug (Iteration 3031) ===
Q mean: -50.375469
Q std: 18.284117
Actor loss: 50.379448
Action reg: 0.003977
  l1.weight: grad_norm = 0.158330
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.400938
Total gradient norm: 0.643795
=== Actor Training Debug (Iteration 3032) ===
Q mean: -52.541649
Q std: 19.088200
Actor loss: 52.545624
Action reg: 0.003975
  l1.weight: grad_norm = 0.098213
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.333008
Total gradient norm: 0.669888
=== Actor Training Debug (Iteration 3033) ===
Q mean: -52.306229
Q std: 19.220592
Actor loss: 52.310215
Action reg: 0.003985
  l1.weight: grad_norm = 0.003021
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.008307
Total gradient norm: 0.016054
=== Actor Training Debug (Iteration 3034) ===
Q mean: -51.962925
Q std: 17.814276
Actor loss: 51.966908
Action reg: 0.003983
  l1.weight: grad_norm = 0.077943
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.185703
Total gradient norm: 0.355141
=== Actor Training Debug (Iteration 3035) ===
Q mean: -50.302010
Q std: 18.997068
Actor loss: 50.305981
Action reg: 0.003970
  l1.weight: grad_norm = 0.112069
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.266682
Total gradient norm: 0.489639
=== Actor Training Debug (Iteration 3036) ===
Q mean: -51.310822
Q std: 17.105652
Actor loss: 51.314819
Action reg: 0.003997
  l1.weight: grad_norm = 0.018910
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.047917
Total gradient norm: 0.081464
=== Actor Training Debug (Iteration 3037) ===
Q mean: -51.821915
Q std: 19.465290
Actor loss: 51.825893
Action reg: 0.003977
  l1.weight: grad_norm = 0.014901
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.039923
Total gradient norm: 0.078837
=== Actor Training Debug (Iteration 3038) ===
Q mean: -51.429489
Q std: 18.342800
Actor loss: 51.433472
Action reg: 0.003982
  l1.weight: grad_norm = 0.006776
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.015373
Total gradient norm: 0.029403
=== Actor Training Debug (Iteration 3039) ===
Q mean: -50.542000
Q std: 19.899338
Actor loss: 50.545959
Action reg: 0.003959
  l1.weight: grad_norm = 0.095640
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.242376
Total gradient norm: 0.450359
=== Actor Training Debug (Iteration 3040) ===
Q mean: -52.116638
Q std: 18.642744
Actor loss: 52.120609
Action reg: 0.003972
  l1.weight: grad_norm = 0.107542
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.280719
Total gradient norm: 0.466136
=== Actor Training Debug (Iteration 3041) ===
Q mean: -52.715870
Q std: 17.179155
Actor loss: 52.719852
Action reg: 0.003984
  l1.weight: grad_norm = 0.045134
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.115267
Total gradient norm: 0.217996
=== Actor Training Debug (Iteration 3042) ===
Q mean: -53.901054
Q std: 18.829285
Actor loss: 53.905041
Action reg: 0.003988
  l1.weight: grad_norm = 0.084873
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.245031
Total gradient norm: 0.525193
=== Actor Training Debug (Iteration 3043) ===
Q mean: -49.068867
Q std: 18.533318
Actor loss: 49.072842
Action reg: 0.003973
  l1.weight: grad_norm = 0.075992
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.171929
Total gradient norm: 0.357991
=== Actor Training Debug (Iteration 3044) ===
Q mean: -51.369610
Q std: 17.477962
Actor loss: 51.373596
Action reg: 0.003987
  l1.weight: grad_norm = 0.195268
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.454377
Total gradient norm: 0.783619
=== Actor Training Debug (Iteration 3045) ===
Q mean: -52.245544
Q std: 18.709770
Actor loss: 52.249516
Action reg: 0.003972
  l1.weight: grad_norm = 0.010918
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.026603
Total gradient norm: 0.051374
=== Actor Training Debug (Iteration 3046) ===
Q mean: -52.865517
Q std: 18.977993
Actor loss: 52.869507
Action reg: 0.003991
  l1.weight: grad_norm = 0.016556
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.041808
Total gradient norm: 0.089149
=== Actor Training Debug (Iteration 3047) ===
Q mean: -53.499290
Q std: 18.307453
Actor loss: 53.503258
Action reg: 0.003968
  l1.weight: grad_norm = 0.031235
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.073488
Total gradient norm: 0.131117
=== Actor Training Debug (Iteration 3048) ===
Q mean: -48.264862
Q std: 18.905499
Actor loss: 48.268845
Action reg: 0.003983
  l1.weight: grad_norm = 0.017846
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.040533
Total gradient norm: 0.071204
=== Actor Training Debug (Iteration 3049) ===
Q mean: -51.840340
Q std: 17.332186
Actor loss: 51.844307
Action reg: 0.003968
  l1.weight: grad_norm = 0.027062
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.070304
Total gradient norm: 0.130663
=== Actor Training Debug (Iteration 3050) ===
Q mean: -54.700256
Q std: 17.932728
Actor loss: 54.704250
Action reg: 0.003996
  l1.weight: grad_norm = 0.219335
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.497279
Total gradient norm: 0.731601
=== Actor Training Debug (Iteration 3051) ===
Q mean: -54.001263
Q std: 18.995188
Actor loss: 54.005245
Action reg: 0.003984
  l1.weight: grad_norm = 0.007890
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.021132
Total gradient norm: 0.034494
=== Actor Training Debug (Iteration 3052) ===
Q mean: -51.679466
Q std: 17.400805
Actor loss: 51.683460
Action reg: 0.003992
  l1.weight: grad_norm = 0.014226
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.032700
Total gradient norm: 0.062676
Total gradient norm: 0.36418592tion 1203) ===
=== Actor Training Debug (Iteration 3063) ===
Q mean: -49.894638
Q std: 17.609642
Actor loss: 49.898624
Action reg: 0.003985
  l1.weight: grad_norm = 0.097438
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.220179
Total gradient norm: 0.403104
=== Actor Training Debug (Iteration 3064) ===
Q mean: -53.247780
Q std: 17.220699
Actor loss: 53.251751
Action reg: 0.003971
  l1.weight: grad_norm = 0.117382
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.306039
Total gradient norm: 0.709481
=== Actor Training Debug (Iteration 3065) ===
Q mean: -52.370140
Q std: 17.953241
Actor loss: 52.374130
Action reg: 0.003990
  l1.weight: grad_norm = 0.059010
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.139671
Total gradient norm: 0.255200
=== Actor Training Debug (Iteration 3066) ===
Q mean: -52.553356
Q std: 18.778566
Actor loss: 52.557316
Action reg: 0.003960
  l1.weight: grad_norm = 0.162546
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.408105
Total gradient norm: 0.737698
=== Actor Training Debug (Iteration 3067) ===
Q mean: -47.648899
Q std: 19.088213
Actor loss: 47.652889
Action reg: 0.003991
  l1.weight: grad_norm = 0.049100
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.117394
Total gradient norm: 0.217374
=== Actor Training Debug (Iteration 3068) ===
Q mean: -47.511272
Q std: 18.883833
Actor loss: 47.515240
Action reg: 0.003966
  l1.weight: grad_norm = 0.144611
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.327710
Total gradient norm: 0.609284
=== Actor Training Debug (Iteration 3069) ===
Q mean: -51.212837
Q std: 18.321245
Actor loss: 51.216827
Action reg: 0.003989
  l1.weight: grad_norm = 0.074743
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.170041
Total gradient norm: 0.320607
=== Actor Training Debug (Iteration 3070) ===
Q mean: -54.914482
Q std: 18.289688
Actor loss: 54.918461
Action reg: 0.003977
  l1.weight: grad_norm = 0.018895
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.054506
Total gradient norm: 0.105148
=== Actor Training Debug (Iteration 3071) ===
Q mean: -55.638336
Q std: 18.034748
Actor loss: 55.642323
Action reg: 0.003988
  l1.weight: grad_norm = 0.087591
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.185952
Total gradient norm: 0.336732
=== Actor Training Debug (Iteration 3072) ===
Q mean: -50.814247
Q std: 18.158640
Actor loss: 50.818230
Action reg: 0.003983
  l1.weight: grad_norm = 0.018336
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.042803
Total gradient norm: 0.095136
=== Actor Training Debug (Iteration 3073) ===
Q mean: -50.036648
Q std: 17.939880
Actor loss: 50.040627
Action reg: 0.003978
  l1.weight: grad_norm = 0.088903
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.195427
Total gradient norm: 0.353032
=== Actor Training Debug (Iteration 3074) ===
Q mean: -47.480236
Q std: 18.536016
Actor loss: 47.484200
Action reg: 0.003962
  l1.weight: grad_norm = 0.041357
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.089525
Total gradient norm: 0.125665
=== Actor Training Debug (Iteration 3075) ===
Q mean: -49.674088
Q std: 17.694387
Actor loss: 49.678078
Action reg: 0.003992
  l1.weight: grad_norm = 0.121917
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.312801
Total gradient norm: 0.561190
=== Actor Training Debug (Iteration 3076) ===
Q mean: -56.127235
Q std: 19.246590
Actor loss: 56.131214
Action reg: 0.003977
  l1.weight: grad_norm = 0.005401
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.011742
Total gradient norm: 0.019169
=== Actor Training Debug (Iteration 3077) ===
Q mean: -56.542332
Q std: 18.824141
Actor loss: 56.546322
Action reg: 0.003989
  l1.weight: grad_norm = 0.095124
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.233765
Total gradient norm: 0.500038
=== Actor Training Debug (Iteration 3078) ===
Q mean: -52.217125
Q std: 18.623529
Actor loss: 52.221104
Action reg: 0.003979
  l1.weight: grad_norm = 0.000489
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.001489
Total gradient norm: 0.004042
=== Actor Training Debug (Iteration 3079) ===
Q mean: -51.662666
Q std: 18.924074
Actor loss: 51.666664
Action reg: 0.003999
  l1.weight: grad_norm = 0.029976
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.077353
Total gradient norm: 0.146968
=== Actor Training Debug (Iteration 3080) ===
Q mean: -50.021111
Q std: 18.103968
Actor loss: 50.025089
Action reg: 0.003978
  l1.weight: grad_norm = 0.001279
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.003214
Total gradient norm: 0.007335
=== Actor Training Debug (Iteration 3081) ===
Q mean: -49.909569
Q std: 18.750401
Actor loss: 49.913559
Action reg: 0.003988
  l1.weight: grad_norm = 0.073082
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.148168
Total gradient norm: 0.217772
=== Actor Training Debug (Iteration 3082) ===
Q mean: -54.101635
Q std: 17.778164
Actor loss: 54.105625
Action reg: 0.003991
  l1.weight: grad_norm = 0.051993
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.105830
Total gradient norm: 0.160562
=== Actor Training Debug (Iteration 3083) ===
Q mean: -54.467819
Q std: 19.404114
Actor loss: 54.471813
Action reg: 0.003993
  l1.weight: grad_norm = 0.011441
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.027139
Total gradient norm: 0.049540
=== Actor Training Debug (Iteration 3084) ===
Q mean: -50.201263
Q std: 18.176016
Actor loss: 50.205242
Action reg: 0.003977
  l1.weight: grad_norm = 0.101764
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.262179
Total gradient norm: 0.453073
=== Actor Training Debug (Iteration 3085) ===
Q mean: -48.854816
Q std: 18.813868
Actor loss: 48.858791
Action reg: 0.003975
  l1.weight: grad_norm = 0.077228
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.195104
Total gradient norm: 0.354517
=== Actor Training Debug (Iteration 3086) ===
Q mean: -51.964603
Q std: 17.967985
Actor loss: 51.968586
Action reg: 0.003982
  l1.weight: grad_norm = 0.088923
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.198725
Total gradient norm: 0.433489
=== Actor Training Debug (Iteration 3087) ===
Q mean: -51.787231
Q std: 17.084051
Actor loss: 51.791214
Action reg: 0.003984
  l1.weight: grad_norm = 0.072807
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.164226
Total gradient norm: 0.278539
=== Actor Training Debug (Iteration 3088) ===
Q mean: -52.572815
Q std: 18.228056
Actor loss: 52.576786
Action reg: 0.003970
  l1.weight: grad_norm = 0.045127
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.128436
Total gradient norm: 0.260256
=== Actor Training Debug (Iteration 3089) ===
Q mean: -49.629158
Q std: 18.343086
Actor loss: 49.633133
Action reg: 0.003973
  l1.weight: grad_norm = 0.034683
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.089718
Total gradient norm: 0.184356
=== Actor Training Debug (Iteration 3090) ===
Q mean: -49.894608
Q std: 18.882797
Actor loss: 49.898563
Action reg: 0.003954
  l1.weight: grad_norm = 0.093183
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.254443
Total gradient norm: 0.482757
=== Actor Training Debug (Iteration 3091) ===
Q mean: -52.590927
Q std: 19.730263
Actor loss: 52.594913
Action reg: 0.003986
  l1.weight: grad_norm = 0.003248
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.008871
Total gradient norm: 0.017537
=== Actor Training Debug (Iteration 3092) ===
Q mean: -54.566849
Q std: 17.424973
Actor loss: 54.570835
Action reg: 0.003986
  l1.weight: grad_norm = 0.000255
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.000734
Total gradient norm: 0.002062
=== Actor Training Debug (Iteration 3093) ===
Q mean: -51.038200
Q std: 17.589949
Actor loss: 51.042175
Action reg: 0.003973
  l1.weight: grad_norm = 0.020944
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.045309
Total gradient norm: 0.065542
=== Actor Training Debug (Iteration 3094) ===
Q mean: -49.667439
Q std: 17.039062
Actor loss: 49.671429
Action reg: 0.003991
  l1.weight: grad_norm = 0.026145
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.066615
Total gradient norm: 0.119709
=== Actor Training Debug (Iteration 3095) ===
Q mean: -51.836823
Q std: 20.640198
Actor loss: 51.840790
Action reg: 0.003968
  l1.weight: grad_norm = 0.135460
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.357628
Total gradient norm: 0.729357
=== Actor Training Debug (Iteration 3096) ===
Q mean: -52.773003
Q std: 18.743242
Actor loss: 52.776974
Action reg: 0.003971
  l1.weight: grad_norm = 0.134420
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.331477
Total gradient norm: 0.671665
=== Actor Training Debug (Iteration 3097) ===
Q mean: -48.719505
Q std: 19.721895
Actor loss: 48.723465
Action reg: 0.003960
  l1.weight: grad_norm = 0.007527
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.017003
Total gradient norm: 0.032424
=== Actor Training Debug (Iteration 3098) ===
Q mean: -49.298592
Q std: 17.113636
Actor loss: 49.302574
Action reg: 0.003984
  l1.weight: grad_norm = 0.010055
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.023215
Total gradient norm: 0.039642
=== Actor Training Debug (Iteration 3099) ===
Q mean: -51.766205
Q std: 19.689346
Actor loss: 51.770184
Action reg: 0.003978
  l1.weight: grad_norm = 0.026646
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.074670
Total gradient norm: 0.147486
=== Actor Training Debug (Iteration 3100) ===
Q mean: -52.171356
Q std: 18.224859
Actor loss: 52.175331
Action reg: 0.003975
  l1.weight: grad_norm = 0.225110
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.507566
Total gradient norm: 0.863963
Episode 81: Steps=100, Reward=-302.401, Buffer_size=8100
=== Actor Training Debug (Iteration 3101) ===
Q mean: -50.994236
Q std: 18.867828
Actor loss: 50.998203
Action reg: 0.003966
  l1.weight: grad_norm = 0.153149
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.322283
Total gradient norm: 0.613235
=== Actor Training Debug (Iteration 3102) ===
Q mean: -51.147404
Q std: 19.041103
Actor loss: 51.151386
Action reg: 0.003984
  l1.weight: grad_norm = 0.019727
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.052207
Total gradient norm: 0.124698
=== Actor Training Debug (Iteration 3103) ===
Q mean: -54.332794
Q std: 19.533577
Actor loss: 54.336773
Action reg: 0.003978
  l1.weight: grad_norm = 0.043446
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.102933
Total gradient norm: 0.185689
=== Actor Training Debug (Iteration 3104) ===
Q mean: -51.819447
Q std: 20.232353
Actor loss: 51.823418
Action reg: 0.003971
  l1.weight: grad_norm = 0.054400
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.148143
Total gradient norm: 0.323395
=== Actor Training Debug (Iteration 3105) ===
Q mean: -49.725605
Q std: 19.018761
Actor loss: 49.729572
Action reg: 0.003968
  l1.weight: grad_norm = 0.050836
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.141213
Total gradient norm: 0.295280
=== Actor Training Debug (Iteration 3106) ===
Q mean: -50.575737
Q std: 18.430367
Actor loss: 50.579727
Action reg: 0.003991
  l1.weight: grad_norm = 0.051366
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.118493
Total gradient norm: 0.212784
=== Actor Training Debug (Iteration 3107) ===
Q mean: -53.478096
Q std: 18.423340
Actor loss: 53.482086
Action reg: 0.003991
  l1.weight: grad_norm = 0.019556
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.047400
Total gradient norm: 0.089316
=== Actor Training Debug (Iteration 3108) ===
Q mean: -53.700821
Q std: 18.469244
Actor loss: 53.704800
Action reg: 0.003977
  l1.weight: grad_norm = 0.089079
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.219867
Total gradient norm: 0.475910
=== Actor Training Debug (Iteration 3109) ===
Q mean: -49.776001
Q std: 17.567289
Actor loss: 49.779980
Action reg: 0.003980
  l1.weight: grad_norm = 0.160809
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.388815
Total gradient norm: 0.708733
=== Actor Training Debug (Iteration 3110) ===
Q mean: -49.856094
Q std: 19.436184
Actor loss: 49.860062
Action reg: 0.003966
  l1.weight: grad_norm = 0.306172
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.754699
Total gradient norm: 1.423900
=== Actor Training Debug (Iteration 3111) ===
Q mean: -51.781761
Q std: 18.662098
Actor loss: 51.785751
Action reg: 0.003990
  l1.weight: grad_norm = 0.066003
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.130877
Total gradient norm: 0.215302
=== Actor Training Debug (Iteration 3112) ===
Q mean: -53.415291
Q std: 18.136305
Actor loss: 53.419277
Action reg: 0.003987
  l1.weight: grad_norm = 0.017375
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.044783
Total gradient norm: 0.086403
=== Actor Training Debug (Iteration 3113) ===
Q mean: -52.217190
Q std: 19.265535
Actor loss: 52.221149
Action reg: 0.003960
  l1.weight: grad_norm = 0.039445
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.107209
Total gradient norm: 0.195258
=== Actor Training Debug (Iteration 3114) ===
Q mean: -49.393715
Q std: 19.246944
Actor loss: 49.397697
Action reg: 0.003982
  l1.weight: grad_norm = 0.026611
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.071942
Total gradient norm: 0.150756
=== Actor Training Debug (Iteration 3115) ===
Q mean: -51.118652
Q std: 18.368460
Actor loss: 51.122643
Action reg: 0.003991
  l1.weight: grad_norm = 0.152551
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.341652
Total gradient norm: 0.688900
=== Actor Training Debug (Iteration 3116) ===
Q mean: -52.659950
Q std: 19.981531
Actor loss: 52.663918
Action reg: 0.003969
  l1.weight: grad_norm = 0.082253
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.200404
Total gradient norm: 0.381829
=== Actor Training Debug (Iteration 3117) ===
Q mean: -52.036652
Q std: 18.702503
Actor loss: 52.040634
Action reg: 0.003984
  l1.weight: grad_norm = 0.114075
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.267849
Total gradient norm: 0.471033
=== Actor Training Debug (Iteration 3118) ===
Q mean: -52.210297
Q std: 18.780411
Actor loss: 52.214283
Action reg: 0.003987
  l1.weight: grad_norm = 0.003041
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.007568
Total gradient norm: 0.013839
=== Actor Training Debug (Iteration 3119) ===
Q mean: -50.860443
Q std: 16.271454
Actor loss: 50.864437
Action reg: 0.003994
  l1.weight: grad_norm = 0.024838
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.058660
Total gradient norm: 0.105608
Total gradient norm: 0.02399492tion 1203) ===
=== Actor Training Debug (Iteration 3130) ===
Q mean: -53.001736
Q std: 19.638575
Actor loss: 53.005703
Action reg: 0.003966
  l1.weight: grad_norm = 0.090562
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.227048
Total gradient norm: 0.442078
=== Actor Training Debug (Iteration 3131) ===
Q mean: -51.386673
Q std: 17.766697
Actor loss: 51.390667
Action reg: 0.003994
  l1.weight: grad_norm = 0.003651
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.007417
Total gradient norm: 0.012827
=== Actor Training Debug (Iteration 3132) ===
Q mean: -50.516994
Q std: 18.144243
Actor loss: 50.520977
Action reg: 0.003984
  l1.weight: grad_norm = 0.029782
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.075188
Total gradient norm: 0.139246
=== Actor Training Debug (Iteration 3133) ===
Q mean: -50.424057
Q std: 18.060730
Actor loss: 50.428040
Action reg: 0.003982
  l1.weight: grad_norm = 0.065608
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.163514
Total gradient norm: 0.298594
=== Actor Training Debug (Iteration 3134) ===
Q mean: -52.696873
Q std: 18.022985
Actor loss: 52.700836
Action reg: 0.003965
  l1.weight: grad_norm = 0.013929
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.033024
Total gradient norm: 0.065779
=== Actor Training Debug (Iteration 3135) ===
Q mean: -52.032486
Q std: 17.000748
Actor loss: 52.036476
Action reg: 0.003991
  l1.weight: grad_norm = 0.058166
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.148814
Total gradient norm: 0.246755
=== Actor Training Debug (Iteration 3136) ===
Q mean: -50.724022
Q std: 17.465714
Actor loss: 50.727993
Action reg: 0.003971
  l1.weight: grad_norm = 0.025824
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.066606
Total gradient norm: 0.118490
=== Actor Training Debug (Iteration 3137) ===
Q mean: -52.817806
Q std: 18.858252
Actor loss: 52.821796
Action reg: 0.003990
  l1.weight: grad_norm = 0.059706
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.122648
Total gradient norm: 0.205641
=== Actor Training Debug (Iteration 3138) ===
Q mean: -52.106361
Q std: 19.376617
Actor loss: 52.110344
Action reg: 0.003984
  l1.weight: grad_norm = 0.071081
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.158955
Total gradient norm: 0.328502
=== Actor Training Debug (Iteration 3139) ===
Q mean: -53.234970
Q std: 19.733822
Actor loss: 53.238941
Action reg: 0.003973
  l1.weight: grad_norm = 0.037382
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.075692
Total gradient norm: 0.132152
=== Actor Training Debug (Iteration 3140) ===
Q mean: -51.716042
Q std: 19.778503
Actor loss: 51.720013
Action reg: 0.003971
  l1.weight: grad_norm = 0.003529
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.008421
Total gradient norm: 0.016627
=== Actor Training Debug (Iteration 3141) ===
Q mean: -49.976440
Q std: 19.605623
Actor loss: 49.980415
Action reg: 0.003973
  l1.weight: grad_norm = 0.051492
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.111375
Total gradient norm: 0.159738
=== Actor Training Debug (Iteration 3142) ===
Q mean: -54.125481
Q std: 16.524200
Actor loss: 54.129471
Action reg: 0.003990
  l1.weight: grad_norm = 0.080280
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.187486
Total gradient norm: 0.359676
=== Actor Training Debug (Iteration 3143) ===
Q mean: -51.787247
Q std: 17.673929
Actor loss: 51.791237
Action reg: 0.003992
  l1.weight: grad_norm = 0.081156
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.216403
Total gradient norm: 0.389270
=== Actor Training Debug (Iteration 3144) ===
Q mean: -54.039963
Q std: 19.411816
Actor loss: 54.043938
Action reg: 0.003977
  l1.weight: grad_norm = 0.085719
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.199947
Total gradient norm: 0.372742
=== Actor Training Debug (Iteration 3145) ===
Q mean: -49.576778
Q std: 17.829924
Actor loss: 49.580753
Action reg: 0.003974
  l1.weight: grad_norm = 0.103316
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.276453
Total gradient norm: 0.581032
=== Actor Training Debug (Iteration 3146) ===
Q mean: -52.588200
Q std: 17.994417
Actor loss: 52.592178
Action reg: 0.003978
  l1.weight: grad_norm = 0.004044
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.010208
Total gradient norm: 0.020224
=== Actor Training Debug (Iteration 3147) ===
Q mean: -52.093132
Q std: 18.827311
Actor loss: 52.097115
Action reg: 0.003981
  l1.weight: grad_norm = 0.046822
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.100185
Total gradient norm: 0.175502
=== Actor Training Debug (Iteration 3148) ===
Q mean: -53.520176
Q std: 19.588186
Actor loss: 53.524158
Action reg: 0.003983
  l1.weight: grad_norm = 0.039734
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.090798
Total gradient norm: 0.164306
=== Actor Training Debug (Iteration 3149) ===
Q mean: -50.231602
Q std: 17.429520
Actor loss: 50.235569
Action reg: 0.003968
  l1.weight: grad_norm = 0.088522
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.176435
Total gradient norm: 0.290401
=== Actor Training Debug (Iteration 3150) ===
Q mean: -52.055313
Q std: 19.229322
Actor loss: 52.059269
Action reg: 0.003954
  l1.weight: grad_norm = 0.019372
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.042928
Total gradient norm: 0.082441
=== Actor Training Debug (Iteration 3151) ===
Q mean: -52.937057
Q std: 18.432159
Actor loss: 52.941040
Action reg: 0.003981
  l1.weight: grad_norm = 0.247349
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.646240
Total gradient norm: 1.198211
=== Actor Training Debug (Iteration 3152) ===
Q mean: -52.329605
Q std: 18.443933
Actor loss: 52.333603
Action reg: 0.003996
  l1.weight: grad_norm = 0.078777
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.213058
Total gradient norm: 0.425810
=== Actor Training Debug (Iteration 3153) ===
Q mean: -51.384602
Q std: 20.221142
Actor loss: 51.388580
Action reg: 0.003979
  l1.weight: grad_norm = 0.000979
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.002730
Total gradient norm: 0.006813
=== Actor Training Debug (Iteration 3154) ===
Q mean: -50.630543
Q std: 18.774269
Actor loss: 50.634529
Action reg: 0.003987
  l1.weight: grad_norm = 0.074867
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.162180
Total gradient norm: 0.260385
=== Actor Training Debug (Iteration 3155) ===
Q mean: -53.105286
Q std: 18.989197
Actor loss: 53.109272
Action reg: 0.003986
  l1.weight: grad_norm = 0.133746
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.288755
Total gradient norm: 0.526572
=== Actor Training Debug (Iteration 3156) ===
Q mean: -51.255508
Q std: 17.847490
Actor loss: 51.259499
Action reg: 0.003991
  l1.weight: grad_norm = 0.094554
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.208893
Total gradient norm: 0.370906
=== Actor Training Debug (Iteration 3157) ===
Q mean: -53.111191
Q std: 17.971432
Actor loss: 53.115170
Action reg: 0.003978
  l1.weight: grad_norm = 0.000974
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.002117
Total gradient norm: 0.003923
=== Actor Training Debug (Iteration 3158) ===
Q mean: -53.195831
Q std: 18.823519
Actor loss: 53.199821
Action reg: 0.003991
  l1.weight: grad_norm = 0.037504
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.092785
Total gradient norm: 0.187462
=== Actor Training Debug (Iteration 3159) ===
Q mean: -50.971588
Q std: 17.196939
Actor loss: 50.975571
Action reg: 0.003984
  l1.weight: grad_norm = 0.039331
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.082455
Total gradient norm: 0.154868
=== Actor Training Debug (Iteration 3160) ===
Q mean: -51.220119
Q std: 19.619768
Actor loss: 51.224098
Action reg: 0.003980
  l1.weight: grad_norm = 0.000490
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.001478
Total gradient norm: 0.004555
=== Actor Training Debug (Iteration 3161) ===
Q mean: -53.303329
Q std: 17.054886
Actor loss: 53.307327
Action reg: 0.003996
  l1.weight: grad_norm = 0.031972
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.068990
Total gradient norm: 0.128206
=== Actor Training Debug (Iteration 3162) ===
Q mean: -52.789925
Q std: 18.364176
Actor loss: 52.793911
Action reg: 0.003986
  l1.weight: grad_norm = 0.025022
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.052690
Total gradient norm: 0.107039
=== Actor Training Debug (Iteration 3163) ===
Q mean: -49.352943
Q std: 18.876635
Actor loss: 49.356922
Action reg: 0.003980
  l1.weight: grad_norm = 0.002528
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.006668
Total gradient norm: 0.014491
=== Actor Training Debug (Iteration 3164) ===
Q mean: -51.716148
Q std: 16.763941
Actor loss: 51.720127
Action reg: 0.003978
  l1.weight: grad_norm = 0.006257
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.014392
Total gradient norm: 0.023003
=== Actor Training Debug (Iteration 3165) ===
Q mean: -53.908863
Q std: 17.934166
Actor loss: 53.912834
Action reg: 0.003969
  l1.weight: grad_norm = 0.147443
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.363045
Total gradient norm: 0.637869
=== Actor Training Debug (Iteration 3166) ===
Q mean: -53.674522
Q std: 17.628700
Actor loss: 53.678505
Action reg: 0.003982
  l1.weight: grad_norm = 0.044298
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.087211
Total gradient norm: 0.150984
=== Actor Training Debug (Iteration 3167) ===
Q mean: -50.873585
Q std: 17.963823
Actor loss: 50.877567
Action reg: 0.003984
  l1.weight: grad_norm = 0.187640
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.413211
Total gradient norm: 0.717140
=== Actor Training Debug (Iteration 3168) ===
Q mean: -49.977856
Q std: 18.893562
Actor loss: 49.981838
Action reg: 0.003982
  l1.weight: grad_norm = 0.120581
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.282600
Total gradient norm: 0.581563
=== Actor Training Debug (Iteration 3169) ===
Q mean: -52.057243
Q std: 20.392265
Actor loss: 52.061203
Action reg: 0.003961
  l1.weight: grad_norm = 0.017008
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.046191
Total gradient norm: 0.091685
=== Actor Training Debug (Iteration 3170) ===
Q mean: -50.758308
Q std: 19.314283
Actor loss: 50.762253
Action reg: 0.003943
  l1.weight: grad_norm = 0.180160
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.410706
Total gradient norm: 0.678548
=== Actor Training Debug (Iteration 3171) ===
Q mean: -52.487865
Q std: 17.505003
Actor loss: 52.491837
Action reg: 0.003973
  l1.weight: grad_norm = 0.108430
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.269554
Total gradient norm: 0.582837
=== Actor Training Debug (Iteration 3172) ===
Q mean: -52.778191
Q std: 17.844557
Actor loss: 52.782173
Action reg: 0.003982
  l1.weight: grad_norm = 0.020877
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.049649
Total gradient norm: 0.093594
=== Actor Training Debug (Iteration 3173) ===
Q mean: -53.059898
Q std: 18.089849
Actor loss: 53.063869
Action reg: 0.003973
  l1.weight: grad_norm = 0.045476
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.124814
Total gradient norm: 0.234626
=== Actor Training Debug (Iteration 3174) ===
Q mean: -55.452820
Q std: 18.714235
Actor loss: 55.456787
Action reg: 0.003968
  l1.weight: grad_norm = 0.090712
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.213413
Total gradient norm: 0.416664
=== Actor Training Debug (Iteration 3175) ===
Q mean: -50.469391
Q std: 18.494436
Actor loss: 50.473381
Action reg: 0.003991
  l1.weight: grad_norm = 0.018100
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.030896
Total gradient norm: 0.046619
=== Actor Training Debug (Iteration 3176) ===
Q mean: -48.923424
Q std: 17.814852
Actor loss: 48.927410
Action reg: 0.003986
  l1.weight: grad_norm = 0.027239
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.060287
Total gradient norm: 0.089872
=== Actor Training Debug (Iteration 3177) ===
Q mean: -51.969925
Q std: 18.146252
Actor loss: 51.973896
Action reg: 0.003970
  l1.weight: grad_norm = 0.098488
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.245094
Total gradient norm: 0.524493
=== Actor Training Debug (Iteration 3178) ===
Q mean: -56.164127
Q std: 20.199526
Actor loss: 56.168098
Action reg: 0.003971
  l1.weight: grad_norm = 0.025193
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.061910
Total gradient norm: 0.109274
=== Actor Training Debug (Iteration 3179) ===
Q mean: -53.033409
Q std: 19.979456
Actor loss: 53.037380
Action reg: 0.003971
  l1.weight: grad_norm = 0.021434
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.050109
Total gradient norm: 0.103419
=== Actor Training Debug (Iteration 3180) ===
Q mean: -53.491055
Q std: 17.061884
Actor loss: 53.495049
Action reg: 0.003993
  l1.weight: grad_norm = 0.152574
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.407391
Total gradient norm: 0.886864
=== Actor Training Debug (Iteration 3181) ===
Q mean: -52.519279
Q std: 18.242737
Actor loss: 52.523266
Action reg: 0.003985
  l1.weight: grad_norm = 0.020714
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.046012
Total gradient norm: 0.086663
=== Actor Training Debug (Iteration 3182) ===
Q mean: -52.222137
Q std: 19.537632
Actor loss: 52.226128
Action reg: 0.003989
  l1.weight: grad_norm = 0.072443
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.211623
Total gradient norm: 0.362338
=== Actor Training Debug (Iteration 3183) ===
Q mean: -53.273590
Q std: 18.774313
Actor loss: 53.277565
Action reg: 0.003974
  l1.weight: grad_norm = 0.003669
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.009659
Total gradient norm: 0.022372
=== Actor Training Debug (Iteration 3184) ===
Q mean: -53.017723
Q std: 17.709656
Actor loss: 53.021721
Action reg: 0.003998
  l1.weight: grad_norm = 0.083046
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.197820
Total gradient norm: 0.384726
=== Actor Training Debug (Iteration 3185) ===
Q mean: -50.297733
Q std: 18.071333
Actor loss: 50.301723
Action reg: 0.003990
  l1.weight: grad_norm = 0.000775
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.001875
Total gradient norm: 0.003862
=== Actor Training Debug (Iteration 3186) ===
Q mean: -52.502663
Q std: 18.016979
Actor loss: 52.506657
Action reg: 0.003993
  l1.weight: grad_norm = 0.203186
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.492532
Total gradient norm: 0.829388
=== Actor Training Debug (Iteration 3187) ===
Q mean: -50.756760
Q std: 18.672863
Actor loss: 50.760742
Action reg: 0.003983
  l1.weight: grad_norm = 0.029385
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.070391
Total gradient norm: 0.130725
Total gradient norm: 0.23332392tion 1203) ===
=== Actor Training Debug (Iteration 3198) ===
Q mean: -53.596066
Q std: 19.441557
Actor loss: 53.600029
Action reg: 0.003963
  l1.weight: grad_norm = 0.023596
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.051208
Total gradient norm: 0.084974
=== Actor Training Debug (Iteration 3199) ===
Q mean: -52.870453
Q std: 18.895760
Actor loss: 52.874420
Action reg: 0.003966
  l1.weight: grad_norm = 0.066921
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.169676
Total gradient norm: 0.331997
=== Actor Training Debug (Iteration 3200) ===
Q mean: -52.453384
Q std: 18.918365
Actor loss: 52.457363
Action reg: 0.003979
  l1.weight: grad_norm = 0.083391
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.189444
Total gradient norm: 0.381102
=== Actor Training Debug (Iteration 3201) ===
Q mean: -53.374367
Q std: 20.013500
Actor loss: 53.378334
Action reg: 0.003968
  l1.weight: grad_norm = 0.049319
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.115437
Total gradient norm: 0.201028
=== Actor Training Debug (Iteration 3202) ===
Q mean: -50.412777
Q std: 18.090431
Actor loss: 50.416763
Action reg: 0.003986
  l1.weight: grad_norm = 0.046584
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.103209
Total gradient norm: 0.207654
=== Actor Training Debug (Iteration 3203) ===
Q mean: -52.372383
Q std: 19.054085
Actor loss: 52.376377
Action reg: 0.003994
  l1.weight: grad_norm = 0.012893
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.030138
Total gradient norm: 0.062233
=== Actor Training Debug (Iteration 3204) ===
Q mean: -53.166138
Q std: 17.918776
Actor loss: 53.170128
Action reg: 0.003991
  l1.weight: grad_norm = 0.022296
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.053957
Total gradient norm: 0.099466
=== Actor Training Debug (Iteration 3205) ===
Q mean: -51.403938
Q std: 18.335506
Actor loss: 51.407921
Action reg: 0.003983
  l1.weight: grad_norm = 0.095648
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.250627
Total gradient norm: 0.542291
=== Actor Training Debug (Iteration 3206) ===
Q mean: -52.381180
Q std: 18.813114
Actor loss: 52.385170
Action reg: 0.003990
  l1.weight: grad_norm = 0.027270
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.060426
Total gradient norm: 0.113733
=== Actor Training Debug (Iteration 3207) ===
Q mean: -51.890865
Q std: 18.252029
Actor loss: 51.894836
Action reg: 0.003969
  l1.weight: grad_norm = 0.050531
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.130541
Total gradient norm: 0.220533
=== Actor Training Debug (Iteration 3208) ===
Q mean: -49.954212
Q std: 18.922556
Actor loss: 49.958191
Action reg: 0.003980
  l1.weight: grad_norm = 0.000463
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.001566
Total gradient norm: 0.005124
=== Actor Training Debug (Iteration 3209) ===
Q mean: -53.362068
Q std: 19.070967
Actor loss: 53.366055
Action reg: 0.003985
  l1.weight: grad_norm = 0.061183
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.140717
Total gradient norm: 0.254320
=== Actor Training Debug (Iteration 3210) ===
Q mean: -54.363953
Q std: 17.389904
Actor loss: 54.367931
Action reg: 0.003979
  l1.weight: grad_norm = 0.003700
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.008966
Total gradient norm: 0.018671
=== Actor Training Debug (Iteration 3211) ===
Q mean: -55.289429
Q std: 18.388512
Actor loss: 55.293427
Action reg: 0.004000
  l1.weight: grad_norm = 0.012686
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.030181
Total gradient norm: 0.056373
=== Actor Training Debug (Iteration 3212) ===
Q mean: -54.016521
Q std: 19.676121
Actor loss: 54.020512
Action reg: 0.003990
  l1.weight: grad_norm = 0.061552
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.142285
Total gradient norm: 0.252113
=== Actor Training Debug (Iteration 3213) ===
Q mean: -51.213020
Q std: 17.437668
Actor loss: 51.217003
Action reg: 0.003981
  l1.weight: grad_norm = 0.153479
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.353339
Total gradient norm: 0.599454
=== Actor Training Debug (Iteration 3214) ===
Q mean: -51.004921
Q std: 18.664398
Actor loss: 51.008907
Action reg: 0.003985
  l1.weight: grad_norm = 0.040172
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.101302
Total gradient norm: 0.168131
=== Actor Training Debug (Iteration 3215) ===
Q mean: -52.973198
Q std: 18.573170
Actor loss: 52.977184
Action reg: 0.003984
  l1.weight: grad_norm = 0.143425
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.342810
Total gradient norm: 0.670412
=== Actor Training Debug (Iteration 3216) ===
Q mean: -55.420792
Q std: 19.268785
Actor loss: 55.424774
Action reg: 0.003981
  l1.weight: grad_norm = 0.001821
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.004123
Total gradient norm: 0.006799
=== Actor Training Debug (Iteration 3217) ===
Q mean: -54.220592
Q std: 18.785618
Actor loss: 54.224575
Action reg: 0.003981
  l1.weight: grad_norm = 0.027706
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.059572
Total gradient norm: 0.104219
=== Actor Training Debug (Iteration 3218) ===
Q mean: -52.573376
Q std: 19.563341
Actor loss: 52.577347
Action reg: 0.003969
  l1.weight: grad_norm = 0.086225
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.184363
Total gradient norm: 0.289242
=== Actor Training Debug (Iteration 3219) ===
Q mean: -53.548050
Q std: 18.769329
Actor loss: 53.552032
Action reg: 0.003983
  l1.weight: grad_norm = 0.026732
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.075911
Total gradient norm: 0.175393
=== Actor Training Debug (Iteration 3220) ===
Q mean: -52.434719
Q std: 18.719755
Actor loss: 52.438705
Action reg: 0.003985
  l1.weight: grad_norm = 0.004643
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.012392
Total gradient norm: 0.024045
=== Actor Training Debug (Iteration 3221) ===
Q mean: -54.442154
Q std: 17.633280
Actor loss: 54.446136
Action reg: 0.003982
  l1.weight: grad_norm = 0.064941
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.140377
Total gradient norm: 0.221210
=== Actor Training Debug (Iteration 3222) ===
Q mean: -51.932278
Q std: 18.432861
Actor loss: 51.936253
Action reg: 0.003974
  l1.weight: grad_norm = 0.022368
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.054156
Total gradient norm: 0.113936
=== Actor Training Debug (Iteration 3223) ===
Q mean: -53.339027
Q std: 17.917685
Actor loss: 53.343021
Action reg: 0.003994
  l1.weight: grad_norm = 0.022781
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.053527
Total gradient norm: 0.120618
=== Actor Training Debug (Iteration 3224) ===
Q mean: -52.493385
Q std: 17.572311
Actor loss: 52.497372
Action reg: 0.003985
  l1.weight: grad_norm = 0.045765
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.112104
Total gradient norm: 0.223724
=== Actor Training Debug (Iteration 3225) ===
Q mean: -52.541130
Q std: 19.090103
Actor loss: 52.545116
Action reg: 0.003987
  l1.weight: grad_norm = 0.211166
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.501947
Total gradient norm: 0.903482
=== Actor Training Debug (Iteration 3226) ===
Q mean: -52.411507
Q std: 17.601835
Actor loss: 52.415501
Action reg: 0.003993
  l1.weight: grad_norm = 0.008405
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.021090
Total gradient norm: 0.036474
=== Actor Training Debug (Iteration 3227) ===
Q mean: -52.576263
Q std: 19.703789
Actor loss: 52.580227
Action reg: 0.003964
  l1.weight: grad_norm = 0.005799
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.016881
Total gradient norm: 0.031465
=== Actor Training Debug (Iteration 3228) ===
Q mean: -52.050423
Q std: 18.388723
Actor loss: 52.054401
Action reg: 0.003978
  l1.weight: grad_norm = 0.135648
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.258509
Total gradient norm: 0.369664
=== Actor Training Debug (Iteration 3229) ===
Q mean: -52.970795
Q std: 17.813650
Actor loss: 52.974762
Action reg: 0.003969
  l1.weight: grad_norm = 0.076906
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.176391
Total gradient norm: 0.336860
=== Actor Training Debug (Iteration 3230) ===
Q mean: -52.355495
Q std: 19.145874
Actor loss: 52.359463
Action reg: 0.003966
  l1.weight: grad_norm = 0.005300
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.013890
Total gradient norm: 0.026758
=== Actor Training Debug (Iteration 3231) ===
Q mean: -55.195480
Q std: 19.569603
Actor loss: 55.199471
Action reg: 0.003989
  l1.weight: grad_norm = 0.002918
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.007108
Total gradient norm: 0.014555
=== Actor Training Debug (Iteration 3232) ===
Q mean: -54.431004
Q std: 19.872797
Actor loss: 54.434990
Action reg: 0.003986
  l1.weight: grad_norm = 0.010097
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.024277
Total gradient norm: 0.053536
=== Actor Training Debug (Iteration 3233) ===
Q mean: -52.577885
Q std: 19.076324
Actor loss: 52.581860
Action reg: 0.003974
  l1.weight: grad_norm = 0.000340
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.000956
Total gradient norm: 0.002672
=== Actor Training Debug (Iteration 3234) ===
Q mean: -49.993187
Q std: 20.393517
Actor loss: 49.997143
Action reg: 0.003956
  l1.weight: grad_norm = 0.032405
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.077312
Total gradient norm: 0.127158
=== Actor Training Debug (Iteration 3235) ===
Q mean: -52.294781
Q std: 18.017431
Actor loss: 52.298767
Action reg: 0.003985
  l1.weight: grad_norm = 0.061499
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.135839
Total gradient norm: 0.253129
=== Actor Training Debug (Iteration 3236) ===
Q mean: -54.036667
Q std: 17.908535
Actor loss: 54.040642
Action reg: 0.003976
  l1.weight: grad_norm = 0.080855
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.185311
Total gradient norm: 0.343599
=== Actor Training Debug (Iteration 3237) ===
Q mean: -53.053242
Q std: 17.474142
Actor loss: 53.057240
Action reg: 0.003998
  l1.weight: grad_norm = 0.067265
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.177159
Total gradient norm: 0.348722
=== Actor Training Debug (Iteration 3238) ===
Q mean: -51.568466
Q std: 16.781189
Actor loss: 51.572453
Action reg: 0.003987
  l1.weight: grad_norm = 0.018924
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.045284
Total gradient norm: 0.098028
=== Actor Training Debug (Iteration 3239) ===
Q mean: -50.915123
Q std: 18.007648
Actor loss: 50.919106
Action reg: 0.003981
  l1.weight: grad_norm = 0.084171
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.193641
Total gradient norm: 0.337653
=== Actor Training Debug (Iteration 3240) ===
Q mean: -51.119072
Q std: 20.184277
Actor loss: 51.123032
Action reg: 0.003959
  l1.weight: grad_norm = 0.025905
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.061361
Total gradient norm: 0.102754
=== Actor Training Debug (Iteration 3241) ===
Q mean: -55.679115
Q std: 18.902210
Actor loss: 55.683113
Action reg: 0.003999
  l1.weight: grad_norm = 0.007163
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.017406
Total gradient norm: 0.034824
=== Actor Training Debug (Iteration 3242) ===
Q mean: -51.892090
Q std: 19.017664
Actor loss: 51.896069
Action reg: 0.003980
  l1.weight: grad_norm = 0.001420
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.003410
Total gradient norm: 0.009096
Total gradient norm: 0.14888492tion 1203) ===
=== Actor Training Debug (Iteration 3253) ===
Q mean: -53.390388
Q std: 20.353592
Actor loss: 53.394352
Action reg: 0.003964
  l1.weight: grad_norm = 0.003157
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.007635
Total gradient norm: 0.013788
=== Actor Training Debug (Iteration 3254) ===
Q mean: -51.321674
Q std: 18.418056
Actor loss: 51.325668
Action reg: 0.003992
  l1.weight: grad_norm = 0.000295
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.000857
Total gradient norm: 0.001876
=== Actor Training Debug (Iteration 3255) ===
Q mean: -55.034195
Q std: 19.802441
Actor loss: 55.038166
Action reg: 0.003971
  l1.weight: grad_norm = 0.125487
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.304597
Total gradient norm: 0.535804
=== Actor Training Debug (Iteration 3256) ===
Q mean: -53.119469
Q std: 18.609726
Actor loss: 53.123459
Action reg: 0.003992
  l1.weight: grad_norm = 0.037186
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.091170
Total gradient norm: 0.163428
=== Actor Training Debug (Iteration 3257) ===
Q mean: -52.004356
Q std: 19.996380
Actor loss: 52.008339
Action reg: 0.003983
  l1.weight: grad_norm = 0.066010
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.173948
Total gradient norm: 0.321503
=== Actor Training Debug (Iteration 3258) ===
Q mean: -51.465103
Q std: 17.812561
Actor loss: 51.469078
Action reg: 0.003974
  l1.weight: grad_norm = 0.044905
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.103404
Total gradient norm: 0.198098
=== Actor Training Debug (Iteration 3259) ===
Q mean: -55.296921
Q std: 18.928059
Actor loss: 55.300907
Action reg: 0.003987
  l1.weight: grad_norm = 0.006763
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.017020
Total gradient norm: 0.034700
=== Actor Training Debug (Iteration 3260) ===
Q mean: -56.414608
Q std: 18.087416
Actor loss: 56.418610
Action reg: 0.004000
  l1.weight: grad_norm = 0.000166
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000374
Total gradient norm: 0.000760
=== Actor Training Debug (Iteration 3261) ===
Q mean: -53.280296
Q std: 17.903189
Actor loss: 53.284290
Action reg: 0.003993
  l1.weight: grad_norm = 0.042773
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.103201
Total gradient norm: 0.177104
=== Actor Training Debug (Iteration 3262) ===
Q mean: -50.304913
Q std: 18.145699
Actor loss: 50.308914
Action reg: 0.004000
  l1.weight: grad_norm = 0.006191
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.016149
Total gradient norm: 0.032148
=== Actor Training Debug (Iteration 3263) ===
Q mean: -51.550571
Q std: 21.129471
Actor loss: 51.554546
Action reg: 0.003974
  l1.weight: grad_norm = 0.028336
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.061486
Total gradient norm: 0.121998
=== Actor Training Debug (Iteration 3264) ===
Q mean: -54.409760
Q std: 19.300671
Actor loss: 54.413761
Action reg: 0.004000
  l1.weight: grad_norm = 0.000098
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000223
Total gradient norm: 0.000410
=== Actor Training Debug (Iteration 3265) ===
Q mean: -53.857841
Q std: 18.944061
Actor loss: 53.861824
Action reg: 0.003984
  l1.weight: grad_norm = 0.051986
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.123137
Total gradient norm: 0.249480
=== Actor Training Debug (Iteration 3266) ===
Q mean: -54.231041
Q std: 17.456211
Actor loss: 54.235031
Action reg: 0.003990
  l1.weight: grad_norm = 0.171120
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.412221
Total gradient norm: 0.680884
=== Actor Training Debug (Iteration 3267) ===
Q mean: -50.863457
Q std: 18.495096
Actor loss: 50.867428
Action reg: 0.003971
  l1.weight: grad_norm = 0.000998
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.002419
Total gradient norm: 0.006426
=== Actor Training Debug (Iteration 3268) ===
Q mean: -52.903320
Q std: 16.245443
Actor loss: 52.907318
Action reg: 0.003999
  l1.weight: grad_norm = 0.028432
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.064341
Total gradient norm: 0.126610
=== Actor Training Debug (Iteration 3269) ===
Q mean: -55.720375
Q std: 18.424526
Actor loss: 55.724361
Action reg: 0.003986
  l1.weight: grad_norm = 0.002988
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.006989
Total gradient norm: 0.014053
=== Actor Training Debug (Iteration 3270) ===
Q mean: -53.347729
Q std: 18.509836
Actor loss: 53.351723
Action reg: 0.003993
  l1.weight: grad_norm = 0.036133
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.090360
Total gradient norm: 0.183861
=== Actor Training Debug (Iteration 3271) ===
Q mean: -53.366177
Q std: 19.069372
Actor loss: 53.370152
Action reg: 0.003975
  l1.weight: grad_norm = 0.253459
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.561382
Total gradient norm: 0.983998
=== Actor Training Debug (Iteration 3272) ===
Q mean: -52.583603
Q std: 18.693998
Actor loss: 52.587597
Action reg: 0.003993
  l1.weight: grad_norm = 0.008896
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.021852
Total gradient norm: 0.049443
=== Actor Training Debug (Iteration 3273) ===
Q mean: -52.456795
Q std: 18.451283
Actor loss: 52.460766
Action reg: 0.003971
  l1.weight: grad_norm = 0.124296
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.270825
Total gradient norm: 0.525754
=== Actor Training Debug (Iteration 3274) ===
Q mean: -52.811897
Q std: 17.894020
Actor loss: 52.815872
Action reg: 0.003977
  l1.weight: grad_norm = 0.037457
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.085190
Total gradient norm: 0.178917
=== Actor Training Debug (Iteration 3275) ===
Q mean: -53.982826
Q std: 17.768616
Actor loss: 53.986824
Action reg: 0.003996
  l1.weight: grad_norm = 0.046604
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.116336
Total gradient norm: 0.254359
=== Actor Training Debug (Iteration 3276) ===
Q mean: -50.987099
Q std: 17.865629
Actor loss: 50.991077
Action reg: 0.003978
  l1.weight: grad_norm = 0.047877
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.120240
Total gradient norm: 0.233610
=== Actor Training Debug (Iteration 3277) ===
Q mean: -53.481766
Q std: 19.008186
Actor loss: 53.485729
Action reg: 0.003962
  l1.weight: grad_norm = 0.159524
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.314187
Total gradient norm: 0.454609
=== Actor Training Debug (Iteration 3278) ===
Q mean: -54.023369
Q std: 18.296377
Actor loss: 54.027336
Action reg: 0.003969
  l1.weight: grad_norm = 0.107245
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.255914
Total gradient norm: 0.430642
=== Actor Training Debug (Iteration 3279) ===
Q mean: -54.847244
Q std: 17.227077
Actor loss: 54.851231
Action reg: 0.003985
  l1.weight: grad_norm = 0.058782
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.140449
Total gradient norm: 0.266593
=== Actor Training Debug (Iteration 3280) ===
Q mean: -50.128197
Q std: 19.756845
Actor loss: 50.132164
Action reg: 0.003965
  l1.weight: grad_norm = 0.040102
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.086592
Total gradient norm: 0.141491
=== Actor Training Debug (Iteration 3281) ===
Q mean: -51.675072
Q std: 17.459364
Actor loss: 51.679054
Action reg: 0.003981
  l1.weight: grad_norm = 0.027710
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.057469
Total gradient norm: 0.088082
=== Actor Training Debug (Iteration 3282) ===
Q mean: -52.619415
Q std: 18.928587
Actor loss: 52.623390
Action reg: 0.003975
  l1.weight: grad_norm = 0.034238
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.083511
Total gradient norm: 0.159864
=== Actor Training Debug (Iteration 3283) ===
Q mean: -55.650276
Q std: 19.470253
Actor loss: 55.654259
Action reg: 0.003982
  l1.weight: grad_norm = 0.001757
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.004067
Total gradient norm: 0.007102
=== Actor Training Debug (Iteration 3284) ===
Q mean: -55.144203
Q std: 19.505781
Actor loss: 55.148178
Action reg: 0.003976
  l1.weight: grad_norm = 0.052229
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.102073
Total gradient norm: 0.139358
=== Actor Training Debug (Iteration 3285) ===
Q mean: -52.392101
Q std: 18.561829
Actor loss: 52.396065
Action reg: 0.003962
  l1.weight: grad_norm = 0.030324
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.076326
Total gradient norm: 0.154442
  l2.weight: grad_norm = 0.102637on 1203) ===
Total gradient norm: 0.226811
=== Actor Training Debug (Iteration 3296) ===
Q mean: -53.986961
Q std: 18.283718
Actor loss: 53.990952
Action reg: 0.003990
  l1.weight: grad_norm = 0.071686
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.148132
Total gradient norm: 0.307758
=== Actor Training Debug (Iteration 3297) ===
Q mean: -54.430279
Q std: 18.472622
Actor loss: 54.434235
Action reg: 0.003954
  l1.weight: grad_norm = 0.020412
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.045960
Total gradient norm: 0.096010
=== Actor Training Debug (Iteration 3298) ===
Q mean: -52.260216
Q std: 17.648378
Actor loss: 52.264202
Action reg: 0.003985
  l1.weight: grad_norm = 0.021205
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.045803
Total gradient norm: 0.091485
=== Actor Training Debug (Iteration 3299) ===
Q mean: -51.927750
Q std: 20.999548
Actor loss: 51.931721
Action reg: 0.003971
  l1.weight: grad_norm = 0.008202
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.020828
Total gradient norm: 0.044269
=== Actor Training Debug (Iteration 3300) ===
Q mean: -52.781601
Q std: 19.714705
Actor loss: 52.785572
Action reg: 0.003971
  l1.weight: grad_norm = 0.125517
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.240783
Total gradient norm: 0.336096
=== Actor Training Debug (Iteration 3301) ===
Q mean: -56.102852
Q std: 18.348894
Actor loss: 56.106834
Action reg: 0.003982
  l1.weight: grad_norm = 0.048299
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.103607
Total gradient norm: 0.196296
=== Actor Training Debug (Iteration 3302) ===
Q mean: -51.411797
Q std: 17.993551
Actor loss: 51.415791
Action reg: 0.003993
  l1.weight: grad_norm = 0.010856
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.024754
Total gradient norm: 0.047899
=== Actor Training Debug (Iteration 3303) ===
Q mean: -51.426395
Q std: 17.564159
Actor loss: 51.430378
Action reg: 0.003981
  l1.weight: grad_norm = 0.024066
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.049516
Total gradient norm: 0.087453
=== Actor Training Debug (Iteration 3304) ===
Q mean: -55.216728
Q std: 19.835316
Actor loss: 55.220684
Action reg: 0.003957
  l1.weight: grad_norm = 0.080890
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.202300
Total gradient norm: 0.432423
=== Actor Training Debug (Iteration 3305) ===
Q mean: -53.931114
Q std: 17.934021
Actor loss: 53.935093
Action reg: 0.003978
  l1.weight: grad_norm = 0.012354
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.037659
Total gradient norm: 0.076464
=== Actor Training Debug (Iteration 3306) ===
Q mean: -51.827263
Q std: 18.120819
Actor loss: 51.831245
Action reg: 0.003981
  l1.weight: grad_norm = 0.029645
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.070897
Total gradient norm: 0.150867
=== Actor Training Debug (Iteration 3307) ===
Q mean: -54.646778
Q std: 18.211641
Actor loss: 54.650772
Action reg: 0.003993
  l1.weight: grad_norm = 0.000229
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.000479
Total gradient norm: 0.001329
=== Actor Training Debug (Iteration 3308) ===
Q mean: -52.620697
Q std: 18.147856
Actor loss: 52.624676
Action reg: 0.003979
  l1.weight: grad_norm = 0.043422
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.090041
Total gradient norm: 0.166986
=== Actor Training Debug (Iteration 3309) ===
Q mean: -51.763725
Q std: 17.724150
Actor loss: 51.767715
Action reg: 0.003991
  l1.weight: grad_norm = 0.027405
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.064388
Total gradient norm: 0.109062
=== Actor Training Debug (Iteration 3310) ===
Q mean: -53.469627
Q std: 18.806143
Actor loss: 53.473606
Action reg: 0.003979
  l1.weight: grad_norm = 0.006920
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.015801
Total gradient norm: 0.032404
=== Actor Training Debug (Iteration 3311) ===
Q mean: -51.165115
Q std: 18.496954
Actor loss: 51.169079
Action reg: 0.003964
  l1.weight: grad_norm = 0.470261
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.980475
Total gradient norm: 1.825594
=== Actor Training Debug (Iteration 3312) ===
Q mean: -52.710155
Q std: 19.426464
Actor loss: 52.714142
Action reg: 0.003986
  l1.weight: grad_norm = 0.009957
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.022123
Total gradient norm: 0.037168
=== Actor Training Debug (Iteration 3313) ===
Q mean: -52.091873
Q std: 19.938927
Actor loss: 52.095848
Action reg: 0.003976
  l1.weight: grad_norm = 0.037066
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.083712
Total gradient norm: 0.148486
=== Actor Training Debug (Iteration 3314) ===
Q mean: -51.270660
Q std: 19.778841
Actor loss: 51.274632
Action reg: 0.003971
  l1.weight: grad_norm = 0.112142
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.255588
Total gradient norm: 0.556022
=== Actor Training Debug (Iteration 3315) ===
Q mean: -52.438183
Q std: 17.523241
Actor loss: 52.442173
Action reg: 0.003991
  l1.weight: grad_norm = 0.188111
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.429811
Total gradient norm: 0.732444
=== Actor Training Debug (Iteration 3316) ===
Q mean: -53.033180
Q std: 18.291000
Actor loss: 53.037174
Action reg: 0.003993
  l1.weight: grad_norm = 0.016229
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.035539
Total gradient norm: 0.074438
=== Actor Training Debug (Iteration 3317) ===
Q mean: -56.319244
Q std: 17.299032
Actor loss: 56.323242
Action reg: 0.003998
  l1.weight: grad_norm = 0.007994
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.017709
Total gradient norm: 0.040351
=== Actor Training Debug (Iteration 3318) ===
Q mean: -51.261009
Q std: 19.199440
Actor loss: 51.264988
Action reg: 0.003977
  l1.weight: grad_norm = 0.349251
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.806198
Total gradient norm: 1.419139
=== Actor Training Debug (Iteration 3319) ===
Q mean: -51.622822
Q std: 18.360533
Actor loss: 51.626816
Action reg: 0.003993
  l1.weight: grad_norm = 0.048237
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.106279
Total gradient norm: 0.186265
=== Actor Training Debug (Iteration 3320) ===
Q mean: -54.788433
Q std: 17.608082
Actor loss: 54.792431
Action reg: 0.003999
  l1.weight: grad_norm = 0.014092
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.035289
Total gradient norm: 0.062238
=== Actor Training Debug (Iteration 3321) ===
Q mean: -54.299019
Q std: 18.723494
Actor loss: 54.303005
Action reg: 0.003986
  l1.weight: grad_norm = 0.018330
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.037041
Total gradient norm: 0.076700
=== Actor Training Debug (Iteration 3322) ===
Q mean: -56.413094
Q std: 17.850750
Actor loss: 56.417084
Action reg: 0.003991
  l1.weight: grad_norm = 0.108452
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.230775
Total gradient norm: 0.515728
=== Actor Training Debug (Iteration 3323) ===
Q mean: -53.530869
Q std: 17.384693
Actor loss: 53.534863
Action reg: 0.003993
  l1.weight: grad_norm = 0.000711
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.001665
Total gradient norm: 0.003273
=== Actor Training Debug (Iteration 3324) ===
Q mean: -54.163574
Q std: 19.380272
Actor loss: 54.167549
Action reg: 0.003973
  l1.weight: grad_norm = 0.167112
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.367909
Total gradient norm: 0.639705
=== Actor Training Debug (Iteration 3325) ===
Q mean: -52.491776
Q std: 18.046995
Actor loss: 52.495754
Action reg: 0.003978
  l1.weight: grad_norm = 0.017296
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.046218
Total gradient norm: 0.088725
=== Actor Training Debug (Iteration 3326) ===
Q mean: -53.296509
Q std: 18.281561
Actor loss: 53.300488
Action reg: 0.003979
  l1.weight: grad_norm = 0.042765
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.095349
Total gradient norm: 0.205181
=== Actor Training Debug (Iteration 3327) ===
Q mean: -55.151524
Q std: 17.543224
Actor loss: 55.155510
Action reg: 0.003987
  l1.weight: grad_norm = 0.021785
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.052588
Total gradient norm: 0.115080
=== Actor Training Debug (Iteration 3328) ===
Q mean: -52.338448
Q std: 18.618546
Actor loss: 52.342426
Action reg: 0.003977
  l1.weight: grad_norm = 0.083458
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.211883
Total gradient norm: 0.457641
=== Actor Training Debug (Iteration 3329) ===
Q mean: -51.266708
Q std: 19.592188
Actor loss: 51.270683
Action reg: 0.003976
  l1.weight: grad_norm = 0.009853
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.023766
Total gradient norm: 0.044622
=== Actor Training Debug (Iteration 3330) ===
Q mean: -54.005325
Q std: 19.074533
Actor loss: 54.009312
Action reg: 0.003986
  l1.weight: grad_norm = 0.036848
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.080447
Total gradient norm: 0.175013
=== Actor Training Debug (Iteration 3331) ===
Q mean: -53.436630
Q std: 17.683619
Actor loss: 53.440620
Action reg: 0.003991
  l1.weight: grad_norm = 0.056833
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.130775
Total gradient norm: 0.279704
Total gradient norm: 0.0048532637on 1203) ===
=== Actor Training Debug (Iteration 3342) ===
Q mean: -51.423504
Q std: 17.943188
Actor loss: 51.427486
Action reg: 0.003981
  l1.weight: grad_norm = 0.025162
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.062975
Total gradient norm: 0.124312
=== Actor Training Debug (Iteration 3343) ===
Q mean: -53.091080
Q std: 17.909979
Actor loss: 53.095058
Action reg: 0.003980
  l1.weight: grad_norm = 0.011835
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.026326
Total gradient norm: 0.044163
=== Actor Training Debug (Iteration 3344) ===
Q mean: -56.190720
Q std: 20.114756
Actor loss: 56.194687
Action reg: 0.003968
  l1.weight: grad_norm = 0.033979
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.076281
Total gradient norm: 0.146877
=== Actor Training Debug (Iteration 3345) ===
Q mean: -54.596386
Q std: 20.465052
Actor loss: 54.600361
Action reg: 0.003974
  l1.weight: grad_norm = 0.019028
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.040674
Total gradient norm: 0.073213
=== Actor Training Debug (Iteration 3346) ===
Q mean: -48.423382
Q std: 18.560839
Actor loss: 48.427345
Action reg: 0.003965
  l1.weight: grad_norm = 0.083182
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.198861
Total gradient norm: 0.430865
=== Actor Training Debug (Iteration 3347) ===
Q mean: -52.023933
Q std: 19.420609
Actor loss: 52.027901
Action reg: 0.003966
  l1.weight: grad_norm = 0.019663
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.040704
Total gradient norm: 0.070673
=== Actor Training Debug (Iteration 3348) ===
Q mean: -54.277340
Q std: 18.374344
Actor loss: 54.281311
Action reg: 0.003970
  l1.weight: grad_norm = 0.108260
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.275796
Total gradient norm: 0.603082
=== Actor Training Debug (Iteration 3349) ===
Q mean: -54.138786
Q std: 18.209562
Actor loss: 54.142780
Action reg: 0.003995
  l1.weight: grad_norm = 0.000131
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.000463
Total gradient norm: 0.001425
=== Actor Training Debug (Iteration 3350) ===
Q mean: -53.741516
Q std: 18.159355
Actor loss: 53.745491
Action reg: 0.003976
  l1.weight: grad_norm = 0.133360
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.246836
Total gradient norm: 0.392883
=== Actor Training Debug (Iteration 3351) ===
Q mean: -51.047195
Q std: 16.373873
Actor loss: 51.051186
Action reg: 0.003990
  l1.weight: grad_norm = 0.042350
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.084514
Total gradient norm: 0.165081
=== Actor Training Debug (Iteration 3352) ===
Q mean: -53.352905
Q std: 17.995449
Actor loss: 53.356892
Action reg: 0.003986
  l1.weight: grad_norm = 0.005108
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.010903
Total gradient norm: 0.019884
=== Actor Training Debug (Iteration 3353) ===
Q mean: -56.141895
Q std: 18.539568
Actor loss: 56.145874
Action reg: 0.003980
  l1.weight: grad_norm = 0.074713
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.179058
Total gradient norm: 0.343992
=== Actor Training Debug (Iteration 3354) ===
Q mean: -52.831291
Q std: 20.299673
Actor loss: 52.835236
Action reg: 0.003943
  l1.weight: grad_norm = 0.084399
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.214145
Total gradient norm: 0.481482
=== Actor Training Debug (Iteration 3355) ===
Q mean: -52.654984
Q std: 19.274323
Actor loss: 52.658962
Action reg: 0.003978
  l1.weight: grad_norm = 0.057329
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.130680
Total gradient norm: 0.273892
=== Actor Training Debug (Iteration 3356) ===
Q mean: -51.589962
Q std: 19.119980
Actor loss: 51.593933
Action reg: 0.003973
  l1.weight: grad_norm = 0.012472
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.026534
Total gradient norm: 0.055847
=== Actor Training Debug (Iteration 3357) ===
Q mean: -53.949196
Q std: 19.951143
Actor loss: 53.953175
Action reg: 0.003979
  l1.weight: grad_norm = 0.035150
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.081239
Total gradient norm: 0.152926
=== Actor Training Debug (Iteration 3358) ===
Q mean: -53.591145
Q std: 18.685196
Actor loss: 53.595127
Action reg: 0.003984
  l1.weight: grad_norm = 0.021519
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.047298
Total gradient norm: 0.089250
=== Actor Training Debug (Iteration 3359) ===
Q mean: -54.333458
Q std: 17.230171
Actor loss: 54.337440
Action reg: 0.003984
  l1.weight: grad_norm = 0.036568
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.089786
Total gradient norm: 0.187107
=== Actor Training Debug (Iteration 3360) ===
Q mean: -50.832691
Q std: 17.850246
Actor loss: 50.836689
Action reg: 0.003996
  l1.weight: grad_norm = 0.040665
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.079073
Total gradient norm: 0.118149
=== Actor Training Debug (Iteration 3361) ===
Q mean: -54.899063
Q std: 18.826031
Actor loss: 54.903057
Action reg: 0.003994
  l1.weight: grad_norm = 0.000658
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.001594
Total gradient norm: 0.003042
=== Actor Training Debug (Iteration 3362) ===
Q mean: -52.464890
Q std: 18.840063
Actor loss: 52.468868
Action reg: 0.003979
  l1.weight: grad_norm = 0.036011
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.092577
Total gradient norm: 0.193234
=== Actor Training Debug (Iteration 3363) ===
Q mean: -53.639893
Q std: 19.690117
Actor loss: 53.643875
Action reg: 0.003981
  l1.weight: grad_norm = 0.071281
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.140675
Total gradient norm: 0.227155
=== Actor Training Debug (Iteration 3364) ===
Q mean: -51.448063
Q std: 17.923887
Actor loss: 51.452057
Action reg: 0.003993
  l1.weight: grad_norm = 0.135885
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.334682
Total gradient norm: 0.579180
=== Actor Training Debug (Iteration 3365) ===
Q mean: -54.028393
Q std: 16.868200
Actor loss: 54.032383
Action reg: 0.003989
  l1.weight: grad_norm = 0.086153
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.182539
Total gradient norm: 0.329470
=== Actor Training Debug (Iteration 3366) ===
Q mean: -56.417511
Q std: 18.031797
Actor loss: 56.421490
Action reg: 0.003977
  l1.weight: grad_norm = 0.015327
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.042788
Total gradient norm: 0.081399
=== Actor Training Debug (Iteration 3367) ===
Q mean: -54.930069
Q std: 18.627861
Actor loss: 54.934052
Action reg: 0.003983
  l1.weight: grad_norm = 0.083821
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.197615
Total gradient norm: 0.336197
=== Actor Training Debug (Iteration 3368) ===
Q mean: -51.223476
Q std: 17.634308
Actor loss: 51.227448
Action reg: 0.003970
  l1.weight: grad_norm = 0.064208
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.158185
Total gradient norm: 0.380241
=== Actor Training Debug (Iteration 3369) ===
Q mean: -52.700008
Q std: 18.438213
Actor loss: 52.703991
Action reg: 0.003984
  l1.weight: grad_norm = 0.188958
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.450994
Total gradient norm: 0.882341
=== Actor Training Debug (Iteration 3370) ===
Q mean: -54.417439
Q std: 18.922749
Actor loss: 54.421421
Action reg: 0.003983
  l1.weight: grad_norm = 0.049549
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.110138
Total gradient norm: 0.195495
=== Actor Training Debug (Iteration 3371) ===
Q mean: -55.716175
Q std: 19.935688
Actor loss: 55.720165
Action reg: 0.003989
  l1.weight: grad_norm = 0.000171
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.000601
Total gradient norm: 0.001901
=== Actor Training Debug (Iteration 3372) ===
Q mean: -51.418411
Q std: 18.383535
Actor loss: 51.422390
Action reg: 0.003977
  l1.weight: grad_norm = 0.018815
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.040172
Total gradient norm: 0.081006
=== Actor Training Debug (Iteration 3373) ===
Q mean: -52.715286
Q std: 18.634569
Actor loss: 52.719276
Action reg: 0.003992
  l1.weight: grad_norm = 0.011794
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.028217
Total gradient norm: 0.062020
=== Actor Training Debug (Iteration 3374) ===
Q mean: -55.399082
Q std: 17.523901
Actor loss: 55.403069
Action reg: 0.003986
  l1.weight: grad_norm = 0.021344
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.047326
Total gradient norm: 0.093889
=== Actor Training Debug (Iteration 3375) ===
Q mean: -52.955425
Q std: 18.395050
Actor loss: 52.959381
Action reg: 0.003957
  l1.weight: grad_norm = 0.022679
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.056193
Total gradient norm: 0.123009
=== Actor Training Debug (Iteration 3376) ===
Q mean: -52.066788
Q std: 17.804972
Actor loss: 52.070774
Action reg: 0.003987
  l1.weight: grad_norm = 0.106526
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.240020
Total gradient norm: 0.434404
=== Actor Training Debug (Iteration 3377) ===
Q mean: -51.558289
Q std: 18.625109
Actor loss: 51.562271
Action reg: 0.003983
  l1.weight: grad_norm = 0.308033
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.623681
Total gradient norm: 1.139301
=== Actor Training Debug (Iteration 3378) ===
Q mean: -54.605968
Q std: 18.174482
Actor loss: 54.609951
Action reg: 0.003982
  l1.weight: grad_norm = 0.001449
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.003059
Total gradient norm: 0.004745
=== Actor Training Debug (Iteration 3379) ===
Q mean: -54.204834
Q std: 19.049715
Actor loss: 54.208809
Action reg: 0.003975
  l1.weight: grad_norm = 0.027398
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.055257
Total gradient norm: 0.106906
=== Actor Training Debug (Iteration 3380) ===
Q mean: -51.629471
Q std: 17.526438
Actor loss: 51.633457
Action reg: 0.003986
  l1.weight: grad_norm = 0.060777
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.133916
Total gradient norm: 0.247777
=== Actor Training Debug (Iteration 3381) ===
Q mean: -50.926964
Q std: 18.870478
Actor loss: 50.930950
Action reg: 0.003987
  l1.weight: grad_norm = 0.084896
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.150624
Total gradient norm: 0.210183
=== Actor Training Debug (Iteration 3382) ===
Q mean: -56.130310
Q std: 18.497293
Actor loss: 56.134289
Action reg: 0.003979
  l1.weight: grad_norm = 0.055657
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.120015
Total gradient norm: 0.238331
=== Actor Training Debug (Iteration 3383) ===
Q mean: -54.045959
Q std: 19.122332
Actor loss: 54.049942
Action reg: 0.003984
  l1.weight: grad_norm = 0.016110
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.033972
Total gradient norm: 0.050395
=== Actor Training Debug (Iteration 3384) ===
Q mean: -53.173088
Q std: 19.155611
Actor loss: 53.177067
Action reg: 0.003977
  l1.weight: grad_norm = 0.028204
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.056660
Total gradient norm: 0.097685
=== Actor Training Debug (Iteration 3385) ===
Q mean: -55.144096
Q std: 18.575766
Actor loss: 55.148079
Action reg: 0.003983
  l1.weight: grad_norm = 0.068906
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.160683
Total gradient norm: 0.337444
=== Actor Training Debug (Iteration 3386) ===
Q mean: -55.258522
Q std: 19.719206
Actor loss: 55.262501
Action reg: 0.003979
  l1.weight: grad_norm = 0.345078
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.665986
Total gradient norm: 1.001933
=== Actor Training Debug (Iteration 3387) ===
Q mean: -53.603508
Q std: 18.799648
Actor loss: 53.607491
Action reg: 0.003983
  l1.weight: grad_norm = 0.080576
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.191728
Total gradient norm: 0.349284
=== Actor Training Debug (Iteration 3388) ===
Q mean: -53.778191
Q std: 18.429049
Actor loss: 53.782177
Action reg: 0.003988
  l1.weight: grad_norm = 0.184511
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.447558
Total gradient norm: 0.817698
=== Actor Training Debug (Iteration 3389) ===
Q mean: -52.286671
Q std: 19.514286
Actor loss: 52.290657
Action reg: 0.003987
  l1.weight: grad_norm = 0.000470
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.001072
Total gradient norm: 0.002658
=== Actor Training Debug (Iteration 3390) ===
Q mean: -54.216450
Q std: 19.567772
Actor loss: 54.220440
Action reg: 0.003990
Actor loss: 58.3852920.0048532637on 1203) ===
Action reg: 0.003998
  l1.weight: grad_norm = 0.146433
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.348709
Total gradient norm: 0.636595
=== Actor Training Debug (Iteration 3401) ===
Q mean: -55.702263
Q std: 18.705467
Actor loss: 55.706257
Action reg: 0.003994
  l1.weight: grad_norm = 0.001629
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.003355
Total gradient norm: 0.007407
=== Actor Training Debug (Iteration 3402) ===
Q mean: -53.617668
Q std: 19.663828
Actor loss: 53.621651
Action reg: 0.003983
  l1.weight: grad_norm = 0.035163
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.095037
Total gradient norm: 0.225114
=== Actor Training Debug (Iteration 3403) ===
Q mean: -50.739933
Q std: 19.728546
Actor loss: 50.743927
Action reg: 0.003995
  l1.weight: grad_norm = 0.002310
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.006141
Total gradient norm: 0.013835
=== Actor Training Debug (Iteration 3404) ===
Q mean: -50.849617
Q std: 18.682238
Actor loss: 50.853603
Action reg: 0.003988
  l1.weight: grad_norm = 0.101082
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.203224
Total gradient norm: 0.290864
=== Actor Training Debug (Iteration 3405) ===
Q mean: -50.510239
Q std: 17.242092
Actor loss: 50.514217
Action reg: 0.003979
  l1.weight: grad_norm = 0.121497
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.248934
Total gradient norm: 0.444916
=== Actor Training Debug (Iteration 3406) ===
Q mean: -57.458469
Q std: 17.753006
Actor loss: 57.462456
Action reg: 0.003987
  l1.weight: grad_norm = 0.028903
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.074403
Total gradient norm: 0.121360
=== Actor Training Debug (Iteration 3407) ===
Q mean: -57.221500
Q std: 19.092329
Actor loss: 57.225487
Action reg: 0.003987
  l1.weight: grad_norm = 0.004754
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.010866
Total gradient norm: 0.021647
=== Actor Training Debug (Iteration 3408) ===
Q mean: -59.059097
Q std: 17.810905
Actor loss: 59.063087
Action reg: 0.003992
  l1.weight: grad_norm = 0.096263
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.204857
Total gradient norm: 0.359121
=== Actor Training Debug (Iteration 3409) ===
Q mean: -54.919941
Q std: 18.875187
Actor loss: 54.923912
Action reg: 0.003972
  l1.weight: grad_norm = 0.025577
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.061241
Total gradient norm: 0.107557
=== Actor Training Debug (Iteration 3410) ===
Q mean: -50.758621
Q std: 18.148336
Actor loss: 50.762592
Action reg: 0.003972
  l1.weight: grad_norm = 0.035276
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.097835
Total gradient norm: 0.219636
=== Actor Training Debug (Iteration 3411) ===
Q mean: -52.467579
Q std: 18.420588
Actor loss: 52.471561
Action reg: 0.003981
  l1.weight: grad_norm = 0.007548
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.020680
Total gradient norm: 0.045720
=== Actor Training Debug (Iteration 3412) ===
Q mean: -54.473663
Q std: 18.250908
Actor loss: 54.477642
Action reg: 0.003981
  l1.weight: grad_norm = 0.028713
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.064354
Total gradient norm: 0.143387
=== Actor Training Debug (Iteration 3413) ===
Q mean: -57.486832
Q std: 18.921782
Actor loss: 57.490810
Action reg: 0.003980
  l1.weight: grad_norm = 0.015822
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.043126
Total gradient norm: 0.081061
=== Actor Training Debug (Iteration 3414) ===
Q mean: -54.620087
Q std: 20.032965
Actor loss: 54.624043
Action reg: 0.003957
  l1.weight: grad_norm = 0.011291
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.031173
Total gradient norm: 0.058251
=== Actor Training Debug (Iteration 3415) ===
Q mean: -52.927017
Q std: 19.010967
Actor loss: 52.931004
Action reg: 0.003985
  l1.weight: grad_norm = 0.021465
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.057416
Total gradient norm: 0.118277
=== Actor Training Debug (Iteration 3416) ===
Q mean: -50.422066
Q std: 18.082899
Actor loss: 50.426037
Action reg: 0.003971
  l1.weight: grad_norm = 0.030357
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.085756
Total gradient norm: 0.213721
=== Actor Training Debug (Iteration 3417) ===
Q mean: -53.823631
Q std: 16.796684
Actor loss: 53.827621
Action reg: 0.003990
  l1.weight: grad_norm = 0.063558
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.164623
Total gradient norm: 0.344579
=== Actor Training Debug (Iteration 3418) ===
Q mean: -54.911987
Q std: 17.301456
Actor loss: 54.915970
Action reg: 0.003982
  l1.weight: grad_norm = 0.000896
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.002580
Total gradient norm: 0.006389
=== Actor Training Debug (Iteration 3419) ===
Q mean: -54.723698
Q std: 19.243694
Actor loss: 54.727665
Action reg: 0.003968
  l1.weight: grad_norm = 0.081572
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.175942
Total gradient norm: 0.284926
=== Actor Training Debug (Iteration 3420) ===
Q mean: -52.589008
Q std: 17.828781
Actor loss: 52.592987
Action reg: 0.003978
  l1.weight: grad_norm = 0.018213
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.042907
Total gradient norm: 0.082742
=== Actor Training Debug (Iteration 3421) ===
Q mean: -52.089897
Q std: 20.569117
Actor loss: 52.093872
Action reg: 0.003973
  l1.weight: grad_norm = 0.029886
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.055494
Total gradient norm: 0.082901
=== Actor Training Debug (Iteration 3422) ===
Q mean: -54.525600
Q std: 18.860588
Actor loss: 54.529583
Action reg: 0.003983
  l1.weight: grad_norm = 0.068085
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.157150
Total gradient norm: 0.290739
=== Actor Training Debug (Iteration 3423) ===
Q mean: -56.039570
Q std: 18.647112
Actor loss: 56.043549
Action reg: 0.003979
  l1.weight: grad_norm = 0.005283
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.014172
Total gradient norm: 0.030040
=== Actor Training Debug (Iteration 3424) ===
Q mean: -54.292942
Q std: 18.261984
Actor loss: 54.296925
Action reg: 0.003983
  l1.weight: grad_norm = 0.050901
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.116607
Total gradient norm: 0.261030
=== Actor Training Debug (Iteration 3425) ===
Q mean: -53.551479
Q std: 18.511787
Actor loss: 53.555450
Action reg: 0.003970
  l1.weight: grad_norm = 0.037041
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.100353
Total gradient norm: 0.240259
=== Actor Training Debug (Iteration 3426) ===
Q mean: -56.749290
Q std: 18.994244
Actor loss: 56.753269
Action reg: 0.003979
  l1.weight: grad_norm = 0.053121
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.105699
Total gradient norm: 0.171701
=== Actor Training Debug (Iteration 3427) ===
Q mean: -55.640278
Q std: 17.907021
Actor loss: 55.644264
Action reg: 0.003986
  l1.weight: grad_norm = 0.020567
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.041892
Total gradient norm: 0.077822
=== Actor Training Debug (Iteration 3428) ===
Q mean: -53.633080
Q std: 17.628151
Actor loss: 53.637070
Action reg: 0.003989
  l1.weight: grad_norm = 0.058789
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.153119
Total gradient norm: 0.351866
=== Actor Training Debug (Iteration 3429) ===
Q mean: -54.775383
Q std: 17.935545
Actor loss: 54.779373
Action reg: 0.003989
  l1.weight: grad_norm = 0.026347
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.064353
Total gradient norm: 0.132402
=== Actor Training Debug (Iteration 3430) ===
Q mean: -53.704124
Q std: 16.913919
Actor loss: 53.708111
Action reg: 0.003987
  l1.weight: grad_norm = 0.086911
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.183529
Total gradient norm: 0.360508
=== Actor Training Debug (Iteration 3431) ===
Q mean: -53.339622
Q std: 17.880053
Actor loss: 53.343605
Action reg: 0.003984
  l1.weight: grad_norm = 0.061730
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.156811
Total gradient norm: 0.303002
=== Actor Training Debug (Iteration 3432) ===
Q mean: -50.801025
Q std: 19.687250
Actor loss: 50.804985
Action reg: 0.003959
  l1.weight: grad_norm = 0.049131
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.105820
Total gradient norm: 0.193225
=== Actor Training Debug (Iteration 3433) ===
Q mean: -55.107075
Q std: 17.646166
Actor loss: 55.111061
Action reg: 0.003987
  l1.weight: grad_norm = 0.003338
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.007673
Total gradient norm: 0.014132
=== Actor Training Debug (Iteration 3434) ===
Q mean: -55.270332
Q std: 18.197721
Actor loss: 55.274326
Action reg: 0.003994
  l1.weight: grad_norm = 0.015010
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.032424
Total gradient norm: 0.056964
=== Actor Training Debug (Iteration 3435) ===
Q mean: -52.226425
Q std: 18.656506
Actor loss: 52.230419
Action reg: 0.003994
  l1.weight: grad_norm = 0.023127
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.053650
Total gradient norm: 0.098346
=== Actor Training Debug (Iteration 3436) ===
Q mean: -51.836330
Q std: 19.333351
Actor loss: 51.840317
Action reg: 0.003986
  l1.weight: grad_norm = 0.018421
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.044589
Total gradient norm: 0.099127
=== Actor Training Debug (Iteration 3437) ===
Q mean: -54.863338
Q std: 19.724648
Actor loss: 54.867306
Action reg: 0.003967
  l1.weight: grad_norm = 0.000552
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.002282
Total gradient norm: 0.008351
=== Actor Training Debug (Iteration 3438) ===
Q mean: -56.235291
Q std: 19.637926
Actor loss: 56.239281
Action reg: 0.003991
  l1.weight: grad_norm = 0.041001
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.095414
Total gradient norm: 0.191438
=== Actor Training Debug (Iteration 3439) ===
Q mean: -54.464249
Q std: 17.688631
Actor loss: 54.468227
Action reg: 0.003981
  l1.weight: grad_norm = 0.053673
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.100676
Total gradient norm: 0.160229
=== Actor Training Debug (Iteration 3440) ===
Q mean: -51.619179
Q std: 20.246952
Actor loss: 51.623146
Action reg: 0.003968
  l1.weight: grad_norm = 0.006027
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.013920
Total gradient norm: 0.028245
=== Actor Training Debug (Iteration 3441) ===
Q mean: -58.369118
Q std: 19.004511
Actor loss: 58.373108
Action reg: 0.003992
  l1.weight: grad_norm = 0.023554
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.052026
Total gradient norm: 0.096522
=== Actor Training Debug (Iteration 3442) ===
Q mean: -53.031975
Q std: 17.523376
Actor loss: 53.035965
Action reg: 0.003988
  l1.weight: grad_norm = 0.048906
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.110324
Total gradient norm: 0.201395
=== Actor Training Debug (Iteration 3443) ===
Q mean: -55.439785
Q std: 18.496727
Actor loss: 55.443768
Action reg: 0.003981
  l1.weight: grad_norm = 0.002255
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.006028
Total gradient norm: 0.015037
Total gradient norm: 0.0766762637on 1203) ===
=== Actor Training Debug (Iteration 3454) ===
Q mean: -57.362801
Q std: 18.966251
Actor loss: 57.366779
Action reg: 0.003979
  l1.weight: grad_norm = 0.046549
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.104433
Total gradient norm: 0.199512
=== Actor Training Debug (Iteration 3455) ===
Q mean: -56.797985
Q std: 18.587194
Actor loss: 56.801979
Action reg: 0.003992
  l1.weight: grad_norm = 0.004105
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.010065
Total gradient norm: 0.020108
=== Actor Training Debug (Iteration 3456) ===
Q mean: -54.264896
Q std: 17.533085
Actor loss: 54.268887
Action reg: 0.003991
  l1.weight: grad_norm = 0.050484
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.103952
Total gradient norm: 0.195472
=== Actor Training Debug (Iteration 3457) ===
Q mean: -52.582241
Q std: 18.089663
Actor loss: 52.586231
Action reg: 0.003991
  l1.weight: grad_norm = 0.134335
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.264421
Total gradient norm: 0.388179
=== Actor Training Debug (Iteration 3458) ===
Q mean: -53.322720
Q std: 18.699497
Actor loss: 53.326691
Action reg: 0.003969
  l1.weight: grad_norm = 0.086381
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.209753
Total gradient norm: 0.406101
=== Actor Training Debug (Iteration 3459) ===
Q mean: -53.508751
Q std: 20.361563
Actor loss: 53.512707
Action reg: 0.003956
  l1.weight: grad_norm = 0.090968
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.167333
Total gradient norm: 0.296365
=== Actor Training Debug (Iteration 3460) ===
Q mean: -57.069664
Q std: 19.255383
Actor loss: 57.073639
Action reg: 0.003975
  l1.weight: grad_norm = 0.000867
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.002353
Total gradient norm: 0.006575
=== Actor Training Debug (Iteration 3461) ===
Q mean: -51.705795
Q std: 18.980732
Actor loss: 51.709770
Action reg: 0.003976
  l1.weight: grad_norm = 0.084843
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.187924
Total gradient norm: 0.394132
=== Actor Training Debug (Iteration 3462) ===
Q mean: -53.634300
Q std: 18.326389
Actor loss: 53.638294
Action reg: 0.003995
  l1.weight: grad_norm = 0.100994
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.246423
Total gradient norm: 0.498816
=== Actor Training Debug (Iteration 3463) ===
Q mean: -55.271458
Q std: 18.390680
Actor loss: 55.275448
Action reg: 0.003991
  l1.weight: grad_norm = 0.133985
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.294160
Total gradient norm: 0.547185
=== Actor Training Debug (Iteration 3464) ===
Q mean: -58.613960
Q std: 19.007833
Actor loss: 58.617954
Action reg: 0.003993
  l1.weight: grad_norm = 0.182672
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.386108
Total gradient norm: 0.758532
=== Actor Training Debug (Iteration 3465) ===
Q mean: -55.105694
Q std: 19.248041
Actor loss: 55.109680
Action reg: 0.003985
  l1.weight: grad_norm = 0.087083
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.183605
Total gradient norm: 0.354037
=== Actor Training Debug (Iteration 3466) ===
Q mean: -52.621307
Q std: 19.527899
Actor loss: 52.625294
Action reg: 0.003986
  l1.weight: grad_norm = 0.003733
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.009153
Total gradient norm: 0.020180
=== Actor Training Debug (Iteration 3467) ===
Q mean: -55.905739
Q std: 20.459715
Actor loss: 55.909710
Action reg: 0.003971
  l1.weight: grad_norm = 0.105231
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.251663
Total gradient norm: 0.478269
=== Actor Training Debug (Iteration 3468) ===
Q mean: -55.730659
Q std: 18.501751
Actor loss: 55.734653
Action reg: 0.003993
  l1.weight: grad_norm = 0.008083
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.020790
Total gradient norm: 0.040522
=== Actor Training Debug (Iteration 3469) ===
Q mean: -54.733566
Q std: 19.742374
Actor loss: 54.737534
Action reg: 0.003969
  l1.weight: grad_norm = 0.025111
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.069745
Total gradient norm: 0.130670
=== Actor Training Debug (Iteration 3470) ===
Q mean: -53.106018
Q std: 17.665808
Actor loss: 53.110008
Action reg: 0.003992
  l1.weight: grad_norm = 0.196690
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.425795
Total gradient norm: 0.834008
=== Actor Training Debug (Iteration 3471) ===
Q mean: -54.121590
Q std: 19.532822
Actor loss: 54.125561
Action reg: 0.003972
  l1.weight: grad_norm = 0.034503
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.073940
Total gradient norm: 0.131602
=== Actor Training Debug (Iteration 3472) ===
Q mean: -55.782890
Q std: 17.054989
Actor loss: 55.786888
Action reg: 0.003999
  l1.weight: grad_norm = 0.069386
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.152698
Total gradient norm: 0.278322
=== Actor Training Debug (Iteration 3473) ===
Q mean: -54.586094
Q std: 18.149912
Actor loss: 54.590076
Action reg: 0.003982
  l1.weight: grad_norm = 0.078226
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.190335
Total gradient norm: 0.305516
=== Actor Training Debug (Iteration 3474) ===
Q mean: -56.902908
Q std: 19.284527
Actor loss: 56.906891
Action reg: 0.003984
  l1.weight: grad_norm = 0.018000
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.038513
Total gradient norm: 0.058339
=== Actor Training Debug (Iteration 3475) ===
Q mean: -53.628292
Q std: 18.039392
Actor loss: 53.632290
Action reg: 0.003998
  l1.weight: grad_norm = 0.061108
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.135668
Total gradient norm: 0.275606
=== Actor Training Debug (Iteration 3476) ===
Q mean: -54.850685
Q std: 18.480055
Actor loss: 54.854671
Action reg: 0.003988
  l1.weight: grad_norm = 0.175536
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.356051
Total gradient norm: 0.703180
=== Actor Training Debug (Iteration 3477) ===
Q mean: -54.307793
Q std: 18.701822
Actor loss: 54.311779
Action reg: 0.003985
  l1.weight: grad_norm = 0.016871
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.032963
Total gradient norm: 0.051208
=== Actor Training Debug (Iteration 3478) ===
Q mean: -54.548229
Q std: 19.877924
Actor loss: 54.552216
Action reg: 0.003986
  l1.weight: grad_norm = 0.023215
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.049883
Total gradient norm: 0.081762
=== Actor Training Debug (Iteration 3479) ===
Q mean: -54.267975
Q std: 19.513100
Actor loss: 54.271961
Action reg: 0.003987
  l1.weight: grad_norm = 0.000368
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.001376
Total gradient norm: 0.003934
=== Actor Training Debug (Iteration 3480) ===
Q mean: -52.085838
Q std: 18.223314
Actor loss: 52.089813
Action reg: 0.003975
  l1.weight: grad_norm = 0.000812
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.002267
Total gradient norm: 0.006381
=== Actor Training Debug (Iteration 3481) ===
Q mean: -55.543518
Q std: 19.434202
Actor loss: 55.547485
Action reg: 0.003967
  l1.weight: grad_norm = 0.007258
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.020725
Total gradient norm: 0.049441
=== Actor Training Debug (Iteration 3482) ===
Q mean: -54.047127
Q std: 18.995008
Actor loss: 54.051109
Action reg: 0.003983
  l1.weight: grad_norm = 0.027559
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.068400
Total gradient norm: 0.121901
=== Actor Training Debug (Iteration 3483) ===
Q mean: -51.458115
Q std: 18.090866
Actor loss: 51.462101
Action reg: 0.003987
  l1.weight: grad_norm = 0.060167
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.124114
Total gradient norm: 0.270544
=== Actor Training Debug (Iteration 3484) ===
Q mean: -52.177441
Q std: 18.544628
Actor loss: 52.181431
Action reg: 0.003991
  l1.weight: grad_norm = 0.052738
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.124886
Total gradient norm: 0.257705
=== Actor Training Debug (Iteration 3485) ===
Q mean: -55.973343
Q std: 19.557720
Actor loss: 55.977329
Action reg: 0.003986
  l1.weight: grad_norm = 0.064047
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.161802
Total gradient norm: 0.340637
=== Actor Training Debug (Iteration 3486) ===
Q mean: -56.769135
Q std: 19.716652
Actor loss: 56.773113
Action reg: 0.003977
  l1.weight: grad_norm = 0.009025
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.017945
Total gradient norm: 0.033398
=== Actor Training Debug (Iteration 3487) ===
Q mean: -55.708473
Q std: 19.353416
Actor loss: 55.712456
Action reg: 0.003981
  l1.weight: grad_norm = 0.019408
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.045379
Total gradient norm: 0.086726
=== Actor Training Debug (Iteration 3488) ===
Q mean: -52.983646
Q std: 16.791504
Actor loss: 52.987644
Action reg: 0.003998
  l1.weight: grad_norm = 0.067022
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.161237
Total gradient norm: 0.338107
Total gradient norm: 0.5546142637on 1203) ===
=== Actor Training Debug (Iteration 3499) ===
Q mean: -54.824856
Q std: 18.354105
Actor loss: 54.828835
Action reg: 0.003978
  l1.weight: grad_norm = 0.052444
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.083640
Total gradient norm: 0.132062
=== Actor Training Debug (Iteration 3500) ===
Q mean: -53.924675
Q std: 17.922819
Actor loss: 53.928658
Action reg: 0.003984
  l1.weight: grad_norm = 0.043813
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.090203
Total gradient norm: 0.135697
  Average reward: -361.302 | Average length: 100.0
Evaluation at episode 85: -361.302
=== Actor Training Debug (Iteration 3501) ===
Q mean: -54.715855
Q std: 19.076683
Actor loss: 54.719818
Action reg: 0.003963
  l1.weight: grad_norm = 0.047868
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.098976
Total gradient norm: 0.243080
=== Actor Training Debug (Iteration 3502) ===
Q mean: -54.780064
Q std: 18.906244
Actor loss: 54.784054
Action reg: 0.003989
  l1.weight: grad_norm = 0.023324
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.052509
Total gradient norm: 0.102629
=== Actor Training Debug (Iteration 3503) ===
Q mean: -58.321121
Q std: 18.658098
Actor loss: 58.325108
Action reg: 0.003985
  l1.weight: grad_norm = 0.055301
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.152326
Total gradient norm: 0.284136
=== Actor Training Debug (Iteration 3504) ===
Q mean: -56.292381
Q std: 19.298405
Actor loss: 56.296356
Action reg: 0.003974
  l1.weight: grad_norm = 0.001277
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.003817
Total gradient norm: 0.011169
=== Actor Training Debug (Iteration 3505) ===
Q mean: -52.891869
Q std: 19.627644
Actor loss: 52.895859
Action reg: 0.003991
  l1.weight: grad_norm = 0.035335
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.083184
Total gradient norm: 0.172869
=== Actor Training Debug (Iteration 3506) ===
Q mean: -55.722816
Q std: 18.816381
Actor loss: 55.726807
Action reg: 0.003989
  l1.weight: grad_norm = 0.058996
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.163976
Total gradient norm: 0.358030
=== Actor Training Debug (Iteration 3507) ===
Q mean: -53.697411
Q std: 16.902515
Actor loss: 53.701408
Action reg: 0.003998
  l1.weight: grad_norm = 0.075265
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.144242
Total gradient norm: 0.251920
=== Actor Training Debug (Iteration 3508) ===
Q mean: -54.856697
Q std: 17.779175
Actor loss: 54.860680
Action reg: 0.003982
  l1.weight: grad_norm = 0.204167
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.392901
Total gradient norm: 0.676893
=== Actor Training Debug (Iteration 3509) ===
Q mean: -54.492516
Q std: 18.511663
Actor loss: 54.496510
Action reg: 0.003993
  l1.weight: grad_norm = 0.040275
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.090541
Total gradient norm: 0.201694
=== Actor Training Debug (Iteration 3510) ===
Q mean: -54.726452
Q std: 18.216722
Actor loss: 54.730446
Action reg: 0.003996
  l1.weight: grad_norm = 0.203908
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.392951
Total gradient norm: 0.714941
=== Actor Training Debug (Iteration 3511) ===
Q mean: -54.410011
Q std: 20.693218
Actor loss: 54.413979
Action reg: 0.003967
  l1.weight: grad_norm = 0.040799
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.097870
Total gradient norm: 0.177495
=== Actor Training Debug (Iteration 3512) ===
Q mean: -53.881653
Q std: 18.280333
Actor loss: 53.885639
Action reg: 0.003986
  l1.weight: grad_norm = 0.113684
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.230346
Total gradient norm: 0.398963
=== Actor Training Debug (Iteration 3513) ===
Q mean: -56.301350
Q std: 18.454189
Actor loss: 56.305340
Action reg: 0.003991
  l1.weight: grad_norm = 0.022553
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.049517
Total gradient norm: 0.096308
=== Actor Training Debug (Iteration 3514) ===
Q mean: -56.041965
Q std: 19.119091
Actor loss: 56.045944
Action reg: 0.003980
  l1.weight: grad_norm = 0.147504
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.302225
Total gradient norm: 0.581628
=== Actor Training Debug (Iteration 3515) ===
Q mean: -54.976791
Q std: 18.342077
Actor loss: 54.980782
Action reg: 0.003992
  l1.weight: grad_norm = 0.009857
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.024674
Total gradient norm: 0.050277
Total gradient norm: 0.0234642637on 1203) ===
=== Actor Training Debug (Iteration 3526) ===
Q mean: -56.433098
Q std: 19.493586
Actor loss: 56.437092
Action reg: 0.003995
  l1.weight: grad_norm = 0.082159
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.163037
Total gradient norm: 0.268966
=== Actor Training Debug (Iteration 3527) ===
Q mean: -56.107246
Q std: 18.340277
Actor loss: 56.111229
Action reg: 0.003981
  l1.weight: grad_norm = 0.003511
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.007445
Total gradient norm: 0.012659
=== Actor Training Debug (Iteration 3528) ===
Q mean: -54.159767
Q std: 19.250690
Actor loss: 54.163731
Action reg: 0.003965
  l1.weight: grad_norm = 0.024367
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.057782
Total gradient norm: 0.145174
=== Actor Training Debug (Iteration 3529) ===
Q mean: -54.505692
Q std: 19.165661
Actor loss: 54.509678
Action reg: 0.003985
  l1.weight: grad_norm = 0.013815
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.031761
Total gradient norm: 0.070149
=== Actor Training Debug (Iteration 3530) ===
Q mean: -55.068573
Q std: 18.552511
Actor loss: 55.072548
Action reg: 0.003977
  l1.weight: grad_norm = 0.102643
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.236695
Total gradient norm: 0.382868
=== Actor Training Debug (Iteration 3531) ===
Q mean: -56.702019
Q std: 19.715090
Actor loss: 56.706005
Action reg: 0.003985
  l1.weight: grad_norm = 0.042501
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.102356
Total gradient norm: 0.202700
=== Actor Training Debug (Iteration 3532) ===
Q mean: -54.555641
Q std: 16.926088
Actor loss: 54.559631
Action reg: 0.003992
  l1.weight: grad_norm = 0.017564
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.036929
Total gradient norm: 0.067409
=== Actor Training Debug (Iteration 3533) ===
Q mean: -53.490452
Q std: 17.205372
Actor loss: 53.494442
Action reg: 0.003989
  l1.weight: grad_norm = 0.013061
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.027779
Total gradient norm: 0.056407
=== Actor Training Debug (Iteration 3534) ===
Q mean: -57.684227
Q std: 17.866350
Actor loss: 57.688221
Action reg: 0.003994
  l1.weight: grad_norm = 0.006759
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.015020
Total gradient norm: 0.028461
=== Actor Training Debug (Iteration 3535) ===
Q mean: -53.753284
Q std: 18.699415
Actor loss: 53.757275
Action reg: 0.003989
  l1.weight: grad_norm = 0.016290
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.039403
Total gradient norm: 0.080006
=== Actor Training Debug (Iteration 3536) ===
Q mean: -54.028717
Q std: 18.916101
Actor loss: 54.032696
Action reg: 0.003979
  l1.weight: grad_norm = 0.126223
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.277098
Total gradient norm: 0.467153
=== Actor Training Debug (Iteration 3537) ===
Q mean: -55.088711
Q std: 19.052774
Actor loss: 55.092682
Action reg: 0.003973
  l1.weight: grad_norm = 0.057200
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.128417
Total gradient norm: 0.238656
=== Actor Training Debug (Iteration 3538) ===
Q mean: -52.380615
Q std: 18.046816
Actor loss: 52.384602
Action reg: 0.003986
  l1.weight: grad_norm = 0.008658
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.019191
Total gradient norm: 0.035841
=== Actor Training Debug (Iteration 3539) ===
Q mean: -55.420841
Q std: 19.002678
Actor loss: 55.424824
Action reg: 0.003981
  l1.weight: grad_norm = 0.036393
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.083425
Total gradient norm: 0.170102
=== Actor Training Debug (Iteration 3540) ===
Q mean: -53.974716
Q std: 18.616486
Actor loss: 53.978699
Action reg: 0.003981
  l1.weight: grad_norm = 0.000517
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.001223
Total gradient norm: 0.003377
=== Actor Training Debug (Iteration 3541) ===
Q mean: -54.199364
Q std: 18.964867
Actor loss: 54.203350
Action reg: 0.003987
  l1.weight: grad_norm = 0.001196
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.002809
Total gradient norm: 0.005819
=== Actor Training Debug (Iteration 3542) ===
Q mean: -54.093407
Q std: 19.136351
Actor loss: 54.097393
Action reg: 0.003988
  l1.weight: grad_norm = 0.067556
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.141328
Total gradient norm: 0.298813
=== Actor Training Debug (Iteration 3543) ===
Q mean: -54.620689
Q std: 20.132858
Actor loss: 54.624680
Action reg: 0.003991
  l1.weight: grad_norm = 0.036850
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.088576
Total gradient norm: 0.172050
=== Actor Training Debug (Iteration 3544) ===
Q mean: -53.937943
Q std: 18.169405
Actor loss: 53.941925
Action reg: 0.003984
  l1.weight: grad_norm = 0.031785
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.074789
Total gradient norm: 0.159417
=== Actor Training Debug (Iteration 3545) ===
Q mean: -54.322296
Q std: 17.050009
Actor loss: 54.326290
Action reg: 0.003993
  l1.weight: grad_norm = 0.026156
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.062953
Total gradient norm: 0.118360
=== Actor Training Debug (Iteration 3546) ===
Q mean: -57.400093
Q std: 19.666882
Actor loss: 57.404076
Action reg: 0.003984
  l1.weight: grad_norm = 0.016358
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.033555
Total gradient norm: 0.055036
=== Actor Training Debug (Iteration 3547) ===
Q mean: -55.480946
Q std: 19.171701
Actor loss: 55.484936
Action reg: 0.003991
  l1.weight: grad_norm = 0.052163
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.095347
Total gradient norm: 0.138116
=== Actor Training Debug (Iteration 3548) ===
Q mean: -54.237534
Q std: 18.930658
Actor loss: 54.241520
Action reg: 0.003985
  l1.weight: grad_norm = 0.034584
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.065931
Total gradient norm: 0.122405
=== Actor Training Debug (Iteration 3549) ===
Q mean: -53.926384
Q std: 19.583286
Actor loss: 53.930351
Action reg: 0.003966
  l1.weight: grad_norm = 0.057366
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.134983
Total gradient norm: 0.273534
=== Actor Training Debug (Iteration 3550) ===
Q mean: -56.273315
Q std: 19.132183
Actor loss: 56.277302
Action reg: 0.003986
  l1.weight: grad_norm = 0.003556
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.008472
Total gradient norm: 0.015798
=== Actor Training Debug (Iteration 3551) ===
Q mean: -55.876324
Q std: 19.708536
Actor loss: 55.880302
Action reg: 0.003978
  l1.weight: grad_norm = 0.019204
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.040655
Total gradient norm: 0.073286
=== Actor Training Debug (Iteration 3552) ===
Q mean: -53.042259
Q std: 18.855007
Actor loss: 53.046238
Action reg: 0.003979
  l1.weight: grad_norm = 0.008364
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.018844
Total gradient norm: 0.029723
=== Actor Training Debug (Iteration 3553) ===
Q mean: -54.604073
Q std: 17.810408
Actor loss: 54.608067
Action reg: 0.003995
  l1.weight: grad_norm = 0.006024
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.014470
Total gradient norm: 0.034697
=== Actor Training Debug (Iteration 3554) ===
Q mean: -55.403900
Q std: 18.544098
Actor loss: 55.407883
Action reg: 0.003984
  l1.weight: grad_norm = 0.189428
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.388891
Total gradient norm: 0.723473
=== Actor Training Debug (Iteration 3555) ===
Q mean: -53.270702
Q std: 18.501804
Actor loss: 53.274689
Action reg: 0.003986
  l1.weight: grad_norm = 0.000885
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.002325
Total gradient norm: 0.004416
=== Actor Training Debug (Iteration 3556) ===
Q mean: -51.589127
Q std: 19.325331
Actor loss: 51.593113
Action reg: 0.003986
  l1.weight: grad_norm = 0.052532
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.132682
Total gradient norm: 0.271755
=== Actor Training Debug (Iteration 3557) ===
Q mean: -57.021099
Q std: 19.136637
Actor loss: 57.025078
Action reg: 0.003981
  l1.weight: grad_norm = 0.016879
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.042289
Total gradient norm: 0.090842
=== Actor Training Debug (Iteration 3558) ===
Q mean: -53.961159
Q std: 18.039488
Actor loss: 53.965153
Action reg: 0.003994
  l1.weight: grad_norm = 0.001333
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.002755
Total gradient norm: 0.005569
=== Actor Training Debug (Iteration 3559) ===
Q mean: -55.506779
Q std: 20.154991
Actor loss: 55.510754
Action reg: 0.003975
  l1.weight: grad_norm = 0.058569
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.127642
Total gradient norm: 0.228451
=== Actor Training Debug (Iteration 3560) ===
Q mean: -54.849930
Q std: 19.510935
Actor loss: 54.853905
Action reg: 0.003973
  l1.weight: grad_norm = 0.027034
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.064836
Total gradient norm: 0.116941
=== Actor Training Debug (Iteration 3561) ===
Q mean: -54.089340
Q std: 19.221470
Actor loss: 54.093311
Action reg: 0.003971
  l1.weight: grad_norm = 0.011474
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.023055
Total gradient norm: 0.043125
=== Actor Training Debug (Iteration 3562) ===
Q mean: -56.259514
Q std: 18.249895
Actor loss: 56.263496
Action reg: 0.003982
  l1.weight: grad_norm = 0.014468
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.041617
Total gradient norm: 0.088855
=== Actor Training Debug (Iteration 3563) ===
Q mean: -55.554607
Q std: 18.558460
Actor loss: 55.558590
Action reg: 0.003982
  l1.weight: grad_norm = 0.000249
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.001164
Total gradient norm: 0.004236
=== Actor Training Debug (Iteration 3564) ===
Q mean: -54.564713
Q std: 21.085640
Actor loss: 54.568691
Action reg: 0.003978
  l1.weight: grad_norm = 0.020026
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.050944
Total gradient norm: 0.098531
=== Actor Training Debug (Iteration 3565) ===
Q mean: -56.498642
Q std: 20.845984
Actor loss: 56.502625
Action reg: 0.003983
  l1.weight: grad_norm = 0.003645
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.008126
Total gradient norm: 0.015760
=== Actor Training Debug (Iteration 3566) ===
Q mean: -58.236977
Q std: 17.870014
Actor loss: 58.240967
Action reg: 0.003988
  l1.weight: grad_norm = 0.011343
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.026730
Total gradient norm: 0.056364
Total gradient norm: 0.2499512637on 1203) ===
=== Actor Training Debug (Iteration 3577) ===
Q mean: -52.685593
Q std: 18.140640
Actor loss: 52.689571
Action reg: 0.003978
  l1.weight: grad_norm = 0.127832
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.261499
Total gradient norm: 0.480633
=== Actor Training Debug (Iteration 3578) ===
Q mean: -54.338951
Q std: 16.782024
Actor loss: 54.342945
Action reg: 0.003993
  l1.weight: grad_norm = 0.011209
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.024218
Total gradient norm: 0.046358
=== Actor Training Debug (Iteration 3579) ===
Q mean: -54.102432
Q std: 18.988951
Actor loss: 54.106411
Action reg: 0.003981
  l1.weight: grad_norm = 0.003340
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.007956
Total gradient norm: 0.017717
=== Actor Training Debug (Iteration 3580) ===
Q mean: -54.699699
Q std: 19.335005
Actor loss: 54.703667
Action reg: 0.003967
  l1.weight: grad_norm = 0.123139
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.302872
Total gradient norm: 0.582285
=== Actor Training Debug (Iteration 3581) ===
Q mean: -53.766396
Q std: 18.924301
Actor loss: 53.770378
Action reg: 0.003983
  l1.weight: grad_norm = 0.050845
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.101180
Total gradient norm: 0.184243
=== Actor Training Debug (Iteration 3582) ===
Q mean: -54.266266
Q std: 20.687346
Actor loss: 54.270241
Action reg: 0.003977
  l1.weight: grad_norm = 0.004642
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.012823
Total gradient norm: 0.025948
=== Actor Training Debug (Iteration 3583) ===
Q mean: -56.111145
Q std: 19.516123
Actor loss: 56.115135
Action reg: 0.003989
  l1.weight: grad_norm = 0.000214
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.000892
Total gradient norm: 0.003106
=== Actor Training Debug (Iteration 3584) ===
Q mean: -56.777176
Q std: 18.883032
Actor loss: 56.781158
Action reg: 0.003982
  l1.weight: grad_norm = 0.019429
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.042722
Total gradient norm: 0.082056
=== Actor Training Debug (Iteration 3585) ===
Q mean: -53.460983
Q std: 20.068872
Actor loss: 53.464970
Action reg: 0.003987
  l1.weight: grad_norm = 0.004559
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.010584
Total gradient norm: 0.021173
=== Actor Training Debug (Iteration 3586) ===
Q mean: -53.885128
Q std: 19.138252
Actor loss: 53.889103
Action reg: 0.003974
  l1.weight: grad_norm = 0.036978
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.094032
Total gradient norm: 0.164369
=== Actor Training Debug (Iteration 3587) ===
Q mean: -56.785999
Q std: 19.181168
Actor loss: 56.789982
Action reg: 0.003984
  l1.weight: grad_norm = 0.028967
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.059251
Total gradient norm: 0.113637
=== Actor Training Debug (Iteration 3588) ===
Q mean: -55.992874
Q std: 18.502171
Actor loss: 55.996857
Action reg: 0.003983
  l1.weight: grad_norm = 0.015482
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.032956
Total gradient norm: 0.063207
=== Actor Training Debug (Iteration 3589) ===
Q mean: -56.087540
Q std: 18.784391
Actor loss: 56.091518
Action reg: 0.003980
  l1.weight: grad_norm = 0.004276
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.010025
Total gradient norm: 0.024110
=== Actor Training Debug (Iteration 3590) ===
Q mean: -55.892750
Q std: 19.473257
Actor loss: 55.896725
Action reg: 0.003975
  l1.weight: grad_norm = 0.000463
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.002053
Total gradient norm: 0.007110
=== Actor Training Debug (Iteration 3591) ===
Q mean: -53.059006
Q std: 18.453323
Actor loss: 53.062981
Action reg: 0.003974
  l1.weight: grad_norm = 0.019697
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.041868
Total gradient norm: 0.078925
=== Actor Training Debug (Iteration 3592) ===
Q mean: -56.403664
Q std: 19.906445
Actor loss: 56.407639
Action reg: 0.003976
  l1.weight: grad_norm = 0.005620
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.011738
Total gradient norm: 0.022464
=== Actor Training Debug (Iteration 3593) ===
Q mean: -55.998363
Q std: 18.734053
Actor loss: 56.002357
Action reg: 0.003994
  l1.weight: grad_norm = 0.003604
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.007521
Total gradient norm: 0.015092
=== Actor Training Debug (Iteration 3594) ===
Q mean: -58.716576
Q std: 18.377260
Actor loss: 58.720562
Action reg: 0.003988
  l1.weight: grad_norm = 0.003600
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.005994
Total gradient norm: 0.010069
=== Actor Training Debug (Iteration 3595) ===
Q mean: -54.792236
Q std: 18.379066
Actor loss: 54.796215
Action reg: 0.003980
  l1.weight: grad_norm = 0.268885
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.550175
Total gradient norm: 0.965303
=== Actor Training Debug (Iteration 3596) ===
Q mean: -55.738304
Q std: 16.905321
Actor loss: 55.742302
Action reg: 0.003998
  l1.weight: grad_norm = 0.007611
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.018349
Total gradient norm: 0.036311
=== Actor Training Debug (Iteration 3597) ===
Q mean: -58.720867
Q std: 16.617630
Actor loss: 58.724857
Action reg: 0.003990
  l1.weight: grad_norm = 0.029573
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.071783
Total gradient norm: 0.152674
=== Actor Training Debug (Iteration 3598) ===
Q mean: -55.963253
Q std: 18.169392
Actor loss: 55.967239
Action reg: 0.003988
  l1.weight: grad_norm = 0.001177
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.002302
Total gradient norm: 0.003633
=== Actor Training Debug (Iteration 3599) ===
Q mean: -55.964531
Q std: 18.493402
Actor loss: 55.968517
Action reg: 0.003985
  l1.weight: grad_norm = 0.076601
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.178810
Total gradient norm: 0.311424
=== Actor Training Debug (Iteration 3600) ===
Q mean: -55.668610
Q std: 18.576820
Actor loss: 55.672600
Action reg: 0.003992
  l1.weight: grad_norm = 0.015434
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.035164
Total gradient norm: 0.069127
=== Actor Training Debug (Iteration 3601) ===
Q mean: -59.522522
Q std: 18.915232
Actor loss: 59.526508
Action reg: 0.003985
  l1.weight: grad_norm = 0.222777
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.509377
Total gradient norm: 0.998919
=== Actor Training Debug (Iteration 3602) ===
Q mean: -55.522964
Q std: 19.175217
Actor loss: 55.526939
Action reg: 0.003976
  l1.weight: grad_norm = 0.058417
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.125369
Total gradient norm: 0.273841
=== Actor Training Debug (Iteration 3603) ===
Q mean: -54.978302
Q std: 17.997421
Actor loss: 54.982296
Action reg: 0.003993
  l1.weight: grad_norm = 0.028074
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.060515
Total gradient norm: 0.114737
=== Actor Training Debug (Iteration 3604) ===
Q mean: -54.996700
Q std: 18.202599
Actor loss: 55.000687
Action reg: 0.003987
  l1.weight: grad_norm = 0.014511
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.036651
Total gradient norm: 0.080596
=== Actor Training Debug (Iteration 3605) ===
Q mean: -57.794434
Q std: 20.288054
Actor loss: 57.798405
Action reg: 0.003970
  l1.weight: grad_norm = 0.014063
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.031787
Total gradient norm: 0.055454
=== Actor Training Debug (Iteration 3606) ===
Q mean: -54.757950
Q std: 19.871031
Actor loss: 54.761925
Action reg: 0.003977
  l1.weight: grad_norm = 0.024742
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.059261
Total gradient norm: 0.116344
=== Actor Training Debug (Iteration 3607) ===
Q mean: -53.135906
Q std: 18.339031
Actor loss: 53.139885
Action reg: 0.003978
  l1.weight: grad_norm = 0.036464
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.089999
Total gradient norm: 0.194929
=== Actor Training Debug (Iteration 3608) ===
Q mean: -55.904198
Q std: 19.301765
Actor loss: 55.908176
Action reg: 0.003981
  l1.weight: grad_norm = 0.004467
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.008959
Total gradient norm: 0.014155
=== Actor Training Debug (Iteration 3609) ===
Q mean: -56.216946
Q std: 18.498346
Actor loss: 56.220932
Action reg: 0.003986
  l1.weight: grad_norm = 0.034433
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.068582
Total gradient norm: 0.130735
=== Actor Training Debug (Iteration 3610) ===
Q mean: -56.601654
Q std: 18.466030
Actor loss: 56.605640
Action reg: 0.003985
  l1.weight: grad_norm = 0.052139
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.115375
Total gradient norm: 0.208826
=== Actor Training Debug (Iteration 3611) ===
Q mean: -55.122536
Q std: 18.251150
Actor loss: 55.126518
Action reg: 0.003982
  l1.weight: grad_norm = 0.000260
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.001114
Total gradient norm: 0.003790
=== Actor Training Debug (Iteration 3612) ===
Q mean: -55.913834
Q std: 18.092123
Actor loss: 55.917831
Action reg: 0.003998
  l1.weight: grad_norm = 0.047929
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.117745
Total gradient norm: 0.241818
=== Actor Training Debug (Iteration 3613) ===
Q mean: -56.040855
Q std: 19.287430
Actor loss: 56.044830
Action reg: 0.003974
  l1.weight: grad_norm = 0.216395
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.509722
Total gradient norm: 1.033685
=== Actor Training Debug (Iteration 3614) ===
Q mean: -56.769318
Q std: 20.773079
Actor loss: 56.773300
Action reg: 0.003983
  l1.weight: grad_norm = 0.007228
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.016744
Total gradient norm: 0.031410
=== Actor Training Debug (Iteration 3615) ===
Q mean: -55.538185
Q std: 20.683475
Actor loss: 55.542156
Action reg: 0.003972
  l1.weight: grad_norm = 0.023918
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.047205
Total gradient norm: 0.068729
=== Actor Training Debug (Iteration 3616) ===
Q mean: -54.923836
Q std: 17.843468
Actor loss: 54.927830
Action reg: 0.003993
  l1.weight: grad_norm = 0.010970
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.021352
Total gradient norm: 0.036342
Total gradient norm: 0.1454272637on 1203) ===
=== Actor Training Debug (Iteration 3627) ===
Q mean: -57.683216
Q std: 18.888882
Actor loss: 57.687214
Action reg: 0.003999
  l1.weight: grad_norm = 0.004737
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.010761
Total gradient norm: 0.019147
=== Actor Training Debug (Iteration 3628) ===
Q mean: -55.130466
Q std: 20.416796
Actor loss: 55.134445
Action reg: 0.003981
  l1.weight: grad_norm = 0.008990
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.019015
Total gradient norm: 0.032998
=== Actor Training Debug (Iteration 3629) ===
Q mean: -55.582836
Q std: 19.185570
Actor loss: 55.586830
Action reg: 0.003992
  l1.weight: grad_norm = 0.095145
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.207783
Total gradient norm: 0.411508
=== Actor Training Debug (Iteration 3630) ===
Q mean: -56.690365
Q std: 18.834837
Actor loss: 56.694355
Action reg: 0.003990
  l1.weight: grad_norm = 0.054830
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.113732
Total gradient norm: 0.180874
=== Actor Training Debug (Iteration 3631) ===
Q mean: -55.685570
Q std: 18.394373
Actor loss: 55.689552
Action reg: 0.003983
  l1.weight: grad_norm = 0.038597
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.082384
Total gradient norm: 0.146123
=== Actor Training Debug (Iteration 3632) ===
Q mean: -57.628731
Q std: 17.770756
Actor loss: 57.632725
Action reg: 0.003995
  l1.weight: grad_norm = 0.002588
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.006444
Total gradient norm: 0.013563
=== Actor Training Debug (Iteration 3633) ===
Q mean: -54.097645
Q std: 19.586628
Actor loss: 54.101646
Action reg: 0.004000
  l1.weight: grad_norm = 0.004371
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.009252
Total gradient norm: 0.017728
=== Actor Training Debug (Iteration 3634) ===
Q mean: -54.988853
Q std: 20.477577
Actor loss: 54.992840
Action reg: 0.003988
  l1.weight: grad_norm = 0.031324
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.072625
Total gradient norm: 0.126622
=== Actor Training Debug (Iteration 3635) ===
Q mean: -54.708176
Q std: 18.095921
Actor loss: 54.712154
Action reg: 0.003980
  l1.weight: grad_norm = 0.067901
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.146528
Total gradient norm: 0.285605
=== Actor Training Debug (Iteration 3636) ===
Q mean: -56.376442
Q std: 18.413483
Actor loss: 56.380436
Action reg: 0.003993
  l1.weight: grad_norm = 0.018301
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.047560
Total gradient norm: 0.095175
=== Actor Training Debug (Iteration 3637) ===
Q mean: -53.274498
Q std: 17.574514
Actor loss: 53.278492
Action reg: 0.003993
  l1.weight: grad_norm = 0.068444
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.141369
Total gradient norm: 0.267902
=== Actor Training Debug (Iteration 3638) ===
Q mean: -57.771049
Q std: 18.783329
Actor loss: 57.775036
Action reg: 0.003985
  l1.weight: grad_norm = 0.018283
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.037479
Total gradient norm: 0.072474
=== Actor Training Debug (Iteration 3639) ===
Q mean: -56.473526
Q std: 19.070211
Actor loss: 56.477520
Action reg: 0.003994
  l1.weight: grad_norm = 0.002329
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.004962
Total gradient norm: 0.009408
=== Actor Training Debug (Iteration 3640) ===
Q mean: -56.225746
Q std: 19.600325
Actor loss: 56.229733
Action reg: 0.003985
  l1.weight: grad_norm = 0.049005
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.111887
Total gradient norm: 0.225160
=== Actor Training Debug (Iteration 3641) ===
Q mean: -53.234085
Q std: 19.648201
Actor loss: 53.238071
Action reg: 0.003985
  l1.weight: grad_norm = 0.122624
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.258675
Total gradient norm: 0.444633
=== Actor Training Debug (Iteration 3642) ===
Q mean: -54.709976
Q std: 19.073822
Actor loss: 54.713951
Action reg: 0.003975
  l1.weight: grad_norm = 0.000456
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.001915
Total gradient norm: 0.006117
=== Actor Training Debug (Iteration 3643) ===
Q mean: -54.899940
Q std: 17.256088
Actor loss: 54.903931
Action reg: 0.003992
  l1.weight: grad_norm = 0.008273
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.019008
Total gradient norm: 0.039708
=== Actor Training Debug (Iteration 3644) ===
Q mean: -55.706795
Q std: 17.934332
Actor loss: 55.710777
Action reg: 0.003983
  l1.weight: grad_norm = 0.000295
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.001526
Total gradient norm: 0.005579
=== Actor Training Debug (Iteration 3645) ===
Q mean: -55.786858
Q std: 17.971079
Actor loss: 55.790844
Action reg: 0.003986
  l1.weight: grad_norm = 0.043439
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.105691
Total gradient norm: 0.220969
=== Actor Training Debug (Iteration 3646) ===
Q mean: -54.590019
Q std: 20.165106
Actor loss: 54.594006
Action reg: 0.003988
  l1.weight: grad_norm = 0.000446
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.001402
Total gradient norm: 0.004409
=== Actor Training Debug (Iteration 3647) ===
Q mean: -55.228073
Q std: 18.339161
Actor loss: 55.232071
Action reg: 0.003998
  l1.weight: grad_norm = 0.023367
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.057314
Total gradient norm: 0.124169
=== Actor Training Debug (Iteration 3648) ===
Q mean: -57.705688
Q std: 20.352867
Actor loss: 57.709671
Action reg: 0.003981
  l1.weight: grad_norm = 0.043492
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.075642
Total gradient norm: 0.107794
=== Actor Training Debug (Iteration 3649) ===
Q mean: -53.983212
Q std: 20.454397
Actor loss: 53.987190
Action reg: 0.003979
  l1.weight: grad_norm = 0.053882
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.114312
Total gradient norm: 0.232611
=== Actor Training Debug (Iteration 3650) ===
Q mean: -54.608635
Q std: 17.721058
Actor loss: 54.612633
Action reg: 0.003998
  l1.weight: grad_norm = 0.008392
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.020931
Total gradient norm: 0.049091
=== Actor Training Debug (Iteration 3651) ===
Q mean: -52.756393
Q std: 17.721252
Actor loss: 52.760391
Action reg: 0.003998
  l1.weight: grad_norm = 0.099675
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.230354
Total gradient norm: 0.423727
=== Actor Training Debug (Iteration 3652) ===
Q mean: -56.453957
Q std: 18.503370
Actor loss: 56.457939
Action reg: 0.003983
  l1.weight: grad_norm = 0.000335
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.001920
Total gradient norm: 0.007399
=== Actor Training Debug (Iteration 3653) ===
Q mean: -58.026402
Q std: 18.602037
Actor loss: 58.030388
Action reg: 0.003987
  l1.weight: grad_norm = 0.002047
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.004886
Total gradient norm: 0.010261
=== Actor Training Debug (Iteration 3654) ===
Q mean: -54.989723
Q std: 18.051010
Actor loss: 54.993725
Action reg: 0.004000
  l1.weight: grad_norm = 0.000427
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000933
Total gradient norm: 0.001548
=== Actor Training Debug (Iteration 3655) ===
Q mean: -54.564690
Q std: 19.507910
Actor loss: 54.568668
Action reg: 0.003979
  l1.weight: grad_norm = 0.111994
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.213720
Total gradient norm: 0.389042
=== Actor Training Debug (Iteration 3656) ===
Q mean: -56.884651
Q std: 19.150421
Actor loss: 56.888626
Action reg: 0.003974
  l1.weight: grad_norm = 0.228060
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.494163
Total gradient norm: 0.888065
=== Actor Training Debug (Iteration 3657) ===
Q mean: -54.323723
Q std: 19.003527
Actor loss: 54.327713
Action reg: 0.003988
  l1.weight: grad_norm = 0.000230
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.001082
Total gradient norm: 0.003752
=== Actor Training Debug (Iteration 3658) ===
Q mean: -55.995728
Q std: 18.749865
Actor loss: 55.999714
Action reg: 0.003986
  l1.weight: grad_norm = 0.070548
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.174736
Total gradient norm: 0.338714
=== Actor Training Debug (Iteration 3659) ===
Q mean: -56.345322
Q std: 19.970852
Actor loss: 56.349281
Action reg: 0.003962
  l1.weight: grad_norm = 0.006135
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.013747
Total gradient norm: 0.025155
=== Actor Training Debug (Iteration 3660) ===
Q mean: -56.569969
Q std: 17.833151
Actor loss: 56.573959
Action reg: 0.003991
  l1.weight: grad_norm = 0.037550
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.067041
Total gradient norm: 0.105796
Total gradient norm: 0.0111922637on 1203) ===
=== Actor Training Debug (Iteration 3671) ===
Q mean: -55.598373
Q std: 18.665373
Actor loss: 55.602360
Action reg: 0.003986
  l1.weight: grad_norm = 0.022608
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.054982
Total gradient norm: 0.112169
=== Actor Training Debug (Iteration 3672) ===
Q mean: -54.430061
Q std: 18.758970
Actor loss: 54.434048
Action reg: 0.003985
  l1.weight: grad_norm = 0.044358
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.100334
Total gradient norm: 0.178848
=== Actor Training Debug (Iteration 3673) ===
Q mean: -56.944695
Q std: 20.306030
Actor loss: 56.948669
Action reg: 0.003974
  l1.weight: grad_norm = 0.022745
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.048937
Total gradient norm: 0.096528
=== Actor Training Debug (Iteration 3674) ===
Q mean: -55.149269
Q std: 20.180399
Actor loss: 55.153240
Action reg: 0.003970
  l1.weight: grad_norm = 0.097177
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.194567
Total gradient norm: 0.362534
=== Actor Training Debug (Iteration 3675) ===
Q mean: -54.110947
Q std: 17.932928
Actor loss: 54.114925
Action reg: 0.003977
  l1.weight: grad_norm = 0.001011
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.003119
Total gradient norm: 0.009148
=== Actor Training Debug (Iteration 3676) ===
Q mean: -54.803085
Q std: 19.556376
Actor loss: 54.807053
Action reg: 0.003967
  l1.weight: grad_norm = 0.133248
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.289366
Total gradient norm: 0.526515
=== Actor Training Debug (Iteration 3677) ===
Q mean: -58.394581
Q std: 18.995428
Actor loss: 58.398563
Action reg: 0.003981
  l1.weight: grad_norm = 0.005337
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.014676
Total gradient norm: 0.033725
=== Actor Training Debug (Iteration 3678) ===
Q mean: -55.440258
Q std: 18.917109
Actor loss: 55.444252
Action reg: 0.003992
  l1.weight: grad_norm = 0.032628
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.058342
Total gradient norm: 0.084647
=== Actor Training Debug (Iteration 3679) ===
Q mean: -56.581467
Q std: 18.214371
Actor loss: 56.585449
Action reg: 0.003984
  l1.weight: grad_norm = 0.166414
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.333078
Total gradient norm: 0.641866
=== Actor Training Debug (Iteration 3680) ===
Q mean: -56.820473
Q std: 19.395197
Actor loss: 56.824451
Action reg: 0.003978
  l1.weight: grad_norm = 0.178451
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.370497
Total gradient norm: 0.725845
=== Actor Training Debug (Iteration 3681) ===
Q mean: -57.603119
Q std: 18.322935
Actor loss: 57.607109
Action reg: 0.003990
  l1.weight: grad_norm = 0.111581
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.282864
Total gradient norm: 0.568503
=== Actor Training Debug (Iteration 3682) ===
Q mean: -56.897125
Q std: 19.391840
Actor loss: 56.901104
Action reg: 0.003981
  l1.weight: grad_norm = 0.046997
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.112093
Total gradient norm: 0.258109
=== Actor Training Debug (Iteration 3683) ===
Q mean: -55.319244
Q std: 19.080696
Actor loss: 55.323223
Action reg: 0.003981
  l1.weight: grad_norm = 0.118155
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.248356
Total gradient norm: 0.439588
=== Actor Training Debug (Iteration 3684) ===
Q mean: -58.011436
Q std: 18.693851
Actor loss: 58.015415
Action reg: 0.003979
  l1.weight: grad_norm = 0.031122
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.070414
Total gradient norm: 0.134744
=== Actor Training Debug (Iteration 3685) ===
Q mean: -56.064804
Q std: 19.208265
Actor loss: 56.068775
Action reg: 0.003972
  l1.weight: grad_norm = 0.066961
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.145869
Total gradient norm: 0.299736
=== Actor Training Debug (Iteration 3686) ===
Q mean: -55.888981
Q std: 18.978951
Actor loss: 55.892967
Action reg: 0.003986
  l1.weight: grad_norm = 0.081323
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.199824
Total gradient norm: 0.424739
=== Actor Training Debug (Iteration 3687) ===
Q mean: -53.958786
Q std: 20.150297
Actor loss: 53.962761
Action reg: 0.003976
  l1.weight: grad_norm = 0.007099
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.018724
Total gradient norm: 0.040227
=== Actor Training Debug (Iteration 3688) ===
Q mean: -58.047127
Q std: 19.300268
Actor loss: 58.051102
Action reg: 0.003977
  l1.weight: grad_norm = 0.188048
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.393564
Total gradient norm: 0.777205
=== Actor Training Debug (Iteration 3689) ===
Q mean: -56.090836
Q std: 20.769966
Actor loss: 56.094795
Action reg: 0.003961
  l1.weight: grad_norm = 0.041282
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.094829
Total gradient norm: 0.169908
=== Actor Training Debug (Iteration 3690) ===
Q mean: -54.934650
Q std: 17.920698
Actor loss: 54.938648
Action reg: 0.003999
  l1.weight: grad_norm = 0.005152
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.012850
Total gradient norm: 0.026347
=== Actor Training Debug (Iteration 3691) ===
Q mean: -55.664974
Q std: 20.489092
Actor loss: 55.668930
Action reg: 0.003958
  l1.weight: grad_norm = 0.154141
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.449434
Total gradient norm: 1.035106
=== Actor Training Debug (Iteration 3692) ===
Q mean: -58.053970
Q std: 19.363724
Actor loss: 58.057945
Action reg: 0.003973
  l1.weight: grad_norm = 0.175710
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.356829
Total gradient norm: 0.612558
=== Actor Training Debug (Iteration 3693) ===
Q mean: -58.474461
Q std: 19.694477
Actor loss: 58.478455
Action reg: 0.003993
  l1.weight: grad_norm = 0.007180
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.015372
Total gradient norm: 0.028377
=== Actor Training Debug (Iteration 3694) ===
Q mean: -56.286591
Q std: 19.540937
Actor loss: 56.290577
Action reg: 0.003985
  l1.weight: grad_norm = 0.037600
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.085201
Total gradient norm: 0.166064
=== Actor Training Debug (Iteration 3695) ===
Q mean: -53.168346
Q std: 18.772743
Actor loss: 53.172333
Action reg: 0.003988
  l1.weight: grad_norm = 0.001354
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.003681
Total gradient norm: 0.008071
=== Actor Training Debug (Iteration 3696) ===
Q mean: -55.435757
Q std: 18.543530
Actor loss: 55.439739
Action reg: 0.003984
  l1.weight: grad_norm = 0.049570
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.106609
Total gradient norm: 0.157604
=== Actor Training Debug (Iteration 3697) ===
Q mean: -56.618080
Q std: 19.251690
Actor loss: 56.622070
Action reg: 0.003992
  l1.weight: grad_norm = 0.031616
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.066772
Total gradient norm: 0.118134
=== Actor Training Debug (Iteration 3698) ===
Q mean: -54.296856
Q std: 19.314447
Actor loss: 54.300842
Action reg: 0.003987
  l1.weight: grad_norm = 0.040825
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.085965
Total gradient norm: 0.170995
=== Actor Training Debug (Iteration 3699) ===
Q mean: -55.239273
Q std: 19.295729
Actor loss: 55.243259
Action reg: 0.003985
  l1.weight: grad_norm = 0.051596
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.142821
Total gradient norm: 0.315653
=== Actor Training Debug (Iteration 3700) ===
Q mean: -57.820469
Q std: 20.904863
Actor loss: 57.824448
Action reg: 0.003979
  l1.weight: grad_norm = 0.047113
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.145821
Total gradient norm: 0.297117
=== Actor Training Debug (Iteration 3701) ===
Q mean: -54.563423
Q std: 19.326595
Actor loss: 54.567406
Action reg: 0.003981
  l1.weight: grad_norm = 0.027194
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.073034
Total gradient norm: 0.142927
=== Actor Training Debug (Iteration 3702) ===
Q mean: -57.014923
Q std: 19.561502
Actor loss: 57.018909
Action reg: 0.003986
  l1.weight: grad_norm = 0.013154
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.024179
Total gradient norm: 0.035209
=== Actor Training Debug (Iteration 3703) ===
Q mean: -58.757999
Q std: 18.664347
Actor loss: 58.761986
Action reg: 0.003987
  l1.weight: grad_norm = 0.033809
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.090417
Total gradient norm: 0.204540
=== Actor Training Debug (Iteration 3704) ===
Q mean: -54.378799
Q std: 19.958586
Actor loss: 54.382767
Action reg: 0.003965
  l1.weight: grad_norm = 0.110906
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.244830
Total gradient norm: 0.460879
=== Actor Training Debug (Iteration 3705) ===
Q mean: -57.545406
Q std: 20.346220
Actor loss: 57.549397
Action reg: 0.003989
  l1.weight: grad_norm = 0.122894
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.287856
Total gradient norm: 0.504069
=== Actor Training Debug (Iteration 3706) ===
Q mean: -56.696903
Q std: 18.568319
Actor loss: 56.700882
Action reg: 0.003979
  l1.weight: grad_norm = 0.117477
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.328651
Total gradient norm: 0.627878
=== Actor Training Debug (Iteration 3707) ===
Q mean: -54.641842
Q std: 18.419266
Actor loss: 54.645832
Action reg: 0.003991
  l1.weight: grad_norm = 0.067198
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.145126
Total gradient norm: 0.325286
=== Actor Training Debug (Iteration 3708) ===
Q mean: -55.923534
Q std: 19.152794
Actor loss: 55.927509
Action reg: 0.003975
  l1.weight: grad_norm = 0.017112
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.037722
Total gradient norm: 0.074781
=== Actor Training Debug (Iteration 3709) ===
Q mean: -57.900307
Q std: 19.906410
Actor loss: 57.904289
Action reg: 0.003983
  l1.weight: grad_norm = 0.013035
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.031497
Total gradient norm: 0.070019
Total gradient norm: 0.0206672637on 1203) ===
=== Actor Training Debug (Iteration 3720) ===
Q mean: -57.919754
Q std: 18.575735
Actor loss: 57.923737
Action reg: 0.003981
  l1.weight: grad_norm = 0.001944
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.006261
Total gradient norm: 0.016996
=== Actor Training Debug (Iteration 3721) ===
Q mean: -54.905899
Q std: 19.150063
Actor loss: 54.909885
Action reg: 0.003986
  l1.weight: grad_norm = 0.000686
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.002156
Total gradient norm: 0.006275
=== Actor Training Debug (Iteration 3722) ===
Q mean: -56.045036
Q std: 18.942133
Actor loss: 56.049015
Action reg: 0.003980
  l1.weight: grad_norm = 0.074307
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.175627
Total gradient norm: 0.409653
=== Actor Training Debug (Iteration 3723) ===
Q mean: -54.525261
Q std: 19.249636
Actor loss: 54.529232
Action reg: 0.003972
  l1.weight: grad_norm = 0.070474
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.137852
Total gradient norm: 0.246460
=== Actor Training Debug (Iteration 3724) ===
Q mean: -56.504677
Q std: 17.653004
Actor loss: 56.508667
Action reg: 0.003989
  l1.weight: grad_norm = 0.036529
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.089789
Total gradient norm: 0.215622
=== Actor Training Debug (Iteration 3725) ===
Q mean: -58.288330
Q std: 17.919802
Actor loss: 58.292320
Action reg: 0.003991
  l1.weight: grad_norm = 0.119388
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.283012
Total gradient norm: 0.578279
=== Actor Training Debug (Iteration 3726) ===
Q mean: -56.681538
Q std: 19.845341
Actor loss: 56.685501
Action reg: 0.003962
  l1.weight: grad_norm = 0.002889
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.006716
Total gradient norm: 0.016237
=== Actor Training Debug (Iteration 3727) ===
Q mean: -55.935635
Q std: 20.601234
Actor loss: 55.939610
Action reg: 0.003976
  l1.weight: grad_norm = 0.027241
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.065512
Total gradient norm: 0.128429
=== Actor Training Debug (Iteration 3728) ===
Q mean: -54.526932
Q std: 19.609903
Actor loss: 54.530914
Action reg: 0.003981
  l1.weight: grad_norm = 0.016504
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.043620
Total gradient norm: 0.087206
=== Actor Training Debug (Iteration 3729) ===
Q mean: -57.196899
Q std: 19.060921
Actor loss: 57.200897
Action reg: 0.003998
  l1.weight: grad_norm = 0.033915
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.086123
Total gradient norm: 0.189509
=== Actor Training Debug (Iteration 3730) ===
Q mean: -58.265568
Q std: 18.831354
Actor loss: 58.269539
Action reg: 0.003969
  l1.weight: grad_norm = 0.030430
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.071692
Total gradient norm: 0.141208
=== Actor Training Debug (Iteration 3731) ===
Q mean: -56.266464
Q std: 18.927151
Actor loss: 56.270458
Action reg: 0.003993
  l1.weight: grad_norm = 0.024549
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.063273
Total gradient norm: 0.135435
=== Actor Training Debug (Iteration 3732) ===
Q mean: -54.989632
Q std: 19.858313
Actor loss: 54.993607
Action reg: 0.003973
  l1.weight: grad_norm = 0.061018
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.118259
Total gradient norm: 0.238702
=== Actor Training Debug (Iteration 3733) ===
Q mean: -57.182812
Q std: 18.976980
Actor loss: 57.186810
Action reg: 0.003998
  l1.weight: grad_norm = 0.089716
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.207800
Total gradient norm: 0.405679
=== Actor Training Debug (Iteration 3734) ===
Q mean: -55.223488
Q std: 18.696087
Actor loss: 55.227474
Action reg: 0.003987
  l1.weight: grad_norm = 0.020231
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.044335
Total gradient norm: 0.084396
=== Actor Training Debug (Iteration 3735) ===
Q mean: -56.119335
Q std: 18.461092
Actor loss: 56.123314
Action reg: 0.003980
  l1.weight: grad_norm = 0.100725
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.192619
Total gradient norm: 0.354608
=== Actor Training Debug (Iteration 3736) ===
Q mean: -57.305634
Q std: 19.014019
Actor loss: 57.309616
Action reg: 0.003981
  l1.weight: grad_norm = 0.045843
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.098345
Total gradient norm: 0.178169
=== Actor Training Debug (Iteration 3737) ===
Q mean: -58.075546
Q std: 19.809320
Actor loss: 58.079533
Action reg: 0.003985
  l1.weight: grad_norm = 0.067470
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.135493
Total gradient norm: 0.267867
=== Actor Training Debug (Iteration 3738) ===
Q mean: -55.162861
Q std: 19.446833
Actor loss: 55.166843
Action reg: 0.003982
  l1.weight: grad_norm = 0.029719
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.077246
Total gradient norm: 0.160573
=== Actor Training Debug (Iteration 3739) ===
Q mean: -57.222343
Q std: 18.445614
Actor loss: 57.226330
Action reg: 0.003987
  l1.weight: grad_norm = 0.004941
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.009508
Total gradient norm: 0.017498
=== Actor Training Debug (Iteration 3740) ===
Q mean: -57.652519
Q std: 19.230919
Actor loss: 57.656506
Action reg: 0.003988
  l1.weight: grad_norm = 0.004022
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.008957
Total gradient norm: 0.018781
=== Actor Training Debug (Iteration 3741) ===
Q mean: -57.767082
Q std: 20.455309
Actor loss: 57.771053
Action reg: 0.003971
  l1.weight: grad_norm = 0.195904
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.415280
Total gradient norm: 0.748124
=== Actor Training Debug (Iteration 3742) ===
Q mean: -55.206718
Q std: 17.514069
Actor loss: 55.210693
Action reg: 0.003976
  l1.weight: grad_norm = 0.001746
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.004324
Total gradient norm: 0.009430
=== Actor Training Debug (Iteration 3743) ===
Q mean: -55.105972
Q std: 17.115847
Actor loss: 55.109974
Action reg: 0.004000
  l1.weight: grad_norm = 0.003411
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.006954
Total gradient norm: 0.011859
=== Actor Training Debug (Iteration 3744) ===
Q mean: -57.316109
Q std: 18.462961
Actor loss: 57.320091
Action reg: 0.003981
  l1.weight: grad_norm = 0.049741
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.122780
Total gradient norm: 0.238061
=== Actor Training Debug (Iteration 3745) ===
Q mean: -56.418526
Q std: 21.261566
Actor loss: 56.422482
Action reg: 0.003957
  l1.weight: grad_norm = 0.001799
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.006230
Total gradient norm: 0.018809
=== Actor Training Debug (Iteration 3746) ===
Q mean: -57.442650
Q std: 17.628523
Actor loss: 57.446636
Action reg: 0.003988
  l1.weight: grad_norm = 0.009018
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.018525
Total gradient norm: 0.033648
=== Actor Training Debug (Iteration 3747) ===
Q mean: -55.731422
Q std: 19.670996
Actor loss: 55.735405
Action reg: 0.003981
  l1.weight: grad_norm = 0.106492
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.241368
Total gradient norm: 0.426436
=== Actor Training Debug (Iteration 3748) ===
Q mean: -54.388718
Q std: 19.960348
Actor loss: 54.392696
Action reg: 0.003977
  l1.weight: grad_norm = 0.001157
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.003767
Total gradient norm: 0.011920
=== Actor Training Debug (Iteration 3749) ===
Q mean: -58.857666
Q std: 19.426762
Actor loss: 58.861652
Action reg: 0.003986
  l1.weight: grad_norm = 0.010669
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.020740
Total gradient norm: 0.033142
=== Actor Training Debug (Iteration 3750) ===
Q mean: -56.119797
Q std: 19.178877
Actor loss: 56.123768
Action reg: 0.003971
  l1.weight: grad_norm = 0.000448
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.003099
Total gradient norm: 0.011303
=== Actor Training Debug (Iteration 3751) ===
Q mean: -56.423294
Q std: 18.208899
Actor loss: 56.427277
Action reg: 0.003983
  l1.weight: grad_norm = 0.157122
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.312788
Total gradient norm: 0.508767
=== Actor Training Debug (Iteration 3752) ===
Q mean: -56.440887
Q std: 17.006618
Actor loss: 56.444874
Action reg: 0.003987
  l1.weight: grad_norm = 0.018764
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.051544
Total gradient norm: 0.108335
=== Actor Training Debug (Iteration 3753) ===
Q mean: -58.039124
Q std: 19.254026
Actor loss: 58.043102
Action reg: 0.003978
  l1.weight: grad_norm = 0.000376
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.002027
Total gradient norm: 0.007144
=== Actor Training Debug (Iteration 3754) ===
Q mean: -55.463470
Q std: 19.265524
Actor loss: 55.467453
Action reg: 0.003983
  l1.weight: grad_norm = 0.060116
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.142305
Total gradient norm: 0.252849
=== Actor Training Debug (Iteration 3755) ===
Q mean: -57.226116
Q std: 19.469694
Actor loss: 57.230095
Action reg: 0.003980
  l1.weight: grad_norm = 0.009700
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.022474
Total gradient norm: 0.046404
=== Actor Training Debug (Iteration 3756) ===
Q mean: -56.138817
Q std: 19.180069
Actor loss: 56.142796
Action reg: 0.003978
  l1.weight: grad_norm = 0.071012
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.170976
Total gradient norm: 0.346378
=== Actor Training Debug (Iteration 3757) ===
Q mean: -58.447510
Q std: 18.969818
Actor loss: 58.451488
Action reg: 0.003981
  l1.weight: grad_norm = 0.050401
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.103101
Total gradient norm: 0.193207
=== Actor Training Debug (Iteration 3758) ===
Q mean: -57.295349
Q std: 18.321571
Actor loss: 57.299343
Action reg: 0.003992
  l1.weight: grad_norm = 0.008976
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.020366
Total gradient norm: 0.039587
=== Actor Training Debug (Iteration 3759) ===
Q mean: -58.839607
Q std: 18.296881
Actor loss: 58.843582
Action reg: 0.003977
  l1.weight: grad_norm = 0.002848
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.005489
Total gradient norm: 0.009942
=== Actor Training Debug (Iteration 3760) ===
Q mean: -54.830551
Q std: 19.571209
Actor loss: 54.834526
Action reg: 0.003974
  l1.weight: grad_norm = 0.123257
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.229150
Total gradient norm: 0.402169
=== Actor Training Debug (Iteration 3761) ===
Q mean: -56.070862
Q std: 19.004284
Actor loss: 56.074860
Action reg: 0.003996
  l1.weight: grad_norm = 0.003082
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.008424
Total gradient norm: 0.018918
=== Actor Training Debug (Iteration 3762) ===
Q mean: -58.022522
Q std: 19.004345
Actor loss: 58.026512
Action reg: 0.003992
  l1.weight: grad_norm = 0.031590
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.089002
Total gradient norm: 0.206100
=== Actor Training Debug (Iteration 3763) ===
Q mean: -56.306671
Q std: 20.301394
Actor loss: 56.310646
Action reg: 0.003974
  l1.weight: grad_norm = 0.083639
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.192637
Total gradient norm: 0.350713
=== Actor Training Debug (Iteration 3764) ===
Q mean: -58.843124
Q std: 19.621370
Actor loss: 58.847099
Action reg: 0.003973
  l1.weight: grad_norm = 0.028229
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.076315
Total gradient norm: 0.163836
=== Actor Training Debug (Iteration 3765) ===
Q mean: -55.162083
Q std: 19.603821
Actor loss: 55.166073
Action reg: 0.003990
  l1.weight: grad_norm = 0.000416
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.001429
Total gradient norm: 0.004693
=== Actor Training Debug (Iteration 3766) ===
Q mean: -56.640301
Q std: 19.012308
Actor loss: 56.644279
Action reg: 0.003978
  l1.weight: grad_norm = 0.138223
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.258683
Total gradient norm: 0.505236
=== Actor Training Debug (Iteration 3767) ===
Q mean: -57.431328
Q std: 18.609379
Actor loss: 57.435318
Action reg: 0.003991
  l1.weight: grad_norm = 0.147921
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.322906
Total gradient norm: 0.573287
=== Actor Training Debug (Iteration 3768) ===
Q mean: -57.253399
Q std: 20.300600
Actor loss: 57.257374
Action reg: 0.003975
  l1.weight: grad_norm = 0.104821
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.253562
Total gradient norm: 0.535121
=== Actor Training Debug (Iteration 3769) ===
Q mean: -57.347790
Q std: 19.868574
Actor loss: 57.351780
Action reg: 0.003989
  l1.weight: grad_norm = 0.011469
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.031957
Total gradient norm: 0.065214
=== Actor Training Debug (Iteration 3770) ===
Q mean: -55.446457
Q std: 20.876480
Actor loss: 55.450424
Action reg: 0.003969
  l1.weight: grad_norm = 0.012770
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.023104
Total gradient norm: 0.033691
=== Actor Training Debug (Iteration 3771) ===
Q mean: -55.936726
Q std: 17.841152
Actor loss: 55.940727
Action reg: 0.004000
  l1.weight: grad_norm = 0.003416
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.007160
Total gradient norm: 0.014498
=== Actor Training Debug (Iteration 3772) ===
Q mean: -56.609940
Q std: 18.657007
Actor loss: 56.613918
Action reg: 0.003979
  l1.weight: grad_norm = 0.177981
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.402934
Total gradient norm: 0.816979
=== Actor Training Debug (Iteration 3773) ===
Q mean: -58.968231
Q std: 20.562405
Actor loss: 58.972214
Action reg: 0.003983
  l1.weight: grad_norm = 0.006602
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.013246
Total gradient norm: 0.024928
=== Actor Training Debug (Iteration 3774) ===
Q mean: -56.408325
Q std: 19.472145
Actor loss: 56.412300
Action reg: 0.003974
  l1.weight: grad_norm = 0.099885
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.191950
Total gradient norm: 0.381195
=== Actor Training Debug (Iteration 3775) ===
Q mean: -56.093002
Q std: 19.156439
Actor loss: 56.096996
Action reg: 0.003994
  l1.weight: grad_norm = 0.003569
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.008222
Total gradient norm: 0.015233
=== Actor Training Debug (Iteration 3776) ===
Q mean: -59.376736
Q std: 20.697620
Actor loss: 59.380726
Action reg: 0.003991
  l1.weight: grad_norm = 0.158394
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.388732
Total gradient norm: 0.755115
=== Actor Training Debug (Iteration 3777) ===
Q mean: -58.030899
Q std: 18.751053
Actor loss: 58.034897
Action reg: 0.003998
  l1.weight: grad_norm = 0.080711
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.193811
Total gradient norm: 0.370343
=== Actor Training Debug (Iteration 3778) ===
Q mean: -56.099949
Q std: 19.116411
Actor loss: 56.103931
Action reg: 0.003982
  l1.weight: grad_norm = 0.009212
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.022962
Total gradient norm: 0.042919
=== Actor Training Debug (Iteration 3779) ===
Q mean: -57.612747
Q std: 18.653610
Actor loss: 57.616722
Action reg: 0.003975
  l1.weight: grad_norm = 0.088587
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.186775
Total gradient norm: 0.366169
Total gradient norm: 0.3898942637on 1203) ===
=== Actor Training Debug (Iteration 3790) ===
Q mean: -56.368988
Q std: 19.709637
Actor loss: 56.372971
Action reg: 0.003982
  l1.weight: grad_norm = 0.000552
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.002462
Total gradient norm: 0.009114
=== Actor Training Debug (Iteration 3791) ===
Q mean: -56.680397
Q std: 19.754038
Actor loss: 56.684380
Action reg: 0.003984
  l1.weight: grad_norm = 0.011175
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.028439
Total gradient norm: 0.056875
=== Actor Training Debug (Iteration 3792) ===
Q mean: -58.831356
Q std: 19.706760
Actor loss: 58.835331
Action reg: 0.003975
  l1.weight: grad_norm = 0.031423
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.072810
Total gradient norm: 0.125692
=== Actor Training Debug (Iteration 3793) ===
Q mean: -56.467735
Q std: 19.206842
Actor loss: 56.471718
Action reg: 0.003982
  l1.weight: grad_norm = 0.000515
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.002089
Total gradient norm: 0.006460
=== Actor Training Debug (Iteration 3794) ===
Q mean: -56.550819
Q std: 18.968418
Actor loss: 56.554802
Action reg: 0.003983
  l1.weight: grad_norm = 0.016191
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.037237
Total gradient norm: 0.082118
=== Actor Training Debug (Iteration 3795) ===
Q mean: -59.390450
Q std: 18.808622
Actor loss: 59.394428
Action reg: 0.003980
  l1.weight: grad_norm = 0.002224
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.005504
Total gradient norm: 0.011309
=== Actor Training Debug (Iteration 3796) ===
Q mean: -57.705807
Q std: 18.584126
Actor loss: 57.709797
Action reg: 0.003989
  l1.weight: grad_norm = 0.000765
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.002286
Total gradient norm: 0.005902
=== Actor Training Debug (Iteration 3797) ===
Q mean: -57.358131
Q std: 20.127420
Actor loss: 57.362099
Action reg: 0.003969
  l1.weight: grad_norm = 0.009983
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.019793
Total gradient norm: 0.031547
=== Actor Training Debug (Iteration 3798) ===
Q mean: -56.290806
Q std: 21.479879
Actor loss: 56.294769
Action reg: 0.003964
  l1.weight: grad_norm = 0.000591
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.003735
Total gradient norm: 0.013606
=== Actor Training Debug (Iteration 3799) ===
Q mean: -56.738693
Q std: 19.061255
Actor loss: 56.742680
Action reg: 0.003987
  l1.weight: grad_norm = 0.002027
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.004362
Total gradient norm: 0.007386
=== Actor Training Debug (Iteration 3800) ===
Q mean: -56.306160
Q std: 17.944899
Actor loss: 56.310154
Action reg: 0.003995
  l1.weight: grad_norm = 0.039563
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.099439
Total gradient norm: 0.198516
=== Actor Training Debug (Iteration 3801) ===
Q mean: -55.994789
Q std: 19.819912
Actor loss: 55.998787
Action reg: 0.003998
  l1.weight: grad_norm = 0.066164
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.145024
Total gradient norm: 0.263209
=== Actor Training Debug (Iteration 3802) ===
Q mean: -58.643387
Q std: 19.128557
Actor loss: 58.647377
Action reg: 0.003990
  l1.weight: grad_norm = 0.132311
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.279638
Total gradient norm: 0.523660
=== Actor Training Debug (Iteration 3803) ===
Q mean: -56.469093
Q std: 19.824291
Actor loss: 56.473072
Action reg: 0.003980
  l1.weight: grad_norm = 0.003273
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.008455
Total gradient norm: 0.016280
=== Actor Training Debug (Iteration 3804) ===
Q mean: -55.412479
Q std: 19.091171
Actor loss: 55.416470
Action reg: 0.003989
  l1.weight: grad_norm = 0.000415
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.001553
Total gradient norm: 0.004948
=== Actor Training Debug (Iteration 3805) ===
Q mean: -56.230656
Q std: 19.089407
Actor loss: 56.234627
Action reg: 0.003972
  l1.weight: grad_norm = 0.034886
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.074777
Total gradient norm: 0.128593
=== Actor Training Debug (Iteration 3806) ===
Q mean: -56.711472
Q std: 19.059450
Actor loss: 56.715446
Action reg: 0.003976
  l1.weight: grad_norm = 0.001835
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.003729
Total gradient norm: 0.007819
=== Actor Training Debug (Iteration 3807) ===
Q mean: -59.696354
Q std: 20.147917
Actor loss: 59.700336
Action reg: 0.003982
  l1.weight: grad_norm = 0.019573
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.051369
Total gradient norm: 0.113267
=== Actor Training Debug (Iteration 3808) ===
Q mean: -56.410133
Q std: 20.257143
Actor loss: 56.414127
Action reg: 0.003993
  l1.weight: grad_norm = 0.065189
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.128901
Total gradient norm: 0.235223
=== Actor Training Debug (Iteration 3809) ===
Q mean: -56.831169
Q std: 19.096786
Actor loss: 56.835155
Action reg: 0.003987
  l1.weight: grad_norm = 0.036443
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.084355
Total gradient norm: 0.188573
=== Actor Training Debug (Iteration 3810) ===
Q mean: -56.594894
Q std: 18.586288
Actor loss: 56.598881
Action reg: 0.003986
  l1.weight: grad_norm = 0.033542
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.075379
Total gradient norm: 0.106226
=== Actor Training Debug (Iteration 3811) ===
Q mean: -58.598076
Q std: 19.209320
Actor loss: 58.602058
Action reg: 0.003982
  l1.weight: grad_norm = 0.001182
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.003483
Total gradient norm: 0.010655
=== Actor Training Debug (Iteration 3812) ===
Q mean: -56.060692
Q std: 18.502188
Actor loss: 56.064678
Action reg: 0.003987
  l1.weight: grad_norm = 0.035036
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.077053
Total gradient norm: 0.145106
=== Actor Training Debug (Iteration 3813) ===
Q mean: -57.009598
Q std: 18.029902
Actor loss: 57.013573
Action reg: 0.003976
  l1.weight: grad_norm = 0.009515
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.026099
Total gradient norm: 0.053101
=== Actor Training Debug (Iteration 3814) ===
Q mean: -56.451401
Q std: 18.148737
Actor loss: 56.455383
Action reg: 0.003981
  l1.weight: grad_norm = 0.081750
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.191854
Total gradient norm: 0.367002
=== Actor Training Debug (Iteration 3815) ===
Q mean: -56.769997
Q std: 19.168482
Actor loss: 56.773975
Action reg: 0.003981
  l1.weight: grad_norm = 0.001092
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.003118
Total gradient norm: 0.009238
=== Actor Training Debug (Iteration 3816) ===
Q mean: -54.475555
Q std: 20.544044
Actor loss: 54.479534
Action reg: 0.003979
  l1.weight: grad_norm = 0.016162
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.037453
Total gradient norm: 0.077795
=== Actor Training Debug (Iteration 3817) ===
Q mean: -56.513142
Q std: 20.024220
Actor loss: 56.517124
Action reg: 0.003982
  l1.weight: grad_norm = 0.012408
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.023581
Total gradient norm: 0.046568
=== Actor Training Debug (Iteration 3818) ===
Q mean: -57.674210
Q std: 20.251360
Actor loss: 57.678185
Action reg: 0.003974
  l1.weight: grad_norm = 0.010570
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.025073
Total gradient norm: 0.052208
=== Actor Training Debug (Iteration 3819) ===
Q mean: -60.029739
Q std: 18.644342
Actor loss: 60.033718
Action reg: 0.003980
  l1.weight: grad_norm = 0.001153
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.002989
Total gradient norm: 0.007512
=== Actor Training Debug (Iteration 3820) ===
Q mean: -55.757347
Q std: 17.806320
Actor loss: 55.761333
Action reg: 0.003986
  l1.weight: grad_norm = 0.223714
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.457668
Total gradient norm: 0.907945
=== Actor Training Debug (Iteration 3821) ===
Q mean: -57.433525
Q std: 18.425743
Actor loss: 57.437504
Action reg: 0.003980
  l1.weight: grad_norm = 0.042867
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.085207
Total gradient norm: 0.132344
=== Actor Training Debug (Iteration 3822) ===
Q mean: -57.473999
Q std: 19.746059
Actor loss: 57.477989
Action reg: 0.003989
  l1.weight: grad_norm = 0.003375
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.008038
Total gradient norm: 0.019324
=== Actor Training Debug (Iteration 3823) ===
Q mean: -57.574390
Q std: 19.803337
Actor loss: 57.578377
Action reg: 0.003988
  l1.weight: grad_norm = 0.005452
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.011957
Total gradient norm: 0.025316
=== Actor Training Debug (Iteration 3824) ===
Q mean: -57.523312
Q std: 19.641254
Actor loss: 57.527287
Action reg: 0.003975
  l1.weight: grad_norm = 0.000623
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.002894
Total gradient norm: 0.009395
=== Actor Training Debug (Iteration 3825) ===
Q mean: -59.043259
Q std: 19.192009
Actor loss: 59.047245
Action reg: 0.003985
  l1.weight: grad_norm = 0.061761
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.148450
Total gradient norm: 0.281493
=== Actor Training Debug (Iteration 3826) ===
Q mean: -58.129349
Q std: 19.078691
Actor loss: 58.133331
Action reg: 0.003984
  l1.weight: grad_norm = 0.096204
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.205564
Total gradient norm: 0.430114
=== Actor Training Debug (Iteration 3827) ===
Q mean: -57.881638
Q std: 19.335257
Actor loss: 57.885620
Action reg: 0.003981
  l1.weight: grad_norm = 0.114326
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.287712
Total gradient norm: 0.585535
=== Actor Training Debug (Iteration 3828) ===
Q mean: -55.038177
Q std: 20.130836
Actor loss: 55.042171
Action reg: 0.003993
  l1.weight: grad_norm = 0.053727
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.115793
Total gradient norm: 0.220023
=== Actor Training Debug (Iteration 3829) ===
Q mean: -54.273872
Q std: 20.884285
Actor loss: 54.277840
Action reg: 0.003969
  l1.weight: grad_norm = 0.002184
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.006243
Total gradient norm: 0.016956
=== Actor Training Debug (Iteration 3830) ===
Q mean: -60.500565
Q std: 19.622103
Actor loss: 60.504559
Action reg: 0.003993
  l1.weight: grad_norm = 0.000263
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.001005
Total gradient norm: 0.002792
=== Actor Training Debug (Iteration 3831) ===
Q mean: -58.724064
Q std: 18.753595
Actor loss: 58.728062
Action reg: 0.003999
  l1.weight: grad_norm = 0.020204
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.052918
Total gradient norm: 0.102655
=== Actor Training Debug (Iteration 3832) ===
Q mean: -53.558846
Q std: 18.860996
Actor loss: 53.562805
Action reg: 0.003958
  l1.weight: grad_norm = 0.056456
  l1.bias: grad_norm = 0.000521
  l2.weight: grad_norm = 0.133444
Total gradient norm: 0.289775
=== Actor Training Debug (Iteration 3833) ===
Q mean: -55.358932
Q std: 17.365189
Actor loss: 55.362926
Action reg: 0.003992
  l1.weight: grad_norm = 0.016412
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.032686
Total gradient norm: 0.062228
=== Actor Training Debug (Iteration 3834) ===
Q mean: -57.088524
Q std: 18.076723
Actor loss: 57.092518
Action reg: 0.003995
  l1.weight: grad_norm = 0.000359
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.001308
Total gradient norm: 0.003357
=== Actor Training Debug (Iteration 3835) ===
Q mean: -58.203430
Q std: 19.475306
Actor loss: 58.207405
Action reg: 0.003977
  l1.weight: grad_norm = 0.003696
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.008449
Total gradient norm: 0.018612
=== Actor Training Debug (Iteration 3836) ===
Q mean: -57.798996
Q std: 19.744047
Actor loss: 57.802971
Action reg: 0.003975
  l1.weight: grad_norm = 0.004817
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.010684
Total gradient norm: 0.019634
=== Actor Training Debug (Iteration 3837) ===
Q mean: -56.683640
Q std: 19.002602
Actor loss: 56.687637
Action reg: 0.003997
  l1.weight: grad_norm = 0.040684
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.092471
Total gradient norm: 0.167794
=== Actor Training Debug (Iteration 3838) ===
Q mean: -58.512749
Q std: 21.009537
Actor loss: 58.516727
Action reg: 0.003977
  l1.weight: grad_norm = 0.000685
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.001946
Total gradient norm: 0.005634
=== Actor Training Debug (Iteration 3839) ===
Q mean: -59.970600
Q std: 19.437975
Actor loss: 59.974586
Action reg: 0.003988
  l1.weight: grad_norm = 0.020570
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.045500
Total gradient norm: 0.102837
=== Actor Training Debug (Iteration 3840) ===
Q mean: -56.579971
Q std: 20.191488
Actor loss: 56.583954
Action reg: 0.003982
  l1.weight: grad_norm = 0.000441
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.002027
Total gradient norm: 0.006604
=== Actor Training Debug (Iteration 3841) ===
Q mean: -57.969078
Q std: 19.508648
Actor loss: 57.973049
Action reg: 0.003971
  l1.weight: grad_norm = 0.000753
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.003187
Total gradient norm: 0.010021
=== Actor Training Debug (Iteration 3842) ===
Q mean: -59.415897
Q std: 19.054708
Actor loss: 59.419884
Action reg: 0.003987
  l1.weight: grad_norm = 0.000984
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.002736
Total gradient norm: 0.007050
=== Actor Training Debug (Iteration 3843) ===
Q mean: -57.864746
Q std: 18.842726
Actor loss: 57.868740
Action reg: 0.003994
  l1.weight: grad_norm = 0.025431
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.048980
Total gradient norm: 0.109102
=== Actor Training Debug (Iteration 3844) ===
Q mean: -56.188774
Q std: 19.166189
Actor loss: 56.192760
Action reg: 0.003988
  l1.weight: grad_norm = 0.007999
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.014183
Total gradient norm: 0.025066
=== Actor Training Debug (Iteration 3845) ===
Q mean: -57.677010
Q std: 18.015339
Actor loss: 57.681004
Action reg: 0.003993
  l1.weight: grad_norm = 0.003452
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.006864
Total gradient norm: 0.011877
=== Actor Training Debug (Iteration 3846) ===
Q mean: -59.684853
Q std: 19.899529
Actor loss: 59.688847
Action reg: 0.003992
  l1.weight: grad_norm = 0.072530
  l1.bias: grad_norm = 0.000130
  l1.weight: grad_norm = 0.000596on 1203) ===
  l1.bias: grad_norm = 0.000378
  l2.weight: grad_norm = 0.003395
Total gradient norm: 0.011120
=== Actor Training Debug (Iteration 3857) ===
Q mean: -57.048447
Q std: 18.942261
Actor loss: 57.052433
Action reg: 0.003986
  l1.weight: grad_norm = 0.023621
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.050104
Total gradient norm: 0.105591
=== Actor Training Debug (Iteration 3858) ===
Q mean: -57.625870
Q std: 18.640764
Actor loss: 57.629864
Action reg: 0.003992
  l1.weight: grad_norm = 0.050629
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.113061
Total gradient norm: 0.200597
=== Actor Training Debug (Iteration 3859) ===
Q mean: -57.879662
Q std: 20.285391
Actor loss: 57.883648
Action reg: 0.003988
  l1.weight: grad_norm = 0.044293
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.100933
Total gradient norm: 0.201139
=== Actor Training Debug (Iteration 3860) ===
Q mean: -56.763557
Q std: 18.233461
Actor loss: 56.767536
Action reg: 0.003978
  l1.weight: grad_norm = 0.038678
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.070209
Total gradient norm: 0.124309
=== Actor Training Debug (Iteration 3861) ===
Q mean: -57.546810
Q std: 19.507357
Actor loss: 57.550812
Action reg: 0.004000
  l1.weight: grad_norm = 0.000383
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000760
Total gradient norm: 0.001252
=== Actor Training Debug (Iteration 3862) ===
Q mean: -58.849968
Q std: 19.605637
Actor loss: 58.853939
Action reg: 0.003970
  l1.weight: grad_norm = 0.011509
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.020403
Total gradient norm: 0.030072
=== Actor Training Debug (Iteration 3863) ===
Q mean: -57.797337
Q std: 18.925142
Actor loss: 57.801323
Action reg: 0.003988
  l1.weight: grad_norm = 0.010114
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.024046
Total gradient norm: 0.051417
=== Actor Training Debug (Iteration 3864) ===
Q mean: -58.403454
Q std: 19.915758
Actor loss: 58.407448
Action reg: 0.003994
  l1.weight: grad_norm = 0.016104
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.038361
Total gradient norm: 0.080232
=== Actor Training Debug (Iteration 3865) ===
Q mean: -55.563065
Q std: 19.167173
Actor loss: 55.567051
Action reg: 0.003986
  l1.weight: grad_norm = 0.020886
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.045892
Total gradient norm: 0.091209
=== Actor Training Debug (Iteration 3866) ===
Q mean: -54.852509
Q std: 19.096075
Actor loss: 54.856480
Action reg: 0.003972
  l1.weight: grad_norm = 0.039002
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.079564
Total gradient norm: 0.143199
=== Actor Training Debug (Iteration 3867) ===
Q mean: -59.010246
Q std: 19.419165
Actor loss: 59.014240
Action reg: 0.003994
  l1.weight: grad_norm = 0.011387
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.023727
Total gradient norm: 0.045025
=== Actor Training Debug (Iteration 3868) ===
Q mean: -58.842003
Q std: 20.184568
Actor loss: 58.845985
Action reg: 0.003983
  l1.weight: grad_norm = 0.023244
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.052604
Total gradient norm: 0.118780
=== Actor Training Debug (Iteration 3869) ===
Q mean: -56.327171
Q std: 18.494148
Actor loss: 56.331158
Action reg: 0.003985
  l1.weight: grad_norm = 0.034393
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.067874
Total gradient norm: 0.102215
=== Actor Training Debug (Iteration 3870) ===
Q mean: -55.916855
Q std: 18.497507
Actor loss: 55.920837
Action reg: 0.003984
  l1.weight: grad_norm = 0.092663
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.169991
Total gradient norm: 0.250922
=== Actor Training Debug (Iteration 3871) ===
Q mean: -55.965530
Q std: 19.478752
Actor loss: 55.969509
Action reg: 0.003978
  l1.weight: grad_norm = 0.143798
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.328285
Total gradient norm: 0.757461
=== Actor Training Debug (Iteration 3872) ===
Q mean: -56.925194
Q std: 20.247658
Actor loss: 56.929169
Action reg: 0.003973
  l1.weight: grad_norm = 0.047358
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.097811
Total gradient norm: 0.201841
=== Actor Training Debug (Iteration 3873) ===
Q mean: -55.033943
Q std: 19.512617
Actor loss: 55.037918
Action reg: 0.003973
  l1.weight: grad_norm = 0.121591
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.211970
Total gradient norm: 0.320129
=== Actor Training Debug (Iteration 3874) ===
Q mean: -58.959824
Q std: 18.424006
Actor loss: 58.963810
Action reg: 0.003986
  l1.weight: grad_norm = 0.035620
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.084957
Total gradient norm: 0.158148
=== Actor Training Debug (Iteration 3875) ===
Q mean: -57.349720
Q std: 18.768274
Actor loss: 57.353706
Action reg: 0.003987
  l1.weight: grad_norm = 0.028743
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.058646
Total gradient norm: 0.110791
=== Actor Training Debug (Iteration 3876) ===
Q mean: -55.769257
Q std: 19.818659
Actor loss: 55.773247
Action reg: 0.003989
  l1.weight: grad_norm = 0.017916
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.037915
Total gradient norm: 0.068327
=== Actor Training Debug (Iteration 3877) ===
Q mean: -57.812473
Q std: 19.971111
Actor loss: 57.816456
Action reg: 0.003983
  l1.weight: grad_norm = 0.076079
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.201888
Total gradient norm: 0.485713
=== Actor Training Debug (Iteration 3878) ===
Q mean: -59.091351
Q std: 18.716871
Actor loss: 59.095341
Action reg: 0.003990
  l1.weight: grad_norm = 0.105742
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.225873
Total gradient norm: 0.477582
=== Actor Training Debug (Iteration 3879) ===
Q mean: -59.639740
Q std: 18.715410
Actor loss: 59.643715
Action reg: 0.003974
  l1.weight: grad_norm = 0.222508
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.529161
Total gradient norm: 0.903288
=== Actor Training Debug (Iteration 3880) ===
Q mean: -58.397842
Q std: 19.800884
Actor loss: 58.401825
Action reg: 0.003981
  l1.weight: grad_norm = 0.000647
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.002498
Total gradient norm: 0.006941
=== Actor Training Debug (Iteration 3881) ===
Q mean: -58.658340
Q std: 18.851767
Actor loss: 58.662331
Action reg: 0.003988
  l1.weight: grad_norm = 0.000540
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.001882
Total gradient norm: 0.005211
=== Actor Training Debug (Iteration 3882) ===
Q mean: -58.387726
Q std: 18.592285
Actor loss: 58.391712
Action reg: 0.003987
  l1.weight: grad_norm = 0.066306
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.121922
Total gradient norm: 0.168919
=== Actor Training Debug (Iteration 3883) ===
Q mean: -58.605522
Q std: 20.703239
Actor loss: 58.609493
Action reg: 0.003973
  l1.weight: grad_norm = 0.007538
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.020219
Total gradient norm: 0.044623
=== Actor Training Debug (Iteration 3884) ===
Q mean: -58.672234
Q std: 19.004570
Actor loss: 58.676220
Action reg: 0.003988
  l1.weight: grad_norm = 0.042357
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.083403
Total gradient norm: 0.148350
=== Actor Training Debug (Iteration 3885) ===
Q mean: -59.549232
Q std: 18.936043
Actor loss: 59.553215
Action reg: 0.003983
  l1.weight: grad_norm = 0.019950
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.045645
Total gradient norm: 0.084187
=== Actor Training Debug (Iteration 3886) ===
Q mean: -60.826149
Q std: 19.629385
Actor loss: 60.830124
Action reg: 0.003975
  l1.weight: grad_norm = 0.009137
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.023897
Total gradient norm: 0.052481
=== Actor Training Debug (Iteration 3887) ===
Q mean: -58.072121
Q std: 19.866079
Actor loss: 58.076103
Action reg: 0.003981
  l1.weight: grad_norm = 0.058147
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.135134
Total gradient norm: 0.246219
=== Actor Training Debug (Iteration 3888) ===
Q mean: -55.563011
Q std: 18.058680
Actor loss: 55.566998
Action reg: 0.003988
  l1.weight: grad_norm = 0.031025
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.064052
Total gradient norm: 0.144057
=== Actor Training Debug (Iteration 3889) ===
Q mean: -60.239429
Q std: 19.144587
Actor loss: 60.243404
Action reg: 0.003975
Action reg: 0.003979rm = 0.000596on 1203) ===
  l1.weight: grad_norm = 0.060396
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.143900
Total gradient norm: 0.288878
=== Actor Training Debug (Iteration 3900) ===
Q mean: -54.818817
Q std: 19.422749
Actor loss: 54.822796
Action reg: 0.003981
  l1.weight: grad_norm = 0.082116
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.178750
Total gradient norm: 0.352149
=== Actor Training Debug (Iteration 3901) ===
Q mean: -56.333508
Q std: 19.611288
Actor loss: 56.337490
Action reg: 0.003983
  l1.weight: grad_norm = 0.000247
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.001237
Total gradient norm: 0.004056
=== Actor Training Debug (Iteration 3902) ===
Q mean: -58.259693
Q std: 18.596441
Actor loss: 58.263691
Action reg: 0.003998
  l1.weight: grad_norm = 0.005039
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.011005
Total gradient norm: 0.022144
=== Actor Training Debug (Iteration 3903) ===
Q mean: -58.283562
Q std: 18.251802
Actor loss: 58.287556
Action reg: 0.003992
  l1.weight: grad_norm = 0.073151
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.175752
Total gradient norm: 0.395943
=== Actor Training Debug (Iteration 3904) ===
Q mean: -57.912880
Q std: 19.450897
Actor loss: 57.916882
Action reg: 0.004000
  l1.weight: grad_norm = 0.000422
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000887
Total gradient norm: 0.001715
=== Actor Training Debug (Iteration 3905) ===
Q mean: -57.100052
Q std: 20.506666
Actor loss: 57.104023
Action reg: 0.003971
  l1.weight: grad_norm = 0.002945
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.007766
Total gradient norm: 0.017999
=== Actor Training Debug (Iteration 3906) ===
Q mean: -56.920471
Q std: 20.260410
Actor loss: 56.924446
Action reg: 0.003977
  l1.weight: grad_norm = 0.003341
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.006694
Total gradient norm: 0.011324
=== Actor Training Debug (Iteration 3907) ===
Q mean: -56.602081
Q std: 19.996382
Actor loss: 56.606083
Action reg: 0.004000
  l1.weight: grad_norm = 0.005915
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.014206
Total gradient norm: 0.026574
=== Actor Training Debug (Iteration 3908) ===
Q mean: -59.764069
Q std: 20.409369
Actor loss: 59.768040
Action reg: 0.003971
  l1.weight: grad_norm = 0.067530
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.145704
Total gradient norm: 0.278763
=== Actor Training Debug (Iteration 3909) ===
Q mean: -58.554028
Q std: 17.682112
Actor loss: 58.558018
Action reg: 0.003992
  l1.weight: grad_norm = 0.142802
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.327668
Total gradient norm: 0.561090
=== Actor Training Debug (Iteration 3910) ===
Q mean: -59.159912
Q std: 18.859451
Actor loss: 59.163895
Action reg: 0.003984
  l1.weight: grad_norm = 0.326372
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.767854
Total gradient norm: 1.311046
=== Actor Training Debug (Iteration 3911) ===
Q mean: -56.917946
Q std: 18.615698
Actor loss: 56.921932
Action reg: 0.003988
  l1.weight: grad_norm = 0.024735
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.055989
Total gradient norm: 0.119076
=== Actor Training Debug (Iteration 3912) ===
Q mean: -57.835266
Q std: 18.885572
Actor loss: 57.839249
Action reg: 0.003983
  l1.weight: grad_norm = 0.013330
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.032925
Total gradient norm: 0.064687
=== Actor Training Debug (Iteration 3913) ===
Q mean: -55.644516
Q std: 19.486267
Actor loss: 55.648502
Action reg: 0.003988
  l1.weight: grad_norm = 0.106198
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.240211
Total gradient norm: 0.481900
=== Actor Training Debug (Iteration 3914) ===
Q mean: -56.631824
Q std: 19.418484
Actor loss: 56.635807
Action reg: 0.003982
  l1.weight: grad_norm = 0.007054
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.013667
Total gradient norm: 0.022269
=== Actor Training Debug (Iteration 3915) ===
Q mean: -58.610794
Q std: 20.206562
Actor loss: 58.614773
Action reg: 0.003979
  l1.weight: grad_norm = 0.125119
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.278788
Total gradient norm: 0.560612
=== Actor Training Debug (Iteration 3916) ===
Q mean: -56.913166
Q std: 21.060234
Actor loss: 56.917137
Action reg: 0.003969
  l1.weight: grad_norm = 0.032181
  l1.bias: grad_norm = 0.000602
  l2.weight: grad_norm = 0.099614
Total gradient norm: 0.193361
=== Actor Training Debug (Iteration 3917) ===
Q mean: -57.205620
Q std: 20.777176
Actor loss: 57.209587
Action reg: 0.003966
  l1.weight: grad_norm = 0.009305
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.026686
Total gradient norm: 0.053598
=== Actor Training Debug (Iteration 3918) ===
Q mean: -58.573887
Q std: 19.788565
Actor loss: 58.577866
Action reg: 0.003977
  l1.weight: grad_norm = 0.005216
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.011589
Total gradient norm: 0.018931
=== Actor Training Debug (Iteration 3919) ===
Q mean: -57.431774
Q std: 20.700163
Actor loss: 57.435760
Action reg: 0.003985
  l1.weight: grad_norm = 0.253031
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.529203
Total gradient norm: 1.086113
=== Actor Training Debug (Iteration 3920) ===
Q mean: -60.509567
Q std: 19.698198
Actor loss: 60.513538
Action reg: 0.003973
  l1.weight: grad_norm = 0.154272
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.301937
Total gradient norm: 0.545145
=== Actor Training Debug (Iteration 3921) ===
Q mean: -59.938904
Q std: 21.488775
Actor loss: 59.942867
Action reg: 0.003965
  l1.weight: grad_norm = 0.099106
  l1.bias: grad_norm = 0.000425
  l2.weight: grad_norm = 0.183042
Total gradient norm: 0.354140
=== Actor Training Debug (Iteration 3922) ===
Q mean: -56.435745
Q std: 19.830795
Actor loss: 56.439728
Action reg: 0.003984
  l1.weight: grad_norm = 0.209242
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.437430
Total gradient norm: 0.725073
=== Actor Training Debug (Iteration 3923) ===
Q mean: -56.360023
Q std: 19.111691
Actor loss: 56.364002
Action reg: 0.003977
  l1.weight: grad_norm = 0.008327
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.019136
Total gradient norm: 0.034937
=== Actor Training Debug (Iteration 3924) ===
Q mean: -56.453560
Q std: 19.284351
Actor loss: 56.457546
Action reg: 0.003986
  l1.weight: grad_norm = 0.045866
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.108377
Total gradient norm: 0.209769
=== Actor Training Debug (Iteration 3925) ===
Q mean: -58.796837
Q std: 19.875713
Actor loss: 58.800823
Action reg: 0.003988
  l1.weight: grad_norm = 0.013067
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.029694
Total gradient norm: 0.066894
=== Actor Training Debug (Iteration 3926) ===
Q mean: -58.960098
Q std: 18.012371
Actor loss: 58.964096
Action reg: 0.003997
  l1.weight: grad_norm = 0.018069
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.035284
Total gradient norm: 0.058751
Total gradient norm: 0.0237640596on 1203) ===
=== Actor Training Debug (Iteration 3937) ===
Q mean: -59.859352
Q std: 18.885351
Actor loss: 59.863342
Action reg: 0.003988
  l1.weight: grad_norm = 0.001461
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.003108
Total gradient norm: 0.005927
=== Actor Training Debug (Iteration 3938) ===
Q mean: -57.206200
Q std: 18.425459
Actor loss: 57.210186
Action reg: 0.003988
  l1.weight: grad_norm = 0.002412
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.004965
Total gradient norm: 0.007567
=== Actor Training Debug (Iteration 3939) ===
Q mean: -61.096191
Q std: 18.921986
Actor loss: 61.100182
Action reg: 0.003991
  l1.weight: grad_norm = 0.114934
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.233481
Total gradient norm: 0.427809
=== Actor Training Debug (Iteration 3940) ===
Q mean: -56.158478
Q std: 19.872803
Actor loss: 56.162449
Action reg: 0.003970
  l1.weight: grad_norm = 0.442538
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.960211
Total gradient norm: 1.873741
=== Actor Training Debug (Iteration 3941) ===
Q mean: -59.628166
Q std: 19.625694
Actor loss: 59.632145
Action reg: 0.003978
  l1.weight: grad_norm = 0.005329
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.010632
Total gradient norm: 0.019296
=== Actor Training Debug (Iteration 3942) ===
Q mean: -56.428444
Q std: 20.370785
Actor loss: 56.432415
Action reg: 0.003973
  l1.weight: grad_norm = 0.021242
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.046847
Total gradient norm: 0.095094
=== Actor Training Debug (Iteration 3943) ===
Q mean: -59.241589
Q std: 18.878910
Actor loss: 59.245579
Action reg: 0.003989
  l1.weight: grad_norm = 0.149907
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.310599
Total gradient norm: 0.616034
=== Actor Training Debug (Iteration 3944) ===
Q mean: -58.473183
Q std: 21.632509
Actor loss: 58.477161
Action reg: 0.003977
  l1.weight: grad_norm = 0.009168
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.020831
Total gradient norm: 0.048282
=== Actor Training Debug (Iteration 3945) ===
Q mean: -57.771538
Q std: 18.547956
Actor loss: 57.775524
Action reg: 0.003985
  l1.weight: grad_norm = 0.010608
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.025432
Total gradient norm: 0.056568
=== Actor Training Debug (Iteration 3946) ===
Q mean: -57.907536
Q std: 19.269287
Actor loss: 57.911530
Action reg: 0.003994
  l1.weight: grad_norm = 0.000181
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.000848
Total gradient norm: 0.002535
=== Actor Training Debug (Iteration 3947) ===
Q mean: -55.637817
Q std: 20.384144
Actor loss: 55.641788
Action reg: 0.003972
  l1.weight: grad_norm = 0.011225
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.026842
Total gradient norm: 0.049473
=== Actor Training Debug (Iteration 3948) ===
Q mean: -61.480900
Q std: 17.640692
Actor loss: 61.484901
Action reg: 0.004000
  l1.weight: grad_norm = 0.002178
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.004215
Total gradient norm: 0.007171
=== Actor Training Debug (Iteration 3949) ===
Q mean: -57.846985
Q std: 18.142834
Actor loss: 57.850983
Action reg: 0.003998
  l1.weight: grad_norm = 0.040678
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.088589
Total gradient norm: 0.156617
=== Actor Training Debug (Iteration 3950) ===
Q mean: -58.474098
Q std: 18.292744
Actor loss: 58.478100
Action reg: 0.004000
  l1.weight: grad_norm = 0.000059
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000123
Total gradient norm: 0.000216
=== Actor Training Debug (Iteration 3951) ===
Q mean: -56.838669
Q std: 20.372982
Actor loss: 56.842651
Action reg: 0.003983
  l1.weight: grad_norm = 0.124743
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.235278
Total gradient norm: 0.425784
=== Actor Training Debug (Iteration 3952) ===
Q mean: -59.265137
Q std: 18.607847
Actor loss: 59.269131
Action reg: 0.003992
  l1.weight: grad_norm = 0.075680
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.165909
Total gradient norm: 0.341393
=== Actor Training Debug (Iteration 3953) ===
Q mean: -57.037323
Q std: 19.186260
Actor loss: 57.041306
Action reg: 0.003984
  l1.weight: grad_norm = 0.000665
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.002492
Total gradient norm: 0.007123
=== Actor Training Debug (Iteration 3954) ===
Q mean: -58.688702
Q std: 19.884272
Actor loss: 58.692684
Action reg: 0.003983
  l1.weight: grad_norm = 0.065744
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.172839
Total gradient norm: 0.362415
=== Actor Training Debug (Iteration 3955) ===
Q mean: -58.467983
Q std: 18.634418
Actor loss: 58.471970
Action reg: 0.003987
  l1.weight: grad_norm = 0.281914
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.590110
Total gradient norm: 1.038681
=== Actor Training Debug (Iteration 3956) ===
Q mean: -61.034943
Q std: 20.544996
Actor loss: 61.038929
Action reg: 0.003986
  l1.weight: grad_norm = 0.067554
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.132732
Total gradient norm: 0.245707
=== Actor Training Debug (Iteration 3957) ===
Q mean: -57.558441
Q std: 19.071922
Actor loss: 57.562431
Action reg: 0.003989
  l1.weight: grad_norm = 0.048363
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.101109
Total gradient norm: 0.166011
=== Actor Training Debug (Iteration 3958) ===
Q mean: -58.621479
Q std: 18.923035
Actor loss: 58.625473
Action reg: 0.003993
  l1.weight: grad_norm = 0.085349
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.194587
Total gradient norm: 0.388512
=== Actor Training Debug (Iteration 3959) ===
Q mean: -57.735409
Q std: 20.340939
Actor loss: 57.739376
Action reg: 0.003966
  l1.weight: grad_norm = 0.109361
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.221967
Total gradient norm: 0.400695
=== Actor Training Debug (Iteration 3960) ===
Q mean: -57.271595
Q std: 19.137947
Actor loss: 57.275585
Action reg: 0.003989
  l1.weight: grad_norm = 0.040475
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.089955
Total gradient norm: 0.153206
=== Actor Training Debug (Iteration 3961) ===
Q mean: -56.298065
Q std: 19.401192
Actor loss: 56.302044
Action reg: 0.003978
  l1.weight: grad_norm = 0.058817
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.108945
Total gradient norm: 0.206360
=== Actor Training Debug (Iteration 3962) ===
Q mean: -57.284016
Q std: 17.924616
Actor loss: 57.288010
Action reg: 0.003993
  l1.weight: grad_norm = 0.019549
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.049600
Total gradient norm: 0.113358
=== Actor Training Debug (Iteration 3963) ===
Q mean: -57.543396
Q std: 19.706457
Actor loss: 57.547379
Action reg: 0.003984
  l1.weight: grad_norm = 0.001207
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.003228
Total gradient norm: 0.009117
=== Actor Training Debug (Iteration 3964) ===
Q mean: -58.413834
Q std: 18.921673
Actor loss: 58.417828
Action reg: 0.003994
  l1.weight: grad_norm = 0.000137
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.000475
Total gradient norm: 0.001507
=== Actor Training Debug (Iteration 3965) ===
Q mean: -57.610344
Q std: 18.990236
Actor loss: 57.614338
Action reg: 0.003993
  l1.weight: grad_norm = 0.060899
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.123383
Total gradient norm: 0.255465
=== Actor Training Debug (Iteration 3966) ===
Q mean: -56.960785
Q std: 19.652132
Actor loss: 56.964764
Action reg: 0.003981
  l1.weight: grad_norm = 0.017798
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.037218
Total gradient norm: 0.063521
=== Actor Training Debug (Iteration 3967) ===
Q mean: -61.560249
Q std: 20.519716
Actor loss: 61.564240
Action reg: 0.003989
  l1.weight: grad_norm = 0.000350
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.001440
Total gradient norm: 0.004692
=== Actor Training Debug (Iteration 3968) ===
Q mean: -61.732376
Q std: 19.127903
Actor loss: 61.736370
Action reg: 0.003994
  l1.weight: grad_norm = 0.000605
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.001722
Total gradient norm: 0.004075
=== Actor Training Debug (Iteration 3969) ===
Q mean: -55.358032
Q std: 19.339083
Actor loss: 55.362007
Action reg: 0.003974
  l1.weight: grad_norm = 0.063050
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.148615
Total gradient norm: 0.266332
=== Actor Training Debug (Iteration 3970) ===
Q mean: -58.130539
Q std: 19.481321
Actor loss: 58.134529
Action reg: 0.003991
  l1.weight: grad_norm = 0.065514
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.145596
Total gradient norm: 0.316380
=== Actor Training Debug (Iteration 3971) ===
Q mean: -59.778687
Q std: 19.441223
Actor loss: 59.782673
Action reg: 0.003987
  l1.weight: grad_norm = 0.028227
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.056084
Total gradient norm: 0.103554
=== Actor Training Debug (Iteration 3972) ===
Q mean: -59.910156
Q std: 20.650856
Actor loss: 59.914127
Actor loss: 56.5168990.0237640596on 1203) ===
Action reg: 0.003973
  l1.weight: grad_norm = 0.009040
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.018629
Total gradient norm: 0.036021
=== Actor Training Debug (Iteration 3983) ===
Q mean: -57.013599
Q std: 18.964897
Actor loss: 57.017570
Action reg: 0.003971
  l1.weight: grad_norm = 0.020583
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.040617
Total gradient norm: 0.071047
=== Actor Training Debug (Iteration 3984) ===
Q mean: -56.764030
Q std: 20.655455
Actor loss: 56.768005
Action reg: 0.003975
  l1.weight: grad_norm = 0.055333
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.117153
Total gradient norm: 0.221222
=== Actor Training Debug (Iteration 3985) ===
Q mean: -59.623455
Q std: 19.907627
Actor loss: 59.627441
Action reg: 0.003988
  l1.weight: grad_norm = 0.007010
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.016824
Total gradient norm: 0.031870
=== Actor Training Debug (Iteration 3986) ===
Q mean: -57.570984
Q std: 20.593510
Actor loss: 57.574955
Action reg: 0.003970
  l1.weight: grad_norm = 0.026465
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.068407
Total gradient norm: 0.151531
=== Actor Training Debug (Iteration 3987) ===
Q mean: -56.465965
Q std: 20.354549
Actor loss: 56.469936
Action reg: 0.003970
  l1.weight: grad_norm = 0.021773
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.058598
Total gradient norm: 0.116331
=== Actor Training Debug (Iteration 3988) ===
Q mean: -60.089024
Q std: 20.120899
Actor loss: 60.093010
Action reg: 0.003987
  l1.weight: grad_norm = 0.106818
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.239907
Total gradient norm: 0.484018
=== Actor Training Debug (Iteration 3989) ===
Q mean: -61.208321
Q std: 19.744076
Actor loss: 61.212315
Action reg: 0.003993
  l1.weight: grad_norm = 0.008321
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.017531
Total gradient norm: 0.027923
=== Actor Training Debug (Iteration 3990) ===
Q mean: -55.908623
Q std: 19.722645
Actor loss: 55.912601
Action reg: 0.003977
  l1.weight: grad_norm = 0.018088
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.038281
Total gradient norm: 0.061384
=== Actor Training Debug (Iteration 3991) ===
Q mean: -60.517220
Q std: 19.027462
Actor loss: 60.521214
Action reg: 0.003993
  l1.weight: grad_norm = 0.019110
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.042076
Total gradient norm: 0.085743
=== Actor Training Debug (Iteration 3992) ===
Q mean: -58.873734
Q std: 20.089808
Actor loss: 58.877716
Action reg: 0.003982
  l1.weight: grad_norm = 0.032311
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.064580
Total gradient norm: 0.104595
=== Actor Training Debug (Iteration 3993) ===
Q mean: -57.957634
Q std: 19.640741
Actor loss: 57.961617
Action reg: 0.003984
  l1.weight: grad_norm = 0.011533
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.026711
Total gradient norm: 0.054373
=== Actor Training Debug (Iteration 3994) ===
Q mean: -57.197556
Q std: 18.888937
Actor loss: 57.201542
Action reg: 0.003987
  l1.weight: grad_norm = 0.116195
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.228772
Total gradient norm: 0.415634
=== Actor Training Debug (Iteration 3995) ===
Q mean: -58.496979
Q std: 19.159878
Actor loss: 58.500957
Action reg: 0.003980
  l1.weight: grad_norm = 0.044408
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.127267
Total gradient norm: 0.289870
=== Actor Training Debug (Iteration 3996) ===
Q mean: -60.265358
Q std: 19.941048
Actor loss: 60.269344
Action reg: 0.003987
  l1.weight: grad_norm = 0.061014
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.116422
Total gradient norm: 0.173896
=== Actor Training Debug (Iteration 3997) ===
Q mean: -58.293457
Q std: 19.765860
Actor loss: 58.297447
Action reg: 0.003991
  l1.weight: grad_norm = 0.028976
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.062112
Total gradient norm: 0.115477
=== Actor Training Debug (Iteration 3998) ===
Q mean: -59.335075
Q std: 19.624235
Actor loss: 59.339062
Action reg: 0.003986
  l1.weight: grad_norm = 0.107587
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.207562
Total gradient norm: 0.342896
=== Actor Training Debug (Iteration 3999) ===
Q mean: -61.082916
Q std: 19.384697
Actor loss: 61.086891
Action reg: 0.003974
  l1.weight: grad_norm = 0.036871
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.087397
Total gradient norm: 0.195358
=== Actor Training Debug (Iteration 4000) ===
Q mean: -59.579185
Q std: 20.581087
Actor loss: 59.583172
Action reg: 0.003985
  l1.weight: grad_norm = 0.003215
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.006402
Total gradient norm: 0.011880
Step 9000: Critic Loss: 6.4495, Actor Loss: 59.5832, Q Value: -59.5792
  Average reward: -361.498 | Average length: 100.0
Evaluation at episode 90: -361.498
Total gradient norm: 0.9913600596on 1203) ===
=== Actor Training Debug (Iteration 4011) ===
Q mean: -60.293106
Q std: 19.271313
Actor loss: 60.297089
Action reg: 0.003984
  l1.weight: grad_norm = 0.007366
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.018062
Total gradient norm: 0.043952
=== Actor Training Debug (Iteration 4012) ===
Q mean: -61.546589
Q std: 19.130730
Actor loss: 61.550571
Action reg: 0.003983
  l1.weight: grad_norm = 0.000698
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.002553
Total gradient norm: 0.007948
=== Actor Training Debug (Iteration 4013) ===
Q mean: -57.461605
Q std: 18.738579
Actor loss: 57.465599
Action reg: 0.003993
  l1.weight: grad_norm = 0.008540
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.019733
Total gradient norm: 0.038250
=== Actor Training Debug (Iteration 4014) ===
Q mean: -55.351780
Q std: 18.867870
Actor loss: 55.355766
Action reg: 0.003986
  l1.weight: grad_norm = 0.142395
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.379818
Total gradient norm: 0.686676
=== Actor Training Debug (Iteration 4015) ===
Q mean: -57.350014
Q std: 20.904573
Actor loss: 57.353981
Action reg: 0.003967
  l1.weight: grad_norm = 0.017149
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.036654
Total gradient norm: 0.069321
=== Actor Training Debug (Iteration 4016) ===
Q mean: -61.430798
Q std: 21.723303
Actor loss: 61.434788
Action reg: 0.003989
  l1.weight: grad_norm = 0.078627
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.168577
Total gradient norm: 0.279044
=== Actor Training Debug (Iteration 4017) ===
Q mean: -58.277428
Q std: 22.309446
Actor loss: 58.281410
Action reg: 0.003981
  l1.weight: grad_norm = 0.041841
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.084127
Total gradient norm: 0.154419
=== Actor Training Debug (Iteration 4018) ===
Q mean: -57.532410
Q std: 21.066488
Actor loss: 57.536404
Action reg: 0.003992
  l1.weight: grad_norm = 0.040164
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.091378
Total gradient norm: 0.164262
=== Actor Training Debug (Iteration 4019) ===
Q mean: -58.121113
Q std: 21.230167
Actor loss: 58.125080
Action reg: 0.003968
  l1.weight: grad_norm = 0.027079
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.057329
Total gradient norm: 0.122872
=== Actor Training Debug (Iteration 4020) ===
Q mean: -59.600040
Q std: 18.576757
Actor loss: 59.604038
Action reg: 0.003998
  l1.weight: grad_norm = 0.025221
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.050373
Total gradient norm: 0.090178
=== Actor Training Debug (Iteration 4021) ===
Q mean: -61.220444
Q std: 18.513819
Actor loss: 61.224442
Action reg: 0.003999
  l1.weight: grad_norm = 0.065161
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.134543
Total gradient norm: 0.243465
=== Actor Training Debug (Iteration 4022) ===
Q mean: -61.076424
Q std: 19.095507
Actor loss: 61.080410
Action reg: 0.003987
  l1.weight: grad_norm = 0.032980
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.072585
Total gradient norm: 0.135632
=== Actor Training Debug (Iteration 4023) ===
Q mean: -59.532013
Q std: 20.820524
Actor loss: 59.535984
Action reg: 0.003971
  l1.weight: grad_norm = 0.028669
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.057910
Total gradient norm: 0.107074
=== Actor Training Debug (Iteration 4024) ===
Q mean: -56.395851
Q std: 19.793188
Actor loss: 56.399841
Action reg: 0.003989
  l1.weight: grad_norm = 0.006896
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.016590
Total gradient norm: 0.030393
=== Actor Training Debug (Iteration 4025) ===
Q mean: -59.243927
Q std: 19.070393
Actor loss: 59.247929
Action reg: 0.004000
  l1.weight: grad_norm = 0.000498
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001028
Total gradient norm: 0.002033
=== Actor Training Debug (Iteration 4026) ===
Q mean: -60.935295
Q std: 20.176399
Actor loss: 60.939278
Action reg: 0.003982
  l1.weight: grad_norm = 0.026553
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.056699
Total gradient norm: 0.131358
=== Actor Training Debug (Iteration 4027) ===
Q mean: -61.128277
Q std: 20.574785
Actor loss: 61.132259
Action reg: 0.003984
  l1.weight: grad_norm = 0.025002
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.051779
Total gradient norm: 0.091850
=== Actor Training Debug (Iteration 4028) ===
Q mean: -60.158184
Q std: 19.248325
Actor loss: 60.162159
Action reg: 0.003975
  l1.weight: grad_norm = 0.114446
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.217060
Total gradient norm: 0.390103
=== Actor Training Debug (Iteration 4029) ===
Q mean: -57.776268
Q std: 19.513189
Actor loss: 57.780251
Action reg: 0.003982
  l1.weight: grad_norm = 0.017913
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.035048
Total gradient norm: 0.064761
=== Actor Training Debug (Iteration 4030) ===
Q mean: -58.314217
Q std: 19.501253
Actor loss: 58.318192
Action reg: 0.003977
  l1.weight: grad_norm = 0.222934
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.426730
Total gradient norm: 0.759671
=== Actor Training Debug (Iteration 4031) ===
Q mean: -60.997604
Q std: 20.392441
Actor loss: 61.001591
Action reg: 0.003986
  l1.weight: grad_norm = 0.009215
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.022484
Total gradient norm: 0.050518
=== Actor Training Debug (Iteration 4032) ===
Q mean: -58.172081
Q std: 18.737772
Actor loss: 58.176071
Action reg: 0.003991
  l1.weight: grad_norm = 0.034666
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.091402
Total gradient norm: 0.201537
=== Actor Training Debug (Iteration 4033) ===
Q mean: -57.179535
Q std: 17.707808
Actor loss: 57.183529
Action reg: 0.003993
  l1.weight: grad_norm = 0.029968
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.071532
Total gradient norm: 0.154227
=== Actor Training Debug (Iteration 4034) ===
Q mean: -57.207001
Q std: 20.122091
Actor loss: 57.210979
Action reg: 0.003978
  l1.weight: grad_norm = 0.000353
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.002030
Total gradient norm: 0.007035
=== Actor Training Debug (Iteration 4035) ===
Q mean: -60.856533
Q std: 19.198559
Actor loss: 60.860527
Action reg: 0.003994
  l1.weight: grad_norm = 0.007463
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.024572
Total gradient norm: 0.063473
=== Actor Training Debug (Iteration 4036) ===
Q mean: -61.049377
Q std: 19.077047
Actor loss: 61.053371
Action reg: 0.003993
  l1.weight: grad_norm = 0.017274
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.040007
Total gradient norm: 0.087878
=== Actor Training Debug (Iteration 4037) ===
Q mean: -58.168762
Q std: 18.694874
Actor loss: 58.172749
Action reg: 0.003985
  l1.weight: grad_norm = 0.010144
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.028264
Total gradient norm: 0.060032
=== Actor Training Debug (Iteration 4038) ===
Q mean: -59.097927
Q std: 19.081755
Actor loss: 59.101917
Action reg: 0.003990
  l1.weight: grad_norm = 0.007278
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.024185
Total gradient norm: 0.054386
=== Actor Training Debug (Iteration 4039) ===
Q mean: -57.004974
Q std: 20.046068
Actor loss: 57.008968
Action reg: 0.003994
  l1.weight: grad_norm = 0.001262
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.003783
Total gradient norm: 0.008342
=== Actor Training Debug (Iteration 4040) ===
Q mean: -59.940620
Q std: 19.134176
Actor loss: 59.944607
Action reg: 0.003987
  l1.weight: grad_norm = 0.032356
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.113566
Total gradient norm: 0.283469
=== Actor Training Debug (Iteration 4041) ===
Q mean: -58.774445
Q std: 19.569635
Actor loss: 58.778431
Action reg: 0.003987
  l1.weight: grad_norm = 0.040203
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.146229
Total gradient norm: 0.306770
Total gradient norm: 0.1022000596on 1203) ===
=== Actor Training Debug (Iteration 4052) ===
Q mean: -61.181854
Q std: 19.162819
Actor loss: 61.185829
Action reg: 0.003975
  l1.weight: grad_norm = 0.022383
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.062449
Total gradient norm: 0.121096
=== Actor Training Debug (Iteration 4053) ===
Q mean: -59.307842
Q std: 19.550554
Actor loss: 59.311836
Action reg: 0.003995
  l1.weight: grad_norm = 0.000122
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.000456
Total gradient norm: 0.001319
=== Actor Training Debug (Iteration 4054) ===
Q mean: -59.073612
Q std: 20.956577
Actor loss: 59.077576
Action reg: 0.003965
  l1.weight: grad_norm = 0.072711
  l1.bias: grad_norm = 0.000541
  l2.weight: grad_norm = 0.174181
Total gradient norm: 0.329915
=== Actor Training Debug (Iteration 4055) ===
Q mean: -60.204498
Q std: 20.336761
Actor loss: 60.208477
Action reg: 0.003977
  l1.weight: grad_norm = 0.013017
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.024656
Total gradient norm: 0.041232
=== Actor Training Debug (Iteration 4056) ===
Q mean: -58.856773
Q std: 18.268620
Actor loss: 58.860764
Action reg: 0.003990
  l1.weight: grad_norm = 0.000563
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.001275
Total gradient norm: 0.003292
=== Actor Training Debug (Iteration 4057) ===
Q mean: -56.562233
Q std: 19.497393
Actor loss: 56.566227
Action reg: 0.003994
  l1.weight: grad_norm = 0.004260
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.009340
Total gradient norm: 0.016890
=== Actor Training Debug (Iteration 4058) ===
Q mean: -63.670322
Q std: 19.130527
Actor loss: 63.674316
Action reg: 0.003993
  l1.weight: grad_norm = 0.020907
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.037990
Total gradient norm: 0.069202
=== Actor Training Debug (Iteration 4059) ===
Q mean: -57.806549
Q std: 18.741871
Actor loss: 57.810551
Action reg: 0.004000
  l1.weight: grad_norm = 0.001173
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002751
Total gradient norm: 0.005320
=== Actor Training Debug (Iteration 4060) ===
Q mean: -56.939835
Q std: 19.967281
Actor loss: 56.943813
Action reg: 0.003978
  l1.weight: grad_norm = 0.003329
  l1.bias: grad_norm = 0.000454
  l2.weight: grad_norm = 0.010104
Total gradient norm: 0.019290
=== Actor Training Debug (Iteration 4061) ===
Q mean: -58.202320
Q std: 20.317013
Actor loss: 58.206303
Action reg: 0.003983
  l1.weight: grad_norm = 0.034848
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.117779
Total gradient norm: 0.282056
=== Actor Training Debug (Iteration 4062) ===
Q mean: -59.659256
Q std: 21.468966
Actor loss: 59.663231
Action reg: 0.003976
  l1.weight: grad_norm = 0.045403
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.092230
Total gradient norm: 0.186447
=== Actor Training Debug (Iteration 4063) ===
Q mean: -62.705078
Q std: 20.214523
Actor loss: 62.709057
Action reg: 0.003978
  l1.weight: grad_norm = 0.020416
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.045634
Total gradient norm: 0.100077
=== Actor Training Debug (Iteration 4064) ===
Q mean: -57.986458
Q std: 18.824299
Actor loss: 57.990452
Action reg: 0.003993
  l1.weight: grad_norm = 0.008355
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.026585
Total gradient norm: 0.067544
=== Actor Training Debug (Iteration 4065) ===
Q mean: -57.146629
Q std: 20.299950
Actor loss: 57.150597
Action reg: 0.003968
  l1.weight: grad_norm = 0.052548
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.096089
Total gradient norm: 0.171906
=== Actor Training Debug (Iteration 4066) ===
Q mean: -58.011536
Q std: 20.366249
Actor loss: 58.015507
Action reg: 0.003971
  l1.weight: grad_norm = 0.079393
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.169803
Total gradient norm: 0.312366
=== Actor Training Debug (Iteration 4067) ===
Q mean: -59.151756
Q std: 19.654200
Actor loss: 59.155739
Action reg: 0.003983
  l1.weight: grad_norm = 0.064800
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.127653
Total gradient norm: 0.236075
=== Actor Training Debug (Iteration 4068) ===
Q mean: -59.093315
Q std: 20.423252
Actor loss: 59.097305
Action reg: 0.003990
  l1.weight: grad_norm = 0.005630
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.013131
Total gradient norm: 0.029957
=== Actor Training Debug (Iteration 4069) ===
Q mean: -58.474392
Q std: 18.844240
Actor loss: 58.478386
Action reg: 0.003994
  l1.weight: grad_norm = 0.024654
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.045448
Total gradient norm: 0.081518
=== Actor Training Debug (Iteration 4070) ===
Q mean: -56.800774
Q std: 19.572701
Actor loss: 56.804764
Action reg: 0.003990
  l1.weight: grad_norm = 0.024607
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.050014
Total gradient norm: 0.087480
=== Actor Training Debug (Iteration 4071) ===
Q mean: -62.555599
Q std: 20.209782
Actor loss: 62.559586
Action reg: 0.003985
  l1.weight: grad_norm = 0.001940
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.003820
Total gradient norm: 0.007019
=== Actor Training Debug (Iteration 4072) ===
Q mean: -59.039459
Q std: 19.256300
Actor loss: 59.043449
Action reg: 0.003989
  l1.weight: grad_norm = 0.000503
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.001495
Total gradient norm: 0.004343
=== Actor Training Debug (Iteration 4073) ===
Q mean: -58.019173
Q std: 20.972267
Actor loss: 58.023155
Action reg: 0.003983
  l1.weight: grad_norm = 0.284821
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.610358
Total gradient norm: 1.159877
=== Actor Training Debug (Iteration 4074) ===
Q mean: -58.927101
Q std: 19.872929
Actor loss: 58.931080
Action reg: 0.003977
  l1.weight: grad_norm = 0.031890
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.079200
Total gradient norm: 0.155274
=== Actor Training Debug (Iteration 4075) ===
Q mean: -60.634422
Q std: 20.250340
Actor loss: 60.638412
Action reg: 0.003992
  l1.weight: grad_norm = 0.016382
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.027614
Total gradient norm: 0.039591
=== Actor Training Debug (Iteration 4076) ===
Q mean: -60.689781
Q std: 19.782703
Actor loss: 60.693764
Action reg: 0.003984
  l1.weight: grad_norm = 0.001168
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.003055
Total gradient norm: 0.008798
=== Actor Training Debug (Iteration 4077) ===
Q mean: -57.573437
Q std: 21.232803
Actor loss: 57.577408
Action reg: 0.003972
  l1.weight: grad_norm = 0.018708
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.032187
Total gradient norm: 0.050305
=== Actor Training Debug (Iteration 4078) ===
Q mean: -57.439682
Q std: 20.653769
Actor loss: 57.443665
Action reg: 0.003983
  l1.weight: grad_norm = 0.074065
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.156746
Total gradient norm: 0.267065
=== Actor Training Debug (Iteration 4079) ===
Q mean: -59.161453
Q std: 19.221277
Actor loss: 59.165436
Action reg: 0.003981
  l1.weight: grad_norm = 0.041826
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.108885
Total gradient norm: 0.246130
=== Actor Training Debug (Iteration 4080) ===
Q mean: -60.972672
Q std: 21.046000
Actor loss: 60.976650
Action reg: 0.003979
  l1.weight: grad_norm = 0.004138
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.009272
Total gradient norm: 0.020482
=== Actor Training Debug (Iteration 4081) ===
Total gradient norm: 0.1022000596on 1203) ===
=== Actor Training Debug (Iteration 4091) ===
Q mean: -58.080261
Q std: 20.052862
Actor loss: 58.084236
Action reg: 0.003975
  l1.weight: grad_norm = 0.186701
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.402465
Total gradient norm: 0.694977
=== Actor Training Debug (Iteration 4092) ===
Q mean: -57.316654
Q std: 19.712193
Actor loss: 57.320629
Action reg: 0.003976
  l1.weight: grad_norm = 0.023933
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.049470
Total gradient norm: 0.092885
=== Actor Training Debug (Iteration 4093) ===
Q mean: -63.060650
Q std: 19.527308
Actor loss: 63.064644
Action reg: 0.003995
  l1.weight: grad_norm = 0.006215
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.013277
Total gradient norm: 0.023405
=== Actor Training Debug (Iteration 4094) ===
Q mean: -59.202087
Q std: 20.882025
Actor loss: 59.206066
Action reg: 0.003977
  l1.weight: grad_norm = 0.109798
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.226247
Total gradient norm: 0.440740
=== Actor Training Debug (Iteration 4095) ===
Q mean: -58.820942
Q std: 21.252003
Actor loss: 58.824932
Action reg: 0.003990
  l1.weight: grad_norm = 0.044965
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.098297
Total gradient norm: 0.182507
=== Actor Training Debug (Iteration 4096) ===
Q mean: -59.598717
Q std: 20.722391
Actor loss: 59.602711
Action reg: 0.003995
  l1.weight: grad_norm = 0.001666
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.004051
Total gradient norm: 0.007726
=== Actor Training Debug (Iteration 4097) ===
Q mean: -60.003887
Q std: 19.834593
Actor loss: 60.007862
Action reg: 0.003977
  l1.weight: grad_norm = 0.063617
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.131584
Total gradient norm: 0.261930
=== Actor Training Debug (Iteration 4098) ===
Q mean: -59.695938
Q std: 19.423010
Actor loss: 59.699921
Action reg: 0.003983
  l1.weight: grad_norm = 0.055778
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.120127
Total gradient norm: 0.214044
=== Actor Training Debug (Iteration 4099) ===
Q mean: -56.499771
Q std: 21.067720
Actor loss: 56.503742
Action reg: 0.003970
  l1.weight: grad_norm = 0.220959
  l1.bias: grad_norm = 0.000538
  l2.weight: grad_norm = 0.453878
Total gradient norm: 0.829321
=== Actor Training Debug (Iteration 4100) ===
Q mean: -60.040825
Q std: 18.943302
Actor loss: 60.044819
Action reg: 0.003994
  l1.weight: grad_norm = 0.049037
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.097039
Total gradient norm: 0.170771
Episode 91: Steps=100, Reward=-312.081, Buffer_size=9100
=== Actor Training Debug (Iteration 4101) ===
Q mean: -57.583771
Q std: 19.392015
Actor loss: 57.587769
Action reg: 0.003996
  l1.weight: grad_norm = 0.027289
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.060077
Total gradient norm: 0.123114
=== Actor Training Debug (Iteration 4102) ===
Q mean: -61.192139
Q std: 20.732174
Actor loss: 61.196121
Action reg: 0.003982
  l1.weight: grad_norm = 0.070091
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.175159
Total gradient norm: 0.404476
=== Actor Training Debug (Iteration 4103) ===
Q mean: -60.032845
Q std: 17.750740
Actor loss: 60.036839
Action reg: 0.003992
  l1.weight: grad_norm = 0.148346
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.304143
Total gradient norm: 0.522333
=== Actor Training Debug (Iteration 4104) ===
Q mean: -58.522224
Q std: 19.816416
Actor loss: 58.526199
Action reg: 0.003977
  l1.weight: grad_norm = 0.073563
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.137862
Total gradient norm: 0.248493
=== Actor Training Debug (Iteration 4105) ===
Q mean: -59.478035
Q std: 19.902956
Actor loss: 59.482010
Action reg: 0.003973
  l1.weight: grad_norm = 0.086095
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.189149
Total gradient norm: 0.337252
=== Actor Training Debug (Iteration 4106) ===
Q mean: -59.517235
Q std: 19.633354
Actor loss: 59.521233
Action reg: 0.003997
  l1.weight: grad_norm = 0.057333
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.121852
Total gradient norm: 0.242587
=== Actor Training Debug (Iteration 4107) ===
Q mean: -59.391865
Q std: 19.301750
Actor loss: 59.395851
Action reg: 0.003985
  l1.weight: grad_norm = 0.023026
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.077725
Total gradient norm: 0.187777
=== Actor Training Debug (Iteration 4108) ===
Q mean: -60.663849
Q std: 20.780535
Actor loss: 60.667835
Action reg: 0.003988
  l1.weight: grad_norm = 0.068288
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.157635
Total gradient norm: 0.355105
=== Actor Training Debug (Iteration 4109) ===
Q mean: -59.299942
Q std: 19.136049
Actor loss: 59.303913
Action reg: 0.003973
  l1.weight: grad_norm = 0.016342
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.036470
Total gradient norm: 0.065551
=== Actor Training Debug (Iteration 4110) ===
Q mean: -58.092499
Q std: 18.689867
Actor loss: 58.096489
Action reg: 0.003989
  l1.weight: grad_norm = 0.027642
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.063563
Total gradient norm: 0.142832
=== Actor Training Debug (Iteration 4111) ===
Q mean: -58.096565
Q std: 20.073725
Actor loss: 58.100536
Action reg: 0.003971
  l1.weight: grad_norm = 0.077153
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.176222
Total gradient norm: 0.423235
=== Actor Training Debug (Iteration 4112) ===
Q mean: -60.039986
Q std: 17.549097
Actor loss: 60.043972
Action reg: 0.003985
  l1.weight: grad_norm = 0.000935
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.002467
Total gradient norm: 0.007149
=== Actor Training Debug (Iteration 4113) ===
Q mean: -60.573483
Q std: 19.104044
Actor loss: 60.577461
Action reg: 0.003977
  l1.weight: grad_norm = 0.008781
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.024977
Total gradient norm: 0.054318
=== Actor Training Debug (Iteration 4114) ===
Q mean: -62.632111
Q std: 20.109154
Actor loss: 62.636097
Action reg: 0.003985
  l1.weight: grad_norm = 0.021649
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.072557
Total gradient norm: 0.172316
=== Actor Training Debug (Iteration 4115) ===
Q mean: -59.559258
Q std: 19.208666
Actor loss: 59.563236
Action reg: 0.003978
  l1.weight: grad_norm = 0.006548
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.014130
Total gradient norm: 0.030188
=== Actor Training Debug (Iteration 4116) ===
Q mean: -57.677261
Q std: 18.474987
Actor loss: 57.681263
Action reg: 0.004000
  l1.weight: grad_norm = 0.000029
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000052
Total gradient norm: 0.000096
=== Actor Training Debug (Iteration 4117) ===
Q mean: -59.738548
Q std: 22.197720
Actor loss: 59.742519
Action reg: 0.003973
  l1.weight: grad_norm = 0.027583
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.058277
Total gradient norm: 0.122940
=== Actor Training Debug (Iteration 4118) ===
Q mean: -59.908958
Q std: 21.233923
Actor loss: 59.912937
Action reg: 0.003980
  l1.weight: grad_norm = 0.009914
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.021817
Total gradient norm: 0.048635
=== Actor Training Debug (Iteration 4119) ===
Q mean: -59.093620
Q std: 19.175131
Actor loss: 59.097607
Action reg: 0.003988
  l1.weight: grad_norm = 0.024747
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.050552
Total gradient norm: 0.102825
=== Actor Training Debug (Iteration 4120) ===
Q mean: -59.097820
Q std: 19.598690
Actor loss: 59.101807
Action reg: 0.003985
  l1.weight: grad_norm = 0.005513
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.011139
Total gradient norm: 0.021745
=== Actor Training Debug (Iteration 4121) ===
Q mean: -57.974224
Q std: 19.353039
Actor loss: 57.978203
Action reg: 0.003980
  l1.weight: grad_norm = 0.000443
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.002635
Total gradient norm: 0.008660
=== Actor Training Debug (Iteration 4122) ===
Q mean: -61.790218
Q std: 19.297169
Actor loss: 61.794209
Action reg: 0.003990
  l1.weight: grad_norm = 0.000711
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.001977
Total gradient norm: 0.004931
=== Actor Training Debug (Iteration 4123) ===
Q mean: -59.815590
Q std: 20.701862
Actor loss: 59.819580
Action reg: 0.003990
  l1.weight: grad_norm = 0.002521
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.004614
Total gradient norm: 0.008089
=== Actor Training Debug (Iteration 4124) ===
Q mean: -58.523712
Q std: 19.697952
Actor loss: 58.527706
Action reg: 0.003993
  l1.weight: grad_norm = 0.047889
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.109281
Total gradient norm: 0.219624
=== Actor Training Debug (Iteration 4125) ===
Q mean: -58.715576
Q std: 20.399828
Actor loss: 58.719570
Action reg: 0.003995
  l1.weight: grad_norm = 0.004157
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.010097
Total gradient norm: 0.023167
=== Actor Training Debug (Iteration 4126) ===
Q mean: -57.858032
Q std: 20.448208
Actor loss: 57.862011
Action reg: 0.003978
  l1.weight: grad_norm = 0.079269
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.160349
Total gradient norm: 0.329998
=== Actor Training Debug (Iteration 4127) ===
Q mean: -59.915771
Q std: 20.004248
Actor loss: 59.919746
Action reg: 0.003974
  l1.weight: grad_norm = 0.039274
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.071157
Total gradient norm: 0.122799
=== Actor Training Debug (Iteration 4128) ===
Q mean: -60.456223
Q std: 21.616390
Actor loss: 60.460201
Action reg: 0.003980
  l1.weight: grad_norm = 0.052092
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.120233
Total gradient norm: 0.218525
=== Actor Training Debug (Iteration 4129) ===
Q mean: -57.824684
Q std: 19.223963
Actor loss: 57.828678
Action reg: 0.003993
  l1.weight: grad_norm = 0.009998
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.023444
Total gradient norm: 0.052208
=== Actor Training Debug (Iteration 4130) ===
Q mean: -59.241531
Q std: 17.926731
Actor loss: 59.245525
Action reg: 0.003994
  l1.weight: grad_norm = 0.029151
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.066584
Total gradient norm: 0.125289
Total gradient norm: 0.1071350596on 1203) ===
=== Actor Training Debug (Iteration 4141) ===
Q mean: -57.255527
Q std: 19.550817
Actor loss: 57.259518
Action reg: 0.003989
  l1.weight: grad_norm = 0.022810
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.053326
Total gradient norm: 0.086047
=== Actor Training Debug (Iteration 4142) ===
Q mean: -61.726662
Q std: 19.710888
Actor loss: 61.730648
Action reg: 0.003988
  l1.weight: grad_norm = 0.009858
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.022219
Total gradient norm: 0.046291
=== Actor Training Debug (Iteration 4143) ===
Q mean: -62.110558
Q std: 19.966837
Actor loss: 62.114544
Action reg: 0.003988
  l1.weight: grad_norm = 0.020854
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.054942
Total gradient norm: 0.105519
=== Actor Training Debug (Iteration 4144) ===
Q mean: -60.514015
Q std: 21.145832
Actor loss: 60.517994
Action reg: 0.003980
  l1.weight: grad_norm = 0.355432
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.788279
Total gradient norm: 1.642406
=== Actor Training Debug (Iteration 4145) ===
Q mean: -57.939713
Q std: 20.140074
Actor loss: 57.943707
Action reg: 0.003995
  l1.weight: grad_norm = 0.000160
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.000748
Total gradient norm: 0.002718
=== Actor Training Debug (Iteration 4146) ===
Q mean: -59.229092
Q std: 20.658064
Actor loss: 59.233082
Action reg: 0.003990
  l1.weight: grad_norm = 0.000455
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.002243
Total gradient norm: 0.006904
=== Actor Training Debug (Iteration 4147) ===
Q mean: -58.948700
Q std: 21.487373
Actor loss: 58.952671
Action reg: 0.003970
  l1.weight: grad_norm = 0.097905
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.174792
Total gradient norm: 0.272920
=== Actor Training Debug (Iteration 4148) ===
Q mean: -62.221138
Q std: 20.605799
Actor loss: 62.225132
Action reg: 0.003994
  l1.weight: grad_norm = 0.003097
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.006532
Total gradient norm: 0.012628
=== Actor Training Debug (Iteration 4149) ===
Q mean: -60.314240
Q std: 20.752522
Actor loss: 60.318218
Action reg: 0.003980
  l1.weight: grad_norm = 0.002053
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.005157
Total gradient norm: 0.012159
=== Actor Training Debug (Iteration 4150) ===
Q mean: -58.435272
Q std: 19.909798
Actor loss: 58.439262
Action reg: 0.003991
  l1.weight: grad_norm = 0.058664
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.116045
Total gradient norm: 0.230247
=== Actor Training Debug (Iteration 4151) ===
Q mean: -58.180443
Q std: 21.893368
Actor loss: 58.184406
Action reg: 0.003962
  l1.weight: grad_norm = 0.036492
  l1.bias: grad_norm = 0.000534
  l2.weight: grad_norm = 0.077902
Total gradient norm: 0.134098
=== Actor Training Debug (Iteration 4152) ===
Q mean: -58.194473
Q std: 20.186270
Actor loss: 58.198460
Action reg: 0.003988
  l1.weight: grad_norm = 0.056121
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.122022
Total gradient norm: 0.306281
=== Actor Training Debug (Iteration 4153) ===
Q mean: -59.786415
Q std: 20.404858
Actor loss: 59.790394
Action reg: 0.003978
  l1.weight: grad_norm = 0.093880
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.199774
Total gradient norm: 0.380265
=== Actor Training Debug (Iteration 4154) ===
Q mean: -59.678612
Q std: 19.835745
Actor loss: 59.682594
Action reg: 0.003983
  l1.weight: grad_norm = 0.054163
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.118433
Total gradient norm: 0.230868
=== Actor Training Debug (Iteration 4155) ===
Q mean: -58.604340
Q std: 19.153435
Actor loss: 58.608330
Action reg: 0.003992
  l1.weight: grad_norm = 0.032373
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.073016
Total gradient norm: 0.147851
=== Actor Training Debug (Iteration 4156) ===
Q mean: -62.697586
Q std: 18.997282
Actor loss: 62.701584
Action reg: 0.004000
  l1.weight: grad_norm = 0.005394
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.013060
Total gradient norm: 0.027713
=== Actor Training Debug (Iteration 4157) ===
Q mean: -60.755268
Q std: 20.540398
Actor loss: 60.759243
Action reg: 0.003976
  l1.weight: grad_norm = 0.032225
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.067409
Total gradient norm: 0.134560
=== Actor Training Debug (Iteration 4158) ===
Q mean: -58.819458
Q std: 20.186024
Actor loss: 58.823448
Action reg: 0.003990
  l1.weight: grad_norm = 0.011161
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.025439
Total gradient norm: 0.050396
=== Actor Training Debug (Iteration 4159) ===
Q mean: -60.128876
Q std: 20.173998
Actor loss: 60.132862
Action reg: 0.003987
  l1.weight: grad_norm = 0.034224
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.080976
Total gradient norm: 0.171158
=== Actor Training Debug (Iteration 4160) ===
Q mean: -59.784050
Q std: 19.439095
Actor loss: 59.788044
Action reg: 0.003996
  l1.weight: grad_norm = 0.047265
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.108029
Total gradient norm: 0.199834
=== Actor Training Debug (Iteration 4161) ===
Q mean: -60.281532
Q std: 20.774137
Actor loss: 60.285511
Action reg: 0.003978
  l1.weight: grad_norm = 0.030716
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.058648
Total gradient norm: 0.090675
=== Actor Training Debug (Iteration 4162) ===
Q mean: -59.848511
Q std: 20.116650
Actor loss: 59.852482
Action reg: 0.003971
  l1.weight: grad_norm = 0.029399
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.066605
Total gradient norm: 0.131382
=== Actor Training Debug (Iteration 4163) ===
Q mean: -59.497902
Q std: 19.223774
Actor loss: 59.501888
Action reg: 0.003986
  l1.weight: grad_norm = 0.193648
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.397124
Total gradient norm: 0.781968
=== Actor Training Debug (Iteration 4164) ===
Q mean: -59.905968
Q std: 20.027729
Actor loss: 59.909950
Action reg: 0.003982
  l1.weight: grad_norm = 0.023991
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.071574
Total gradient norm: 0.161400
=== Actor Training Debug (Iteration 4165) ===
Q mean: -59.002960
Q std: 19.749475
Actor loss: 59.006954
Action reg: 0.003995
  l1.weight: grad_norm = 0.079086
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.164687
Total gradient norm: 0.291886
Total gradient norm: 0.5954250596on 1203) ===
=== Actor Training Debug (Iteration 4176) ===
Q mean: -59.405880
Q std: 21.498251
Actor loss: 59.409859
Action reg: 0.003979
  l1.weight: grad_norm = 0.000586
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.002950
Total gradient norm: 0.009433
=== Actor Training Debug (Iteration 4177) ===
Q mean: -58.479492
Q std: 20.629883
Actor loss: 58.483471
Action reg: 0.003980
  l1.weight: grad_norm = 0.032576
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.068768
Total gradient norm: 0.123086
=== Actor Training Debug (Iteration 4178) ===
Q mean: -55.611855
Q std: 19.597034
Actor loss: 55.615837
Action reg: 0.003982
  l1.weight: grad_norm = 0.025315
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.061338
Total gradient norm: 0.125169
=== Actor Training Debug (Iteration 4179) ===
Q mean: -57.966835
Q std: 19.465158
Actor loss: 57.970818
Action reg: 0.003984
  l1.weight: grad_norm = 0.016848
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.034131
Total gradient norm: 0.068758
=== Actor Training Debug (Iteration 4180) ===
Q mean: -58.552391
Q std: 19.974098
Actor loss: 58.556362
Action reg: 0.003972
  l1.weight: grad_norm = 0.030410
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.073202
Total gradient norm: 0.146996
=== Actor Training Debug (Iteration 4181) ===
Q mean: -62.775246
Q std: 19.587204
Actor loss: 62.779247
Action reg: 0.004000
  l1.weight: grad_norm = 0.000037
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000088
Total gradient norm: 0.000211
=== Actor Training Debug (Iteration 4182) ===
Q mean: -62.138832
Q std: 19.566490
Actor loss: 62.142826
Action reg: 0.003994
  l1.weight: grad_norm = 0.018876
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.041828
Total gradient norm: 0.077030
=== Actor Training Debug (Iteration 4183) ===
Q mean: -58.248848
Q std: 21.683918
Actor loss: 58.252823
Action reg: 0.003973
  l1.weight: grad_norm = 0.030383
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.057820
Total gradient norm: 0.109893
=== Actor Training Debug (Iteration 4184) ===
Q mean: -58.441624
Q std: 19.965649
Actor loss: 58.445621
Action reg: 0.003997
  l1.weight: grad_norm = 0.159307
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.311131
Total gradient norm: 0.626445
=== Actor Training Debug (Iteration 4185) ===
Q mean: -62.051277
Q std: 22.363914
Actor loss: 62.055248
Action reg: 0.003971
  l1.weight: grad_norm = 0.000719
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.004064
Total gradient norm: 0.014197
=== Actor Training Debug (Iteration 4186) ===
Q mean: -61.238846
Q std: 19.957001
Actor loss: 61.242840
Action reg: 0.003993
  l1.weight: grad_norm = 0.136670
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.261174
Total gradient norm: 0.447728
=== Actor Training Debug (Iteration 4187) ===
Q mean: -57.575447
Q std: 19.100012
Actor loss: 57.579418
Action reg: 0.003973
  l1.weight: grad_norm = 0.040421
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.084033
Total gradient norm: 0.181043
=== Actor Training Debug (Iteration 4188) ===
Q mean: -59.369118
Q std: 20.566841
Actor loss: 59.373112
Action reg: 0.003995
  l1.weight: grad_norm = 0.000238
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.000958
Total gradient norm: 0.002654
=== Actor Training Debug (Iteration 4189) ===
Q mean: -60.801094
Q std: 21.074989
Actor loss: 60.805073
Action reg: 0.003978
  l1.weight: grad_norm = 0.024648
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.055495
Total gradient norm: 0.092421
=== Actor Training Debug (Iteration 4190) ===
Q mean: -61.375832
Q std: 20.446674
Actor loss: 61.379814
Action reg: 0.003984
  l1.weight: grad_norm = 0.015308
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.031464
Total gradient norm: 0.065201
=== Actor Training Debug (Iteration 4191) ===
Q mean: -57.860649
Q std: 20.080833
Actor loss: 57.864628
Action reg: 0.003979
  l1.weight: grad_norm = 0.001953
  l1.bias: grad_norm = 0.000642
  l2.weight: grad_norm = 0.004958
Total gradient norm: 0.012329
=== Actor Training Debug (Iteration 4192) ===
Q mean: -58.792702
Q std: 20.126877
Actor loss: 58.796684
Action reg: 0.003983
  l1.weight: grad_norm = 0.023535
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.041958
Total gradient norm: 0.059448
=== Actor Training Debug (Iteration 4193) ===
Q mean: -62.046021
Q std: 18.814756
Actor loss: 62.050011
Action reg: 0.003992
  l1.weight: grad_norm = 0.037541
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.107794
Total gradient norm: 0.227924
=== Actor Training Debug (Iteration 4194) ===
Q mean: -62.660721
Q std: 19.943289
Actor loss: 62.664711
Action reg: 0.003991
  l1.weight: grad_norm = 0.082690
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.196006
Total gradient norm: 0.387722
=== Actor Training Debug (Iteration 4195) ===
Q mean: -60.452965
Q std: 20.448553
Actor loss: 60.456940
Action reg: 0.003976
  l1.weight: grad_norm = 0.080978
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.176014
Total gradient norm: 0.336792
=== Actor Training Debug (Iteration 4196) ===
Q mean: -58.665211
Q std: 18.988977
Actor loss: 58.669201
Action reg: 0.003991
  l1.weight: grad_norm = 0.010322
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.023582
Total gradient norm: 0.044418
=== Actor Training Debug (Iteration 4197) ===
Q mean: -60.706326
Q std: 20.577906
Actor loss: 60.710308
Action reg: 0.003981
  l1.weight: grad_norm = 0.031849
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.059676
Total gradient norm: 0.124499
=== Actor Training Debug (Iteration 4198) ===
Q mean: -61.057571
Q std: 20.368637
Actor loss: 61.061558
Action reg: 0.003985
  l1.weight: grad_norm = 0.000597
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.002833
Total gradient norm: 0.008694
=== Actor Training Debug (Iteration 4199) ===
Q mean: -56.515594
Q std: 19.491150
Actor loss: 56.519588
Action reg: 0.003995
  l1.weight: grad_norm = 0.005637
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.013889
Total gradient norm: 0.030296
=== Actor Training Debug (Iteration 4200) ===
Q mean: -59.162495
Q std: 18.985657
Actor loss: 59.166489
Action reg: 0.003994
  l1.weight: grad_norm = 0.150824
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.248352
Total gradient norm: 0.393158
=== Actor Training Debug (Iteration 4201) ===
Q mean: -62.008507
Q std: 20.945778
Actor loss: 62.012489
Action reg: 0.003984
  l1.weight: grad_norm = 0.011193
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.019068
Total gradient norm: 0.031109
=== Actor Training Debug (Iteration 4202) ===
Q mean: -62.958336
Q std: 19.203653
Actor loss: 62.962330
Action reg: 0.003996
  l1.weight: grad_norm = 0.000223
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.000735
Total gradient norm: 0.002146
=== Actor Training Debug (Iteration 4203) ===
Q mean: -62.325397
Q std: 20.231071
Actor loss: 62.329391
Action reg: 0.003994
  l1.weight: grad_norm = 0.010868
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.028486
Total gradient norm: 0.068479
=== Actor Training Debug (Iteration 4204) ===
Q mean: -60.685509
Q std: 19.570580
Actor loss: 60.689499
Action reg: 0.003991
  l1.weight: grad_norm = 0.182749
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.307679
Total gradient norm: 0.615956
=== Actor Training Debug (Iteration 4205) ===
Q mean: -60.998825
Q std: 19.623075
Actor loss: 61.002811
Action reg: 0.003987
  l1.weight: grad_norm = 0.065932
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.145256
Total gradient norm: 0.268201
=== Actor Training Debug (Iteration 4206) ===
Q mean: -60.311035
Q std: 20.642849
Actor loss: 60.315018
Action reg: 0.003982
  l1.weight: grad_norm = 0.062152
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.124521
Total gradient norm: 0.233291
=== Actor Training Debug (Iteration 4207) ===
Q mean: -61.287197
Q std: 20.526670
Actor loss: 61.291168
Action reg: 0.003972
  l1.weight: grad_norm = 0.057799
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.143778
Total gradient norm: 0.298794
=== Actor Training Debug (Iteration 4208) ===
Q mean: -58.343964
Q std: 20.666710
Actor loss: 58.347946
Action reg: 0.003981
  l1.weight: grad_norm = 0.004792
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.012048
Total gradient norm: 0.031077
=== Actor Training Debug (Iteration 4209) ===
Q mean: -58.109535
Q std: 20.755274
Actor loss: 58.113503
Action reg: 0.003967
  l1.weight: grad_norm = 0.089858
  l1.bias: grad_norm = 0.000596
  l2.weight: grad_norm = 0.182827
Total gradient norm: 0.343565
=== Actor Training Debug (Iteration 4210) ===
Q mean: -60.095284
Q std: 20.459379
Actor loss: 60.099277
Action reg: 0.003995
  l1.weight: grad_norm = 0.004378
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.008888
Total gradient norm: 0.015675
=== Actor Training Debug (Iteration 4211) ===
Q mean: -60.018383
Q std: 19.620510
Actor loss: 60.022369
Action reg: 0.003985
  l1.weight: grad_norm = 0.000283
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.002209
Total gradient norm: 0.008399
=== Actor Training Debug (Iteration 4212) ===
Q mean: -60.261791
Q std: 19.305513
Actor loss: 60.265785
Action reg: 0.003995
  l1.weight: grad_norm = 0.000460
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.001436
Total gradient norm: 0.003844
=== Actor Training Debug (Iteration 4213) ===
Q mean: -60.426430
Q std: 18.329288
Q mean: -62.119331m: 0.5954250596on 1203) ===
Q std: 21.049065
Actor loss: 62.123302
Action reg: 0.003970
  l1.weight: grad_norm = 0.000741
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.004764
Total gradient norm: 0.015789
=== Actor Training Debug (Iteration 4224) ===
Q mean: -63.003414
Q std: 19.027109
Actor loss: 63.007401
Action reg: 0.003986
  l1.weight: grad_norm = 0.006361
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.013488
Total gradient norm: 0.022706
=== Actor Training Debug (Iteration 4225) ===
Q mean: -61.666702
Q std: 18.979136
Actor loss: 61.670689
Action reg: 0.003988
  l1.weight: grad_norm = 0.020012
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.054461
Total gradient norm: 0.122409
=== Actor Training Debug (Iteration 4226) ===
Q mean: -56.819443
Q std: 19.310080
Actor loss: 56.823433
Action reg: 0.003990
  l1.weight: grad_norm = 0.005758
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.015663
Total gradient norm: 0.033938
=== Actor Training Debug (Iteration 4227) ===
Q mean: -58.940315
Q std: 21.170000
Actor loss: 58.944309
Action reg: 0.003995
  l1.weight: grad_norm = 0.000580
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.001394
Total gradient norm: 0.003729
=== Actor Training Debug (Iteration 4228) ===
Q mean: -59.139885
Q std: 22.475548
Actor loss: 59.143860
Action reg: 0.003976
  l1.weight: grad_norm = 0.051334
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.103961
Total gradient norm: 0.195610
=== Actor Training Debug (Iteration 4229) ===
Q mean: -62.015137
Q std: 20.476418
Actor loss: 62.019127
Action reg: 0.003991
  l1.weight: grad_norm = 0.004124
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.009595
Total gradient norm: 0.018383
=== Actor Training Debug (Iteration 4230) ===
Q mean: -60.541519
Q std: 20.267426
Actor loss: 60.545509
Action reg: 0.003991
  l1.weight: grad_norm = 0.000397
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.001666
Total gradient norm: 0.004686
=== Actor Training Debug (Iteration 4231) ===
Q mean: -59.047913
Q std: 19.524788
Actor loss: 59.051895
Action reg: 0.003982
  l1.weight: grad_norm = 0.000718
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.003375
Total gradient norm: 0.011120
=== Actor Training Debug (Iteration 4232) ===
Q mean: -61.931496
Q std: 19.607174
Actor loss: 61.935474
Action reg: 0.003978
  l1.weight: grad_norm = 0.096388
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.176421
Total gradient norm: 0.274895
=== Actor Training Debug (Iteration 4233) ===
Q mean: -61.903622
Q std: 20.025326
Actor loss: 61.907600
Action reg: 0.003978
  l1.weight: grad_norm = 0.079035
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.173646
Total gradient norm: 0.300223
=== Actor Training Debug (Iteration 4234) ===
Q mean: -62.440361
Q std: 19.667826
Actor loss: 62.444351
Action reg: 0.003990
  l1.weight: grad_norm = 0.037041
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.079612
Total gradient norm: 0.194338
=== Actor Training Debug (Iteration 4235) ===
Q mean: -58.712261
Q std: 18.423777
Actor loss: 58.716251
Action reg: 0.003989
  l1.weight: grad_norm = 0.001089
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.002994
Total gradient norm: 0.007117
=== Actor Training Debug (Iteration 4236) ===
Q mean: -59.474815
Q std: 19.373436
Actor loss: 59.478813
Action reg: 0.003999
  l1.weight: grad_norm = 0.006893
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.015544
Total gradient norm: 0.028498
=== Actor Training Debug (Iteration 4237) ===
Q mean: -60.073135
Q std: 22.119278
Actor loss: 60.077103
Action reg: 0.003969
  l1.weight: grad_norm = 0.012661
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.024645
Total gradient norm: 0.054939
=== Actor Training Debug (Iteration 4238) ===
Q mean: -60.914017
Q std: 21.812737
Actor loss: 60.917992
Action reg: 0.003973
  l1.weight: grad_norm = 0.015509
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.028945
Total gradient norm: 0.063277
=== Actor Training Debug (Iteration 4239) ===
Q mean: -60.333290
Q std: 21.183628
Actor loss: 60.337265
Action reg: 0.003976
  l1.weight: grad_norm = 0.037782
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.105548
Total gradient norm: 0.227470
=== Actor Training Debug (Iteration 4240) ===
Q mean: -60.589870
Q std: 18.863270
Actor loss: 60.593849
Action reg: 0.003980
  l1.weight: grad_norm = 0.077303
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.157449
Total gradient norm: 0.294561
=== Actor Training Debug (Iteration 4241) ===
Q mean: -58.902687
Q std: 19.455006
Actor loss: 58.906681
Action reg: 0.003992
  l1.weight: grad_norm = 0.017698
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.039523
Total gradient norm: 0.071311
=== Actor Training Debug (Iteration 4242) ===
Q mean: -59.338955
Q std: 21.427441
Actor loss: 59.342926
Action reg: 0.003972
  l1.weight: grad_norm = 0.045197
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.101113
Total gradient norm: 0.222361
=== Actor Training Debug (Iteration 4243) ===
Q mean: -60.915794
Q std: 19.422985
Actor loss: 60.919792
Action reg: 0.003998
  l1.weight: grad_norm = 0.106737
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.275639
Total gradient norm: 0.585280
=== Actor Training Debug (Iteration 4244) ===
Q mean: -59.577896
Q std: 19.935724
Actor loss: 59.581882
Action reg: 0.003987
  l1.weight: grad_norm = 0.058350
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.184424
Total gradient norm: 0.378963
=== Actor Training Debug (Iteration 4245) ===
Q mean: -60.829853
Q std: 19.492088
Actor loss: 60.833843
Action reg: 0.003989
  l1.weight: grad_norm = 0.006503
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.021457
Total gradient norm: 0.047838
=== Actor Training Debug (Iteration 4246) ===
Q mean: -62.465652
Q std: 19.962875
Actor loss: 62.469639
Action reg: 0.003986
  l1.weight: grad_norm = 0.117283
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.235841
Total gradient norm: 0.457975
=== Actor Training Debug (Iteration 4247) ===
Q mean: -61.872375
Q std: 20.979982
Actor loss: 61.876366
Action reg: 0.003990
  l1.weight: grad_norm = 0.009415
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.026114
Total gradient norm: 0.056870
=== Actor Training Debug (Iteration 4248) ===
Q mean: -60.108494
Q std: 20.027937
Actor loss: 60.112480
Action reg: 0.003987
  l1.weight: grad_norm = 0.022996
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.050105
Total gradient norm: 0.105191
=== Actor Training Debug (Iteration 4249) ===
Q mean: -61.237148
Q std: 19.424654
Actor loss: 61.241142
Action reg: 0.003994
  l1.weight: grad_norm = 0.027280
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.063812
Total gradient norm: 0.116884
=== Actor Training Debug (Iteration 4250) ===
Q mean: -60.644943
Q std: 19.292711
Actor loss: 60.648926
Action reg: 0.003981
  l1.weight: grad_norm = 0.008408
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.020282
Total gradient norm: 0.048887
=== Actor Training Debug (Iteration 4251) ===
Q mean: -61.509682
Q std: 19.570280
Actor loss: 61.513668
Action reg: 0.003987
  l1.weight: grad_norm = 0.137426
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.288652
Total gradient norm: 0.521741
=== Actor Training Debug (Iteration 4252) ===
Q mean: -60.561996
Q std: 19.408621
Actor loss: 60.565987
Action reg: 0.003989
  l1.weight: grad_norm = 0.065478
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.150815
Total gradient norm: 0.335384
=== Actor Training Debug (Iteration 4253) ===
Q mean: -60.877769
Q std: 19.292048
Actor loss: 60.881763
Action reg: 0.003994
  l1.weight: grad_norm = 0.003975
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.008649
Total gradient norm: 0.016850
Total gradient norm: 0.0188360596on 1203) ===
=== Actor Training Debug (Iteration 4264) ===
Q mean: -61.955025
Q std: 19.350248
Actor loss: 61.959011
Action reg: 0.003986
  l1.weight: grad_norm = 0.002859
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.006645
Total gradient norm: 0.012875
=== Actor Training Debug (Iteration 4265) ===
Q mean: -58.797127
Q std: 20.211456
Actor loss: 58.801105
Action reg: 0.003977
  l1.weight: grad_norm = 0.072474
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.153143
Total gradient norm: 0.308741
=== Actor Training Debug (Iteration 4266) ===
Q mean: -61.177902
Q std: 19.401445
Actor loss: 61.181896
Action reg: 0.003993
  l1.weight: grad_norm = 0.037984
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.083635
Total gradient norm: 0.146414
=== Actor Training Debug (Iteration 4267) ===
Q mean: -62.717514
Q std: 20.479485
Actor loss: 62.721500
Action reg: 0.003987
  l1.weight: grad_norm = 0.005820
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.012158
Total gradient norm: 0.023014
=== Actor Training Debug (Iteration 4268) ===
Q mean: -62.370232
Q std: 21.581116
Actor loss: 62.374222
Action reg: 0.003990
  l1.weight: grad_norm = 0.050027
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.114300
Total gradient norm: 0.189617
=== Actor Training Debug (Iteration 4269) ===
Q mean: -59.768341
Q std: 21.361389
Actor loss: 59.772331
Action reg: 0.003989
  l1.weight: grad_norm = 0.135128
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.300591
Total gradient norm: 0.596179
=== Actor Training Debug (Iteration 4270) ===
Q mean: -63.516266
Q std: 20.856426
Actor loss: 63.520256
Action reg: 0.003990
  l1.weight: grad_norm = 0.011860
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.024539
Total gradient norm: 0.044906
=== Actor Training Debug (Iteration 4271) ===
Q mean: -61.743225
Q std: 19.367657
Actor loss: 61.747215
Action reg: 0.003989
  l1.weight: grad_norm = 0.042107
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.106679
Total gradient norm: 0.225570
=== Actor Training Debug (Iteration 4272) ===
Q mean: -60.801350
Q std: 18.641497
Actor loss: 60.805344
Action reg: 0.003993
  l1.weight: grad_norm = 0.080366
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.158577
Total gradient norm: 0.301294
=== Actor Training Debug (Iteration 4273) ===
Q mean: -62.038979
Q std: 21.198074
Actor loss: 62.042957
Action reg: 0.003977
  l1.weight: grad_norm = 0.000696
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.004316
Total gradient norm: 0.015138
=== Actor Training Debug (Iteration 4274) ===
Q mean: -60.974461
Q std: 22.319134
Actor loss: 60.978428
Action reg: 0.003966
  l1.weight: grad_norm = 0.157600
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.354286
Total gradient norm: 0.602487
=== Actor Training Debug (Iteration 4275) ===
Q mean: -62.045750
Q std: 20.766857
Actor loss: 62.049732
Action reg: 0.003984
  l1.weight: grad_norm = 0.010382
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.025569
Total gradient norm: 0.056468
=== Actor Training Debug (Iteration 4276) ===
Q mean: -61.248383
Q std: 20.229561
Actor loss: 61.252361
Action reg: 0.003979
  l1.weight: grad_norm = 0.007836
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.020785
Total gradient norm: 0.052493
=== Actor Training Debug (Iteration 4277) ===
Q mean: -61.009773
Q std: 19.335751
Actor loss: 61.013752
Action reg: 0.003980
  l1.weight: grad_norm = 0.162175
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.351609
Total gradient norm: 0.647037
=== Actor Training Debug (Iteration 4278) ===
Q mean: -62.214664
Q std: 18.700268
Actor loss: 62.218655
Action reg: 0.003990
  l1.weight: grad_norm = 0.000470
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.002155
Total gradient norm: 0.006630
=== Actor Training Debug (Iteration 4279) ===
Q mean: -64.884689
Q std: 20.295280
Actor loss: 64.888664
Action reg: 0.003977
  l1.weight: grad_norm = 0.008151
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.016164
Total gradient norm: 0.030927
=== Actor Training Debug (Iteration 4280) ===
Q mean: -66.126862
Q std: 19.406982
Actor loss: 66.130836
Action reg: 0.003976
  l1.weight: grad_norm = 0.026923
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.064763
Total gradient norm: 0.129302
=== Actor Training Debug (Iteration 4281) ===
Q mean: -58.789001
Q std: 18.854170
Actor loss: 58.792999
Action reg: 0.003999
  l1.weight: grad_norm = 0.005238
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.012115
Total gradient norm: 0.027171
Total gradient norm: 0.0106690596on 1203) ===
=== Actor Training Debug (Iteration 4292) ===
Q mean: -62.632458
Q std: 21.059040
Actor loss: 62.636436
Action reg: 0.003978
  l1.weight: grad_norm = 0.000544
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.003676
Total gradient norm: 0.012227
=== Actor Training Debug (Iteration 4293) ===
Q mean: -60.610107
Q std: 20.702433
Actor loss: 60.614094
Action reg: 0.003988
  l1.weight: grad_norm = 0.189061
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.384311
Total gradient norm: 0.766274
=== Actor Training Debug (Iteration 4294) ===
Q mean: -58.533318
Q std: 18.797941
Actor loss: 58.537300
Action reg: 0.003984
  l1.weight: grad_norm = 0.104634
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.218865
Total gradient norm: 0.364664
=== Actor Training Debug (Iteration 4295) ===
Q mean: -62.534973
Q std: 21.041752
Actor loss: 62.538960
Action reg: 0.003985
  l1.weight: grad_norm = 0.028381
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.078429
Total gradient norm: 0.176558
=== Actor Training Debug (Iteration 4296) ===
Q mean: -61.525166
Q std: 19.425724
Actor loss: 61.529163
Action reg: 0.003998
  l1.weight: grad_norm = 0.138536
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.286956
Total gradient norm: 0.492667
=== Actor Training Debug (Iteration 4297) ===
Q mean: -60.242477
Q std: 19.600639
Actor loss: 60.246468
Action reg: 0.003990
  l1.weight: grad_norm = 0.006893
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.014735
Total gradient norm: 0.028473
=== Actor Training Debug (Iteration 4298) ===
Q mean: -62.828857
Q std: 20.030981
Actor loss: 62.832851
Action reg: 0.003995
  l1.weight: grad_norm = 0.001700
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.003083
Total gradient norm: 0.005556
=== Actor Training Debug (Iteration 4299) ===
Q mean: -61.277733
Q std: 20.862427
Actor loss: 61.281727
Action reg: 0.003993
  l1.weight: grad_norm = 0.038136
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.083508
Total gradient norm: 0.176552
=== Actor Training Debug (Iteration 4300) ===
Q mean: -59.363091
Q std: 20.305056
Actor loss: 59.367077
Action reg: 0.003985
  l1.weight: grad_norm = 0.004972
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.014089
Total gradient norm: 0.032109
=== Actor Training Debug (Iteration 4301) ===
Q mean: -59.751884
Q std: 21.099989
Actor loss: 59.755875
Action reg: 0.003991
  l1.weight: grad_norm = 0.000743
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.002494
Total gradient norm: 0.006868
=== Actor Training Debug (Iteration 4302) ===
Q mean: -58.334080
Q std: 21.221682
Actor loss: 58.338055
Action reg: 0.003975
  l1.weight: grad_norm = 0.054413
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.105656
Total gradient norm: 0.225040
=== Actor Training Debug (Iteration 4303) ===
Q mean: -62.936687
Q std: 19.145693
Actor loss: 62.940681
Action reg: 0.003995
  l1.weight: grad_norm = 0.022092
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.044067
Total gradient norm: 0.089567
=== Actor Training Debug (Iteration 4304) ===
Q mean: -62.390999
Q std: 20.622658
Actor loss: 62.394989
Action reg: 0.003989
  l1.weight: grad_norm = 0.021713
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.039568
Total gradient norm: 0.064186
=== Actor Training Debug (Iteration 4305) ===
Q mean: -58.935516
Q std: 19.640192
Actor loss: 58.939495
Action reg: 0.003980
  l1.weight: grad_norm = 0.101918
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.219520
Total gradient norm: 0.363035
=== Actor Training Debug (Iteration 4306) ===
Q mean: -61.944977
Q std: 19.453529
Actor loss: 61.948967
Action reg: 0.003991
  l1.weight: grad_norm = 0.006549
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.013586
Total gradient norm: 0.026594
=== Actor Training Debug (Iteration 4307) ===
Q mean: -62.495163
Q std: 20.717215
Actor loss: 62.499157
Action reg: 0.003992
  l1.weight: grad_norm = 0.038731
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.089293
Total gradient norm: 0.192244
=== Actor Training Debug (Iteration 4308) ===
Q mean: -60.409302
Q std: 21.230658
Actor loss: 60.413284
Action reg: 0.003981
  l1.weight: grad_norm = 0.056121
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.157935
Total gradient norm: 0.328531
=== Actor Training Debug (Iteration 4309) ===
Q mean: -58.932190
Q std: 20.727243
Actor loss: 58.936165
Action reg: 0.003976
  l1.weight: grad_norm = 0.032347
  l1.bias: grad_norm = 0.000478
  l2.weight: grad_norm = 0.082139
Total gradient norm: 0.197824
=== Actor Training Debug (Iteration 4310) ===
Q mean: -61.240746
Q std: 20.154749
Actor loss: 61.244740
Action reg: 0.003993
  l1.weight: grad_norm = 0.242776
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.791822
Total gradient norm: 1.779798
=== Actor Training Debug (Iteration 4311) ===
Q mean: -60.138630
Q std: 21.643984
Actor loss: 60.142601
Action reg: 0.003972
  l1.weight: grad_norm = 0.019355
  l1.bias: grad_norm = 0.000586
  l2.weight: grad_norm = 0.039270
Total gradient norm: 0.083250
=== Actor Training Debug (Iteration 4312) ===
Q mean: -59.735638
Q std: 21.102453
Actor loss: 59.739624
Action reg: 0.003987
  l1.weight: grad_norm = 0.001780
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.004944
Total gradient norm: 0.012606
=== Actor Training Debug (Iteration 4313) ===
Q mean: -58.990799
Q std: 20.816242
Actor loss: 58.994785
Action reg: 0.003987
  l1.weight: grad_norm = 0.003307
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.007492
Total gradient norm: 0.014650
=== Actor Training Debug (Iteration 4314) ===
Q mean: -62.948765
Q std: 20.619804
Actor loss: 62.952740
Action reg: 0.003975
  l1.weight: grad_norm = 0.135676
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.265678
Total gradient norm: 0.401148
=== Actor Training Debug (Iteration 4315) ===
Q mean: -63.414150
Q std: 19.032398
Actor loss: 63.418144
Action reg: 0.003995
  l1.weight: grad_norm = 0.002430
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.004683
Total gradient norm: 0.007965
=== Actor Training Debug (Iteration 4316) ===
Q mean: -61.735226
Q std: 20.359753
Actor loss: 61.739201
Action reg: 0.003976
  l1.weight: grad_norm = 0.036465
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.080670
Total gradient norm: 0.185643
=== Actor Training Debug (Iteration 4317) ===
Q mean: -58.598927
Q std: 19.578663
Actor loss: 58.602921
Action reg: 0.003994
  l1.weight: grad_norm = 0.013800
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.034112
Total gradient norm: 0.060708
=== Actor Training Debug (Iteration 4318) ===
Q mean: -61.419273
Q std: 19.840532
Actor loss: 61.423264
Action reg: 0.003991
  l1.weight: grad_norm = 0.000656
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.002446
Total gradient norm: 0.007602
=== Actor Training Debug (Iteration 4319) ===
Q mean: -61.004478
Q mean: -62.797348m: 0.0106690596on 1203) ===
Q std: 20.826252
Actor loss: 62.801338
Action reg: 0.003991
  l1.weight: grad_norm = 0.004914
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.010350
Total gradient norm: 0.018949
=== Actor Training Debug (Iteration 4330) ===
Q mean: -62.992443
Q std: 20.559256
Actor loss: 62.996437
Action reg: 0.003995
  l1.weight: grad_norm = 0.000344
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.001253
Total gradient norm: 0.003186
=== Actor Training Debug (Iteration 4331) ===
Q mean: -61.059532
Q std: 19.961050
Actor loss: 61.063530
Action reg: 0.003997
  l1.weight: grad_norm = 0.254174
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.537400
Total gradient norm: 1.091082
=== Actor Training Debug (Iteration 4332) ===
Q mean: -58.848850
Q std: 21.247137
Actor loss: 58.852833
Action reg: 0.003984
  l1.weight: grad_norm = 0.004301
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.010602
Total gradient norm: 0.024251
=== Actor Training Debug (Iteration 4333) ===
Q mean: -62.235134
Q std: 21.962561
Actor loss: 62.239113
Action reg: 0.003979
  l1.weight: grad_norm = 0.099551
  l1.bias: grad_norm = 0.000510
  l2.weight: grad_norm = 0.212639
Total gradient norm: 0.441280
=== Actor Training Debug (Iteration 4334) ===
Q mean: -62.357628
Q std: 21.459620
Actor loss: 62.361603
Action reg: 0.003974
  l1.weight: grad_norm = 0.020421
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.042751
Total gradient norm: 0.063343
=== Actor Training Debug (Iteration 4335) ===
Q mean: -61.256256
Q std: 22.060909
Actor loss: 61.260227
Action reg: 0.003972
  l1.weight: grad_norm = 0.018105
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.036532
Total gradient norm: 0.060163
=== Actor Training Debug (Iteration 4336) ===
Q mean: -62.198223
Q std: 20.456240
Actor loss: 62.202213
Action reg: 0.003991
  l1.weight: grad_norm = 0.079421
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.199423
Total gradient norm: 0.418195
=== Actor Training Debug (Iteration 4337) ===
Q mean: -63.330273
Q std: 20.917868
Actor loss: 63.334263
Action reg: 0.003991
  l1.weight: grad_norm = 0.048363
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.077207
Total gradient norm: 0.128131
=== Actor Training Debug (Iteration 4338) ===
Q mean: -59.850018
Q std: 21.052818
Actor loss: 59.853992
Action reg: 0.003976
  l1.weight: grad_norm = 0.000909
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.004396
Total gradient norm: 0.015248
=== Actor Training Debug (Iteration 4339) ===
Q mean: -61.025940
Q std: 20.772543
Actor loss: 61.029930
Action reg: 0.003990
  l1.weight: grad_norm = 0.000310
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.002023
Total gradient norm: 0.006755
=== Actor Training Debug (Iteration 4340) ===
Q mean: -62.023960
Q std: 19.519142
Actor loss: 62.027958
Action reg: 0.003999
  l1.weight: grad_norm = 0.037649
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.072211
Total gradient norm: 0.122982
=== Actor Training Debug (Iteration 4341) ===
Q mean: -62.110378
Q std: 19.655003
Actor loss: 62.114368
Action reg: 0.003991
  l1.weight: grad_norm = 0.061295
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.128659
Total gradient norm: 0.253932
=== Actor Training Debug (Iteration 4342) ===
Q mean: -62.352543
Q std: 20.837692
Actor loss: 62.356533
Action reg: 0.003990
  l1.weight: grad_norm = 0.080731
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.168353
Total gradient norm: 0.303392
=== Actor Training Debug (Iteration 4343) ===
Q mean: -60.146881
Q std: 21.324705
Actor loss: 60.150856
Action reg: 0.003975
  l1.weight: grad_norm = 0.047875
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.105557
Total gradient norm: 0.192678
=== Actor Training Debug (Iteration 4344) ===
Q mean: -62.196716
Q std: 20.169054
Actor loss: 62.200710
Action reg: 0.003992
  l1.weight: grad_norm = 0.051763
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.142729
Total gradient norm: 0.294767
=== Actor Training Debug (Iteration 4345) ===
Q mean: -62.523903
Q std: 22.764519
Actor loss: 62.527885
Action reg: 0.003982
  l1.weight: grad_norm = 0.050945
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.121303
Total gradient norm: 0.301595
=== Actor Training Debug (Iteration 4346) ===
Q mean: -61.039879
Q std: 21.333271
Actor loss: 61.043846
Action reg: 0.003967
  l1.weight: grad_norm = 0.000692
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.006252
Total gradient norm: 0.022947
=== Actor Training Debug (Iteration 4347) ===
Q mean: -60.640862
Q std: 20.157219
Actor loss: 60.644844
Action reg: 0.003983
  l1.weight: grad_norm = 0.071395
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.152227
Total gradient norm: 0.267668
=== Actor Training Debug (Iteration 4348) ===
Q mean: -62.377827
Q std: 21.012037
Actor loss: 62.381817
Action reg: 0.003989
  l1.weight: grad_norm = 0.082660
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.170108
Total gradient norm: 0.300386
=== Actor Training Debug (Iteration 4349) ===
Q mean: -62.356674
Q std: 20.119339
Actor loss: 62.360664
Action reg: 0.003991
  l1.weight: grad_norm = 0.000496
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.002553
Total gradient norm: 0.008321
=== Actor Training Debug (Iteration 4350) ===
Q mean: -62.594898
Q std: 21.170446
Actor loss: 62.598881
Action reg: 0.003984
  l1.weight: grad_norm = 0.071590
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.160939
Total gradient norm: 0.393591
=== Actor Training Debug (Iteration 4351) ===
Q mean: -62.180927
Q std: 20.235464
Actor loss: 62.184914
Action reg: 0.003986
  l1.weight: grad_norm = 0.009373
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.016514
Total gradient norm: 0.027902
=== Actor Training Debug (Iteration 4352) ===
Q mean: -60.921997
Q std: 20.260464
Actor loss: 60.925987
Action reg: 0.003989
  l1.weight: grad_norm = 0.006004
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.017827
Total gradient norm: 0.044657
=== Actor Training Debug (Iteration 4353) ===
Q mean: -63.498058
Q std: 20.563282
Actor loss: 63.502048
Action reg: 0.003991
  l1.weight: grad_norm = 0.010908
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.022368
Total gradient norm: 0.042311
=== Actor Training Debug (Iteration 4354) ===
Q mean: -61.365120
Q std: 20.045010
Actor loss: 61.369114
Action reg: 0.003994
  l1.weight: grad_norm = 0.038554
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.093376
Total gradient norm: 0.197946
=== Actor Training Debug (Iteration 4355) ===
Q mean: -60.098892
Q std: 20.473719
Actor loss: 60.102879
Action reg: 0.003986
  l1.weight: grad_norm = 0.070967
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.181990
Total gradient norm: 0.371832
=== Actor Training Debug (Iteration 4356) ===
Q mean: -61.993759
Q std: 19.369186
Actor loss: 61.997746
Action reg: 0.003988
  l1.weight: grad_norm = 0.032339
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.072066
Total gradient norm: 0.137291
=== Actor Training Debug (Iteration 4357) ===
Q mean: -62.004955
Q std: 20.241175
Actor loss: 62.008938
Action reg: 0.003984
  l1.weight: grad_norm = 0.086818
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.158510
Total gradient norm: 0.220173
=== Actor Training Debug (Iteration 4358) ===
Q mean: -61.114502
Q std: 20.099297
Actor loss: 61.118492
Action reg: 0.003990
  l1.weight: grad_norm = 0.023270
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.050030
Total gradient norm: 0.105098
=== Actor Training Debug (Iteration 4359) ===
Q mean: -60.127640
Q std: 20.059748
Actor loss: 60.131626
Action reg: 0.003986
  l1.weight: grad_norm = 0.031907
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.065922
Total gradient norm: 0.138547
=== Actor Training Debug (Iteration 4360) ===
Q mean: -59.293709
Q std: 21.289732
Actor loss: 59.297676
Action reg: 0.003968
  l1.weight: grad_norm = 0.104828
  l1.bias: grad_norm = 0.000634
  l2.weight: grad_norm = 0.209616
Total gradient norm: 0.365099
=== Actor Training Debug (Iteration 4361) ===
Q mean: -62.416470
Q std: 20.131420
Actor loss: 62.420460
Action reg: 0.003992
  l1.weight: grad_norm = 0.001121
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.003519
Total gradient norm: 0.010942
=== Actor Training Debug (Iteration 4362) ===
Q mean: -60.420380
Q std: 19.832439
Actor loss: 60.424370
Action reg: 0.003990
  l1.weight: grad_norm = 0.053812
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.104459
Total gradient norm: 0.184450
=== Actor Training Debug (Iteration 4363) ===
Q mean: -58.903542
Q std: 19.514839
Actor loss: 58.907532
Action reg: 0.003991
  l1.weight: grad_norm = 0.044170
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.088968
Total gradient norm: 0.192803
=== Actor Training Debug (Iteration 4364) ===
Q mean: -62.330925
Q std: 20.564373
Actor loss: 62.334911
Action reg: 0.003987
  l1.weight: grad_norm = 0.000595
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.003072
Total gradient norm: 0.009866
=== Actor Training Debug (Iteration 4365) ===
Q mean: -60.240761
Q std: 19.838591
Actor loss: 60.244747
Action reg: 0.003987
  l1.weight: grad_norm = 0.031428
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.080005
Total gradient norm: 0.155448
=== Actor Training Debug (Iteration 4366) ===
Q mean: -58.888443
Q std: 20.560703
Actor loss: 58.892422
Action reg: 0.003980
  l1.weight: grad_norm = 0.040854
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.100419
Total gradient norm: 0.221296
=== Actor Training Debug (Iteration 4367) ===
Q mean: -59.282665
Q std: 22.132566
Actor loss: 59.286655
Action reg: 0.003990
  l1.weight: grad_norm = 0.029879
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.058832
Total gradient norm: 0.095260
=== Actor Training Debug (Iteration 4368) ===
Q mean: -63.001148
Q std: 19.990107
Actor loss: 63.005135
Action reg: 0.003988
  l1.weight: grad_norm = 0.153012
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.312125
Total gradient norm: 0.608045
=== Actor Training Debug (Iteration 4369) ===
Q mean: -63.254669
Q std: 20.370268
Actor loss: 63.258656
Action reg: 0.003987
  l1.weight: grad_norm = 0.001221
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.003989
Total gradient norm: 0.010743
=== Actor Training Debug (Iteration 4370) ===
Q mean: -59.415874
Q std: 20.834616
Actor loss: 59.419861
Action reg: 0.003987
  l1.weight: grad_norm = 0.002973
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.005919
Total gradient norm: 0.011847
=== Actor Training Debug (Iteration 4371) ===
Q mean: -60.386330
Q std: 18.874681
Actor loss: 60.390316
Action reg: 0.003985
  l1.weight: grad_norm = 0.061117
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.117383
Total gradient norm: 0.229070
=== Actor Training Debug (Iteration 4372) ===
Q mean: -61.343861
Q std: 19.801304
Actor loss: 61.347843
Action reg: 0.003983
  l1.weight: grad_norm = 0.001290
  l1.bias: grad_norm = 0.000701
  l2.weight: grad_norm = 0.005748
Total gradient norm: 0.017618
=== Actor Training Debug (Iteration 4373) ===
Q mean: -61.952599
Q std: 20.740250
Actor loss: 61.956585
Action reg: 0.003988
  l1.weight: grad_norm = 0.138577
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.275593
Total gradient norm: 0.540198
=== Actor Training Debug (Iteration 4374) ===
Q mean: -62.462360
Q std: 20.244349
Actor loss: 62.466351
Action reg: 0.003992
  l1.weight: grad_norm = 0.000843
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.002788
Total gradient norm: 0.008394
=== Actor Training Debug (Iteration 4375) ===
Q mean: -58.702255
Q std: 20.907400
Actor loss: 58.706238
Action reg: 0.003982
  l1.weight: grad_norm = 0.052902
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.104338
Total gradient norm: 0.162791
=== Actor Training Debug (Iteration 4376) ===
Q mean: -60.224968
Q std: 21.364939
Actor loss: 60.228951
Action reg: 0.003982
  l1.weight: grad_norm = 0.000698
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.003846
Total gradient norm: 0.012406
=== Actor Training Debug (Iteration 4377) ===
Q mean: -61.410225
Q std: 20.723843
Actor loss: 61.414207
Action reg: 0.003983
  l1.weight: grad_norm = 0.000752
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.004569
Total gradient norm: 0.013970
=== Actor Training Debug (Iteration 4378) ===
Q mean: -61.352848
Q std: 20.671572
Actor loss: 61.356831
Action reg: 0.003983
  l1.weight: grad_norm = 0.041546
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.102603
Total gradient norm: 0.218229
=== Actor Training Debug (Iteration 4379) ===
Q mean: -63.101788
Q std: 21.828913
Actor loss: 63.105774
Action reg: 0.003985
  l1.weight: grad_norm = 0.120014
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.211546
Total gradient norm: 0.328971
=== Actor Training Debug (Iteration 4380) ===
Q mean: -62.646358
Q std: 20.914055
Actor loss: 62.650349
Action reg: 0.003991
  l1.weight: grad_norm = 0.060426
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.111441
Total gradient norm: 0.192756
=== Actor Training Debug (Iteration 4381) ===
Q mean: -61.579086
Q std: 21.358524
Actor loss: 61.583065
Action reg: 0.003977
  l1.weight: grad_norm = 0.032372
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.082728
Total gradient norm: 0.194789
=== Actor Training Debug (Iteration 4382) ===
Q mean: -62.797348m: 0.0106690596on 1203) ===
=== Actor Training Debug (Iteration 4392) ===
Q mean: -60.249546
Q std: 19.973272
Actor loss: 60.253536
Action reg: 0.003991
  l1.weight: grad_norm = 0.001646
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.004578
Total gradient norm: 0.009904
=== Actor Training Debug (Iteration 4393) ===
Q mean: -60.803413
Q std: 20.514292
Actor loss: 60.807404
Action reg: 0.003990
  l1.weight: grad_norm = 0.014336
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.036245
Total gradient norm: 0.085437
=== Actor Training Debug (Iteration 4394) ===
Q mean: -63.429878
Q std: 19.322851
Actor loss: 63.433876
Action reg: 0.003996
  l1.weight: grad_norm = 0.006927
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.017572
Total gradient norm: 0.039879
=== Actor Training Debug (Iteration 4395) ===
Q mean: -61.969700
Q std: 20.673347
Actor loss: 61.973686
Action reg: 0.003987
  l1.weight: grad_norm = 0.001385
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.004176
Total gradient norm: 0.010566
=== Actor Training Debug (Iteration 4396) ===
Q mean: -62.404324
Q std: 21.542900
Actor loss: 62.408314
Action reg: 0.003989
  l1.weight: grad_norm = 0.006718
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.020258
Total gradient norm: 0.050623
=== Actor Training Debug (Iteration 4397) ===
Q mean: -62.840237
Q std: 19.555538
Actor loss: 62.844219
Action reg: 0.003984
  l1.weight: grad_norm = 0.000769
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.003650
Total gradient norm: 0.011834
=== Actor Training Debug (Iteration 4398) ===
Q mean: -60.779541
Q std: 21.262491
Actor loss: 60.783524
Action reg: 0.003982
  l1.weight: grad_norm = 0.024589
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.060038
Total gradient norm: 0.119474
=== Actor Training Debug (Iteration 4399) ===
Q mean: -59.064556
Q std: 20.608976
Actor loss: 59.068539
Action reg: 0.003984
  l1.weight: grad_norm = 0.115928
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.251535
Total gradient norm: 0.423879
=== Actor Training Debug (Iteration 4400) ===
Q mean: -62.271721
Q std: 20.182241
Actor loss: 62.275707
Action reg: 0.003988
  l1.weight: grad_norm = 0.021402
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.046005
Total gradient norm: 0.091965
=== Actor Training Debug (Iteration 4401) ===
Q mean: -62.778511
Q std: 20.186789
Actor loss: 62.782494
Action reg: 0.003984
  l1.weight: grad_norm = 0.066254
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.145736
Total gradient norm: 0.275194
=== Actor Training Debug (Iteration 4402) ===
Q mean: -61.516846
Q std: 21.336700
Actor loss: 61.520828
Action reg: 0.003982
  l1.weight: grad_norm = 0.229912
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.512979
Total gradient norm: 0.915214
=== Actor Training Debug (Iteration 4403) ===
Q mean: -61.469601
Q std: 20.785295
Actor loss: 61.473587
Action reg: 0.003986
  l1.weight: grad_norm = 0.081016
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.197786
Total gradient norm: 0.425905
=== Actor Training Debug (Iteration 4404) ===
Q mean: -59.537720
Q std: 20.352028
Actor loss: 59.541714
Action reg: 0.003992
  l1.weight: grad_norm = 0.082616
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.165233
Total gradient norm: 0.312158
=== Actor Training Debug (Iteration 4405) ===
Q mean: -61.347271
Q std: 20.000439
Actor loss: 61.351257
Action reg: 0.003985
  l1.weight: grad_norm = 0.000864
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.003397
Total gradient norm: 0.010613
=== Actor Training Debug (Iteration 4406) ===
Q mean: -60.558716
Q std: 19.066336
Actor loss: 60.562698
Action reg: 0.003984
  l1.weight: grad_norm = 0.000937
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.004101
Total gradient norm: 0.012714
=== Actor Training Debug (Iteration 4407) ===
Q mean: -62.875252
Q std: 20.168358
Actor loss: 62.879253
Action reg: 0.004000
  l1.weight: grad_norm = 0.006828
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.014610
Total gradient norm: 0.031505
=== Actor Training Debug (Iteration 4408) ===
Q mean: -60.860821
Q std: 21.726101
Actor loss: 60.864811
Action reg: 0.003992
  l1.weight: grad_norm = 0.000677
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.002022
Total gradient norm: 0.005567
=== Actor Training Debug (Iteration 4409) ===
Q mean: -62.359867
Q std: 21.239523
Actor loss: 62.363869
Action reg: 0.004000
  l1.weight: grad_norm = 0.003497
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006978
Total gradient norm: 0.015275
=== Actor Training Debug (Iteration 4410) ===
Q mean: -60.801796
Q std: 20.773252
Actor loss: 60.805786
Action reg: 0.003991
  l1.weight: grad_norm = 0.102906
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.201624
Total gradient norm: 0.349459
=== Actor Training Debug (Iteration 4411) ===
Q mean: -63.473171
Q std: 19.812906
Actor loss: 63.477165
Action reg: 0.003994
  l1.weight: grad_norm = 0.002137
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.004637
Total gradient norm: 0.007492
=== Actor Training Debug (Iteration 4412) ===
Q mean: -63.693531
Q std: 20.741922
Actor loss: 63.697521
Action reg: 0.003989
  l1.weight: grad_norm = 0.002820
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.006015
Total gradient norm: 0.013765
=== Actor Training Debug (Iteration 4413) ===
Q mean: -62.847523
Q std: 21.753496
Actor loss: 62.851501
Action reg: 0.003979
  l1.weight: grad_norm = 0.035637
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.075955
Total gradient norm: 0.155525
=== Actor Training Debug (Iteration 4414) ===
Q mean: -61.208027
Q std: 20.988424
Actor loss: 61.212017
Action reg: 0.003991
  l1.weight: grad_norm = 0.014140
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.027352
Total gradient norm: 0.049294
=== Actor Training Debug (Iteration 4415) ===
Q mean: -61.271416
Q std: 19.582527
Actor loss: 61.275410
Action reg: 0.003992
  l1.weight: grad_norm = 0.137013
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.342159
Total gradient norm: 0.706827
=== Actor Training Debug (Iteration 4416) ===
Q mean: -62.768845
Q std: 20.166977
Actor loss: 62.772839
Action reg: 0.003995
  l1.weight: grad_norm = 0.007936
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.018873
Total gradient norm: 0.042505
=== Actor Training Debug (Iteration 4417) ===
Q mean: -60.270260
Q std: 20.938217
Actor loss: 60.274250
Action reg: 0.003992
  l1.weight: grad_norm = 0.004239
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.013547
Total gradient norm: 0.034276
=== Actor Training Debug (Iteration 4418) ===
Q mean: -60.114899
Q std: 20.277187
Actor loss: 60.118881
Action reg: 0.003984
  l1.weight: grad_norm = 0.165752
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.330392
Total gradient norm: 0.671720
=== Actor Training Debug (Iteration 4419) ===
Q mean: -62.342598
Q std: 20.250084
Actor loss: 62.346581
Action reg: 0.003982
  l1.weight: grad_norm = 0.108071
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.261651
Total gradient norm: 0.525672
=== Actor Training Debug (Iteration 4420) ===
Q mean: -63.502148
Q std: 19.906328
Actor loss: 63.506142
Action reg: 0.003992
  l1.weight: grad_norm = 0.190959
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.690094
Total gradient norm: 1.525135
=== Actor Training Debug (Iteration 4421) ===
Q mean: -61.956474
Q std: 19.304140
Actor loss: 61.960464
Action reg: 0.003990
  l1.weight: grad_norm = 0.102358
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.233539
Total gradient norm: 0.448312
=== Actor Training Debug (Iteration 4422) ===
Q mean: -62.886295
Q std: 20.520166
Actor loss: 62.890285
Action reg: 0.003991
  l1.weight: grad_norm = 0.001359
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.003073
Total gradient norm: 0.006454
=== Actor Training Debug (Iteration 4423) ===
Q mean: -63.279381
Q std: 21.657619
Actor loss: 63.283360
Action reg: 0.003979
  l1.weight: grad_norm = 0.002809
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.008487
Total gradient norm: 0.023049
=== Actor Training Debug (Iteration 4424) ===
Q mean: -64.836151
Q std: 20.423918
Actor loss: 64.840141
Action reg: 0.003990
  l1.weight: grad_norm = 0.048078
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.096844
Total gradient norm: 0.208584
=== Actor Training Debug (Iteration 4425) ===
Q mean: -62.087532
Q std: 21.048101
Actor loss: 62.091515
Action reg: 0.003982
  l1.weight: grad_norm = 0.002744
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.007211
Total gradient norm: 0.017585
=== Actor Training Debug (Iteration 4426) ===
Q mean: -61.161968
Q std: 21.759645
Actor loss: 61.165951
Action reg: 0.003981
  l1.weight: grad_norm = 0.054719
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.152820
Total gradient norm: 0.335763
=== Actor Training Debug (Iteration 4427) ===
Q mean: -62.846359
Q std: 21.326014
Actor loss: 62.850353
Action reg: 0.003994
  l1.weight: grad_norm = 0.041789
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.106452
Total gradient norm: 0.167989
=== Actor Training Debug (Iteration 4428) ===
Q mean: -64.067116
Q std: 20.671543
Actor loss: 64.071114
Action reg: 0.003999
  l1.weight: grad_norm = 0.021923
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.055098
Total gradient norm: 0.128197
Total gradient norm: 0.3981420596on 1203) ===
=== Actor Training Debug (Iteration 4439) ===
Q mean: -63.076389
Q std: 21.527571
Actor loss: 63.080364
Action reg: 0.003976
  l1.weight: grad_norm = 0.146155
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.363172
Total gradient norm: 0.676713
=== Actor Training Debug (Iteration 4440) ===
Q mean: -61.385628
Q std: 19.277943
Actor loss: 61.389614
Action reg: 0.003985
  l1.weight: grad_norm = 0.151902
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.389597
Total gradient norm: 0.600242
=== Actor Training Debug (Iteration 4441) ===
Q mean: -63.011440
Q std: 20.709658
Actor loss: 63.015427
Action reg: 0.003986
  l1.weight: grad_norm = 0.081764
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.264251
Total gradient norm: 0.545095
=== Actor Training Debug (Iteration 4442) ===
Q mean: -61.961899
Q std: 20.867384
Actor loss: 61.965881
Action reg: 0.003982
  l1.weight: grad_norm = 0.267187
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.583839
Total gradient norm: 1.063279
=== Actor Training Debug (Iteration 4443) ===
Q mean: -63.671017
Q std: 21.484444
Actor loss: 63.674995
Action reg: 0.003980
  l1.weight: grad_norm = 0.071346
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.195120
Total gradient norm: 0.295954
=== Actor Training Debug (Iteration 4444) ===
Q mean: -60.843742
Q std: 22.073235
Actor loss: 60.847721
Action reg: 0.003980
  l1.weight: grad_norm = 0.078567
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.242586
Total gradient norm: 0.391155
=== Actor Training Debug (Iteration 4445) ===
Q mean: -60.246513
Q std: 19.418079
Actor loss: 60.250484
Action reg: 0.003972
  l1.weight: grad_norm = 0.154654
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.418484
Total gradient norm: 0.782438
=== Actor Training Debug (Iteration 4446) ===
Q mean: -62.190987
Q std: 21.012789
Actor loss: 62.194977
Action reg: 0.003990
  l1.weight: grad_norm = 0.045891
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.131817
Total gradient norm: 0.202738
=== Actor Training Debug (Iteration 4447) ===
Q mean: -60.085278
Q std: 21.018204
Actor loss: 60.089264
Action reg: 0.003986
  l1.weight: grad_norm = 0.095100
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.331710
Total gradient norm: 0.831944
=== Actor Training Debug (Iteration 4448) ===
Q mean: -62.032196
Q std: 20.369558
Actor loss: 62.036140
Action reg: 0.003944
  l1.weight: grad_norm = 0.459999
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 1.577617
Total gradient norm: 3.447123
=== Actor Training Debug (Iteration 4449) ===
Q mean: -62.685509
Q std: 21.163614
Actor loss: 62.689499
Action reg: 0.003988
  l1.weight: grad_norm = 0.123253
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.323690
Total gradient norm: 0.584605
=== Actor Training Debug (Iteration 4450) ===
Q mean: -63.411266
Q std: 20.213781
Actor loss: 63.415260
Action reg: 0.003993
  l1.weight: grad_norm = 0.107034
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.261445
Total gradient norm: 0.515540
=== Actor Training Debug (Iteration 4451) ===
Q mean: -60.664070
Q std: 20.494963
Actor loss: 60.668056
Action reg: 0.003987
  l1.weight: grad_norm = 0.048357
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.136745
Total gradient norm: 0.255982
=== Actor Training Debug (Iteration 4452) ===
Q mean: -60.553436
Q std: 20.931894
Actor loss: 60.557415
Action reg: 0.003980
  l1.weight: grad_norm = 0.034972
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.137040
Total gradient norm: 0.306809
=== Actor Training Debug (Iteration 4453) ===
Q mean: -64.883026
Q std: 22.020025
Actor loss: 64.887001
Action reg: 0.003975
  l1.weight: grad_norm = 0.062836
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.160499
Total gradient norm: 0.299214
=== Actor Training Debug (Iteration 4454) ===
Q mean: -62.291817
Q std: 21.145353
Actor loss: 62.295803
Action reg: 0.003987
  l1.weight: grad_norm = 0.090524
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.217420
Total gradient norm: 0.330766
=== Actor Training Debug (Iteration 4455) ===
Q mean: -61.016586
Q std: 20.372797
Actor loss: 61.020576
Action reg: 0.003990
  l1.weight: grad_norm = 0.024884
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.084985
Total gradient norm: 0.193027
=== Actor Training Debug (Iteration 4456) ===
Q mean: -60.406731
Q std: 20.299900
Actor loss: 60.410717
Action reg: 0.003988
  l1.weight: grad_norm = 0.003091
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.007928
Total gradient norm: 0.015391
=== Actor Training Debug (Iteration 4457) ===
Q mean: -58.544060
Q std: 21.092331
Actor loss: 58.548042
Action reg: 0.003982
  l1.weight: grad_norm = 0.241400
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.533234
Total gradient norm: 1.069109
=== Actor Training Debug (Iteration 4458) ===
Q mean: -64.046799
Q std: 20.803650
Actor loss: 64.050781
Action reg: 0.003979
  l1.weight: grad_norm = 0.058886
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 0.192729
Total gradient norm: 0.420702
=== Actor Training Debug (Iteration 4459) ===
Q mean: -61.178284
Q std: 21.300888
Actor loss: 61.182270
Action reg: 0.003987
  l1.weight: grad_norm = 0.022539
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.061559
Total gradient norm: 0.102427
=== Actor Training Debug (Iteration 4460) ===
Q mean: -61.844849
Q std: 20.578140
Actor loss: 61.848824
Action reg: 0.003976
  l1.weight: grad_norm = 0.052509
  l1.bias: grad_norm = 0.000541
  l2.weight: grad_norm = 0.150842
Total gradient norm: 0.343658
=== Actor Training Debug (Iteration 4461) ===
Q mean: -62.715759
Q std: 20.381012
Actor loss: 62.719746
Action reg: 0.003987
  l1.weight: grad_norm = 0.081832
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.185869
Total gradient norm: 0.390350
=== Actor Training Debug (Iteration 4462) ===
Q mean: -61.025970
Q std: 22.354618
Actor loss: 61.029953
Action reg: 0.003981
  l1.weight: grad_norm = 0.150917
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.524452
Total gradient norm: 1.190146
=== Actor Training Debug (Iteration 4463) ===
Q mean: -62.331497
Q std: 21.125561
Actor loss: 62.335491
Action reg: 0.003994
  l1.weight: grad_norm = 0.096907
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.314197
Total gradient norm: 0.664220
=== Actor Training Debug (Iteration 4464) ===
Q mean: -62.327099
Q std: 21.265671
Actor loss: 62.331078
Action reg: 0.003978
  l1.weight: grad_norm = 0.131743
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.395995
Total gradient norm: 0.557721
=== Actor Training Debug (Iteration 4465) ===
Q mean: -62.167034
Q std: 21.172602
Actor loss: 62.171028
Action reg: 0.003992
  l1.weight: grad_norm = 0.017906
  l1.bias: grad_norm = 0.000087
  l1.bias: grad_norm = 0.00025196on 1203) ===
  l2.weight: grad_norm = 0.318678
Total gradient norm: 0.584417
=== Actor Training Debug (Iteration 4476) ===
Q mean: -63.435188
Q std: 21.828283
Actor loss: 63.439175
Action reg: 0.003985
  l1.weight: grad_norm = 0.011035
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.037309
Total gradient norm: 0.077611
=== Actor Training Debug (Iteration 4477) ===
Q mean: -63.045532
Q std: 20.119749
Actor loss: 63.049519
Action reg: 0.003987
  l1.weight: grad_norm = 0.029413
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.081368
Total gradient norm: 0.190276
=== Actor Training Debug (Iteration 4478) ===
Q mean: -64.590942
Q std: 20.462582
Actor loss: 64.594933
Action reg: 0.003987
  l1.weight: grad_norm = 0.025190
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.081274
Total gradient norm: 0.151676
=== Actor Training Debug (Iteration 4479) ===
Q mean: -60.884266
Q std: 21.224104
Actor loss: 60.888241
Action reg: 0.003977
  l1.weight: grad_norm = 0.208659
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.612981
Total gradient norm: 1.474058
=== Actor Training Debug (Iteration 4480) ===
Q mean: -60.664497
Q std: 21.385134
Actor loss: 60.668484
Action reg: 0.003987
  l1.weight: grad_norm = 0.052871
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.154675
Total gradient norm: 0.353934
=== Actor Training Debug (Iteration 4481) ===
Q mean: -61.740982
Q std: 20.922176
Actor loss: 61.744965
Action reg: 0.003981
  l1.weight: grad_norm = 0.081252
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.235020
Total gradient norm: 0.489156
=== Actor Training Debug (Iteration 4482) ===
Q mean: -63.667374
Q std: 20.116375
Actor loss: 63.671356
Action reg: 0.003984
  l1.weight: grad_norm = 0.071245
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.190839
Total gradient norm: 0.318165
=== Actor Training Debug (Iteration 4483) ===
Q mean: -59.872299
Q std: 22.130957
Actor loss: 59.876274
Action reg: 0.003974
  l1.weight: grad_norm = 0.006896
  l1.bias: grad_norm = 0.000663
  l2.weight: grad_norm = 0.020823
Total gradient norm: 0.038291
=== Actor Training Debug (Iteration 4484) ===
Q mean: -63.963280
Q std: 20.456333
Actor loss: 63.967278
Action reg: 0.003996
  l1.weight: grad_norm = 0.051526
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.195538
Total gradient norm: 0.506989
=== Actor Training Debug (Iteration 4485) ===
Q mean: -60.395180
Q std: 20.982168
Actor loss: 60.399151
Action reg: 0.003972
  l1.weight: grad_norm = 0.099365
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.261023
Total gradient norm: 0.385571
=== Actor Training Debug (Iteration 4486) ===
Q mean: -61.196556
Q std: 20.526068
Actor loss: 61.200546
Action reg: 0.003991
  l1.weight: grad_norm = 0.176118
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.404298
Total gradient norm: 0.874677
=== Actor Training Debug (Iteration 4487) ===
Q mean: -60.496937
Q std: 20.712629
Actor loss: 60.500919
Action reg: 0.003984
  l1.weight: grad_norm = 0.108575
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.320527
Total gradient norm: 0.572631
=== Actor Training Debug (Iteration 4488) ===
Q mean: -61.165478
Q std: 20.477041
Actor loss: 61.169460
Action reg: 0.003984
  l1.weight: grad_norm = 0.099416
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.303132
Total gradient norm: 0.665156
=== Actor Training Debug (Iteration 4489) ===
Q mean: -65.408829
Q std: 20.502588
Actor loss: 65.412811
Action reg: 0.003980
  l1.weight: grad_norm = 0.042567
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.097018
Total gradient norm: 0.190586
=== Actor Training Debug (Iteration 4490) ===
Q mean: -62.078400
Q std: 21.242971
Actor loss: 62.082378
Action reg: 0.003978
  l1.weight: grad_norm = 0.074416
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.180618
Total gradient norm: 0.407191
=== Actor Training Debug (Iteration 4491) ===
Q mean: -60.493984
Q std: 21.810476
Actor loss: 60.497948
Action reg: 0.003963
  l1.weight: grad_norm = 1.303132
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 3.759345
Total gradient norm: 8.203733
=== Actor Training Debug (Iteration 4492) ===
Q mean: -62.639488
Q std: 19.396963
Actor loss: 62.643478
Action reg: 0.003990
  l1.weight: grad_norm = 0.096565
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.244064
Total gradient norm: 0.532171
=== Actor Training Debug (Iteration 4493) ===
Q mean: -59.157799
Q std: 20.924179
Actor loss: 59.161774
Action reg: 0.003976
  l1.weight: grad_norm = 0.127708
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.376277
Total gradient norm: 0.616777
=== Actor Training Debug (Iteration 4494) ===
Q mean: -63.093811
Q std: 22.201651
Actor loss: 63.097786
Action reg: 0.003975
  l1.weight: grad_norm = 0.081231
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.233277
Total gradient norm: 0.496084
=== Actor Training Debug (Iteration 4495) ===
Q mean: -63.384792
Q std: 21.105772
Actor loss: 63.388786
Action reg: 0.003994
  l1.weight: grad_norm = 0.112417
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.316631
Total gradient norm: 0.573363
=== Actor Training Debug (Iteration 4496) ===
Q mean: -62.636444
Q std: 20.457636
Actor loss: 62.640442
Action reg: 0.003997
  l1.weight: grad_norm = 0.007988
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.018094
Total gradient norm: 0.025739
=== Actor Training Debug (Iteration 4497) ===
Q mean: -62.288872
Q std: 19.894670
Actor loss: 62.292870
Action reg: 0.003997
  l1.weight: grad_norm = 0.036738
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.097682
Total gradient norm: 0.224542
=== Actor Training Debug (Iteration 4498) ===
Q mean: -63.127655
Q std: 20.073187
Actor loss: 63.131638
Action reg: 0.003984
  l1.weight: grad_norm = 0.119702
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.381699
Total gradient norm: 0.872129
=== Actor Training Debug (Iteration 4499) ===
Q mean: -64.184021
Q std: 20.446049
Actor loss: 64.188019
Action reg: 0.003994
  l1.weight: grad_norm = 0.111549
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.286501
Total gradient norm: 0.543665
=== Actor Training Debug (Iteration 4500) ===
Q mean: -65.379532
Q std: 20.511316
Actor loss: 65.383522
Action reg: 0.003988
  l1.weight: grad_norm = 0.025421
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.060174
Total gradient norm: 0.121702
  l1.bias: grad_norm = 0.00014696on 1203) ===
  l2.weight: grad_norm = 0.422861
Total gradient norm: 0.901255
=== Actor Training Debug (Iteration 4511) ===
Q mean: -64.707451
Q std: 21.477461
Actor loss: 64.711441
Action reg: 0.003987
  l1.weight: grad_norm = 0.226498
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.590653
Total gradient norm: 0.822509
=== Actor Training Debug (Iteration 4512) ===
Q mean: -62.855759
Q std: 21.908783
Actor loss: 62.859745
Action reg: 0.003985
  l1.weight: grad_norm = 0.164850
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.340980
Total gradient norm: 0.548392
=== Actor Training Debug (Iteration 4513) ===
Q mean: -61.926208
Q std: 20.229887
Actor loss: 61.930195
Action reg: 0.003988
  l1.weight: grad_norm = 0.007075
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.019126
Total gradient norm: 0.044694
=== Actor Training Debug (Iteration 4514) ===
Q mean: -62.628593
Q std: 20.283743
Actor loss: 62.632576
Action reg: 0.003983
  l1.weight: grad_norm = 0.093253
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.294191
Total gradient norm: 0.639291
=== Actor Training Debug (Iteration 4515) ===
Q mean: -62.460419
Q std: 22.793432
Actor loss: 62.464397
Action reg: 0.003980
  l1.weight: grad_norm = 0.050336
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.138679
Total gradient norm: 0.319120
=== Actor Training Debug (Iteration 4516) ===
Q mean: -62.952690
Q std: 20.442379
Actor loss: 62.956688
Action reg: 0.003998
  l1.weight: grad_norm = 0.007281
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.017799
Total gradient norm: 0.037057
=== Actor Training Debug (Iteration 4517) ===
Q mean: -62.833466
Q std: 21.239248
Actor loss: 62.837456
Action reg: 0.003990
  l1.weight: grad_norm = 0.014437
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.035582
Total gradient norm: 0.063500
=== Actor Training Debug (Iteration 4518) ===
Q mean: -61.419163
Q std: 21.250509
Actor loss: 61.423145
Action reg: 0.003984
  l1.weight: grad_norm = 0.041320
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.080116
Total gradient norm: 0.131854
=== Actor Training Debug (Iteration 4519) ===
Q mean: -64.330750
Q std: 22.487108
Actor loss: 64.334732
Action reg: 0.003981
  l1.weight: grad_norm = 0.114161
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.338674
Total gradient norm: 0.652519
=== Actor Training Debug (Iteration 4520) ===
Q mean: -62.714550
Q std: 20.732731
Actor loss: 62.718533
Action reg: 0.003981
  l1.weight: grad_norm = 0.073795
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.209844
Total gradient norm: 0.421136
=== Actor Training Debug (Iteration 4521) ===
Q mean: -63.709663
Q std: 20.069197
Actor loss: 63.713650
Action reg: 0.003988
  l1.weight: grad_norm = 0.097835
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.263071
Total gradient norm: 0.389665
=== Actor Training Debug (Iteration 4522) ===
Q mean: -61.607101
Q std: 20.466633
Actor loss: 61.611080
Action reg: 0.003977
  l1.weight: grad_norm = 0.205726
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.479941
Total gradient norm: 0.965334
=== Actor Training Debug (Iteration 4523) ===
Q mean: -62.659515
Q std: 21.614281
Actor loss: 62.663498
Action reg: 0.003982
  l1.weight: grad_norm = 0.078476
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.190925
Total gradient norm: 0.292531
=== Actor Training Debug (Iteration 4524) ===
Q mean: -61.927612
Q std: 19.556139
Actor loss: 61.931606
Action reg: 0.003996
  l1.weight: grad_norm = 0.062893
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.131361
Total gradient norm: 0.232426
=== Actor Training Debug (Iteration 4525) ===
Q mean: -63.313362
Q std: 22.983051
Actor loss: 63.317345
Action reg: 0.003982
  l1.weight: grad_norm = 0.029693
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.085046
Total gradient norm: 0.173942
=== Actor Training Debug (Iteration 4526) ===
Q mean: -60.120773
Q std: 20.273985
Actor loss: 60.124744
Action reg: 0.003973
  l1.weight: grad_norm = 0.105707
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.308357
Total gradient norm: 0.585421
=== Actor Training Debug (Iteration 4527) ===
Q mean: -62.228649
Q std: 22.497259
Actor loss: 62.232624
Action reg: 0.003977
  l1.weight: grad_norm = 0.037649
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.093862
Total gradient norm: 0.193025
=== Actor Training Debug (Iteration 4528) ===
Q mean: -61.472031
Q std: 21.581749
Actor loss: 61.476013
Action reg: 0.003983
  l1.weight: grad_norm = 0.036609
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.103594
Total gradient norm: 0.201331
=== Actor Training Debug (Iteration 4529) ===
Q mean: -62.578495
Q std: 21.144201
Actor loss: 62.582485
Action reg: 0.003990
  l1.weight: grad_norm = 0.016856
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.043982
Total gradient norm: 0.087510
=== Actor Training Debug (Iteration 4530) ===
Q mean: -62.864021
Q std: 20.747028
Actor loss: 62.868015
Action reg: 0.003994
  l1.weight: grad_norm = 0.078788
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.167213
Total gradient norm: 0.304631
=== Actor Training Debug (Iteration 4531) ===
Q mean: -63.263733
Q std: 22.359314
Actor loss: 63.267700
Action reg: 0.003969
  l1.weight: grad_norm = 0.451184
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 1.265371
Total gradient norm: 2.360526
=== Actor Training Debug (Iteration 4532) ===
Q mean: -62.704311
Q std: 22.023634
Actor loss: 62.708294
Action reg: 0.003983
  l1.weight: grad_norm = 0.072710
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.153402
Total gradient norm: 0.317415
=== Actor Training Debug (Iteration 4533) ===
Q mean: -59.473213
Q std: 22.116810
Actor loss: 59.477203
Action reg: 0.003989
  l1.weight: grad_norm = 0.009941
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.024064
Total gradient norm: 0.046063
=== Actor Training Debug (Iteration 4534) ===
Q mean: -60.631142
Q std: 21.450806
Actor loss: 60.635128
Action reg: 0.003986
  l1.weight: grad_norm = 0.021308
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.058557
Total gradient norm: 0.101578
=== Actor Training Debug (Iteration 4535) ===
Q mean: -64.086975
Q std: 19.886751
Actor loss: 64.090958
Action reg: 0.003980
  l1.weight: grad_norm = 0.018350
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.055371
Total gradient norm: 0.112183
=== Actor Training Debug (Iteration 4536) ===
Q mean: -64.051254
Q std: 23.023487
Actor loss: 64.055237
Action reg: 0.003984
  l1.weight: grad_norm = 0.086463
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.236257
Total gradient norm: 0.510475
=== Actor Training Debug (Iteration 4537) ===
Q mean: -64.348907
Q std: 20.790573
Actor loss: 64.352890
Action reg: 0.003983
  l1.weight: grad_norm = 0.037711
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.096694
Total gradient norm: 0.192603
Total gradient norm: 0.1397724696on 1203) ===
=== Actor Training Debug (Iteration 4548) ===
Q mean: -64.472824
Q std: 20.496649
Actor loss: 64.476814
Action reg: 0.003992
  l1.weight: grad_norm = 0.007975
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.019465
Total gradient norm: 0.033423
=== Actor Training Debug (Iteration 4549) ===
Q mean: -61.146706
Q std: 21.052259
Actor loss: 61.150696
Action reg: 0.003988
  l1.weight: grad_norm = 0.148625
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.329588
Total gradient norm: 0.594792
=== Actor Training Debug (Iteration 4550) ===
Q mean: -61.596870
Q std: 19.218395
Actor loss: 61.600864
Action reg: 0.003995
  l1.weight: grad_norm = 0.018011
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.051499
Total gradient norm: 0.115108
=== Actor Training Debug (Iteration 4551) ===
Q mean: -63.352112
Q std: 21.855610
Actor loss: 63.356098
Action reg: 0.003987
  l1.weight: grad_norm = 0.050650
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.128833
Total gradient norm: 0.220512
=== Actor Training Debug (Iteration 4552) ===
Q mean: -67.114029
Q std: 20.533113
Actor loss: 67.118027
Action reg: 0.003996
  l1.weight: grad_norm = 0.100977
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.201410
Total gradient norm: 0.370536
=== Actor Training Debug (Iteration 4553) ===
Q mean: -63.219940
Q std: 20.409407
Actor loss: 63.223927
Action reg: 0.003985
  l1.weight: grad_norm = 0.011096
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.026511
Total gradient norm: 0.048823
=== Actor Training Debug (Iteration 4554) ===
Q mean: -61.175400
Q std: 21.293671
Actor loss: 61.179390
Action reg: 0.003988
  l1.weight: grad_norm = 0.001802
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.004826
Total gradient norm: 0.008173
=== Actor Training Debug (Iteration 4555) ===
Q mean: -60.599564
Q std: 20.632051
Actor loss: 60.603546
Action reg: 0.003983
  l1.weight: grad_norm = 0.007631
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.022702
Total gradient norm: 0.048685
=== Actor Training Debug (Iteration 4556) ===
Q mean: -64.947449
Q std: 22.823145
Actor loss: 64.951431
Action reg: 0.003986
  l1.weight: grad_norm = 0.193272
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.352944
Total gradient norm: 0.635701
=== Actor Training Debug (Iteration 4557) ===
Q mean: -63.807125
Q std: 21.798372
Actor loss: 63.811111
Action reg: 0.003988
  l1.weight: grad_norm = 0.050941
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.108511
Total gradient norm: 0.176817
=== Actor Training Debug (Iteration 4558) ===
Q mean: -63.915089
Q std: 20.186628
Actor loss: 63.919079
Action reg: 0.003992
  l1.weight: grad_norm = 0.031913
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.066038
Total gradient norm: 0.128306
=== Actor Training Debug (Iteration 4559) ===
Q mean: -64.267685
Q std: 20.767921
Actor loss: 64.271675
Action reg: 0.003989
  l1.weight: grad_norm = 0.058890
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.171531
Total gradient norm: 0.339496
=== Actor Training Debug (Iteration 4560) ===
Q mean: -62.403919
Q std: 21.705662
Actor loss: 62.407906
Action reg: 0.003985
  l1.weight: grad_norm = 0.071022
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.148483
Total gradient norm: 0.288615
=== Actor Training Debug (Iteration 4561) ===
Q mean: -63.917812
Q std: 21.313282
Actor loss: 63.921795
Action reg: 0.003984
  l1.weight: grad_norm = 0.237824
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.492415
Total gradient norm: 1.082514
=== Actor Training Debug (Iteration 4562) ===
Q mean: -63.591785
Q std: 19.970268
Actor loss: 63.595768
Action reg: 0.003984
  l1.weight: grad_norm = 0.096121
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.233818
Total gradient norm: 0.382272
=== Actor Training Debug (Iteration 4563) ===
Q mean: -63.372818
Q std: 21.118874
Actor loss: 63.376808
Action reg: 0.003991
  l1.weight: grad_norm = 0.072462
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.143464
Total gradient norm: 0.250570
=== Actor Training Debug (Iteration 4564) ===
Q mean: -61.802910
Q std: 20.723698
Actor loss: 61.806889
Action reg: 0.003979
  l1.weight: grad_norm = 0.132892
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.394245
Total gradient norm: 0.887469
=== Actor Training Debug (Iteration 4565) ===
Q mean: -64.017670
Q std: 21.649290
Actor loss: 64.021660
Action reg: 0.003989
  l1.weight: grad_norm = 0.179179
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.412730
Total gradient norm: 0.816420
=== Actor Training Debug (Iteration 4566) ===
Q mean: -63.947403
Q std: 21.402920
Actor loss: 63.951393
Action reg: 0.003989
  l1.weight: grad_norm = 0.068090
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.191613
Total gradient norm: 0.323577
=== Actor Training Debug (Iteration 4567) ===
Q mean: -62.182728
Q std: 21.824347
Actor loss: 62.186722
Action reg: 0.003993
  l1.weight: grad_norm = 0.037470
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.078046
Total gradient norm: 0.138585
=== Actor Training Debug (Iteration 4568) ===
Q mean: -63.335297
Q std: 20.317032
Actor loss: 63.339298
Action reg: 0.004000
  l1.weight: grad_norm = 0.000766
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001710
Total gradient norm: 0.002587
=== Actor Training Debug (Iteration 4569) ===
Q mean: -63.772362
Q std: 20.669043
Actor loss: 63.776360
Action reg: 0.003996
  l1.weight: grad_norm = 0.090561
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.276882
Total gradient norm: 0.627210
=== Actor Training Debug (Iteration 4570) ===
Q mean: -66.686523
Q std: 21.775101
Actor loss: 66.690506
Action reg: 0.003986
  l1.weight: grad_norm = 0.060379
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.141383
Total gradient norm: 0.279743
=== Actor Training Debug (Iteration 4571) ===
Q mean: -66.830063
Q std: 19.293510
Actor loss: 66.834053
Action reg: 0.003990
  l1.weight: grad_norm = 0.133873
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.356916
Total gradient norm: 0.678484
=== Actor Training Debug (Iteration 4572) ===
Q mean: -61.071201
Q std: 23.177530
Actor loss: 61.075184
Action reg: 0.003981
  l1.weight: grad_norm = 0.217774
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.453491
Total gradient norm: 0.795576
=== Actor Training Debug (Iteration 4573) ===
Q mean: -65.260391
Q std: 21.108496
Actor loss: 65.264381
Action reg: 0.003991
  l1.weight: grad_norm = 0.020627
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.042282
Total gradient norm: 0.072441
=== Actor Training Debug (Iteration 4574) ===
Q mean: -61.331654
Q std: 21.526472
Actor loss: 61.335644
Action reg: 0.003990
  l1.weight: grad_norm = 0.040550
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.092730
Total gradient norm: 0.181254
=== Actor Training Debug (Iteration 4575) ===
Q mean: -63.053051
Q std: 21.086742
Actor loss: 63.057045
Action reg: 0.003993
  l1.weight: grad_norm = 0.041991
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.117974
Total gradient norm: 0.275907
=== Actor Training Debug (Iteration 4576) ===
Q mean: -64.778442
Q std: 21.298975
Actor loss: 64.782433
Action reg: 0.003990
  l1.weight: grad_norm = 0.090629
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.186627
Total gradient norm: 0.359220
=== Actor Training Debug (Iteration 4577) ===
Q mean: -62.111946
Q std: 20.681934
Actor loss: 62.115936
Action reg: 0.003989
  l1.weight: grad_norm = 0.009358
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.020395
Total gradient norm: 0.028747
=== Actor Training Debug (Iteration 4578) ===
Q mean: -65.362717
Q std: 21.247486
Actor loss: 65.366707
Action reg: 0.003987
  l1.weight: grad_norm = 0.063185
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.174669
Total gradient norm: 0.385503
=== Actor Training Debug (Iteration 4579) ===
Q mean: -61.016747
Q std: 22.205242
Actor loss: 61.020718
Action reg: 0.003973
  l1.weight: grad_norm = 0.208730
  l1.bias: grad_norm = 0.000557
  l2.weight: grad_norm = 0.602229
Total gradient norm: 1.216329
=== Actor Training Debug (Iteration 4580) ===
Q mean: -64.824287
Q std: 21.293257
Actor loss: 64.828262
Action reg: 0.003976
  l1.weight: grad_norm = 0.128842
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.306903
Total gradient norm: 0.557082
=== Actor Training Debug (Iteration 4581) ===
Q mean: -64.472557
Q std: 20.674332
Actor loss: 64.476540
Action reg: 0.003984
  l1.weight: grad_norm = 0.194423
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.404118
Total gradient norm: 0.724041
=== Actor Training Debug (Iteration 4582) ===
Q mean: -63.916267
Q std: 20.282984
Actor loss: 63.920258
Action reg: 0.003988
  l1.weight: grad_norm = 0.037176
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.111368
Total gradient norm: 0.270941
=== Actor Training Debug (Iteration 4583) ===
Q mean: -64.190750
Q std: 21.414684
Actor loss: 64.194740
Action reg: 0.003991
  l1.weight: grad_norm = 0.063182
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.164972
Total gradient norm: 0.347357
=== Actor Training Debug (Iteration 4584) ===
Q mean: -63.023651
Q std: 21.343500
Actor loss: 63.027641
Action reg: 0.003990
  l1.weight: grad_norm = 0.084403
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.200867
Total gradient norm: 0.288341
=== Actor Training Debug (Iteration 4585) ===
Q mean: -60.557354
Q std: 19.488373
Actor loss: 60.561348
Action reg: 0.003993
  l1.weight: grad_norm = 0.078473
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.201977
Total gradient norm: 0.374748
=== Actor Training Debug (Iteration 4586) ===
Q mean: -65.005325
Q std: 20.439283
Actor loss: 65.009315
Action reg: 0.003991
  l1.weight: grad_norm = 0.117041
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.300643
Total gradient norm: 0.460580
=== Actor Training Debug (Iteration 4587) ===
Q mean: -64.290260
Q std: 22.460701
Actor loss: 64.294250
Action reg: 0.003990
  l1.weight: grad_norm = 0.025582
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.069114
Total gradient norm: 0.161864
=== Actor Training Debug (Iteration 4588) ===
Q mean: -62.336075
Q std: 22.201200
Actor loss: 62.340061
Action reg: 0.003985
  l1.weight: grad_norm = 0.118746
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.255463
Total gradient norm: 0.510866
=== Actor Training Debug (Iteration 4589) ===
Q mean: -61.298141
Q std: 22.130398
Actor loss: 61.302124
Action reg: 0.003981
  l1.weight: grad_norm = 0.041937
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.117469
Total gradient norm: 0.243618
=== Actor Training Debug (Iteration 4590) ===
Q mean: -63.441963
Q std: 21.098709
Actor loss: 63.445950
Action reg: 0.003987
  l1.weight: grad_norm = 0.044571
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.108143
Total gradient norm: 0.152576
=== Actor Training Debug (Iteration 4591) ===
Q mean: -64.159554
Q std: 20.626402
Actor loss: 64.163528
Action reg: 0.003978
  l1.weight: grad_norm = 0.074917
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.205562
Total gradient norm: 0.400154
=== Actor Training Debug (Iteration 4592) ===
Q mean: -63.530148
Q std: 20.238863
Actor loss: 63.534138
Action reg: 0.003991
  l1.weight: grad_norm = 0.028354
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.062914
Total gradient norm: 0.133338
=== Actor Training Debug (Iteration 4593) ===
Q mean: -62.375961
Q std: 19.832039
Actor loss: 62.379955
Action reg: 0.003994
  l1.weight: grad_norm = 0.067044
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.175174
Total gradient norm: 0.356436
=== Actor Training Debug (Iteration 4594) ===
Q mean: -65.420898
Q std: 20.612553
Actor loss: 65.424881
Action reg: 0.003986
  l1.weight: grad_norm = 0.101764
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.290128
Total gradient norm: 0.469368
=== Actor Training Debug (Iteration 4595) ===
Q mean: -64.617981
Q std: 20.558353
Actor loss: 64.621979
Action reg: 0.003995
  l1.weight: grad_norm = 0.030794
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.086868
Total gradient norm: 0.217016
=== Actor Training Debug (Iteration 4596) ===
Q mean: -62.165409
Q std: 20.091307
Actor loss: 62.169395
Action reg: 0.003986
  l1.weight: grad_norm = 0.140018
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.337245
Total gradient norm: 0.547658
=== Actor Training Debug (Iteration 4597) ===
Q mean: -62.151226
Q std: 21.402357
Actor loss: 62.155220
Action reg: 0.003996
  l1.weight: grad_norm = 0.001709
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.004311
Total gradient norm: 0.007741
=== Actor Training Debug (Iteration 4598) ===
Q mean: -62.179398
Q std: 22.403057
Actor loss: 62.183376
Action reg: 0.003979
  l1.weight: grad_norm = 0.107492
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.221821
Total gradient norm: 0.371399
=== Actor Training Debug (Iteration 4599) ===
Q mean: -63.788998
Q std: 21.031317
Actor loss: 63.792984
Action reg: 0.003985
  l1.weight: grad_norm = 0.093580
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.278863
Total gradient norm: 0.501717
=== Actor Training Debug (Iteration 4600) ===
Q mean: -62.666237
Q std: 21.345575
Actor loss: 62.670231
Action reg: 0.003993
  l1.weight: grad_norm = 0.004404
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.012677
Total gradient norm: 0.026321
=== Actor Training Debug (Iteration 4601) ===
Q mean: -63.385220
Q std: 21.071384
Actor loss: 63.389206
Action reg: 0.003986
  l1.weight: grad_norm = 0.142824
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.348997
Total gradient norm: 0.783798
=== Actor Training Debug (Iteration 4602) ===
Q mean: -64.078873
Q std: 20.733753
Actor loss: 64.082863
Action reg: 0.003989
  l1.weight: grad_norm = 0.146710
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.297578
Total gradient norm: 0.518342
=== Actor Training Debug (Iteration 4603) ===
Q mean: -64.268692
Q std: 21.285507
Actor loss: 64.272682
Action reg: 0.003989
  l1.weight: grad_norm = 0.097019
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.280876
Total gradient norm: 0.613850
=== Actor Training Debug (Iteration 4604) ===
Q mean: -63.236584
Q std: 20.404676
Actor loss: 63.240574
Action reg: 0.003991
  l1.weight: grad_norm = 0.153889
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.346120
Total gradient norm: 0.718799
=== Actor Training Debug (Iteration 4605) ===
Q mean: -62.774483
Q std: 22.345772
Actor loss: 62.778465
Action reg: 0.003982
  l1.weight: grad_norm = 0.094740
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.197214
Total gradient norm: 0.347655
=== Actor Training Debug (Iteration 4606) ===
Q mean: -64.015442
Q std: 19.560438
Actor loss: 64.019432
Action reg: 0.003988
  l1.weight: grad_norm = 0.060721
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.144311
Total gradient norm: 0.281116
=== Actor Training Debug (Iteration 4607) ===
Q mean: -62.178329
Q std: 20.389891
Actor loss: 62.182289
Action reg: 0.003959
  l1.weight: grad_norm = 0.360474
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 1.046536
Total gradient norm: 2.324641
=== Actor Training Debug (Iteration 4608) ===
Q mean: -63.834679
Q std: 19.095997
Actor loss: 63.838673
Action reg: 0.003994
  l1.weight: grad_norm = 0.016822
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.033485
Total gradient norm: 0.064102
=== Actor Training Debug (Iteration 4609) ===
Q mean: -66.232361
Q std: 20.215872
Actor loss: 66.236343
Action reg: 0.003985
  l1.weight: grad_norm = 0.203433
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.458233
Total gradient norm: 0.808356
=== Actor Training Debug (Iteration 4610) ===
Q mean: -64.815201
Q std: 22.051500
Actor loss: 64.819191
Action reg: 0.003989
  l1.weight: grad_norm = 0.067660
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.153834
Total gradient norm: 0.313769
=== Actor Training Debug (Iteration 4611) ===
Q mean: -62.559696
Q std: 21.590223
Actor loss: 62.563686
Action reg: 0.003989
  l1.weight: grad_norm = 0.162808
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.356011
Total gradient norm: 0.606118
=== Actor Training Debug (Iteration 4612) ===
Q mean: -63.908081
Q std: 22.860506
Actor loss: 63.912071
Action reg: 0.003991
  l1.weight: grad_norm = 0.055151
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.116538
Total gradient norm: 0.183984
=== Actor Training Debug (Iteration 4613) ===
Q mean: -63.490334
Q std: 20.768927
Actor loss: 63.494328
Action reg: 0.003995
  l1.weight: grad_norm = 0.050041
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.102926
Total gradient norm: 0.209516
=== Actor Training Debug (Iteration 4614) ===
Q mean: -64.310928
Q std: 20.909935
Actor loss: 64.314919
Action reg: 0.003987
  l1.weight: grad_norm = 0.070949
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.166094
Total gradient norm: 0.369603
Total gradient norm: 1.2553594696on 1203) ===
=== Actor Training Debug (Iteration 4625) ===
Q mean: -63.620392
Q std: 22.123436
Actor loss: 63.624374
Action reg: 0.003984
  l1.weight: grad_norm = 0.113261
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.268148
Total gradient norm: 0.425652
=== Actor Training Debug (Iteration 4626) ===
Q mean: -63.578522
Q std: 23.443804
Actor loss: 63.582500
Action reg: 0.003979
  l1.weight: grad_norm = 0.150923
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.363005
Total gradient norm: 0.705226
=== Actor Training Debug (Iteration 4627) ===
Q mean: -65.291756
Q std: 21.645063
Actor loss: 65.295746
Action reg: 0.003993
  l1.weight: grad_norm = 0.157712
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.404116
Total gradient norm: 0.828608
=== Actor Training Debug (Iteration 4628) ===
Q mean: -64.295746
Q std: 22.268204
Actor loss: 64.299736
Action reg: 0.003987
  l1.weight: grad_norm = 0.238537
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.684322
Total gradient norm: 1.374121
=== Actor Training Debug (Iteration 4629) ===
Q mean: -63.655807
Q std: 19.736113
Actor loss: 63.659790
Action reg: 0.003983
  l1.weight: grad_norm = 0.179742
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.398547
Total gradient norm: 0.723090
=== Actor Training Debug (Iteration 4630) ===
Q mean: -67.145996
Q std: 20.448677
Actor loss: 67.149994
Action reg: 0.003997
  l1.weight: grad_norm = 0.018177
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.046081
Total gradient norm: 0.089120
=== Actor Training Debug (Iteration 4631) ===
Q mean: -64.844131
Q std: 21.137880
Actor loss: 64.848122
Action reg: 0.003988
  l1.weight: grad_norm = 0.061829
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.146929
Total gradient norm: 0.317354
=== Actor Training Debug (Iteration 4632) ===
Q mean: -62.381218
Q std: 20.740389
Actor loss: 62.385204
Action reg: 0.003985
  l1.weight: grad_norm = 0.018530
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.057361
Total gradient norm: 0.133059
=== Actor Training Debug (Iteration 4633) ===
Q mean: -61.092392
Q std: 22.622988
Actor loss: 61.096371
Action reg: 0.003980
  l1.weight: grad_norm = 0.075473
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.182318
Total gradient norm: 0.287775
=== Actor Training Debug (Iteration 4634) ===
Q mean: -63.925400
Q std: 21.106039
Actor loss: 63.929386
Action reg: 0.003987
  l1.weight: grad_norm = 0.084128
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.198560
Total gradient norm: 0.433565
=== Actor Training Debug (Iteration 4635) ===
Q mean: -64.186760
Q std: 21.299570
Actor loss: 64.190742
Action reg: 0.003982
  l1.weight: grad_norm = 0.167592
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.375586
Total gradient norm: 0.797707
=== Actor Training Debug (Iteration 4636) ===
Q mean: -65.211685
Q std: 21.222538
Actor loss: 65.215668
Action reg: 0.003983
  l1.weight: grad_norm = 0.234157
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.648795
Total gradient norm: 1.422829
=== Actor Training Debug (Iteration 4637) ===
Q mean: -63.442146
Q std: 20.913710
Actor loss: 63.446136
Action reg: 0.003991
  l1.weight: grad_norm = 0.010281
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.026212
Total gradient norm: 0.060168
=== Actor Training Debug (Iteration 4638) ===
Q mean: -62.135567
Q std: 21.733562
Actor loss: 62.139561
Action reg: 0.003993
  l1.weight: grad_norm = 0.001429
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.003416
Total gradient norm: 0.005219
=== Actor Training Debug (Iteration 4639) ===
Q mean: -66.291153
Q std: 20.728388
Actor loss: 66.295143
Action reg: 0.003990
  l1.weight: grad_norm = 0.064381
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.162292
Total gradient norm: 0.331251
=== Actor Training Debug (Iteration 4640) ===
Q mean: -64.067253
Q std: 21.369326
Actor loss: 64.071236
Action reg: 0.003985
  l1.weight: grad_norm = 0.086747
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.212354
Total gradient norm: 0.366851
=== Actor Training Debug (Iteration 4641) ===
Q mean: -63.034401
Q std: 20.938021
Actor loss: 63.038391
Action reg: 0.003990
  l1.weight: grad_norm = 0.253252
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.570326
Total gradient norm: 1.109286
=== Actor Training Debug (Iteration 4642) ===
Q mean: -61.970036
Q std: 21.812193
Actor loss: 61.974022
Action reg: 0.003985
  l1.weight: grad_norm = 0.026169
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.070064
Total gradient norm: 0.144348
=== Actor Training Debug (Iteration 4643) ===
Q mean: -64.980942
Q std: 20.252193
Actor loss: 64.984940
Action reg: 0.003995
  l1.weight: grad_norm = 0.049370
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.137373
Total gradient norm: 0.303014
=== Actor Training Debug (Iteration 4644) ===
Q mean: -66.964005
Q std: 21.340120
Actor loss: 66.967987
Action reg: 0.003983
  l1.weight: grad_norm = 0.021534
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.051143
Total gradient norm: 0.085532
=== Actor Training Debug (Iteration 4645) ===
Q mean: -63.142349
Q std: 21.474911
Actor loss: 63.146336
Action reg: 0.003988
  l1.weight: grad_norm = 0.164398
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.342705
Total gradient norm: 0.616571
=== Actor Training Debug (Iteration 4646) ===
Q mean: -63.361725
Q std: 19.158640
Actor loss: 63.365719
Action reg: 0.003994
  l1.weight: grad_norm = 0.032936
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.095022
Total gradient norm: 0.221449
=== Actor Training Debug (Iteration 4647) ===
Q mean: -65.170486
Q std: 21.748243
Actor loss: 65.174469
Action reg: 0.003980
  l1.weight: grad_norm = 0.029111
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.070236
Total gradient norm: 0.136026
=== Actor Training Debug (Iteration 4648) ===
Q mean: -65.367508
Q std: 21.094290
Actor loss: 65.371498
Action reg: 0.003991
  l1.weight: grad_norm = 0.433030
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 1.246298
Total gradient norm: 2.227848
=== Actor Training Debug (Iteration 4649) ===
Q mean: -64.929718
Q std: 21.212019
Actor loss: 64.933708
Action reg: 0.003987
  l1.weight: grad_norm = 0.190343
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.387634
Total gradient norm: 0.712499
=== Actor Training Debug (Iteration 4650) ===
Q mean: -60.404152
Q std: 20.827475
Actor loss: 60.408142
Action reg: 0.003991
  l1.weight: grad_norm = 0.170951
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.501618
Total gradient norm: 1.072169
=== Actor Training Debug (Iteration 4651) ===
Q mean: -63.651134
Q std: 22.226812
Actor loss: 63.655113
Action reg: 0.003978
  l1.weight: grad_norm = 0.064920
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.157141
Total gradient norm: 0.324077
=== Actor Training Debug (Iteration 4652) ===
Q mean: -65.514099
Q std: 22.264191
Actor loss: 65.518089
Action reg: 0.003991
  l1.weight: grad_norm = 0.001694
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.003979
Total gradient norm: 0.007158
=== Actor Training Debug (Iteration 4653) ===
Q mean: -65.803200
Q std: 22.055668
Actor loss: 65.807190
Action reg: 0.003989
  l1.weight: grad_norm = 0.006509
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.015286
Total gradient norm: 0.035363
=== Actor Training Debug (Iteration 4654) ===
Q mean: -64.646088
Q std: 21.947390
Actor loss: 64.650078
Action reg: 0.003989
  l1.weight: grad_norm = 0.004829
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.011690
Total gradient norm: 0.022285
=== Actor Training Debug (Iteration 4655) ===
Q mean: -61.801018
Q std: 20.738882
Actor loss: 61.804996
Action reg: 0.003978
  l1.weight: grad_norm = 0.178193
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.550666
Total gradient norm: 0.994371
=== Actor Training Debug (Iteration 4656) ===
Q mean: -66.212036
Q std: 20.368784
Actor loss: 66.216026
Action reg: 0.003991
  l1.weight: grad_norm = 0.023693
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.062106
Total gradient norm: 0.108425
=== Actor Training Debug (Iteration 4657) ===
Q mean: -65.949188
Q std: 19.779884
Actor loss: 65.953178
Action reg: 0.003991
  l1.weight: grad_norm = 0.078611
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.161094
Total gradient norm: 0.319194
=== Actor Training Debug (Iteration 4658) ===
Q mean: -61.376251
Q std: 20.870705
Actor loss: 61.380238
Action reg: 0.003987
  l1.weight: grad_norm = 0.064489
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.157234
Total gradient norm: 0.304991
=== Actor Training Debug (Iteration 4659) ===
Q mean: -62.963951
Q std: 21.940054
Actor loss: 62.967930
Action reg: 0.003977
  l1.weight: grad_norm = 0.307220
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.684119
Total gradient norm: 1.356704
=== Actor Training Debug (Iteration 4660) ===
Q mean: -65.268768
Q std: 21.700363
Actor loss: 65.272758
Action reg: 0.003989
  l1.weight: grad_norm = 0.136578
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.394435
Total gradient norm: 0.838648
=== Actor Training Debug (Iteration 4661) ===
Q mean: -66.159874
Q std: 20.684986
Actor loss: 66.163864
Action reg: 0.003994
  l1.weight: grad_norm = 0.076819
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.171431
Total gradient norm: 0.320452
=== Actor Training Debug (Iteration 4662) ===
Q mean: -63.555511
Q std: 21.551365
Actor loss: 63.559502
Action reg: 0.003989
  l1.weight: grad_norm = 0.057574
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.132934
Total gradient norm: 0.246439
=== Actor Training Debug (Iteration 4663) ===
Q mean: -61.903851
Q std: 21.620014
Actor loss: 61.907833
Action reg: 0.003983
  l1.weight: grad_norm = 0.105558
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.271703
Total gradient norm: 0.510527
=== Actor Training Debug (Iteration 4664) ===
Q mean: -63.205765
Q std: 23.176977
Actor loss: 63.209736
Action reg: 0.003969
  l1.weight: grad_norm = 0.128520
  l1.bias: grad_norm = 0.000578
  l2.weight: grad_norm = 0.237551
Total gradient norm: 0.379730
=== Actor Training Debug (Iteration 4665) ===
Q mean: -64.501419
Q std: 21.920017
Actor loss: 64.505409
Action reg: 0.003987
  l1.weight: grad_norm = 0.079781
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.161468
Total gradient norm: 0.307988
=== Actor Training Debug (Iteration 4666) ===
Q mean: -63.662956
Q std: 21.657291
Actor loss: 63.666939
Action reg: 0.003984
  l1.weight: grad_norm = 0.233549
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.519596
Total gradient norm: 1.052860
=== Actor Training Debug (Iteration 4667) ===
Q mean: -63.177685
Q std: 21.694216
Actor loss: 63.181667
Action reg: 0.003983
  l1.weight: grad_norm = 0.099332
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.210366
Total gradient norm: 0.441096
=== Actor Training Debug (Iteration 4668) ===
Q mean: -64.290848
Q std: 21.032717
Actor loss: 64.294830
Action reg: 0.003984
  l1.weight: grad_norm = 0.161806
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.457457
Total gradient norm: 1.018392
=== Actor Training Debug (Iteration 4669) ===
Q mean: -64.298523
Q std: 23.197289
Actor loss: 64.302505
Action reg: 0.003979
  l1.weight: grad_norm = 0.035573
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.073406
Total gradient norm: 0.145867
=== Actor Training Debug (Iteration 4670) ===
Q mean: -63.647057
Q std: 22.267370
Actor loss: 63.651031
Action reg: 0.003976
  l1.weight: grad_norm = 0.086345
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.201653
Total gradient norm: 0.319514
=== Actor Training Debug (Iteration 4671) ===
Q mean: -63.912281
Q std: 22.694607
Actor loss: 63.916267
Action reg: 0.003985
  l1.weight: grad_norm = 0.217104
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.409313
Total gradient norm: 0.808940
=== Actor Training Debug (Iteration 4672) ===
Q mean: -65.526413
Q std: 21.871704
Actor loss: 65.530396
Action reg: 0.003981
  l1.weight: grad_norm = 0.097848
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.249285
Total gradient norm: 0.431161
=== Actor Training Debug (Iteration 4673) ===
Q mean: -62.740688
Q std: 21.167274
Actor loss: 62.744671
Action reg: 0.003983
  l1.weight: grad_norm = 0.116445
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.240281
Total gradient norm: 0.421169
=== Actor Training Debug (Iteration 4674) ===
Q mean: -64.679237
Q std: 21.972374
Actor loss: 64.683220
Action reg: 0.003984
  l1.weight: grad_norm = 0.253532
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.573705
Total gradient norm: 1.078769
=== Actor Training Debug (Iteration 4675) ===
Q mean: -66.560501
Q std: 22.760357
Actor loss: 66.564484
Action reg: 0.003979
  l1.weight: grad_norm = 0.189626
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.442821
Total gradient norm: 0.720969
=== Actor Training Debug (Iteration 4676) ===
Q mean: -64.006157
Q std: 21.626093
Actor loss: 64.010132
Action reg: 0.003977
  l1.weight: grad_norm = 0.002060
  l1.bias: grad_norm = 0.000553
  l2.weight: grad_norm = 0.007245
Total gradient norm: 0.022551
=== Actor Training Debug (Iteration 4677) ===
Q mean: -61.307724
Q std: 19.127831
Actor loss: 61.311718
Action reg: 0.003993
  l1.weight: grad_norm = 0.028281
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.060981
Total gradient norm: 0.124820
=== Actor Training Debug (Iteration 4678) ===
Q mean: -60.319950
Q std: 21.876808
Actor loss: 60.323940
Action reg: 0.003992
  l1.weight: grad_norm = 0.028491
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.068109
Total gradient norm: 0.148719
=== Actor Training Debug (Iteration 4679) ===
Q mean: -64.611481
Q std: 20.588020
Actor loss: 64.615463
Action reg: 0.003985
  l1.weight: grad_norm = 0.061063
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.132403
Total gradient norm: 0.243085
=== Actor Training Debug (Iteration 4680) ===
Q mean: -64.811523
Q std: 22.509216
Actor loss: 64.815506
Action reg: 0.003984
  l1.weight: grad_norm = 0.034968
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.063098
Total gradient norm: 0.114154
=== Actor Training Debug (Iteration 4681) ===
Q mean: -65.658348
Q std: 21.441561
Actor loss: 65.662338
Action reg: 0.003988
  l1.weight: grad_norm = 0.104456
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.311558
Total gradient norm: 0.684571
=== Actor Training Debug (Iteration 4682) ===
Q mean: -64.142059
Q std: 21.575861
Actor loss: 64.146049
Action reg: 0.003994
  l1.weight: grad_norm = 0.037219
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.076266
Total gradient norm: 0.141439
=== Actor Training Debug (Iteration 4683) ===
Q mean: -62.108158
Q std: 21.313271
Actor loss: 62.112148
Action reg: 0.003992
  l1.weight: grad_norm = 0.018973
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.048946
Total gradient norm: 0.101984
=== Actor Training Debug (Iteration 4684) ===
Q mean: -61.557724
Q std: 22.079351
Actor loss: 61.561714
Action reg: 0.003990
  l1.weight: grad_norm = 0.018331
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.043045
Total gradient norm: 0.088788
=== Actor Training Debug (Iteration 4685) ===
Q mean: -64.030121
Q std: 20.176794
Actor loss: 64.034119
Action reg: 0.003995
  l1.weight: grad_norm = 0.124764
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.284480
Total gradient norm: 0.535293
=== Actor Training Debug (Iteration 4686) ===
Q mean: -65.181435
Q std: 23.630747
Actor loss: 65.185410
Action reg: 0.003979
  l1.weight: grad_norm = 0.065433
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.153332
Total gradient norm: 0.253641
=== Actor Training Debug (Iteration 4687) ===
Q mean: -67.311081
Q std: 21.974077
Actor loss: 67.315079
Action reg: 0.003995
  l1.weight: grad_norm = 0.014629
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.035639
Total gradient norm: 0.063134
=== Actor Training Debug (Iteration 4688) ===
Q mean: -60.842697
Q std: 21.613085
Actor loss: 60.846672
Action reg: 0.003973
  l1.weight: grad_norm = 0.196957
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.432642
Total gradient norm: 0.749413
=== Actor Training Debug (Iteration 4689) ===
Q mean: -61.887886
Q std: 22.658785
Actor loss: 61.891861
Action reg: 0.003976
  l1.weight: grad_norm = 0.111071
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.266495
Total gradient norm: 0.475462
=== Actor Training Debug (Iteration 4690) ===
Q mean: -60.473530
Q std: 21.662601
Actor loss: 60.477516
Action reg: 0.003987
  l1.weight: grad_norm = 0.047179
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.128680
Total gradient norm: 0.253934
=== Actor Training Debug (Iteration 4691) ===
Q mean: -62.671387
Q std: 21.852974
Actor loss: 62.675373
Action reg: 0.003985
  l1.weight: grad_norm = 0.019760
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.058543
Total gradient norm: 0.119071
=== Actor Training Debug (Iteration 4692) ===
Q mean: -65.523468
Q std: 21.294092
Actor loss: 65.527451
Action reg: 0.003983
  l1.weight: grad_norm = 0.127376
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.279971
Total gradient norm: 0.461713
=== Actor Training Debug (Iteration 4693) ===
Q mean: -65.081635
Q std: 22.864630
Actor loss: 65.085617
Action reg: 0.003982
  l1.weight: grad_norm = 0.055007
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.161260
Total gradient norm: 0.315482
=== Actor Training Debug (Iteration 4694) ===
Q mean: -66.497009
Q std: 21.951982
Actor loss: 66.500999
Action reg: 0.003993
  l1.weight: grad_norm = 0.067246
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.175914
Total gradient norm: 0.359252
=== Actor Training Debug (Iteration 4695) ===
Q mean: -61.555771
Q std: 22.467716
Actor loss: 61.559753
Action reg: 0.003982
  l1.weight: grad_norm = 0.057784
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.161045
Total gradient norm: 0.389399
=== Actor Training Debug (Iteration 4696) ===
Q mean: -63.239822
Q std: 22.677631
Actor loss: 63.243805
Action reg: 0.003983
  l1.weight: grad_norm = 0.161484
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.316007
Total gradient norm: 0.561275
=== Actor Training Debug (Iteration 4697) ===
Q mean: -63.761726
Q std: 21.036934
Actor loss: 63.765713
Action reg: 0.003986
  l1.weight: grad_norm = 0.156178
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.322532
Total gradient norm: 0.540759
=== Actor Training Debug (Iteration 4698) ===
Q mean: -66.405792
Q std: 21.295813
Actor loss: 66.409782
Action reg: 0.003990
  l1.weight: grad_norm = 0.077394
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.183726
Total gradient norm: 0.339551
=== Actor Training Debug (Iteration 4699) ===
Q mean: -69.282059
Q std: 21.838411
Actor loss: 69.286057
Action reg: 0.003999
  l1.weight: grad_norm = 0.049599
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.100576
Total gradient norm: 0.184227
=== Actor Training Debug (Iteration 4700) ===
Q mean: -64.831635
Q std: 22.302763
Actor loss: 64.835617
Action reg: 0.003985
  l1.weight: grad_norm = 0.125755
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.243751
Total gradient norm: 0.437911
=== Actor Training Debug (Iteration 4701) ===
Q mean: -62.754848
Q std: 22.141754
Actor loss: 62.758831
Action reg: 0.003983
  l1.weight: grad_norm = 0.121740
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.317955
Total gradient norm: 0.614806
=== Actor Training Debug (Iteration 4702) ===
Q mean: -62.414940
Q std: 21.004045
Actor loss: 62.418926
Action reg: 0.003986
  l1.weight: grad_norm = 0.037326
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.082892
Total gradient norm: 0.140887
=== Actor Training Debug (Iteration 4703) ===
Q mean: -64.867516
Q std: 20.536861
Actor loss: 64.871506
Action reg: 0.003987
  l1.weight: grad_norm = 0.027368
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.074765
Total gradient norm: 0.144167
=== Actor Training Debug (Iteration 4704) ===
Q mean: -62.971687
Q std: 22.571970
Actor loss: 62.975670
Action reg: 0.003984
  l1.weight: grad_norm = 0.013025
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.037056
Total gradient norm: 0.069439
=== Actor Training Debug (Iteration 4705) ===
Q mean: -66.203705
Q std: 21.311899
Actor loss: 66.207695
Action reg: 0.003993
  l1.weight: grad_norm = 0.035786
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.096182
Total gradient norm: 0.191446
=== Actor Training Debug (Iteration 4706) ===
Q mean: -62.423660
Q std: 20.602064
Actor loss: 62.427654
Action reg: 0.003993
  l1.weight: grad_norm = 0.030608
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.070116
Total gradient norm: 0.146421
=== Actor Training Debug (Iteration 4707) ===
Q mean: -64.122437
Q std: 21.626694
Actor loss: 64.126427
Action reg: 0.003988
  l1.weight: grad_norm = 0.046096
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.080255
Total gradient norm: 0.140469
=== Actor Training Debug (Iteration 4708) ===
Q mean: -64.163086
Q std: 23.855110
Actor loss: 64.167068
Action reg: 0.003984
  l1.weight: grad_norm = 0.169905
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.297673
Total gradient norm: 0.498163
=== Actor Training Debug (Iteration 4709) ===
Q mean: -66.841263
Q std: 21.404896
Actor loss: 66.845253
Action reg: 0.003987
  l1.weight: grad_norm = 0.103087
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.269043
Total gradient norm: 0.536223
=== Actor Training Debug (Iteration 4710) ===
Q mean: -67.237503
Q std: 21.268869
Actor loss: 67.241493
Action reg: 0.003988
  l1.weight: grad_norm = 0.161295
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.343976
Total gradient norm: 0.681992
=== Actor Training Debug (Iteration 4711) ===
Q mean: -63.714924
Q std: 20.572180
Actor loss: 63.718914
Action reg: 0.003990
  l1.weight: grad_norm = 0.023472
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.052972
Total gradient norm: 0.097265
=== Actor Training Debug (Iteration 4712) ===
Q mean: -65.359779
Q std: 21.185982
Actor loss: 65.363754
Action reg: 0.003979
  l1.weight: grad_norm = 0.010688
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.026421
Total gradient norm: 0.050080
=== Actor Training Debug (Iteration 4713) ===
Q mean: -65.379242
Q std: 19.960526
Actor loss: 65.383232
Action reg: 0.003991
  l1.weight: grad_norm = 0.177731
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.515192
Total gradient norm: 1.065190
=== Actor Training Debug (Iteration 4714) ===
Q mean: -60.977013
Q std: 22.495338
Actor loss: 60.980991
Action reg: 0.003980
  l1.weight: grad_norm = 0.047012
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.101680
Total gradient norm: 0.211477
=== Actor Training Debug (Iteration 4715) ===
Q mean: -64.456909
Q std: 20.141788
Actor loss: 64.460884
Action reg: 0.003973
  l1.weight: grad_norm = 0.234201
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.678064
Total gradient norm: 1.498748
=== Actor Training Debug (Iteration 4716) ===
Q mean: -63.228893
Q std: 22.722757
Actor loss: 63.232872
Action reg: 0.003978
  l1.weight: grad_norm = 0.066723
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.131334
Total gradient norm: 0.216129
=== Actor Training Debug (Iteration 4717) ===
Q mean: -65.734474
Q std: 20.787766
Actor loss: 65.738457
Action reg: 0.003984
  l1.weight: grad_norm = 0.061599
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.164775
Total gradient norm: 0.319403
=== Actor Training Debug (Iteration 4718) ===
Q mean: -66.949692
Q std: 21.192200
Actor loss: 66.953682
Action reg: 0.003989
  l1.weight: grad_norm = 0.005895
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.013014
Total gradient norm: 0.028873
=== Actor Training Debug (Iteration 4719) ===
Q mean: -64.040588
Q std: 21.890139
Actor loss: 64.044579
Action reg: 0.003988
  l1.weight: grad_norm = 0.071400
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.152002
Total gradient norm: 0.340390
=== Actor Training Debug (Iteration 4720) ===
Q mean: -63.130821
Q std: 21.190140
Actor loss: 63.134811
Action reg: 0.003991
  l1.weight: grad_norm = 0.152151
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.401915
Total gradient norm: 0.790228
=== Actor Training Debug (Iteration 4721) ===
Q mean: -65.369232
Q std: 22.377876
Actor loss: 65.373222
Action reg: 0.003987
  l1.weight: grad_norm = 0.041280
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.116062
Total gradient norm: 0.237449
=== Actor Training Debug (Iteration 4722) ===
Q mean: -66.798889
Q std: 22.676321
Actor loss: 66.802879
Action reg: 0.003987
  l1.weight: grad_norm = 0.010339
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.024328
Total gradient norm: 0.051707
=== Actor Training Debug (Iteration 4723) ===
Q mean: -65.613358
Q std: 22.686222
Actor loss: 65.617348
Action reg: 0.003988
  l1.weight: grad_norm = 0.093241
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.169062
Total gradient norm: 0.389122
=== Actor Training Debug (Iteration 4724) ===
Q mean: -63.347809
Q std: 21.257221
Actor loss: 63.351791
Action reg: 0.003984
  l1.weight: grad_norm = 0.123313
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.256736
Total gradient norm: 0.525900
=== Actor Training Debug (Iteration 4725) ===
Q mean: -64.552368
Q std: 20.627010
Actor loss: 64.556366
Action reg: 0.003998
  l1.weight: grad_norm = 0.006233
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.012071
Total gradient norm: 0.018409
=== Actor Training Debug (Iteration 4726) ===
Q mean: -63.256889
Q std: 23.047611
Actor loss: 63.260864
Action reg: 0.003975
  l1.weight: grad_norm = 0.116726
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.248382
Total gradient norm: 0.490418
=== Actor Training Debug (Iteration 4727) ===
Q mean: -67.456139
Q std: 20.236881
Actor loss: 67.460129
Action reg: 0.003989
  l1.weight: grad_norm = 0.053517
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.163381
Total gradient norm: 0.345588
=== Actor Training Debug (Iteration 4728) ===
Q mean: -67.083725
Q std: 21.198576
Actor loss: 67.087715
Action reg: 0.003988
  l1.weight: grad_norm = 0.041206
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.088301
Total gradient norm: 0.175435
=== Actor Training Debug (Iteration 4729) ===
Q mean: -61.168915
Q std: 21.844288
Actor loss: 61.172897
Action reg: 0.003982
  l1.weight: grad_norm = 0.193572
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.437688
Total gradient norm: 0.829436
=== Actor Training Debug (Iteration 4730) ===
Q mean: -65.070274
Q std: 22.215210
Actor loss: 65.074257
Action reg: 0.003986
  l1.weight: grad_norm = 0.013153
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.034887
Total gradient norm: 0.070418
=== Actor Training Debug (Iteration 4731) ===
Q mean: -65.596680
Q std: 21.728640
Actor loss: 65.600655
Action reg: 0.003977
  l1.weight: grad_norm = 0.099885
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.203542
Total gradient norm: 0.351997
=== Actor Training Debug (Iteration 4732) ===
Q mean: -63.938488
Q std: 21.128483
Actor loss: 63.942482
Action reg: 0.003994
  l1.weight: grad_norm = 0.218347
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.408644
Total gradient norm: 0.779683
=== Actor Training Debug (Iteration 4733) ===
Q mean: -64.516479
Q std: 22.459404
Actor loss: 64.520470
Action reg: 0.003989
  l1.weight: grad_norm = 0.035771
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.087658
Total gradient norm: 0.190853
=== Actor Training Debug (Iteration 4734) ===
Q mean: -65.228310
Q std: 23.041874
Actor loss: 65.232292
Action reg: 0.003986
  l1.weight: grad_norm = 0.045748
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.120419
Total gradient norm: 0.255660
=== Actor Training Debug (Iteration 4735) ===
Q mean: -65.893311
Q std: 22.586418
Actor loss: 65.897293
Action reg: 0.003981
  l1.weight: grad_norm = 0.078437
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.158929
Total gradient norm: 0.330771
=== Actor Training Debug (Iteration 4736) ===
Q mean: -64.470535
Q std: 21.492111
Actor loss: 64.474518
Action reg: 0.003983
  l1.weight: grad_norm = 0.008125
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.023216
Total gradient norm: 0.046710
=== Actor Training Debug (Iteration 4737) ===
Q mean: -66.188492
Q std: 21.849619
Actor loss: 66.192474
Action reg: 0.003980
  l1.weight: grad_norm = 0.063873
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.159248
Total gradient norm: 0.325849
=== Actor Training Debug (Iteration 4738) ===
Q mean: -66.990463
Q std: 21.492979
Actor loss: 66.994453
Action reg: 0.003993
  l1.weight: grad_norm = 0.016015
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.030548
Total gradient norm: 0.050689
=== Actor Training Debug (Iteration 4739) ===
Q mean: -60.006447
Q std: 23.586281
Actor loss: 60.010414
Action reg: 0.003965
  l1.weight: grad_norm = 0.024732
  l1.bias: grad_norm = 0.000687
  l2.weight: grad_norm = 0.054201
Total gradient norm: 0.092236
=== Actor Training Debug (Iteration 4740) ===
Q mean: -66.845062
Q std: 21.804184
Actor loss: 66.849052
Action reg: 0.003991
  l1.weight: grad_norm = 0.148209
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.312226
Total gradient norm: 0.542880
=== Actor Training Debug (Iteration 4741) ===
Q mean: -66.201591
Q std: 21.333307
Actor loss: 66.205574
Action reg: 0.003984
  l1.weight: grad_norm = 0.166692
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.340027
Total gradient norm: 0.709284
=== Actor Training Debug (Iteration 4742) ===
Q mean: -64.145676
Q std: 22.173504
Actor loss: 64.149658
Action reg: 0.003983
  l1.weight: grad_norm = 0.145765
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.273141
Total gradient norm: 0.617769
=== Actor Training Debug (Iteration 4743) ===
Q mean: -66.504478
Q std: 21.993252
Actor loss: 66.508469
Action reg: 0.003989
  l1.weight: grad_norm = 0.172559
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.351585
Total gradient norm: 0.702496
=== Actor Training Debug (Iteration 4744) ===
Q mean: -63.468590
Q std: 21.338751
Actor loss: 63.472580
Action reg: 0.003990
  l1.weight: grad_norm = 0.069587
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.139712
Total gradient norm: 0.269154
=== Actor Training Debug (Iteration 4745) ===
Q mean: -63.336884
Q std: 23.463823
Actor loss: 63.340858
Action reg: 0.003977
  l1.weight: grad_norm = 0.060464
  l1.bias: grad_norm = 0.000658
  l2.weight: grad_norm = 0.123024
Total gradient norm: 0.277969
Total gradient norm: 0.2557104696on 1203) ===
=== Actor Training Debug (Iteration 4756) ===
Q mean: -62.995850
Q std: 19.894981
Actor loss: 62.999828
Action reg: 0.003980
  l1.weight: grad_norm = 0.073540
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.181615
Total gradient norm: 0.368047
=== Actor Training Debug (Iteration 4757) ===
Q mean: -65.360123
Q std: 22.347921
Actor loss: 65.364105
Action reg: 0.003981
  l1.weight: grad_norm = 0.035413
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.101028
Total gradient norm: 0.253094
=== Actor Training Debug (Iteration 4758) ===
Q mean: -64.345306
Q std: 23.250912
Actor loss: 64.349289
Action reg: 0.003983
  l1.weight: grad_norm = 0.003471
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.011545
Total gradient norm: 0.033880
=== Actor Training Debug (Iteration 4759) ===
Q mean: -64.959213
Q std: 22.870399
Actor loss: 64.963196
Action reg: 0.003983
  l1.weight: grad_norm = 0.013651
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.032766
Total gradient norm: 0.057756
=== Actor Training Debug (Iteration 4760) ===
Q mean: -62.348923
Q std: 23.818489
Actor loss: 62.352898
Action reg: 0.003977
  l1.weight: grad_norm = 0.017794
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.037979
Total gradient norm: 0.067339
=== Actor Training Debug (Iteration 4761) ===
Q mean: -64.091606
Q std: 22.127930
Actor loss: 64.095604
Action reg: 0.003995
  l1.weight: grad_norm = 0.027624
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.073010
Total gradient norm: 0.157657
=== Actor Training Debug (Iteration 4762) ===
Q mean: -67.619049
Q std: 21.129030
Actor loss: 67.623032
Action reg: 0.003984
  l1.weight: grad_norm = 0.088139
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.224180
Total gradient norm: 0.416067
=== Actor Training Debug (Iteration 4763) ===
Q mean: -65.163506
Q std: 21.749840
Actor loss: 65.167480
Action reg: 0.003978
  l1.weight: grad_norm = 0.095713
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.212344
Total gradient norm: 0.369072
=== Actor Training Debug (Iteration 4764) ===
Q mean: -63.631916
Q std: 22.579035
Actor loss: 63.635902
Action reg: 0.003986
  l1.weight: grad_norm = 0.039486
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.112652
Total gradient norm: 0.250217
=== Actor Training Debug (Iteration 4765) ===
Q mean: -64.154510
Q std: 21.877043
Actor loss: 64.158501
Action reg: 0.003993
  l1.weight: grad_norm = 0.025390
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.052384
Total gradient norm: 0.093333
=== Actor Training Debug (Iteration 4766) ===
Q mean: -67.627411
Q std: 22.863276
Actor loss: 67.631409
Action reg: 0.003997
  l1.weight: grad_norm = 0.001559
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.003874
Total gradient norm: 0.007879
=== Actor Training Debug (Iteration 4767) ===
Q mean: -66.873039
Q std: 22.228964
Actor loss: 66.877029
Action reg: 0.003989
  l1.weight: grad_norm = 0.079300
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.152177
Total gradient norm: 0.293506
=== Actor Training Debug (Iteration 4768) ===
Q mean: -64.939377
Q std: 20.578436
Actor loss: 64.943367
Action reg: 0.003993
  l1.weight: grad_norm = 0.010102
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.027770
Total gradient norm: 0.069384
=== Actor Training Debug (Iteration 4769) ===
Q mean: -63.523766
Q std: 22.482126
Actor loss: 63.527737
Action reg: 0.003971
  l1.weight: grad_norm = 0.099621
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.197265
Total gradient norm: 0.410910
=== Actor Training Debug (Iteration 4770) ===
Q mean: -65.087730
Q std: 21.296389
Actor loss: 65.091721
Action reg: 0.003987
  l1.weight: grad_norm = 0.289632
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.609407
Total gradient norm: 1.181414
=== Actor Training Debug (Iteration 4771) ===
Q mean: -63.580368
Q std: 21.341574
Actor loss: 63.584358
Action reg: 0.003989
  l1.weight: grad_norm = 0.061127
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.159546
Total gradient norm: 0.316793
=== Actor Training Debug (Iteration 4772) ===
Q mean: -66.993668
Q std: 21.083775
Actor loss: 66.997650
Action reg: 0.003982
  l1.weight: grad_norm = 0.045127
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.093672
Total gradient norm: 0.167613
=== Actor Training Debug (Iteration 4773) ===
Q mean: -65.100372
Q std: 22.243366
Actor loss: 65.104362
Action reg: 0.003987
  l1.weight: grad_norm = 0.307695
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.708718
Total gradient norm: 1.186960
=== Actor Training Debug (Iteration 4774) ===
Q mean: -64.868896
Q std: 21.136951
Actor loss: 64.872887
Action reg: 0.003990
  l1.weight: grad_norm = 0.129191
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.256079
Total gradient norm: 0.486574
=== Actor Training Debug (Iteration 4775) ===
Q mean: -65.735947
Q std: 23.261868
Actor loss: 65.739929
Action reg: 0.003980
  l1.weight: grad_norm = 0.094676
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.240534
Total gradient norm: 0.438085
=== Actor Training Debug (Iteration 4776) ===
Q mean: -63.489746
Q std: 20.794941
Actor loss: 63.493736
Action reg: 0.003989
  l1.weight: grad_norm = 0.102948
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.262367
Total gradient norm: 0.520033
=== Actor Training Debug (Iteration 4777) ===
Q mean: -65.552094
Q std: 21.186550
Actor loss: 65.556076
Action reg: 0.003982
  l1.weight: grad_norm = 0.258636
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.528889
Total gradient norm: 0.956783
=== Actor Training Debug (Iteration 4778) ===
Q mean: -64.527679
Q std: 22.281868
Actor loss: 64.531662
Action reg: 0.003984
  l1.weight: grad_norm = 0.074591
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.151501
Total gradient norm: 0.300394
=== Actor Training Debug (Iteration 4779) ===
Q mean: -67.280449
Q std: 22.745445
Actor loss: 67.284439
Action reg: 0.003988
  l1.weight: grad_norm = 0.058996
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.125428
Total gradient norm: 0.242591
=== Actor Training Debug (Iteration 4780) ===
Q mean: -66.368248
Q std: 20.485743
Actor loss: 66.372231
Action reg: 0.003985
  l1.weight: grad_norm = 0.168821
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.377496
Total gradient norm: 0.667496
=== Actor Training Debug (Iteration 4781) ===
Q mean: -65.803650
Q std: 21.467876
Actor loss: 65.807640
Action reg: 0.003990
  l1.weight: grad_norm = 0.105870
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.246467
Total gradient norm: 0.445753
=== Actor Training Debug (Iteration 4782) ===
Q mean: -67.052200
Q std: 20.775934
Actor loss: 67.056198
Action reg: 0.003996
  l1.weight: grad_norm = 0.029889
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.084583
Total gradient norm: 0.189984
=== Actor Training Debug (Iteration 4783) ===
Q mean: -66.487366
Q std: 21.378599
Actor loss: 66.491364
Action reg: 0.004000
  l1.weight: grad_norm = 0.013586
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.030505
Total gradient norm: 0.056372
=== Actor Training Debug (Iteration 4784) ===
Q mean: -64.886681
Q std: 21.232733
Actor loss: 64.890671
Action reg: 0.003990
  l1.weight: grad_norm = 0.234401
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.496278
Total gradient norm: 0.869307
=== Actor Training Debug (Iteration 4785) ===
Q mean: -65.209976
Q std: 21.012739
Actor loss: 65.213966
Action reg: 0.003989
  l1.weight: grad_norm = 0.045906
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.096763
Total gradient norm: 0.191863
=== Actor Training Debug (Iteration 4786) ===
Q mean: -66.310318
Q std: 23.165043
Actor loss: 66.314301
Action reg: 0.003979
  l1.weight: grad_norm = 0.256282
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.533468
Total gradient norm: 1.029184
=== Actor Training Debug (Iteration 4787) ===
Q mean: -65.404541
Q std: 21.541828
Actor loss: 65.408524
Action reg: 0.003982
  l1.weight: grad_norm = 0.148011
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.329773
Total gradient norm: 0.493381
=== Actor Training Debug (Iteration 4788) ===
Q mean: -65.428635
Q std: 22.986689
Actor loss: 65.432617
Action reg: 0.003981
  l1.weight: grad_norm = 0.075137
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.214111
Total gradient norm: 0.395097
=== Actor Training Debug (Iteration 4789) ===
Q mean: -66.705040
Q std: 22.323580
Actor loss: 66.709023
Action reg: 0.003984
  l1.weight: grad_norm = 0.035867
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.080164
Total gradient norm: 0.119845
=== Actor Training Debug (Iteration 4790) ===
Q mean: -64.367508
Q std: 21.718077
Actor loss: 64.371490
Action reg: 0.003986
  l1.weight: grad_norm = 0.030666
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.086969
Total gradient norm: 0.170784
=== Actor Training Debug (Iteration 4791) ===
Q mean: -62.404678
Q std: 21.085033
Actor loss: 62.408653
Action reg: 0.003977
  l1.weight: grad_norm = 0.275319
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.563240
Total gradient norm: 0.947456
=== Actor Training Debug (Iteration 4792) ===
Q mean: -65.029709
Q std: 21.929735
Actor loss: 65.033707
Action reg: 0.003996
  l1.weight: grad_norm = 0.083376
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.191067
Total gradient norm: 0.343106
=== Actor Training Debug (Iteration 4793) ===
Q mean: -66.871506
Q std: 23.447971
Actor loss: 66.875488
Action reg: 0.003983
  l1.weight: grad_norm = 0.103427
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.217276
Total gradient norm: 0.403546
=== Actor Training Debug (Iteration 4794) ===
Q mean: -68.967743
Q std: 22.069897
Actor loss: 68.971741
Action reg: 0.003995
  l1.weight: grad_norm = 0.057402
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.120002
Total gradient norm: 0.202776
=== Actor Training Debug (Iteration 4795) ===
Q mean: -64.151054
Q std: 21.662073
Actor loss: 64.155045
Action reg: 0.003990
  l1.weight: grad_norm = 0.067469
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.135445
Total gradient norm: 0.250059
=== Actor Training Debug (Iteration 4796) ===
Q mean: -66.040131
Q std: 21.154755
Actor loss: 66.044121
Action reg: 0.003987
  l1.weight: grad_norm = 0.105211
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.327315
Total gradient norm: 0.779061
=== Actor Training Debug (Iteration 4797) ===
Q mean: -66.170197
Q std: 20.902607
Actor loss: 66.174179
Action reg: 0.003984
  l1.weight: grad_norm = 0.134254
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.315990
Total gradient norm: 0.591033
=== Actor Training Debug (Iteration 4798) ===
Q mean: -65.697807
Q std: 22.096569
Actor loss: 65.701782
Action reg: 0.003975
  l1.weight: grad_norm = 0.069168
  l1.bias: grad_norm = 0.000505
  l2.weight: grad_norm = 0.169313
Total gradient norm: 0.341150
=== Actor Training Debug (Iteration 4799) ===
Q mean: -66.022354
Q std: 21.300488
Actor loss: 66.026352
Action reg: 0.003996
  l1.weight: grad_norm = 0.000993
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.002888
Total gradient norm: 0.007606
=== Actor Training Debug (Iteration 4800) ===
Q mean: -64.679947
Q std: 22.338095
Actor loss: 64.683929
Action reg: 0.003986
  l1.weight: grad_norm = 0.093953
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.195826
Total gradient norm: 0.337602
=== Actor Training Debug (Iteration 4801) ===
Q mean: -66.764793
Q std: 20.532618
Actor loss: 66.768791
Action reg: 0.003999
  l1.weight: grad_norm = 0.012415
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.039752
Total gradient norm: 0.087532
=== Actor Training Debug (Iteration 4802) ===
Q mean: -65.118309
Q std: 23.232500
Actor loss: 65.122292
Action reg: 0.003981
  l1.weight: grad_norm = 0.017153
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.043648
Total gradient norm: 0.069383
=== Actor Training Debug (Iteration 4803) ===
Q mean: -64.881012
Q std: 21.677794
Actor loss: 64.884987
Action reg: 0.003977
  l1.weight: grad_norm = 0.133485
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.360396
Total gradient norm: 0.666813
=== Actor Training Debug (Iteration 4804) ===
Q mean: -65.895935
Q std: 22.291965
Actor loss: 65.899910
Action reg: 0.003974
  l1.weight: grad_norm = 0.068436
  l1.bias: grad_norm = 0.000669
  l2.weight: grad_norm = 0.186091
Total gradient norm: 0.377105
=== Actor Training Debug (Iteration 4805) ===
Q mean: -66.165276
Q std: 22.368355
Actor loss: 66.169258
Action reg: 0.003986
  l1.weight: grad_norm = 0.001987
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.005320
Total gradient norm: 0.010982
=== Actor Training Debug (Iteration 4806) ===
Q mean: -63.645378
Q std: 20.256371
Actor loss: 63.649368
Action reg: 0.003990
  l1.weight: grad_norm = 0.230590
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.439312
Total gradient norm: 0.734733
=== Actor Training Debug (Iteration 4807) ===
Q mean: -65.041435
Q std: 21.117044
Actor loss: 65.045425
Action reg: 0.003989
  l1.weight: grad_norm = 0.000653
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.003593
Total gradient norm: 0.010607
=== Actor Training Debug (Iteration 4808) ===
Q mean: -66.543732
Q std: 21.640423
Actor loss: 66.547722
Action reg: 0.003988
  l1.weight: grad_norm = 0.018615
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.043742
Total gradient norm: 0.077060
=== Actor Training Debug (Iteration 4809) ===
Q mean: -65.761391
Q std: 21.181984
Actor loss: 65.765373
Action reg: 0.003985
  l1.weight: grad_norm = 0.122661
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.264674
Total gradient norm: 0.498067
=== Actor Training Debug (Iteration 4810) ===
Q mean: -64.678101
Q std: 20.651476
Actor loss: 64.682098
Action reg: 0.003995
  l1.weight: grad_norm = 0.158853
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.369817
Total gradient norm: 0.832794
=== Actor Training Debug (Iteration 4811) ===
Q mean: -62.928852
Q std: 22.127306
Actor loss: 62.932831
Action reg: 0.003977
  l1.weight: grad_norm = 0.244163
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.597756
Total gradient norm: 1.215935
=== Actor Training Debug (Iteration 4812) ===
Q mean: -64.654556
Q std: 23.483406
Actor loss: 64.658539
Action reg: 0.003986
  l1.weight: grad_norm = 0.040433
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.085806
Total gradient norm: 0.168456
=== Actor Training Debug (Iteration 4813) ===
Q mean: -66.819427
Q std: 22.283989
Actor loss: 66.823418
Action reg: 0.003994
  l1.weight: grad_norm = 0.017573
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.033887
Total gradient norm: 0.064259
=== Actor Training Debug (Iteration 4814) ===
Q mean: -65.901886
Q std: 20.998533
Actor loss: 65.905869
Action reg: 0.003985
  l1.weight: grad_norm = 0.159566
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.323027
Total gradient norm: 0.578819
=== Actor Training Debug (Iteration 4815) ===
Q mean: -62.979046
Q std: 21.631657
Actor loss: 62.983028
Action reg: 0.003982
  l1.weight: grad_norm = 0.036117
  l1.bias: grad_norm = 0.000552
  l2.weight: grad_norm = 0.081916
Total gradient norm: 0.141237
=== Actor Training Debug (Iteration 4816) ===
Q mean: -67.008759
Q std: 22.871199
Actor loss: 67.012733
Action reg: 0.003977
  l1.weight: grad_norm = 0.088189
  l1.bias: grad_norm = 0.000848
  l2.weight: grad_norm = 0.234895
Total gradient norm: 0.519353
=== Actor Training Debug (Iteration 4817) ===
Q mean: -67.590797
Q std: 22.128149
Actor loss: 67.594780
Action reg: 0.003983
  l1.weight: grad_norm = 0.211450
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.410371
Total gradient norm: 0.836239
=== Actor Training Debug (Iteration 4818) ===
Q mean: -64.421188
Q std: 21.699438
Actor loss: 64.425186
Action reg: 0.003995
  l1.weight: grad_norm = 0.036043
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.102671
Total gradient norm: 0.218092
=== Actor Training Debug (Iteration 4819) ===
Q mean: -65.313293
Q std: 20.446394
Actor loss: 65.317284
Action reg: 0.003993
  l1.weight: grad_norm = 0.050855
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.138507
Total gradient norm: 0.277334
=== Actor Training Debug (Iteration 4820) ===
Q mean: -67.741669
Q std: 21.504129
Actor loss: 67.745659
Action reg: 0.003988
  l1.weight: grad_norm = 0.012377
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.023676
Total gradient norm: 0.033737
=== Actor Training Debug (Iteration 4821) ===
Q mean: -64.291580
Q std: 21.007853
Actor loss: 64.295555
Action reg: 0.003979
  l1.weight: grad_norm = 0.166256
  l1.bias: grad_norm = 0.000820
  l2.weight: grad_norm = 0.453521
Total gradient norm: 0.872602
=== Actor Training Debug (Iteration 4822) ===
Q mean: -64.373718
Q std: 22.462337
Actor loss: 64.377708
Action reg: 0.003989
  l1.weight: grad_norm = 0.121567
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.244578
Total gradient norm: 0.457195
=== Actor Training Debug (Iteration 4823) ===
Q mean: -63.912315
Q std: 21.754568
Actor loss: 63.916294
Action reg: 0.003978
  l1.weight: grad_norm = 0.132665
  l1.bias: grad_norm = 0.000591
  l2.weight: grad_norm = 0.261950
Total gradient norm: 0.515537
=== Actor Training Debug (Iteration 4824) ===
Q mean: -64.650749
Q std: 22.487856
Actor loss: 64.654732
Action reg: 0.003984
  l1.weight: grad_norm = 0.033806
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.094422
Total gradient norm: 0.162359
=== Actor Training Debug (Iteration 4825) ===
Q mean: -68.662659
Q std: 22.534420
Actor loss: 68.666649
Action reg: 0.003992
  l1.weight: grad_norm = 0.034398
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.066167
Total gradient norm: 0.114768
=== Actor Training Debug (Iteration 4826) ===
Q mean: -63.549679
Q std: 22.222198
Actor loss: 63.553665
Action reg: 0.003986
  l1.weight: grad_norm = 0.046557
  l1.bias: grad_norm = 0.000552
  l2.weight: grad_norm = 0.138722
Total gradient norm: 0.311241
=== Actor Training Debug (Iteration 4827) ===
Q mean: -62.261829
Q std: 23.873201
Actor loss: 62.265804
Action reg: 0.003973
  l1.weight: grad_norm = 0.025004
  l1.bias: grad_norm = 0.000885
  l2.weight: grad_norm = 0.054082
Total gradient norm: 0.106926
=== Actor Training Debug (Iteration 4828) ===
Q mean: -66.244301
Q std: 22.643232
Actor loss: 66.248276
Action reg: 0.003977
  l1.weight: grad_norm = 0.030359
  l1.bias: grad_norm = 0.000945
  l2.weight: grad_norm = 0.070497
Total gradient norm: 0.125834
=== Actor Training Debug (Iteration 4829) ===
Q mean: -63.188911
Q std: 20.916424
Actor loss: 63.192902
Action reg: 0.003989
  l1.weight: grad_norm = 0.036744
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.108701
Total gradient norm: 0.201419
=== Actor Training Debug (Iteration 4830) ===
Q mean: -65.769890
Q std: 20.584591
Actor loss: 65.773865
Action reg: 0.003973
  l1.weight: grad_norm = 0.008681
  l1.bias: grad_norm = 0.000744
  l2.weight: grad_norm = 0.020093
Total gradient norm: 0.041717
=== Actor Training Debug (Iteration 4831) ===
Q mean: -64.352592
Q std: 21.501390
Actor loss: 64.356583
Action reg: 0.003989
  l1.weight: grad_norm = 0.010206
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.025691
Total gradient norm: 0.062318
=== Actor Training Debug (Iteration 4832) ===
Q mean: -66.830429
Q std: 20.891281
Actor loss: 66.834412
Action reg: 0.003985
  l1.weight: grad_norm = 0.258813
  l1.bias: grad_norm = 0.000405
  l2.weight: grad_norm = 0.599710
Total gradient norm: 1.107503
=== Actor Training Debug (Iteration 4833) ===
Q mean: -66.343086
Q std: 21.515274
Actor loss: 66.347069
Action reg: 0.003979
  l1.weight: grad_norm = 0.056821
  l1.bias: grad_norm = 0.000633
  l2.weight: grad_norm = 0.121381
Total gradient norm: 0.167316
=== Actor Training Debug (Iteration 4834) ===
Q mean: -65.350296
Q std: 22.874174
Actor loss: 65.354279
Action reg: 0.003983
  l1.weight: grad_norm = 0.124482
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.267548
Total gradient norm: 0.662929
=== Actor Training Debug (Iteration 4835) ===
Q mean: -65.229752
Q std: 21.098785
Actor loss: 65.233734
Action reg: 0.003985
  l1.weight: grad_norm = 0.019278
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.056877
Total gradient norm: 0.125976
=== Actor Training Debug (Iteration 4836) ===
Q mean: -67.657333
Q std: 23.837936
Actor loss: 67.661308
Action reg: 0.003974
  l1.weight: grad_norm = 0.001653
  l1.bias: grad_norm = 0.000995
  l2.weight: grad_norm = 0.010088
Total gradient norm: 0.031652
=== Actor Training Debug (Iteration 4837) ===
Q mean: -65.159477
Q std: 22.297344
Actor loss: 65.163467
Action reg: 0.003988
  l1.weight: grad_norm = 0.060655
  l1.bias: grad_norm = 0.000435
  l2.weight: grad_norm = 0.133665
Total gradient norm: 0.283004
=== Actor Training Debug (Iteration 4838) ===
Q mean: -62.896946
Q std: 21.062555
Actor loss: 62.900928
Action reg: 0.003982
  l1.weight: grad_norm = 0.213193
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.378904
Total gradient norm: 0.532169
=== Actor Training Debug (Iteration 4839) ===
Q mean: -66.773590
Q std: 21.154484
Actor loss: 66.777573
Action reg: 0.003983
  l1.weight: grad_norm = 0.053934
  l1.bias: grad_norm = 0.000596
  l2.weight: grad_norm = 0.105012
Total gradient norm: 0.219509
=== Actor Training Debug (Iteration 4840) ===
Q mean: -66.177429
Q std: 21.726646
Actor loss: 66.181412
Action reg: 0.003986
  l1.weight: grad_norm = 0.155717
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.328288
Total gradient norm: 0.569800
=== Actor Training Debug (Iteration 4841) ===
Q mean: -63.956768
Q std: 22.510212
Actor loss: 63.960732
Action reg: 0.003963
  l1.weight: grad_norm = 0.321863
  l1.bias: grad_norm = 0.000775
  l2.weight: grad_norm = 0.918628
Total gradient norm: 1.942152
=== Actor Training Debug (Iteration 4842) ===
Q mean: -67.323547
Q std: 22.400497
Actor loss: 67.327530
Action reg: 0.003985
  l1.weight: grad_norm = 0.036476
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.094249
Total gradient norm: 0.185163
=== Actor Training Debug (Iteration 4843) ===
Q mean: -65.760681
Q std: 21.557835
Actor loss: 65.764671
Action reg: 0.003993
  l1.weight: grad_norm = 0.153496
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.322476
Total gradient norm: 0.645796
=== Actor Training Debug (Iteration 4844) ===
Q mean: -65.100700
Q std: 21.295168
Actor loss: 65.104683
Action reg: 0.003984
  l1.weight: grad_norm = 0.099128
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.255874
Total gradient norm: 0.401327
=== Actor Training Debug (Iteration 4845) ===
Q mean: -65.411751
Q std: 22.328476
Actor loss: 65.415733
Action reg: 0.003985
  l1.weight: grad_norm = 0.139199
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.250416
Total gradient norm: 0.391443
=== Actor Training Debug (Iteration 4846) ===
Q mean: -65.521820
Q std: 22.692179
Actor loss: 65.525795
Action reg: 0.003978
  l1.weight: grad_norm = 0.037963
  l1.bias: grad_norm = 0.000757
  l2.weight: grad_norm = 0.104787
Total gradient norm: 0.237063
=== Actor Training Debug (Iteration 4847) ===
Q mean: -66.768341
Q std: 23.451719
Actor loss: 66.772324
Action reg: 0.003984
  l1.weight: grad_norm = 0.016481
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.041307
Total gradient norm: 0.073035
=== Actor Training Debug (Iteration 4848) ===
Q mean: -65.039200
Q std: 21.040377
Actor loss: 65.043182
Action reg: 0.003984
  l1.weight: grad_norm = 0.097661
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.246076
Total gradient norm: 0.471716
=== Actor Training Debug (Iteration 4849) ===
Q mean: -66.031937
Q std: 20.951113
Actor loss: 66.035919
Action reg: 0.003985
  l1.weight: grad_norm = 0.233052
  l1.bias: grad_norm = 0.000550
  l2.weight: grad_norm = 0.522116
Total gradient norm: 1.083270
=== Actor Training Debug (Iteration 4850) ===
Q mean: -64.862923
Q std: 20.996698
Actor loss: 64.866905
Action reg: 0.003986
  l1.weight: grad_norm = 0.036531
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.094172
Total gradient norm: 0.187356
=== Actor Training Debug (Iteration 4851) ===
Q mean: -68.376511
Q std: 20.598951
Actor loss: 68.380508
Action reg: 0.003994
  l1.weight: grad_norm = 0.019480
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.052133
Total gradient norm: 0.106944
=== Actor Training Debug (Iteration 4852) ===
Q mean: -66.942238
Q std: 22.799618
Actor loss: 66.946220
Action reg: 0.003983
  l1.weight: grad_norm = 0.018252
  l1.bias: grad_norm = 0.000562
  l2.weight: grad_norm = 0.055000
Total gradient norm: 0.131655
=== Actor Training Debug (Iteration 4853) ===
Q mean: -65.236488
Q std: 22.461060
Actor loss: 65.240471
Action reg: 0.003983
  l1.weight: grad_norm = 0.041002
  l1.bias: grad_norm = 0.000685
  l2.weight: grad_norm = 0.102516
Total gradient norm: 0.206371
=== Actor Training Debug (Iteration 4854) ===
Q mean: -64.500137
Q std: 21.754866
Actor loss: 64.504128
Action reg: 0.003993
  l1.weight: grad_norm = 0.038714
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.104903
Total gradient norm: 0.199697
=== Actor Training Debug (Iteration 4855) ===
Q mean: -65.314713
Q std: 23.025421
Actor loss: 65.318703
Action reg: 0.003986
  l1.weight: grad_norm = 0.126067
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.258912
Total gradient norm: 0.477929
=== Actor Training Debug (Iteration 4856) ===
Q mean: -67.105591
Q std: 21.838606
Actor loss: 67.109573
Action reg: 0.003985
  l1.weight: grad_norm = 0.078991
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.156316
Total gradient norm: 0.281550
=== Actor Training Debug (Iteration 4857) ===
Q mean: -64.229248
Q std: 21.691568
Actor loss: 64.233223
Action reg: 0.003974
  l1.weight: grad_norm = 0.124837
  l1.bias: grad_norm = 0.000691
  l2.weight: grad_norm = 0.222183
Total gradient norm: 0.304011
=== Actor Training Debug (Iteration 4858) ===
Q mean: -65.183792
Q std: 21.475471
Actor loss: 65.187767
Action reg: 0.003977
  l1.weight: grad_norm = 0.144267
  l1.bias: grad_norm = 0.000606
  l2.weight: grad_norm = 0.302588
Total gradient norm: 0.614536
=== Actor Training Debug (Iteration 4859) ===
Q mean: -67.646347
Q std: 21.711451
Actor loss: 67.650322
Action reg: 0.003978
  l1.weight: grad_norm = 0.040968
  l1.bias: grad_norm = 0.000939
  l2.weight: grad_norm = 0.093817
Total gradient norm: 0.194205
=== Actor Training Debug (Iteration 4860) ===
Q mean: -66.142876
Q std: 22.302031
Actor loss: 66.146866
Action reg: 0.003993
Action reg: 0.003991 0.2557104696on 1203) ===
  l1.weight: grad_norm = 0.205375
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.562141
Total gradient norm: 1.267697
=== Actor Training Debug (Iteration 4871) ===
Q mean: -62.985031
Q std: 22.867603
Actor loss: 62.989014
Action reg: 0.003983
  l1.weight: grad_norm = 0.006369
  l1.bias: grad_norm = 0.000798
  l2.weight: grad_norm = 0.016733
Total gradient norm: 0.035305
=== Actor Training Debug (Iteration 4872) ===
Q mean: -67.745361
Q std: 21.582712
Actor loss: 67.749352
Action reg: 0.003989
  l1.weight: grad_norm = 0.048828
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.104582
Total gradient norm: 0.182139
=== Actor Training Debug (Iteration 4873) ===
Q mean: -65.694130
Q std: 22.158209
Actor loss: 65.698112
Action reg: 0.003981
  l1.weight: grad_norm = 0.039438
  l1.bias: grad_norm = 0.000910
  l2.weight: grad_norm = 0.102105
Total gradient norm: 0.215120
=== Actor Training Debug (Iteration 4874) ===
Q mean: -64.510635
Q std: 21.164188
Actor loss: 64.514633
Action reg: 0.003998
  l1.weight: grad_norm = 0.015236
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.037238
Total gradient norm: 0.079636
=== Actor Training Debug (Iteration 4875) ===
Q mean: -65.219025
Q std: 22.692944
Actor loss: 65.223000
Action reg: 0.003976
  l1.weight: grad_norm = 0.107381
  l1.bias: grad_norm = 0.001011
  l2.weight: grad_norm = 0.225149
Total gradient norm: 0.469114
=== Actor Training Debug (Iteration 4876) ===
Q mean: -66.635239
Q std: 22.894888
Actor loss: 66.639221
Action reg: 0.003982
  l1.weight: grad_norm = 0.034916
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.097145
Total gradient norm: 0.219941
=== Actor Training Debug (Iteration 4877) ===
Q mean: -65.960678
Q std: 23.545708
Actor loss: 65.964645
Action reg: 0.003969
  l1.weight: grad_norm = 0.027912
  l1.bias: grad_norm = 0.001001
  l2.weight: grad_norm = 0.079671
Total gradient norm: 0.171599
=== Actor Training Debug (Iteration 4878) ===
Q mean: -66.599747
Q std: 21.527866
Actor loss: 66.603737
Action reg: 0.003988
  l1.weight: grad_norm = 0.038189
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.077452
Total gradient norm: 0.149508
=== Actor Training Debug (Iteration 4879) ===
Q mean: -65.968697
Q std: 22.214598
Actor loss: 65.972672
Action reg: 0.003978
  l1.weight: grad_norm = 0.082327
  l1.bias: grad_norm = 0.000726
  l2.weight: grad_norm = 0.204488
Total gradient norm: 0.390218
=== Actor Training Debug (Iteration 4880) ===
Q mean: -66.377594
Q std: 22.664665
Actor loss: 66.381577
Action reg: 0.003984
  l1.weight: grad_norm = 0.086984
  l1.bias: grad_norm = 0.000546
  l2.weight: grad_norm = 0.216375
Total gradient norm: 0.406620
=== Actor Training Debug (Iteration 4881) ===
Q mean: -67.370956
Q std: 21.471804
Actor loss: 67.374939
Action reg: 0.003986
  l1.weight: grad_norm = 0.335409
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.647083
Total gradient norm: 1.420546
=== Actor Training Debug (Iteration 4882) ===
Q mean: -64.765488
Q std: 20.538912
Actor loss: 64.769447
Action reg: 0.003959
  l1.weight: grad_norm = 0.426771
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 1.175762
Total gradient norm: 2.461677
=== Actor Training Debug (Iteration 4883) ===
Q mean: -67.983109
Q std: 19.820332
Actor loss: 67.987106
Action reg: 0.003999
  l1.weight: grad_norm = 0.033558
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.089441
Total gradient norm: 0.169557
=== Actor Training Debug (Iteration 4884) ===
Q mean: -65.065681
Q std: 23.306065
Actor loss: 65.069656
Action reg: 0.003976
  l1.weight: grad_norm = 0.004444
  l1.bias: grad_norm = 0.000891
  l2.weight: grad_norm = 0.013019
Total gradient norm: 0.034855
=== Actor Training Debug (Iteration 4885) ===
Q mean: -65.942047
Q std: 23.453693
Actor loss: 65.946037
Action reg: 0.003988
  l1.weight: grad_norm = 0.027831
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.076866
Total gradient norm: 0.155960
=== Actor Training Debug (Iteration 4886) ===
Q mean: -66.524033
Q std: 24.055937
Actor loss: 66.528000
Action reg: 0.003967
  l1.weight: grad_norm = 0.112664
  l1.bias: grad_norm = 0.001088
  l2.weight: grad_norm = 0.267404
Total gradient norm: 0.553923
=== Actor Training Debug (Iteration 4887) ===
Q mean: -67.229370
Q std: 21.355326
Actor loss: 67.233360
Action reg: 0.003991
  l1.weight: grad_norm = 0.169893
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.357966
Total gradient norm: 0.685799
=== Actor Training Debug (Iteration 4888) ===
Q mean: -67.664978
Q std: 21.527641
Actor loss: 67.668961
Action reg: 0.003982
  l1.weight: grad_norm = 0.013034
  l1.bias: grad_norm = 0.000578
  l2.weight: grad_norm = 0.026444
Total gradient norm: 0.048952
=== Actor Training Debug (Iteration 4889) ===
Q mean: -68.502121
Q std: 23.199833
Actor loss: 68.506096
Action reg: 0.003972
  l1.weight: grad_norm = 0.011111
  l1.bias: grad_norm = 0.001086
  l2.weight: grad_norm = 0.023511
Total gradient norm: 0.055532
=== Actor Training Debug (Iteration 4890) ===
Q mean: -66.216827
Q std: 21.664732
Actor loss: 66.220810
Action reg: 0.003985
  l1.weight: grad_norm = 0.095340
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.210107
Total gradient norm: 0.417398
=== Actor Training Debug (Iteration 4891) ===
Q mean: -65.988266
Q std: 22.052040
Actor loss: 65.992241
Action reg: 0.003979
  l1.weight: grad_norm = 0.134806
  l1.bias: grad_norm = 0.000690
  l2.weight: grad_norm = 0.336086
Total gradient norm: 0.719297
=== Actor Training Debug (Iteration 4892) ===
Q mean: -65.480415
Q std: 21.652235
Actor loss: 65.484406
Action reg: 0.003988
  l1.weight: grad_norm = 0.006189
  l1.bias: grad_norm = 0.000616
  l2.weight: grad_norm = 0.014282
Total gradient norm: 0.027560
=== Actor Training Debug (Iteration 4893) ===
Q mean: -65.783081
Q std: 22.147907
Actor loss: 65.787064
Action reg: 0.003981
  l1.weight: grad_norm = 0.030562
  l1.bias: grad_norm = 0.000751
  l2.weight: grad_norm = 0.064609
Total gradient norm: 0.111985
=== Actor Training Debug (Iteration 4894) ===
Q mean: -68.343475
Q std: 21.398043
Actor loss: 68.347458
Action reg: 0.003986
  l1.weight: grad_norm = 0.046721
  l1.bias: grad_norm = 0.000494
  l2.weight: grad_norm = 0.131006
Total gradient norm: 0.299414
=== Actor Training Debug (Iteration 4895) ===
Q mean: -65.424835
Q std: 22.823898
Actor loss: 65.428802
Action reg: 0.003969
  l1.weight: grad_norm = 0.044950
  l1.bias: grad_norm = 0.001262
  l2.weight: grad_norm = 0.090839
Total gradient norm: 0.174712
=== Actor Training Debug (Iteration 4896) ===
Q mean: -64.729683
Q std: 21.774582
Actor loss: 64.733673
Action reg: 0.003987
  l1.weight: grad_norm = 0.112903
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.285137
Total gradient norm: 0.510878
=== Actor Training Debug (Iteration 4897) ===
Q mean: -68.826141
Q std: 22.344725
Actor loss: 68.830124
Action reg: 0.003983
  l1.weight: grad_norm = 0.269223
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.609593
Total gradient norm: 1.110675
=== Actor Training Debug (Iteration 4898) ===
Q mean: -67.305984
Q std: 21.986290
Actor loss: 67.309959
Action reg: 0.003975
  l1.weight: grad_norm = 0.013446
  l1.bias: grad_norm = 0.000838
  l2.weight: grad_norm = 0.037504
Total gradient norm: 0.081871
=== Actor Training Debug (Iteration 4899) ===
Q mean: -65.045807
Q std: 20.310410
Actor loss: 65.049797
Action reg: 0.003992
  l1.weight: grad_norm = 0.032284
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.086526
Total gradient norm: 0.217094
=== Actor Training Debug (Iteration 4900) ===
Q mean: -64.874397
Q std: 22.445595
Actor loss: 64.878380
Action reg: 0.003982
  l1.weight: grad_norm = 0.006630
  l1.bias: grad_norm = 0.000821
  l2.weight: grad_norm = 0.014674
Total gradient norm: 0.030597
=== Actor Training Debug (Iteration 4901) ===
Q mean: -65.751312
Q std: 22.768978
Actor loss: 65.755280
Action reg: 0.003964
  l1.weight: grad_norm = 0.098486
  l1.bias: grad_norm = 0.001494
  l2.weight: grad_norm = 0.246437
Total gradient norm: 0.546287
=== Actor Training Debug (Iteration 4902) ===
Q mean: -66.226593
Q std: 22.923002
Actor loss: 66.230576
Action reg: 0.003981
  l1.weight: grad_norm = 0.048751
  l1.bias: grad_norm = 0.000552
  l2.weight: grad_norm = 0.125892
Total gradient norm: 0.274335
=== Actor Training Debug (Iteration 4903) ===
Q mean: -67.601486
Q std: 22.725426
Actor loss: 67.605461
Action reg: 0.003972
  l1.weight: grad_norm = 0.081844
  l1.bias: grad_norm = 0.001049
  l2.weight: grad_norm = 0.152378
Total gradient norm: 0.274269
=== Actor Training Debug (Iteration 4904) ===
Q mean: -63.868740
Q std: 23.128769
Actor loss: 63.872704
Action reg: 0.003965
  l1.weight: grad_norm = 0.002957
  l1.bias: grad_norm = 0.001418
  l2.weight: grad_norm = 0.013147
Total gradient norm: 0.042822
=== Actor Training Debug (Iteration 4905) ===
Q mean: -69.284668
Q std: 21.737953
Actor loss: 69.288651
Action reg: 0.003981
  l1.weight: grad_norm = 0.107724
  l1.bias: grad_norm = 0.000538
  l2.weight: grad_norm = 0.265107
Total gradient norm: 0.597247
=== Actor Training Debug (Iteration 4906) ===
Q mean: -65.985001
Q std: 22.518406
Actor loss: 65.988976
Action reg: 0.003973
  l1.weight: grad_norm = 0.208450
  l1.bias: grad_norm = 0.000725
  l2.weight: grad_norm = 0.441072
Total gradient norm: 0.738626
=== Actor Training Debug (Iteration 4907) ===
Q mean: -64.093925
Q std: 21.779486
Actor loss: 64.097916
Action reg: 0.003988
  l1.weight: grad_norm = 0.112500
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.300562
Total gradient norm: 0.708385
=== Actor Training Debug (Iteration 4908) ===
Q mean: -67.443825
Q std: 21.189875
Actor loss: 67.447800
Action reg: 0.003973
  l1.weight: grad_norm = 0.267283
  l1.bias: grad_norm = 0.000604
  l2.weight: grad_norm = 0.556261
Total gradient norm: 0.931945
=== Actor Training Debug (Iteration 4909) ===
Q mean: -64.224037
Q std: 23.230499
Actor loss: 64.228004
Action reg: 0.003967
  l1.weight: grad_norm = 0.153477
  l1.bias: grad_norm = 0.001051
  l2.weight: grad_norm = 0.359667
Total gradient norm: 0.736254
=== Actor Training Debug (Iteration 4910) ===
Q mean: -67.957275
Q std: 20.638758
Actor loss: 67.961266
Action reg: 0.003989
  l1.weight: grad_norm = 0.139837
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.289326
Total gradient norm: 0.579011
=== Actor Training Debug (Iteration 4911) ===
Q mean: -69.897453
Q std: 21.798836
Actor loss: 69.901443
Action reg: 0.003986
  l1.weight: grad_norm = 0.130627
  l1.bias: grad_norm = 0.000513
  l2.weight: grad_norm = 0.320098
Total gradient norm: 0.526362
=== Actor Training Debug (Iteration 4912) ===
Q mean: -68.592239
Q std: 22.476894
Actor loss: 68.596191
Action reg: 0.003956
  l1.weight: grad_norm = 0.050340
  l1.bias: grad_norm = 0.001538
  l2.weight: grad_norm = 0.109166
Total gradient norm: 0.163109
=== Actor Training Debug (Iteration 4913) ===
Q mean: -67.755043
Q std: 19.654882
Actor loss: 67.759033
Action reg: 0.003991
  l1.weight: grad_norm = 0.026708
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.059154
Total gradient norm: 0.103847
=== Actor Training Debug (Iteration 4914) ===
Q mean: -66.282547
Q std: 19.365282
Actor loss: 66.286537
Action reg: 0.003994
  l1.weight: grad_norm = 0.142557
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.363074
Total gradient norm: 0.789990
=== Actor Training Debug (Iteration 4915) ===
Q mean: -66.484299
Q std: 22.165627
Actor loss: 66.488266
Action reg: 0.003970
  l1.weight: grad_norm = 0.026300
  l1.bias: grad_norm = 0.001162
  l2.weight: grad_norm = 0.078592
Total gradient norm: 0.161194
=== Actor Training Debug (Iteration 4916) ===
Q mean: -63.934532
Q std: 22.693846
Actor loss: 63.938511
Action reg: 0.003978
  l1.weight: grad_norm = 0.163872
  l1.bias: grad_norm = 0.000715
  l2.weight: grad_norm = 0.530004
Total gradient norm: 1.003090
=== Actor Training Debug (Iteration 4917) ===
Q mean: -66.029816
Q std: 20.603661
Actor loss: 66.033806
Action reg: 0.003989
  l1.weight: grad_norm = 0.131711
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.277533
Total gradient norm: 0.550581
=== Actor Training Debug (Iteration 4918) ===
Q mean: -64.101318
Q std: 22.336935
Actor loss: 64.105301
Action reg: 0.003981
  l1.weight: grad_norm = 0.075029
  l1.bias: grad_norm = 0.000639
  l2.weight: grad_norm = 0.181509
Total gradient norm: 0.380098
=== Actor Training Debug (Iteration 4919) ===
Q mean: -68.297989
Q std: 21.329065
Actor loss: 68.301979
Action reg: 0.003993
  l1.weight: grad_norm = 0.222287
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.390279
Total gradient norm: 0.703380
=== Actor Training Debug (Iteration 4920) ===
Q mean: -67.982407
Q std: 21.440804
Actor loss: 67.986382
Action reg: 0.003975
  l1.weight: grad_norm = 0.039585
  l1.bias: grad_norm = 0.001025
  l2.weight: grad_norm = 0.113963
Total gradient norm: 0.216715
=== Actor Training Debug (Iteration 4921) ===
Q mean: -66.094864
Q std: 21.134003
Actor loss: 66.098839
Action reg: 0.003972
  l1.weight: grad_norm = 0.801561
  l1.bias: grad_norm = 0.000606
  l2.weight: grad_norm = 2.588514
Total gradient norm: 5.160580
=== Actor Training Debug (Iteration 4922) ===
Q mean: -65.181183
Q std: 21.301338
Actor loss: 65.185173
Action reg: 0.003993
  l1.weight: grad_norm = 0.012696
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.035385
Total gradient norm: 0.073742
=== Actor Training Debug (Iteration 4923) ===
Q mean: -68.284500
Q std: 23.575537
Actor loss: 68.288490
Action reg: 0.003989
  l1.weight: grad_norm = 0.093474
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.174731
Total gradient norm: 0.345754
=== Actor Training Debug (Iteration 4924) ===
Q mean: -68.669716
Q std: 22.774149
Actor loss: 68.673698
Action reg: 0.003986
  l1.weight: grad_norm = 0.057820
  l1.bias: grad_norm = 0.000697
  l2.weight: grad_norm = 0.166692
Total gradient norm: 0.297551
=== Actor Training Debug (Iteration 4925) ===
Q mean: -66.992546
Q std: 21.345579
Actor loss: 66.996544
Action reg: 0.003997
  l1.weight: grad_norm = 0.056390
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.153042
Total gradient norm: 0.286036
=== Actor Training Debug (Iteration 4926) ===
Q mean: -65.521690
Q std: 21.621073
Actor loss: 65.525673
Action reg: 0.003984
  l1.weight: grad_norm = 0.024859
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.074849
Total gradient norm: 0.175885
=== Actor Training Debug (Iteration 4927) ===
Q mean: -67.480759
Q std: 22.140661
Actor loss: 67.484741
Action reg: 0.003982
  l1.weight: grad_norm = 0.196324
  l1.bias: grad_norm = 0.000625
  l2.weight: grad_norm = 0.368810
Total gradient norm: 0.703784
=== Actor Training Debug (Iteration 4928) ===
Q mean: -64.945602
Q std: 23.407846
Actor loss: 64.949570
Action reg: 0.003968
  l1.weight: grad_norm = 0.114251
  l1.bias: grad_norm = 0.000868
  l2.weight: grad_norm = 0.226118
Total gradient norm: 0.419749
=== Actor Training Debug (Iteration 4929) ===
Q mean: -63.684151
Q std: 21.805653
Actor loss: 63.688122
Action reg: 0.003973
  l1.weight: grad_norm = 0.036946
  l1.bias: grad_norm = 0.001096
  l2.weight: grad_norm = 0.073631
Total gradient norm: 0.132021
=== Actor Training Debug (Iteration 4930) ===
Q mean: -64.366058
Q std: 22.420048
Actor loss: 64.370033
Action reg: 0.003972
  l1.weight: grad_norm = 0.126848
  l1.bias: grad_norm = 0.000773
  l2.weight: grad_norm = 0.319946
Total gradient norm: 0.604874
=== Actor Training Debug (Iteration 4931) ===
Q mean: -67.445557
Q std: 24.310431
Actor loss: 67.449547
Action reg: 0.003987
  l1.weight: grad_norm = 0.056910
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.137772
Total gradient norm: 0.240286
=== Actor Training Debug (Iteration 4932) ===
Q mean: -64.507248
Q std: 22.124636
Actor loss: 64.511230
Action reg: 0.003981
  l1.weight: grad_norm = 0.033735
  l1.bias: grad_norm = 0.000542
  l2.weight: grad_norm = 0.072962
Total gradient norm: 0.141037
=== Actor Training Debug (Iteration 4933) ===
Q mean: -65.805214
Q std: 23.738228
Actor loss: 65.809174
Action reg: 0.003961
  l1.weight: grad_norm = 0.178099
  l1.bias: grad_norm = 0.001192
  l2.weight: grad_norm = 0.452091
Total gradient norm: 0.846558
=== Actor Training Debug (Iteration 4934) ===
Q mean: -67.202408
Q std: 22.553049
Actor loss: 67.206383
Action reg: 0.003976
  l1.weight: grad_norm = 0.035834
  l1.bias: grad_norm = 0.000874
  l2.weight: grad_norm = 0.094758
Total gradient norm: 0.198659
=== Actor Training Debug (Iteration 4935) ===
Q mean: -67.532166
Q std: 22.224771
Actor loss: 67.536140
Action reg: 0.003977
  l1.weight: grad_norm = 0.135115
  l1.bias: grad_norm = 0.000813
  l2.weight: grad_norm = 0.367482
Total gradient norm: 0.752297
=== Actor Training Debug (Iteration 4936) ===
Q mean: -68.773659
Q std: 21.692017
Actor loss: 68.777641
Action reg: 0.003985
  l1.weight: grad_norm = 0.054288
  l1.bias: grad_norm = 0.000634
  l2.weight: grad_norm = 0.140202
Total gradient norm: 0.277746
=== Actor Training Debug (Iteration 4937) ===
Q mean: -67.497849
Q std: 22.689970
Actor loss: 67.501823
Action reg: 0.003975
  l1.weight: grad_norm = 0.137512
  l1.bias: grad_norm = 0.000939
  l2.weight: grad_norm = 0.356141
Total gradient norm: 0.502033
=== Actor Training Debug (Iteration 4938) ===
Q mean: -64.917351
Q std: 23.814814
Actor loss: 64.921310
Action reg: 0.003960
  l1.weight: grad_norm = 0.027290
  l1.bias: grad_norm = 0.001460
  l2.weight: grad_norm = 0.060051
Total gradient norm: 0.135403
=== Actor Training Debug (Iteration 4939) ===
Q mean: -66.624451
Q std: 21.720516
Actor loss: 66.628433
Action reg: 0.003985
  l1.weight: grad_norm = 0.247295
  l1.bias: grad_norm = 0.000619
  l2.weight: grad_norm = 0.722898
Total gradient norm: 1.336665
=== Actor Training Debug (Iteration 4940) ===
Q mean: -65.560989
Q std: 22.214897
Actor loss: 65.564957
Action reg: 0.003971
  l1.weight: grad_norm = 0.172891
  l1.bias: grad_norm = 0.000906
  l2.weight: grad_norm = 0.351802
Total gradient norm: 0.641079
=== Actor Training Debug (Iteration 4941) ===
Q mean: -66.262230
Q std: 23.902029
Actor loss: 66.266205
Action reg: 0.003978
  l1.weight: grad_norm = 0.027329
  l1.bias: grad_norm = 0.000644
  l2.weight: grad_norm = 0.049338
Total gradient norm: 0.076352
=== Actor Training Debug (Iteration 4942) ===
Q mean: -66.607147
Q std: 21.710739
Actor loss: 66.611122
Action reg: 0.003975
  l1.weight: grad_norm = 0.047724
  l1.bias: grad_norm = 0.000902
  l2.weight: grad_norm = 0.096219
Total gradient norm: 0.171086
=== Actor Training Debug (Iteration 4943) ===
Q mean: -66.008820
Q std: 20.022617
Actor loss: 66.012810
Action reg: 0.003993
  l1.weight: grad_norm = 0.144148
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.452022
Total gradient norm: 0.800021
=== Actor Training Debug (Iteration 4944) ===
Q mean: -69.783707
Q std: 21.080128
Actor loss: 69.787697
Action reg: 0.003987
  l1.weight: grad_norm = 0.069845
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.151101
Total gradient norm: 0.316793
=== Actor Training Debug (Iteration 4945) ===
Q mean: -67.131355
Q std: 21.686968
Actor loss: 67.135338
Action reg: 0.003984
  l1.weight: grad_norm = 0.018787
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.051604
Total gradient norm: 0.105358
=== Actor Training Debug (Iteration 4946) ===
Q mean: -66.083443
Q std: 21.187283
Actor loss: 66.087440
Action reg: 0.003996
  l1.weight: grad_norm = 0.024882
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.076989
Total gradient norm: 0.165619
=== Actor Training Debug (Iteration 4947) ===
Q mean: -66.311111
Q std: 22.722870
Actor loss: 66.315094
Action reg: 0.003980
  l1.weight: grad_norm = 0.044277
  l1.bias: grad_norm = 0.000653
  l2.weight: grad_norm = 0.107489
Total gradient norm: 0.232715
=== Actor Training Debug (Iteration 4948) ===
Q mean: -65.076271
Q std: 22.744930
Actor loss: 65.080254
Action reg: 0.003985
  l1.weight: grad_norm = 0.032335
  l1.bias: grad_norm = 0.000546
  l2.weight: grad_norm = 0.061476
Total gradient norm: 0.123650
=== Actor Training Debug (Iteration 4949) ===
Q mean: -65.741638
Q std: 22.013672
Actor loss: 65.745628
Action reg: 0.003989
  l1.weight: grad_norm = 0.081844
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.198367
Total gradient norm: 0.405828
=== Actor Training Debug (Iteration 4950) ===
Q mean: -66.785088
Q std: 22.743179
Actor loss: 66.789055
Action reg: 0.003969
  l1.weight: grad_norm = 0.123275
  l1.bias: grad_norm = 0.000861
  l2.weight: grad_norm = 0.324779
Total gradient norm: 0.718573
=== Actor Training Debug (Iteration 4951) ===
Q mean: -66.807259
Q std: 21.148609
Actor loss: 66.811241
Action reg: 0.003982
  l1.weight: grad_norm = 0.049164
  l1.bias: grad_norm = 0.000684
  l2.weight: grad_norm = 0.103103
Total gradient norm: 0.203847
=== Actor Training Debug (Iteration 4952) ===
Q mean: -61.619446
Q std: 21.253716
Actor loss: 61.623436
Action reg: 0.003992
  l1.weight: grad_norm = 0.150701
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.402826
Total gradient norm: 0.696849
=== Actor Training Debug (Iteration 4953) ===
Q mean: -69.295219
Q std: 20.779316
Actor loss: 69.299149
Action reg: 0.003928
  l1.weight: grad_norm = 0.842879
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 1.707446
Total gradient norm: 3.281724
=== Actor Training Debug (Iteration 4954) ===
Q mean: -67.264992
Q std: 23.094849
Actor loss: 67.268959
Action reg: 0.003964
  l1.weight: grad_norm = 0.014405
  l1.bias: grad_norm = 0.001582
  l2.weight: grad_norm = 0.038630
Total gradient norm: 0.083313
=== Actor Training Debug (Iteration 4955) ===
Q mean: -66.388687
Q std: 21.389292
Actor loss: 66.392670
Action reg: 0.003980
  l1.weight: grad_norm = 0.104598
  l1.bias: grad_norm = 0.000623
  l2.weight: grad_norm = 0.300028
Total gradient norm: 0.624761
=== Actor Training Debug (Iteration 4956) ===
Q mean: -67.444717
Q std: 22.174990
Actor loss: 67.448692
Action reg: 0.003973
  l1.weight: grad_norm = 0.221705
  l1.bias: grad_norm = 0.000728
  l2.weight: grad_norm = 0.490479
Total gradient norm: 1.122176
=== Actor Training Debug (Iteration 4957) ===
Q mean: -66.573273
Q std: 21.548845
Actor loss: 66.577240
Action reg: 0.003970
  l1.weight: grad_norm = 0.076163
  l1.bias: grad_norm = 0.000893
  l2.weight: grad_norm = 0.166461
Total gradient norm: 0.349115
=== Actor Training Debug (Iteration 4958) ===
Q mean: -66.575699
Q std: 22.551640
Actor loss: 66.579681
Action reg: 0.003985
  l1.weight: grad_norm = 0.031678
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.093071
Total gradient norm: 0.216223
=== Actor Training Debug (Iteration 4959) ===
Q mean: -65.938316
Q std: 21.338915
Actor loss: 65.942307
Action reg: 0.003986
  l1.weight: grad_norm = 0.014281
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.027689
Total gradient norm: 0.041976
=== Actor Training Debug (Iteration 4960) ===
Q mean: -67.344002
Q std: 22.160975
Actor loss: 67.347969
Action reg: 0.003967
  l1.weight: grad_norm = 0.077860
  l1.bias: grad_norm = 0.001194
  l2.weight: grad_norm = 0.168360
Total gradient norm: 0.288203
Total gradient norm: 0.2842534696on 1203) ===
=== Actor Training Debug (Iteration 4971) ===
Q mean: -64.413429
Q std: 21.758797
Actor loss: 64.417404
Action reg: 0.003977
  l1.weight: grad_norm = 0.070876
  l1.bias: grad_norm = 0.000867
  l2.weight: grad_norm = 0.206081
Total gradient norm: 0.449306
=== Actor Training Debug (Iteration 4972) ===
Q mean: -67.252625
Q std: 22.484362
Actor loss: 67.256584
Action reg: 0.003961
  l1.weight: grad_norm = 0.723379
  l1.bias: grad_norm = 0.001027
  l2.weight: grad_norm = 1.374789
Total gradient norm: 2.839479
=== Actor Training Debug (Iteration 4973) ===
Q mean: -67.113846
Q std: 23.239082
Actor loss: 67.117813
Action reg: 0.003967
  l1.weight: grad_norm = 0.195855
  l1.bias: grad_norm = 0.001507
  l2.weight: grad_norm = 0.389144
Total gradient norm: 0.742253
=== Actor Training Debug (Iteration 4974) ===
Q mean: -67.087555
Q std: 22.785891
Actor loss: 67.091537
Action reg: 0.003981
  l1.weight: grad_norm = 0.154757
  l1.bias: grad_norm = 0.000598
  l2.weight: grad_norm = 0.424395
Total gradient norm: 0.776000
=== Actor Training Debug (Iteration 4975) ===
Q mean: -66.652802
Q std: 22.386744
Actor loss: 66.656769
Action reg: 0.003969
  l1.weight: grad_norm = 0.120218
  l1.bias: grad_norm = 0.001026
  l2.weight: grad_norm = 0.275891
Total gradient norm: 0.564248
=== Actor Training Debug (Iteration 4976) ===
Q mean: -68.659714
Q std: 22.174040
Actor loss: 68.663681
Action reg: 0.003970
  l1.weight: grad_norm = 0.012835
  l1.bias: grad_norm = 0.001014
  l2.weight: grad_norm = 0.035053
Total gradient norm: 0.088437
=== Actor Training Debug (Iteration 4977) ===
Q mean: -69.106140
Q std: 21.939169
Actor loss: 69.110107
Action reg: 0.003965
  l1.weight: grad_norm = 0.021864
  l1.bias: grad_norm = 0.001355
  l2.weight: grad_norm = 0.057642
Total gradient norm: 0.111253
=== Actor Training Debug (Iteration 4978) ===
Q mean: -67.384270
Q std: 20.987614
Actor loss: 67.388252
Action reg: 0.003985
  l1.weight: grad_norm = 0.006252
  l1.bias: grad_norm = 0.000760
  l2.weight: grad_norm = 0.016239
Total gradient norm: 0.037968
=== Actor Training Debug (Iteration 4979) ===
Q mean: -66.750259
Q std: 21.470829
Actor loss: 66.754242
Action reg: 0.003980
  l1.weight: grad_norm = 0.161850
  l1.bias: grad_norm = 0.000831
  l2.weight: grad_norm = 0.475984
Total gradient norm: 0.920846
=== Actor Training Debug (Iteration 4980) ===
Q mean: -66.467918
Q std: 22.968988
Actor loss: 66.471893
Action reg: 0.003974
  l1.weight: grad_norm = 0.078296
  l1.bias: grad_norm = 0.001002
  l2.weight: grad_norm = 0.187042
Total gradient norm: 0.351112
=== Actor Training Debug (Iteration 4981) ===
Q mean: -67.501297
Q std: 22.631159
Actor loss: 67.505280
Action reg: 0.003985
  l1.weight: grad_norm = 0.096652
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.280080
Total gradient norm: 0.456252
=== Actor Training Debug (Iteration 4982) ===
Q mean: -66.773239
Q std: 22.273172
Actor loss: 66.777214
Action reg: 0.003977
  l1.weight: grad_norm = 0.105532
  l1.bias: grad_norm = 0.000624
  l2.weight: grad_norm = 0.282111
Total gradient norm: 0.542142
=== Actor Training Debug (Iteration 4983) ===
Q mean: -67.909729
Q std: 21.241589
Actor loss: 67.913712
Action reg: 0.003983
  l1.weight: grad_norm = 0.330377
  l1.bias: grad_norm = 0.000738
  l2.weight: grad_norm = 0.697508
Total gradient norm: 1.473732
=== Actor Training Debug (Iteration 4984) ===
Q mean: -67.110092
Q std: 21.870285
Actor loss: 67.114067
Action reg: 0.003973
  l1.weight: grad_norm = 0.164457
  l1.bias: grad_norm = 0.000940
  l2.weight: grad_norm = 0.557706
Total gradient norm: 1.143011
=== Actor Training Debug (Iteration 4985) ===
Q mean: -66.470993
Q std: 22.759758
Actor loss: 66.474968
Action reg: 0.003977
  l1.weight: grad_norm = 0.225368
  l1.bias: grad_norm = 0.000723
  l2.weight: grad_norm = 0.633141
Total gradient norm: 1.285143
=== Actor Training Debug (Iteration 4986) ===
Q mean: -65.225876
Q std: 20.650343
Actor loss: 65.229805
Action reg: 0.003931
  l1.weight: grad_norm = 1.807927
  l1.bias: grad_norm = 0.000611
  l2.weight: grad_norm = 5.804872
Total gradient norm: 9.904057
=== Actor Training Debug (Iteration 4987) ===
Q mean: -65.286499
Q std: 22.102001
Actor loss: 65.290466
Action reg: 0.003965
  l1.weight: grad_norm = 0.463621
  l1.bias: grad_norm = 0.000781
  l2.weight: grad_norm = 1.311709
Total gradient norm: 2.432783
=== Actor Training Debug (Iteration 4988) ===
Q mean: -67.153717
Q std: 22.747696
Actor loss: 67.157692
Action reg: 0.003977
  l1.weight: grad_norm = 0.104972
  l1.bias: grad_norm = 0.000648
  l2.weight: grad_norm = 0.290732
Total gradient norm: 0.619068
=== Actor Training Debug (Iteration 4989) ===
Q mean: -66.976944
Q std: 22.709890
Actor loss: 66.980927
Action reg: 0.003985
  l1.weight: grad_norm = 0.202901
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.536929
Total gradient norm: 0.971189
=== Actor Training Debug (Iteration 4990) ===
Q mean: -64.758591
Q std: 23.047438
Actor loss: 64.762573
Action reg: 0.003983
  l1.weight: grad_norm = 0.167435
  l1.bias: grad_norm = 0.000567
  l2.weight: grad_norm = 0.450885
Total gradient norm: 0.868714
=== Actor Training Debug (Iteration 4991) ===
Q mean: -66.250877
Q std: 22.693594
Actor loss: 66.254860
Action reg: 0.003982
  l1.weight: grad_norm = 0.155925
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.477951
Total gradient norm: 0.859351
=== Actor Training Debug (Iteration 4992) ===
Q mean: -66.286430
Q std: 22.616142
Actor loss: 66.290421
Action reg: 0.003987
  l1.weight: grad_norm = 0.037666
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.092002
Total gradient norm: 0.160668
=== Actor Training Debug (Iteration 4993) ===
Q mean: -67.788658
Q std: 21.077808
Actor loss: 67.792641
Action reg: 0.003981
  l1.weight: grad_norm = 0.124615
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.342162
Total gradient norm: 0.654553
=== Actor Training Debug (Iteration 4994) ===
Q mean: -68.371315
Q std: 21.528612
Actor loss: 68.375305
Action reg: 0.003990
  l1.weight: grad_norm = 0.045579
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.115141
Total gradient norm: 0.240142
=== Actor Training Debug (Iteration 4995) ===
Q mean: -66.879852
Q std: 23.733389
Actor loss: 66.883812
Action reg: 0.003963
  l1.weight: grad_norm = 0.068777
  l1.bias: grad_norm = 0.001161
  l2.weight: grad_norm = 0.154668
Total gradient norm: 0.363358
=== Actor Training Debug (Iteration 4996) ===
Q mean: -69.145401
Q std: 21.668737
Actor loss: 69.149391
Action reg: 0.003989
  l1.weight: grad_norm = 0.050692
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.129139
Total gradient norm: 0.320784
=== Actor Training Debug (Iteration 4997) ===
Q mean: -66.938400
Q std: 20.988310
Actor loss: 66.942390
Action reg: 0.003987
  l1.weight: grad_norm = 0.391165
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 1.136221
Total gradient norm: 2.149301
=== Actor Training Debug (Iteration 4998) ===
Q mean: -65.752975
Q std: 22.714495
Actor loss: 65.756966
Action reg: 0.003993
  l1.weight: grad_norm = 0.009376
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.019313
Total gradient norm: 0.041972
=== Actor Training Debug (Iteration 4999) ===
Q mean: -67.741898
Q std: 21.902018
Actor loss: 67.745880
Action reg: 0.003982
  l1.weight: grad_norm = 0.065434
  l1.bias: grad_norm = 0.000646
  l2.weight: grad_norm = 0.155029
Total gradient norm: 0.289331
=== Actor Training Debug (Iteration 5000) ===
Q mean: -66.344482
Q std: 22.442068
Actor loss: 66.348457
Action reg: 0.003977
  l1.weight: grad_norm = 0.084269
  l1.bias: grad_norm = 0.000733
  l2.weight: grad_norm = 0.201377
Total gradient norm: 0.379261
Step 10000: Critic Loss: 5.4976, Actor Loss: 66.3485, Q Value: -66.3445
  Average reward: -353.655 | Average length: 100.0
Evaluation at episode 100: -353.655
=== Actor Training Debug (Iteration 5001) ===
Q mean: -66.264542
Q std: 22.436203
Actor loss: 66.268517
Action reg: 0.003972
  l1.weight: grad_norm = 0.007652
  l1.bias: grad_norm = 0.000810
  l2.weight: grad_norm = 0.018432
Total gradient norm: 0.038669
=== Actor Training Debug (Iteration 5002) ===
Q mean: -66.217697
Q std: 22.394119
Actor loss: 66.221664
Action reg: 0.003970
  l1.weight: grad_norm = 0.002439
  l1.bias: grad_norm = 0.000996
  l2.weight: grad_norm = 0.008950
Total gradient norm: 0.027710
=== Actor Training Debug (Iteration 5003) ===
Q mean: -64.087036
Q std: 23.601713
Actor loss: 64.090996
Action reg: 0.003960
  l1.weight: grad_norm = 0.044178
  l1.bias: grad_norm = 0.001197
  l2.weight: grad_norm = 0.105626
Total gradient norm: 0.189036
=== Actor Training Debug (Iteration 5004) ===
Q mean: -66.177795
Q std: 23.257608
Actor loss: 66.181763
Action reg: 0.003968
  l1.weight: grad_norm = 0.076688
  l1.bias: grad_norm = 0.000884
  l2.weight: grad_norm = 0.177393
Total gradient norm: 0.274807
=== Actor Training Debug (Iteration 5005) ===
Q mean: -65.199806
Q std: 20.821274
Actor loss: 65.203796
Action reg: 0.003988
  l1.weight: grad_norm = 0.168041
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.397311
Total gradient norm: 0.846225
=== Actor Training Debug (Iteration 5006) ===
Q mean: -65.452721
Q std: 23.452961
Actor loss: 65.456673
Action reg: 0.003952
  l1.weight: grad_norm = 0.217179
  l1.bias: grad_norm = 0.001268
  l2.weight: grad_norm = 0.392415
Total gradient norm: 0.836720
=== Actor Training Debug (Iteration 5007) ===
Q mean: -64.725349
Q std: 22.590725
Actor loss: 64.729340
Action reg: 0.003989
  l1.weight: grad_norm = 0.061623
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.153770
Total gradient norm: 0.313389
=== Actor Training Debug (Iteration 5008) ===
Q mean: -67.367317
Q std: 22.101158
Actor loss: 67.371292
Action reg: 0.003972
  l1.weight: grad_norm = 0.007085
  l1.bias: grad_norm = 0.000768
  l2.weight: grad_norm = 0.017427
Total gradient norm: 0.034901
=== Actor Training Debug (Iteration 5009) ===
Q mean: -68.417465
Q std: 22.929602
Actor loss: 68.421440
Action reg: 0.003973
  l1.weight: grad_norm = 0.272728
  l1.bias: grad_norm = 0.000561
  l2.weight: grad_norm = 0.548912
Total gradient norm: 0.853700
=== Actor Training Debug (Iteration 5010) ===
Q mean: -68.203964
Q std: 20.358755
Actor loss: 68.207954
Action reg: 0.003989
  l1.weight: grad_norm = 0.085419
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.165993
Total gradient norm: 0.289410
=== Actor Training Debug (Iteration 5011) ===
Q mean: -67.188232
Q std: 20.620411
Actor loss: 67.192215
Action reg: 0.003982
  l1.weight: grad_norm = 0.150130
  l1.bias: grad_norm = 0.000770
  l2.weight: grad_norm = 0.304908
Total gradient norm: 0.654603
=== Actor Training Debug (Iteration 5012) ===
Q mean: -66.369247
Q std: 21.825016
Actor loss: 66.373230
Action reg: 0.003983
  l1.weight: grad_norm = 0.073606
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.154667
Total gradient norm: 0.241382
=== Actor Training Debug (Iteration 5013) ===
Q mean: -69.393044
Q std: 21.551256
Actor loss: 69.397026
Action reg: 0.003982
  l1.weight: grad_norm = 0.037059
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.101039
Total gradient norm: 0.219053
=== Actor Training Debug (Iteration 5014) ===
Q mean: -69.680557
Q std: 20.890829
Actor loss: 69.684540
Action reg: 0.003979
  l1.weight: grad_norm = 0.247015
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.679344
Total gradient norm: 1.370861
=== Actor Training Debug (Iteration 5015) ===
Q mean: -66.118759
Q std: 22.149889
Actor loss: 66.122719
Action reg: 0.003961
  l1.weight: grad_norm = 0.074948
  l1.bias: grad_norm = 0.001481
  l2.weight: grad_norm = 0.184042
Total gradient norm: 0.336347
=== Actor Training Debug (Iteration 5016) ===
Q mean: -67.477875
Q std: 21.742224
Actor loss: 67.481850
Action reg: 0.003978
  l1.weight: grad_norm = 0.023857
  l1.bias: grad_norm = 0.000694
  l2.weight: grad_norm = 0.056677
Total gradient norm: 0.120203
=== Actor Training Debug (Iteration 5017) ===
Q mean: -69.940567
Q std: 23.693577
Actor loss: 69.944527
Action reg: 0.003962
  l1.weight: grad_norm = 0.049964
  l1.bias: grad_norm = 0.001187
  l2.weight: grad_norm = 0.142408
Total gradient norm: 0.300875
=== Actor Training Debug (Iteration 5018) ===
Q mean: -66.334930
Q std: 23.324715
Actor loss: 66.338905
Action reg: 0.003976
  l1.weight: grad_norm = 0.048979
  l1.bias: grad_norm = 0.000806
  l2.weight: grad_norm = 0.130735
Total gradient norm: 0.254063
=== Actor Training Debug (Iteration 5019) ===
Q mean: -67.025887
Q std: 23.535366
Actor loss: 67.029854
Action reg: 0.003970
  l1.weight: grad_norm = 0.035616
  l1.bias: grad_norm = 0.000982
  l2.weight: grad_norm = 0.083203
Total gradient norm: 0.146317
=== Actor Training Debug (Iteration 5020) ===
Q mean: -66.818321
Q std: 22.357225
Actor loss: 66.822296
Action reg: 0.003975
  l1.weight: grad_norm = 0.062674
  l1.bias: grad_norm = 0.000588
  l2.weight: grad_norm = 0.167850
Total gradient norm: 0.321594
=== Actor Training Debug (Iteration 5021) ===
Q mean: -68.377914
Q std: 22.531828
Actor loss: 68.381882
Action reg: 0.003966
  l1.weight: grad_norm = 0.017804
  l1.bias: grad_norm = 0.001606
  l2.weight: grad_norm = 0.048627
Total gradient norm: 0.099479
=== Actor Training Debug (Iteration 5022) ===
Q mean: -69.530624
Q std: 21.184944
Actor loss: 69.534615
Action reg: 0.003992
  l1.weight: grad_norm = 0.030512
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.075845
Total gradient norm: 0.151297
=== Actor Training Debug (Iteration 5023) ===
Q mean: -66.114754
Q std: 23.312017
Actor loss: 66.118729
Action reg: 0.003972
  l1.weight: grad_norm = 0.019999
  l1.bias: grad_norm = 0.000861
  l2.weight: grad_norm = 0.058306
Total gradient norm: 0.126321
=== Actor Training Debug (Iteration 5024) ===
Q mean: -66.031288
Q std: 22.693333
Actor loss: 66.035255
Action reg: 0.003969
  l1.weight: grad_norm = 0.117597
  l1.bias: grad_norm = 0.001076
  l2.weight: grad_norm = 0.321387
Total gradient norm: 0.714555
=== Actor Training Debug (Iteration 5025) ===
Q mean: -67.113647
Q std: 21.955351
Actor loss: 67.117622
Action reg: 0.003976
  l1.weight: grad_norm = 0.070552
  l1.bias: grad_norm = 0.000848
  l2.weight: grad_norm = 0.189480
Total gradient norm: 0.353149
=== Actor Training Debug (Iteration 5026) ===
Q mean: -66.709526
Q std: 21.581108
Actor loss: 66.713516
Action reg: 0.003992
  l1.weight: grad_norm = 0.049910
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.103916
Total gradient norm: 0.176237
=== Actor Training Debug (Iteration 5027) ===
Q mean: -64.878326
Q std: 22.037140
Actor loss: 64.882294
Action reg: 0.003964
  l1.weight: grad_norm = 0.066500
  l1.bias: grad_norm = 0.001648
  l2.weight: grad_norm = 0.141475
Total gradient norm: 0.309648
=== Actor Training Debug (Iteration 5028) ===
Q mean: -66.862488
Q std: 22.573980
Actor loss: 66.866478
Action reg: 0.003992
  l1.weight: grad_norm = 0.308073
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.634852
Total gradient norm: 1.250622
=== Actor Training Debug (Iteration 5029) ===
Q mean: -67.641289
Q std: 21.363045
Actor loss: 67.645271
Action reg: 0.003985
  l1.weight: grad_norm = 0.048475
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.134912
Total gradient norm: 0.267948
=== Actor Training Debug (Iteration 5030) ===
Q mean: -66.357742
Q std: 21.197025
Actor loss: 66.361732
Action reg: 0.003989
  l1.weight: grad_norm = 0.162919
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.457343
Total gradient norm: 0.931278
=== Actor Training Debug (Iteration 5031) ===
Q mean: -66.469498
Q std: 23.392597
Actor loss: 66.473465
Action reg: 0.003970
  l1.weight: grad_norm = 0.146188
  l1.bias: grad_norm = 0.001080
  l2.weight: grad_norm = 0.328533
Total gradient norm: 0.756135
=== Actor Training Debug (Iteration 5032) ===
Q mean: -66.116623
Q std: 22.044725
Actor loss: 66.120598
Action reg: 0.003972
  l1.weight: grad_norm = 0.075664
  l1.bias: grad_norm = 0.000828
  l2.weight: grad_norm = 0.186325
Total gradient norm: 0.337920
=== Actor Training Debug (Iteration 5033) ===
Q mean: -66.884720
Q std: 22.560061
Actor loss: 66.888695
Action reg: 0.003976
  l1.weight: grad_norm = 0.114817
  l1.bias: grad_norm = 0.000740
  l2.weight: grad_norm = 0.285696
Total gradient norm: 0.465039
=== Actor Training Debug (Iteration 5034) ===
Q mean: -69.182304
Q std: 23.635103
Actor loss: 69.186279
Action reg: 0.003976
  l1.weight: grad_norm = 0.105668
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.253318
Total gradient norm: 0.442506
=== Actor Training Debug (Iteration 5035) ===
Q mean: -63.625748
Q std: 24.035807
Actor loss: 63.629715
Action reg: 0.003969
  l1.weight: grad_norm = 0.087846
  l1.bias: grad_norm = 0.000744
  l2.weight: grad_norm = 0.257008
Total gradient norm: 0.584315
=== Actor Training Debug (Iteration 5036) ===
Q mean: -67.702484
Q std: 22.360554
Actor loss: 67.706459
Action reg: 0.003976
  l1.weight: grad_norm = 0.036442
  l1.bias: grad_norm = 0.000794
  l2.weight: grad_norm = 0.103700
Total gradient norm: 0.195804
=== Actor Training Debug (Iteration 5037) ===
Q mean: -70.316956
Q std: 21.817282
Actor loss: 70.320946
Action reg: 0.003988
  l1.weight: grad_norm = 0.309145
  l1.bias: grad_norm = 0.000534
  l2.weight: grad_norm = 0.816251
Total gradient norm: 1.360602
=== Actor Training Debug (Iteration 5038) ===
Q mean: -68.475174
Q std: 23.449200
Actor loss: 68.479126
Action reg: 0.003951
  l1.weight: grad_norm = 0.030725
  l1.bias: grad_norm = 0.001645
  l2.weight: grad_norm = 0.089868
Total gradient norm: 0.192307
=== Actor Training Debug (Iteration 5039) ===
Q mean: -65.801224
Q std: 22.717245
Actor loss: 65.805206
Action reg: 0.003981
  l1.weight: grad_norm = 0.115665
  l1.bias: grad_norm = 0.000721
  l2.weight: grad_norm = 0.247449
Total gradient norm: 0.402248
=== Actor Training Debug (Iteration 5040) ===
Q mean: -65.238495
Q std: 22.516672
Actor loss: 65.242462
Action reg: 0.003964
  l1.weight: grad_norm = 0.058883
  l1.bias: grad_norm = 0.001038
  l2.weight: grad_norm = 0.130542
Total gradient norm: 0.252945
=== Actor Training Debug (Iteration 5041) ===
Q mean: -67.067177
Q std: 22.708237
Actor loss: 67.071159
Action reg: 0.003986
  l1.weight: grad_norm = 0.244166
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.639439
Total gradient norm: 1.064472
=== Actor Training Debug (Iteration 5042) ===
Q mean: -68.766708
Q std: 22.914076
Actor loss: 68.770699
Action reg: 0.003990
  l1.weight: grad_norm = 0.054412
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.118960
Total gradient norm: 0.203185
=== Actor Training Debug (Iteration 5043) ===
Q mean: -68.889236
Q std: 23.187172
Actor loss: 68.893204
Action reg: 0.003965
  l1.weight: grad_norm = 0.047640
  l1.bias: grad_norm = 0.001085
  l2.weight: grad_norm = 0.120517
Total gradient norm: 0.244995
=== Actor Training Debug (Iteration 5044) ===
Q mean: -66.020576
Q std: 22.910625
Actor loss: 66.024551
Action reg: 0.003978
  l1.weight: grad_norm = 0.183810
  l1.bias: grad_norm = 0.000509
  l2.weight: grad_norm = 0.415274
Total gradient norm: 0.848659
=== Actor Training Debug (Iteration 5045) ===
Q mean: -68.330055
Q std: 23.270309
Actor loss: 68.334015
Action reg: 0.003962
  l1.weight: grad_norm = 0.099986
  l1.bias: grad_norm = 0.000976
  l2.weight: grad_norm = 0.270392
Total gradient norm: 0.600179
=== Actor Training Debug (Iteration 5046) ===
Q mean: -69.183525
Q std: 20.180565
Actor loss: 69.187515
Action reg: 0.003988
  l1.weight: grad_norm = 0.036440
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.073672
Total gradient norm: 0.134774
=== Actor Training Debug (Iteration 5047) ===
Q mean: -64.156693
Q std: 21.379662
Actor loss: 64.160683
Action reg: 0.003988
  l1.weight: grad_norm = 0.078000
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.215277
Total gradient norm: 0.364933
=== Actor Training Debug (Iteration 5048) ===
Q mean: -65.411819
Q std: 21.576521
Actor loss: 65.415794
Action reg: 0.003972
  l1.weight: grad_norm = 0.272243
  l1.bias: grad_norm = 0.000599
  l2.weight: grad_norm = 0.839553
Total gradient norm: 1.428794
=== Actor Training Debug (Iteration 5049) ===
Q mean: -67.987091
Q std: 23.293629
Actor loss: 67.991066
Action reg: 0.003978
  l1.weight: grad_norm = 0.201970
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.565312
Total gradient norm: 1.150561
=== Actor Training Debug (Iteration 5050) ===
Q mean: -66.973045
Q std: 22.159870
Actor loss: 66.977036
Action reg: 0.003991
  l1.weight: grad_norm = 0.044344
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.097250
Total gradient norm: 0.177852
=== Actor Training Debug (Iteration 5051) ===
Q mean: -65.930984
Q std: 21.196669
Actor loss: 65.934975
Action reg: 0.003990
  l1.weight: grad_norm = 0.072000
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.160993
Total gradient norm: 0.217248
=== Actor Training Debug (Iteration 5052) ===
Q mean: -66.333542
Q std: 23.104115
Actor loss: 66.337494
Action reg: 0.003953
  l1.weight: grad_norm = 0.073382
  l1.bias: grad_norm = 0.001248
  l2.weight: grad_norm = 0.207778
Total gradient norm: 0.407645
=== Actor Training Debug (Iteration 5053) ===
Q mean: -67.698593
Q std: 23.227760
Actor loss: 67.702568
Action reg: 0.003973
  l1.weight: grad_norm = 0.060671
  l1.bias: grad_norm = 0.000875
  l2.weight: grad_norm = 0.140397
Total gradient norm: 0.332246
=== Actor Training Debug (Iteration 5054) ===
Q mean: -67.645134
Q std: 22.287298
Actor loss: 67.649124
Action reg: 0.003987
  l1.weight: grad_norm = 0.045637
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.132948
Total gradient norm: 0.221493
=== Actor Training Debug (Iteration 5055) ===
Q mean: -68.074310
Q std: 24.096930
Actor loss: 68.078285
Action reg: 0.003972
  l1.weight: grad_norm = 0.084959
  l1.bias: grad_norm = 0.001122
  l2.weight: grad_norm = 0.160860
Total gradient norm: 0.314860
=== Actor Training Debug (Iteration 5056) ===
Q mean: -66.697952
Q std: 23.533457
Actor loss: 66.701920
Action reg: 0.003971
  l1.weight: grad_norm = 0.054762
  l1.bias: grad_norm = 0.000938
  l2.weight: grad_norm = 0.130522
Total gradient norm: 0.225391
=== Actor Training Debug (Iteration 5057) ===
Q mean: -66.568756
Q std: 21.816851
Actor loss: 66.572754
Action reg: 0.003998
  l1.weight: grad_norm = 0.028904
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.067700
Total gradient norm: 0.128668
=== Actor Training Debug (Iteration 5058) ===
Q mean: -67.177895
Q std: 22.848513
Actor loss: 67.181862
Action reg: 0.003970
  l1.weight: grad_norm = 0.060742
  l1.bias: grad_norm = 0.000743
  l2.weight: grad_norm = 0.137241
Total gradient norm: 0.227906
=== Actor Training Debug (Iteration 5059) ===
Q mean: -67.738281
Q std: 22.006498
Actor loss: 67.742264
Action reg: 0.003982
  l1.weight: grad_norm = 0.019323
  l1.bias: grad_norm = 0.000854
  l2.weight: grad_norm = 0.043718
Total gradient norm: 0.082869
=== Actor Training Debug (Iteration 5060) ===
Q mean: -64.943947
Q std: 21.839474
Actor loss: 64.947922
Action reg: 0.003978
  l1.weight: grad_norm = 0.246689
  l1.bias: grad_norm = 0.000507
  l2.weight: grad_norm = 0.671343
Total gradient norm: 1.220037
=== Actor Training Debug (Iteration 5061) ===
Q mean: -64.347206
Q std: 22.926968
Actor loss: 64.351189
Action reg: 0.003982
  l1.weight: grad_norm = 0.095059
  l1.bias: grad_norm = 0.000481
  l2.weight: grad_norm = 0.198193
Total gradient norm: 0.352519
=== Actor Training Debug (Iteration 5062) ===
Q mean: -68.466812
Q std: 20.567659
Actor loss: 68.470802
Action reg: 0.003994
  l1.weight: grad_norm = 0.009533
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.023209
Total gradient norm: 0.048294
=== Actor Training Debug (Iteration 5063) ===
Q mean: -63.273212
Q std: 24.243969
Actor loss: 63.277168
Action reg: 0.003958
  l1.weight: grad_norm = 0.100885
  l1.bias: grad_norm = 0.001342
  l2.weight: grad_norm = 0.258353
Total gradient norm: 0.544637
=== Actor Training Debug (Iteration 5064) ===
Q mean: -68.779106
Q std: 21.228256
Actor loss: 68.783104
Action reg: 0.003999
  l1.weight: grad_norm = 0.028485
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.075049
Total gradient norm: 0.138100
=== Actor Training Debug (Iteration 5065) ===
Q mean: -68.149429
Q std: 20.143412
Actor loss: 68.153419
Action reg: 0.003994
  l1.weight: grad_norm = 0.115464
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.298962
Total gradient norm: 0.574006
=== Actor Training Debug (Iteration 5066) ===
Q mean: -68.127350
Q std: 22.581535
Actor loss: 68.131340
Action reg: 0.003990
  l1.weight: grad_norm = 0.098035
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.273095
Total gradient norm: 0.520562
=== Actor Training Debug (Iteration 5067) ===
Q mean: -65.975677
Q std: 22.793028
Actor loss: 65.979645
Action reg: 0.003971
  l1.weight: grad_norm = 0.235258
  l1.bias: grad_norm = 0.000795
  l2.weight: grad_norm = 0.513764
Total gradient norm: 0.780051
=== Actor Training Debug (Iteration 5068) ===
Q mean: -66.233734
Q std: 24.193954
Actor loss: 66.237686
Action reg: 0.003955
  l1.weight: grad_norm = 0.054963
  l1.bias: grad_norm = 0.001450
  l2.weight: grad_norm = 0.135767
Total gradient norm: 0.271402
=== Actor Training Debug (Iteration 5069) ===
Q mean: -68.828117
Q std: 20.265686
Actor loss: 68.832100
Action reg: 0.003986
  l1.weight: grad_norm = 0.144111
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.347680
Total gradient norm: 0.635757
=== Actor Training Debug (Iteration 5070) ===
Q mean: -65.788803
Q std: 22.244648
Actor loss: 65.792786
Action reg: 0.003980
  l1.weight: grad_norm = 0.048939
  l1.bias: grad_norm = 0.000635
  l2.weight: grad_norm = 0.125764
Total gradient norm: 0.223729
=== Actor Training Debug (Iteration 5071) ===
Q mean: -68.453690
Q std: 22.122587
Actor loss: 68.457680
Action reg: 0.003989
  l1.weight: grad_norm = 0.153337
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.399093
Total gradient norm: 0.818034
=== Actor Training Debug (Iteration 5072) ===
Q mean: -67.412300
Q std: 22.613569
Actor loss: 67.416267
Action reg: 0.003969
  l1.weight: grad_norm = 0.021125
  l1.bias: grad_norm = 0.001083
  l2.weight: grad_norm = 0.050322
Total gradient norm: 0.086972
=== Actor Training Debug (Iteration 5073) ===
Q mean: -64.700737
Q std: 23.746531
Actor loss: 64.704712
Action reg: 0.003973
  l1.weight: grad_norm = 0.036246
  l1.bias: grad_norm = 0.000951
  l2.weight: grad_norm = 0.097568
Total gradient norm: 0.193864
=== Actor Training Debug (Iteration 5074) ===
Q mean: -66.652756
Q std: 22.734224
Actor loss: 66.656746
Action reg: 0.003990
  l1.weight: grad_norm = 0.048139
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.126870
Total gradient norm: 0.274153
Total gradient norm: 0.0463414696on 1203) ===
=== Actor Training Debug (Iteration 5085) ===
Q mean: -67.180984
Q std: 22.889303
Actor loss: 67.184967
Action reg: 0.003980
  l1.weight: grad_norm = 0.133058
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.270148
Total gradient norm: 0.519404
=== Actor Training Debug (Iteration 5086) ===
Q mean: -64.851700
Q std: 22.140026
Actor loss: 64.855667
Action reg: 0.003971
  l1.weight: grad_norm = 0.121647
  l1.bias: grad_norm = 0.000893
  l2.weight: grad_norm = 0.256590
Total gradient norm: 0.468715
=== Actor Training Debug (Iteration 5087) ===
Q mean: -68.204292
Q std: 21.097466
Actor loss: 68.208260
Action reg: 0.003964
  l1.weight: grad_norm = 0.284889
  l1.bias: grad_norm = 0.001432
  l2.weight: grad_norm = 0.644192
Total gradient norm: 1.243939
=== Actor Training Debug (Iteration 5088) ===
Q mean: -65.315475
Q std: 22.369841
Actor loss: 65.319450
Action reg: 0.003973
  l1.weight: grad_norm = 0.151703
  l1.bias: grad_norm = 0.000961
  l2.weight: grad_norm = 0.341615
Total gradient norm: 0.692453
=== Actor Training Debug (Iteration 5089) ===
Q mean: -66.804993
Q std: 22.770638
Actor loss: 66.808968
Action reg: 0.003977
  l1.weight: grad_norm = 0.159897
  l1.bias: grad_norm = 0.000609
  l2.weight: grad_norm = 0.358539
Total gradient norm: 0.586432
=== Actor Training Debug (Iteration 5090) ===
Q mean: -67.591255
Q std: 22.616192
Actor loss: 67.595245
Action reg: 0.003988
  l1.weight: grad_norm = 0.058334
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.159324
Total gradient norm: 0.327252
=== Actor Training Debug (Iteration 5091) ===
Q mean: -65.770340
Q std: 21.250933
Actor loss: 65.774330
Action reg: 0.003989
  l1.weight: grad_norm = 0.151257
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.346143
Total gradient norm: 0.754887
=== Actor Training Debug (Iteration 5092) ===
Q mean: -63.678043
Q std: 21.698856
Actor loss: 63.682026
Action reg: 0.003981
  l1.weight: grad_norm = 0.111040
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.249221
Total gradient norm: 0.452274
=== Actor Training Debug (Iteration 5093) ===
Q mean: -66.996384
Q std: 22.692575
Actor loss: 67.000366
Action reg: 0.003980
  l1.weight: grad_norm = 0.037696
  l1.bias: grad_norm = 0.000613
  l2.weight: grad_norm = 0.089700
Total gradient norm: 0.132942
=== Actor Training Debug (Iteration 5094) ===
Q mean: -67.978043
Q std: 23.487514
Actor loss: 67.982025
Action reg: 0.003983
  l1.weight: grad_norm = 0.227892
  l1.bias: grad_norm = 0.000703
  l2.weight: grad_norm = 0.659860
Total gradient norm: 1.073927
=== Actor Training Debug (Iteration 5095) ===
Q mean: -66.102440
Q std: 22.136957
Actor loss: 66.106415
Action reg: 0.003973
  l1.weight: grad_norm = 0.360378
  l1.bias: grad_norm = 0.000864
  l2.weight: grad_norm = 1.066742
Total gradient norm: 1.978764
=== Actor Training Debug (Iteration 5096) ===
Q mean: -65.043640
Q std: 21.465700
Actor loss: 65.047623
Action reg: 0.003986
  l1.weight: grad_norm = 0.098734
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.253262
Total gradient norm: 0.480000
=== Actor Training Debug (Iteration 5097) ===
Q mean: -65.009766
Q std: 20.187624
Actor loss: 65.013756
Action reg: 0.003987
  l1.weight: grad_norm = 0.013738
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.036104
Total gradient norm: 0.081624
=== Actor Training Debug (Iteration 5098) ===
Q mean: -68.644592
Q std: 23.104679
Actor loss: 68.648582
Action reg: 0.003987
  l1.weight: grad_norm = 0.238347
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.631019
Total gradient norm: 1.036878
=== Actor Training Debug (Iteration 5099) ===
Q mean: -68.094833
Q std: 22.901445
Actor loss: 68.098801
Action reg: 0.003967
  l1.weight: grad_norm = 0.400130
  l1.bias: grad_norm = 0.001143
  l2.weight: grad_norm = 1.014875
Total gradient norm: 1.903925
=== Actor Training Debug (Iteration 5100) ===
Q mean: -67.663345
Q std: 22.022017
Actor loss: 67.667328
Action reg: 0.003982
  l1.weight: grad_norm = 0.119505
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.307199
Total gradient norm: 0.567413
Episode 101: Steps=100, Reward=-264.883, Buffer_size=10100
=== Actor Training Debug (Iteration 5101) ===
Q mean: -68.032440
Q std: 22.839870
Actor loss: 68.036423
Action reg: 0.003984
  l1.weight: grad_norm = 0.400715
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 1.029178
Total gradient norm: 1.754558
=== Actor Training Debug (Iteration 5102) ===
Q mean: -66.342514
Q std: 22.909292
Actor loss: 66.346489
Action reg: 0.003974
  l1.weight: grad_norm = 0.374430
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 1.133484
Total gradient norm: 2.001111
=== Actor Training Debug (Iteration 5103) ===
Q mean: -66.855232
Q std: 25.142500
Actor loss: 66.859177
Action reg: 0.003947
  l1.weight: grad_norm = 0.493631
  l1.bias: grad_norm = 0.001919
  l2.weight: grad_norm = 1.261679
Total gradient norm: 2.317540
=== Actor Training Debug (Iteration 5104) ===
Q mean: -68.351685
Q std: 22.700846
Actor loss: 68.355675
Action reg: 0.003987
  l1.weight: grad_norm = 0.088128
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.208970
Total gradient norm: 0.315573
=== Actor Training Debug (Iteration 5105) ===
Q mean: -66.449242
Q std: 21.978218
Actor loss: 66.453224
Action reg: 0.003983
  l1.weight: grad_norm = 0.004286
  l1.bias: grad_norm = 0.000565
  l2.weight: grad_norm = 0.011159
Total gradient norm: 0.024242
=== Actor Training Debug (Iteration 5106) ===
Q mean: -66.508133
Q std: 21.911676
Actor loss: 66.512108
Action reg: 0.003975
  l1.weight: grad_norm = 0.676829
  l1.bias: grad_norm = 0.000533
  l2.weight: grad_norm = 1.880805
Total gradient norm: 3.064332
=== Actor Training Debug (Iteration 5107) ===
Q mean: -69.241112
Q std: 22.070189
Actor loss: 69.245087
Action reg: 0.003978
  l1.weight: grad_norm = 0.081624
  l1.bias: grad_norm = 0.000623
  l2.weight: grad_norm = 0.198370
Total gradient norm: 0.364953
=== Actor Training Debug (Iteration 5108) ===
Q mean: -69.502213
Q std: 22.090004
Actor loss: 69.506195
Action reg: 0.003985
  l1.weight: grad_norm = 0.277567
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.775928
Total gradient norm: 1.219346
=== Actor Training Debug (Iteration 5109) ===
Q mean: -67.303223
Q std: 21.779041
Actor loss: 67.307205
Action reg: 0.003983
  l1.weight: grad_norm = 0.068529
  l1.bias: grad_norm = 0.000596
  l2.weight: grad_norm = 0.166491
Total gradient norm: 0.364119
=== Actor Training Debug (Iteration 5110) ===
Q mean: -65.991455
Q std: 24.025019
Actor loss: 65.995407
Action reg: 0.003953
  l1.weight: grad_norm = 0.138347
  l1.bias: grad_norm = 0.001316
  l2.weight: grad_norm = 0.302809
Total gradient norm: 0.580301
=== Actor Training Debug (Iteration 5111) ===
Q mean: -66.688095
Q std: 22.347767
Actor loss: 66.692062
Action reg: 0.003971
  l1.weight: grad_norm = 0.202309
  l1.bias: grad_norm = 0.000637
  l2.weight: grad_norm = 0.499102
Total gradient norm: 0.947464
=== Actor Training Debug (Iteration 5112) ===
Q mean: -67.809616
Q std: 21.903320
Actor loss: 67.813606
Action reg: 0.003988
  l1.weight: grad_norm = 0.056337
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.141714
Total gradient norm: 0.287661
=== Actor Training Debug (Iteration 5113) ===
Q mean: -71.315689
Q std: 22.372444
Actor loss: 71.319672
Action reg: 0.003985
  l1.weight: grad_norm = 0.090702
  l1.bias: grad_norm = 0.000724
  l2.weight: grad_norm = 0.209218
Total gradient norm: 0.427215
=== Actor Training Debug (Iteration 5114) ===
Q mean: -66.518486
Q std: 23.148123
Actor loss: 66.522476
Action reg: 0.003989
  l1.weight: grad_norm = 0.305618
  l1.bias: grad_norm = 0.000714
  l2.weight: grad_norm = 0.757264
Total gradient norm: 1.280757
=== Actor Training Debug (Iteration 5115) ===
Q mean: -66.867554
Q std: 23.139648
Actor loss: 66.871544
Action reg: 0.003989
  l1.weight: grad_norm = 0.167561
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.370745
Total gradient norm: 0.683285
=== Actor Training Debug (Iteration 5116) ===
Q mean: -67.986786
Q std: 23.009089
Actor loss: 67.990761
Action reg: 0.003975
  l1.weight: grad_norm = 0.118276
  l1.bias: grad_norm = 0.000854
  l2.weight: grad_norm = 0.270868
Total gradient norm: 0.475050
=== Actor Training Debug (Iteration 5117) ===
Q mean: -66.535370
Q std: 22.003050
Actor loss: 66.539345
Action reg: 0.003978
  l1.weight: grad_norm = 0.250559
  l1.bias: grad_norm = 0.000887
  l2.weight: grad_norm = 0.696789
Total gradient norm: 1.207577
=== Actor Training Debug (Iteration 5118) ===
Q mean: -66.719368
Q std: 22.596136
Actor loss: 66.723328
Action reg: 0.003956
  l1.weight: grad_norm = 0.340242
  l1.bias: grad_norm = 0.001311
  l2.weight: grad_norm = 1.078697
Total gradient norm: 2.266986
=== Actor Training Debug (Iteration 5119) ===
Q mean: -67.346756
Q std: 22.949329
Actor loss: 67.350739
Action reg: 0.003982
  l1.weight: grad_norm = 0.191792
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.485474
Total gradient norm: 0.765075
=== Actor Training Debug (Iteration 5120) ===
Q mean: -68.288956
Q std: 22.532127
Actor loss: 68.292931
Action reg: 0.003978
  l1.weight: grad_norm = 0.301414
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.824188
Total gradient norm: 1.529275
=== Actor Training Debug (Iteration 5121) ===
Q mean: -69.379089
Q std: 22.208961
Actor loss: 69.383080
Action reg: 0.003993
  l1.weight: grad_norm = 0.060961
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.139378
Total gradient norm: 0.241624
=== Actor Training Debug (Iteration 5122) ===
Q mean: -66.075500
Q std: 22.764620
Actor loss: 66.079475
Action reg: 0.003972
  l1.weight: grad_norm = 0.040050
  l1.bias: grad_norm = 0.000842
  l2.weight: grad_norm = 0.108293
Total gradient norm: 0.183611
=== Actor Training Debug (Iteration 5123) ===
Q mean: -68.224319
Q std: 20.465645
Actor loss: 68.228310
Action reg: 0.003990
  l1.weight: grad_norm = 0.137123
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.307274
Total gradient norm: 0.515376
=== Actor Training Debug (Iteration 5124) ===
Q mean: -70.631958
Q std: 21.488691
Actor loss: 70.635948
Action reg: 0.003988
  l1.weight: grad_norm = 0.098458
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.264607
Total gradient norm: 0.491472
=== Actor Training Debug (Iteration 5125) ===
Q mean: -67.384163
Q std: 23.304598
Actor loss: 67.388153
Action reg: 0.003987
  l1.weight: grad_norm = 0.018225
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.038531
Total gradient norm: 0.069772
=== Actor Training Debug (Iteration 5126) ===
Q mean: -69.425095
Q std: 23.604847
Actor loss: 69.429070
Action reg: 0.003978
  l1.weight: grad_norm = 0.176059
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.493253
Total gradient norm: 0.981434
=== Actor Training Debug (Iteration 5127) ===
Q mean: -68.314514
Q std: 23.685986
Actor loss: 68.318489
Action reg: 0.003974
  l1.weight: grad_norm = 0.135357
  l1.bias: grad_norm = 0.000801
  l2.weight: grad_norm = 0.338885
Total gradient norm: 0.589948
=== Actor Training Debug (Iteration 5128) ===
Q mean: -68.376129
Q std: 23.551411
Actor loss: 68.380112
Action reg: 0.003981
  l1.weight: grad_norm = 0.017257
  l1.bias: grad_norm = 0.000856
  l2.weight: grad_norm = 0.033806
Total gradient norm: 0.062515
=== Actor Training Debug (Iteration 5129) ===
Q mean: -68.051773
Q std: 23.556185
Actor loss: 68.055733
Action reg: 0.003957
  l1.weight: grad_norm = 0.175569
  l1.bias: grad_norm = 0.001385
  l2.weight: grad_norm = 0.401276
Total gradient norm: 0.744638
=== Actor Training Debug (Iteration 5130) ===
Q mean: -68.128708
Q std: 23.256578
Actor loss: 68.132690
Action reg: 0.003979
  l1.weight: grad_norm = 0.034254
  l1.bias: grad_norm = 0.000932
  l2.weight: grad_norm = 0.098550
Total gradient norm: 0.202565
=== Actor Training Debug (Iteration 5131) ===
Q mean: -63.159138
Q std: 24.577421
Actor loss: 63.163097
Action reg: 0.003961
  l1.weight: grad_norm = 0.075024
  l1.bias: grad_norm = 0.001228
  l2.weight: grad_norm = 0.214757
Total gradient norm: 0.442832
=== Actor Training Debug (Iteration 5132) ===
Q mean: -67.237289
Q std: 23.723347
Actor loss: 67.241280
Action reg: 0.003988
  l1.weight: grad_norm = 0.198997
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.434815
Total gradient norm: 0.712770
=== Actor Training Debug (Iteration 5133) ===
Q mean: -66.823425
Q std: 21.943089
Actor loss: 66.827415
Action reg: 0.003989
  l1.weight: grad_norm = 0.114948
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.308348
Total gradient norm: 0.541129
=== Actor Training Debug (Iteration 5134) ===
Q mean: -68.507668
Q std: 22.315825
Actor loss: 68.511642
Action reg: 0.003977
  l1.weight: grad_norm = 0.073369
  l1.bias: grad_norm = 0.000883
  l2.weight: grad_norm = 0.186713
Total gradient norm: 0.339466
=== Actor Training Debug (Iteration 5135) ===
Q mean: -66.468887
Q std: 21.121803
Actor loss: 66.472862
Action reg: 0.003978
  l1.weight: grad_norm = 0.094913
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.211674
Total gradient norm: 0.382584
=== Actor Training Debug (Iteration 5136) ===
Q mean: -68.561996
Q std: 22.416250
Actor loss: 68.565971
Action reg: 0.003975
  l1.weight: grad_norm = 0.079498
  l1.bias: grad_norm = 0.000684
  l2.weight: grad_norm = 0.169875
Total gradient norm: 0.299252
=== Actor Training Debug (Iteration 5137) ===
Q mean: -66.715240
Q std: 22.108171
Actor loss: 66.719231
Action reg: 0.003987
  l1.weight: grad_norm = 0.097014
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.252968
Total gradient norm: 0.448245
=== Actor Training Debug (Iteration 5138) ===
Q mean: -68.472359
Q std: 23.197285
Actor loss: 68.476341
Action reg: 0.003986
  l1.weight: grad_norm = 0.192083
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.488004
Total gradient norm: 0.958972
=== Actor Training Debug (Iteration 5139) ===
Q mean: -68.339897
Q std: 23.593435
Actor loss: 68.343887
Action reg: 0.003990
  l1.weight: grad_norm = 0.116450
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.274173
Total gradient norm: 0.506138
=== Actor Training Debug (Iteration 5140) ===
Q mean: -65.898598
Q std: 23.433029
Actor loss: 65.902573
Action reg: 0.003976
  l1.weight: grad_norm = 0.042779
  l1.bias: grad_norm = 0.000856
  l2.weight: grad_norm = 0.098238
Total gradient norm: 0.177904
=== Actor Training Debug (Iteration 5141) ===
Q mean: -65.853600
Q std: 22.661343
Actor loss: 65.857582
Action reg: 0.003983
  l1.weight: grad_norm = 0.144968
  l1.bias: grad_norm = 0.000691
  l2.weight: grad_norm = 0.383803
Total gradient norm: 0.585255
=== Actor Training Debug (Iteration 5142) ===
Q mean: -67.219170
Q std: 22.561852
Actor loss: 67.223145
Action reg: 0.003975
  l1.weight: grad_norm = 0.157742
  l1.bias: grad_norm = 0.000519
  l2.weight: grad_norm = 0.399632
Total gradient norm: 0.637707
=== Actor Training Debug (Iteration 5143) ===
Q mean: -66.778610
Q std: 24.242945
Actor loss: 66.782593
Action reg: 0.003979
  l1.weight: grad_norm = 0.055273
  l1.bias: grad_norm = 0.000712
  l2.weight: grad_norm = 0.137996
Total gradient norm: 0.224714
=== Actor Training Debug (Iteration 5144) ===
Q mean: -67.611053
Q std: 23.760107
Actor loss: 67.615036
Action reg: 0.003979
  l1.weight: grad_norm = 0.115365
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.345836
Total gradient norm: 0.711375
=== Actor Training Debug (Iteration 5145) ===
Q mean: -63.528812
Q std: 23.127712
Actor loss: 63.532806
Action reg: 0.003993
  l1.weight: grad_norm = 0.217706
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.655301
Total gradient norm: 1.213964
=== Actor Training Debug (Iteration 5146) ===
Q mean: -66.521149
Q std: 22.820457
Actor loss: 66.525131
Action reg: 0.003984
  l1.weight: grad_norm = 0.031635
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.066036
Total gradient norm: 0.132925
=== Actor Training Debug (Iteration 5147) ===
Q mean: -68.868713
Q std: 21.730270
Actor loss: 68.872704
Action reg: 0.003987
  l1.weight: grad_norm = 0.029100
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.080853
Total gradient norm: 0.146848
=== Actor Training Debug (Iteration 5148) ===
Q mean: -70.625450
Q std: 21.477657
Actor loss: 70.629440
Action reg: 0.003991
  l1.weight: grad_norm = 0.144406
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.344302
Total gradient norm: 0.624896
=== Actor Training Debug (Iteration 5149) ===
Q mean: -63.651459
Q std: 23.688349
Actor loss: 63.655445
Action reg: 0.003987
  l1.weight: grad_norm = 0.086604
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.214101
Total gradient norm: 0.453801
=== Actor Training Debug (Iteration 5150) ===
Q mean: -65.697220
Q std: 22.428112
Actor loss: 65.701195
Action reg: 0.003978
  l1.weight: grad_norm = 0.136273
  l1.bias: grad_norm = 0.000734
  l2.weight: grad_norm = 0.317053
Total gradient norm: 0.595032
=== Actor Training Debug (Iteration 5151) ===
Q mean: -65.216423
Q std: 23.109777
Actor loss: 65.220390
Action reg: 0.003970
  l1.weight: grad_norm = 0.067188
  l1.bias: grad_norm = 0.000943
  l2.weight: grad_norm = 0.130740
Total gradient norm: 0.203030
=== Actor Training Debug (Iteration 5152) ===
Q mean: -66.991638
Q std: 23.578184
Actor loss: 66.995605
Action reg: 0.003968
  l1.weight: grad_norm = 0.046573
  l1.bias: grad_norm = 0.001041
  l2.weight: grad_norm = 0.118149
Total gradient norm: 0.267234
=== Actor Training Debug (Iteration 5153) ===
Q mean: -69.100662
Q std: 21.827181
Actor loss: 69.104637
Action reg: 0.003979
  l1.weight: grad_norm = 0.292346
  l1.bias: grad_norm = 0.000611
  l2.weight: grad_norm = 0.853543
Total gradient norm: 1.460504
=== Actor Training Debug (Iteration 5154) ===
Q mean: -65.369171
Q std: 23.195137
Actor loss: 65.373161
Action reg: 0.003993
  l1.weight: grad_norm = 0.053023
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.113239
Total gradient norm: 0.197470
=== Actor Training Debug (Iteration 5155) ===
Q mean: -66.167068
Q std: 22.327913
Actor loss: 66.171051
Action reg: 0.003979
  l1.weight: grad_norm = 0.170418
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.491918
Total gradient norm: 0.945175
=== Actor Training Debug (Iteration 5156) ===
Q mean: -70.699532
Q std: 22.888737
Actor loss: 70.703514
Action reg: 0.003984
  l1.weight: grad_norm = 0.174856
  l1.bias: grad_norm = 0.000594
  l2.weight: grad_norm = 0.403747
Total gradient norm: 0.705155
=== Actor Training Debug (Iteration 5157) ===
Q mean: -70.073357
Q std: 22.216848
Actor loss: 70.077339
Action reg: 0.003985
  l1.weight: grad_norm = 0.035172
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.097441
Total gradient norm: 0.174131
=== Actor Training Debug (Iteration 5158) ===
Q mean: -68.175598
Q std: 22.029430
Actor loss: 68.179588
Action reg: 0.003991
  l1.weight: grad_norm = 0.043231
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.117527
Total gradient norm: 0.236891
=== Actor Training Debug (Iteration 5159) ===
Q mean: -65.096100
Q std: 22.092516
Actor loss: 65.100082
Action reg: 0.003981
  l1.weight: grad_norm = 0.126739
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.362112
Total gradient norm: 0.702847
=== Actor Training Debug (Iteration 5160) ===
Q mean: -67.290123
Q std: 21.633102
Actor loss: 67.294098
Action reg: 0.003978
  l1.weight: grad_norm = 0.052735
  l1.bias: grad_norm = 0.000726
  l2.weight: grad_norm = 0.129325
Total gradient norm: 0.279660
=== Actor Training Debug (Iteration 5161) ===
Q mean: -68.839165
Q std: 22.138994
Actor loss: 68.843147
Action reg: 0.003983
  l1.weight: grad_norm = 0.154848
  l1.bias: grad_norm = 0.000633
  l2.weight: grad_norm = 0.444504
Total gradient norm: 0.820697
=== Actor Training Debug (Iteration 5162) ===
Q mean: -65.329346
Q std: 23.553541
Actor loss: 65.333321
Action reg: 0.003975
  l1.weight: grad_norm = 0.047437
  l1.bias: grad_norm = 0.000560
  l2.weight: grad_norm = 0.135823
Total gradient norm: 0.313376
=== Actor Training Debug (Iteration 5163) ===
Q mean: -65.455948
Q std: 22.906425
Actor loss: 65.459930
Action reg: 0.003981
  l1.weight: grad_norm = 0.248137
  l1.bias: grad_norm = 0.000846
  l2.weight: grad_norm = 0.525168
Total gradient norm: 0.894716
=== Actor Training Debug (Iteration 5164) ===
Q mean: -66.993179
Q std: 22.598030
Actor loss: 66.997169
Action reg: 0.003988
  l1.weight: grad_norm = 0.010276
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.024843
Total gradient norm: 0.041288
=== Actor Training Debug (Iteration 5165) ===
Q mean: -66.698814
Q std: 24.837221
Actor loss: 66.702759
Action reg: 0.003947
  l1.weight: grad_norm = 0.625085
  l1.bias: grad_norm = 0.001604
  l2.weight: grad_norm = 1.605709
Total gradient norm: 3.395555
=== Actor Training Debug (Iteration 5166) ===
Q mean: -64.794662
Q std: 22.696901
Actor loss: 64.798645
Action reg: 0.003980
  l1.weight: grad_norm = 0.090187
  l1.bias: grad_norm = 0.000642
  l2.weight: grad_norm = 0.181218
Total gradient norm: 0.310185
=== Actor Training Debug (Iteration 5167) ===
Q mean: -66.405426
Q std: 22.055698
Actor loss: 66.409401
Action reg: 0.003978
  l1.weight: grad_norm = 0.340279
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.831805
Total gradient norm: 1.629579
=== Actor Training Debug (Iteration 5168) ===
Q mean: -69.800323
Q std: 22.053555
Actor loss: 69.804306
Action reg: 0.003980
  l1.weight: grad_norm = 0.294379
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.859422
Total gradient norm: 1.589476
=== Actor Training Debug (Iteration 5169) ===
Q mean: -68.163681
Q std: 20.965721
Actor loss: 68.167664
Action reg: 0.003982
  l1.weight: grad_norm = 0.303202
  l1.bias: grad_norm = 0.000637
  l2.weight: grad_norm = 0.771749
Total gradient norm: 1.487750
=== Actor Training Debug (Iteration 5170) ===
Q mean: -67.477142
Q std: 21.913929
Actor loss: 67.481133
Action reg: 0.003987
  l1.weight: grad_norm = 0.042705
  l1.bias: grad_norm = 0.000481
  l2.weight: grad_norm = 0.113334
Total gradient norm: 0.178184
=== Actor Training Debug (Iteration 5171) ===
Q mean: -68.859161
Q std: 21.727140
Actor loss: 68.863152
Action reg: 0.003994
  l1.weight: grad_norm = 0.008596
  l1.bias: grad_norm = 0.000476
  l2.weight: grad_norm = 0.021852
Total gradient norm: 0.039422
=== Actor Training Debug (Iteration 5172) ===
Q mean: -66.663452
Q std: 24.118126
Actor loss: 66.667412
Action reg: 0.003958
  l1.weight: grad_norm = 0.241519
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.571099
Total gradient norm: 1.021672
=== Actor Training Debug (Iteration 5173) ===
Q mean: -70.153076
Q std: 21.788755
Actor loss: 70.157074
Action reg: 0.003996
  l1.weight: grad_norm = 0.163556
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.402042
Total gradient norm: 0.705405
=== Actor Training Debug (Iteration 5174) ===
Q mean: -68.722290
Q std: 22.847935
Actor loss: 68.726273
Action reg: 0.003982
  l1.weight: grad_norm = 0.329918
  l1.bias: grad_norm = 0.000378
  l2.weight: grad_norm = 0.759108
Total gradient norm: 1.333193
=== Actor Training Debug (Iteration 5175) ===
Q mean: -64.804291
Q std: 23.143770
Actor loss: 64.808266
Action reg: 0.003977
  l1.weight: grad_norm = 0.290230
  l1.bias: grad_norm = 0.000739
  l2.weight: grad_norm = 0.636119
Total gradient norm: 1.478107
=== Actor Training Debug (Iteration 5176) ===
Q mean: -70.161682
Q std: 22.673656
Actor loss: 70.165649
Action reg: 0.003967
  l1.weight: grad_norm = 1.042259
  l1.bias: grad_norm = 0.000917
  l2.weight: grad_norm = 2.673606
Total gradient norm: 5.121904
=== Actor Training Debug (Iteration 5177) ===
Q mean: -67.558983
Q std: 21.674017
Actor loss: 67.562973
Action reg: 0.003992
  l1.weight: grad_norm = 0.026691
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.064697
Total gradient norm: 0.127550
=== Actor Training Debug (Iteration 5178) ===
Q mean: -66.988190
Q std: 23.761225
Actor loss: 66.992149
Action reg: 0.003959
  l1.weight: grad_norm = 0.087506
  l1.bias: grad_norm = 0.001324
  l2.weight: grad_norm = 0.213747
Total gradient norm: 0.428292
=== Actor Training Debug (Iteration 5179) ===
Q mean: -69.905777
Q std: 23.557060
Actor loss: 69.909760
Action reg: 0.003986
  l1.weight: grad_norm = 0.100835
  l1.bias: grad_norm = 0.000623
  l2.weight: grad_norm = 0.220930
Total gradient norm: 0.397587
=== Actor Training Debug (Iteration 5180) ===
Q mean: -68.887840
Q std: 20.801359
Actor loss: 68.891815
Action reg: 0.003977
  l1.weight: grad_norm = 0.428070
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 1.037133
Total gradient norm: 1.990388
=== Actor Training Debug (Iteration 5181) ===
Q mean: -67.767319
Q std: 22.747818
Actor loss: 67.771309
Action reg: 0.003993
  l1.weight: grad_norm = 0.011029
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.025955
Total gradient norm: 0.057030
=== Actor Training Debug (Iteration 5182) ===
Q mean: -63.979214
Q std: 23.371555
Actor loss: 63.983196
Action reg: 0.003984
  l1.weight: grad_norm = 0.205830
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.497333
Total gradient norm: 1.027754
=== Actor Training Debug (Iteration 5183) ===
Q mean: -66.125259
Q std: 23.239204
Actor loss: 66.129227
Action reg: 0.003966
  l1.weight: grad_norm = 0.204754
  l1.bias: grad_norm = 0.001003
  l2.weight: grad_norm = 0.475079
Total gradient norm: 0.890318
=== Actor Training Debug (Iteration 5184) ===
Q mean: -67.680244
Q std: 23.314884
Actor loss: 67.684212
Action reg: 0.003964
  l1.weight: grad_norm = 0.180034
  l1.bias: grad_norm = 0.000757
  l2.weight: grad_norm = 0.548457
Total gradient norm: 1.368584
=== Actor Training Debug (Iteration 5185) ===
Q mean: -69.312408
Q std: 21.586161
Actor loss: 69.316399
Action reg: 0.003991
  l1.weight: grad_norm = 0.009318
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.018970
Total gradient norm: 0.030049
=== Actor Training Debug (Iteration 5186) ===
Q mean: -68.213730
Q std: 24.233080
Actor loss: 68.217697
Action reg: 0.003964
  l1.weight: grad_norm = 0.052108
  l1.bias: grad_norm = 0.001460
  l2.weight: grad_norm = 0.107559
Total gradient norm: 0.211002
=== Actor Training Debug (Iteration 5187) ===
Q mean: -68.777847
Q std: 21.816080
Actor loss: 68.781837
Action reg: 0.003989
  l1.weight: grad_norm = 0.009264
  l1.bias: grad_norm = 0.000540
  l2.weight: grad_norm = 0.018732
Total gradient norm: 0.036094
=== Actor Training Debug (Iteration 5188) ===
Q mean: -69.742447
Q std: 22.481117
Actor loss: 69.746429
Action reg: 0.003981
  l1.weight: grad_norm = 0.111535
  l1.bias: grad_norm = 0.000910
  l2.weight: grad_norm = 0.236646
Total gradient norm: 0.428375
=== Actor Training Debug (Iteration 5189) ===
Q mean: -68.107697
Q std: 22.481600
Actor loss: 68.111694
Action reg: 0.003995
  l1.weight: grad_norm = 0.070983
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.150463
Total gradient norm: 0.279580
=== Actor Training Debug (Iteration 5190) ===
Q mean: -66.476028
Q std: 23.319416
Actor loss: 66.480011
Action reg: 0.003982
  l1.weight: grad_norm = 0.010154
  l1.bias: grad_norm = 0.000642
  l2.weight: grad_norm = 0.022072
Total gradient norm: 0.041956
=== Actor Training Debug (Iteration 5191) ===
Q mean: -69.263092
Q std: 21.165989
Actor loss: 69.267082
Action reg: 0.003989
  l1.weight: grad_norm = 0.007932
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.019952
Total gradient norm: 0.040588
=== Actor Training Debug (Iteration 5192) ===
Q mean: -66.205109
Q std: 24.942364
Actor loss: 66.209061
Action reg: 0.003952
  l1.weight: grad_norm = 0.163798
  l1.bias: grad_norm = 0.001793
  l2.weight: grad_norm = 0.361158
Total gradient norm: 0.605669
=== Actor Training Debug (Iteration 5193) ===
Q mean: -67.280746
Q std: 21.528397
Actor loss: 67.284737
Action reg: 0.003987
  l1.weight: grad_norm = 0.274032
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.519743
Total gradient norm: 0.741537
=== Actor Training Debug (Iteration 5194) ===
Q mean: -71.372787
Q std: 22.025490
Actor loss: 71.376778
Action reg: 0.003992
  l1.weight: grad_norm = 0.204495
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.582563
Total gradient norm: 1.083524
=== Actor Training Debug (Iteration 5195) ===
Q mean: -68.760727
Q std: 23.466709
Q mean: -68.110954m: 0.0463414696on 1203) ===
Q std: 22.950121
Actor loss: 68.114937
Action reg: 0.003986
  l1.weight: grad_norm = 0.025112
  l1.bias: grad_norm = 0.000597
  l2.weight: grad_norm = 0.058878
Total gradient norm: 0.105254
=== Actor Training Debug (Iteration 5206) ===
Q mean: -68.194733
Q std: 22.925859
Actor loss: 68.198708
Action reg: 0.003974
  l1.weight: grad_norm = 0.060833
  l1.bias: grad_norm = 0.001445
  l2.weight: grad_norm = 0.163886
Total gradient norm: 0.352228
=== Actor Training Debug (Iteration 5207) ===
Q mean: -70.064682
Q std: 22.806816
Actor loss: 70.068665
Action reg: 0.003982
  l1.weight: grad_norm = 0.064516
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.148909
Total gradient norm: 0.293215
=== Actor Training Debug (Iteration 5208) ===
Q mean: -69.071838
Q std: 22.977203
Actor loss: 69.075813
Action reg: 0.003976
  l1.weight: grad_norm = 0.042512
  l1.bias: grad_norm = 0.001003
  l2.weight: grad_norm = 0.087600
Total gradient norm: 0.161021
=== Actor Training Debug (Iteration 5209) ===
Q mean: -67.818588
Q std: 23.543383
Actor loss: 67.822571
Action reg: 0.003982
  l1.weight: grad_norm = 0.014740
  l1.bias: grad_norm = 0.000656
  l2.weight: grad_norm = 0.037942
Total gradient norm: 0.080117
=== Actor Training Debug (Iteration 5210) ===
Q mean: -68.580788
Q std: 23.795280
Actor loss: 68.584770
Action reg: 0.003985
  l1.weight: grad_norm = 0.015826
  l1.bias: grad_norm = 0.000719
  l2.weight: grad_norm = 0.032926
Total gradient norm: 0.064554
=== Actor Training Debug (Iteration 5211) ===
Q mean: -65.638374
Q std: 23.480310
Actor loss: 65.642357
Action reg: 0.003980
  l1.weight: grad_norm = 0.070631
  l1.bias: grad_norm = 0.000710
  l2.weight: grad_norm = 0.138924
Total gradient norm: 0.293340
=== Actor Training Debug (Iteration 5212) ===
Q mean: -71.018585
Q std: 23.185833
Actor loss: 71.022568
Action reg: 0.003981
  l1.weight: grad_norm = 0.145048
  l1.bias: grad_norm = 0.000918
  l2.weight: grad_norm = 0.304223
Total gradient norm: 0.532355
=== Actor Training Debug (Iteration 5213) ===
Q mean: -68.950714
Q std: 23.859985
Actor loss: 68.954674
Action reg: 0.003958
  l1.weight: grad_norm = 0.276986
  l1.bias: grad_norm = 0.001647
  l2.weight: grad_norm = 0.636382
Total gradient norm: 0.981090
=== Actor Training Debug (Iteration 5214) ===
Q mean: -68.363617
Q std: 23.427591
Actor loss: 68.367615
Action reg: 0.003998
  l1.weight: grad_norm = 0.025773
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.060311
Total gradient norm: 0.106767
=== Actor Training Debug (Iteration 5215) ===
Q mean: -68.998489
Q std: 23.940784
Actor loss: 69.002472
Action reg: 0.003983
  l1.weight: grad_norm = 0.098226
  l1.bias: grad_norm = 0.000782
  l2.weight: grad_norm = 0.272493
Total gradient norm: 0.508293
=== Actor Training Debug (Iteration 5216) ===
Q mean: -66.210564
Q std: 22.061739
Actor loss: 66.214546
Action reg: 0.003984
  l1.weight: grad_norm = 0.086194
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.200119
Total gradient norm: 0.384180
=== Actor Training Debug (Iteration 5217) ===
Q mean: -68.354767
Q std: 21.770977
Actor loss: 68.358749
Action reg: 0.003980
  l1.weight: grad_norm = 0.026340
  l1.bias: grad_norm = 0.000856
  l2.weight: grad_norm = 0.074180
Total gradient norm: 0.146287
=== Actor Training Debug (Iteration 5218) ===
Q mean: -68.428780
Q std: 21.706238
Actor loss: 68.432777
Action reg: 0.003994
  l1.weight: grad_norm = 0.005853
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.014474
Total gradient norm: 0.029829
=== Actor Training Debug (Iteration 5219) ===
Q mean: -68.957764
Q std: 23.528290
Actor loss: 68.961754
Action reg: 0.003989
  l1.weight: grad_norm = 0.104425
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.247286
Total gradient norm: 0.454948
=== Actor Training Debug (Iteration 5220) ===
Q mean: -67.467789
Q std: 23.544386
Actor loss: 67.471779
Action reg: 0.003989
  l1.weight: grad_norm = 0.102834
  l1.bias: grad_norm = 0.000533
  l2.weight: grad_norm = 0.260091
Total gradient norm: 0.557004
=== Actor Training Debug (Iteration 5221) ===
Q mean: -67.399933
Q std: 21.892178
Actor loss: 67.403923
Action reg: 0.003992
  l1.weight: grad_norm = 0.138223
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.353213
Total gradient norm: 0.672487
=== Actor Training Debug (Iteration 5222) ===
Q mean: -68.470436
Q std: 23.219517
Actor loss: 68.474419
Action reg: 0.003980
  l1.weight: grad_norm = 0.061952
  l1.bias: grad_norm = 0.000782
  l2.weight: grad_norm = 0.164114
Total gradient norm: 0.298274
=== Actor Training Debug (Iteration 5223) ===
Q mean: -67.270996
Q std: 22.035892
Actor loss: 67.274986
Action reg: 0.003993
  l1.weight: grad_norm = 0.024677
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.069674
Total gradient norm: 0.122110
=== Actor Training Debug (Iteration 5224) ===
Q mean: -68.366447
Q std: 24.388288
Actor loss: 68.370430
Action reg: 0.003982
  l1.weight: grad_norm = 0.387672
  l1.bias: grad_norm = 0.000706
  l2.weight: grad_norm = 0.959957
Total gradient norm: 1.699303
=== Actor Training Debug (Iteration 5225) ===
Q mean: -66.795288
Q std: 23.262897
Actor loss: 66.799271
Action reg: 0.003984
  l1.weight: grad_norm = 0.080737
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.174901
Total gradient norm: 0.320367
=== Actor Training Debug (Iteration 5226) ===
Q mean: -71.013596
Q std: 22.208477
Actor loss: 71.017578
Action reg: 0.003986
  l1.weight: grad_norm = 0.053750
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.142727
Total gradient norm: 0.254172
=== Actor Training Debug (Iteration 5227) ===
Q mean: -69.592216
Q std: 22.896063
Actor loss: 69.596199
Action reg: 0.003985
  l1.weight: grad_norm = 0.086546
  l1.bias: grad_norm = 0.000521
  l2.weight: grad_norm = 0.220355
Total gradient norm: 0.483595
=== Actor Training Debug (Iteration 5228) ===
Q mean: -66.309128
Q std: 23.000164
Actor loss: 66.313118
Action reg: 0.003986
  l1.weight: grad_norm = 0.063508
  l1.bias: grad_norm = 0.000585
  l2.weight: grad_norm = 0.164175
Total gradient norm: 0.377828
=== Actor Training Debug (Iteration 5229) ===
Q mean: -67.038788
Q std: 21.884415
Actor loss: 67.042786
Action reg: 0.003995
  l1.weight: grad_norm = 0.105110
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.243949
Total gradient norm: 0.451626
=== Actor Training Debug (Iteration 5230) ===
Q mean: -72.339355
Q std: 20.956192
Actor loss: 72.343353
Action reg: 0.003997
  l1.weight: grad_norm = 0.086843
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.190268
Total gradient norm: 0.333504
=== Actor Training Debug (Iteration 5231) ===
Q mean: -66.287491
Q std: 23.322861
Actor loss: 66.291473
Action reg: 0.003986
  l1.weight: grad_norm = 0.066497
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.172992
Total gradient norm: 0.354264
=== Actor Training Debug (Iteration 5232) ===
Q mean: -66.391731
Q std: 22.015915
Actor loss: 66.395721
Action reg: 0.003994
  l1.weight: grad_norm = 0.074002
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.154767
Total gradient norm: 0.291249
=== Actor Training Debug (Iteration 5233) ===
Q mean: -67.759377
Q std: 22.709482
Actor loss: 67.763359
Action reg: 0.003985
  l1.weight: grad_norm = 0.046117
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.120491
Total gradient norm: 0.221641
=== Actor Training Debug (Iteration 5234) ===
Q mean: -68.491837
Q std: 23.179693
Actor loss: 68.495827
Action reg: 0.003992
  l1.weight: grad_norm = 0.088282
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.207471
Total gradient norm: 0.414680
=== Actor Training Debug (Iteration 5235) ===
Q mean: -69.360199
Q std: 23.542873
Actor loss: 69.364174
Action reg: 0.003978
  l1.weight: grad_norm = 0.017899
  l1.bias: grad_norm = 0.000840
  l2.weight: grad_norm = 0.039773
Total gradient norm: 0.072993
=== Actor Training Debug (Iteration 5236) ===
Q mean: -66.997574
Q std: 24.022942
Actor loss: 67.001549
Action reg: 0.003973
  l1.weight: grad_norm = 0.096964
  l1.bias: grad_norm = 0.001275
  l2.weight: grad_norm = 0.245405
Total gradient norm: 0.476732
=== Actor Training Debug (Iteration 5237) ===
Q mean: -71.868011
Q std: 22.834869
Actor loss: 71.872009
Action reg: 0.003996
  l1.weight: grad_norm = 0.047315
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.123355
Total gradient norm: 0.299942
=== Actor Training Debug (Iteration 5238) ===
Q mean: -70.983131
Q std: 23.072433
Actor loss: 70.987129
Action reg: 0.003994
  l1.weight: grad_norm = 0.057733
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.162726
Total gradient norm: 0.273233
=== Actor Training Debug (Iteration 5239) ===
Q mean: -66.965195
Q std: 23.106363
Actor loss: 66.969170
Action reg: 0.003976
  l1.weight: grad_norm = 0.084486
  l1.bias: grad_norm = 0.001047
  l2.weight: grad_norm = 0.196510
Total gradient norm: 0.357908
=== Actor Training Debug (Iteration 5240) ===
Q mean: -69.030037
Q std: 23.226849
Actor loss: 69.034019
Action reg: 0.003983
  l1.weight: grad_norm = 0.069297
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.153196
Total gradient norm: 0.235378
=== Actor Training Debug (Iteration 5241) ===
Q mean: -69.988419
Q std: 23.138906
Actor loss: 69.992378
Action reg: 0.003960
  l1.weight: grad_norm = 0.025025
  l1.bias: grad_norm = 0.001709
  l2.weight: grad_norm = 0.053204
Total gradient norm: 0.086194
=== Actor Training Debug (Iteration 5242) ===
Q mean: -67.380928
Q std: 23.196472
Actor loss: 67.384903
Action reg: 0.003974
  l1.weight: grad_norm = 0.301486
  l1.bias: grad_norm = 0.001041
  l2.weight: grad_norm = 0.658930
Total gradient norm: 1.298953
=== Actor Training Debug (Iteration 5243) ===
Q mean: -67.822617
Q std: 22.409037
Actor loss: 67.826591
Action reg: 0.003974
  l1.weight: grad_norm = 0.080345
  l1.bias: grad_norm = 0.001066
  l2.weight: grad_norm = 0.170373
Total gradient norm: 0.280860
=== Actor Training Debug (Iteration 5244) ===
Q mean: -65.892731
Q std: 23.019060
Actor loss: 65.896706
Action reg: 0.003972
  l1.weight: grad_norm = 0.696804
  l1.bias: grad_norm = 0.001081
  l2.weight: grad_norm = 1.765340
Total gradient norm: 3.525398
=== Actor Training Debug (Iteration 5245) ===
Q mean: -68.382858
Q std: 24.237686
Actor loss: 68.386826
Action reg: 0.003971
  l1.weight: grad_norm = 0.189797
  l1.bias: grad_norm = 0.001032
  l2.weight: grad_norm = 0.469336
Total gradient norm: 1.002453
=== Actor Training Debug (Iteration 5246) ===
Q mean: -69.574203
Q std: 23.003986
Actor loss: 69.578186
Action reg: 0.003984
  l1.weight: grad_norm = 0.123541
  l1.bias: grad_norm = 0.000684
  l2.weight: grad_norm = 0.261328
Total gradient norm: 0.435270
=== Actor Training Debug (Iteration 5247) ===
Q mean: -67.989670
Q std: 23.853331
Actor loss: 67.993652
Action reg: 0.003979
  l1.weight: grad_norm = 0.082848
  l1.bias: grad_norm = 0.000893
  l2.weight: grad_norm = 0.236924
Total gradient norm: 0.500158
=== Actor Training Debug (Iteration 5248) ===
Q mean: -65.777481
Q std: 23.928686
Actor loss: 65.781456
Action reg: 0.003974
  l1.weight: grad_norm = 0.094300
  l1.bias: grad_norm = 0.001028
  l2.weight: grad_norm = 0.192834
Total gradient norm: 0.361063
=== Actor Training Debug (Iteration 5249) ===
Q mean: -70.191307
Q std: 22.101673
Actor loss: 70.195305
Action reg: 0.003995
  l1.weight: grad_norm = 0.176763
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.412651
Total gradient norm: 0.790283
=== Actor Training Debug (Iteration 5250) ===
Q mean: -67.762001
Q std: 23.822605
Actor loss: 67.765961
Action reg: 0.003961
  l1.weight: grad_norm = 0.503185
  l1.bias: grad_norm = 0.001325
  l2.weight: grad_norm = 1.341350
Total gradient norm: 2.558604
=== Actor Training Debug (Iteration 5251) ===
Q mean: -68.657806
Q std: 21.330284
Actor loss: 68.661789
Action reg: 0.003984
  l1.weight: grad_norm = 0.123428
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.302105
Total gradient norm: 0.632139
=== Actor Training Debug (Iteration 5252) ===
Q mean: -67.297943
Q std: 21.296003
Actor loss: 67.301941
Action reg: 0.003999
  l1.weight: grad_norm = 0.011526
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.024671
Total gradient norm: 0.045300
=== Actor Training Debug (Iteration 5253) ===
Q mean: -71.145042
Q std: 22.889982
Actor loss: 71.149025
Action reg: 0.003986
  l1.weight: grad_norm = 0.348578
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.562429
Total gradient norm: 0.932051
=== Actor Training Debug (Iteration 5254) ===
Q mean: -69.198540
Q std: 22.875267
Actor loss: 69.202522
Action reg: 0.003982
  l1.weight: grad_norm = 0.015355
  l1.bias: grad_norm = 0.000662
  l2.weight: grad_norm = 0.034884
Total gradient norm: 0.062626
=== Actor Training Debug (Iteration 5255) ===
Q mean: -66.877914
Q std: 23.230427
Actor loss: 66.881889
Action reg: 0.003976
  l1.weight: grad_norm = 0.220745
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.503904
Total gradient norm: 1.015785
=== Actor Training Debug (Iteration 5256) ===
Q mean: -68.996651
Q std: 21.337801
Actor loss: 69.000648
Action reg: 0.003994
  l1.weight: grad_norm = 0.064448
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.140251
Total gradient norm: 0.244826
=== Actor Training Debug (Iteration 5257) ===
Q mean: -69.871674
Q std: 22.091974
Actor loss: 69.875664
Action reg: 0.003986
  l1.weight: grad_norm = 0.205233
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.431616
Total gradient norm: 0.725062
=== Actor Training Debug (Iteration 5258) ===
Q mean: -67.842957
Q std: 24.340902
Actor loss: 67.846939
Action reg: 0.003979
  l1.weight: grad_norm = 0.185749
  l1.bias: grad_norm = 0.000747
  l2.weight: grad_norm = 0.415448
Total gradient norm: 0.924057
=== Actor Training Debug (Iteration 5259) ===
Q mean: -68.588943
Q std: 22.090826
Actor loss: 68.592934
Action reg: 0.003994
  l1.weight: grad_norm = 0.163182
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.372379
Total gradient norm: 0.726192
=== Actor Training Debug (Iteration 5260) ===
Q mean: -69.445747
Q std: 23.057709
Actor loss: 69.449715
Action reg: 0.003970
  l1.weight: grad_norm = 0.234888
  l1.bias: grad_norm = 0.001062
  l2.weight: grad_norm = 0.538621
Total gradient norm: 1.062933
=== Actor Training Debug (Iteration 5261) ===
Q mean: -67.032593
Q std: 23.361624
Actor loss: 67.036583
Action reg: 0.003988
  l1.weight: grad_norm = 0.070822
  l1.bias: grad_norm = 0.000476
  l2.weight: grad_norm = 0.167966
Total gradient norm: 0.328959
=== Actor Training Debug (Iteration 5262) ===
Q mean: -71.577629
Q std: 22.597500
Actor loss: 71.581612
Action reg: 0.003983
  l1.weight: grad_norm = 0.164473
  l1.bias: grad_norm = 0.000984
  l2.weight: grad_norm = 0.382216
Total gradient norm: 0.662342
=== Actor Training Debug (Iteration 5263) ===
Q mean: -68.244019
Q std: 23.031721
Actor loss: 68.248009
Action reg: 0.003991
  l1.weight: grad_norm = 0.212315
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.416397
Total gradient norm: 0.792290
=== Actor Training Debug (Iteration 5264) ===
Q mean: -69.740479
Q std: 22.586933
Actor loss: 69.744469
Action reg: 0.003987
  l1.weight: grad_norm = 0.070358
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.155388
Total gradient norm: 0.355968
=== Actor Training Debug (Iteration 5265) ===
Q mean: -70.334755
Q std: 22.890877
Actor loss: 70.338737
Action reg: 0.003984
  l1.weight: grad_norm = 0.025639
  l1.bias: grad_norm = 0.000597
  l2.weight: grad_norm = 0.068145
Total gradient norm: 0.130017
=== Actor Training Debug (Iteration 5266) ===
Q mean: -68.477905
Q std: 22.512606
Actor loss: 68.481888
Action reg: 0.003985
  l1.weight: grad_norm = 0.156917
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.403761
Total gradient norm: 0.815333
=== Actor Training Debug (Iteration 5267) ===
Q mean: -67.053391
Q std: 23.046167
Actor loss: 67.057365
Action reg: 0.003972
  l1.weight: grad_norm = 0.181453
  l1.bias: grad_norm = 0.001012
  l2.weight: grad_norm = 0.400554
Total gradient norm: 0.858854
=== Actor Training Debug (Iteration 5268) ===
Q mean: -68.979347
Q std: 22.477890
Actor loss: 68.983337
Action reg: 0.003988
  l1.weight: grad_norm = 0.068149
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.149176
Total gradient norm: 0.212110
=== Actor Training Debug (Iteration 5269) ===
Q mean: -66.849113
Q std: 24.579081
Actor loss: 66.853065
Action reg: 0.003951
  l1.weight: grad_norm = 0.102809
  l1.bias: grad_norm = 0.001879
  l2.weight: grad_norm = 0.230354
Total gradient norm: 0.343561
=== Actor Training Debug (Iteration 5270) ===
Q mean: -66.716141
Q std: 23.011293
Actor loss: 66.720131
Action reg: 0.003987
  l1.weight: grad_norm = 0.111467
  l1.bias: grad_norm = 0.000725
  l2.weight: grad_norm = 0.256431
Total gradient norm: 0.522512
=== Actor Training Debug (Iteration 5271) ===
Q mean: -68.789703
Q std: 21.708357
Actor loss: 68.793694
Action reg: 0.003990
  l1.weight: grad_norm = 0.027626
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.073365
Total gradient norm: 0.145051
=== Actor Training Debug (Iteration 5272) ===
Q mean: -66.052139
Q std: 23.211460
Actor loss: 66.056122
Action reg: 0.003986
  l1.weight: grad_norm = 0.173505
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.436378
Total gradient norm: 0.888164
=== Actor Training Debug (Iteration 5273) ===
Q mean: -67.719208
Q std: 22.597172
Actor loss: 67.723206
Action reg: 0.003996
  l1.weight: grad_norm = 0.099240
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.231859
Total gradient norm: 0.382034
=== Actor Training Debug (Iteration 5274) ===
Q mean: -69.252762
Q std: 23.312647
Actor loss: 69.256752
Action reg: 0.003994
  l1.weight: grad_norm = 0.013254
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.028353
Total gradient norm: 0.058105
=== Actor Training Debug (Iteration 5275) ===
Q mean: -68.958176
Q std: 23.348825
Actor loss: 68.962158
Action reg: 0.003981
  l1.weight: grad_norm = 0.174324
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.405739
Total gradient norm: 0.681576
=== Actor Training Debug (Iteration 5276) ===
Q mean: -70.636909
Q std: 22.942955
Actor loss: 70.640900
Action reg: 0.003989
  l1.weight: grad_norm = 0.055631
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.112220
Total gradient norm: 0.192229
=== Actor Training Debug (Iteration 5277) ===
Q mean: -67.051025
Q std: 22.766668
Actor loss: 67.055008
Action reg: 0.003983
  l1.weight: grad_norm = 0.196514
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.523940
Total gradient norm: 0.913739
=== Actor Training Debug (Iteration 5278) ===
Q mean: -70.299675
Q std: 23.504578
Actor loss: 70.303658
Action reg: 0.003986
  l1.weight: grad_norm = 0.140223
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.354231
Total gradient norm: 0.574269
=== Actor Training Debug (Iteration 5279) ===
Q mean: -65.856857
Q std: 23.040792
Actor loss: 65.860840
Action reg: 0.003984
  l1.weight: grad_norm = 0.063607
  l1.bias: grad_norm = 0.000505
  l2.weight: grad_norm = 0.125574
Total gradient norm: 0.214918
=== Actor Training Debug (Iteration 5280) ===
Q mean: -67.319778
Q std: 23.624125
Actor loss: 67.323769
Action reg: 0.003987
  l1.weight: grad_norm = 0.025799
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.055368
Total gradient norm: 0.102952
=== Actor Training Debug (Iteration 5281) ===
Q mean: -66.798431
Q std: 22.499947
Actor loss: 66.802406
Action reg: 0.003975
  l1.weight: grad_norm = 0.699307
  l1.bias: grad_norm = 0.000793
  l2.weight: grad_norm = 1.609870
Total gradient norm: 2.907924
=== Actor Training Debug (Iteration 5282) ===
Q mean: -69.003998
Q std: 22.888594
Actor loss: 69.007980
Action reg: 0.003985
  l1.weight: grad_norm = 0.111476
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.252999
Total gradient norm: 0.469575
=== Actor Training Debug (Iteration 5283) ===
Q mean: -67.890976
Q std: 23.739002
Actor loss: 67.894958
Action reg: 0.003981
  l1.weight: grad_norm = 0.192336
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.520923
Total gradient norm: 0.905445
=== Actor Training Debug (Iteration 5284) ===
Q mean: -70.043488
Q std: 22.064138
Actor loss: 70.047485
Action reg: 0.003998
  l1.weight: grad_norm = 0.133023
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.291734
Total gradient norm: 0.506200
=== Actor Training Debug (Iteration 5285) ===
Q mean: -68.024170
Q std: 22.786274
Actor loss: 68.028160
Action reg: 0.003987
  l1.weight: grad_norm = 0.043879
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.090248
Total gradient norm: 0.162322
=== Actor Training Debug (Iteration 5286) ===
Q mean: -66.209343
Q std: 21.893803
Actor loss: 66.213326
Action reg: 0.003984
  l1.weight: grad_norm = 0.083190
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.213764
Total gradient norm: 0.469034
=== Actor Training Debug (Iteration 5287) ===
Q mean: -70.094498
Q std: 23.670235
Actor loss: 70.098480
Action reg: 0.003982
  l1.weight: grad_norm = 0.176289
  l1.bias: grad_norm = 0.000656
  l2.weight: grad_norm = 0.381382
Total gradient norm: 0.715041
=== Actor Training Debug (Iteration 5288) ===
Q mean: -69.367180
Q std: 22.720522
Actor loss: 69.371162
Action reg: 0.003984
  l1.weight: grad_norm = 0.178271
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.479833
Total gradient norm: 0.818453
=== Actor Training Debug (Iteration 5289) ===
Q mean: -72.374481
Q std: 22.242554
Actor loss: 72.378456
Action reg: 0.003978
  l1.weight: grad_norm = 0.098930
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.237774
Total gradient norm: 0.452288
=== Actor Training Debug (Iteration 5290) ===
Q mean: -69.081650
Q std: 23.080805
Actor loss: 69.085632
Action reg: 0.003981
  l1.weight: grad_norm = 0.015782
  l1.bias: grad_norm = 0.000765
  l2.weight: grad_norm = 0.034912
Total gradient norm: 0.056866
=== Actor Training Debug (Iteration 5291) ===
Q mean: -67.474747
Q std: 23.382843
Actor loss: 67.478729
Action reg: 0.003979
  l1.weight: grad_norm = 0.205640
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.544836
Total gradient norm: 0.987252
=== Actor Training Debug (Iteration 5292) ===
Q mean: -67.827118
Q std: 24.701342
Actor loss: 67.831085
Action reg: 0.003966
  l1.weight: grad_norm = 0.097258
  l1.bias: grad_norm = 0.001324
  l2.weight: grad_norm = 0.271601
Total gradient norm: 0.474242
=== Actor Training Debug (Iteration 5293) ===
Q mean: -70.316849
Q std: 23.060522
Actor loss: 70.320839
Action reg: 0.003987
  l1.weight: grad_norm = 0.159614
  l1.bias: grad_norm = 0.000548
  l2.weight: grad_norm = 0.318552
Total gradient norm: 0.600324
=== Actor Training Debug (Iteration 5294) ===
Q mean: -70.741074
Q std: 23.154182
Actor loss: 70.745064
Action reg: 0.003989
  l1.weight: grad_norm = 0.015199
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.031131
Total gradient norm: 0.062857
=== Actor Training Debug (Iteration 5295) ===
Q mean: -63.287350
Q std: 23.002436
Actor loss: 63.291336
Action reg: 0.003985
  l1.weight: grad_norm = 0.235395
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.531200
Total gradient norm: 1.113722
=== Actor Training Debug (Iteration 5296) ===
Q mean: -67.160629
Q std: 21.608511
Actor loss: 67.164627
Action reg: 0.003999
  l1.weight: grad_norm = 0.059452
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.151675
Total gradient norm: 0.259390
=== Actor Training Debug (Iteration 5297) ===
Q mean: -69.306267
Q std: 22.820263
Actor loss: 69.310257
Action reg: 0.003987
  l1.weight: grad_norm = 0.153877
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.403887
Total gradient norm: 0.661348
=== Actor Training Debug (Iteration 5298) ===
Q mean: -68.818893
Q std: 22.046341
Actor loss: 68.822891
Action reg: 0.003995
  l1.weight: grad_norm = 0.259079
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.466070
Total gradient norm: 0.886272
=== Actor Training Debug (Iteration 5299) ===
Q mean: -66.492554
Q std: 23.627237
Actor loss: 66.496529
Action reg: 0.003977
  l1.weight: grad_norm = 0.423427
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 1.159244
Total gradient norm: 1.923993
=== Actor Training Debug (Iteration 5300) ===
Q mean: -69.361786
Q std: 23.698620
Actor loss: 69.365768
Action reg: 0.003980
  l1.weight: grad_norm = 0.176113
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.390860
Total gradient norm: 0.686565
=== Actor Training Debug (Iteration 5301) ===
Q mean: -68.845116
Q std: 23.683020
Actor loss: 68.849098
Action reg: 0.003981
  l1.weight: grad_norm = 0.030359
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.073086
Total gradient norm: 0.131751
=== Actor Training Debug (Iteration 5302) ===
Q mean: -69.168533
Q std: 22.780872
Actor loss: 69.172516
Action reg: 0.003983
  l1.weight: grad_norm = 0.046888
  l1.bias: grad_norm = 0.000754
  l2.weight: grad_norm = 0.114991
Total gradient norm: 0.255063
=== Actor Training Debug (Iteration 5303) ===
Q mean: -69.596642
Q std: 23.752285
Actor loss: 69.600616
Action reg: 0.003977
  l1.weight: grad_norm = 0.171436
  l1.bias: grad_norm = 0.001055
  l2.weight: grad_norm = 0.437363
Total gradient norm: 0.712077
=== Actor Training Debug (Iteration 5304) ===
Q mean: -69.024094
Q std: 22.426493
Actor loss: 69.028076
Action reg: 0.003986
  l1.weight: grad_norm = 0.380586
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.885074
Total gradient norm: 1.548202
=== Actor Training Debug (Iteration 5305) ===
Q mean: -68.585602
Q std: 23.849384
Actor loss: 68.589592
Action reg: 0.003990
  l1.weight: grad_norm = 0.263439
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.621651
Total gradient norm: 0.971017
=== Actor Training Debug (Iteration 5306) ===
Q mean: -68.465279
Q std: 24.479956
Actor loss: 68.469246
Action reg: 0.003969
  l1.weight: grad_norm = 0.389379
  l1.bias: grad_norm = 0.001194
  l2.weight: grad_norm = 0.806489
Total gradient norm: 1.503903
=== Actor Training Debug (Iteration 5307) ===
Q mean: -69.358551
Q std: 21.582287
Actor loss: 69.362541
Action reg: 0.003989
  l1.weight: grad_norm = 0.161157
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.343140
Total gradient norm: 0.588179
=== Actor Training Debug (Iteration 5308) ===
Q mean: -70.643524
Q std: 22.011883
Actor loss: 70.647507
Action reg: 0.003984
  l1.weight: grad_norm = 0.497243
  l1.bias: grad_norm = 0.000696
  l2.weight: grad_norm = 1.122728
Total gradient norm: 2.025051
=== Actor Training Debug (Iteration 5309) ===
Q mean: -69.675827
Q std: 21.947788
Actor loss: 69.679810
Action reg: 0.003980
  l1.weight: grad_norm = 0.269463
  l1.bias: grad_norm = 0.000553
  l2.weight: grad_norm = 0.639882
Total gradient norm: 1.233476
=== Actor Training Debug (Iteration 5310) ===
Q mean: -64.469971
Q std: 25.360826
Actor loss: 64.473923
Action reg: 0.003950
  l1.weight: grad_norm = 0.181041
  l1.bias: grad_norm = 0.002003
  l2.weight: grad_norm = 0.407710
Total gradient norm: 0.729934
=== Actor Training Debug (Iteration 5311) ===
Q mean: -67.556778
Q std: 22.384565
Actor loss: 67.560768
Action reg: 0.003993
  l1.weight: grad_norm = 0.045693
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.108681
Total gradient norm: 0.214585
=== Actor Training Debug (Iteration 5312) ===
Q mean: -69.473824
Q std: 23.990978
Actor loss: 69.477798
Action reg: 0.003977
  l1.weight: grad_norm = 0.248970
  l1.bias: grad_norm = 0.000996
  l2.weight: grad_norm = 0.536328
Total gradient norm: 0.928622
=== Actor Training Debug (Iteration 5313) ===
Q mean: -69.184723
Q std: 23.477552
Actor loss: 69.188705
Action reg: 0.003986
  l1.weight: grad_norm = 0.109525
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.319331
Total gradient norm: 0.746060
=== Actor Training Debug (Iteration 5314) ===
Q mean: -67.890869
Q std: 23.541290
Actor loss: 67.894852
Action reg: 0.003979
  l1.weight: grad_norm = 0.129501
  l1.bias: grad_norm = 0.000865
  l2.weight: grad_norm = 0.308804
Total gradient norm: 0.631774
=== Actor Training Debug (Iteration 5315) ===
Q mean: -68.007629
Q std: 22.869274
Actor loss: 68.011612
Action reg: 0.003984
  l1.weight: grad_norm = 0.113323
  l1.bias: grad_norm = 0.000752
  l2.weight: grad_norm = 0.224906
Total gradient norm: 0.414295
=== Actor Training Debug (Iteration 5316) ===
Q mean: -70.577087
Q std: 22.750004
Actor loss: 70.581078
Action reg: 0.003987
  l1.weight: grad_norm = 0.299274
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.800927
Total gradient norm: 1.740821
=== Actor Training Debug (Iteration 5317) ===
Q mean: -69.002563
Q std: 22.705105
Actor loss: 69.006546
Action reg: 0.003981
  l1.weight: grad_norm = 0.200714
  l1.bias: grad_norm = 0.000644
  l2.weight: grad_norm = 0.533045
Total gradient norm: 0.915733
=== Actor Training Debug (Iteration 5318) ===
Q mean: -69.474907
Q std: 23.607594
Actor loss: 69.478889
Action reg: 0.003984
  l1.weight: grad_norm = 0.079395
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.190950
Total gradient norm: 0.393048
=== Actor Training Debug (Iteration 5319) ===
Q mean: -68.938225
Q std: 23.366552
Actor loss: 68.942207
Action reg: 0.003982
  l1.weight: grad_norm = 0.238223
  l1.bias: grad_norm = 0.000672
  l2.weight: grad_norm = 0.577513
Total gradient norm: 0.970777
=== Actor Training Debug (Iteration 5320) ===
Q mean: -68.461456
Q std: 23.313349
Actor loss: 68.465431
Action reg: 0.003976
  l1.weight: grad_norm = 0.481689
  l1.bias: grad_norm = 0.000578
  l2.weight: grad_norm = 1.651762
Total gradient norm: 4.176907
=== Actor Training Debug (Iteration 5321) ===
Q mean: -69.024811
Q std: 24.039417
Actor loss: 69.028778
Action reg: 0.003971
  l1.weight: grad_norm = 0.193895
  l1.bias: grad_norm = 0.000890
  l2.weight: grad_norm = 0.608519
Total gradient norm: 1.343401
=== Actor Training Debug (Iteration 5322) ===
Q mean: -68.441132
Q std: 22.027420
Actor loss: 68.445114
Action reg: 0.003985
  l1.weight: grad_norm = 0.042888
  l1.bias: grad_norm = 0.000540
  l2.weight: grad_norm = 0.119016
Total gradient norm: 0.268881
=== Actor Training Debug (Iteration 5323) ===
Q mean: -69.718475
Q std: 24.441721
Actor loss: 69.722435
Action reg: 0.003958
  l1.weight: grad_norm = 0.440024
  l1.bias: grad_norm = 0.001539
  l2.weight: grad_norm = 1.018387
Total gradient norm: 1.502317
=== Actor Training Debug (Iteration 5324) ===
Q mean: -68.263901
Q std: 23.077333
Actor loss: 68.267876
Action reg: 0.003975
  l1.weight: grad_norm = 0.273732
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.687472
Total gradient norm: 1.700229
=== Actor Training Debug (Iteration 5325) ===
Q mean: -68.672073
Q std: 24.588961
Actor loss: 68.676048
Action reg: 0.003978
  l1.weight: grad_norm = 0.073398
  l1.bias: grad_norm = 0.000791
  l2.weight: grad_norm = 0.168752
Total gradient norm: 0.396519
=== Actor Training Debug (Iteration 5326) ===
Q mean: -68.670143
Q std: 23.277819
Actor loss: 68.674118
Action reg: 0.003977
  l1.weight: grad_norm = 0.147964
  l1.bias: grad_norm = 0.001007
  l2.weight: grad_norm = 0.366099
Total gradient norm: 0.757583
=== Actor Training Debug (Iteration 5327) ===
Q mean: -69.612061
Q std: 22.800079
Actor loss: 69.616043
Action reg: 0.003984
  l1.weight: grad_norm = 0.276569
  l1.bias: grad_norm = 0.000595
  l2.weight: grad_norm = 0.584557
Total gradient norm: 0.967702
=== Actor Training Debug (Iteration 5328) ===
Q mean: -70.328705
Q std: 24.521093
Actor loss: 70.332680
Action reg: 0.003977
  l1.weight: grad_norm = 0.178446
  l1.bias: grad_norm = 0.000901
  l2.weight: grad_norm = 0.395875
Total gradient norm: 0.684802
=== Actor Training Debug (Iteration 5329) ===
Q mean: -71.421700
Q std: 22.085285
Actor loss: 71.425690
Action reg: 0.003990
  l1.weight: grad_norm = 0.100391
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.194710
Total gradient norm: 0.330824
=== Actor Training Debug (Iteration 5330) ===
Q mean: -66.505569
Q std: 24.192865
Actor loss: 66.509552
Action reg: 0.003981
  l1.weight: grad_norm = 0.111359
  l1.bias: grad_norm = 0.000540
  l2.weight: grad_norm = 0.280911
Total gradient norm: 0.614145
=== Actor Training Debug (Iteration 5331) ===
Q mean: -68.082199
Q std: 22.195807
Actor loss: 68.086182
Action reg: 0.003986
  l1.weight: grad_norm = 0.361808
  l1.bias: grad_norm = 0.000562
  l2.weight: grad_norm = 1.044629
Total gradient norm: 2.161233
=== Actor Training Debug (Iteration 5332) ===
Q mean: -68.732437
Q std: 22.999863
Actor loss: 68.736427
Action reg: 0.003992
  l1.weight: grad_norm = 0.025061
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.055260
Total gradient norm: 0.112320
=== Actor Training Debug (Iteration 5333) ===
Q mean: -69.224350
Q std: 23.307146
Actor loss: 69.228325
Action reg: 0.003973
  l1.weight: grad_norm = 0.103256
  l1.bias: grad_norm = 0.001133
  l2.weight: grad_norm = 0.204126
Total gradient norm: 0.340735
Total gradient norm: 2.1034034696on 1203) ===
=== Actor Training Debug (Iteration 5344) ===
Q mean: -66.190514
Q std: 23.847250
Actor loss: 66.194496
Action reg: 0.003984
  l1.weight: grad_norm = 0.008457
  l1.bias: grad_norm = 0.000693
  l2.weight: grad_norm = 0.018758
Total gradient norm: 0.037121
=== Actor Training Debug (Iteration 5345) ===
Q mean: -67.932144
Q std: 24.568167
Actor loss: 67.936127
Action reg: 0.003986
  l1.weight: grad_norm = 0.031091
  l1.bias: grad_norm = 0.000567
  l2.weight: grad_norm = 0.077021
Total gradient norm: 0.162159
=== Actor Training Debug (Iteration 5346) ===
Q mean: -69.296844
Q std: 25.030249
Actor loss: 69.300812
Action reg: 0.003966
  l1.weight: grad_norm = 0.081820
  l1.bias: grad_norm = 0.001220
  l2.weight: grad_norm = 0.188431
Total gradient norm: 0.364536
=== Actor Training Debug (Iteration 5347) ===
Q mean: -68.106873
Q std: 22.430958
Actor loss: 68.110855
Action reg: 0.003984
  l1.weight: grad_norm = 0.021233
  l1.bias: grad_norm = 0.000750
  l2.weight: grad_norm = 0.058206
Total gradient norm: 0.108091
=== Actor Training Debug (Iteration 5348) ===
Q mean: -69.923820
Q std: 22.474442
Actor loss: 69.927811
Action reg: 0.003991
  l1.weight: grad_norm = 0.173604
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.450015
Total gradient norm: 0.998502
=== Actor Training Debug (Iteration 5349) ===
Q mean: -69.313736
Q std: 23.749710
Actor loss: 69.317703
Action reg: 0.003969
  l1.weight: grad_norm = 0.221610
  l1.bias: grad_norm = 0.001059
  l2.weight: grad_norm = 0.544759
Total gradient norm: 0.947455
=== Actor Training Debug (Iteration 5350) ===
Q mean: -68.991333
Q std: 24.290350
Actor loss: 68.995316
Action reg: 0.003983
  l1.weight: grad_norm = 0.208784
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.491412
Total gradient norm: 0.957220
=== Actor Training Debug (Iteration 5351) ===
Q mean: -70.587158
Q std: 24.655807
Actor loss: 70.591141
Action reg: 0.003982
  l1.weight: grad_norm = 0.336738
  l1.bias: grad_norm = 0.000425
  l2.weight: grad_norm = 0.758796
Total gradient norm: 1.284817
=== Actor Training Debug (Iteration 5352) ===
Q mean: -66.221489
Q std: 25.509899
Actor loss: 66.225464
Action reg: 0.003976
  l1.weight: grad_norm = 0.148818
  l1.bias: grad_norm = 0.000877
  l2.weight: grad_norm = 0.326843
Total gradient norm: 0.575753
=== Actor Training Debug (Iteration 5353) ===
Q mean: -66.859238
Q std: 22.467045
Actor loss: 66.863228
Action reg: 0.003988
  l1.weight: grad_norm = 0.096513
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.194295
Total gradient norm: 0.286336
=== Actor Training Debug (Iteration 5354) ===
Q mean: -68.036743
Q std: 23.766159
Actor loss: 68.040733
Action reg: 0.003988
  l1.weight: grad_norm = 0.044947
  l1.bias: grad_norm = 0.000540
  l2.weight: grad_norm = 0.107269
Total gradient norm: 0.211382
=== Actor Training Debug (Iteration 5355) ===
Q mean: -69.563911
Q std: 22.734594
Actor loss: 69.567902
Action reg: 0.003989
  l1.weight: grad_norm = 0.073485
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.191778
Total gradient norm: 0.424392
=== Actor Training Debug (Iteration 5356) ===
Q mean: -68.096542
Q std: 24.130178
Actor loss: 68.100525
Action reg: 0.003983
  l1.weight: grad_norm = 0.056981
  l1.bias: grad_norm = 0.000752
  l2.weight: grad_norm = 0.112380
Total gradient norm: 0.221593
=== Actor Training Debug (Iteration 5357) ===
Q mean: -69.559845
Q std: 24.311598
Actor loss: 69.563835
Action reg: 0.003987
  l1.weight: grad_norm = 0.214361
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.464567
Total gradient norm: 0.853614
=== Actor Training Debug (Iteration 5358) ===
Q mean: -71.554100
Q std: 24.154942
Actor loss: 71.558083
Action reg: 0.003986
  l1.weight: grad_norm = 0.074544
  l1.bias: grad_norm = 0.000601
  l2.weight: grad_norm = 0.189688
Total gradient norm: 0.362921
=== Actor Training Debug (Iteration 5359) ===
Q mean: -67.965302
Q std: 21.992844
Actor loss: 67.969292
Action reg: 0.003990
  l1.weight: grad_norm = 0.050434
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.115771
Total gradient norm: 0.232459
=== Actor Training Debug (Iteration 5360) ===
Q mean: -69.206009
Q std: 23.486521
Actor loss: 69.209991
Action reg: 0.003986
  l1.weight: grad_norm = 0.045513
  l1.bias: grad_norm = 0.000753
  l2.weight: grad_norm = 0.105180
Total gradient norm: 0.220808
=== Actor Training Debug (Iteration 5361) ===
Q mean: -70.284836
Q std: 23.675098
Actor loss: 70.288834
Action reg: 0.003995
  l1.weight: grad_norm = 0.014753
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.046144
Total gradient norm: 0.095202
=== Actor Training Debug (Iteration 5362) ===
Q mean: -68.269806
Q std: 22.836241
Actor loss: 68.273788
Action reg: 0.003985
  l1.weight: grad_norm = 0.071779
  l1.bias: grad_norm = 0.000509
  l2.weight: grad_norm = 0.158472
Total gradient norm: 0.279365
=== Actor Training Debug (Iteration 5363) ===
Q mean: -67.087013
Q std: 23.728014
Actor loss: 67.090973
Action reg: 0.003963
  l1.weight: grad_norm = 0.515261
  l1.bias: grad_norm = 0.001396
  l2.weight: grad_norm = 1.305945
Total gradient norm: 2.195913
=== Actor Training Debug (Iteration 5364) ===
Q mean: -69.777962
Q std: 25.263222
Actor loss: 69.781929
Action reg: 0.003967
  l1.weight: grad_norm = 0.219071
  l1.bias: grad_norm = 0.001315
  l2.weight: grad_norm = 0.621869
Total gradient norm: 1.321244
=== Actor Training Debug (Iteration 5365) ===
Q mean: -69.677429
Q std: 24.141270
Actor loss: 69.681412
Action reg: 0.003980
  l1.weight: grad_norm = 0.115202
  l1.bias: grad_norm = 0.000986
  l2.weight: grad_norm = 0.264679
Total gradient norm: 0.623736
=== Actor Training Debug (Iteration 5366) ===
Q mean: -67.578232
Q std: 22.528358
Actor loss: 67.582222
Action reg: 0.003988
  l1.weight: grad_norm = 0.056231
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.154242
Total gradient norm: 0.349724
=== Actor Training Debug (Iteration 5367) ===
Q mean: -70.619759
Q std: 21.999664
Actor loss: 70.623741
Action reg: 0.003985
  l1.weight: grad_norm = 0.184868
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.422829
Total gradient norm: 0.787436
=== Actor Training Debug (Iteration 5368) ===
Q mean: -68.326752
Q std: 22.703833
Actor loss: 68.330734
Action reg: 0.003984
  l1.weight: grad_norm = 0.166099
  l1.bias: grad_norm = 0.000461
  l2.weight: grad_norm = 0.411763
Total gradient norm: 0.756861
=== Actor Training Debug (Iteration 5369) ===
Q mean: -65.106499
Q std: 23.507622
Actor loss: 65.110474
Action reg: 0.003978
  l1.weight: grad_norm = 0.300805
  l1.bias: grad_norm = 0.000765
  l2.weight: grad_norm = 0.578948
Total gradient norm: 1.013550
=== Actor Training Debug (Iteration 5370) ===
Q mean: -68.256836
Q std: 25.694185
Actor loss: 68.260803
Action reg: 0.003969
  l1.weight: grad_norm = 0.125511
  l1.bias: grad_norm = 0.001472
  l2.weight: grad_norm = 0.297366
Total gradient norm: 0.650876
=== Actor Training Debug (Iteration 5371) ===
Q mean: -72.890663
Q std: 23.332083
Actor loss: 72.894646
Action reg: 0.003981
  l1.weight: grad_norm = 0.169199
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.443919
Total gradient norm: 0.893336
=== Actor Training Debug (Iteration 5372) ===
Q mean: -67.002182
Q std: 23.260954
Actor loss: 67.006165
Action reg: 0.003983
  l1.weight: grad_norm = 0.067855
  l1.bias: grad_norm = 0.000844
  l2.weight: grad_norm = 0.163571
Total gradient norm: 0.285903
=== Actor Training Debug (Iteration 5373) ===
Q mean: -68.192337
Q std: 24.120701
Actor loss: 68.196312
Action reg: 0.003973
  l1.weight: grad_norm = 0.298300
  l1.bias: grad_norm = 0.001318
  l2.weight: grad_norm = 0.703493
Total gradient norm: 1.422684
=== Actor Training Debug (Iteration 5374) ===
Q mean: -71.183777
Q std: 23.911129
Actor loss: 71.187759
Action reg: 0.003983
  l1.weight: grad_norm = 0.109463
  l1.bias: grad_norm = 0.000604
  l2.weight: grad_norm = 0.233301
Total gradient norm: 0.434393
=== Actor Training Debug (Iteration 5375) ===
Q mean: -71.665291
Q std: 22.272146
Actor loss: 71.669281
Action reg: 0.003989
  l1.weight: grad_norm = 0.423533
  l1.bias: grad_norm = 0.000762
  l2.weight: grad_norm = 1.014665
Total gradient norm: 1.954152
=== Actor Training Debug (Iteration 5376) ===
Q mean: -67.304085
Q std: 23.169523
Actor loss: 67.308075
Action reg: 0.003992
  l1.weight: grad_norm = 0.112255
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.243972
Total gradient norm: 0.462648
=== Actor Training Debug (Iteration 5377) ===
Q mean: -68.274689
Q std: 22.680799
Actor loss: 68.278679
Action reg: 0.003992
  l1.weight: grad_norm = 0.064374
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.161668
Total gradient norm: 0.330594
=== Actor Training Debug (Iteration 5378) ===
Q mean: -68.502930
Q std: 25.395800
Actor loss: 68.506905
Action reg: 0.003974
  l1.weight: grad_norm = 0.043071
  l1.bias: grad_norm = 0.001187
  l2.weight: grad_norm = 0.104395
Total gradient norm: 0.222610
=== Actor Training Debug (Iteration 5379) ===
Q mean: -69.059967
Q std: 23.196844
Actor loss: 69.063957
Action reg: 0.003989
  l1.weight: grad_norm = 0.265076
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.532142
Total gradient norm: 0.773739
=== Actor Training Debug (Iteration 5380) ===
Q mean: -70.338364
Q std: 22.875866
Actor loss: 70.342354
Action reg: 0.003993
  l1.weight: grad_norm = 0.075017
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.175432
Total gradient norm: 0.272064
=== Actor Training Debug (Iteration 5381) ===
Q mean: -66.755493
Q std: 22.140221
Actor loss: 66.759483
Action reg: 0.003989
  l1.weight: grad_norm = 0.071514
  l1.bias: grad_norm = 0.000563
  l2.weight: grad_norm = 0.152801
Total gradient norm: 0.302767
=== Actor Training Debug (Iteration 5382) ===
Q mean: -70.274857
Q std: 23.697113
Actor loss: 70.278854
Action reg: 0.003995
  l1.weight: grad_norm = 0.035210
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.082082
Total gradient norm: 0.153300
=== Actor Training Debug (Iteration 5383) ===
Q mean: -67.505226
Q std: 24.653505
Actor loss: 67.509201
Action reg: 0.003974
  l1.weight: grad_norm = 0.170321
  l1.bias: grad_norm = 0.001034
  l2.weight: grad_norm = 0.341361
Total gradient norm: 0.657652
=== Actor Training Debug (Iteration 5384) ===
Q mean: -67.858116
Q std: 23.607868
Actor loss: 67.862106
Action reg: 0.003991
  l1.weight: grad_norm = 0.116502
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.273492
Total gradient norm: 0.507742
=== Actor Training Debug (Iteration 5385) ===
Q mean: -68.274445
Q std: 22.737494
Actor loss: 68.278427
Action reg: 0.003985
  l1.weight: grad_norm = 0.061502
  l1.bias: grad_norm = 0.000710
  l2.weight: grad_norm = 0.144383
Total gradient norm: 0.307412
=== Actor Training Debug (Iteration 5386) ===
Q mean: -69.101639
Q std: 23.123301
Actor loss: 69.105629
Action reg: 0.003987
  l1.weight: grad_norm = 0.158225
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.317272
Total gradient norm: 0.593578
=== Actor Training Debug (Iteration 5387) ===
Q mean: -71.032684
Q std: 23.209202
Actor loss: 71.036659
Action reg: 0.003979
  l1.weight: grad_norm = 0.121315
  l1.bias: grad_norm = 0.000600
  l2.weight: grad_norm = 0.265383
Total gradient norm: 0.545765
=== Actor Training Debug (Iteration 5388) ===
Q mean: -68.468735
Q std: 21.984179
Actor loss: 68.472725
Action reg: 0.003987
  l1.weight: grad_norm = 0.177865
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.406635
Total gradient norm: 0.703872
=== Actor Training Debug (Iteration 5389) ===
Q mean: -69.397278
Q std: 24.712515
Actor loss: 69.401268
Action reg: 0.003989
  l1.weight: grad_norm = 0.122658
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.303830
Total gradient norm: 0.522069
=== Actor Training Debug (Iteration 5390) ===
Q mean: -70.915680
Q std: 22.128653
Actor loss: 70.919670
Action reg: 0.003988
  l1.weight: grad_norm = 0.068255
  l1.bias: grad_norm = 0.000575
  l2.weight: grad_norm = 0.160649
Total gradient norm: 0.312394
=== Actor Training Debug (Iteration 5391) ===
Q mean: -67.104965
Q std: 23.809774
Actor loss: 67.108940
Action reg: 0.003978
  l1.weight: grad_norm = 0.194593
  l1.bias: grad_norm = 0.000653
  l2.weight: grad_norm = 0.476354
Total gradient norm: 1.005878
=== Actor Training Debug (Iteration 5392) ===
Q mean: -66.462479
Q std: 24.632204
Actor loss: 66.466454
Action reg: 0.003977
  l1.weight: grad_norm = 0.073191
  l1.bias: grad_norm = 0.000597
  l2.weight: grad_norm = 0.176585
Total gradient norm: 0.277549
=== Actor Training Debug (Iteration 5393) ===
Q mean: -67.351418
Q std: 25.080433
Actor loss: 67.355392
Action reg: 0.003976
  l1.weight: grad_norm = 0.137408
  l1.bias: grad_norm = 0.000745
  l2.weight: grad_norm = 0.282712
Total gradient norm: 0.521737
=== Actor Training Debug (Iteration 5394) ===
Q mean: -70.490341
Q std: 23.688614
Actor loss: 70.494331
Action reg: 0.003987
  l1.weight: grad_norm = 0.105265
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.243312
Total gradient norm: 0.397984
=== Actor Training Debug (Iteration 5395) ===
Q mean: -71.108505
Q std: 22.426231
Actor loss: 71.112488
Action reg: 0.003985
  l1.weight: grad_norm = 0.185181
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.419668
Total gradient norm: 0.799323
=== Actor Training Debug (Iteration 5396) ===
Q mean: -67.054268
Q std: 23.190006
Actor loss: 67.058250
Action reg: 0.003983
  l1.weight: grad_norm = 0.077818
  l1.bias: grad_norm = 0.000758
  l2.weight: grad_norm = 0.180737
Total gradient norm: 0.337056
=== Actor Training Debug (Iteration 5397) ===
Q mean: -69.624496
Q std: 22.866537
Actor loss: 69.628494
Action reg: 0.003995
  l1.weight: grad_norm = 0.016685
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.035570
Total gradient norm: 0.065299
=== Actor Training Debug (Iteration 5398) ===
Q mean: -67.131027
Q std: 23.667206
Actor loss: 67.134995
Action reg: 0.003967
  l1.weight: grad_norm = 0.194567
  l1.bias: grad_norm = 0.000989
  l2.weight: grad_norm = 0.465764
Total gradient norm: 0.912419
=== Actor Training Debug (Iteration 5399) ===
Q mean: -71.265717
Q std: 24.310530
Actor loss: 71.269691
Action reg: 0.003977
  l1.weight: grad_norm = 0.102612
  l1.bias: grad_norm = 0.000688
  l2.weight: grad_norm = 0.257765
Total gradient norm: 0.517441
=== Actor Training Debug (Iteration 5400) ===
Q mean: -65.998413
Q std: 23.756298
Actor loss: 66.002388
Action reg: 0.003976
  l1.weight: grad_norm = 0.016809
  l1.bias: grad_norm = 0.001320
  l2.weight: grad_norm = 0.038186
Total gradient norm: 0.063939
=== Actor Training Debug (Iteration 5401) ===
Q mean: -67.614426
Q std: 23.299013
Actor loss: 67.618416
Action reg: 0.003992
  l1.weight: grad_norm = 0.080251
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.185364
Total gradient norm: 0.365090
=== Actor Training Debug (Iteration 5402) ===
Q mean: -70.634766
Q std: 24.236927
Actor loss: 70.638748
Action reg: 0.003980
  l1.weight: grad_norm = 0.135770
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.328321
Total gradient norm: 0.626208
=== Actor Training Debug (Iteration 5403) ===
Q mean: -69.652031
Q std: 23.975325
Actor loss: 69.656013
Action reg: 0.003981
  l1.weight: grad_norm = 0.083152
  l1.bias: grad_norm = 0.000941
  l2.weight: grad_norm = 0.216222
Total gradient norm: 0.343857
=== Actor Training Debug (Iteration 5404) ===
Q mean: -67.324554
Q std: 23.250996
Actor loss: 67.328537
Action reg: 0.003979
  l1.weight: grad_norm = 0.147048
  l1.bias: grad_norm = 0.000698
  l2.weight: grad_norm = 0.359754
Total gradient norm: 0.833487
=== Actor Training Debug (Iteration 5405) ===
Q mean: -71.441666
Q std: 22.237762
Actor loss: 71.445663
Action reg: 0.003995
  l1.weight: grad_norm = 0.088832
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.226188
Total gradient norm: 0.438768
=== Actor Training Debug (Iteration 5406) ===
Q mean: -73.811462
Q std: 22.305376
Actor loss: 73.815453
Action reg: 0.003990
  l1.weight: grad_norm = 0.030404
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.066702
Total gradient norm: 0.118712
=== Actor Training Debug (Iteration 5407) ===
Q mean: -68.050613
Q std: 23.681091
Actor loss: 68.054596
Action reg: 0.003986
  l1.weight: grad_norm = 0.087954
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.194432
Total gradient norm: 0.366615
=== Actor Training Debug (Iteration 5408) ===
Q mean: -66.008453
Q std: 23.629791
Actor loss: 66.012436
Action reg: 0.003981
  l1.weight: grad_norm = 0.016292
  l1.bias: grad_norm = 0.000909
  l2.weight: grad_norm = 0.044153
Total gradient norm: 0.086979
=== Actor Training Debug (Iteration 5409) ===
Q mean: -66.197632
Q std: 23.927357
Actor loss: 66.201614
Action reg: 0.003981
  l1.weight: grad_norm = 0.079683
  l1.bias: grad_norm = 0.000815
  l2.weight: grad_norm = 0.220852
Total gradient norm: 0.486163
=== Actor Training Debug (Iteration 5410) ===
Q mean: -71.063004
Q std: 23.391169
Actor loss: 71.066994
Action reg: 0.003993
  l1.weight: grad_norm = 0.083290
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.188110
Total gradient norm: 0.303891
=== Actor Training Debug (Iteration 5411) ===
Q mean: -69.261124
Q std: 23.835909
Actor loss: 69.265114
Action reg: 0.003988
  l1.weight: grad_norm = 0.139956
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.279686
Total gradient norm: 0.515746
=== Actor Training Debug (Iteration 5412) ===
Q mean: -69.793152
Q std: 24.016594
Actor loss: 69.797142
Action reg: 0.003990
  l1.weight: grad_norm = 0.033321
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.075364
Total gradient norm: 0.158353
=== Actor Training Debug (Iteration 5413) ===
Q mean: -69.336594
Q std: 22.380911
Actor loss: 69.340591
Action reg: 0.003998
  l1.weight: grad_norm = 0.077169
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.194028
Total gradient norm: 0.431996
=== Actor Training Debug (Iteration 5414) ===
Q mean: -69.959618
Q std: 23.151562
Actor loss: 69.963608
Action reg: 0.003988
  l1.weight: grad_norm = 0.066613
  l1.bias: grad_norm = 0.000651
  l2.weight: grad_norm = 0.138761
Total gradient norm: 0.235207
=== Actor Training Debug (Iteration 5415) ===
Q mean: -69.566307
Q std: 23.037691
Actor loss: 69.570297
Action reg: 0.003987
  l1.weight: grad_norm = 0.033935
  l1.bias: grad_norm = 0.000704
  l2.weight: grad_norm = 0.082761
Total gradient norm: 0.178375
=== Actor Training Debug (Iteration 5416) ===
Q mean: -69.671158
Q std: 24.589500
Actor loss: 69.675140
Action reg: 0.003986
  l1.weight: grad_norm = 0.092758
  l1.bias: grad_norm = 0.000688
  l2.weight: grad_norm = 0.193808
Total gradient norm: 0.322013
=== Actor Training Debug (Iteration 5417) ===
Q mean: -67.312408
Q std: 24.326458
Actor loss: 67.316383
Action reg: 0.003975
  l1.weight: grad_norm = 0.242676
  l1.bias: grad_norm = 0.000897
  l2.weight: grad_norm = 0.506569
Total gradient norm: 0.898771
=== Actor Training Debug (Iteration 5418) ===
Q mean: -67.060989
Q std: 22.976938
Actor loss: 67.064987
Action reg: 0.003995
  l1.weight: grad_norm = 0.105064
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.250666
Total gradient norm: 0.548226
=== Actor Training Debug (Iteration 5419) ===
Q mean: -68.184235
Q std: 23.727936
Actor loss: 68.188225
Action reg: 0.003991
  l1.weight: grad_norm = 0.005224
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.011039
Total gradient norm: 0.029939
=== Actor Training Debug (Iteration 5420) ===
Q mean: -69.200218
Q std: 24.560665
Actor loss: 69.204193
Action reg: 0.003978
  l1.weight: grad_norm = 0.073517
  l1.bias: grad_norm = 0.000758
  l2.weight: grad_norm = 0.183585
Total gradient norm: 0.371313
=== Actor Training Debug (Iteration 5421) ===
Q mean: -70.762459
Q std: 22.123404
Actor loss: 70.766449
Action reg: 0.003993
  l1.weight: grad_norm = 0.170084
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.394133
Total gradient norm: 0.756703
=== Actor Training Debug (Iteration 5422) ===
Q mean: -71.243134
Q std: 23.192562
Actor loss: 71.247124
Action reg: 0.003990
  l1.weight: grad_norm = 0.089006
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.214342
Total gradient norm: 0.388018
=== Actor Training Debug (Iteration 5423) ===
Q mean: -71.856720
Q std: 22.781202
Actor loss: 71.860710
Action reg: 0.003994
  l1.weight: grad_norm = 0.088283
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.212994
Total gradient norm: 0.478726
=== Actor Training Debug (Iteration 5424) ===
Q mean: -69.486794
Q std: 25.057396
Actor loss: 69.490768
Action reg: 0.003973
  l1.weight: grad_norm = 0.096803
  l1.bias: grad_norm = 0.001314
  l2.weight: grad_norm = 0.228484
Total gradient norm: 0.413410
=== Actor Training Debug (Iteration 5425) ===
Q mean: -67.573174
Q std: 22.524740
Actor loss: 67.577156
Action reg: 0.003983
  l1.weight: grad_norm = 0.037595
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.094783
Total gradient norm: 0.179145
=== Actor Training Debug (Iteration 5426) ===
Q mean: -69.848465
Q std: 23.574148
Actor loss: 69.852448
Action reg: 0.003986
  l1.weight: grad_norm = 0.038963
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.104042
Total gradient norm: 0.226189
=== Actor Training Debug (Iteration 5427) ===
Q mean: -70.167374
Q std: 23.210787
Actor loss: 70.171364
Action reg: 0.003989
  l1.weight: grad_norm = 0.169569
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.371333
Total gradient norm: 0.678183
=== Actor Training Debug (Iteration 5428) ===
Q mean: -72.131927
Q std: 23.992987
Actor loss: 72.135910
Action reg: 0.003985
  l1.weight: grad_norm = 0.161676
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.350178
Total gradient norm: 0.570240
=== Actor Training Debug (Iteration 5429) ===
Q mean: -67.869049
Q std: 23.899271
Actor loss: 67.873032
Action reg: 0.003986
  l1.weight: grad_norm = 0.030024
  l1.bias: grad_norm = 0.000572
  l2.weight: grad_norm = 0.070848
Total gradient norm: 0.128839
=== Actor Training Debug (Iteration 5430) ===
Q mean: -71.055389
Q std: 22.657784
Actor loss: 71.059380
Action reg: 0.003990
  l1.weight: grad_norm = 0.075782
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.173689
Total gradient norm: 0.261091
=== Actor Training Debug (Iteration 5431) ===
Q mean: -67.915771
Q std: 22.676409
Actor loss: 67.919754
Action reg: 0.003982
  l1.weight: grad_norm = 0.060413
  l1.bias: grad_norm = 0.000860
  l2.weight: grad_norm = 0.147145
Total gradient norm: 0.239024
=== Actor Training Debug (Iteration 5432) ===
Q mean: -70.222687
Q std: 22.976202
Actor loss: 70.226677
Action reg: 0.003986
  l1.weight: grad_norm = 0.363408
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.752957
Total gradient norm: 1.380276
=== Actor Training Debug (Iteration 5433) ===
Q mean: -68.256012
Q std: 23.561739
Actor loss: 68.259995
Action reg: 0.003979
  l1.weight: grad_norm = 0.150933
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.416412
Total gradient norm: 0.908255
=== Actor Training Debug (Iteration 5434) ===
Q mean: -71.150833
Q std: 23.718170
Actor loss: 71.154823
Action reg: 0.003991
  l1.weight: grad_norm = 0.045730
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.127518
Total gradient norm: 0.275174
=== Actor Training Debug (Iteration 5435) ===
Q mean: -69.945114
Q std: 24.077875
Actor loss: 69.949097
Action reg: 0.003983
  l1.weight: grad_norm = 0.057306
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.125139
Total gradient norm: 0.213453
=== Actor Training Debug (Iteration 5436) ===
Q mean: -71.712212
Q std: 25.199682
Actor loss: 71.716194
Action reg: 0.003980
  l1.weight: grad_norm = 0.135255
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.320203
Total gradient norm: 0.602279
=== Actor Training Debug (Iteration 5437) ===
Q mean: -68.196243
Q std: 23.633698
Actor loss: 68.200226
Action reg: 0.003984
  l1.weight: grad_norm = 0.068130
  l1.bias: grad_norm = 0.000803
  l2.weight: grad_norm = 0.182120
Total gradient norm: 0.393045
=== Actor Training Debug (Iteration 5438) ===
Q mean: -68.928970
Q std: 23.322128
Actor loss: 68.932953
Action reg: 0.003986
  l1.weight: grad_norm = 0.126678
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.253637
Total gradient norm: 0.341228
=== Actor Training Debug (Iteration 5439) ===
Q mean: -71.000099
Q std: 23.267982
Actor loss: 71.004089
Action reg: 0.003992
  l1.weight: grad_norm = 0.030026
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.078116
Total gradient norm: 0.158389
=== Actor Training Debug (Iteration 5440) ===
Q mean: -71.268272
Q std: 24.947836
Actor loss: 71.272247
Action reg: 0.003972
  l1.weight: grad_norm = 0.045478
  l1.bias: grad_norm = 0.001240
  l2.weight: grad_norm = 0.136362
Total gradient norm: 0.285727
=== Actor Training Debug (Iteration 5441) ===
Q mean: -70.563538
Q std: 23.420900
Actor loss: 70.567528
Action reg: 0.003990
  l1.weight: grad_norm = 0.072960
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.151123
Total gradient norm: 0.270594
=== Actor Training Debug (Iteration 5442) ===
Q mean: -67.614510
Q std: 22.615690
Actor loss: 67.618507
Action reg: 0.003994
  l1.weight: grad_norm = 0.186473
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.421259
Total gradient norm: 0.681929
=== Actor Training Debug (Iteration 5443) ===
Q mean: -68.634308
Q std: 23.469608
Actor loss: 68.638306
Action reg: 0.003994
  l1.weight: grad_norm = 0.050922
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.113175
Total gradient norm: 0.217800
=== Actor Training Debug (Iteration 5444) ===
Q mean: -68.348976
Q std: 23.199028
Actor loss: 68.352959
Action reg: 0.003980
  l1.weight: grad_norm = 0.082873
  l1.bias: grad_norm = 0.000861
  l2.weight: grad_norm = 0.197598
Total gradient norm: 0.340065
=== Actor Training Debug (Iteration 5445) ===
Q mean: -72.277054
Q std: 22.925894
Actor loss: 72.281044
Action reg: 0.003991
  l1.weight: grad_norm = 0.146943
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.433135
Total gradient norm: 0.858029
=== Actor Training Debug (Iteration 5446) ===
Q mean: -67.292465
Q std: 24.737804
Actor loss: 67.296440
Action reg: 0.003972
  l1.weight: grad_norm = 0.125268
  l1.bias: grad_norm = 0.001000
  l2.weight: grad_norm = 0.264782
Total gradient norm: 0.573402
=== Actor Training Debug (Iteration 5447) ===
Q mean: -69.668198
Q std: 24.308769
Actor loss: 69.672180
Action reg: 0.003980
  l1.weight: grad_norm = 0.007554
  l1.bias: grad_norm = 0.000895
  l2.weight: grad_norm = 0.015907
Total gradient norm: 0.035023
=== Actor Training Debug (Iteration 5448) ===
Q mean: -65.763733
Q std: 25.571991
Actor loss: 65.767700
Action reg: 0.003970
  l1.weight: grad_norm = 0.087233
  l1.bias: grad_norm = 0.001366
  l2.weight: grad_norm = 0.237449
Total gradient norm: 0.565364
=== Actor Training Debug (Iteration 5449) ===
Q mean: -67.656059
Q std: 25.520590
Actor loss: 67.660042
Action reg: 0.003981
  l1.weight: grad_norm = 0.469522
  l1.bias: grad_norm = 0.001004
  l2.weight: grad_norm = 0.885239
Total gradient norm: 1.763001
=== Actor Training Debug (Iteration 5450) ===
Q mean: -69.834839
Q std: 23.879232
Actor loss: 69.838837
Action reg: 0.003997
  l1.weight: grad_norm = 0.096992
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.249426
Total gradient norm: 0.436794
=== Actor Training Debug (Iteration 5451) ===
Q mean: -68.281067
Q std: 24.173273
Actor loss: 68.285042
Action reg: 0.003977
  l1.weight: grad_norm = 0.006498
  l1.bias: grad_norm = 0.000884
  l2.weight: grad_norm = 0.016112
Total gradient norm: 0.034509
=== Actor Training Debug (Iteration 5452) ===
Q mean: -68.256683
Q std: 22.173193
Actor loss: 68.260666
Action reg: 0.003983
  l1.weight: grad_norm = 0.026922
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.067110
Total gradient norm: 0.121078
=== Actor Training Debug (Iteration 5453) ===
Q mean: -68.653053
Q std: 25.561754
Actor loss: 68.657028
Action reg: 0.003975
  l1.weight: grad_norm = 0.036083
  l1.bias: grad_norm = 0.001187
  l2.weight: grad_norm = 0.083691
Total gradient norm: 0.170996
=== Actor Training Debug (Iteration 5454) ===
Q mean: -69.596687
Q std: 24.845243
Actor loss: 69.600662
Action reg: 0.003972
  l1.weight: grad_norm = 0.192760
  l1.bias: grad_norm = 0.001010
  l2.weight: grad_norm = 0.499180
Total gradient norm: 1.015354
=== Actor Training Debug (Iteration 5455) ===
Q mean: -68.212616
Q std: 23.351252
Actor loss: 68.216606
Action reg: 0.003993
  l1.weight: grad_norm = 0.061272
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.139980
Total gradient norm: 0.236391
=== Actor Training Debug (Iteration 5456) ===
Q mean: -68.083977
Q std: 24.630465
Actor loss: 68.087952
Action reg: 0.003978
  l1.weight: grad_norm = 0.180559
  l1.bias: grad_norm = 0.000934
  l2.weight: grad_norm = 0.429037
Total gradient norm: 0.747166
=== Actor Training Debug (Iteration 5457) ===
Q mean: -71.638626
Q std: 23.628435
Actor loss: 71.642609
Action reg: 0.003981
  l1.weight: grad_norm = 0.144575
  l1.bias: grad_norm = 0.000620
  l2.weight: grad_norm = 0.291612
Total gradient norm: 0.617908
=== Actor Training Debug (Iteration 5458) ===
Q mean: -70.952942
Q std: 23.869232
Actor loss: 70.956940
Action reg: 0.003995
  l1.weight: grad_norm = 0.010638
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.028706
Total gradient norm: 0.056481
=== Actor Training Debug (Iteration 5459) ===
Q mean: -67.257179
Q std: 23.548235
Actor loss: 67.261162
Action reg: 0.003983
  l1.weight: grad_norm = 0.238110
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.485414
Total gradient norm: 0.920183
=== Actor Training Debug (Iteration 5460) ===
Q mean: -69.650055
Q std: 23.178123
Actor loss: 69.654045
Action reg: 0.003988
  l1.weight: grad_norm = 0.176500
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.422541
Total gradient norm: 0.751446
=== Actor Training Debug (Iteration 5461) ===
Q mean: -70.303886
Q std: 23.417969
Actor loss: 70.307869
Action reg: 0.003985
  l1.weight: grad_norm = 0.138104
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.363845
Total gradient norm: 0.666844
Total gradient norm: 0.4315214696on 1203) ===
=== Actor Training Debug (Iteration 5472) ===
Q mean: -72.337830
Q std: 24.312557
Actor loss: 72.341820
Action reg: 0.003989
  l1.weight: grad_norm = 0.173074
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.383719
Total gradient norm: 0.637970
=== Actor Training Debug (Iteration 5473) ===
Q mean: -69.394852
Q std: 24.263288
Actor loss: 69.398834
Action reg: 0.003981
  l1.weight: grad_norm = 0.102733
  l1.bias: grad_norm = 0.000834
  l2.weight: grad_norm = 0.276183
Total gradient norm: 0.605747
=== Actor Training Debug (Iteration 5474) ===
Q mean: -68.604836
Q std: 24.061848
Actor loss: 68.608818
Action reg: 0.003984
  l1.weight: grad_norm = 0.046689
  l1.bias: grad_norm = 0.000674
  l2.weight: grad_norm = 0.110321
Total gradient norm: 0.208975
=== Actor Training Debug (Iteration 5475) ===
Q mean: -71.305237
Q std: 22.698368
Actor loss: 71.309227
Action reg: 0.003987
  l1.weight: grad_norm = 0.078390
  l1.bias: grad_norm = 0.000541
  l2.weight: grad_norm = 0.161287
Total gradient norm: 0.270263
=== Actor Training Debug (Iteration 5476) ===
Q mean: -71.736702
Q std: 22.059200
Actor loss: 71.740692
Action reg: 0.003988
  l1.weight: grad_norm = 0.211523
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.408747
Total gradient norm: 0.911189
=== Actor Training Debug (Iteration 5477) ===
Q mean: -68.337234
Q std: 24.218323
Actor loss: 68.341225
Action reg: 0.003988
  l1.weight: grad_norm = 0.027349
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.054471
Total gradient norm: 0.099112
=== Actor Training Debug (Iteration 5478) ===
Q mean: -67.681335
Q std: 24.409655
Actor loss: 67.685310
Action reg: 0.003975
  l1.weight: grad_norm = 0.387690
  l1.bias: grad_norm = 0.000505
  l2.weight: grad_norm = 0.991935
Total gradient norm: 1.934660
=== Actor Training Debug (Iteration 5479) ===
Q mean: -69.346573
Q std: 24.162031
Actor loss: 69.350563
Action reg: 0.003994
  l1.weight: grad_norm = 0.019442
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.049377
Total gradient norm: 0.094183
=== Actor Training Debug (Iteration 5480) ===
Q mean: -69.330856
Q std: 25.666523
Actor loss: 69.334839
Action reg: 0.003979
  l1.weight: grad_norm = 0.029548
  l1.bias: grad_norm = 0.000926
  l2.weight: grad_norm = 0.074984
Total gradient norm: 0.140856
=== Actor Training Debug (Iteration 5481) ===
Q mean: -67.726028
Q std: 22.708532
Actor loss: 67.730003
Action reg: 0.003977
  l1.weight: grad_norm = 0.222445
  l1.bias: grad_norm = 0.000797
  l2.weight: grad_norm = 0.500966
Total gradient norm: 0.747421
=== Actor Training Debug (Iteration 5482) ===
Q mean: -70.641815
Q std: 23.127628
Actor loss: 70.645805
Action reg: 0.003989
  l1.weight: grad_norm = 0.087238
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.177387
Total gradient norm: 0.318723
=== Actor Training Debug (Iteration 5483) ===
Q mean: -72.633598
Q std: 23.949802
Actor loss: 72.637589
Action reg: 0.003987
  l1.weight: grad_norm = 0.189926
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.425424
Total gradient norm: 0.779947
=== Actor Training Debug (Iteration 5484) ===
Q mean: -69.424896
Q std: 24.011614
Actor loss: 69.428879
Action reg: 0.003979
  l1.weight: grad_norm = 0.320120
  l1.bias: grad_norm = 0.000703
  l2.weight: grad_norm = 0.704423
Total gradient norm: 1.218998
=== Actor Training Debug (Iteration 5485) ===
Q mean: -70.089195
Q std: 24.217129
Actor loss: 70.093178
Action reg: 0.003982
  l1.weight: grad_norm = 0.305408
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.736065
Total gradient norm: 1.203249
=== Actor Training Debug (Iteration 5486) ===
Q mean: -68.046326
Q std: 23.264355
Actor loss: 68.050308
Action reg: 0.003985
  l1.weight: grad_norm = 0.115086
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.233378
Total gradient norm: 0.377781
=== Actor Training Debug (Iteration 5487) ===
Q mean: -71.138474
Q std: 24.061245
Actor loss: 71.142448
Action reg: 0.003977
  l1.weight: grad_norm = 0.009616
  l1.bias: grad_norm = 0.000987
  l2.weight: grad_norm = 0.021804
Total gradient norm: 0.040620
=== Actor Training Debug (Iteration 5488) ===
Q mean: -69.587753
Q std: 23.987328
Actor loss: 69.591743
Action reg: 0.003987
  l1.weight: grad_norm = 0.050322
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.127627
Total gradient norm: 0.256004
=== Actor Training Debug (Iteration 5489) ===
Q mean: -67.508698
Q std: 25.064146
Actor loss: 67.512680
Action reg: 0.003979
  l1.weight: grad_norm = 0.007033
  l1.bias: grad_norm = 0.000865
  l2.weight: grad_norm = 0.013784
Total gradient norm: 0.022566
=== Actor Training Debug (Iteration 5490) ===
Q mean: -67.680313
Q std: 23.527500
Actor loss: 67.684303
Action reg: 0.003987
  l1.weight: grad_norm = 0.151227
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.331854
Total gradient norm: 0.573492
=== Actor Training Debug (Iteration 5491) ===
Q mean: -72.232758
Q std: 22.233824
Actor loss: 72.236755
Action reg: 0.003996
  l1.weight: grad_norm = 0.050846
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.110045
Total gradient norm: 0.186957
=== Actor Training Debug (Iteration 5492) ===
Q mean: -69.744965
Q std: 24.905529
Actor loss: 69.748947
Action reg: 0.003980
  l1.weight: grad_norm = 0.212057
  l1.bias: grad_norm = 0.000728
  l2.weight: grad_norm = 0.449270
Total gradient norm: 0.873534
=== Actor Training Debug (Iteration 5493) ===
Q mean: -67.282402
Q std: 23.742849
Actor loss: 67.286385
Action reg: 0.003979
  l1.weight: grad_norm = 0.199411
  l1.bias: grad_norm = 0.000749
  l2.weight: grad_norm = 0.429344
Total gradient norm: 0.730437
=== Actor Training Debug (Iteration 5494) ===
Q mean: -68.587311
Q std: 24.405828
Actor loss: 68.591293
Action reg: 0.003982
  l1.weight: grad_norm = 0.042526
  l1.bias: grad_norm = 0.000982
  l2.weight: grad_norm = 0.088403
Total gradient norm: 0.154666
=== Actor Training Debug (Iteration 5495) ===
Q mean: -68.881882
Q std: 23.226734
Actor loss: 68.885872
Action reg: 0.003991
  l1.weight: grad_norm = 0.081753
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.178951
Total gradient norm: 0.352203
=== Actor Training Debug (Iteration 5496) ===
Q mean: -69.928299
Q std: 25.365284
Actor loss: 69.932281
Action reg: 0.003982
  l1.weight: grad_norm = 0.150942
  l1.bias: grad_norm = 0.000811
  l2.weight: grad_norm = 0.373584
Total gradient norm: 0.827654
=== Actor Training Debug (Iteration 5497) ===
Q mean: -69.717094
Q std: 23.469227
Actor loss: 69.721085
Action reg: 0.003988
  l1.weight: grad_norm = 0.066860
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.169492
Total gradient norm: 0.344455
=== Actor Training Debug (Iteration 5498) ===
Q mean: -71.471939
Q std: 23.466490
Actor loss: 71.475922
Action reg: 0.003983
  l1.weight: grad_norm = 0.053344
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.125547
Total gradient norm: 0.239249
=== Actor Training Debug (Iteration 5499) ===
Q mean: -69.334061
Q std: 24.310072
Actor loss: 69.338051
Action reg: 0.003990
  l1.weight: grad_norm = 0.141122
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.410383
Total gradient norm: 0.739702
=== Actor Training Debug (Iteration 5500) ===
Q mean: -67.513321
Q std: 25.019064
Actor loss: 67.517296
Action reg: 0.003977
  l1.weight: grad_norm = 0.180832
  l1.bias: grad_norm = 0.000837
  l2.weight: grad_norm = 0.378050
Total gradient norm: 0.707821
  Average reward: -334.557 | Average length: 100.0
Evaluation at episode 105: -334.557
=== Actor Training Debug (Iteration 5501) ===
Q mean: -72.247635
Q std: 23.837704
Actor loss: 72.251633
Action reg: 0.003995
  l1.weight: grad_norm = 0.076748
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.188819
Total gradient norm: 0.330452
=== Actor Training Debug (Iteration 5502) ===
Q mean: -70.458778
Q std: 23.619493
Actor loss: 70.462761
Action reg: 0.003985
  l1.weight: grad_norm = 0.010615
  l1.bias: grad_norm = 0.000820
  l2.weight: grad_norm = 0.021008
Total gradient norm: 0.035019
=== Actor Training Debug (Iteration 5503) ===
Q mean: -67.970474
Q std: 23.100395
Actor loss: 67.974464
Action reg: 0.003990
  l1.weight: grad_norm = 0.354052
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.731247
Total gradient norm: 1.127239
=== Actor Training Debug (Iteration 5504) ===
Q mean: -70.330307
Q std: 24.632442
Actor loss: 70.334290
Action reg: 0.003981
  l1.weight: grad_norm = 0.152531
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.321057
Total gradient norm: 0.510309
=== Actor Training Debug (Iteration 5505) ===
Q mean: -73.087021
Q std: 22.522238
Actor loss: 73.091011
Action reg: 0.003987
  l1.weight: grad_norm = 0.185409
  l1.bias: grad_norm = 0.000538
  l2.weight: grad_norm = 0.457450
Total gradient norm: 0.738091
=== Actor Training Debug (Iteration 5506) ===
Q mean: -71.027451
Q std: 22.821272
Actor loss: 71.031433
Action reg: 0.003985
  l1.weight: grad_norm = 0.124933
  l1.bias: grad_norm = 0.000559
  l2.weight: grad_norm = 0.275193
Total gradient norm: 0.550801
=== Actor Training Debug (Iteration 5507) ===
Q mean: -70.105354
Q std: 23.341578
Actor loss: 70.109344
Action reg: 0.003991
  l1.weight: grad_norm = 0.035829
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.083590
Total gradient norm: 0.174974
=== Actor Training Debug (Iteration 5508) ===
Q mean: -70.279167
Q std: 24.159998
Actor loss: 70.283157
Action reg: 0.003988
  l1.weight: grad_norm = 0.038942
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.077685
Total gradient norm: 0.160258
=== Actor Training Debug (Iteration 5509) ===
Q mean: -69.450745
Q std: 24.677874
Actor loss: 69.454727
Action reg: 0.003985
  l1.weight: grad_norm = 0.039828
  l1.bias: grad_norm = 0.000709
  l2.weight: grad_norm = 0.076289
Total gradient norm: 0.164899
=== Actor Training Debug (Iteration 5510) ===
Q mean: -71.532242
Q std: 23.202364
Actor loss: 71.536217
Action reg: 0.003978
  l1.weight: grad_norm = 0.149566
  l1.bias: grad_norm = 0.000958
  l2.weight: grad_norm = 0.327865
Total gradient norm: 0.622105
=== Actor Training Debug (Iteration 5511) ===
Q mean: -71.831696
Q std: 25.225073
Actor loss: 71.835678
Action reg: 0.003980
  l1.weight: grad_norm = 0.034412
  l1.bias: grad_norm = 0.000949
  l2.weight: grad_norm = 0.068541
Total gradient norm: 0.118822
=== Actor Training Debug (Iteration 5512) ===
Q mean: -70.236610
Q std: 23.599211
Actor loss: 70.240601
Action reg: 0.003988
  l1.weight: grad_norm = 0.058999
  l1.bias: grad_norm = 0.000661
  l2.weight: grad_norm = 0.111871
Total gradient norm: 0.220137
=== Actor Training Debug (Iteration 5513) ===
Q mean: -70.313126
Q std: 24.502014
Actor loss: 70.317108
Action reg: 0.003981
  l1.weight: grad_norm = 0.255788
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.624228
Total gradient norm: 1.149224
=== Actor Training Debug (Iteration 5514) ===
Q mean: -68.992493
Q std: 25.067986
Actor loss: 68.996475
Action reg: 0.003983
  l1.weight: grad_norm = 0.071234
  l1.bias: grad_norm = 0.000855
  l2.weight: grad_norm = 0.174445
Total gradient norm: 0.308106
=== Actor Training Debug (Iteration 5515) ===
Q mean: -67.522118
Q std: 24.600292
Actor loss: 67.526108
Action reg: 0.003987
  l1.weight: grad_norm = 0.343729
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.673662
Total gradient norm: 1.189534
=== Actor Training Debug (Iteration 5516) ===
Q mean: -68.489281
Q std: 25.236795
Actor loss: 68.493263
Action reg: 0.003980
  l1.weight: grad_norm = 0.160314
  l1.bias: grad_norm = 0.000907
  l2.weight: grad_norm = 0.359451
Total gradient norm: 0.774657
=== Actor Training Debug (Iteration 5517) ===
Q mean: -70.251755
Q std: 24.657030
Actor loss: 70.255745
Action reg: 0.003989
  l1.weight: grad_norm = 0.127504
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.282651
Total gradient norm: 0.447759
=== Actor Training Debug (Iteration 5518) ===
Q mean: -69.422546
Q std: 24.774487
Actor loss: 69.426529
Action reg: 0.003985
  l1.weight: grad_norm = 0.086503
  l1.bias: grad_norm = 0.000800
  l2.weight: grad_norm = 0.214088
Total gradient norm: 0.366742
=== Actor Training Debug (Iteration 5519) ===
Q mean: -65.716705
Q std: 22.888969
Actor loss: 65.720703
Action reg: 0.003995
  l1.weight: grad_norm = 0.058622
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.164828
Total gradient norm: 0.323153
=== Actor Training Debug (Iteration 5520) ===
Q mean: -70.941940
Q std: 23.021193
Actor loss: 70.945930
Action reg: 0.003993
  l1.weight: grad_norm = 0.030108
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.068958
Total gradient norm: 0.127018
=== Actor Training Debug (Iteration 5521) ===
Q mean: -68.347198
Q std: 23.122583
Actor loss: 68.351181
Action reg: 0.003984
  l1.weight: grad_norm = 0.038207
  l1.bias: grad_norm = 0.000624
  l2.weight: grad_norm = 0.081947
Total gradient norm: 0.149728
=== Actor Training Debug (Iteration 5522) ===
Q mean: -67.311325
Q std: 23.430962
Actor loss: 67.315315
Action reg: 0.003989
  l1.weight: grad_norm = 0.164276
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.358451
Total gradient norm: 0.724394
=== Actor Training Debug (Iteration 5523) ===
Q mean: -69.341736
Q std: 23.625645
Actor loss: 69.345711
Action reg: 0.003971
  l1.weight: grad_norm = 0.377497
  l1.bias: grad_norm = 0.001195
  l2.weight: grad_norm = 0.722133
Total gradient norm: 1.004213
=== Actor Training Debug (Iteration 5524) ===
Q mean: -71.925499
Q std: 24.070267
Actor loss: 71.929489
Action reg: 0.003988
  l1.weight: grad_norm = 0.091500
  l1.bias: grad_norm = 0.000754
  l2.weight: grad_norm = 0.192651
Total gradient norm: 0.350843
=== Actor Training Debug (Iteration 5525) ===
Q mean: -71.615852
Q std: 23.834879
Actor loss: 71.619835
Action reg: 0.003983
  l1.weight: grad_norm = 0.065373
  l1.bias: grad_norm = 0.000769
  l2.weight: grad_norm = 0.157237
Total gradient norm: 0.330621
=== Actor Training Debug (Iteration 5526) ===
Q mean: -70.753754
Q std: 22.904202
Actor loss: 70.757744
Action reg: 0.003988
  l1.weight: grad_norm = 0.155477
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.310210
Total gradient norm: 0.412683
=== Actor Training Debug (Iteration 5527) ===
Q mean: -70.385345
Q std: 24.336107
Actor loss: 70.389320
Action reg: 0.003977
  l1.weight: grad_norm = 0.094876
  l1.bias: grad_norm = 0.000878
  l2.weight: grad_norm = 0.211976
Total gradient norm: 0.341894
=== Actor Training Debug (Iteration 5528) ===
Q mean: -69.395721
Q std: 25.121712
Actor loss: 69.399696
Action reg: 0.003972
  l1.weight: grad_norm = 0.101497
  l1.bias: grad_norm = 0.001609
  l2.weight: grad_norm = 0.245499
Total gradient norm: 0.444537
=== Actor Training Debug (Iteration 5529) ===
Q mean: -67.570457
Q std: 24.095768
Actor loss: 67.574432
Action reg: 0.003974
  l1.weight: grad_norm = 0.175369
  l1.bias: grad_norm = 0.000524
  l2.weight: grad_norm = 0.368840
Total gradient norm: 0.635942
=== Actor Training Debug (Iteration 5530) ===
Q mean: -70.334366
Q std: 24.713308
Actor loss: 70.338333
Action reg: 0.003971
  l1.weight: grad_norm = 0.152957
  l1.bias: grad_norm = 0.001424
  l2.weight: grad_norm = 0.302532
Total gradient norm: 0.451593
=== Actor Training Debug (Iteration 5531) ===
Q mean: -70.518906
Q std: 22.987963
Actor loss: 70.522888
Action reg: 0.003984
  l1.weight: grad_norm = 0.043700
  l1.bias: grad_norm = 0.000861
  l2.weight: grad_norm = 0.123422
Total gradient norm: 0.250785
=== Actor Training Debug (Iteration 5532) ===
Q mean: -69.202988
Q std: 23.618248
Actor loss: 69.206978
Action reg: 0.003990
  l1.weight: grad_norm = 0.083426
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.171707
Total gradient norm: 0.313507
=== Actor Training Debug (Iteration 5533) ===
Q mean: -68.309280
Q std: 24.433208
Actor loss: 68.313271
Action reg: 0.003992
  l1.weight: grad_norm = 0.043406
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.094063
Total gradient norm: 0.181634
=== Actor Training Debug (Iteration 5534) ===
Q mean: -67.345261
Q std: 24.428251
Actor loss: 67.349251
Action reg: 0.003988
  l1.weight: grad_norm = 0.230625
  l1.bias: grad_norm = 0.000654
  l2.weight: grad_norm = 0.548652
Total gradient norm: 0.885404
=== Actor Training Debug (Iteration 5535) ===
Q mean: -70.602356
Q std: 24.484379
Actor loss: 70.606346
Action reg: 0.003988
  l1.weight: grad_norm = 0.071642
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.137802
Total gradient norm: 0.228552
=== Actor Training Debug (Iteration 5536) ===
Q mean: -70.218506
Q std: 23.595131
Actor loss: 70.222496
Action reg: 0.003991
  l1.weight: grad_norm = 0.119770
  l1.bias: grad_norm = 0.000518
  l2.weight: grad_norm = 0.254173
Total gradient norm: 0.442794
=== Actor Training Debug (Iteration 5537) ===
Q mean: -71.956879
Q std: 23.399855
Actor loss: 71.960869
Action reg: 0.003988
  l1.weight: grad_norm = 0.150861
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.344122
Total gradient norm: 0.623468
=== Actor Training Debug (Iteration 5538) ===
Q mean: -68.890999
Q std: 24.136778
Actor loss: 68.894981
Action reg: 0.003979
  l1.weight: grad_norm = 0.212322
  l1.bias: grad_norm = 0.000891
  l2.weight: grad_norm = 0.472228
Total gradient norm: 0.767265
=== Actor Training Debug (Iteration 5539) ===
Q mean: -69.802963
Q std: 24.864241
Actor loss: 69.806946
Action reg: 0.003986
  l1.weight: grad_norm = 0.148314
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.332366
Total gradient norm: 0.671607
=== Actor Training Debug (Iteration 5540) ===
Q mean: -69.396812
Q std: 23.334734
Actor loss: 69.400795
Action reg: 0.003985
  l1.weight: grad_norm = 0.046335
  l1.bias: grad_norm = 0.000909
  l2.weight: grad_norm = 0.100167
Total gradient norm: 0.176642
=== Actor Training Debug (Iteration 5541) ===
Q mean: -70.670929
Q std: 25.507713
Actor loss: 70.674896
Action reg: 0.003969
  l1.weight: grad_norm = 0.244552
  l1.bias: grad_norm = 0.001935
  l2.weight: grad_norm = 0.505562
Total gradient norm: 0.919361
=== Actor Training Debug (Iteration 5542) ===
Q mean: -69.351608
Q std: 23.246115
Actor loss: 69.355598
Action reg: 0.003992
  l1.weight: grad_norm = 0.084602
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.176662
Total gradient norm: 0.307372
=== Actor Training Debug (Iteration 5543) ===
Q mean: -67.978760
Q std: 23.625935
Actor loss: 67.982750
Action reg: 0.003987
  l1.weight: grad_norm = 0.063648
  l1.bias: grad_norm = 0.000646
  l2.weight: grad_norm = 0.153816
Total gradient norm: 0.319571
=== Actor Training Debug (Iteration 5544) ===
Q mean: -72.929428
Q std: 22.332783
Actor loss: 72.933418
Action reg: 0.003988
  l1.weight: grad_norm = 0.107127
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.243302
Total gradient norm: 0.435496
=== Actor Training Debug (Iteration 5545) ===
Q mean: -68.833160
Q std: 24.522694
Actor loss: 68.837151
Action reg: 0.003987
  l1.weight: grad_norm = 0.197186
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.485853
Total gradient norm: 0.977784
=== Actor Training Debug (Iteration 5546) ===
Q mean: -66.893692
Q std: 26.111118
Actor loss: 66.897659
Action reg: 0.003968
  l1.weight: grad_norm = 0.079538
  l1.bias: grad_norm = 0.001767
  l2.weight: grad_norm = 0.163643
Total gradient norm: 0.312748
=== Actor Training Debug (Iteration 5547) ===
Q mean: -68.190697
Q std: 24.177273
Actor loss: 68.194695
Action reg: 0.003997
  l1.weight: grad_norm = 0.062698
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.125835
Total gradient norm: 0.244018
=== Actor Training Debug (Iteration 5548) ===
Q mean: -67.060043
Q std: 24.588501
Actor loss: 67.064026
Action reg: 0.003984
  l1.weight: grad_norm = 0.090099
  l1.bias: grad_norm = 0.000535
  l2.weight: grad_norm = 0.177494
Total gradient norm: 0.238312
=== Actor Training Debug (Iteration 5549) ===
Q mean: -69.646217
Q std: 23.910088
Actor loss: 69.650200
Action reg: 0.003983
  l1.weight: grad_norm = 0.157249
  l1.bias: grad_norm = 0.000698
  l2.weight: grad_norm = 0.329603
Total gradient norm: 0.646437
=== Actor Training Debug (Iteration 5550) ===
Q mean: -71.173531
Q std: 24.262814
Actor loss: 71.177528
Action reg: 0.003996
  l1.weight: grad_norm = 0.013802
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.035390
Total gradient norm: 0.066297
=== Actor Training Debug (Iteration 5551) ===
Q mean: -69.023636
Q std: 23.620167
Actor loss: 69.027626
Action reg: 0.003994
  l1.weight: grad_norm = 0.071605
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.168952
Total gradient norm: 0.280663
=== Actor Training Debug (Iteration 5552) ===
Q mean: -69.540924
Q std: 24.374371
Actor loss: 69.544907
Action reg: 0.003986
  l1.weight: grad_norm = 0.089339
  l1.bias: grad_norm = 0.000726
  l2.weight: grad_norm = 0.224989
Total gradient norm: 0.365840
=== Actor Training Debug (Iteration 5553) ===
Q mean: -69.576935
Q std: 23.771996
Actor loss: 69.580925
Action reg: 0.003993
  l1.weight: grad_norm = 0.014483
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.031555
Total gradient norm: 0.062982
=== Actor Training Debug (Iteration 5554) ===
Q mean: -70.308647
Q std: 23.640305
Actor loss: 70.312645
Action reg: 0.003995
  l1.weight: grad_norm = 0.020465
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.044593
Total gradient norm: 0.083399
=== Actor Training Debug (Iteration 5555) ===
Q mean: -68.621017
Q std: 24.778391
Actor loss: 68.625000
Action reg: 0.003979
  l1.weight: grad_norm = 0.017059
  l1.bias: grad_norm = 0.001004
  l2.weight: grad_norm = 0.043864
Total gradient norm: 0.089140
=== Actor Training Debug (Iteration 5556) ===
Q mean: -67.040070
Q std: 25.756203
Actor loss: 67.044052
Action reg: 0.003980
  l1.weight: grad_norm = 0.013952
  l1.bias: grad_norm = 0.001186
  l2.weight: grad_norm = 0.033810
Total gradient norm: 0.078781
=== Actor Training Debug (Iteration 5557) ===
Q mean: -70.341034
Q std: 23.711189
Actor loss: 70.345016
Action reg: 0.003986
  l1.weight: grad_norm = 0.202465
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.446118
Total gradient norm: 0.862892
=== Actor Training Debug (Iteration 5558) ===
Q mean: -70.053169
Q std: 24.704210
Actor loss: 70.057152
Action reg: 0.003981
  l1.weight: grad_norm = 0.144174
  l1.bias: grad_norm = 0.001026
  l2.weight: grad_norm = 0.347020
Total gradient norm: 0.682442
=== Actor Training Debug (Iteration 5559) ===
Q mean: -71.132812
Q std: 23.864664
Actor loss: 71.136803
Action reg: 0.003990
  l1.weight: grad_norm = 0.139915
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.306634
Total gradient norm: 0.496544
=== Actor Training Debug (Iteration 5560) ===
Q mean: -66.987717
Q std: 23.761951
Actor loss: 66.991684
Action reg: 0.003968
  l1.weight: grad_norm = 0.166328
  l1.bias: grad_norm = 0.001327
  l2.weight: grad_norm = 0.334778
Total gradient norm: 0.521145
=== Actor Training Debug (Iteration 5561) ===
Q mean: -69.078384
Q std: 24.388021
Actor loss: 69.082375
Action reg: 0.003991
  l1.weight: grad_norm = 0.052281
  l1.bias: grad_norm = 0.000536
  l2.weight: grad_norm = 0.128833
Total gradient norm: 0.242973
=== Actor Training Debug (Iteration 5562) ===
Q mean: -70.761787
Q std: 24.419466
Actor loss: 70.765778
Action reg: 0.003987
  l1.weight: grad_norm = 0.227626
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.569815
Total gradient norm: 1.186009
=== Actor Training Debug (Iteration 5563) ===
Q mean: -72.144394
Q std: 24.006504
Actor loss: 72.148384
Action reg: 0.003989
  l1.weight: grad_norm = 0.101172
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.229847
Total gradient norm: 0.390356
=== Actor Training Debug (Iteration 5564) ===
Q mean: -69.246307
Q std: 24.030525
Actor loss: 69.250298
Action reg: 0.003993
  l1.weight: grad_norm = 0.102760
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.209400
Total gradient norm: 0.402800
=== Actor Training Debug (Iteration 5565) ===
Q mean: -69.863937
Q std: 24.186337
Actor loss: 69.867920
Action reg: 0.003981
  l1.weight: grad_norm = 0.033064
  l1.bias: grad_norm = 0.000871
  l2.weight: grad_norm = 0.080940
Total gradient norm: 0.190692
=== Actor Training Debug (Iteration 5566) ===
Q mean: -68.713318
Q std: 23.592079
Actor loss: 68.717308
Action reg: 0.003988
  l1.weight: grad_norm = 0.068275
  l1.bias: grad_norm = 0.000432
  l2.weight: grad_norm = 0.175382
Total gradient norm: 0.317879
=== Actor Training Debug (Iteration 5567) ===
Q mean: -69.060692
Q std: 23.250952
Actor loss: 69.064682
Action reg: 0.003989
  l1.weight: grad_norm = 0.199183
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.458217
Total gradient norm: 0.856749
=== Actor Training Debug (Iteration 5568) ===
Q mean: -70.496346
Q std: 25.320271
Actor loss: 70.500328
Action reg: 0.003983
  l1.weight: grad_norm = 0.052081
  l1.bias: grad_norm = 0.000944
  l2.weight: grad_norm = 0.122936
Total gradient norm: 0.219340
=== Actor Training Debug (Iteration 5569) ===
Q mean: -69.915268
Q std: 23.981409
Actor loss: 69.919243
Action reg: 0.003977
  l1.weight: grad_norm = 0.510605
  l1.bias: grad_norm = 0.000906
  l2.weight: grad_norm = 1.130958
Total gradient norm: 1.768252
=== Actor Training Debug (Iteration 5570) ===
Q mean: -68.003242
Q std: 26.125965
Actor loss: 68.007233
Action reg: 0.003989
  l1.weight: grad_norm = 0.030088
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.070993
Total gradient norm: 0.135733
=== Actor Training Debug (Iteration 5571) ===
Q mean: -69.591965
Q std: 24.112366
Actor loss: 69.595947
Action reg: 0.003985
  l1.weight: grad_norm = 0.045684
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.124664
Total gradient norm: 0.254759
=== Actor Training Debug (Iteration 5572) ===
Q mean: -72.369164
Q std: 23.607126
Actor loss: 72.373154
Action reg: 0.003987
  l1.weight: grad_norm = 0.251446
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.573715
Total gradient norm: 0.972816
=== Actor Training Debug (Iteration 5573) ===
Q mean: -70.041336
Q std: 23.279387
Actor loss: 70.045319
Action reg: 0.003984
  l1.weight: grad_norm = 0.125711
  l1.bias: grad_norm = 0.000780
  l2.weight: grad_norm = 0.321912
Total gradient norm: 0.701431
=== Actor Training Debug (Iteration 5574) ===
Q mean: -70.382187
Q std: 24.096209
Actor loss: 70.386177
Action reg: 0.003993
  l1.weight: grad_norm = 0.018429
  l1.bias: grad_norm = 0.000584
  l2.weight: grad_norm = 0.037148
Total gradient norm: 0.062808
=== Actor Training Debug (Iteration 5575) ===
Q mean: -70.853806
Q std: 22.674753
Actor loss: 70.857803
Action reg: 0.003996
  l1.weight: grad_norm = 0.034518
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.074944
Total gradient norm: 0.121859
=== Actor Training Debug (Iteration 5576) ===
Q mean: -69.191742
Q std: 23.336111
Actor loss: 69.195724
Action reg: 0.003986
  l1.weight: grad_norm = 0.099546
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.233637
Total gradient norm: 0.369912
=== Actor Training Debug (Iteration 5577) ===
Q mean: -70.991219
Q std: 23.710945
Actor loss: 70.995209
Action reg: 0.003990
  l1.weight: grad_norm = 0.097106
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.219635
Total gradient norm: 0.413424
=== Actor Training Debug (Iteration 5578) ===
Q mean: -67.359581
Q std: 25.278910
Actor loss: 67.363556
Action reg: 0.003979
  l1.weight: grad_norm = 0.450724
  l1.bias: grad_norm = 0.001275
  l2.weight: grad_norm = 1.036270
Total gradient norm: 1.644735
Total gradient norm: 0.5179214696on 1203) ===
=== Actor Training Debug (Iteration 5589) ===
Q mean: -69.167755
Q std: 24.797810
Actor loss: 69.171738
Action reg: 0.003979
  l1.weight: grad_norm = 0.186771
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.370988
Total gradient norm: 0.546818
=== Actor Training Debug (Iteration 5590) ===
Q mean: -68.581543
Q std: 25.453028
Actor loss: 68.585526
Action reg: 0.003980
  l1.weight: grad_norm = 0.072987
  l1.bias: grad_norm = 0.000811
  l2.weight: grad_norm = 0.158745
Total gradient norm: 0.272003
=== Actor Training Debug (Iteration 5591) ===
Q mean: -70.239532
Q std: 25.755503
Actor loss: 70.243500
Action reg: 0.003970
  l1.weight: grad_norm = 0.382357
  l1.bias: grad_norm = 0.001288
  l2.weight: grad_norm = 0.882869
Total gradient norm: 1.668962
=== Actor Training Debug (Iteration 5592) ===
Q mean: -67.288750
Q std: 23.979977
Actor loss: 67.292732
Action reg: 0.003986
  l1.weight: grad_norm = 0.147719
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.358676
Total gradient norm: 0.651350
=== Actor Training Debug (Iteration 5593) ===
Q mean: -69.144234
Q std: 22.165842
Actor loss: 69.148216
Action reg: 0.003985
  l1.weight: grad_norm = 0.083774
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.208957
Total gradient norm: 0.409279
=== Actor Training Debug (Iteration 5594) ===
Q mean: -71.817352
Q std: 24.107349
Actor loss: 71.821320
Action reg: 0.003967
  l1.weight: grad_norm = 1.166949
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 2.268683
Total gradient norm: 4.122035
=== Actor Training Debug (Iteration 5595) ===
Q mean: -70.880066
Q std: 23.351128
Actor loss: 70.884056
Action reg: 0.003993
  l1.weight: grad_norm = 0.078711
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.191114
Total gradient norm: 0.356475
=== Actor Training Debug (Iteration 5596) ===
Q mean: -69.737625
Q std: 24.690832
Actor loss: 69.741615
Action reg: 0.003992
  l1.weight: grad_norm = 0.142972
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.333801
Total gradient norm: 0.695831
=== Actor Training Debug (Iteration 5597) ===
Q mean: -68.803986
Q std: 25.920446
Actor loss: 68.807961
Action reg: 0.003978
  l1.weight: grad_norm = 0.354679
  l1.bias: grad_norm = 0.000693
  l2.weight: grad_norm = 0.867321
Total gradient norm: 1.361964
=== Actor Training Debug (Iteration 5598) ===
Q mean: -74.343124
Q std: 25.536900
Actor loss: 74.347099
Action reg: 0.003977
  l1.weight: grad_norm = 0.227312
  l1.bias: grad_norm = 0.000616
  l2.weight: grad_norm = 0.538582
Total gradient norm: 1.018060
=== Actor Training Debug (Iteration 5599) ===
Q mean: -67.995140
Q std: 24.582569
Actor loss: 67.999123
Action reg: 0.003980
  l1.weight: grad_norm = 0.289599
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.601239
Total gradient norm: 1.029757
=== Actor Training Debug (Iteration 5600) ===
Q mean: -68.516571
Q std: 23.459799
Actor loss: 68.520561
Action reg: 0.003991
  l1.weight: grad_norm = 0.129992
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.342829
Total gradient norm: 0.692995
=== Actor Training Debug (Iteration 5601) ===
Q mean: -69.646988
Q std: 22.902370
Actor loss: 69.650978
Action reg: 0.003989
  l1.weight: grad_norm = 0.261980
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.668890
Total gradient norm: 1.115716
=== Actor Training Debug (Iteration 5602) ===
Q mean: -70.047867
Q std: 24.390787
Actor loss: 70.051857
Action reg: 0.003990
  l1.weight: grad_norm = 0.038698
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.075663
Total gradient norm: 0.135385
=== Actor Training Debug (Iteration 5603) ===
Q mean: -69.694191
Q std: 27.041544
Actor loss: 69.698158
Action reg: 0.003971
  l1.weight: grad_norm = 0.038243
  l1.bias: grad_norm = 0.001212
  l2.weight: grad_norm = 0.087860
Total gradient norm: 0.160761
=== Actor Training Debug (Iteration 5604) ===
Q mean: -69.178848
Q std: 23.395739
Actor loss: 69.182831
Action reg: 0.003986
  l1.weight: grad_norm = 0.065774
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.154753
Total gradient norm: 0.333255
=== Actor Training Debug (Iteration 5605) ===
Q mean: -68.695541
Q std: 25.208488
Actor loss: 68.699524
Action reg: 0.003980
  l1.weight: grad_norm = 0.170964
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.431356
Total gradient norm: 0.914189
=== Actor Training Debug (Iteration 5606) ===
Q mean: -70.202438
Q std: 24.705833
Actor loss: 70.206421
Action reg: 0.003981
  l1.weight: grad_norm = 0.252750
  l1.bias: grad_norm = 0.000541
  l2.weight: grad_norm = 0.587776
Total gradient norm: 1.006380
=== Actor Training Debug (Iteration 5607) ===
Q mean: -72.452766
Q std: 24.151257
Actor loss: 72.456757
Action reg: 0.003994
  l1.weight: grad_norm = 0.047956
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.106257
Total gradient norm: 0.150135
=== Actor Training Debug (Iteration 5608) ===
Q mean: -68.100464
Q std: 25.928436
Actor loss: 68.104439
Action reg: 0.003972
  l1.weight: grad_norm = 0.240638
  l1.bias: grad_norm = 0.000871
  l2.weight: grad_norm = 0.645310
Total gradient norm: 1.211994
=== Actor Training Debug (Iteration 5609) ===
Q mean: -66.542252
Q std: 24.699066
Actor loss: 66.546234
Action reg: 0.003983
  l1.weight: grad_norm = 0.055782
  l1.bias: grad_norm = 0.000659
  l2.weight: grad_norm = 0.126973
Total gradient norm: 0.233157
=== Actor Training Debug (Iteration 5610) ===
Q mean: -70.223999
Q std: 23.383741
Actor loss: 70.227989
Action reg: 0.003989
  l1.weight: grad_norm = 0.012560
  l1.bias: grad_norm = 0.000582
  l2.weight: grad_norm = 0.028516
Total gradient norm: 0.046260
=== Actor Training Debug (Iteration 5611) ===
Q mean: -70.851982
Q std: 23.896368
Actor loss: 70.855965
Action reg: 0.003984
  l1.weight: grad_norm = 0.118250
  l1.bias: grad_norm = 0.000461
  l2.weight: grad_norm = 0.240221
Total gradient norm: 0.444418
=== Actor Training Debug (Iteration 5612) ===
Q mean: -69.354889
Q std: 23.179197
Actor loss: 69.358879
Action reg: 0.003991
  l1.weight: grad_norm = 0.009374
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.020758
Total gradient norm: 0.038426
=== Actor Training Debug (Iteration 5613) ===
Q mean: -69.352097
Q std: 23.044371
Actor loss: 69.356087
Action reg: 0.003994
  l1.weight: grad_norm = 0.219769
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.503117
Total gradient norm: 0.969581
=== Actor Training Debug (Iteration 5614) ===
Q mean: -71.326065
Q std: 23.189554
Actor loss: 71.330048
Action reg: 0.003983
  l1.weight: grad_norm = 0.155641
  l1.bias: grad_norm = 0.000535
  l2.weight: grad_norm = 0.346218
Total gradient norm: 0.641084
=== Actor Training Debug (Iteration 5615) ===
Q mean: -74.641533
Q std: 25.654015
Actor loss: 74.645523
Action reg: 0.003989
  l1.weight: grad_norm = 0.228180
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.556498
Total gradient norm: 1.049365
=== Actor Training Debug (Iteration 5616) ===
Q mean: -67.204987
Q std: 25.559834
Actor loss: 67.208961
Action reg: 0.003974
  l1.weight: grad_norm = 0.214376
  l1.bias: grad_norm = 0.001210
  l2.weight: grad_norm = 0.460407
Total gradient norm: 0.723021
=== Actor Training Debug (Iteration 5617) ===
Q mean: -68.094673
Q std: 25.471502
Actor loss: 68.098648
Action reg: 0.003977
  l1.weight: grad_norm = 0.147653
  l1.bias: grad_norm = 0.000597
  l2.weight: grad_norm = 0.341554
Total gradient norm: 0.707360
=== Actor Training Debug (Iteration 5618) ===
Q mean: -69.877411
Q std: 23.600990
Actor loss: 69.881401
Action reg: 0.003987
  l1.weight: grad_norm = 0.463119
  l1.bias: grad_norm = 0.000796
  l2.weight: grad_norm = 0.911674
Total gradient norm: 1.757677
=== Actor Training Debug (Iteration 5619) ===
Q mean: -71.555771
Q std: 23.175009
Actor loss: 71.559769
Action reg: 0.003995
  l1.weight: grad_norm = 0.040807
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.091109
Total gradient norm: 0.165323
=== Actor Training Debug (Iteration 5620) ===
Q mean: -69.873619
Q std: 22.931562
Actor loss: 69.877586
Action reg: 0.003964
  l1.weight: grad_norm = 0.053820
  l1.bias: grad_norm = 0.000641
  l2.weight: grad_norm = 0.114136
Total gradient norm: 0.204177
=== Actor Training Debug (Iteration 5621) ===
Q mean: -70.047577
Q std: 23.957443
Actor loss: 70.051559
Action reg: 0.003986
  l1.weight: grad_norm = 0.266087
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.564347
Total gradient norm: 1.016520
=== Actor Training Debug (Iteration 5622) ===
Q mean: -71.872681
Q std: 23.519537
Actor loss: 71.876671
Action reg: 0.003989
  l1.weight: grad_norm = 0.095281
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.231182
Total gradient norm: 0.375244
=== Actor Training Debug (Iteration 5623) ===
Q mean: -71.660263
Q std: 24.697086
Actor loss: 71.664253
Action reg: 0.003989
  l1.weight: grad_norm = 0.072345
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.145354
Total gradient norm: 0.254810
=== Actor Training Debug (Iteration 5624) ===
Q mean: -67.499435
Q std: 24.545570
Actor loss: 67.503433
Action reg: 0.003995
  l1.weight: grad_norm = 0.109848
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.228487
Total gradient norm: 0.430477
=== Actor Training Debug (Iteration 5625) ===
Q mean: -71.371475
Q std: 23.826817
Actor loss: 71.375465
Action reg: 0.003987
  l1.weight: grad_norm = 0.090610
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.250734
Total gradient norm: 0.574986
=== Actor Training Debug (Iteration 5626) ===
Q mean: -69.191528
Q std: 24.302013
Actor loss: 69.195511
Action reg: 0.003983
  l1.weight: grad_norm = 0.067417
  l1.bias: grad_norm = 0.000590
  l2.weight: grad_norm = 0.174256
Total gradient norm: 0.333841
=== Actor Training Debug (Iteration 5627) ===
Q mean: -71.552887
Q std: 24.352575
Actor loss: 71.556862
Action reg: 0.003977
  l1.weight: grad_norm = 0.143832
  l1.bias: grad_norm = 0.001231
  l2.weight: grad_norm = 0.366263
Total gradient norm: 0.672347
=== Actor Training Debug (Iteration 5628) ===
Q mean: -67.240593
Q std: 24.810846
Actor loss: 67.244576
Action reg: 0.003980
  l1.weight: grad_norm = 0.419454
  l1.bias: grad_norm = 0.000807
  l2.weight: grad_norm = 0.852500
Total gradient norm: 1.651242
=== Actor Training Debug (Iteration 5629) ===
Q mean: -71.358490
Q std: 24.031221
Actor loss: 71.362434
Action reg: 0.003946
  l1.weight: grad_norm = 0.147831
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.331490
Total gradient norm: 0.637824
=== Actor Training Debug (Iteration 5630) ===
Q mean: -69.614861
Q std: 23.715681
Actor loss: 69.618805
Action reg: 0.003946
  l1.weight: grad_norm = 0.125094
  l1.bias: grad_norm = 0.000716
  l2.weight: grad_norm = 0.240870
Total gradient norm: 0.395323
=== Actor Training Debug (Iteration 5631) ===
Q mean: -73.942398
Q std: 23.975382
Actor loss: 73.946388
Action reg: 0.003992
  l1.weight: grad_norm = 0.150282
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.350635
Total gradient norm: 0.589785
=== Actor Training Debug (Iteration 5632) ===
Q mean: -67.379150
Q std: 24.424133
Actor loss: 67.383133
Action reg: 0.003985
  l1.weight: grad_norm = 0.032058
  l1.bias: grad_norm = 0.000703
  l2.weight: grad_norm = 0.075643
Total gradient norm: 0.146404
=== Actor Training Debug (Iteration 5633) ===
Q mean: -68.614021
Q std: 24.167585
Actor loss: 68.618019
Action reg: 0.003996
  l1.weight: grad_norm = 0.025254
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.055286
Total gradient norm: 0.110042
=== Actor Training Debug (Iteration 5634) ===
Q mean: -70.763863
Q std: 23.522776
Actor loss: 70.767845
Action reg: 0.003983
  l1.weight: grad_norm = 0.173310
  l1.bias: grad_norm = 0.000909
  l2.weight: grad_norm = 0.382862
Total gradient norm: 0.708613
=== Actor Training Debug (Iteration 5635) ===
Q mean: -68.534531
Q std: 23.684196
Actor loss: 68.538521
Action reg: 0.003987
  l1.weight: grad_norm = 0.057358
  l1.bias: grad_norm = 0.000619
  l2.weight: grad_norm = 0.147557
Total gradient norm: 0.280131
=== Actor Training Debug (Iteration 5636) ===
Q mean: -71.380920
Q std: 23.857620
Actor loss: 71.384918
Action reg: 0.003995
  l1.weight: grad_norm = 0.032183
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.076547
Total gradient norm: 0.154909
=== Actor Training Debug (Iteration 5637) ===
Q mean: -70.008232
Q std: 25.065805
Actor loss: 70.012215
Action reg: 0.003980
  l1.weight: grad_norm = 0.208604
  l1.bias: grad_norm = 0.001008
  l2.weight: grad_norm = 0.490450
Total gradient norm: 0.993916
=== Actor Training Debug (Iteration 5638) ===
Q mean: -71.385971
Q std: 24.108009
Actor loss: 71.389954
Action reg: 0.003985
  l1.weight: grad_norm = 0.507996
  l1.bias: grad_norm = 0.000710
  l2.weight: grad_norm = 1.113052
Total gradient norm: 1.752367
=== Actor Training Debug (Iteration 5639) ===
Q mean: -69.366745
Q std: 23.591875
Actor loss: 69.370728
Action reg: 0.003986
  l1.weight: grad_norm = 0.139569
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.310866
Total gradient norm: 0.515600
=== Actor Training Debug (Iteration 5640) ===
Q mean: -70.213852
Q std: 24.299835
Actor loss: 70.217834
Action reg: 0.003979
  l1.weight: grad_norm = 0.188788
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.422394
Total gradient norm: 0.728092
=== Actor Training Debug (Iteration 5641) ===
Q mean: -70.550003
Q std: 23.816868
Actor loss: 70.553963
Action reg: 0.003961
  l1.weight: grad_norm = 0.257295
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.605811
Total gradient norm: 1.191120
=== Actor Training Debug (Iteration 5642) ===
Q mean: -67.592422
Q std: 25.160542
Actor loss: 67.596405
Action reg: 0.003981
  l1.weight: grad_norm = 0.095182
  l1.bias: grad_norm = 0.000665
  l2.weight: grad_norm = 0.190889
Total gradient norm: 0.283145
=== Actor Training Debug (Iteration 5643) ===
Q mean: -71.841339
Q std: 23.344938
Actor loss: 71.845337
Action reg: 0.003996
  l1.weight: grad_norm = 0.065979
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.184350
Total gradient norm: 0.366889
=== Actor Training Debug (Iteration 5644) ===
Q mean: -67.201797
Q std: 24.328310
Actor loss: 67.205765
Action reg: 0.003965
  l1.weight: grad_norm = 0.242235
  l1.bias: grad_norm = 0.000880
  l2.weight: grad_norm = 0.595882
Total gradient norm: 1.038742
=== Actor Training Debug (Iteration 5645) ===
Q mean: -71.072495
Q std: 23.663549
Actor loss: 71.076485
Action reg: 0.003988
  l1.weight: grad_norm = 0.150562
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.346030
Total gradient norm: 0.520261
=== Actor Training Debug (Iteration 5646) ===
Q mean: -70.561584
Q std: 23.153131
Actor loss: 70.565575
Action reg: 0.003993
  l1.weight: grad_norm = 0.072805
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.168779
Total gradient norm: 0.341515
=== Actor Training Debug (Iteration 5647) ===
Q mean: -71.774078
Q std: 24.587599
Actor loss: 71.778069
Action reg: 0.003990
  l1.weight: grad_norm = 0.208319
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.579431
Total gradient norm: 1.307450
=== Actor Training Debug (Iteration 5648) ===
Q mean: -70.700203
Q std: 22.989998
Actor loss: 70.704193
Action reg: 0.003987
  l1.weight: grad_norm = 0.264489
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.601302
Total gradient norm: 1.057390
=== Actor Training Debug (Iteration 5649) ===
Q mean: -70.418152
Q std: 23.459604
Actor loss: 70.422134
Action reg: 0.003985
  l1.weight: grad_norm = 0.064750
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.169553
Total gradient norm: 0.347467
=== Actor Training Debug (Iteration 5650) ===
Q mean: -71.892563
Q std: 21.551058
Actor loss: 71.896561
Action reg: 0.003997
  l1.weight: grad_norm = 0.133261
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.283128
Total gradient norm: 0.627481
=== Actor Training Debug (Iteration 5651) ===
Q mean: -67.678932
Q std: 24.635084
Actor loss: 67.682861
Action reg: 0.003927
  l1.weight: grad_norm = 0.210435
  l1.bias: grad_norm = 0.000889
  l2.weight: grad_norm = 0.416502
Total gradient norm: 0.690330
=== Actor Training Debug (Iteration 5652) ===
Q mean: -70.303909
Q std: 25.982735
Actor loss: 70.307892
Action reg: 0.003985
  l1.weight: grad_norm = 0.236766
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.539248
Total gradient norm: 1.076022
=== Actor Training Debug (Iteration 5653) ===
Q mean: -71.519928
Q std: 23.076208
Actor loss: 71.523918
Action reg: 0.003991
  l1.weight: grad_norm = 0.015004
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.036620
Total gradient norm: 0.086740
=== Actor Training Debug (Iteration 5654) ===
Q mean: -69.291306
Q std: 25.206814
Actor loss: 69.295288
Action reg: 0.003980
  l1.weight: grad_norm = 0.128801
  l1.bias: grad_norm = 0.000920
  l2.weight: grad_norm = 0.337974
Total gradient norm: 0.727203
=== Actor Training Debug (Iteration 5655) ===
Q mean: -72.917030
Q std: 23.005539
Actor loss: 72.921028
Action reg: 0.003995
  l1.weight: grad_norm = 0.102284
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.196925
Total gradient norm: 0.376501
=== Actor Training Debug (Iteration 5656) ===
Q mean: -68.506424
Q std: 24.544538
Actor loss: 68.510406
Action reg: 0.003983
  l1.weight: grad_norm = 0.570470
  l1.bias: grad_norm = 0.000668
  l2.weight: grad_norm = 1.186306
Total gradient norm: 2.032712
=== Actor Training Debug (Iteration 5657) ===
Q mean: -71.156662
Q std: 24.560844
Actor loss: 71.160652
Action reg: 0.003992
  l1.weight: grad_norm = 0.154036
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.347925
Total gradient norm: 0.699815
=== Actor Training Debug (Iteration 5658) ===
Q mean: -68.652878
Q std: 24.773005
Actor loss: 68.656868
Action reg: 0.003987
  l1.weight: grad_norm = 0.322656
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.657700
Total gradient norm: 1.052814
=== Actor Training Debug (Iteration 5659) ===
Q mean: -69.475204
Q std: 23.867081
Actor loss: 69.479202
Action reg: 0.003995
  l1.weight: grad_norm = 0.030417
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.070809
Total gradient norm: 0.155329
=== Actor Training Debug (Iteration 5660) ===
Q mean: -67.754768
Q std: 24.968220
Actor loss: 67.758751
Action reg: 0.003981
  l1.weight: grad_norm = 0.298087
  l1.bias: grad_norm = 0.000783
  l2.weight: grad_norm = 0.740625
Total gradient norm: 1.541031
=== Actor Training Debug (Iteration 5661) ===
Q mean: -71.117508
Q std: 24.259756
Actor loss: 71.121498
Action reg: 0.003992
  l1.weight: grad_norm = 0.280765
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.535969
Total gradient norm: 0.928745
=== Actor Training Debug (Iteration 5662) ===
Q mean: -72.732117
Q std: 25.084583
Actor loss: 72.736107
Action reg: 0.003989
  l1.weight: grad_norm = 0.110393
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.225594
Total gradient norm: 0.438067
=== Actor Training Debug (Iteration 5663) ===
Q mean: -69.346375
Q std: 25.526493
Actor loss: 69.350365
Action reg: 0.003988
  l1.weight: grad_norm = 0.130121
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.313214
Total gradient norm: 0.577700
=== Actor Training Debug (Iteration 5664) ===
Q mean: -68.735443
Q std: 25.319557
Actor loss: 68.739380
Action reg: 0.003939
  l1.weight: grad_norm = 0.248065
  l1.bias: grad_norm = 0.000829
  l2.weight: grad_norm = 0.561659
Total gradient norm: 1.085370
=== Actor Training Debug (Iteration 5665) ===
Q mean: -71.215645
Q std: 25.265791
Actor loss: 71.219627
Action reg: 0.003980
  l1.weight: grad_norm = 0.275269
  l1.bias: grad_norm = 0.000565
  l2.weight: grad_norm = 0.609044
Total gradient norm: 1.059546
=== Actor Training Debug (Iteration 5666) ===
Q mean: -70.779495
Q std: 25.788204
Actor loss: 70.783485
Action reg: 0.003990
  l1.weight: grad_norm = 0.021064
  l1.bias: grad_norm = 0.000560
  l2.weight: grad_norm = 0.048388
Total gradient norm: 0.083489
=== Actor Training Debug (Iteration 5667) ===
Q mean: -69.476074
Q std: 23.853949
Actor loss: 69.480064
Action reg: 0.003989
  l1.weight: grad_norm = 0.176608
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.431941
Total gradient norm: 0.730082
=== Actor Training Debug (Iteration 5668) ===
Q mean: -69.118164
Q std: 25.104479
Actor loss: 69.122139
Action reg: 0.003979
  l1.weight: grad_norm = 0.039806
  l1.bias: grad_norm = 0.001262
  l2.weight: grad_norm = 0.093988
Total gradient norm: 0.157544
=== Actor Training Debug (Iteration 5669) ===
Q mean: -68.986084
Q std: 26.434174
Actor loss: 68.990051
Action reg: 0.003965
  l1.weight: grad_norm = 0.468922
  l1.bias: grad_norm = 0.001289
  l2.weight: grad_norm = 1.129834
Total gradient norm: 1.969019
=== Actor Training Debug (Iteration 5670) ===
Q mean: -70.876831
Q std: 23.285261
Actor loss: 70.880821
Action reg: 0.003990
  l1.weight: grad_norm = 0.185380
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.431097
Total gradient norm: 0.782914
=== Actor Training Debug (Iteration 5671) ===
Q mean: -70.726898
Q std: 24.755774
Actor loss: 70.730888
Action reg: 0.003987
  l1.weight: grad_norm = 0.132121
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.314078
Total gradient norm: 0.527969
=== Actor Training Debug (Iteration 5672) ===
Q mean: -71.286652
Q std: 23.970396
Actor loss: 71.290649
Action reg: 0.003996
  l1.weight: grad_norm = 0.037956
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.072152
Total gradient norm: 0.142269
=== Actor Training Debug (Iteration 5673) ===
Q mean: -71.294121
Q std: 24.107946
Actor loss: 71.298103
Action reg: 0.003985
  l1.weight: grad_norm = 0.205809
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.443346
Total gradient norm: 0.794322
=== Actor Training Debug (Iteration 5674) ===
Q mean: -72.221176
Q std: 24.979713
Actor loss: 72.225159
Action reg: 0.003983
  l1.weight: grad_norm = 0.107101
  l1.bias: grad_norm = 0.000801
  l2.weight: grad_norm = 0.252481
Total gradient norm: 0.458749
=== Actor Training Debug (Iteration 5675) ===
Q mean: -69.815674
Q std: 24.618670
Actor loss: 69.819664
Action reg: 0.003992
  l1.weight: grad_norm = 0.019059
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.044982
Total gradient norm: 0.078659
=== Actor Training Debug (Iteration 5676) ===
Q mean: -70.341187
Q std: 25.956631
Actor loss: 70.345169
Action reg: 0.003982
  l1.weight: grad_norm = 0.184191
  l1.bias: grad_norm = 0.000937
  l2.weight: grad_norm = 0.402511
Total gradient norm: 0.666868
=== Actor Training Debug (Iteration 5677) ===
Q mean: -70.303429
Q std: 24.948166
Actor loss: 70.307426
Action reg: 0.003995
  l1.weight: grad_norm = 0.102698
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.213628
Total gradient norm: 0.392371
=== Actor Training Debug (Iteration 5678) ===
Q mean: -73.701797
Q std: 25.370159
Actor loss: 73.705780
Action reg: 0.003985
  l1.weight: grad_norm = 0.474039
  l1.bias: grad_norm = 0.000656
  l2.weight: grad_norm = 1.066686
Total gradient norm: 1.791880
=== Actor Training Debug (Iteration 5679) ===
Q mean: -70.967392
Q std: 24.063282
Actor loss: 70.971382
Action reg: 0.003991
  l1.weight: grad_norm = 0.056647
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.123186
Total gradient norm: 0.208754
=== Actor Training Debug (Iteration 5680) ===
Q mean: -70.858681
Q std: 24.828213
Actor loss: 70.862671
Action reg: 0.003991
  l1.weight: grad_norm = 0.095981
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.190707
Total gradient norm: 0.345101
=== Actor Training Debug (Iteration 5681) ===
Q mean: -68.673317
Q std: 24.274708
Actor loss: 68.677307
Action reg: 0.003993
  l1.weight: grad_norm = 0.169669
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.356229
Total gradient norm: 0.678734
=== Actor Training Debug (Iteration 5682) ===
Q mean: -66.974915
Q std: 24.054741
Actor loss: 66.978897
Action reg: 0.003984
  l1.weight: grad_norm = 0.202654
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.484792
Total gradient norm: 0.890606
=== Actor Training Debug (Iteration 5683) ===
Q mean: -68.614166
Q std: 24.887802
Actor loss: 68.618156
Action reg: 0.003987
  l1.weight: grad_norm = 0.020464
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.046427
Total gradient norm: 0.076509
=== Actor Training Debug (Iteration 5684) ===
Q mean: -71.494293
Q std: 23.474716
Actor loss: 71.498283
Action reg: 0.003993
  l1.weight: grad_norm = 0.152063
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.313463
Total gradient norm: 0.617239
=== Actor Training Debug (Iteration 5685) ===
Q mean: -69.472473
Q std: 24.445896
Actor loss: 69.476448
Action reg: 0.003978
  l1.weight: grad_norm = 0.244468
  l1.bias: grad_norm = 0.000730
  l2.weight: grad_norm = 0.550785
Total gradient norm: 1.037487
=== Actor Training Debug (Iteration 5686) ===
Q mean: -66.613640
Q std: 25.594160
Actor loss: 66.617622
Action reg: 0.003986
  l1.weight: grad_norm = 0.198064
  l1.bias: grad_norm = 0.000557
  l2.weight: grad_norm = 0.484442
Total gradient norm: 1.030989
=== Actor Training Debug (Iteration 5687) ===
Q mean: -69.262192
Q std: 24.373770
Actor loss: 69.266144
Action reg: 0.003956
  l1.weight: grad_norm = 0.608367
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 1.496759
Total gradient norm: 2.444257
=== Actor Training Debug (Iteration 5688) ===
Q mean: -68.741486
Q std: 24.691662
Actor loss: 68.745476
Action reg: 0.003994
  l1.weight: grad_norm = 0.008329
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.023380
Total gradient norm: 0.051452
=== Actor Training Debug (Iteration 5689) ===
Q mean: -72.011543
Q std: 23.175629
Actor loss: 72.015533
Action reg: 0.003989
  l1.weight: grad_norm = 0.224195
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.452200
Total gradient norm: 0.909645
=== Actor Training Debug (Iteration 5690) ===
Q mean: -70.986893
Q std: 23.313612
Actor loss: 70.990891
Action reg: 0.003995
  l1.weight: grad_norm = 0.024255
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.055830
Total gradient norm: 0.101979
=== Actor Training Debug (Iteration 5691) ===
Q mean: -69.822647
Q std: 24.639376
Actor loss: 69.826637
Action reg: 0.003993
  l1.weight: grad_norm = 0.052058
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.121182
Total gradient norm: 0.202104
=== Actor Training Debug (Iteration 5692) ===
Q mean: -68.196228
Q std: 25.778849
Actor loss: 68.200211
Action reg: 0.003982
  l1.weight: grad_norm = 0.345245
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.870202
Total gradient norm: 1.702541
=== Actor Training Debug (Iteration 5693) ===
Q mean: -71.502701
Q std: 24.830675
Actor loss: 71.506699
Action reg: 0.003997
  l1.weight: grad_norm = 0.197519
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.415341
Total gradient norm: 0.675972
=== Actor Training Debug (Iteration 5694) ===
Q mean: -68.695297
Q std: 24.152943
Actor loss: 68.699287
Action reg: 0.003989
  l1.weight: grad_norm = 0.062689
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.165599
Total gradient norm: 0.272043
=== Actor Training Debug (Iteration 5695) ===
Q mean: -68.968201
Q std: 24.659647
Actor loss: 68.972183
Action reg: 0.003984
  l1.weight: grad_norm = 0.651227
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 1.440022
Total gradient norm: 2.282185
=== Actor Training Debug (Iteration 5696) ===
Q mean: -73.376526
Q std: 22.921741
Actor loss: 73.380524
Action reg: 0.004000
  l1.weight: grad_norm = 0.038645
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.094286
Total gradient norm: 0.153549
=== Actor Training Debug (Iteration 5697) ===
Q mean: -69.836426
Q std: 23.677469
Actor loss: 69.840424
Action reg: 0.003996
Action reg: 0.003983 0.5179214696on 1203) ===
  l1.weight: grad_norm = 0.422228
  l1.bias: grad_norm = 0.000808
  l2.weight: grad_norm = 0.904877
Total gradient norm: 1.692269
=== Actor Training Debug (Iteration 5708) ===
Q mean: -70.919357
Q std: 24.230347
Actor loss: 70.923340
Action reg: 0.003986
  l1.weight: grad_norm = 0.100628
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.183075
Total gradient norm: 0.374435
=== Actor Training Debug (Iteration 5709) ===
Q mean: -69.118279
Q std: 24.734312
Actor loss: 69.122261
Action reg: 0.003985
  l1.weight: grad_norm = 0.289133
  l1.bias: grad_norm = 0.000642
  l2.weight: grad_norm = 0.598424
Total gradient norm: 1.006005
=== Actor Training Debug (Iteration 5710) ===
Q mean: -73.065231
Q std: 24.650621
Actor loss: 73.069221
Action reg: 0.003989
  l1.weight: grad_norm = 0.312495
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.756844
Total gradient norm: 1.454354
=== Actor Training Debug (Iteration 5711) ===
Q mean: -67.237762
Q std: 24.219189
Actor loss: 67.241753
Action reg: 0.003994
  l1.weight: grad_norm = 0.057937
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.120092
Total gradient norm: 0.223703
=== Actor Training Debug (Iteration 5712) ===
Q mean: -70.275818
Q std: 23.591904
Actor loss: 70.279816
Action reg: 0.003997
  l1.weight: grad_norm = 0.006651
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.018499
Total gradient norm: 0.039632
=== Actor Training Debug (Iteration 5713) ===
Q mean: -72.936508
Q std: 24.560776
Actor loss: 72.940491
Action reg: 0.003983
  l1.weight: grad_norm = 0.252405
  l1.bias: grad_norm = 0.000903
  l2.weight: grad_norm = 0.519144
Total gradient norm: 0.921923
=== Actor Training Debug (Iteration 5714) ===
Q mean: -72.284164
Q std: 24.225084
Actor loss: 72.288147
Action reg: 0.003985
  l1.weight: grad_norm = 0.253397
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.533431
Total gradient norm: 0.923507
=== Actor Training Debug (Iteration 5715) ===
Q mean: -70.924919
Q std: 24.094185
Actor loss: 70.928909
Action reg: 0.003992
  l1.weight: grad_norm = 0.125083
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.294873
Total gradient norm: 0.615725
=== Actor Training Debug (Iteration 5716) ===
Q mean: -70.252045
Q std: 25.327852
Actor loss: 70.256035
Action reg: 0.003990
  l1.weight: grad_norm = 0.214319
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.532015
Total gradient norm: 1.115937
=== Actor Training Debug (Iteration 5717) ===
Q mean: -71.227798
Q std: 23.704720
Actor loss: 71.231789
Action reg: 0.003989
  l1.weight: grad_norm = 0.202586
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.433050
Total gradient norm: 0.740069
=== Actor Training Debug (Iteration 5718) ===
Q mean: -69.592377
Q std: 25.378605
Actor loss: 69.596352
Action reg: 0.003979
  l1.weight: grad_norm = 0.102050
  l1.bias: grad_norm = 0.001012
  l2.weight: grad_norm = 0.203481
Total gradient norm: 0.341113
=== Actor Training Debug (Iteration 5719) ===
Q mean: -71.625793
Q std: 24.898977
Actor loss: 71.629784
Action reg: 0.003993
  l1.weight: grad_norm = 0.012052
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.025591
Total gradient norm: 0.047086
=== Actor Training Debug (Iteration 5720) ===
Q mean: -72.688362
Q std: 25.505657
Actor loss: 72.692345
Action reg: 0.003986
  l1.weight: grad_norm = 0.099220
  l1.bias: grad_norm = 0.000760
  l2.weight: grad_norm = 0.195243
Total gradient norm: 0.350316
=== Actor Training Debug (Iteration 5721) ===
Q mean: -69.462898
Q std: 24.391766
Actor loss: 69.466888
Action reg: 0.003991
  l1.weight: grad_norm = 0.030154
  l1.bias: grad_norm = 0.000498
  l2.weight: grad_norm = 0.067682
Total gradient norm: 0.150191
=== Actor Training Debug (Iteration 5722) ===
Q mean: -66.952805
Q std: 25.141445
Actor loss: 66.956787
Action reg: 0.003985
  l1.weight: grad_norm = 0.067873
  l1.bias: grad_norm = 0.001015
  l2.weight: grad_norm = 0.161690
Total gradient norm: 0.340785
=== Actor Training Debug (Iteration 5723) ===
Q mean: -73.190643
Q std: 24.251579
Actor loss: 73.194626
Action reg: 0.003984
  l1.weight: grad_norm = 0.571576
  l1.bias: grad_norm = 0.000703
  l2.weight: grad_norm = 1.349684
Total gradient norm: 2.508117
=== Actor Training Debug (Iteration 5724) ===
Q mean: -70.093895
Q std: 24.848957
Actor loss: 70.097885
Action reg: 0.003990
  l1.weight: grad_norm = 0.180621
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.409269
Total gradient norm: 0.926779
=== Actor Training Debug (Iteration 5725) ===
Q mean: -68.935959
Q std: 24.793734
Actor loss: 68.939941
Action reg: 0.003980
  l1.weight: grad_norm = 0.175535
  l1.bias: grad_norm = 0.000646
  l2.weight: grad_norm = 0.370267
Total gradient norm: 0.697733
=== Actor Training Debug (Iteration 5726) ===
Q mean: -71.062851
Q std: 25.152845
Actor loss: 71.066841
Action reg: 0.003987
  l1.weight: grad_norm = 0.015669
  l1.bias: grad_norm = 0.000666
  l2.weight: grad_norm = 0.029221
Total gradient norm: 0.047195
=== Actor Training Debug (Iteration 5727) ===
Q mean: -71.815216
Q std: 25.917246
Actor loss: 71.819206
Action reg: 0.003990
  l1.weight: grad_norm = 0.103717
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.213970
Total gradient norm: 0.444543
=== Actor Training Debug (Iteration 5728) ===
Q mean: -69.125198
Q std: 23.203598
Actor loss: 69.129189
Action reg: 0.003987
  l1.weight: grad_norm = 0.175445
  l1.bias: grad_norm = 0.000600
  l2.weight: grad_norm = 0.416227
Total gradient norm: 0.727850
=== Actor Training Debug (Iteration 5729) ===
Q mean: -66.279427
Q std: 23.824341
Actor loss: 66.283417
Action reg: 0.003991
  l1.weight: grad_norm = 0.146624
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.288613
Total gradient norm: 0.533046
=== Actor Training Debug (Iteration 5730) ===
Q mean: -72.614334
Q std: 23.520164
Actor loss: 72.618324
Action reg: 0.003991
  l1.weight: grad_norm = 0.131678
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.283666
Total gradient norm: 0.481424
=== Actor Training Debug (Iteration 5731) ===
Q mean: -70.719864
Q std: 24.122616
Actor loss: 70.723846
Action reg: 0.003981
  l1.weight: grad_norm = 0.144238
  l1.bias: grad_norm = 0.001142
  l2.weight: grad_norm = 0.293672
Total gradient norm: 0.489884
=== Actor Training Debug (Iteration 5732) ===
Q mean: -69.099792
Q std: 23.129395
Actor loss: 69.103775
Action reg: 0.003985
  l1.weight: grad_norm = 0.144357
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.313062
Total gradient norm: 0.640792
=== Actor Training Debug (Iteration 5733) ===
Q mean: -69.701408
Q std: 25.197193
Actor loss: 69.705391
Action reg: 0.003981
  l1.weight: grad_norm = 0.065593
  l1.bias: grad_norm = 0.001247
  l2.weight: grad_norm = 0.172407
Total gradient norm: 0.328244
=== Actor Training Debug (Iteration 5734) ===
Q mean: -68.583794
Q std: 23.832769
Actor loss: 68.587784
Action reg: 0.003993
  l1.weight: grad_norm = 0.090834
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.195067
Total gradient norm: 0.349734
=== Actor Training Debug (Iteration 5735) ===
Q mean: -73.312958
Q std: 25.613781
Actor loss: 73.316940
Action reg: 0.003982
  l1.weight: grad_norm = 0.279040
  l1.bias: grad_norm = 0.000798
  l2.weight: grad_norm = 0.648599
Total gradient norm: 1.369586
=== Actor Training Debug (Iteration 5736) ===
Q mean: -69.971634
Q std: 25.757427
Actor loss: 69.975616
Action reg: 0.003979
  l1.weight: grad_norm = 0.544238
  l1.bias: grad_norm = 0.001653
  l2.weight: grad_norm = 1.295356
Total gradient norm: 2.482616
=== Actor Training Debug (Iteration 5737) ===
Q mean: -68.997574
Q std: 24.499382
Actor loss: 69.001564
Action reg: 0.003992
  l1.weight: grad_norm = 0.119056
  l1.bias: grad_norm = 0.000618
  l2.weight: grad_norm = 0.252782
Total gradient norm: 0.471787
=== Actor Training Debug (Iteration 5738) ===
Q mean: -71.643387
Q std: 25.504034
Actor loss: 71.647362
Action reg: 0.003977
  l1.weight: grad_norm = 0.117880
  l1.bias: grad_norm = 0.001055
  l2.weight: grad_norm = 0.261114
Total gradient norm: 0.469165
=== Actor Training Debug (Iteration 5739) ===
Q mean: -68.135330
Q std: 27.013044
Actor loss: 68.139313
Action reg: 0.003986
  l1.weight: grad_norm = 0.237641
  l1.bias: grad_norm = 0.000884
  l2.weight: grad_norm = 0.480673
Total gradient norm: 0.820177
=== Actor Training Debug (Iteration 5740) ===
Q mean: -68.043785
Q std: 24.741692
Actor loss: 68.047768
Action reg: 0.003981
  l1.weight: grad_norm = 0.194019
  l1.bias: grad_norm = 0.000783
  l2.weight: grad_norm = 0.436826
Total gradient norm: 0.773777
=== Actor Training Debug (Iteration 5741) ===
Q mean: -68.365181
Q std: 24.375034
Actor loss: 68.369148
Action reg: 0.003967
  l1.weight: grad_norm = 0.108965
  l1.bias: grad_norm = 0.001469
  l2.weight: grad_norm = 0.276616
Total gradient norm: 0.605211
=== Actor Training Debug (Iteration 5742) ===
Q mean: -70.917099
Q std: 23.780827
Actor loss: 70.921089
Action reg: 0.003991
  l1.weight: grad_norm = 0.170994
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.323618
Total gradient norm: 0.649915
=== Actor Training Debug (Iteration 5743) ===
Q mean: -69.259018
Q std: 25.203104
Actor loss: 69.262978
Action reg: 0.003956
  l1.weight: grad_norm = 0.135570
  l1.bias: grad_norm = 0.001299
  l2.weight: grad_norm = 0.350532
Total gradient norm: 0.695085
=== Actor Training Debug (Iteration 5744) ===
Q mean: -70.849106
Q std: 25.048973
Actor loss: 70.853096
Action reg: 0.003990
  l1.weight: grad_norm = 0.056095
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.105368
Total gradient norm: 0.177247
=== Actor Training Debug (Iteration 5745) ===
Q mean: -70.143936
Q std: 26.923311
Actor loss: 70.147919
Action reg: 0.003983
  l1.weight: grad_norm = 0.116362
  l1.bias: grad_norm = 0.000831
  l2.weight: grad_norm = 0.291889
Total gradient norm: 0.595579
=== Actor Training Debug (Iteration 5746) ===
Q mean: -67.903999
Q std: 25.733059
Actor loss: 67.907982
Action reg: 0.003982
  l1.weight: grad_norm = 0.192919
  l1.bias: grad_norm = 0.001048
  l2.weight: grad_norm = 0.364112
Total gradient norm: 0.596912
=== Actor Training Debug (Iteration 5747) ===
Q mean: -74.640991
Q std: 23.605850
Actor loss: 74.644989
Action reg: 0.003994
  l1.weight: grad_norm = 0.492083
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.972510
Total gradient norm: 1.813070
=== Actor Training Debug (Iteration 5748) ===
Q mean: -73.677460
Q std: 24.661581
Actor loss: 73.681442
Action reg: 0.003986
  l1.weight: grad_norm = 0.051817
  l1.bias: grad_norm = 0.000821
  l2.weight: grad_norm = 0.113984
Total gradient norm: 0.159820
=== Actor Training Debug (Iteration 5749) ===
Q mean: -71.983383
Q std: 23.773989
Actor loss: 71.987373
Action reg: 0.003987
  l1.weight: grad_norm = 0.014163
  l1.bias: grad_norm = 0.000590
  l2.weight: grad_norm = 0.031978
Total gradient norm: 0.066440
=== Actor Training Debug (Iteration 5750) ===
Q mean: -71.892807
Q std: 24.799412
Actor loss: 71.896805
Action reg: 0.003995
  l1.weight: grad_norm = 0.135387
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.301994
Total gradient norm: 0.554046
=== Actor Training Debug (Iteration 5751) ===
Q mean: -69.637398
Q std: 25.312553
Actor loss: 69.641388
Action reg: 0.003986
  l1.weight: grad_norm = 0.273289
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.595330
Total gradient norm: 0.965938
=== Actor Training Debug (Iteration 5752) ===
Q mean: -68.192413
Q std: 26.372057
Actor loss: 68.196388
Action reg: 0.003974
  l1.weight: grad_norm = 0.184291
  l1.bias: grad_norm = 0.001399
  l2.weight: grad_norm = 0.406354
Total gradient norm: 0.735957
=== Actor Training Debug (Iteration 5753) ===
Q mean: -73.896675
Q std: 24.842546
Actor loss: 73.900665
Action reg: 0.003990
  l1.weight: grad_norm = 0.392885
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.928677
Total gradient norm: 1.687447
=== Actor Training Debug (Iteration 5754) ===
Q mean: -67.753464
Q std: 25.773533
Actor loss: 67.757446
Action reg: 0.003979
  l1.weight: grad_norm = 0.244886
  l1.bias: grad_norm = 0.000615
  l2.weight: grad_norm = 0.587425
Total gradient norm: 1.090878
=== Actor Training Debug (Iteration 5755) ===
Q mean: -67.255722
Q std: 24.762650
Actor loss: 67.259705
Action reg: 0.003986
  l1.weight: grad_norm = 0.372297
  l1.bias: grad_norm = 0.000604
  l2.weight: grad_norm = 0.811629
Total gradient norm: 1.709303
=== Actor Training Debug (Iteration 5756) ===
Q mean: -70.451904
Q std: 24.224541
Actor loss: 70.455887
Action reg: 0.003981
  l1.weight: grad_norm = 0.120803
  l1.bias: grad_norm = 0.000828
  l2.weight: grad_norm = 0.276213
Total gradient norm: 0.540836
=== Actor Training Debug (Iteration 5757) ===
Q mean: -67.477226
Q std: 24.041498
Actor loss: 67.481216
Action reg: 0.003989
  l1.weight: grad_norm = 0.052380
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.105953
Total gradient norm: 0.188070
=== Actor Training Debug (Iteration 5758) ===
Q mean: -70.148575
Q std: 24.671471
Actor loss: 70.152565
Action reg: 0.003990
  l1.weight: grad_norm = 0.158959
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.380197
Total gradient norm: 0.649039
=== Actor Training Debug (Iteration 5759) ===
Q mean: -72.912010
Q std: 23.852915
Actor loss: 72.916000
Action reg: 0.003988
  l1.weight: grad_norm = 0.182538
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.420584
Total gradient norm: 0.872966
=== Actor Training Debug (Iteration 5760) ===
Q mean: -70.535614
Q std: 24.675682
Actor loss: 70.539597
Action reg: 0.003984
  l1.weight: grad_norm = 0.173398
  l1.bias: grad_norm = 0.000755
  l2.weight: grad_norm = 0.350891
Total gradient norm: 0.571651
=== Actor Training Debug (Iteration 5761) ===
Q mean: -68.448456
Q std: 26.242020
Actor loss: 68.452446
Action reg: 0.003991
  l1.weight: grad_norm = 0.085469
  l1.bias: grad_norm = 0.000710
  l2.weight: grad_norm = 0.174869
Total gradient norm: 0.395264
=== Actor Training Debug (Iteration 5762) ===
Q mean: -72.064117
Q std: 23.897184
Actor loss: 72.068108
Action reg: 0.003987
  l1.weight: grad_norm = 0.588350
  l1.bias: grad_norm = 0.000833
  l2.weight: grad_norm = 1.363987
Total gradient norm: 2.533755
=== Actor Training Debug (Iteration 5763) ===
Q mean: -73.141479
Q std: 24.695570
Actor loss: 73.145477
Action reg: 0.003996
  l1.weight: grad_norm = 0.057297
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.151881
Total gradient norm: 0.330439
=== Actor Training Debug (Iteration 5764) ===
Q mean: -69.011955
Q std: 25.529205
Actor loss: 69.015945
Action reg: 0.003989
  l1.weight: grad_norm = 0.077389
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.177967
Total gradient norm: 0.353865
=== Actor Training Debug (Iteration 5765) ===
Q mean: -67.396164
Q std: 26.407288
Actor loss: 67.400139
Action reg: 0.003977
  l1.weight: grad_norm = 0.151233
  l1.bias: grad_norm = 0.001400
  l2.weight: grad_norm = 0.365854
Total gradient norm: 0.589232
=== Actor Training Debug (Iteration 5766) ===
Q mean: -72.845444
Q std: 24.060080
Actor loss: 72.849442
Action reg: 0.003996
  l1.weight: grad_norm = 0.097915
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.197290
Total gradient norm: 0.400605
=== Actor Training Debug (Iteration 5767) ===
Q mean: -72.770676
Q std: 24.577702
Actor loss: 72.774666
Action reg: 0.003987
  l1.weight: grad_norm = 0.102793
  l1.bias: grad_norm = 0.000625
  l2.weight: grad_norm = 0.230094
Total gradient norm: 0.384938
=== Actor Training Debug (Iteration 5768) ===
Q mean: -69.905388
Q std: 24.631916
Actor loss: 69.909370
Action reg: 0.003986
  l1.weight: grad_norm = 0.052379
  l1.bias: grad_norm = 0.000851
  l2.weight: grad_norm = 0.120894
Total gradient norm: 0.237182
=== Actor Training Debug (Iteration 5769) ===
Q mean: -69.637840
Q std: 24.485336
Actor loss: 69.641830
Action reg: 0.003989
  l1.weight: grad_norm = 0.042346
  l1.bias: grad_norm = 0.000665
  l2.weight: grad_norm = 0.103644
Total gradient norm: 0.215882
=== Actor Training Debug (Iteration 5770) ===
Q mean: -70.125015
Q std: 23.548950
Actor loss: 70.129013
Action reg: 0.003995
  l1.weight: grad_norm = 0.193707
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.392171
Total gradient norm: 0.671434
=== Actor Training Debug (Iteration 5771) ===
Q mean: -73.244011
Q std: 25.328133
Actor loss: 73.248001
Action reg: 0.003990
  l1.weight: grad_norm = 0.082234
  l1.bias: grad_norm = 0.000560
  l2.weight: grad_norm = 0.184908
Total gradient norm: 0.358006
=== Actor Training Debug (Iteration 5772) ===
Q mean: -70.759163
Q std: 24.454586
Actor loss: 70.763153
Action reg: 0.003990
  l1.weight: grad_norm = 0.040440
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.076317
Total gradient norm: 0.138701
=== Actor Training Debug (Iteration 5773) ===
Q mean: -70.142395
Q std: 24.753883
Actor loss: 70.146378
Action reg: 0.003985
  l1.weight: grad_norm = 0.171971
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.379085
Total gradient norm: 0.683203
=== Actor Training Debug (Iteration 5774) ===
Q mean: -69.064865
Q std: 26.328636
Actor loss: 69.068855
Action reg: 0.003988
  l1.weight: grad_norm = 0.119579
  l1.bias: grad_norm = 0.000768
  l2.weight: grad_norm = 0.299375
Total gradient norm: 0.600954
=== Actor Training Debug (Iteration 5775) ===
Q mean: -71.165794
Q std: 24.806921
Actor loss: 71.169785
Action reg: 0.003989
  l1.weight: grad_norm = 0.045576
  l1.bias: grad_norm = 0.000674
  l2.weight: grad_norm = 0.106229
Total gradient norm: 0.223804
=== Actor Training Debug (Iteration 5776) ===
Q mean: -70.167862
Q std: 25.799780
Actor loss: 70.171844
Action reg: 0.003985
  l1.weight: grad_norm = 0.060872
  l1.bias: grad_norm = 0.001083
  l2.weight: grad_norm = 0.097180
Total gradient norm: 0.159610
=== Actor Training Debug (Iteration 5777) ===
Q mean: -68.430122
Q std: 24.522926
Actor loss: 68.434113
Action reg: 0.003992
  l1.weight: grad_norm = 0.002578
  l1.bias: grad_norm = 0.000673
  l2.weight: grad_norm = 0.006826
Total gradient norm: 0.018041
=== Actor Training Debug (Iteration 5778) ===
Q mean: -71.167145
Q std: 25.011868
Actor loss: 71.171143
Action reg: 0.003997
  l1.weight: grad_norm = 0.057569
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.123416
Total gradient norm: 0.244850
=== Actor Training Debug (Iteration 5779) ===
Q mean: -68.472977
Q std: 25.883165
Actor loss: 68.476952
Action reg: 0.003978
  l1.weight: grad_norm = 0.070395
  l1.bias: grad_norm = 0.001493
  l2.weight: grad_norm = 0.144247
Total gradient norm: 0.251628
=== Actor Training Debug (Iteration 5780) ===
Q mean: -72.147446
Q std: 24.922405
Actor loss: 72.151436
Action reg: 0.003989
  l1.weight: grad_norm = 0.036189
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.080496
Total gradient norm: 0.174631
=== Actor Training Debug (Iteration 5781) ===
Q mean: -69.968857
Q std: 25.077995
Actor loss: 69.972839
Action reg: 0.003979
  l1.weight: grad_norm = 0.161693
  l1.bias: grad_norm = 0.001475
  l2.weight: grad_norm = 0.425270
Total gradient norm: 0.824648
=== Actor Training Debug (Iteration 5782) ===
Q mean: -71.495178
Q std: 24.898176
Actor loss: 71.499161
Action reg: 0.003985
  l1.weight: grad_norm = 0.041819
  l1.bias: grad_norm = 0.000895
  l2.weight: grad_norm = 0.083323
Total gradient norm: 0.138477
=== Actor Training Debug (Iteration 5783) ===
Q mean: -70.398575
Q std: 25.397156
Actor loss: 70.402565
Action reg: 0.003993
  l1.weight: grad_norm = 0.121806
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.248549
Total gradient norm: 0.565893
=== Actor Training Debug (Iteration 5784) ===
Q mean: -69.245430
Q std: 25.421112
Actor loss: 69.249420
Action reg: 0.003990
  l1.weight: grad_norm = 0.066288
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.143869
Total gradient norm: 0.266374
=== Actor Training Debug (Iteration 5785) ===
Q mean: -70.161682
Q std: 25.812706
Actor loss: 70.165665
Action reg: 0.003982
  l1.weight: grad_norm = 0.133797
  l1.bias: grad_norm = 0.000691
  l2.weight: grad_norm = 0.310312
Total gradient norm: 0.619121
=== Actor Training Debug (Iteration 5786) ===
Q mean: -70.601578
Q std: 24.720156
Actor loss: 70.605560
Action reg: 0.003983
  l1.weight: grad_norm = 0.061813
  l1.bias: grad_norm = 0.001032
  l2.weight: grad_norm = 0.145519
Total gradient norm: 0.255130
=== Actor Training Debug (Iteration 5787) ===
Q mean: -74.217995
Q std: 24.316696
Actor loss: 74.221985
Action reg: 0.003991
  l1.weight: grad_norm = 0.144848
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.349997
Total gradient norm: 0.615151
=== Actor Training Debug (Iteration 5788) ===
Q mean: -71.399857
Q std: 25.800892
Actor loss: 71.403847
Action reg: 0.003992
  l1.weight: grad_norm = 0.410320
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.922847
Total gradient norm: 1.365528
=== Actor Training Debug (Iteration 5789) ===
Q mean: -72.039482
Q std: 26.899435
Actor loss: 72.043442
Action reg: 0.003962
  l1.weight: grad_norm = 0.670214
  l1.bias: grad_norm = 0.002735
  l2.weight: grad_norm = 1.440632
Total gradient norm: 2.578921
=== Actor Training Debug (Iteration 5790) ===
Q mean: -67.295349
Q std: 24.420376
Actor loss: 67.299339
Action reg: 0.003993
  l1.weight: grad_norm = 0.010690
  l1.bias: grad_norm = 0.000567
  l2.weight: grad_norm = 0.026125
Total gradient norm: 0.063287
=== Actor Training Debug (Iteration 5791) ===
Q mean: -68.827057
Q std: 24.337008
Actor loss: 68.831039
Action reg: 0.003985
  l1.weight: grad_norm = 0.268024
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.576556
Total gradient norm: 0.976403
=== Actor Training Debug (Iteration 5792) ===
Q mean: -68.793610
Q std: 25.084824
Actor loss: 68.797600
Action reg: 0.003989
  l1.weight: grad_norm = 0.361288
  l1.bias: grad_norm = 0.000749
  l2.weight: grad_norm = 0.709139
Total gradient norm: 0.991681
=== Actor Training Debug (Iteration 5793) ===
Q mean: -73.901352
Q std: 23.152409
Actor loss: 73.905342
Action reg: 0.003987
  l1.weight: grad_norm = 0.333951
  l1.bias: grad_norm = 0.000579
  l2.weight: grad_norm = 0.749461
Total gradient norm: 1.279989
=== Actor Training Debug (Iteration 5794) ===
Q mean: -70.899933
Q std: 25.768597
Actor loss: 70.903915
Action reg: 0.003981
  l1.weight: grad_norm = 0.036681
  l1.bias: grad_norm = 0.001221
  l2.weight: grad_norm = 0.074655
Total gradient norm: 0.131515
=== Actor Training Debug (Iteration 5795) ===
Q mean: -69.910706
Q std: 26.112232
Actor loss: 69.914696
Action reg: 0.003990
  l1.weight: grad_norm = 0.006253
  l1.bias: grad_norm = 0.000811
  l2.weight: grad_norm = 0.012706
Total gradient norm: 0.028206
=== Actor Training Debug (Iteration 5796) ===
Q mean: -68.972397
Q std: 27.224602
Actor loss: 68.976372
Action reg: 0.003974
  l1.weight: grad_norm = 0.151394
  l1.bias: grad_norm = 0.001769
  l2.weight: grad_norm = 0.335178
Total gradient norm: 0.641988
=== Actor Training Debug (Iteration 5797) ===
Q mean: -70.027191
Q std: 25.354601
Actor loss: 70.031181
Action reg: 0.003988
  l1.weight: grad_norm = 0.064655
  l1.bias: grad_norm = 0.000507
  l2.weight: grad_norm = 0.157565
Total gradient norm: 0.292481
=== Actor Training Debug (Iteration 5798) ===
Q mean: -73.660713
Q std: 23.984571
Actor loss: 73.664703
Action reg: 0.003993
  l1.weight: grad_norm = 0.090266
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.215099
Total gradient norm: 0.455739
=== Actor Training Debug (Iteration 5799) ===
Q mean: -72.021988
Q std: 25.864752
Actor loss: 72.025955
Action reg: 0.003970
  l1.weight: grad_norm = 0.171475
  l1.bias: grad_norm = 0.001708
  l2.weight: grad_norm = 0.326626
Total gradient norm: 0.545395
=== Actor Training Debug (Iteration 5800) ===
Q mean: -69.075623
Q std: 24.311073
Actor loss: 69.079613
Action reg: 0.003987
  l1.weight: grad_norm = 0.111254
  l1.bias: grad_norm = 0.000687
  l2.weight: grad_norm = 0.292894
Total gradient norm: 0.650059
=== Actor Training Debug (Iteration 5801) ===
Q mean: -71.092682
Q std: 26.093719
Actor loss: 71.096672
Action reg: 0.003987
  l1.weight: grad_norm = 0.061687
  l1.bias: grad_norm = 0.000552
  l2.weight: grad_norm = 0.103673
Total gradient norm: 0.169719
=== Actor Training Debug (Iteration 5802) ===
Q mean: -71.872040
Q std: 26.166206
Actor loss: 71.876015
Action reg: 0.003978
  l1.weight: grad_norm = 0.045452
  l1.bias: grad_norm = 0.001099
  l2.weight: grad_norm = 0.121666
Total gradient norm: 0.224686
=== Actor Training Debug (Iteration 5803) ===
Q mean: -71.517265
Q std: 27.016733
Actor loss: 71.521255
Action reg: 0.003989
  l1.weight: grad_norm = 0.112531
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.260635
Total gradient norm: 0.487842
=== Actor Training Debug (Iteration 5804) ===
Q mean: -68.960815
Q std: 25.841236
Actor loss: 68.964798
Action reg: 0.003981
  l1.weight: grad_norm = 0.129013
  l1.bias: grad_norm = 0.001185
  l2.weight: grad_norm = 0.321070
Total gradient norm: 0.607920
=== Actor Training Debug (Iteration 5805) ===
Q mean: -68.103424
Q std: 25.084753
Actor loss: 68.107414
Action reg: 0.003991
  l1.weight: grad_norm = 0.049910
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.105946
Total gradient norm: 0.206456
=== Actor Training Debug (Iteration 5806) ===
Q mean: -71.963989
Q std: 25.131680
Actor loss: 71.967979
Action reg: 0.003987
  l1.weight: grad_norm = 0.038379
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.096155
Total gradient norm: 0.206122
=== Actor Training Debug (Iteration 5807) ===
Q mean: -71.518272
Q std: 25.760084
Actor loss: 71.522255
Action reg: 0.003982
  l1.weight: grad_norm = 0.209684
  l1.bias: grad_norm = 0.000712
  l2.weight: grad_norm = 0.486444
Total gradient norm: 0.966756
=== Actor Training Debug (Iteration 5808) ===
Q mean: -71.994568
Q std: 24.859531
Actor loss: 71.998512
Action reg: 0.003943
  l1.weight: grad_norm = 0.039051
  l1.bias: grad_norm = 0.000678
  l2.weight: grad_norm = 0.075174
Total gradient norm: 0.124130
=== Actor Training Debug (Iteration 5809) ===
Q mean: -71.180450
Q std: 24.583946
Actor loss: 71.184441
Action reg: 0.003991
  l1.weight: grad_norm = 0.046174
  l1.bias: grad_norm = 0.000828
  l2.weight: grad_norm = 0.095413
Total gradient norm: 0.141222
=== Actor Training Debug (Iteration 5810) ===
Q mean: -72.492722
Q std: 24.856741
Actor loss: 72.496712
Action reg: 0.003994
  l1.weight: grad_norm = 0.139231
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.306488
Total gradient norm: 0.600398
=== Actor Training Debug (Iteration 5811) ===
Q mean: -73.239586
Q std: 23.876139
Actor loss: 73.243568
Action reg: 0.003983
  l1.weight: grad_norm = 0.583672
  l1.bias: grad_norm = 0.001101
  l2.weight: grad_norm = 1.226092
Total gradient norm: 2.181699
=== Actor Training Debug (Iteration 5812) ===
Q mean: -69.076523
Q std: 25.115671
Actor loss: 69.080505
Action reg: 0.003986
  l1.weight: grad_norm = 0.050607
  l1.bias: grad_norm = 0.000680
  l2.weight: grad_norm = 0.108902
Total gradient norm: 0.226597
=== Actor Training Debug (Iteration 5813) ===
Q mean: -70.553436
Q std: 26.581005
Actor loss: 70.557419
Action reg: 0.003983
  l1.weight: grad_norm = 0.069611
  l1.bias: grad_norm = 0.001053
  l2.weight: grad_norm = 0.141265
Total gradient norm: 0.268625
=== Actor Training Debug (Iteration 5814) ===
Q mean: -73.642830
Q std: 25.509212
Actor loss: 73.646820
Action reg: 0.003989
  l1.weight: grad_norm = 0.102313
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.195074
Total gradient norm: 0.269629
=== Actor Training Debug (Iteration 5815) ===
Q mean: -73.926147
Q std: 24.744263
Actor loss: 73.930138
Action reg: 0.003988
  l1.weight: grad_norm = 0.396051
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.773535
Total gradient norm: 1.418000
=== Actor Training Debug (Iteration 5816) ===
Q mean: -71.448868
Q std: 27.710180
Actor loss: 71.452843
Action reg: 0.003975
  l1.weight: grad_norm = 0.037777
  l1.bias: grad_norm = 0.001577
  l2.weight: grad_norm = 0.089644
Total gradient norm: 0.155298
=== Actor Training Debug (Iteration 5817) ===
Q mean: -70.155014
Q std: 25.415211
Actor loss: 70.159004
Action reg: 0.003988
  l1.weight: grad_norm = 0.036298
  l1.bias: grad_norm = 0.000719
  l2.weight: grad_norm = 0.082421
Total gradient norm: 0.155172
=== Actor Training Debug (Iteration 5818) ===
Q mean: -70.547928
Q std: 25.514343
Actor loss: 70.551910
Action reg: 0.003982
Action reg: 0.003996 0.5179214696on 1203) ===
  l1.weight: grad_norm = 0.063280
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.120345
Total gradient norm: 0.216834
=== Actor Training Debug (Iteration 5829) ===
Q mean: -69.185959
Q std: 26.205883
Actor loss: 69.189949
Action reg: 0.003987
  l1.weight: grad_norm = 0.155911
  l1.bias: grad_norm = 0.000517
  l2.weight: grad_norm = 0.397613
Total gradient norm: 0.794620
=== Actor Training Debug (Iteration 5830) ===
Q mean: -71.287842
Q std: 25.643709
Actor loss: 71.291832
Action reg: 0.003988
  l1.weight: grad_norm = 0.122475
  l1.bias: grad_norm = 0.000845
  l2.weight: grad_norm = 0.248083
Total gradient norm: 0.458497
=== Actor Training Debug (Iteration 5831) ===
Q mean: -69.887909
Q std: 26.025513
Actor loss: 69.891891
Action reg: 0.003982
  l1.weight: grad_norm = 0.234829
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.529523
Total gradient norm: 1.163511
=== Actor Training Debug (Iteration 5832) ===
Q mean: -71.667786
Q std: 24.976583
Actor loss: 71.671776
Action reg: 0.003989
  l1.weight: grad_norm = 0.014263
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.028965
Total gradient norm: 0.047036
=== Actor Training Debug (Iteration 5833) ===
Q mean: -71.451782
Q std: 23.901646
Actor loss: 71.455765
Action reg: 0.003980
  l1.weight: grad_norm = 0.050908
  l1.bias: grad_norm = 0.000929
  l2.weight: grad_norm = 0.137467
Total gradient norm: 0.281412
=== Actor Training Debug (Iteration 5834) ===
Q mean: -73.678421
Q std: 24.206726
Actor loss: 73.682404
Action reg: 0.003986
  l1.weight: grad_norm = 0.379626
  l1.bias: grad_norm = 0.000794
  l2.weight: grad_norm = 0.785049
Total gradient norm: 1.463459
=== Actor Training Debug (Iteration 5835) ===
Q mean: -70.533401
Q std: 24.913374
Actor loss: 70.537392
Action reg: 0.003993
  l1.weight: grad_norm = 0.031781
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.071515
Total gradient norm: 0.150642
=== Actor Training Debug (Iteration 5836) ===
Q mean: -70.858528
Q std: 25.455648
Actor loss: 70.862518
Action reg: 0.003993
  l1.weight: grad_norm = 0.070848
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.155313
Total gradient norm: 0.293011
=== Actor Training Debug (Iteration 5837) ===
Q mean: -73.250992
Q std: 26.827055
Actor loss: 73.254974
Action reg: 0.003981
  l1.weight: grad_norm = 0.092845
  l1.bias: grad_norm = 0.001512
  l2.weight: grad_norm = 0.209490
Total gradient norm: 0.405118
=== Actor Training Debug (Iteration 5838) ===
Q mean: -71.945068
Q std: 25.501785
Actor loss: 71.949059
Action reg: 0.003987
  l1.weight: grad_norm = 0.243893
  l1.bias: grad_norm = 0.000886
  l2.weight: grad_norm = 0.526497
Total gradient norm: 0.896550
=== Actor Training Debug (Iteration 5839) ===
Q mean: -70.316101
Q std: 24.358736
Actor loss: 70.320099
Action reg: 0.003997
  l1.weight: grad_norm = 0.063825
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.147979
Total gradient norm: 0.294861
=== Actor Training Debug (Iteration 5840) ===
Q mean: -74.741394
Q std: 25.242596
Actor loss: 74.745384
Action reg: 0.003987
  l1.weight: grad_norm = 0.140725
  l1.bias: grad_norm = 0.000781
  l2.weight: grad_norm = 0.304100
Total gradient norm: 0.646300
=== Actor Training Debug (Iteration 5841) ===
Q mean: -71.796906
Q std: 24.899939
Actor loss: 71.800896
Action reg: 0.003990
  l1.weight: grad_norm = 0.178473
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.345545
Total gradient norm: 0.591704
=== Actor Training Debug (Iteration 5842) ===
Q mean: -69.279770
Q std: 23.928200
Actor loss: 69.283760
Action reg: 0.003988
  l1.weight: grad_norm = 0.146382
  l1.bias: grad_norm = 0.000526
  l2.weight: grad_norm = 0.279090
Total gradient norm: 0.526012
=== Actor Training Debug (Iteration 5843) ===
Q mean: -69.658775
Q std: 28.037836
Actor loss: 69.662750
Action reg: 0.003978
  l1.weight: grad_norm = 0.022307
  l1.bias: grad_norm = 0.001900
  l2.weight: grad_norm = 0.050543
Total gradient norm: 0.104549
=== Actor Training Debug (Iteration 5844) ===
Q mean: -71.442551
Q std: 26.775759
Actor loss: 71.446533
Action reg: 0.003981
  l1.weight: grad_norm = 0.592241
  l1.bias: grad_norm = 0.001414
  l2.weight: grad_norm = 1.074277
Total gradient norm: 2.168270
=== Actor Training Debug (Iteration 5845) ===
Q mean: -68.629723
Q std: 25.487421
Actor loss: 68.633713
Action reg: 0.003990
  l1.weight: grad_norm = 0.066900
  l1.bias: grad_norm = 0.000599
  l2.weight: grad_norm = 0.159671
Total gradient norm: 0.299893
=== Actor Training Debug (Iteration 5846) ===
Q mean: -71.158798
Q std: 24.007599
Actor loss: 71.162788
Action reg: 0.003992
  l1.weight: grad_norm = 0.219023
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.453093
Total gradient norm: 0.831494
=== Actor Training Debug (Iteration 5847) ===
Q mean: -68.607224
Q std: 24.913568
Actor loss: 68.611206
Action reg: 0.003984
  l1.weight: grad_norm = 0.489312
  l1.bias: grad_norm = 0.001211
  l2.weight: grad_norm = 1.162864
Total gradient norm: 1.791894
=== Actor Training Debug (Iteration 5848) ===
Q mean: -75.892700
Q std: 24.389334
Actor loss: 75.896690
Action reg: 0.003992
  l1.weight: grad_norm = 0.106798
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.246959
Total gradient norm: 0.490211
=== Actor Training Debug (Iteration 5849) ===
Q mean: -73.085968
Q std: 26.001402
Actor loss: 73.089958
Action reg: 0.003987
  l1.weight: grad_norm = 0.047333
  l1.bias: grad_norm = 0.001164
  l2.weight: grad_norm = 0.105010
Total gradient norm: 0.191252
=== Actor Training Debug (Iteration 5850) ===
Q mean: -72.335938
Q std: 25.141602
Actor loss: 72.339928
Action reg: 0.003989
  l1.weight: grad_norm = 0.051737
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.118029
Total gradient norm: 0.242035
=== Actor Training Debug (Iteration 5851) ===
Q mean: -71.732117
Q std: 24.865606
Actor loss: 71.736084
Action reg: 0.003970
  l1.weight: grad_norm = 0.432757
  l1.bias: grad_norm = 0.000879
  l2.weight: grad_norm = 0.948534
Total gradient norm: 1.933319
=== Actor Training Debug (Iteration 5852) ===
Q mean: -69.129822
Q std: 24.267435
Actor loss: 69.133804
Action reg: 0.003985
  l1.weight: grad_norm = 0.194555
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.454714
Total gradient norm: 0.847355
=== Actor Training Debug (Iteration 5853) ===
Q mean: -72.067078
Q std: 24.895617
Actor loss: 72.071068
Action reg: 0.003987
  l1.weight: grad_norm = 0.100660
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.239839
Total gradient norm: 0.405240
=== Actor Training Debug (Iteration 5854) ===
Q mean: -69.002609
Q std: 25.258623
Actor loss: 69.006607
Action reg: 0.003994
  l1.weight: grad_norm = 0.028537
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.067361
Total gradient norm: 0.130319
=== Actor Training Debug (Iteration 5855) ===
Q mean: -69.895782
Q std: 24.117029
Actor loss: 69.899773
Action reg: 0.003988
  l1.weight: grad_norm = 0.102634
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.210048
Total gradient norm: 0.418521
=== Actor Training Debug (Iteration 5856) ===
Q mean: -69.768456
Q std: 25.251457
Actor loss: 69.772438
Action reg: 0.003985
  l1.weight: grad_norm = 0.164397
  l1.bias: grad_norm = 0.000661
  l2.weight: grad_norm = 0.419633
Total gradient norm: 0.901869
=== Actor Training Debug (Iteration 5857) ===
Q mean: -72.223602
Q std: 27.031721
Actor loss: 72.227577
Action reg: 0.003974
  l1.weight: grad_norm = 0.319857
  l1.bias: grad_norm = 0.001585
  l2.weight: grad_norm = 0.722618
Total gradient norm: 1.235991
=== Actor Training Debug (Iteration 5858) ===
Q mean: -72.669868
Q std: 25.392244
Actor loss: 72.673859
Action reg: 0.003991
  l1.weight: grad_norm = 0.386242
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.816902
Total gradient norm: 1.635706
=== Actor Training Debug (Iteration 5859) ===
Q mean: -69.295944
Q std: 24.831881
Actor loss: 69.299934
Action reg: 0.003991
  l1.weight: grad_norm = 0.148592
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.394216
Total gradient norm: 0.911253
=== Actor Training Debug (Iteration 5860) ===
Q mean: -67.600327
Q std: 25.605352
Actor loss: 67.604317
Action reg: 0.003987
  l1.weight: grad_norm = 0.076502
  l1.bias: grad_norm = 0.000683
  l2.weight: grad_norm = 0.178623
Total gradient norm: 0.321248
=== Actor Training Debug (Iteration 5861) ===
Q mean: -71.059456
Q std: 25.201672
Actor loss: 71.063438
Action reg: 0.003982
  l1.weight: grad_norm = 0.051253
  l1.bias: grad_norm = 0.001683
  l2.weight: grad_norm = 0.107364
Total gradient norm: 0.187804
=== Actor Training Debug (Iteration 5862) ===
Q mean: -70.318520
Q std: 25.544664
Actor loss: 70.322510
Action reg: 0.003988
  l1.weight: grad_norm = 0.208099
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.432164
Total gradient norm: 0.740974
=== Actor Training Debug (Iteration 5863) ===
Q mean: -70.199577
Q std: 23.347910
Actor loss: 70.203568
Action reg: 0.003991
  l1.weight: grad_norm = 0.415668
  l1.bias: grad_norm = 0.000658
  l2.weight: grad_norm = 1.019617
Total gradient norm: 1.832604
=== Actor Training Debug (Iteration 5864) ===
Q mean: -68.314339
Q std: 25.714237
Actor loss: 68.318314
Action reg: 0.003977
  l1.weight: grad_norm = 0.112340
  l1.bias: grad_norm = 0.000891
  l2.weight: grad_norm = 0.238843
Total gradient norm: 0.452787
=== Actor Training Debug (Iteration 5865) ===
Q mean: -72.459732
Q std: 25.917540
Actor loss: 72.463715
Action reg: 0.003979
  l1.weight: grad_norm = 0.133507
  l1.bias: grad_norm = 0.000846
  l2.weight: grad_norm = 0.305448
Total gradient norm: 0.560192
=== Actor Training Debug (Iteration 5866) ===
Q mean: -72.452957
Q std: 26.241734
Actor loss: 72.456932
Action reg: 0.003976
  l1.weight: grad_norm = 0.021399
  l1.bias: grad_norm = 0.000588
  l2.weight: grad_norm = 0.047497
Total gradient norm: 0.104330
=== Actor Training Debug (Iteration 5867) ===
Q mean: -70.937157
Q std: 24.644978
Actor loss: 70.941124
Action reg: 0.003969
  l1.weight: grad_norm = 0.198684
  l1.bias: grad_norm = 0.000898
  l2.weight: grad_norm = 0.422143
Total gradient norm: 0.793652
=== Actor Training Debug (Iteration 5868) ===
Q mean: -69.743439
Q std: 25.104733
Actor loss: 69.747429
Action reg: 0.003992
  l1.weight: grad_norm = 0.058492
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.130313
Total gradient norm: 0.262230
=== Actor Training Debug (Iteration 5869) ===
Q mean: -72.111290
Q std: 24.856785
Actor loss: 72.115273
Action reg: 0.003984
  l1.weight: grad_norm = 0.011332
  l1.bias: grad_norm = 0.001168
  l2.weight: grad_norm = 0.026964
Total gradient norm: 0.050543
=== Actor Training Debug (Iteration 5870) ===
Q mean: -73.781738
Q std: 25.057251
Actor loss: 73.785721
Action reg: 0.003984
  l1.weight: grad_norm = 0.197858
  l1.bias: grad_norm = 0.000615
  l2.weight: grad_norm = 0.371588
Total gradient norm: 0.714686
=== Actor Training Debug (Iteration 5871) ===
Q mean: -72.361305
Q std: 24.776051
Actor loss: 72.365303
Action reg: 0.003995
  l1.weight: grad_norm = 0.152016
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.286599
Total gradient norm: 0.549542
=== Actor Training Debug (Iteration 5872) ===
Q mean: -68.157455
Q std: 26.278246
Actor loss: 68.161446
Action reg: 0.003987
  l1.weight: grad_norm = 0.110253
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.228247
Total gradient norm: 0.362267
=== Actor Training Debug (Iteration 5873) ===
Q mean: -74.129852
Q std: 24.355154
Actor loss: 74.133835
Action reg: 0.003983
  l1.weight: grad_norm = 0.179707
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.378925
Total gradient norm: 0.837083
=== Actor Training Debug (Iteration 5874) ===
Q mean: -72.269775
Q std: 24.805210
Actor loss: 72.273758
Action reg: 0.003983
  l1.weight: grad_norm = 0.212828
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.450661
Total gradient norm: 0.807097
=== Actor Training Debug (Iteration 5875) ===
Q mean: -72.845413
Q std: 26.670683
Actor loss: 72.849365
Action reg: 0.003952
  l1.weight: grad_norm = 0.143657
  l1.bias: grad_norm = 0.001590
  l2.weight: grad_norm = 0.351656
Total gradient norm: 0.640981
=== Actor Training Debug (Iteration 5876) ===
Q mean: -70.310928
Q std: 25.368702
Actor loss: 70.314926
Action reg: 0.003995
  l1.weight: grad_norm = 0.032734
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.069018
Total gradient norm: 0.141670
=== Actor Training Debug (Iteration 5877) ===
Q mean: -73.568130
Q std: 24.646189
Actor loss: 73.572121
Action reg: 0.003990
  l1.weight: grad_norm = 0.082273
  l1.bias: grad_norm = 0.000753
  l2.weight: grad_norm = 0.200770
Total gradient norm: 0.394147
=== Actor Training Debug (Iteration 5878) ===
Q mean: -70.215668
Q std: 24.344088
Actor loss: 70.219666
Action reg: 0.003996
  l1.weight: grad_norm = 0.121670
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.220524
Total gradient norm: 0.337529
=== Actor Training Debug (Iteration 5879) ===
Q mean: -69.009750
Q std: 25.606123
Actor loss: 69.013733
Action reg: 0.003985
  l1.weight: grad_norm = 0.158767
  l1.bias: grad_norm = 0.000991
  l2.weight: grad_norm = 0.346679
Total gradient norm: 0.640953
=== Actor Training Debug (Iteration 5880) ===
Q mean: -69.771103
Q std: 28.292295
Actor loss: 69.775085
Action reg: 0.003981
  l1.weight: grad_norm = 0.130582
  l1.bias: grad_norm = 0.001583
  l2.weight: grad_norm = 0.291488
Total gradient norm: 0.431876
=== Actor Training Debug (Iteration 5881) ===
Q mean: -71.416107
Q std: 25.665573
Actor loss: 71.420090
Action reg: 0.003986
  l1.weight: grad_norm = 0.148453
  l1.bias: grad_norm = 0.000937
  l2.weight: grad_norm = 0.320010
Total gradient norm: 0.576886
=== Actor Training Debug (Iteration 5882) ===
Q mean: -73.742188
Q std: 24.866772
Actor loss: 73.746178
Action reg: 0.003987
  l1.weight: grad_norm = 0.021812
  l1.bias: grad_norm = 0.000889
  l2.weight: grad_norm = 0.041670
Total gradient norm: 0.077431
=== Actor Training Debug (Iteration 5883) ===
Q mean: -74.357498
Q std: 24.314766
Actor loss: 74.361496
Action reg: 0.003996
  l1.weight: grad_norm = 0.023483
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.055897
Total gradient norm: 0.091252
=== Actor Training Debug (Iteration 5884) ===
Q mean: -71.564651
Q std: 24.677675
Actor loss: 71.568634
Action reg: 0.003985
  l1.weight: grad_norm = 0.112441
  l1.bias: grad_norm = 0.000694
  l2.weight: grad_norm = 0.297766
Total gradient norm: 0.646685
=== Actor Training Debug (Iteration 5885) ===
Q mean: -68.605118
Q std: 25.920416
Actor loss: 68.609085
Action reg: 0.003968
  l1.weight: grad_norm = 0.056473
  l1.bias: grad_norm = 0.002396
  l2.weight: grad_norm = 0.130987
Total gradient norm: 0.293505
=== Actor Training Debug (Iteration 5886) ===
Q mean: -71.344604
Q std: 25.445230
Actor loss: 71.348572
Action reg: 0.003970
  l1.weight: grad_norm = 0.111879
  l1.bias: grad_norm = 0.001146
  l2.weight: grad_norm = 0.292046
Total gradient norm: 0.619987
=== Actor Training Debug (Iteration 5887) ===
Q mean: -66.459435
Q std: 26.084368
Actor loss: 66.463417
Action reg: 0.003984
  l1.weight: grad_norm = 0.151088
  l1.bias: grad_norm = 0.001414
  l2.weight: grad_norm = 0.328991
Total gradient norm: 0.620918
=== Actor Training Debug (Iteration 5888) ===
Q mean: -69.029007
Q std: 24.777601
Actor loss: 69.032997
Action reg: 0.003993
  l1.weight: grad_norm = 0.143616
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.279058
Total gradient norm: 0.473476
=== Actor Training Debug (Iteration 5889) ===
Q mean: -71.204216
Q std: 24.383314
Actor loss: 71.208199
Action reg: 0.003983
  l1.weight: grad_norm = 0.117053
  l1.bias: grad_norm = 0.001418
  l2.weight: grad_norm = 0.258122
Total gradient norm: 0.530316
=== Actor Training Debug (Iteration 5890) ===
Q mean: -71.311958
Q std: 24.976416
Actor loss: 71.315948
Action reg: 0.003988
  l1.weight: grad_norm = 0.059201
  l1.bias: grad_norm = 0.000628
  l2.weight: grad_norm = 0.135664
Total gradient norm: 0.267987
=== Actor Training Debug (Iteration 5891) ===
Q mean: -73.305222
Q std: 23.204597
Actor loss: 73.309219
Action reg: 0.003995
  l1.weight: grad_norm = 0.079937
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.170855
Total gradient norm: 0.295232
=== Actor Training Debug (Iteration 5892) ===
Q mean: -70.936142
Q std: 25.179333
Actor loss: 70.940132
Action reg: 0.003993
  l1.weight: grad_norm = 0.102664
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.208627
Total gradient norm: 0.400437
=== Actor Training Debug (Iteration 5893) ===
Q mean: -73.290756
Q std: 25.575029
Actor loss: 73.294746
Action reg: 0.003993
  l1.weight: grad_norm = 0.130318
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.281129
Total gradient norm: 0.459941
=== Actor Training Debug (Iteration 5894) ===
Q mean: -72.921219
Q std: 25.534023
Actor loss: 72.925209
Action reg: 0.003989
  l1.weight: grad_norm = 0.225546
  l1.bias: grad_norm = 0.000597
  l2.weight: grad_norm = 0.570162
Total gradient norm: 1.271268
=== Actor Training Debug (Iteration 5895) ===
Q mean: -71.834534
Q std: 25.126312
Actor loss: 71.838524
Action reg: 0.003990
  l1.weight: grad_norm = 0.058178
  l1.bias: grad_norm = 0.001336
  l2.weight: grad_norm = 0.127668
Total gradient norm: 0.254022
=== Actor Training Debug (Iteration 5896) ===
Q mean: -71.347580
Q std: 24.538471
Actor loss: 71.351570
Action reg: 0.003987
  l1.weight: grad_norm = 0.237367
  l1.bias: grad_norm = 0.000612
  l2.weight: grad_norm = 0.584783
Total gradient norm: 1.182320
=== Actor Training Debug (Iteration 5897) ===
Q mean: -74.018639
Q std: 26.179831
Actor loss: 74.022621
Action reg: 0.003985
  l1.weight: grad_norm = 0.625577
  l1.bias: grad_norm = 0.001330
  l2.weight: grad_norm = 1.673161
Total gradient norm: 3.305556
=== Actor Training Debug (Iteration 5898) ===
Q mean: -71.297478
Q std: 25.451281
Actor loss: 71.301468
Action reg: 0.003991
  l1.weight: grad_norm = 0.272686
  l1.bias: grad_norm = 0.000524
  l2.weight: grad_norm = 0.535840
Total gradient norm: 0.906113
=== Actor Training Debug (Iteration 5899) ===
Q mean: -71.742462
Q std: 25.199743
Actor loss: 71.746437
Action reg: 0.003979
  l1.weight: grad_norm = 0.095842
  l1.bias: grad_norm = 0.001397
  l2.weight: grad_norm = 0.184916
Total gradient norm: 0.393790
=== Actor Training Debug (Iteration 5900) ===
Q mean: -68.097107
Q std: 26.104355
Actor loss: 68.101089
Action reg: 0.003981
  l1.weight: grad_norm = 0.078725
  l1.bias: grad_norm = 0.001512
  l2.weight: grad_norm = 0.158686
Total gradient norm: 0.279786
=== Actor Training Debug (Iteration 5901) ===
Q mean: -74.003922
Q std: 25.965786
Actor loss: 74.007919
Action reg: 0.003996
  l1.weight: grad_norm = 0.038736
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.080273
Total gradient norm: 0.130713
=== Actor Training Debug (Iteration 5902) ===
Q mean: -74.408127
Q std: 24.312990
Actor loss: 74.412117
Action reg: 0.003992
  l1.weight: grad_norm = 0.193750
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.395873
Total gradient norm: 0.564656
=== Actor Training Debug (Iteration 5903) ===
Q mean: -68.247047
Q std: 25.644627
Actor loss: 68.251022
Action reg: 0.003978
  l1.weight: grad_norm = 0.307444
  l1.bias: grad_norm = 0.000822
  l2.weight: grad_norm = 0.642300
Total gradient norm: 1.131161
=== Actor Training Debug (Iteration 5904) ===
Q mean: -68.794212
Q std: 26.442076
Actor loss: 68.798195
Action reg: 0.003986
  l1.weight: grad_norm = 0.128778
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.314676
Total gradient norm: 0.525026
=== Actor Training Debug (Iteration 5905) ===
Q mean: -71.006180
Q std: 25.380131
Actor loss: 71.010170
Action reg: 0.003990
  l1.weight: grad_norm = 0.320428
  l1.bias: grad_norm = 0.000695
  l2.weight: grad_norm = 0.605549
Total gradient norm: 1.063304
=== Actor Training Debug (Iteration 5906) ===
Q mean: -72.792801
Q std: 25.617744
Actor loss: 72.796768
Action reg: 0.003967
  l1.weight: grad_norm = 0.286488
  l1.bias: grad_norm = 0.001312
  l2.weight: grad_norm = 0.767598
Total gradient norm: 1.384178
=== Actor Training Debug (Iteration 5907) ===
Q mean: -75.380295
Q std: 25.614502
Actor loss: 75.384239
Action reg: 0.003942
  l1.weight: grad_norm = 0.282014
  l1.bias: grad_norm = 0.001102
  l2.weight: grad_norm = 0.642540
Total gradient norm: 1.288756
=== Actor Training Debug (Iteration 5908) ===
Q mean: -70.988678
Q std: 25.411680
Actor loss: 70.992668
Action reg: 0.003989
  l1.weight: grad_norm = 0.180787
  l1.bias: grad_norm = 0.000710
  l2.weight: grad_norm = 0.395077
Total gradient norm: 0.737165
=== Actor Training Debug (Iteration 5909) ===
Q mean: -73.835907
Q std: 25.555187
Actor loss: 73.839897
Action reg: 0.003989
  l1.weight: grad_norm = 0.182457
  l1.bias: grad_norm = 0.000543
  l2.weight: grad_norm = 0.479827
Total gradient norm: 1.057343
=== Actor Training Debug (Iteration 5910) ===
Q mean: -72.996155
Q std: 25.365902
Actor loss: 73.000130
Action reg: 0.003977
  l1.weight: grad_norm = 0.251895
  l1.bias: grad_norm = 0.002236
  l2.weight: grad_norm = 0.446198
Total gradient norm: 0.835014
=== Actor Training Debug (Iteration 5911) ===
Q mean: -70.763016
Q std: 24.474613
Actor loss: 70.767006
Action reg: 0.003987
  l1.weight: grad_norm = 0.057693
  l1.bias: grad_norm = 0.000538
  l2.weight: grad_norm = 0.108249
Total gradient norm: 0.228723
=== Actor Training Debug (Iteration 5912) ===
Q mean: -68.933350
Q std: 25.293016
Actor loss: 68.937340
Action reg: 0.003988
  l1.weight: grad_norm = 0.046941
  l1.bias: grad_norm = 0.001023
  l2.weight: grad_norm = 0.101722
Total gradient norm: 0.179112
=== Actor Training Debug (Iteration 5913) ===
Q mean: -69.967445
Q std: 25.559027
Actor loss: 69.971428
Action reg: 0.003980
  l1.weight: grad_norm = 0.195363
  l1.bias: grad_norm = 0.000754
  l2.weight: grad_norm = 0.392747
Total gradient norm: 0.620949
=== Actor Training Debug (Iteration 5914) ===
Q mean: -69.533211
Q std: 25.027697
Actor loss: 69.537201
Action reg: 0.003990
  l1.weight: grad_norm = 0.089478
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.205301
Total gradient norm: 0.438438
=== Actor Training Debug (Iteration 5915) ===
Q mean: -71.944954
Q std: 25.524670
Actor loss: 71.948936
Action reg: 0.003985
  l1.weight: grad_norm = 0.190608
  l1.bias: grad_norm = 0.000745
  l2.weight: grad_norm = 0.391359
Total gradient norm: 0.628633
=== Actor Training Debug (Iteration 5916) ===
Q mean: -70.700943
Q std: 25.310270
Actor loss: 70.704933
Action reg: 0.003992
  l1.weight: grad_norm = 0.042412
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.053249
Total gradient norm: 0.074063
=== Actor Training Debug (Iteration 5917) ===
Q mean: -69.177399
Q std: 26.087540
Actor loss: 69.181389
Action reg: 0.003993
  l1.weight: grad_norm = 0.092026
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.207792
Total gradient norm: 0.375174
=== Actor Training Debug (Iteration 5918) ===
Q mean: -75.526398
Q std: 25.570755
Actor loss: 75.530380
Action reg: 0.003984
  l1.weight: grad_norm = 0.068341
  l1.bias: grad_norm = 0.000740
  l2.weight: grad_norm = 0.149670
Total gradient norm: 0.275662
=== Actor Training Debug (Iteration 5919) ===
Q mean: -73.425484
Q std: 26.295176
Actor loss: 73.429466
Action reg: 0.003985
  l1.weight: grad_norm = 0.149103
  l1.bias: grad_norm = 0.001177
  l2.weight: grad_norm = 0.298215
Total gradient norm: 0.599284
=== Actor Training Debug (Iteration 5920) ===
Q mean: -72.335640
Q mean: -72.12011796 0.5179214696on 1203) ===
Q std: 24.001112
Actor loss: 72.124107
Action reg: 0.003987
  l1.weight: grad_norm = 0.025870
  l1.bias: grad_norm = 0.001559
  l2.weight: grad_norm = 0.062620
Total gradient norm: 0.117132
=== Actor Training Debug (Iteration 5931) ===
Q mean: -69.659050
Q std: 26.230631
Actor loss: 69.663033
Action reg: 0.003983
  l1.weight: grad_norm = 0.280195
  l1.bias: grad_norm = 0.001393
  l2.weight: grad_norm = 0.577636
Total gradient norm: 1.156607
=== Actor Training Debug (Iteration 5932) ===
Q mean: -68.429039
Q std: 25.485666
Actor loss: 68.433037
Action reg: 0.003996
  l1.weight: grad_norm = 0.123789
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.285978
Total gradient norm: 0.555789
=== Actor Training Debug (Iteration 5933) ===
Q mean: -70.976486
Q std: 25.795248
Actor loss: 70.980476
Action reg: 0.003991
  l1.weight: grad_norm = 0.125806
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.262758
Total gradient norm: 0.443114
=== Actor Training Debug (Iteration 5934) ===
Q mean: -73.983459
Q std: 25.642776
Actor loss: 73.987450
Action reg: 0.003988
  l1.weight: grad_norm = 0.054502
  l1.bias: grad_norm = 0.001196
  l2.weight: grad_norm = 0.117880
Total gradient norm: 0.229078
=== Actor Training Debug (Iteration 5935) ===
Q mean: -73.337753
Q std: 24.886740
Actor loss: 73.341743
Action reg: 0.003988
  l1.weight: grad_norm = 0.081443
  l1.bias: grad_norm = 0.000950
  l2.weight: grad_norm = 0.163435
Total gradient norm: 0.242562
=== Actor Training Debug (Iteration 5936) ===
Q mean: -71.248688
Q std: 24.146292
Actor loss: 71.252678
Action reg: 0.003987
  l1.weight: grad_norm = 0.275197
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.607061
Total gradient norm: 1.042936
=== Actor Training Debug (Iteration 5937) ===
Q mean: -73.077164
Q std: 24.719288
Actor loss: 73.081146
Action reg: 0.003986
  l1.weight: grad_norm = 0.232046
  l1.bias: grad_norm = 0.000680
  l2.weight: grad_norm = 0.512955
Total gradient norm: 1.072689
=== Actor Training Debug (Iteration 5938) ===
Q mean: -74.533737
Q std: 25.804493
Actor loss: 74.537712
Action reg: 0.003976
  l1.weight: grad_norm = 0.095633
  l1.bias: grad_norm = 0.001602
  l2.weight: grad_norm = 0.215163
Total gradient norm: 0.448687
=== Actor Training Debug (Iteration 5939) ===
Q mean: -72.346817
Q std: 26.057081
Actor loss: 72.350807
Action reg: 0.003991
  l1.weight: grad_norm = 0.045282
  l1.bias: grad_norm = 0.000611
  l2.weight: grad_norm = 0.088376
Total gradient norm: 0.169875
=== Actor Training Debug (Iteration 5940) ===
Q mean: -69.660889
Q std: 26.557646
Actor loss: 69.664879
Action reg: 0.003992
  l1.weight: grad_norm = 0.354522
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.750698
Total gradient norm: 1.201968
=== Actor Training Debug (Iteration 5941) ===
Q mean: -70.388489
Q std: 25.976414
Actor loss: 70.392471
Action reg: 0.003980
  l1.weight: grad_norm = 0.222219
  l1.bias: grad_norm = 0.001338
  l2.weight: grad_norm = 0.634297
Total gradient norm: 1.354189
=== Actor Training Debug (Iteration 5942) ===
Q mean: -72.421181
Q std: 25.162605
Actor loss: 72.425171
Action reg: 0.003991
  l1.weight: grad_norm = 0.093029
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.199372
Total gradient norm: 0.325014
=== Actor Training Debug (Iteration 5943) ===
Q mean: -71.330612
Q std: 27.853125
Actor loss: 71.334587
Action reg: 0.003973
  l1.weight: grad_norm = 0.016487
  l1.bias: grad_norm = 0.002277
  l2.weight: grad_norm = 0.034469
Total gradient norm: 0.078252
=== Actor Training Debug (Iteration 5944) ===
Q mean: -71.765251
Q std: 25.660078
Actor loss: 71.769234
Action reg: 0.003986
  l1.weight: grad_norm = 0.223150
  l1.bias: grad_norm = 0.000948
  l2.weight: grad_norm = 0.501120
Total gradient norm: 0.852663
=== Actor Training Debug (Iteration 5945) ===
Q mean: -69.852432
Q std: 26.755465
Actor loss: 69.856422
Action reg: 0.003989
  l1.weight: grad_norm = 0.191159
  l1.bias: grad_norm = 0.001023
  l2.weight: grad_norm = 0.431708
Total gradient norm: 0.735329
=== Actor Training Debug (Iteration 5946) ===
Q mean: -71.391708
Q std: 26.267057
Actor loss: 71.395699
Action reg: 0.003992
  l1.weight: grad_norm = 0.102697
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.191665
Total gradient norm: 0.302407
=== Actor Training Debug (Iteration 5947) ===
Q mean: -71.002708
Q std: 25.903639
Actor loss: 71.006691
Action reg: 0.003982
  l1.weight: grad_norm = 0.036431
  l1.bias: grad_norm = 0.001444
  l2.weight: grad_norm = 0.079858
Total gradient norm: 0.148610
=== Actor Training Debug (Iteration 5948) ===
Q mean: -72.570877
Q std: 25.900805
Actor loss: 72.574860
Action reg: 0.003985
  l1.weight: grad_norm = 0.068693
  l1.bias: grad_norm = 0.000838
  l2.weight: grad_norm = 0.131985
Total gradient norm: 0.181954
=== Actor Training Debug (Iteration 5949) ===
Q mean: -73.138412
Q std: 25.840265
Actor loss: 73.142395
Action reg: 0.003984
  l1.weight: grad_norm = 0.226157
  l1.bias: grad_norm = 0.000772
  l2.weight: grad_norm = 0.472338
Total gradient norm: 0.718205
=== Actor Training Debug (Iteration 5950) ===
Q mean: -69.604874
Q std: 24.802658
Actor loss: 69.608856
Action reg: 0.003986
  l1.weight: grad_norm = 0.216812
  l1.bias: grad_norm = 0.000798
  l2.weight: grad_norm = 0.467427
Total gradient norm: 0.778377
=== Actor Training Debug (Iteration 5951) ===
Q mean: -67.921967
Q std: 25.917103
Actor loss: 67.925957
Action reg: 0.003990
  l1.weight: grad_norm = 0.131308
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.282676
Total gradient norm: 0.589010
=== Actor Training Debug (Iteration 5952) ===
Q mean: -72.974289
Q std: 26.069351
Actor loss: 72.978287
Action reg: 0.003996
  l1.weight: grad_norm = 0.034435
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.067898
Total gradient norm: 0.117321
=== Actor Training Debug (Iteration 5953) ===
Q mean: -73.277832
Q std: 25.809450
Actor loss: 73.281815
Action reg: 0.003981
  l1.weight: grad_norm = 0.342644
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.770733
Total gradient norm: 1.551125
=== Actor Training Debug (Iteration 5954) ===
Q mean: -72.783386
Q std: 25.178982
Actor loss: 72.787376
Action reg: 0.003989
  l1.weight: grad_norm = 0.135635
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.264627
Total gradient norm: 0.503322
=== Actor Training Debug (Iteration 5955) ===
Q mean: -69.999924
Q std: 25.086761
Actor loss: 70.003906
Action reg: 0.003986
  l1.weight: grad_norm = 0.334158
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.704515
Total gradient norm: 1.230643
=== Actor Training Debug (Iteration 5956) ===
Q mean: -72.399277
Q std: 23.806021
Actor loss: 72.403267
Action reg: 0.003987
  l1.weight: grad_norm = 0.147494
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.345550
Total gradient norm: 0.628143
=== Actor Training Debug (Iteration 5957) ===
Q mean: -72.390366
Q std: 24.592802
Actor loss: 72.394348
Action reg: 0.003985
  l1.weight: grad_norm = 0.168394
  l1.bias: grad_norm = 0.001180
  l2.weight: grad_norm = 0.354149
Total gradient norm: 0.587609
Total gradient norm: 1.4559904696on 1203) ===
=== Actor Training Debug (Iteration 5968) ===
Q mean: -71.608345
Q std: 25.196297
Actor loss: 71.612335
Action reg: 0.003994
  l1.weight: grad_norm = 0.035554
  l1.bias: grad_norm = 0.001000
  l2.weight: grad_norm = 0.081273
Total gradient norm: 0.160895
=== Actor Training Debug (Iteration 5969) ===
Q mean: -71.268997
Q std: 24.822536
Actor loss: 71.272987
Action reg: 0.003991
  l1.weight: grad_norm = 0.040848
  l1.bias: grad_norm = 0.001024
  l2.weight: grad_norm = 0.083456
Total gradient norm: 0.144737
=== Actor Training Debug (Iteration 5970) ===
Q mean: -70.622421
Q std: 26.644947
Actor loss: 70.626411
Action reg: 0.003992
  l1.weight: grad_norm = 0.059818
  l1.bias: grad_norm = 0.000689
  l2.weight: grad_norm = 0.130508
Total gradient norm: 0.236084
=== Actor Training Debug (Iteration 5971) ===
Q mean: -70.301682
Q std: 24.166847
Actor loss: 70.305672
Action reg: 0.003992
  l1.weight: grad_norm = 0.184165
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.409845
Total gradient norm: 0.639488
=== Actor Training Debug (Iteration 5972) ===
Q mean: -71.782883
Q std: 26.833231
Actor loss: 71.786865
Action reg: 0.003981
  l1.weight: grad_norm = 0.217846
  l1.bias: grad_norm = 0.000898
  l2.weight: grad_norm = 0.440159
Total gradient norm: 0.801090
=== Actor Training Debug (Iteration 5973) ===
Q mean: -69.961433
Q std: 25.383762
Actor loss: 69.965416
Action reg: 0.003983
  l1.weight: grad_norm = 0.149241
  l1.bias: grad_norm = 0.001790
  l2.weight: grad_norm = 0.313981
Total gradient norm: 0.610344
=== Actor Training Debug (Iteration 5974) ===
Q mean: -69.155869
Q std: 26.407295
Actor loss: 69.159859
Action reg: 0.003993
  l1.weight: grad_norm = 0.063571
  l1.bias: grad_norm = 0.000782
  l2.weight: grad_norm = 0.140273
Total gradient norm: 0.223679
=== Actor Training Debug (Iteration 5975) ===
Q mean: -74.459969
Q std: 25.119267
Actor loss: 74.463959
Action reg: 0.003989
  l1.weight: grad_norm = 0.112078
  l1.bias: grad_norm = 0.000744
  l2.weight: grad_norm = 0.273821
Total gradient norm: 0.595085
=== Actor Training Debug (Iteration 5976) ===
Q mean: -72.187378
Q std: 24.698093
Actor loss: 72.191368
Action reg: 0.003993
  l1.weight: grad_norm = 0.056861
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.115858
Total gradient norm: 0.220570
=== Actor Training Debug (Iteration 5977) ===
Q mean: -68.113022
Q std: 25.377277
Actor loss: 68.117004
Action reg: 0.003982
  l1.weight: grad_norm = 0.241044
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.508860
Total gradient norm: 0.919999
=== Actor Training Debug (Iteration 5978) ===
Q mean: -69.844116
Q std: 27.431509
Actor loss: 69.848091
Action reg: 0.003976
  l1.weight: grad_norm = 0.026497
  l1.bias: grad_norm = 0.002265
  l2.weight: grad_norm = 0.060591
Total gradient norm: 0.122708
=== Actor Training Debug (Iteration 5979) ===
Q mean: -71.627487
Q std: 26.764311
Actor loss: 71.631462
Action reg: 0.003978
  l1.weight: grad_norm = 0.379682
  l1.bias: grad_norm = 0.001622
  l2.weight: grad_norm = 0.918053
Total gradient norm: 1.890598
=== Actor Training Debug (Iteration 5980) ===
Q mean: -72.391449
Q std: 26.103954
Actor loss: 72.395447
Action reg: 0.003995
  l1.weight: grad_norm = 0.155880
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.310377
Total gradient norm: 0.548030
=== Actor Training Debug (Iteration 5981) ===
Q mean: -72.838303
Q std: 24.774111
Actor loss: 72.842293
Action reg: 0.003989
  l1.weight: grad_norm = 0.257441
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.580191
Total gradient norm: 1.113565
=== Actor Training Debug (Iteration 5982) ===
Q mean: -70.805527
Q std: 27.103405
Actor loss: 70.809517
Action reg: 0.003988
  l1.weight: grad_norm = 0.052149
  l1.bias: grad_norm = 0.000642
  l2.weight: grad_norm = 0.112735
Total gradient norm: 0.206095
=== Actor Training Debug (Iteration 5983) ===
Q mean: -67.879372
Q std: 25.871714
Actor loss: 67.883354
Action reg: 0.003984
  l1.weight: grad_norm = 0.049338
  l1.bias: grad_norm = 0.000781
  l2.weight: grad_norm = 0.107406
Total gradient norm: 0.244576
=== Actor Training Debug (Iteration 5984) ===
Q mean: -72.678650
Q std: 25.682169
Actor loss: 72.682632
Action reg: 0.003985
  l1.weight: grad_norm = 0.710342
  l1.bias: grad_norm = 0.001036
  l2.weight: grad_norm = 1.473548
Total gradient norm: 2.564927
=== Actor Training Debug (Iteration 5985) ===
Q mean: -75.401245
Q std: 24.628178
Actor loss: 75.405243
Action reg: 0.003996
  l1.weight: grad_norm = 0.070721
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.160408
Total gradient norm: 0.270596
=== Actor Training Debug (Iteration 5986) ===
Q mean: -68.316948
Q std: 23.552402
Actor loss: 68.320946
Action reg: 0.003996
  l1.weight: grad_norm = 0.142242
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.313575
Total gradient norm: 0.534039
=== Actor Training Debug (Iteration 5987) ===
Q mean: -70.182800
Q std: 25.808296
Actor loss: 70.186798
Action reg: 0.003996
  l1.weight: grad_norm = 0.010626
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.023934
Total gradient norm: 0.048241
Total gradient norm: 0.7450064696on 1203) ===
=== Actor Training Debug (Iteration 5998) ===
Q mean: -69.142410
Q std: 26.054770
Actor loss: 69.146393
Action reg: 0.003983
  l1.weight: grad_norm = 0.182402
  l1.bias: grad_norm = 0.001569
  l2.weight: grad_norm = 0.363814
Total gradient norm: 0.704994
=== Actor Training Debug (Iteration 5999) ===
Q mean: -73.117630
Q std: 24.300991
Actor loss: 73.121628
Action reg: 0.003996
  l1.weight: grad_norm = 0.077732
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.160711
Total gradient norm: 0.261544
=== Actor Training Debug (Iteration 6000) ===
Q mean: -70.061798
Q std: 25.640825
Actor loss: 70.065781
Action reg: 0.003985
  l1.weight: grad_norm = 0.311445
  l1.bias: grad_norm = 0.001313
  l2.weight: grad_norm = 0.787206
Total gradient norm: 1.486515
Step 11000: Critic Loss: 5.9190, Actor Loss: 70.0658, Q Value: -70.0618
  Average reward: -335.771 | Average length: 100.0
Evaluation at episode 110: -335.771
=== Actor Training Debug (Iteration 6001) ===
Q mean: -73.803642
Q std: 25.524624
Actor loss: 73.807640
Action reg: 0.003997
  l1.weight: grad_norm = 0.019101
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.037935
Total gradient norm: 0.072054
=== Actor Training Debug (Iteration 6002) ===
Q mean: -72.645309
Q std: 25.922995
Actor loss: 72.649307
Action reg: 0.003995
  l1.weight: grad_norm = 0.023014
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.048598
Total gradient norm: 0.071426
=== Actor Training Debug (Iteration 6003) ===
Q mean: -73.029785
Q std: 26.816296
Actor loss: 73.033768
Action reg: 0.003981
  l1.weight: grad_norm = 0.219809
  l1.bias: grad_norm = 0.001005
  l2.weight: grad_norm = 0.514684
Total gradient norm: 1.018884
=== Actor Training Debug (Iteration 6004) ===
Q mean: -68.598022
Q std: 26.615131
Actor loss: 68.602013
Action reg: 0.003988
  l1.weight: grad_norm = 0.314541
  l1.bias: grad_norm = 0.000891
  l2.weight: grad_norm = 0.626151
Total gradient norm: 1.060746
=== Actor Training Debug (Iteration 6005) ===
Q mean: -71.691971
Q std: 25.395594
Actor loss: 71.695961
Action reg: 0.003988
  l1.weight: grad_norm = 0.261801
  l1.bias: grad_norm = 0.001102
  l2.weight: grad_norm = 0.581805
Total gradient norm: 0.966741
=== Actor Training Debug (Iteration 6006) ===
Q mean: -73.230774
Q std: 23.914160
Actor loss: 73.234764
Action reg: 0.003990
  l1.weight: grad_norm = 0.005031
  l1.bias: grad_norm = 0.001668
  l2.weight: grad_norm = 0.014121
Total gradient norm: 0.044563
=== Actor Training Debug (Iteration 6007) ===
Q mean: -73.359421
Q std: 24.958208
Actor loss: 73.363411
Action reg: 0.003992
  l1.weight: grad_norm = 0.085595
  l1.bias: grad_norm = 0.000699
  l2.weight: grad_norm = 0.224274
Total gradient norm: 0.472535
=== Actor Training Debug (Iteration 6008) ===
Q mean: -72.828400
Q std: 24.659901
Actor loss: 72.832382
Action reg: 0.003986
  l1.weight: grad_norm = 0.235332
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.494651
Total gradient norm: 0.889124
=== Actor Training Debug (Iteration 6009) ===
Q mean: -71.383774
Q std: 26.239349
Actor loss: 71.387764
Action reg: 0.003992
  l1.weight: grad_norm = 0.238295
  l1.bias: grad_norm = 0.000747
  l2.weight: grad_norm = 0.639497
Total gradient norm: 1.263688
=== Actor Training Debug (Iteration 6010) ===
Q mean: -69.216476
Q std: 26.247847
Actor loss: 69.220467
Action reg: 0.003992
  l1.weight: grad_norm = 0.160890
  l1.bias: grad_norm = 0.000792
  l2.weight: grad_norm = 0.364921
Total gradient norm: 0.686241
=== Actor Training Debug (Iteration 6011) ===
Q mean: -70.516083
Q std: 25.765581
Actor loss: 70.520073
Action reg: 0.003987
  l1.weight: grad_norm = 0.426340
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.915946
Total gradient norm: 1.625988
=== Actor Training Debug (Iteration 6012) ===
Q mean: -73.052383
Q std: 26.344507
Actor loss: 73.056366
Action reg: 0.003986
  l1.weight: grad_norm = 0.312295
  l1.bias: grad_norm = 0.000824
  l2.weight: grad_norm = 0.653239
Total gradient norm: 1.074899
=== Actor Training Debug (Iteration 6013) ===
Q mean: -72.498444
Q std: 24.475073
Actor loss: 72.502434
Action reg: 0.003993
  l1.weight: grad_norm = 0.144354
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.265272
Total gradient norm: 0.406983
=== Actor Training Debug (Iteration 6014) ===
Q mean: -73.469902
Q std: 24.735703
Actor loss: 73.473892
Action reg: 0.003989
  l1.weight: grad_norm = 0.152764
  l1.bias: grad_norm = 0.000948
  l2.weight: grad_norm = 0.284480
Total gradient norm: 0.536273
=== Actor Training Debug (Iteration 6015) ===
Q mean: -72.544205
Q std: 26.393761
Actor loss: 72.548195
Action reg: 0.003989
  l1.weight: grad_norm = 0.070183
  l1.bias: grad_norm = 0.000773
  l2.weight: grad_norm = 0.145414
Total gradient norm: 0.246828
=== Actor Training Debug (Iteration 6016) ===
Q mean: -72.405121
Q std: 26.460678
Actor loss: 72.409103
Action reg: 0.003983
  l1.weight: grad_norm = 0.242221
  l1.bias: grad_norm = 0.001029
  l2.weight: grad_norm = 0.475551
Total gradient norm: 0.858422
=== Actor Training Debug (Iteration 6017) ===
Q mean: -71.150177
Q std: 27.029207
Actor loss: 71.154160
Action reg: 0.003979
  l1.weight: grad_norm = 0.311682
  l1.bias: grad_norm = 0.001638
  l2.weight: grad_norm = 0.708959
Total gradient norm: 1.227782
=== Actor Training Debug (Iteration 6018) ===
Q mean: -70.729240
Q std: 26.067753
Actor loss: 70.733223
Action reg: 0.003985
  l1.weight: grad_norm = 0.218359
  l1.bias: grad_norm = 0.000904
  l2.weight: grad_norm = 0.515059
Total gradient norm: 1.030273
=== Actor Training Debug (Iteration 6019) ===
Q mean: -73.069923
Q std: 27.545366
Actor loss: 73.073914
Action reg: 0.003987
  l1.weight: grad_norm = 0.047489
  l1.bias: grad_norm = 0.001284
  l2.weight: grad_norm = 0.109549
Total gradient norm: 0.241713
=== Actor Training Debug (Iteration 6020) ===
Q mean: -70.520615
Q std: 26.661230
Actor loss: 70.524597
Action reg: 0.003985
  l1.weight: grad_norm = 0.180178
  l1.bias: grad_norm = 0.001175
  l2.weight: grad_norm = 0.434420
Total gradient norm: 0.856103
=== Actor Training Debug (Iteration 6021) ===
Q mean: -71.877365
Q std: 26.909119
Actor loss: 71.881363
Action reg: 0.003995
  l1.weight: grad_norm = 0.006654
  l1.bias: grad_norm = 0.000798
  l2.weight: grad_norm = 0.015538
Total gradient norm: 0.033529
Q std: 24.590086orm: 0.7450064696on 1203) ===
Actor loss: 72.617332
Action reg: 0.003993
  l1.weight: grad_norm = 0.243509
  l1.bias: grad_norm = 0.000650
  l2.weight: grad_norm = 0.439138
Total gradient norm: 0.852156
=== Actor Training Debug (Iteration 6044) ===
Q mean: -73.438515
Q std: 25.378857
Actor loss: 73.442513
Action reg: 0.003996
  l1.weight: grad_norm = 0.009116
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.024031
Total gradient norm: 0.056544
=== Actor Training Debug (Iteration 6045) ===
Q mean: -71.405151
Q std: 26.463161
Actor loss: 71.409134
Action reg: 0.003986
  l1.weight: grad_norm = 0.206749
  l1.bias: grad_norm = 0.001682
  l2.weight: grad_norm = 0.533886
Total gradient norm: 1.026422
=== Actor Training Debug (Iteration 6046) ===
Q mean: -70.542480
Q std: 24.224140
Actor loss: 70.546448
Action reg: 0.003967
  l1.weight: grad_norm = 1.184655
  l1.bias: grad_norm = 0.000943
  l2.weight: grad_norm = 2.837877
Total gradient norm: 5.801608
=== Actor Training Debug (Iteration 6047) ===
Q mean: -75.186546
Q std: 25.040350
Actor loss: 75.190536
Action reg: 0.003991
  l1.weight: grad_norm = 0.014755
  l1.bias: grad_norm = 0.001014
  l2.weight: grad_norm = 0.030076
Total gradient norm: 0.057351
=== Actor Training Debug (Iteration 6048) ===
Q mean: -70.308807
Q std: 24.400181
Actor loss: 70.312798
Action reg: 0.003991
  l1.weight: grad_norm = 0.165966
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.432364
Total gradient norm: 0.864391
=== Actor Training Debug (Iteration 6049) ===
Q mean: -69.755829
Q std: 24.854376
Actor loss: 69.759811
Action reg: 0.003981
  l1.weight: grad_norm = 0.177822
  l1.bias: grad_norm = 0.001832
  l2.weight: grad_norm = 0.344174
Total gradient norm: 0.583063
=== Actor Training Debug (Iteration 6050) ===
Q mean: -70.775909
Q std: 24.568672
Actor loss: 70.779900
Action reg: 0.003992
  l1.weight: grad_norm = 0.020266
  l1.bias: grad_norm = 0.001351
  l2.weight: grad_norm = 0.050454
Total gradient norm: 0.083649
=== Actor Training Debug (Iteration 6051) ===
Q mean: -72.962273
Q std: 25.891247
Actor loss: 72.966255
Action reg: 0.003983
  l1.weight: grad_norm = 0.036412
  l1.bias: grad_norm = 0.002061
  l2.weight: grad_norm = 0.069447
Total gradient norm: 0.139536
=== Actor Training Debug (Iteration 6052) ===
Q mean: -72.230278
Q std: 27.260668
Actor loss: 72.234253
Action reg: 0.003973
  l1.weight: grad_norm = 0.325660
  l1.bias: grad_norm = 0.003419
  l2.weight: grad_norm = 0.802097
Total gradient norm: 1.549108
=== Actor Training Debug (Iteration 6053) ===
Q mean: -67.742249
Q std: 25.708635
Actor loss: 67.746239
Action reg: 0.003989
  l1.weight: grad_norm = 0.134228
  l1.bias: grad_norm = 0.001176
  l2.weight: grad_norm = 0.254651
Total gradient norm: 0.407261
=== Actor Training Debug (Iteration 6054) ===
Q mean: -68.219299
Q std: 26.079401
Actor loss: 68.223274
Action reg: 0.003971
  l1.weight: grad_norm = 0.105433
  l1.bias: grad_norm = 0.003416
  l2.weight: grad_norm = 0.294537
Total gradient norm: 0.590478
=== Actor Training Debug (Iteration 6055) ===
Q mean: -70.398338
Q std: 26.654253
Actor loss: 70.402328
Action reg: 0.003988
  l1.weight: grad_norm = 0.035706
  l1.bias: grad_norm = 0.001882
  l2.weight: grad_norm = 0.107709
Total gradient norm: 0.218259
=== Actor Training Debug (Iteration 6056) ===
Q mean: -73.832321
Q std: 26.030312
Actor loss: 73.836304
Action reg: 0.003985
  l1.weight: grad_norm = 0.365327
  l1.bias: grad_norm = 0.001533
  l2.weight: grad_norm = 0.874006
Total gradient norm: 1.473279
=== Actor Training Debug (Iteration 6057) ===
Q mean: -73.139404
Q std: 26.079350
Actor loss: 73.143387
Action reg: 0.003986
  l1.weight: grad_norm = 1.511456
  l1.bias: grad_norm = 0.001681
  l2.weight: grad_norm = 3.861318
Total gradient norm: 7.795715
=== Actor Training Debug (Iteration 6058) ===
Q mean: -70.821442
Q std: 28.279266
Actor loss: 70.825417
Action reg: 0.003977
  l1.weight: grad_norm = 0.122764
  l1.bias: grad_norm = 0.002439
  l2.weight: grad_norm = 0.236164
Total gradient norm: 0.490972
=== Actor Training Debug (Iteration 6059) ===
Q mean: -73.919083
Q std: 26.337767
Actor loss: 73.923065
Action reg: 0.003985
  l1.weight: grad_norm = 0.093026
  l1.bias: grad_norm = 0.001671
  l2.weight: grad_norm = 0.204345
Total gradient norm: 0.358510
=== Actor Training Debug (Iteration 6060) ===
Q mean: -70.948563
Q std: 25.106384
Actor loss: 70.952545
Action reg: 0.003986
  l1.weight: grad_norm = 0.091319
  l1.bias: grad_norm = 0.001492
  l2.weight: grad_norm = 0.225356
Total gradient norm: 0.479003
=== Actor Training Debug (Iteration 6061) ===
Q mean: -69.009857
Q std: 26.733458
Actor loss: 69.013840
Action reg: 0.003979
  l1.weight: grad_norm = 0.122688
  l1.bias: grad_norm = 0.002039
  l2.weight: grad_norm = 0.269354
Total gradient norm: 0.584658
=== Actor Training Debug (Iteration 6062) ===
Q mean: -72.054070
Q std: 25.664455
Actor loss: 72.058060
Action reg: 0.003990
  l1.weight: grad_norm = 0.107991
  l1.bias: grad_norm = 0.000927
  l2.weight: grad_norm = 0.224492
Total gradient norm: 0.437865
=== Actor Training Debug (Iteration 6063) ===
Q mean: -72.233032
Q std: 25.918842
Actor loss: 72.237022
Action reg: 0.003993
  l1.weight: grad_norm = 0.010411
  l1.bias: grad_norm = 0.001411
  l2.weight: grad_norm = 0.023646
Total gradient norm: 0.046290
=== Actor Training Debug (Iteration 6064) ===
Q mean: -71.506027
Q std: 25.817440
Actor loss: 71.510025
Action reg: 0.003997
Action reg: 0.003985 0.7450064696on 1203) ===
  l1.weight: grad_norm = 0.166917
  l1.bias: grad_norm = 0.001658
  l2.weight: grad_norm = 0.373008
Total gradient norm: 0.677832
=== Actor Training Debug (Iteration 6075) ===
Q mean: -71.198547
Q std: 25.403078
Actor loss: 71.202538
Action reg: 0.003989
  l1.weight: grad_norm = 0.063327
  l1.bias: grad_norm = 0.000899
  l2.weight: grad_norm = 0.172026
Total gradient norm: 0.364404
=== Actor Training Debug (Iteration 6076) ===
Q mean: -71.865105
Q std: 25.093298
Actor loss: 71.869095
Action reg: 0.003988
  l1.weight: grad_norm = 0.233085
  l1.bias: grad_norm = 0.000707
  l2.weight: grad_norm = 0.641401
Total gradient norm: 1.290099
=== Actor Training Debug (Iteration 6077) ===
Q mean: -69.637390
Q std: 25.867937
Actor loss: 69.641365
Action reg: 0.003977
  l1.weight: grad_norm = 0.491232
  l1.bias: grad_norm = 0.001571
  l2.weight: grad_norm = 1.004010
Total gradient norm: 2.026162
=== Actor Training Debug (Iteration 6078) ===
Q mean: -69.607071
Q std: 26.942532
Actor loss: 69.611053
Action reg: 0.003982
  l1.weight: grad_norm = 0.119571
  l1.bias: grad_norm = 0.001496
  l2.weight: grad_norm = 0.293405
Total gradient norm: 0.554443
=== Actor Training Debug (Iteration 6079) ===
Q mean: -74.107689
Q std: 25.853447
Actor loss: 74.111671
Action reg: 0.003984
  l1.weight: grad_norm = 0.127763
  l1.bias: grad_norm = 0.002190
  l2.weight: grad_norm = 0.318778
Total gradient norm: 0.617266
=== Actor Training Debug (Iteration 6080) ===
Q mean: -73.070465
Q std: 27.133085
Actor loss: 73.074448
Action reg: 0.003985
  l1.weight: grad_norm = 0.174013
  l1.bias: grad_norm = 0.001388
  l2.weight: grad_norm = 0.454955
Total gradient norm: 0.834947
=== Actor Training Debug (Iteration 6081) ===
Q mean: -69.673256
Q std: 25.514805
Actor loss: 69.677238
Action reg: 0.003985
  l1.weight: grad_norm = 0.159955
  l1.bias: grad_norm = 0.000969
  l2.weight: grad_norm = 0.394911
Total gradient norm: 0.827378
=== Actor Training Debug (Iteration 6082) ===
Q mean: -70.647430
Q std: 25.117844
Actor loss: 70.651421
Action reg: 0.003992
  l1.weight: grad_norm = 0.158747
  l1.bias: grad_norm = 0.000776
  l2.weight: grad_norm = 0.325266
Total gradient norm: 0.658500
=== Actor Training Debug (Iteration 6083) ===
Q mean: -71.029694
Q std: 26.746399
Actor loss: 71.033669
Action reg: 0.003972
  l1.weight: grad_norm = 0.055238
  l1.bias: grad_norm = 0.003602
  l2.weight: grad_norm = 0.123936
Total gradient norm: 0.281254
=== Actor Training Debug (Iteration 6084) ===
Q mean: -72.039886
Q std: 25.801317
Actor loss: 72.043869
Action reg: 0.003983
  l1.weight: grad_norm = 0.269754
  l1.bias: grad_norm = 0.001277
  l2.weight: grad_norm = 0.603047
Total gradient norm: 1.188523
=== Actor Training Debug (Iteration 6085) ===
Q mean: -75.674744
Q std: 26.314400
Actor loss: 75.678726
Action reg: 0.003981
  l1.weight: grad_norm = 0.239124
  l1.bias: grad_norm = 0.002393
  l2.weight: grad_norm = 0.560993
Total gradient norm: 1.013951
=== Actor Training Debug (Iteration 6086) ===
Q mean: -71.418930
Q std: 25.872480
Actor loss: 71.422920
Action reg: 0.003987
  l1.weight: grad_norm = 0.033145
  l1.bias: grad_norm = 0.001261
  l2.weight: grad_norm = 0.074017
Total gradient norm: 0.134075
=== Actor Training Debug (Iteration 6087) ===
Q mean: -73.293938
Q std: 26.416592
Actor loss: 73.297920
Action reg: 0.003984
  l1.weight: grad_norm = 0.109192
  l1.bias: grad_norm = 0.001932
  l2.weight: grad_norm = 0.239219
Total gradient norm: 0.514048
=== Actor Training Debug (Iteration 6088) ===
Q mean: -71.746933
Q std: 26.418669
Actor loss: 71.750916
Action reg: 0.003981
  l1.weight: grad_norm = 0.336416
  l1.bias: grad_norm = 0.001782
  l2.weight: grad_norm = 0.642044
Total gradient norm: 1.045984
=== Actor Training Debug (Iteration 6089) ===
Q mean: -73.309814
Q std: 25.938267
Actor loss: 73.313797
Action reg: 0.003984
  l1.weight: grad_norm = 0.174035
  l1.bias: grad_norm = 0.001552
  l2.weight: grad_norm = 0.394569
Total gradient norm: 0.762810
=== Actor Training Debug (Iteration 6090) ===
Q mean: -70.819550
Q std: 26.525541
Actor loss: 70.823532
Action reg: 0.003982
  l1.weight: grad_norm = 0.256519
  l1.bias: grad_norm = 0.001748
  l2.weight: grad_norm = 0.633348
Total gradient norm: 1.183006
=== Actor Training Debug (Iteration 6091) ===
Q mean: -69.708649
Q std: 27.320230
Actor loss: 69.712631
Action reg: 0.003982
  l1.weight: grad_norm = 0.116845
  l1.bias: grad_norm = 0.002203
  l2.weight: grad_norm = 0.256121
Total gradient norm: 0.538645
=== Actor Training Debug (Iteration 6092) ===
Q mean: -71.988091
Q std: 27.057314
Actor loss: 71.992081
Action reg: 0.003990
  l1.weight: grad_norm = 0.131379
  l1.bias: grad_norm = 0.000910
  l2.weight: grad_norm = 0.417185
Total gradient norm: 0.878487
=== Actor Training Debug (Iteration 6093) ===
Q mean: -71.506271
Q std: 26.790445
Actor loss: 71.510254
Action reg: 0.003981
  l1.weight: grad_norm = 0.162690
  l1.bias: grad_norm = 0.002017
  l2.weight: grad_norm = 0.471533
Total gradient norm: 1.022260
=== Actor Training Debug (Iteration 6094) ===
Q mean: -73.650192
Q std: 25.672915
Actor loss: 73.654182
Action reg: 0.003990
  l1.weight: grad_norm = 0.030772
  l1.bias: grad_norm = 0.001278
  l2.weight: grad_norm = 0.078476
Total gradient norm: 0.150938
=== Actor Training Debug (Iteration 6095) ===
Q mean: -69.986435
Q std: 27.283215
Actor loss: 69.990402
Action reg: 0.003965
  l1.weight: grad_norm = 0.125383
  l1.bias: grad_norm = 0.003875
  l2.weight: grad_norm = 0.272757
Total gradient norm: 0.459123
=== Actor Training Debug (Iteration 6096) ===
Q mean: -69.149017
Q std: 27.021725
Actor loss: 69.153000
Action reg: 0.003981
  l1.weight: grad_norm = 0.434418
  l1.bias: grad_norm = 0.001582
  l2.weight: grad_norm = 0.906655
Total gradient norm: 1.732930
=== Actor Training Debug (Iteration 6097) ===
Q mean: -70.918503
Q std: 26.705126
Actor loss: 70.922493
Action reg: 0.003987
  l1.weight: grad_norm = 0.048722
  l1.bias: grad_norm = 0.001312
  l2.weight: grad_norm = 0.114391
Total gradient norm: 0.216315
=== Actor Training Debug (Iteration 6098) ===
Q mean: -70.919708
Q std: 26.271091
Actor loss: 70.923691
Action reg: 0.003980
  l1.weight: grad_norm = 0.492827
  l1.bias: grad_norm = 0.001350
  l2.weight: grad_norm = 1.043895
Total gradient norm: 1.989642
=== Actor Training Debug (Iteration 6099) ===
Q mean: -75.057205
Q std: 24.958628
Actor loss: 75.061195
Action reg: 0.003990
  l1.weight: grad_norm = 0.189622
  l1.bias: grad_norm = 0.000691
  l2.weight: grad_norm = 0.445296
Total gradient norm: 1.031222
=== Actor Training Debug (Iteration 6100) ===
Q mean: -70.693497
Q std: 25.401747
Actor loss: 70.697479
Action reg: 0.003980
  l1.weight: grad_norm = 0.221471
  l1.bias: grad_norm = 0.001296
  l2.weight: grad_norm = 0.534909
Total gradient norm: 1.054021
Episode 111: Steps=100, Reward=-297.688, Buffer_size=11100
=== Actor Training Debug (Iteration 6101) ===
Q mean: -69.638428
Q std: 24.309938
Actor loss: 69.642418
Action reg: 0.003993
  l1.weight: grad_norm = 0.038073
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.072377
Total gradient norm: 0.122581
=== Actor Training Debug (Iteration 6102) ===
Q mean: -68.222984
Q std: 26.155184
Actor loss: 68.226974
Action reg: 0.003988
  l1.weight: grad_norm = 0.051694
  l1.bias: grad_norm = 0.001522
  l2.weight: grad_norm = 0.099228
Total gradient norm: 0.182961
=== Actor Training Debug (Iteration 6103) ===
Q mean: -70.583771
Q std: 26.852600
Actor loss: 70.587746
Action reg: 0.003971
  l1.weight: grad_norm = 0.462607
  l1.bias: grad_norm = 0.003207
  l2.weight: grad_norm = 1.095163
Total gradient norm: 2.197831
=== Actor Training Debug (Iteration 6104) ===
Q mean: -73.606712
Q std: 26.111595
Actor loss: 73.610695
Action reg: 0.003986
  l1.weight: grad_norm = 0.134347
  l1.bias: grad_norm = 0.000572
  l2.weight: grad_norm = 0.282307
Total gradient norm: 0.583348
=== Actor Training Debug (Iteration 6105) ===
Q mean: -70.118645
Q std: 26.621862
Actor loss: 70.122620
Action reg: 0.003977
  l1.weight: grad_norm = 0.189090
  l1.bias: grad_norm = 0.002488
  l2.weight: grad_norm = 0.453718
Total gradient norm: 0.818227
=== Actor Training Debug (Iteration 6106) ===
Q mean: -72.828270
Q std: 26.053911
Actor loss: 72.832253
Action reg: 0.003981
  l1.weight: grad_norm = 0.186986
  l1.bias: grad_norm = 0.002998
  l2.weight: grad_norm = 0.349634
Total gradient norm: 0.679217
=== Actor Training Debug (Iteration 6107) ===
Q mean: -72.058548
Q std: 25.028135
Actor loss: 72.062546
Action reg: 0.003998
  l1.weight: grad_norm = 0.287251
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.624873
Total gradient norm: 1.151392
=== Actor Training Debug (Iteration 6108) ===
Q mean: -74.436630
Q std: 25.645552
Actor loss: 74.440620
Action reg: 0.003989
  l1.weight: grad_norm = 0.218663
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.448451
Total gradient norm: 0.773077
=== Actor Training Debug (Iteration 6109) ===
Q mean: -73.136963
Q std: 25.655300
Actor loss: 73.140945
Action reg: 0.003985
  l1.weight: grad_norm = 0.127267
  l1.bias: grad_norm = 0.001780
  l2.weight: grad_norm = 0.321474
Total gradient norm: 0.557806
=== Actor Training Debug (Iteration 6110) ===
Q mean: -70.478325
Q std: 26.470293
Actor loss: 70.482300
Action reg: 0.003973
  l1.weight: grad_norm = 0.387078
  l1.bias: grad_norm = 0.002673
  l2.weight: grad_norm = 0.884441
Total gradient norm: 1.369223
=== Actor Training Debug (Iteration 6111) ===
Q mean: -69.736969
Q std: 24.971039
Actor loss: 69.740952
Action reg: 0.003979
  l1.weight: grad_norm = 0.198626
  l1.bias: grad_norm = 0.001299
  l2.weight: grad_norm = 0.424823
Total gradient norm: 0.733919
=== Actor Training Debug (Iteration 6112) ===
Q mean: -72.443031
Q std: 26.394903
Actor loss: 72.447006
Action reg: 0.003978
  l1.weight: grad_norm = 0.471324
  l1.bias: grad_norm = 0.001862
  l2.weight: grad_norm = 1.039875
Total gradient norm: 1.978390
=== Actor Training Debug (Iteration 6113) ===
Q mean: -70.232796
Q std: 27.138563
Actor loss: 70.236763
Action reg: 0.003966
  l1.weight: grad_norm = 0.103285
  l1.bias: grad_norm = 0.003739
  l2.weight: grad_norm = 0.254238
Total gradient norm: 0.474613
=== Actor Training Debug (Iteration 6114) ===
Q mean: -72.155800
Q std: 24.945381
Actor loss: 72.159775
Action reg: 0.003979
  l1.weight: grad_norm = 0.448532
  l1.bias: grad_norm = 0.001413
  l2.weight: grad_norm = 0.973394
Total gradient norm: 1.819267
=== Actor Training Debug (Iteration 6115) ===
Q mean: -72.993401
Q std: 24.194508
Actor loss: 72.997391
Action reg: 0.003989
  l1.weight: grad_norm = 0.140722
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.359880
Total gradient norm: 0.640119
=== Actor Training Debug (Iteration 6116) ===
Q mean: -73.901108
Q std: 25.519083
Actor loss: 73.905098
Action reg: 0.003991
  l1.weight: grad_norm = 0.061177
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.152022
Total gradient norm: 0.320257
=== Actor Training Debug (Iteration 6117) ===
Q mean: -70.207748
Q std: 26.280418
Actor loss: 70.211739
Action reg: 0.003992
  l1.weight: grad_norm = 0.054840
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.106551
Total gradient norm: 0.182021
=== Actor Training Debug (Iteration 6118) ===
Q mean: -71.840675
Q std: 26.143705
Actor loss: 71.844666
Action reg: 0.003989
  l1.weight: grad_norm = 0.362878
  l1.bias: grad_norm = 0.000521
  l2.weight: grad_norm = 0.795290
Total gradient norm: 1.603853
=== Actor Training Debug (Iteration 6119) ===
Q mean: -73.141159
Q std: 25.975346
Actor loss: 73.145149
Action reg: 0.003987
  l1.weight: grad_norm = 0.002751
  l1.bias: grad_norm = 0.001533
  l2.weight: grad_norm = 0.013409
Total gradient norm: 0.049163
=== Actor Training Debug (Iteration 6120) ===
Q mean: -71.307587
Q std: 25.034563
Actor loss: 71.311562
Action reg: 0.003976
  l1.weight: grad_norm = 0.289175
  l1.bias: grad_norm = 0.001675
  l2.weight: grad_norm = 0.655800
Total gradient norm: 1.158285
=== Actor Training Debug (Iteration 6121) ===
Q mean: -71.669434
Q std: 25.520197
Actor loss: 71.673424
Action reg: 0.003988
  l1.weight: grad_norm = 0.265987
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.539256
Total gradient norm: 0.926905
=== Actor Training Debug (Iteration 6122) ===
Q mean: -72.816422
Q std: 27.649277
Actor loss: 72.820396
Action reg: 0.003976
  l1.weight: grad_norm = 0.065634
  l1.bias: grad_norm = 0.001931
  l2.weight: grad_norm = 0.141756
Total gradient norm: 0.255773
=== Actor Training Debug (Iteration 6123) ===
Q mean: -72.610931
Q std: 25.796005
Actor loss: 72.614922
Action reg: 0.003987
  l1.weight: grad_norm = 0.170160
  l1.bias: grad_norm = 0.000602
  l2.weight: grad_norm = 0.336906
Total gradient norm: 0.531712
=== Actor Training Debug (Iteration 6124) ===
Q mean: -72.798683
Q std: 27.134171
Actor loss: 72.802666
Action reg: 0.003984
  l1.weight: grad_norm = 0.091776
  l1.bias: grad_norm = 0.001029
  l2.weight: grad_norm = 0.199701
Total gradient norm: 0.334032
=== Actor Training Debug (Iteration 6125) ===
Q mean: -72.664467
Q std: 27.546209
Actor loss: 72.668442
Action reg: 0.003979
  l1.weight: grad_norm = 0.104475
  l1.bias: grad_norm = 0.001908
  l2.weight: grad_norm = 0.218444
Total gradient norm: 0.396404
=== Actor Training Debug (Iteration 6126) ===
Q mean: -69.211212
Q std: 25.883762
Actor loss: 69.215187
Action reg: 0.003976
  l1.weight: grad_norm = 0.117929
  l1.bias: grad_norm = 0.001789
  l2.weight: grad_norm = 0.266780
Total gradient norm: 0.470531
=== Actor Training Debug (Iteration 6127) ===
Q mean: -71.795700
Q std: 25.309095
Actor loss: 71.799690
Action reg: 0.003991
  l1.weight: grad_norm = 0.100441
  l1.bias: grad_norm = 0.000732
  l2.weight: grad_norm = 0.221223
Total gradient norm: 0.433405
=== Actor Training Debug (Iteration 6128) ===
Q mean: -71.714211
Q std: 27.035839
Actor loss: 71.718208
Action reg: 0.003995
  l1.weight: grad_norm = 0.237044
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.476280
Total gradient norm: 0.793432
=== Actor Training Debug (Iteration 6129) ===
Q mean: -71.799164
Q std: 25.235622
Actor loss: 71.803162
Action reg: 0.003995
  l1.weight: grad_norm = 0.212168
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.426765
Total gradient norm: 0.828724
=== Actor Training Debug (Iteration 6130) ===
Q mean: -70.896568
Q std: 25.934303
Actor loss: 70.900551
Action reg: 0.003983
  l1.weight: grad_norm = 0.051013
  l1.bias: grad_norm = 0.001279
  l2.weight: grad_norm = 0.113848
Total gradient norm: 0.220567
=== Actor Training Debug (Iteration 6131) ===
Q mean: -70.884140
Q std: 25.640762
Actor loss: 70.888123
Action reg: 0.003984
  l1.weight: grad_norm = 0.106950
  l1.bias: grad_norm = 0.001114
  l2.weight: grad_norm = 0.242524
Total gradient norm: 0.452294
=== Actor Training Debug (Iteration 6132) ===
Q mean: -73.920807
Q std: 24.819736
Actor loss: 73.924797
Action reg: 0.003991
  l1.weight: grad_norm = 0.049154
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.120007
Total gradient norm: 0.250931
=== Actor Training Debug (Iteration 6133) ===
Q mean: -71.199615
Q std: 25.910139
Actor loss: 71.203606
Action reg: 0.003987
  l1.weight: grad_norm = 0.054142
  l1.bias: grad_norm = 0.001509
  l2.weight: grad_norm = 0.092707
Total gradient norm: 0.144450
=== Actor Training Debug (Iteration 6134) ===
Q mean: -71.118950
Q std: 24.291649
Actor loss: 71.122948
Action reg: 0.003997
  l1.weight: grad_norm = 0.058839
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.136357
Total gradient norm: 0.258567
=== Actor Training Debug (Iteration 6135) ===
Q mean: -71.050049
Q std: 25.905296
Actor loss: 71.054031
Action reg: 0.003985
  l1.weight: grad_norm = 0.027398
  l1.bias: grad_norm = 0.001910
  l2.weight: grad_norm = 0.056851
Total gradient norm: 0.112890
=== Actor Training Debug (Iteration 6136) ===
Q mean: -76.649551
Q std: 26.717186
Actor loss: 76.653534
Action reg: 0.003981
  l1.weight: grad_norm = 0.221520
  l1.bias: grad_norm = 0.001840
  l2.weight: grad_norm = 0.531698
Total gradient norm: 0.940606
=== Actor Training Debug (Iteration 6137) ===
Q mean: -73.598572
Q std: 27.185602
Actor loss: 73.602554
Action reg: 0.003982
  l1.weight: grad_norm = 0.167083
  l1.bias: grad_norm = 0.002093
  l2.weight: grad_norm = 0.374278
Total gradient norm: 0.707100
=== Actor Training Debug (Iteration 6138) ===
Q mean: -71.543518
Q std: 24.922083
Actor loss: 71.547501
Action reg: 0.003979
  l1.weight: grad_norm = 0.522876
  l1.bias: grad_norm = 0.003151
  l2.weight: grad_norm = 1.340273
Total gradient norm: 2.849575
=== Actor Training Debug (Iteration 6139) ===
Q mean: -76.314186
Q std: 25.355560
Actor loss: 76.318176
Action reg: 0.003990
  l1.weight: grad_norm = 0.085402
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.174948
Total gradient norm: 0.379283
=== Actor Training Debug (Iteration 6140) ===
Q mean: -73.369339
Q std: 25.073364
Actor loss: 73.373322
Action reg: 0.003986
  l1.weight: grad_norm = 0.033020
  l1.bias: grad_norm = 0.001345
  l2.weight: grad_norm = 0.068720
Total gradient norm: 0.140955
=== Actor Training Debug (Iteration 6141) ===
Q mean: -74.469795
Q std: 24.686476
Actor loss: 74.473785
Action reg: 0.003993
  l1.weight: grad_norm = 0.188243
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.366373
Total gradient norm: 0.668084
=== Actor Training Debug (Iteration 6142) ===
Q mean: -68.355957
Q std: 25.684267
Actor loss: 68.359940
Action reg: 0.003980
  l1.weight: grad_norm = 0.089702
  l1.bias: grad_norm = 0.000867
  l2.weight: grad_norm = 0.207178
Total gradient norm: 0.369880
=== Actor Training Debug (Iteration 6143) ===
Q mean: -71.839645
Q std: 26.049128
Actor loss: 71.843628
Action reg: 0.003984
  l1.weight: grad_norm = 0.271532
  l1.bias: grad_norm = 0.001213
  l2.weight: grad_norm = 0.602167
Total gradient norm: 1.203972
=== Actor Training Debug (Iteration 6144) ===
Q mean: -73.853821
Q std: 26.740696
Actor loss: 73.857803
Action reg: 0.003979
  l1.weight: grad_norm = 0.194747
  l1.bias: grad_norm = 0.001721
  l2.weight: grad_norm = 0.486911
Total gradient norm: 1.016971
=== Actor Training Debug (Iteration 6145) ===
Q mean: -72.793213
Q std: 26.655830
Actor loss: 72.797188
Action reg: 0.003972
  l1.weight: grad_norm = 1.195743
  l1.bias: grad_norm = 0.003556
  l2.weight: grad_norm = 2.284292
Total gradient norm: 4.127318
=== Actor Training Debug (Iteration 6146) ===
Q mean: -72.913887
Q std: 25.911737
Actor loss: 72.917885
Action reg: 0.003997
  l1.weight: grad_norm = 0.315166
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.776398
Total gradient norm: 1.198005
=== Actor Training Debug (Iteration 6147) ===
Q mean: -69.743027
Q std: 26.058634
Actor loss: 69.747009
Action reg: 0.003985
  l1.weight: grad_norm = 0.163805
  l1.bias: grad_norm = 0.000714
  l2.weight: grad_norm = 0.360163
Total gradient norm: 0.718550
=== Actor Training Debug (Iteration 6148) ===
Q mean: -73.265762
Q std: 25.615837
Actor loss: 73.269745
Action reg: 0.003982
  l1.weight: grad_norm = 0.119498
  l1.bias: grad_norm = 0.001545
  l2.weight: grad_norm = 0.285933
Total gradient norm: 0.474938
=== Actor Training Debug (Iteration 6149) ===
Q mean: -74.027176
Q std: 24.713568
Actor loss: 74.031166
Action reg: 0.003993
  l1.weight: grad_norm = 0.208498
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.510391
Total gradient norm: 1.103735
=== Actor Training Debug (Iteration 6150) ===
Q mean: -68.105179
Q std: 25.339138
Actor loss: 68.109177
Action reg: 0.003994
  l1.weight: grad_norm = 0.050042
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.143622
Total gradient norm: 0.330474
=== Actor Training Debug (Iteration 6151) ===
Q mean: -67.411484
Q std: 26.320681
Actor loss: 67.415466
Action reg: 0.003981
  l1.weight: grad_norm = 0.048833
  l1.bias: grad_norm = 0.001160
  l2.weight: grad_norm = 0.136481
Total gradient norm: 0.312977
=== Actor Training Debug (Iteration 6152) ===
Q mean: -71.465408
Q std: 27.307899
Actor loss: 71.469376
Action reg: 0.003969
  l1.weight: grad_norm = 0.176446
  l1.bias: grad_norm = 0.001127
  l2.weight: grad_norm = 0.441955
Total gradient norm: 0.710405
=== Actor Training Debug (Iteration 6153) ===
Q mean: -75.677444
Q std: 25.742849
Actor loss: 75.681435
Action reg: 0.003989
  l1.weight: grad_norm = 0.170929
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.347625
Total gradient norm: 0.636595
=== Actor Training Debug (Iteration 6154) ===
Q mean: -72.927086
Q std: 27.599970
Actor loss: 72.931061
Action reg: 0.003977
  l1.weight: grad_norm = 0.336077
  l1.bias: grad_norm = 0.001269
  l2.weight: grad_norm = 0.773725
Total gradient norm: 1.294868
=== Actor Training Debug (Iteration 6155) ===
Q mean: -73.039780
Q std: 26.006136
Actor loss: 73.043770
Action reg: 0.003991
  l1.weight: grad_norm = 0.165633
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.367763
Total gradient norm: 0.639186
=== Actor Training Debug (Iteration 6156) ===
Q mean: -70.666451
Q std: 26.414062
Actor loss: 70.670433
Action reg: 0.003979
  l1.weight: grad_norm = 0.091032
  l1.bias: grad_norm = 0.001333
  l2.weight: grad_norm = 0.192527
Total gradient norm: 0.333385
=== Actor Training Debug (Iteration 6157) ===
Q mean: -72.519913
Q std: 25.839611
Actor loss: 72.523903
Action reg: 0.003987
  l1.weight: grad_norm = 0.118833
  l1.bias: grad_norm = 0.001401
  l2.weight: grad_norm = 0.214041
Total gradient norm: 0.383261
=== Actor Training Debug (Iteration 6158) ===
Q mean: -70.504982
Q std: 27.072392
Actor loss: 70.508949
Action reg: 0.003966
  l1.weight: grad_norm = 0.264631
  l1.bias: grad_norm = 0.003235
  l2.weight: grad_norm = 0.390443
Total gradient norm: 0.606521
=== Actor Training Debug (Iteration 6159) ===
Q mean: -68.425247
Q std: 26.921919
Actor loss: 68.429214
Action reg: 0.003969
  l1.weight: grad_norm = 0.389144
  l1.bias: grad_norm = 0.001745
  l2.weight: grad_norm = 0.756434
Total gradient norm: 1.257398
=== Actor Training Debug (Iteration 6160) ===
Q mean: -70.248367
Q std: 25.282869
Actor loss: 70.252357
Action reg: 0.003987
  l1.weight: grad_norm = 0.045469
  l1.bias: grad_norm = 0.002127
  l2.weight: grad_norm = 0.118343
Total gradient norm: 0.233477
=== Actor Training Debug (Iteration 6161) ===
Q mean: -72.205673
Q std: 26.579527
Actor loss: 72.209656
Action reg: 0.003985
  l1.weight: grad_norm = 0.145365
  l1.bias: grad_norm = 0.001189
  l2.weight: grad_norm = 0.324717
Total gradient norm: 0.637745
=== Actor Training Debug (Iteration 6162) ===
Q mean: -70.806610
Q std: 26.919937
Actor loss: 70.810585
Action reg: 0.003976
  l1.weight: grad_norm = 0.127895
  l1.bias: grad_norm = 0.002392
  l2.weight: grad_norm = 0.262077
Total gradient norm: 0.473822
=== Actor Training Debug (Iteration 6163) ===
Q mean: -71.004692
Q std: 25.898260
Actor loss: 71.008675
Action reg: 0.003980
  l1.weight: grad_norm = 0.229673
  l1.bias: grad_norm = 0.001562
  l2.weight: grad_norm = 0.496972
Total gradient norm: 0.871948
=== Actor Training Debug (Iteration 6164) ===
Q mean: -71.052711
Q std: 25.702009
Actor loss: 71.056686
Action reg: 0.003978
  l1.weight: grad_norm = 0.292688
  l1.bias: grad_norm = 0.001517
  l2.weight: grad_norm = 0.707253
Total gradient norm: 1.453945
=== Actor Training Debug (Iteration 6165) ===
Q mean: -73.284531
Q std: 25.798218
Actor loss: 73.288521
Action reg: 0.003990
  l1.weight: grad_norm = 0.203924
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.417271
Total gradient norm: 0.721783
=== Actor Training Debug (Iteration 6166) ===
Q mean: -72.127274
Q std: 24.737696
Actor loss: 72.131264
Action reg: 0.003988
  l1.weight: grad_norm = 0.344582
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.802089
Total gradient norm: 1.419941
=== Actor Training Debug (Iteration 6167) ===
Q mean: -69.926537
Q std: 26.627337
Actor loss: 69.930519
Action reg: 0.003980
  l1.weight: grad_norm = 0.199091
  l1.bias: grad_norm = 0.001842
  l2.weight: grad_norm = 0.513928
Total gradient norm: 1.000252
=== Actor Training Debug (Iteration 6168) ===
Q mean: -73.982796
Q std: 27.538120
Actor loss: 73.986778
Action reg: 0.003983
  l1.weight: grad_norm = 0.074927
  l1.bias: grad_norm = 0.001252
  l2.weight: grad_norm = 0.162492
Total gradient norm: 0.331239
=== Actor Training Debug (Iteration 6169) ===
Q mean: -71.832191
Q std: 25.805681
Actor loss: 71.836174
Action reg: 0.003981
  l1.weight: grad_norm = 0.075503
  l1.bias: grad_norm = 0.001767
  l2.weight: grad_norm = 0.183920
Total gradient norm: 0.421814
=== Actor Training Debug (Iteration 6170) ===
Q mean: -72.279900
Q std: 24.094711
Actor loss: 72.283890
Action reg: 0.003992
  l1.weight: grad_norm = 0.359644
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.847861
Total gradient norm: 1.423388
=== Actor Training Debug (Iteration 6171) ===
Q mean: -74.313858
Q std: 25.563755
Actor loss: 74.317848
Action reg: 0.003989
  l1.weight: grad_norm = 0.031839
  l1.bias: grad_norm = 0.001344
  l2.weight: grad_norm = 0.083328
Total gradient norm: 0.186541
=== Actor Training Debug (Iteration 6172) ===
Q mean: -72.050423
Q std: 26.486956
Actor loss: 72.054413
Action reg: 0.003988
  l1.weight: grad_norm = 0.227871
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.467355
Total gradient norm: 0.826398
=== Actor Training Debug (Iteration 6173) ===
Q mean: -71.751236
Q std: 26.283815
Actor loss: 71.755226
Action reg: 0.003990
  l1.weight: grad_norm = 0.094973
  l1.bias: grad_norm = 0.000677
  l2.weight: grad_norm = 0.182819
Total gradient norm: 0.318760
=== Actor Training Debug (Iteration 6174) ===
Q mean: -70.707420
Q std: 25.803814
Actor loss: 70.711403
Action reg: 0.003983
  l1.weight: grad_norm = 0.348738
  l1.bias: grad_norm = 0.000883
  l2.weight: grad_norm = 0.757394
Total gradient norm: 1.392977
=== Actor Training Debug (Iteration 6175) ===
Q mean: -73.531334
Q std: 25.580727
Actor loss: 73.535324
Action reg: 0.003991
  l1.weight: grad_norm = 0.059097
  l1.bias: grad_norm = 0.000608
  l2.weight: grad_norm = 0.120228
Total gradient norm: 0.219748
=== Actor Training Debug (Iteration 6176) ===
Q mean: -72.431747
Q std: 26.111361
Actor loss: 72.435730
Action reg: 0.003981
  l1.weight: grad_norm = 0.032277
  l1.bias: grad_norm = 0.001441
  l2.weight: grad_norm = 0.076765
Total gradient norm: 0.138257
=== Actor Training Debug (Iteration 6177) ===
Q mean: -72.633255
Q std: 25.326059
Actor loss: 72.637245
Action reg: 0.003992
  l1.weight: grad_norm = 0.045468
  l1.bias: grad_norm = 0.000435
  l2.weight: grad_norm = 0.088958
Total gradient norm: 0.179216
=== Actor Training Debug (Iteration 6178) ===
Q mean: -71.080643
Q std: 26.315956
Actor loss: 71.084625
Action reg: 0.003981
  l1.weight: grad_norm = 0.023168
  l1.bias: grad_norm = 0.001879
  l2.weight: grad_norm = 0.070445
Total gradient norm: 0.166208
=== Actor Training Debug (Iteration 6179) ===
Q mean: -74.849236
Q std: 24.084789
Actor loss: 74.853233
Action reg: 0.003994
  l1.weight: grad_norm = 0.419353
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.879232
Total gradient norm: 1.619184
=== Actor Training Debug (Iteration 6180) ===
Q mean: -71.925644
Q std: 27.122169
Actor loss: 71.929619
Action reg: 0.003974
  l1.weight: grad_norm = 0.269662
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.597764
Total gradient norm: 0.956215
=== Actor Training Debug (Iteration 6181) ===
Q mean: -70.651428
Q std: 26.219027
Actor loss: 70.655418
Action reg: 0.003988
  l1.weight: grad_norm = 0.166624
  l1.bias: grad_norm = 0.000722
  l2.weight: grad_norm = 0.371151
Total gradient norm: 0.679568
=== Actor Training Debug (Iteration 6182) ===
Q mean: -71.247986
Q std: 27.076323
Actor loss: 71.251961
Action reg: 0.003974
  l1.weight: grad_norm = 0.243397
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.526539
Total gradient norm: 0.968513
=== Actor Training Debug (Iteration 6183) ===
Q mean: -72.219482
Q std: 26.098438
Actor loss: 72.223473
Action reg: 0.003989
  l1.weight: grad_norm = 0.187132
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.387455
Total gradient norm: 0.717850
=== Actor Training Debug (Iteration 6184) ===
Q mean: -72.094910
Q std: 25.626976
Actor loss: 72.098892
Action reg: 0.003983
  l1.weight: grad_norm = 0.021094
  l1.bias: grad_norm = 0.001053
  l2.weight: grad_norm = 0.036873
Total gradient norm: 0.067049
=== Actor Training Debug (Iteration 6185) ===
Q mean: -75.420349
Q std: 26.727880
Actor loss: 75.424324
Action reg: 0.003973
  l1.weight: grad_norm = 0.553978
  l1.bias: grad_norm = 0.001066
  l2.weight: grad_norm = 1.403957
Total gradient norm: 2.710400
=== Actor Training Debug (Iteration 6186) ===
Q mean: -71.368515
Q std: 25.659199
Actor loss: 71.372498
Action reg: 0.003985
  l1.weight: grad_norm = 0.036257
  l1.bias: grad_norm = 0.001000
  l2.weight: grad_norm = 0.083232
Total gradient norm: 0.147139
=== Actor Training Debug (Iteration 6187) ===
Q mean: -73.556198
Q std: 27.297159
Actor loss: 73.560181
Action reg: 0.003979
  l1.weight: grad_norm = 0.083478
  l1.bias: grad_norm = 0.001281
  l2.weight: grad_norm = 0.225827
Total gradient norm: 0.443942
=== Actor Training Debug (Iteration 6188) ===
Q mean: -71.765289
Q std: 24.759880
Actor loss: 71.769279
Action reg: 0.003994
Action reg: 0.003988 0.7450064696on 1203) ===
  l1.weight: grad_norm = 0.010103
  l1.bias: grad_norm = 0.001120
  l2.weight: grad_norm = 0.023710
Total gradient norm: 0.041265
=== Actor Training Debug (Iteration 6199) ===
Q mean: -71.787308
Q std: 26.424858
Actor loss: 71.791290
Action reg: 0.003985
  l1.weight: grad_norm = 0.525137
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 1.176627
Total gradient norm: 2.345767
=== Actor Training Debug (Iteration 6200) ===
Q mean: -73.454895
Q std: 27.667538
Actor loss: 73.458878
Action reg: 0.003982
  l1.weight: grad_norm = 0.134096
  l1.bias: grad_norm = 0.001214
  l2.weight: grad_norm = 0.359607
Total gradient norm: 0.637495
=== Actor Training Debug (Iteration 6201) ===
Q mean: -71.616196
Q std: 26.961985
Actor loss: 71.620186
Action reg: 0.003989
  l1.weight: grad_norm = 0.090999
  l1.bias: grad_norm = 0.000971
  l2.weight: grad_norm = 0.182162
Total gradient norm: 0.365063
=== Actor Training Debug (Iteration 6202) ===
Q mean: -74.924973
Q std: 26.504248
Actor loss: 74.928963
Action reg: 0.003989
  l1.weight: grad_norm = 0.239061
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.567468
Total gradient norm: 1.094995
=== Actor Training Debug (Iteration 6203) ===
Q mean: -71.283669
Q std: 24.748602
Actor loss: 71.287651
Action reg: 0.003983
  l1.weight: grad_norm = 0.197829
  l1.bias: grad_norm = 0.001457
  l2.weight: grad_norm = 0.424553
Total gradient norm: 0.629220
=== Actor Training Debug (Iteration 6204) ===
Q mean: -72.566498
Q std: 26.955513
Actor loss: 72.570480
Action reg: 0.003985
  l1.weight: grad_norm = 0.082064
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.179144
Total gradient norm: 0.366328
=== Actor Training Debug (Iteration 6205) ===
Q mean: -72.441025
Q std: 25.011658
Actor loss: 72.445015
Action reg: 0.003987
  l1.weight: grad_norm = 0.048421
  l1.bias: grad_norm = 0.001304
  l2.weight: grad_norm = 0.097549
Total gradient norm: 0.198423
=== Actor Training Debug (Iteration 6206) ===
Q mean: -71.950226
Q std: 27.079975
Actor loss: 71.954208
Action reg: 0.003983
  l1.weight: grad_norm = 0.539941
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 1.112161
Total gradient norm: 2.016036
=== Actor Training Debug (Iteration 6207) ===
Q mean: -72.591125
Q std: 24.936316
Actor loss: 72.595116
Action reg: 0.003988
  l1.weight: grad_norm = 0.104728
  l1.bias: grad_norm = 0.000655
  l2.weight: grad_norm = 0.268260
Total gradient norm: 0.541845
=== Actor Training Debug (Iteration 6208) ===
Q mean: -69.974167
Q std: 26.525429
Actor loss: 69.978149
Action reg: 0.003985
  l1.weight: grad_norm = 0.084171
  l1.bias: grad_norm = 0.001054
  l2.weight: grad_norm = 0.205210
Total gradient norm: 0.425862
=== Actor Training Debug (Iteration 6209) ===
Q mean: -70.027687
Q std: 26.784180
Actor loss: 70.031677
Action reg: 0.003991
  l1.weight: grad_norm = 0.061810
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.131181
Total gradient norm: 0.284703
=== Actor Training Debug (Iteration 6210) ===
Q mean: -73.687035
Q std: 25.937864
Actor loss: 73.691025
Action reg: 0.003988
  l1.weight: grad_norm = 0.078874
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.197186
Total gradient norm: 0.324338
=== Actor Training Debug (Iteration 6211) ===
Q mean: -73.486000
Q std: 25.442787
Actor loss: 73.489990
Action reg: 0.003989
  l1.weight: grad_norm = 0.072420
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.178714
Total gradient norm: 0.398225
=== Actor Training Debug (Iteration 6212) ===
Q mean: -74.029655
Q std: 26.583847
Actor loss: 74.033646
Action reg: 0.003987
  l1.weight: grad_norm = 0.191291
  l1.bias: grad_norm = 0.000625
  l2.weight: grad_norm = 0.397101
Total gradient norm: 0.734999
=== Actor Training Debug (Iteration 6213) ===
Q mean: -75.662132
Q std: 25.259148
Actor loss: 75.666122
Action reg: 0.003992
  l1.weight: grad_norm = 0.043504
  l1.bias: grad_norm = 0.000820
  l2.weight: grad_norm = 0.100651
Total gradient norm: 0.182132
=== Actor Training Debug (Iteration 6214) ===
Q mean: -70.687241
Q std: 27.565872
Actor loss: 70.691223
Action reg: 0.003980
  l1.weight: grad_norm = 0.406773
  l1.bias: grad_norm = 0.001113
  l2.weight: grad_norm = 0.743098
Total gradient norm: 1.299354
=== Actor Training Debug (Iteration 6215) ===
Q mean: -71.389915
Q std: 25.536011
Actor loss: 71.393898
Action reg: 0.003984
  l1.weight: grad_norm = 0.276724
  l1.bias: grad_norm = 0.000801
  l2.weight: grad_norm = 0.696725
Total gradient norm: 1.248084
=== Actor Training Debug (Iteration 6216) ===
Q mean: -71.196396
Q std: 27.204952
Actor loss: 71.200356
Action reg: 0.003959
  l1.weight: grad_norm = 0.313705
  l1.bias: grad_norm = 0.001124
  l2.weight: grad_norm = 0.698958
Total gradient norm: 1.230316
=== Actor Training Debug (Iteration 6217) ===
Q mean: -69.284615
Q std: 25.824337
Actor loss: 69.288589
Action reg: 0.003977
  l1.weight: grad_norm = 0.288154
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.582973
Total gradient norm: 0.972155
=== Actor Training Debug (Iteration 6218) ===
Q mean: -74.146233
Q std: 27.461893
Actor loss: 74.150208
Action reg: 0.003972
  l1.weight: grad_norm = 0.221378
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.394561
Total gradient norm: 0.592765
=== Actor Training Debug (Iteration 6219) ===
Q mean: -72.986511
Q std: 26.657810
Actor loss: 72.990494
Action reg: 0.003979
  l1.weight: grad_norm = 0.158213
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.330624
Total gradient norm: 0.556015
=== Actor Training Debug (Iteration 6220) ===
Q mean: -73.314384
Q std: 26.323421
Actor loss: 73.318382
Action reg: 0.003997
  l1.weight: grad_norm = 0.069422
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.146329
Total gradient norm: 0.243124
=== Actor Training Debug (Iteration 6221) ===
Q mean: -71.038101
Q std: 26.895784
Actor loss: 71.042076
Action reg: 0.003978
  l1.weight: grad_norm = 0.173272
  l1.bias: grad_norm = 0.000776
  l2.weight: grad_norm = 0.379734
Total gradient norm: 0.694301
=== Actor Training Debug (Iteration 6222) ===
Q mean: -70.645828
Q std: 26.649488
Actor loss: 70.649811
Action reg: 0.003985
  l1.weight: grad_norm = 0.095743
  l1.bias: grad_norm = 0.000560
  l2.weight: grad_norm = 0.234751
Total gradient norm: 0.439918
=== Actor Training Debug (Iteration 6223) ===
Q mean: -76.101105
Q std: 24.589386
Actor loss: 76.105095
Action reg: 0.003988
  l1.weight: grad_norm = 0.039222
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.078017
Total gradient norm: 0.147483
=== Actor Training Debug (Iteration 6224) ===
Q mean: -74.620338
Q std: 25.928089
Actor loss: 74.624329
Action reg: 0.003993
  l1.weight: grad_norm = 0.023240
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.050558
Total gradient norm: 0.091652
=== Actor Training Debug (Iteration 6225) ===
Q mean: -73.274643
Q std: 26.351276
Actor loss: 73.278625
Action reg: 0.003984
  l1.weight: grad_norm = 0.210825
  l1.bias: grad_norm = 0.000571
  l2.weight: grad_norm = 0.356039
Total gradient norm: 0.549804
=== Actor Training Debug (Iteration 6226) ===
Q mean: -73.072876
Q std: 26.690363
Actor loss: 73.076866
Action reg: 0.003986
  l1.weight: grad_norm = 0.108880
  l1.bias: grad_norm = 0.000637
  l2.weight: grad_norm = 0.255549
Total gradient norm: 0.514693
=== Actor Training Debug (Iteration 6227) ===
Q mean: -72.613899
Q std: 26.476223
Actor loss: 72.617882
Action reg: 0.003985
  l1.weight: grad_norm = 0.493987
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 1.213483
Total gradient norm: 2.288564
=== Actor Training Debug (Iteration 6228) ===
Q mean: -73.353668
Q std: 28.931889
Actor loss: 73.357651
Action reg: 0.003982
  l1.weight: grad_norm = 0.071083
  l1.bias: grad_norm = 0.000816
  l2.weight: grad_norm = 0.173891
Total gradient norm: 0.336440
=== Actor Training Debug (Iteration 6229) ===
Q mean: -71.995636
Q std: 26.725651
Actor loss: 71.999611
Action reg: 0.003978
  l1.weight: grad_norm = 0.227539
  l1.bias: grad_norm = 0.000953
  l2.weight: grad_norm = 0.528811
Total gradient norm: 0.906210
=== Actor Training Debug (Iteration 6230) ===
Q mean: -72.897644
Q std: 26.836266
Actor loss: 72.901611
Action reg: 0.003969
  l1.weight: grad_norm = 1.243898
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 3.256695
Total gradient norm: 5.916573
=== Actor Training Debug (Iteration 6231) ===
Q mean: -74.152878
Q std: 25.089520
Actor loss: 74.156853
Action reg: 0.003978
  l1.weight: grad_norm = 0.387487
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.899800
Total gradient norm: 1.783430
=== Actor Training Debug (Iteration 6232) ===
Q mean: -74.040337
Q std: 25.222277
Actor loss: 74.044319
Action reg: 0.003986
  l1.weight: grad_norm = 0.236252
  l1.bias: grad_norm = 0.000599
  l2.weight: grad_norm = 0.623072
Total gradient norm: 1.172168
=== Actor Training Debug (Iteration 6233) ===
Q mean: -68.407120
Q std: 27.849667
Actor loss: 68.411087
Action reg: 0.003971
  l1.weight: grad_norm = 0.109775
  l1.bias: grad_norm = 0.001276
  l2.weight: grad_norm = 0.228995
Total gradient norm: 0.458742
=== Actor Training Debug (Iteration 6234) ===
Q mean: -70.875854
Q std: 26.468321
Actor loss: 70.879837
Action reg: 0.003984
  l1.weight: grad_norm = 0.112282
  l1.bias: grad_norm = 0.000738
  l2.weight: grad_norm = 0.284763
Total gradient norm: 0.474187
=== Actor Training Debug (Iteration 6235) ===
Q mean: -71.278183
Q std: 25.259789
Actor loss: 71.282158
Action reg: 0.003975
  l1.weight: grad_norm = 0.279755
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.562392
Total gradient norm: 0.931420
=== Actor Training Debug (Iteration 6236) ===
Q mean: -72.727188
Q std: 27.009834
Actor loss: 72.731178
Action reg: 0.003987
  l1.weight: grad_norm = 0.216215
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.537060
Total gradient norm: 0.973166
=== Actor Training Debug (Iteration 6237) ===
Q mean: -72.924171
Q std: 25.711708
Actor loss: 72.928162
Action reg: 0.003988
  l1.weight: grad_norm = 0.453936
  l1.bias: grad_norm = 0.000481
  l2.weight: grad_norm = 0.866757
Total gradient norm: 1.530807
=== Actor Training Debug (Iteration 6238) ===
Q mean: -71.331024
Q std: 27.222536
Actor loss: 71.335007
Action reg: 0.003980
  l1.weight: grad_norm = 0.061665
  l1.bias: grad_norm = 0.000772
  l2.weight: grad_norm = 0.125282
Total gradient norm: 0.228844
=== Actor Training Debug (Iteration 6239) ===
Q mean: -71.179520
Q std: 26.747137
Actor loss: 71.183510
Action reg: 0.003987
  l1.weight: grad_norm = 0.331294
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.655714
Total gradient norm: 1.233197
=== Actor Training Debug (Iteration 6240) ===
Q mean: -71.203796
Q std: 26.462561
Actor loss: 71.207787
Action reg: 0.003993
  l1.weight: grad_norm = 0.092266
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.207621
Total gradient norm: 0.394289
=== Actor Training Debug (Iteration 6241) ===
Q mean: -74.064995
Q std: 25.758059
Actor loss: 74.068977
Action reg: 0.003982
  l1.weight: grad_norm = 0.050888
  l1.bias: grad_norm = 0.000627
  l2.weight: grad_norm = 0.114475
Total gradient norm: 0.202227
=== Actor Training Debug (Iteration 6242) ===
Q mean: -74.277130
Q std: 25.416239
Actor loss: 74.281120
Action reg: 0.003991
  l1.weight: grad_norm = 0.188423
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.378662
Total gradient norm: 0.738199
=== Actor Training Debug (Iteration 6243) ===
Q mean: -74.188721
Q std: 26.173889
Actor loss: 74.192711
Action reg: 0.003988
  l1.weight: grad_norm = 0.023098
  l1.bias: grad_norm = 0.000905
  l2.weight: grad_norm = 0.047096
Total gradient norm: 0.084016
=== Actor Training Debug (Iteration 6244) ===
Q mean: -69.744141
Q std: 27.220022
Actor loss: 69.748108
Action reg: 0.003970
  l1.weight: grad_norm = 0.152512
  l1.bias: grad_norm = 0.002060
  l2.weight: grad_norm = 0.396026
Total gradient norm: 0.710720
=== Actor Training Debug (Iteration 6245) ===
Q mean: -73.892174
Q std: 26.573469
Actor loss: 73.896156
Action reg: 0.003983
  l1.weight: grad_norm = 0.326739
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.726785
Total gradient norm: 1.457505
=== Actor Training Debug (Iteration 6246) ===
Q mean: -73.182968
Q std: 25.846439
Actor loss: 73.186951
Action reg: 0.003985
  l1.weight: grad_norm = 0.262376
  l1.bias: grad_norm = 0.000652
  l2.weight: grad_norm = 0.596076
Total gradient norm: 1.072284
=== Actor Training Debug (Iteration 6247) ===
Q mean: -69.159981
Q std: 26.748617
Actor loss: 69.163971
Action reg: 0.003987
  l1.weight: grad_norm = 0.117744
  l1.bias: grad_norm = 0.000690
  l2.weight: grad_norm = 0.227319
Total gradient norm: 0.443454
=== Actor Training Debug (Iteration 6248) ===
Q mean: -73.585632
Q std: 25.979591
Actor loss: 73.589615
Action reg: 0.003983
  l1.weight: grad_norm = 0.340630
  l1.bias: grad_norm = 0.000692
  l2.weight: grad_norm = 0.691146
Total gradient norm: 1.398758
=== Actor Training Debug (Iteration 6249) ===
Q mean: -70.337212
Q std: 26.984524
Actor loss: 70.341179
Action reg: 0.003971
  l1.weight: grad_norm = 0.639853
  l1.bias: grad_norm = 0.002120
  l2.weight: grad_norm = 1.264234
Total gradient norm: 2.504698
=== Actor Training Debug (Iteration 6250) ===
Q mean: -73.700729
Q std: 26.449631
Actor loss: 73.704720
Action reg: 0.003988
  l1.weight: grad_norm = 0.202854
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.453301
Total gradient norm: 0.830900
=== Actor Training Debug (Iteration 6251) ===
Q mean: -76.482651
Q std: 25.191826
Actor loss: 76.486633
Action reg: 0.003983
  l1.weight: grad_norm = 0.415773
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 1.147463
Total gradient norm: 2.124668
=== Actor Training Debug (Iteration 6252) ===
Q mean: -71.991661
Q std: 25.756750
Actor loss: 71.995644
Action reg: 0.003984
  l1.weight: grad_norm = 0.046106
  l1.bias: grad_norm = 0.000656
  l2.weight: grad_norm = 0.117300
Total gradient norm: 0.249001
=== Actor Training Debug (Iteration 6253) ===
Q mean: -72.777641
Q std: 26.789589
Actor loss: 72.781631
Action reg: 0.003986
  l1.weight: grad_norm = 0.664538
  l1.bias: grad_norm = 0.000783
  l2.weight: grad_norm = 1.431686
Total gradient norm: 2.983461
=== Actor Training Debug (Iteration 6254) ===
Q mean: -72.360168
Q std: 26.521042
Actor loss: 72.364151
Action reg: 0.003985
  l1.weight: grad_norm = 0.504737
  l1.bias: grad_norm = 0.000679
  l2.weight: grad_norm = 1.165417
Total gradient norm: 1.760673
=== Actor Training Debug (Iteration 6255) ===
Q mean: -73.467773
Q std: 26.806196
Actor loss: 73.471756
Action reg: 0.003984
  l1.weight: grad_norm = 0.034885
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.081026
Total gradient norm: 0.165588
=== Actor Training Debug (Iteration 6256) ===
Q mean: -73.133598
Q std: 25.792063
Actor loss: 73.137589
Action reg: 0.003993
  l1.weight: grad_norm = 0.158337
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.322754
Total gradient norm: 0.543911
=== Actor Training Debug (Iteration 6257) ===
Q mean: -72.133041
Q std: 26.899782
Actor loss: 72.137024
Action reg: 0.003986
  l1.weight: grad_norm = 0.149736
  l1.bias: grad_norm = 0.000863
  l2.weight: grad_norm = 0.341763
Total gradient norm: 0.541206
=== Actor Training Debug (Iteration 6258) ===
Q mean: -73.115250
Q std: 26.080948
Actor loss: 73.119232
Action reg: 0.003984
  l1.weight: grad_norm = 0.174179
  l1.bias: grad_norm = 0.000706
  l2.weight: grad_norm = 0.373010
Total gradient norm: 0.710749
=== Actor Training Debug (Iteration 6259) ===
Q mean: -76.086647
Q std: 25.922998
Actor loss: 76.090637
Action reg: 0.003992
  l1.weight: grad_norm = 0.157142
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.366729
Total gradient norm: 0.601552
=== Actor Training Debug (Iteration 6260) ===
Q mean: -72.128159
Q std: 25.791281
Actor loss: 72.132149
Action reg: 0.003994
  l1.weight: grad_norm = 0.036503
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.075644
Total gradient norm: 0.134650
=== Actor Training Debug (Iteration 6261) ===
Q mean: -69.718193
Q std: 27.151758
Actor loss: 69.722176
Action reg: 0.003980
  l1.weight: grad_norm = 0.119120
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.258646
Total gradient norm: 0.479247
=== Actor Training Debug (Iteration 6262) ===
Q mean: -73.628410
Q std: 26.085661
Actor loss: 73.632401
Action reg: 0.003988
  l1.weight: grad_norm = 0.312001
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.741559
Total gradient norm: 1.453202
=== Actor Training Debug (Iteration 6263) ===
Q mean: -75.834030
Q std: 27.636292
Actor loss: 75.838020
Action reg: 0.003988
  l1.weight: grad_norm = 0.095743
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.204596
Total gradient norm: 0.323212
=== Actor Training Debug (Iteration 6264) ===
Q mean: -73.421570
Q std: 26.971769
Actor loss: 73.425552
Action reg: 0.003985
  l1.weight: grad_norm = 0.104301
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.242234
Total gradient norm: 0.527815
=== Actor Training Debug (Iteration 6265) ===
Q mean: -69.367996
Q std: 25.074404
Actor loss: 69.371979
Action reg: 0.003986
  l1.weight: grad_norm = 0.053198
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.135286
Total gradient norm: 0.273144
=== Actor Training Debug (Iteration 6266) ===
Q mean: -68.947144
Q std: 27.303503
Actor loss: 68.951126
Action reg: 0.003980
  l1.weight: grad_norm = 0.125466
  l1.bias: grad_norm = 0.000736
  l2.weight: grad_norm = 0.302090
Total gradient norm: 0.595144
=== Actor Training Debug (Iteration 6267) ===
Q mean: -71.092407
Q std: 26.119572
Actor loss: 71.096397
Action reg: 0.003987
  l1.weight: grad_norm = 0.141831
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.286962
Total gradient norm: 0.498056
=== Actor Training Debug (Iteration 6268) ===
Q mean: -71.706375
Q std: 26.932814
Actor loss: 71.710358
Action reg: 0.003983
  l1.weight: grad_norm = 0.094257
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.214290
Total gradient norm: 0.408344
=== Actor Training Debug (Iteration 6269) ===
Q mean: -71.130333
Q std: 26.396240
Actor loss: 71.134308
Action reg: 0.003976
  l1.weight: grad_norm = 0.897592
  l1.bias: grad_norm = 0.000941
  l2.weight: grad_norm = 2.218450
Total gradient norm: 4.258960
=== Actor Training Debug (Iteration 6270) ===
Q mean: -71.255859
Q std: 24.596567
Actor loss: 71.259857
Action reg: 0.003997
  l1.weight: grad_norm = 0.054345
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.139185
Total gradient norm: 0.319481
=== Actor Training Debug (Iteration 6271) ===
Q mean: -73.233772
Q std: 27.049517
Actor loss: 73.237762
Action reg: 0.003987
  l1.weight: grad_norm = 0.186740
  l1.bias: grad_norm = 0.000803
  l2.weight: grad_norm = 0.344903
Total gradient norm: 0.644759
=== Actor Training Debug (Iteration 6272) ===
Q mean: -72.569229
Q std: 27.130566
Actor loss: 72.573212
Action reg: 0.003979
  l1.weight: grad_norm = 0.140049
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.341178
Total gradient norm: 0.598043
=== Actor Training Debug (Iteration 6273) ===
Q mean: -71.557190
Q std: 26.821899
Actor loss: 71.561180
Action reg: 0.003990
  l1.weight: grad_norm = 0.020517
  l1.bias: grad_norm = 0.000691
  l2.weight: grad_norm = 0.047576
Total gradient norm: 0.085781
=== Actor Training Debug (Iteration 6274) ===
Q mean: -71.624374
Q std: 26.299780
Actor loss: 71.628357
Action reg: 0.003982
  l1.weight: grad_norm = 0.114553
  l1.bias: grad_norm = 0.000634
  l2.weight: grad_norm = 0.295476
Total gradient norm: 0.471589
=== Actor Training Debug (Iteration 6275) ===
Q mean: -71.507866
Q std: 26.403465
Actor loss: 71.511848
Action reg: 0.003982
  l1.weight: grad_norm = 0.176620
  l1.bias: grad_norm = 0.000820
  l2.weight: grad_norm = 0.357107
Total gradient norm: 0.629048
=== Actor Training Debug (Iteration 6276) ===
Q mean: -74.050354
Q std: 27.240993
Actor loss: 74.054337
Action reg: 0.003984
  l1.weight: grad_norm = 0.560413
  l1.bias: grad_norm = 0.000927
  l2.weight: grad_norm = 1.188581
Total gradient norm: 2.104758
=== Actor Training Debug (Iteration 6277) ===
Q mean: -71.571884
Q std: 27.408281
Actor loss: 71.575859
Action reg: 0.003971
  l1.weight: grad_norm = 0.113224
  l1.bias: grad_norm = 0.001404
  l2.weight: grad_norm = 0.256488
Total gradient norm: 0.560361
=== Actor Training Debug (Iteration 6278) ===
Q mean: -72.913925
Q std: 27.136580
Actor loss: 72.917908
Action reg: 0.003982
  l1.weight: grad_norm = 0.280982
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.562861
Total gradient norm: 1.125431
=== Actor Training Debug (Iteration 6279) ===
Q mean: -71.286011
Q std: 27.219744
Actor loss: 71.289993
Action reg: 0.003983
  l1.weight: grad_norm = 0.070528
  l1.bias: grad_norm = 0.001100
  l2.weight: grad_norm = 0.154627
Total gradient norm: 0.264228
=== Actor Training Debug (Iteration 6280) ===
Q mean: -69.851898
Q std: 28.418722
Actor loss: 69.855873
Action reg: 0.003973
  l1.weight: grad_norm = 0.269732
  l1.bias: grad_norm = 0.001172
  l2.weight: grad_norm = 0.572146
Total gradient norm: 1.041047
=== Actor Training Debug (Iteration 6281) ===
Q mean: -71.039795
Q std: 26.718863
Actor loss: 71.043777
Action reg: 0.003982
  l1.weight: grad_norm = 0.049265
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.104365
Total gradient norm: 0.192382
=== Actor Training Debug (Iteration 6282) ===
Q mean: -71.202324
Q std: 25.724195
Actor loss: 71.206314
Action reg: 0.003987
  l1.weight: grad_norm = 0.185004
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.377145
Total gradient norm: 0.655800
=== Actor Training Debug (Iteration 6283) ===
Q mean: -76.374931
Q std: 25.129601
Actor loss: 76.378914
Action reg: 0.003982
  l1.weight: grad_norm = 0.185138
  l1.bias: grad_norm = 0.001196
  l2.weight: grad_norm = 0.405528
Total gradient norm: 0.793038
=== Actor Training Debug (Iteration 6284) ===
Q mean: -74.828735
Q std: 25.571245
Actor loss: 74.832733
Action reg: 0.003998
  l1.weight: grad_norm = 0.046814
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.125919
Total gradient norm: 0.249575
=== Actor Training Debug (Iteration 6285) ===
Q mean: -71.209488
Q std: 25.156565
Actor loss: 71.213486
Action reg: 0.003994
  l1.weight: grad_norm = 0.040890
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.096587
Total gradient norm: 0.173082
=== Actor Training Debug (Iteration 6286) ===
Q mean: -74.924026
Q std: 25.885218
Actor loss: 74.928009
Action reg: 0.003982
  l1.weight: grad_norm = 0.160642
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.379588
Total gradient norm: 0.726441
=== Actor Training Debug (Iteration 6287) ===
Q mean: -74.567123
Q std: 27.617952
Actor loss: 74.571098
Action reg: 0.003973
  l1.weight: grad_norm = 1.132510
  l1.bias: grad_norm = 0.001854
  l2.weight: grad_norm = 2.481578
Total gradient norm: 4.185727
=== Actor Training Debug (Iteration 6288) ===
Q mean: -73.538864
Q std: 25.349863
Actor loss: 73.542854
Action reg: 0.003991
  l1.weight: grad_norm = 0.025512
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.067083
Total gradient norm: 0.135157
=== Actor Training Debug (Iteration 6289) ===
Q mean: -72.911743
Q std: 25.232464
Actor loss: 72.915733
Action reg: 0.003989
  l1.weight: grad_norm = 0.138804
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.297707
Total gradient norm: 0.523115
=== Actor Training Debug (Iteration 6290) ===
Q mean: -71.539902
Q std: 26.887373
Actor loss: 71.543892
Action reg: 0.003991
  l1.weight: grad_norm = 0.038082
  l1.bias: grad_norm = 0.000550
  l2.weight: grad_norm = 0.068471
Total gradient norm: 0.121202
=== Actor Training Debug (Iteration 6291) ===
Q mean: -72.872696
Q std: 27.676172
Actor loss: 72.876671
Action reg: 0.003975
  l1.weight: grad_norm = 0.019330
  l1.bias: grad_norm = 0.001558
  l2.weight: grad_norm = 0.050242
Total gradient norm: 0.132487
=== Actor Training Debug (Iteration 6292) ===
Q mean: -72.861374
Q std: 26.997784
Actor loss: 72.865356
Action reg: 0.003981
  l1.weight: grad_norm = 0.112607
  l1.bias: grad_norm = 0.000689
  l2.weight: grad_norm = 0.216964
Total gradient norm: 0.344475
=== Actor Training Debug (Iteration 6293) ===
Q mean: -72.358871
Q std: 25.334480
Actor loss: 72.362862
Action reg: 0.003990
  l1.weight: grad_norm = 0.105739
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.219445
Total gradient norm: 0.406085
=== Actor Training Debug (Iteration 6294) ===
Q mean: -71.669128
Q std: 28.081663
Actor loss: 71.673119
Action reg: 0.003987
  l1.weight: grad_norm = 0.090803
  l1.bias: grad_norm = 0.000548
  l2.weight: grad_norm = 0.252334
Total gradient norm: 0.509516
=== Actor Training Debug (Iteration 6295) ===
Q mean: -72.305122
Q std: 27.283304
Actor loss: 72.309105
Action reg: 0.003982
  l1.weight: grad_norm = 0.135133
  l1.bias: grad_norm = 0.000897
  l2.weight: grad_norm = 0.300365
Total gradient norm: 0.531624
=== Actor Training Debug (Iteration 6296) ===
Q mean: -72.805763
Q std: 26.192482
Actor loss: 72.809746
Action reg: 0.003980
  l1.weight: grad_norm = 0.452449
  l1.bias: grad_norm = 0.000754
  l2.weight: grad_norm = 0.973815
Total gradient norm: 1.719083
=== Actor Training Debug (Iteration 6297) ===
Q mean: -74.943596
Q std: 26.012812
Actor loss: 74.947578
Action reg: 0.003983
  l1.weight: grad_norm = 0.217271
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.523179
Total gradient norm: 1.110518
=== Actor Training Debug (Iteration 6298) ===
Q mean: -77.155853
Q std: 24.457354
Actor loss: 77.159843
Action reg: 0.003990
  l1.weight: grad_norm = 0.098765
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.226908
Total gradient norm: 0.432293
=== Actor Training Debug (Iteration 6299) ===
Q mean: -70.362061
Q std: 25.757498
Actor loss: 70.366051
Action reg: 0.003993
  l1.weight: grad_norm = 0.415725
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.873212
Total gradient norm: 1.695153
=== Actor Training Debug (Iteration 6300) ===
Q mean: -74.187408
Q std: 26.721140
Actor loss: 74.191399
Action reg: 0.003993
  l1.weight: grad_norm = 0.168714
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.409997
Total gradient norm: 0.820984
=== Actor Training Debug (Iteration 6301) ===
Q mean: -72.213898
Q std: 26.896523
Actor loss: 72.217880
Action reg: 0.003980
  l1.weight: grad_norm = 0.055571
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.136882
Total gradient norm: 0.273315
=== Actor Training Debug (Iteration 6302) ===
Q mean: -72.322098
Q std: 25.948744
Actor loss: 72.326088
Action reg: 0.003991
  l1.weight: grad_norm = 0.222776
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.511763
Total gradient norm: 0.942181
=== Actor Training Debug (Iteration 6303) ===
Q mean: -75.180992
Q std: 25.389233
Actor loss: 75.184990
Action reg: 0.003995
  l1.weight: grad_norm = 0.169730
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.337990
Total gradient norm: 0.693560
=== Actor Training Debug (Iteration 6304) ===
Q mean: -73.149933
Q std: 26.739155
Actor loss: 73.153915
Action reg: 0.003982
  l1.weight: grad_norm = 0.275330
  l1.bias: grad_norm = 0.000832
  l2.weight: grad_norm = 0.748453
Total gradient norm: 1.620552
=== Actor Training Debug (Iteration 6305) ===
Q mean: -72.354362
Q std: 26.720209
Actor loss: 72.358353
Action reg: 0.003991
  l1.weight: grad_norm = 0.155176
  l1.bias: grad_norm = 0.000521
  l2.weight: grad_norm = 0.383002
Total gradient norm: 0.634153
=== Actor Training Debug (Iteration 6306) ===
Q mean: -72.043076
Q std: 27.591497
Actor loss: 72.047066
Action reg: 0.003987
  l1.weight: grad_norm = 0.091213
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.206766
Total gradient norm: 0.383559
=== Actor Training Debug (Iteration 6307) ===
Q mean: -71.733093
Q std: 26.205988
Actor loss: 71.737076
Action reg: 0.003984
  l1.weight: grad_norm = 0.110807
  l1.bias: grad_norm = 0.001080
  l2.weight: grad_norm = 0.237014
Total gradient norm: 0.414008
=== Actor Training Debug (Iteration 6308) ===
Q mean: -74.798401
Q std: 25.079617
Actor loss: 74.802391
Action reg: 0.003988
  l1.weight: grad_norm = 0.117003
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.225532
Total gradient norm: 0.491860
=== Actor Training Debug (Iteration 6309) ===
Q mean: -73.295845
Q std: 24.487432
Actor loss: 73.299828
Action reg: 0.003984
  l1.weight: grad_norm = 0.015966
  l1.bias: grad_norm = 0.001285
  l2.weight: grad_norm = 0.034531
Total gradient norm: 0.071352
=== Actor Training Debug (Iteration 6310) ===
Q mean: -73.741646
Q std: 24.511562
Actor loss: 73.745628
Action reg: 0.003980
  l1.weight: grad_norm = 0.222349
  l1.bias: grad_norm = 0.000881
  l2.weight: grad_norm = 0.598792
Total gradient norm: 1.142491
=== Actor Training Debug (Iteration 6311) ===
Q mean: -74.814598
Q std: 27.098011
Actor loss: 74.818573
Action reg: 0.003978
  l1.weight: grad_norm = 0.232842
  l1.bias: grad_norm = 0.001418
  l2.weight: grad_norm = 0.495004
Total gradient norm: 0.984748
=== Actor Training Debug (Iteration 6312) ===
Q mean: -75.885178
Q std: 26.281160
Actor loss: 75.889168
Action reg: 0.003994
  l1.weight: grad_norm = 0.294434
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.658115
Total gradient norm: 1.136061
=== Actor Training Debug (Iteration 6313) ===
Q mean: -75.436577
Q std: 27.169437
Actor loss: 75.440575
Action reg: 0.003994
  l1.weight: grad_norm = 0.024589
  l1.bias: grad_norm = 0.000562
  l2.weight: grad_norm = 0.066290
Total gradient norm: 0.122293
=== Actor Training Debug (Iteration 6314) ===
Q mean: -72.149643
Q std: 26.398256
Actor loss: 72.153625
Action reg: 0.003979
  l1.weight: grad_norm = 0.344145
  l1.bias: grad_norm = 0.000923
  l2.weight: grad_norm = 0.989041
Total gradient norm: 2.088874
=== Actor Training Debug (Iteration 6315) ===
Q mean: -75.501633
Q std: 25.894739
Actor loss: 75.505615
Action reg: 0.003985
  l1.weight: grad_norm = 0.228612
  l1.bias: grad_norm = 0.000948
  l2.weight: grad_norm = 0.491929
Total gradient norm: 1.105449
=== Actor Training Debug (Iteration 6316) ===
Q mean: -71.485283
Q std: 26.088528
Actor loss: 71.489273
Action reg: 0.003992
  l1.weight: grad_norm = 0.065046
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.127323
Total gradient norm: 0.201022
=== Actor Training Debug (Iteration 6317) ===
Q mean: -73.403717
Q std: 26.550858
Actor loss: 73.407707
Action reg: 0.003988
  l1.weight: grad_norm = 0.143988
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.346274
Total gradient norm: 0.610945
=== Actor Training Debug (Iteration 6318) ===
Q mean: -72.402283
Q std: 26.160116
Actor loss: 72.406273
Action reg: 0.003993
  l1.weight: grad_norm = 0.033970
  l1.bias: grad_norm = 0.000637
  l2.weight: grad_norm = 0.084201
Total gradient norm: 0.164945
=== Actor Training Debug (Iteration 6319) ===
Q mean: -72.376495
Q std: 26.034107
Actor loss: 72.380486
Action reg: 0.003991
  l1.weight: grad_norm = 0.173554
  l1.bias: grad_norm = 0.001105
  l2.weight: grad_norm = 0.370197
Total gradient norm: 0.648674
=== Actor Training Debug (Iteration 6320) ===
Q mean: -73.233932
Q std: 26.106405
Actor loss: 73.237923
Action reg: 0.003992
  l1.weight: grad_norm = 0.185038
  l1.bias: grad_norm = 0.000461
  l2.weight: grad_norm = 0.394346
Total gradient norm: 0.773676
=== Actor Training Debug (Iteration 6321) ===
Q mean: -75.541687
Q std: 26.721104
Actor loss: 75.545662
Action reg: 0.003975
  l1.weight: grad_norm = 0.295697
  l1.bias: grad_norm = 0.001553
  l2.weight: grad_norm = 0.572966
Total gradient norm: 1.170741
=== Actor Training Debug (Iteration 6322) ===
Q mean: -74.584480
Q std: 26.129042
Actor loss: 74.588470
Action reg: 0.003992
  l1.weight: grad_norm = 0.535459
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 1.291289
Total gradient norm: 2.235847
=== Actor Training Debug (Iteration 6323) ===
Q mean: -72.414520
Q std: 27.119894
Actor loss: 72.418503
Action reg: 0.003985
  l1.weight: grad_norm = 0.307272
  l1.bias: grad_norm = 0.000736
  l2.weight: grad_norm = 0.634920
Total gradient norm: 1.203865
=== Actor Training Debug (Iteration 6324) ===
Q mean: -71.357590
Q std: 26.424702
Actor loss: 71.361572
Action reg: 0.003986
  l1.weight: grad_norm = 0.144825
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.273059
Total gradient norm: 0.494035
=== Actor Training Debug (Iteration 6325) ===
Q mean: -76.406769
Q std: 26.925833
Actor loss: 76.410751
Action reg: 0.003979
  l1.weight: grad_norm = 0.250794
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.563610
Total gradient norm: 1.096098
=== Actor Training Debug (Iteration 6326) ===
Q mean: -73.317856
Q std: 27.458059
Actor loss: 73.321846
Action reg: 0.003988
  l1.weight: grad_norm = 0.176055
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.427736
Total gradient norm: 1.057008
=== Actor Training Debug (Iteration 6327) ===
Q mean: -72.609665
Q std: 26.970440
Actor loss: 72.613655
Action reg: 0.003990
  l1.weight: grad_norm = 0.008492
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.020449
Total gradient norm: 0.040245
=== Actor Training Debug (Iteration 6328) ===
Q mean: -73.689240
Q std: 26.811649
Actor loss: 73.693222
Action reg: 0.003983
  l1.weight: grad_norm = 0.203015
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.540548
Total gradient norm: 1.078234
=== Actor Training Debug (Iteration 6329) ===
Q mean: -76.657089
Q std: 24.844368
Actor loss: 76.661087
Action reg: 0.003996
  l1.weight: grad_norm = 0.074258
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.171898
Total gradient norm: 0.357399
=== Actor Training Debug (Iteration 6330) ===
Q mean: -71.262100
Q std: 26.707226
Actor loss: 71.266068
Action reg: 0.003969
  l1.weight: grad_norm = 0.982283
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 2.267889
Total gradient norm: 4.997699
=== Actor Training Debug (Iteration 6331) ===
Q mean: -72.016365
Q std: 26.130476
Actor loss: 72.020355
Action reg: 0.003990
  l1.weight: grad_norm = 0.207595
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.574531
Total gradient norm: 1.522182
=== Actor Training Debug (Iteration 6332) ===
Q mean: -72.076996
Q std: 26.409313
Actor loss: 72.080986
Action reg: 0.003987
  l1.weight: grad_norm = 0.025260
  l1.bias: grad_norm = 0.000425
  l2.weight: grad_norm = 0.052989
Total gradient norm: 0.080784
=== Actor Training Debug (Iteration 6333) ===
Q mean: -69.380142
Q std: 26.265453
Actor loss: 69.384125
Action reg: 0.003984
  l1.weight: grad_norm = 0.068458
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.185855
Total gradient norm: 0.424065
=== Actor Training Debug (Iteration 6334) ===
Q mean: -73.220184
Q std: 27.501652
Actor loss: 73.224167
Action reg: 0.003984
  l1.weight: grad_norm = 0.237896
  l1.bias: grad_norm = 0.000616
  l2.weight: grad_norm = 0.488063
Total gradient norm: 0.881499
=== Actor Training Debug (Iteration 6335) ===
Q mean: -72.881699
Q std: 27.736181
Actor loss: 72.885674
Action reg: 0.003978
  l1.weight: grad_norm = 0.170344
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.444545
Total gradient norm: 1.013763
=== Actor Training Debug (Iteration 6336) ===
Q mean: -76.024246
Q std: 25.713387
Actor loss: 76.028236
Action reg: 0.003993
  l1.weight: grad_norm = 0.118556
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.263211
Total gradient norm: 0.603201
=== Actor Training Debug (Iteration 6337) ===
Q mean: -72.149719
Q std: 26.580019
Actor loss: 72.153709
Action reg: 0.003989
  l1.weight: grad_norm = 0.089648
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.202877
Total gradient norm: 0.289665
Total gradient norm: 0.7297274696on 1203) ===
=== Actor Training Debug (Iteration 6348) ===
Q mean: -73.998795
Q std: 27.552471
Actor loss: 74.002777
Action reg: 0.003980
  l1.weight: grad_norm = 0.156968
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.320544
Total gradient norm: 0.540197
=== Actor Training Debug (Iteration 6349) ===
Q mean: -73.770905
Q std: 27.030981
Actor loss: 73.774879
Action reg: 0.003978
  l1.weight: grad_norm = 0.061979
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.134248
Total gradient norm: 0.268078
=== Actor Training Debug (Iteration 6350) ===
Q mean: -71.439400
Q std: 26.191051
Actor loss: 71.443390
Action reg: 0.003991
  l1.weight: grad_norm = 0.071082
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.163482
Total gradient norm: 0.317357
=== Actor Training Debug (Iteration 6351) ===
Q mean: -72.896278
Q std: 27.204269
Actor loss: 72.900269
Action reg: 0.003991
  l1.weight: grad_norm = 0.085444
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.228792
Total gradient norm: 0.465849
=== Actor Training Debug (Iteration 6352) ===
Q mean: -70.856178
Q std: 26.438667
Actor loss: 70.860168
Action reg: 0.003990
  l1.weight: grad_norm = 0.051030
  l1.bias: grad_norm = 0.000652
  l2.weight: grad_norm = 0.109838
Total gradient norm: 0.182578
=== Actor Training Debug (Iteration 6353) ===
Q mean: -73.556366
Q std: 26.033693
Actor loss: 73.560364
Action reg: 0.003995
  l1.weight: grad_norm = 0.179152
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.412579
Total gradient norm: 0.834869
=== Actor Training Debug (Iteration 6354) ===
Q mean: -70.108002
Q std: 27.071642
Actor loss: 70.111977
Action reg: 0.003974
  l1.weight: grad_norm = 0.374719
  l1.bias: grad_norm = 0.000771
  l2.weight: grad_norm = 0.871297
Total gradient norm: 1.688812
=== Actor Training Debug (Iteration 6355) ===
Q mean: -71.210526
Q std: 27.214958
Actor loss: 71.214508
Action reg: 0.003986
  l1.weight: grad_norm = 0.213729
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.513178
Total gradient norm: 1.109884
=== Actor Training Debug (Iteration 6356) ===
Q mean: -73.040062
Q std: 25.456049
Actor loss: 73.044044
Action reg: 0.003984
  l1.weight: grad_norm = 0.162077
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.364418
Total gradient norm: 0.707511
=== Actor Training Debug (Iteration 6357) ===
Q mean: -73.704102
Q std: 26.634317
Actor loss: 73.708084
Action reg: 0.003984
  l1.weight: grad_norm = 0.175200
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.400434
Total gradient norm: 0.858863
=== Actor Training Debug (Iteration 6358) ===
Q mean: -74.179024
Q std: 27.871395
Actor loss: 74.183014
Action reg: 0.003989
  l1.weight: grad_norm = 0.253545
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.577342
Total gradient norm: 0.805465
=== Actor Training Debug (Iteration 6359) ===
Q mean: -70.261063
Q std: 26.112581
Actor loss: 70.265053
Action reg: 0.003991
  l1.weight: grad_norm = 0.104924
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.229233
Total gradient norm: 0.394233
=== Actor Training Debug (Iteration 6360) ===
Q mean: -75.459717
Q std: 26.946970
Actor loss: 75.463699
Action reg: 0.003986
  l1.weight: grad_norm = 0.215173
  l1.bias: grad_norm = 0.000509
  l2.weight: grad_norm = 0.456691
Total gradient norm: 0.793116
=== Actor Training Debug (Iteration 6361) ===
Q mean: -73.325165
Q std: 26.536467
Actor loss: 73.329155
Action reg: 0.003989
  l1.weight: grad_norm = 0.020992
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.052088
Total gradient norm: 0.104834
=== Actor Training Debug (Iteration 6362) ===
Q mean: -73.322540
Q std: 27.321810
Actor loss: 73.326530
Action reg: 0.003989
  l1.weight: grad_norm = 0.365736
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.857805
Total gradient norm: 1.400283
=== Actor Training Debug (Iteration 6363) ===
Q mean: -74.661057
Q std: 26.193434
Actor loss: 74.665039
Action reg: 0.003980
  l1.weight: grad_norm = 0.094566
  l1.bias: grad_norm = 0.000903
  l2.weight: grad_norm = 0.254534
Total gradient norm: 0.599713
=== Actor Training Debug (Iteration 6364) ===
Q mean: -71.139130
Q std: 27.677567
Actor loss: 71.143105
Action reg: 0.003978
  l1.weight: grad_norm = 0.116530
  l1.bias: grad_norm = 0.000756
  l2.weight: grad_norm = 0.232591
Total gradient norm: 0.380869
=== Actor Training Debug (Iteration 6365) ===
Q mean: -76.058205
Q std: 28.091549
Actor loss: 76.062187
Action reg: 0.003981
  l1.weight: grad_norm = 0.347198
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.898235
Total gradient norm: 1.837992
=== Actor Training Debug (Iteration 6366) ===
Q mean: -69.214500
Q std: 25.824148
Actor loss: 69.218483
Action reg: 0.003980
  l1.weight: grad_norm = 0.096925
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.211896
Total gradient norm: 0.362221
=== Actor Training Debug (Iteration 6367) ===
Q mean: -73.248901
Q std: 26.438829
Actor loss: 73.252884
Action reg: 0.003979
  l1.weight: grad_norm = 0.115194
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.260034
Total gradient norm: 0.462321
=== Actor Training Debug (Iteration 6368) ===
Q mean: -74.316498
Q std: 26.584538
Actor loss: 74.320488
Action reg: 0.003988
  l1.weight: grad_norm = 0.039528
  l1.bias: grad_norm = 0.000478
  l2.weight: grad_norm = 0.084716
Total gradient norm: 0.170297
=== Actor Training Debug (Iteration 6369) ===
Q mean: -73.589592
Q std: 26.695118
Actor loss: 73.593575
Action reg: 0.003986
  l1.weight: grad_norm = 0.029733
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.071727
Total gradient norm: 0.153374
=== Actor Training Debug (Iteration 6370) ===
Q mean: -72.694229
Q std: 24.819082
Actor loss: 72.698219
Action reg: 0.003989
  l1.weight: grad_norm = 0.115420
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.257840
Total gradient norm: 0.571946
=== Actor Training Debug (Iteration 6371) ===
Q mean: -69.377403
Q std: 26.433767
Actor loss: 69.381393
Action reg: 0.003987
  l1.weight: grad_norm = 0.055521
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.133451
Total gradient norm: 0.246770
=== Actor Training Debug (Iteration 6372) ===
Q mean: -75.123306
Q std: 26.824427
Actor loss: 75.127281
Action reg: 0.003978
  l1.weight: grad_norm = 0.486412
  l1.bias: grad_norm = 0.000481
  l2.weight: grad_norm = 1.148865
Total gradient norm: 2.300732
=== Actor Training Debug (Iteration 6373) ===
Q mean: -72.619408
Q std: 28.270639
Actor loss: 72.623383
Action reg: 0.003972
  l1.weight: grad_norm = 0.188114
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.384326
Total gradient norm: 0.790728
=== Actor Training Debug (Iteration 6374) ===
Q mean: -70.220779
Q std: 27.111790
Actor loss: 70.224770
Action reg: 0.003987
  l1.weight: grad_norm = 0.384696
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.840424
Total gradient norm: 1.562175
=== Actor Training Debug (Iteration 6375) ===
Q mean: -70.839050
Q std: 27.588799
Actor loss: 70.843025
Action reg: 0.003975
  l1.weight: grad_norm = 0.370403
  l1.bias: grad_norm = 0.000759
  l2.weight: grad_norm = 0.860968
Total gradient norm: 1.823462
=== Actor Training Debug (Iteration 6376) ===
Q mean: -71.519737
Q std: 27.749037
Actor loss: 71.523727
Action reg: 0.003987
  l1.weight: grad_norm = 0.327332
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.744597
Total gradient norm: 1.369101
=== Actor Training Debug (Iteration 6377) ===
Q mean: -73.130539
Q std: 26.384377
Actor loss: 73.134529
Action reg: 0.003988
  l1.weight: grad_norm = 0.047624
  l1.bias: grad_norm = 0.000816
  l2.weight: grad_norm = 0.115613
Total gradient norm: 0.226251
=== Actor Training Debug (Iteration 6378) ===
Q mean: -73.060272
Q std: 26.841524
Actor loss: 73.064262
Action reg: 0.003988
  l1.weight: grad_norm = 0.325652
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.625336
Total gradient norm: 1.235598
=== Actor Training Debug (Iteration 6379) ===
Q mean: -73.393677
Q std: 25.085377
Actor loss: 73.397667
Action reg: 0.003991
  l1.weight: grad_norm = 0.164521
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.401776
Total gradient norm: 0.793125
=== Actor Training Debug (Iteration 6380) ===
Q mean: -73.755112
Q std: 29.011967
Actor loss: 73.759087
Action reg: 0.003976
  l1.weight: grad_norm = 0.123430
  l1.bias: grad_norm = 0.001082
  l2.weight: grad_norm = 0.257749
Total gradient norm: 0.420010
=== Actor Training Debug (Iteration 6381) ===
Q mean: -71.786392
Q std: 27.690643
Actor loss: 71.790375
Action reg: 0.003985
  l1.weight: grad_norm = 0.048685
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.122629
Total gradient norm: 0.279990
=== Actor Training Debug (Iteration 6382) ===
Q mean: -70.206062
Q std: 27.513123
Actor loss: 70.210052
Action reg: 0.003989
  l1.weight: grad_norm = 0.188417
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.434602
Total gradient norm: 0.817061
=== Actor Training Debug (Iteration 6383) ===
Q mean: -69.386337
Q std: 25.864445
Actor loss: 69.390320
Action reg: 0.003985
  l1.weight: grad_norm = 0.101265
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.213213
Total gradient norm: 0.380895
=== Actor Training Debug (Iteration 6384) ===
Q mean: -75.460602
Q std: 25.041660
Actor loss: 75.464592
Action reg: 0.003989
  l1.weight: grad_norm = 0.424912
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.874159
Total gradient norm: 1.658545
=== Actor Training Debug (Iteration 6385) ===
Q mean: -78.326828
Q std: 24.912386
Actor loss: 78.330818
Action reg: 0.003994
  l1.weight: grad_norm = 0.136430
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.320601
Total gradient norm: 0.673134
=== Actor Training Debug (Iteration 6386) ===
Q mean: -76.759697
Q std: 25.190308
Actor loss: 76.763687
Action reg: 0.003991
  l1.weight: grad_norm = 0.226419
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.547535
Total gradient norm: 1.068919
=== Actor Training Debug (Iteration 6387) ===
Q mean: -71.734985
Q std: 27.769262
Actor loss: 71.738976
Action reg: 0.003987
  l1.weight: grad_norm = 0.119815
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.214505
Total gradient norm: 0.367734
=== Actor Training Debug (Iteration 6388) ===
Q mean: -71.627060
Q std: 27.198038
Actor loss: 71.631050
Action reg: 0.003992
  l1.weight: grad_norm = 0.227971
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.486884
Total gradient norm: 0.746536
=== Actor Training Debug (Iteration 6389) ===
Q mean: -72.261673
Q std: 27.084091
Actor loss: 72.265656
Action reg: 0.003981
  l1.weight: grad_norm = 0.414482
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.964788
Total gradient norm: 1.846499
=== Actor Training Debug (Iteration 6390) ===
Q mean: -73.493271
Q std: 28.492168
Actor loss: 73.497253
Action reg: 0.003985
  l1.weight: grad_norm = 0.063811
  l1.bias: grad_norm = 0.000770
  l2.weight: grad_norm = 0.147958
Total gradient norm: 0.297743
=== Actor Training Debug (Iteration 6391) ===
Q mean: -72.364700
Q std: 26.713037
Actor loss: 72.368690
Action reg: 0.003991
  l1.weight: grad_norm = 0.110445
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.190716
Total gradient norm: 0.346853
=== Actor Training Debug (Iteration 6392) ===
Q mean: -71.048042
Q std: 27.510378
Actor loss: 71.052025
Action reg: 0.003982
  l1.weight: grad_norm = 0.078495
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.134154
Total gradient norm: 0.196394
=== Actor Training Debug (Iteration 6393) ===
Q mean: -72.730476
Q std: 27.248520
Actor loss: 72.734467
Action reg: 0.003989
  l1.weight: grad_norm = 0.084604
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.191994
Total gradient norm: 0.337469
=== Actor Training Debug (Iteration 6394) ===
Q mean: -75.600143
Q std: 25.686140
Actor loss: 75.604134
Action reg: 0.003992
  l1.weight: grad_norm = 0.334370
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.687825
Total gradient norm: 1.412558
=== Actor Training Debug (Iteration 6395) ===
Q mean: -72.497871
Q std: 26.520496
Actor loss: 72.501862
Action reg: 0.003992
  l1.weight: grad_norm = 0.459444
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 1.235407
Total gradient norm: 2.571952
=== Actor Training Debug (Iteration 6396) ===
Q mean: -71.342331
Q std: 26.582079
Actor loss: 71.346321
Action reg: 0.003993
  l1.weight: grad_norm = 0.394761
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.826021
Total gradient norm: 1.487332
=== Actor Training Debug (Iteration 6397) ===
Q mean: -75.689972
Q std: 28.106409
Actor loss: 75.693962
Action reg: 0.003988
  l1.weight: grad_norm = 0.164364
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.339843
Total gradient norm: 0.520626
=== Actor Training Debug (Iteration 6398) ===
Q mean: -74.196449
Q std: 28.026258
Actor loss: 74.200439
Action reg: 0.003988
  l1.weight: grad_norm = 0.065357
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.154324
Total gradient norm: 0.270127
=== Actor Training Debug (Iteration 6399) ===
Q mean: -71.500000
Q std: 26.637945
Actor loss: 71.503990
Action reg: 0.003991
  l1.weight: grad_norm = 0.184766
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.407894
Total gradient norm: 0.810468
=== Actor Training Debug (Iteration 6400) ===
Q mean: -69.694153
Q std: 26.034504
Actor loss: 69.698143
Action reg: 0.003991
  l1.weight: grad_norm = 0.237217
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.491665
Total gradient norm: 0.836844
=== Actor Training Debug (Iteration 6401) ===
Q mean: -69.700989
Q std: 27.206964
Actor loss: 69.704971
Action reg: 0.003986
  l1.weight: grad_norm = 0.150852
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.294768
Total gradient norm: 0.603689
=== Actor Training Debug (Iteration 6402) ===
Q mean: -71.841263
Q std: 25.572058
Actor loss: 71.845245
Action reg: 0.003985
  l1.weight: grad_norm = 0.174018
  l1.bias: grad_norm = 0.000638
  l2.weight: grad_norm = 0.391849
Total gradient norm: 0.793151
=== Actor Training Debug (Iteration 6403) ===
Q mean: -73.617798
Q std: 26.222364
Actor loss: 73.621780
Action reg: 0.003985
  l1.weight: grad_norm = 0.223861
  l1.bias: grad_norm = 0.000944
  l2.weight: grad_norm = 0.523721
Total gradient norm: 0.939996
=== Actor Training Debug (Iteration 6404) ===
Q mean: -75.247665
Q std: 25.525974
Actor loss: 75.251656
Action reg: 0.003991
  l1.weight: grad_norm = 0.139396
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.236332
Total gradient norm: 0.464057
=== Actor Training Debug (Iteration 6405) ===
Q mean: -70.923569
Q std: 26.180321
Actor loss: 70.927559
Action reg: 0.003988
  l1.weight: grad_norm = 0.145137
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.290790
Total gradient norm: 0.497458
=== Actor Training Debug (Iteration 6406) ===
Q mean: -68.441330
Q std: 27.903643
Actor loss: 68.445320
Action reg: 0.003988
  l1.weight: grad_norm = 0.117953
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.249884
Total gradient norm: 0.466990
=== Actor Training Debug (Iteration 6407) ===
Q mean: -71.726456
Q std: 26.429306
Actor loss: 71.730446
Action reg: 0.003990
  l1.weight: grad_norm = 0.434131
  l1.bias: grad_norm = 0.000633
  l2.weight: grad_norm = 0.986029
Total gradient norm: 2.129703
=== Actor Training Debug (Iteration 6408) ===
Q mean: -75.468575
Q std: 26.382227
Actor loss: 75.472557
Action reg: 0.003986
  l1.weight: grad_norm = 0.045850
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.109525
Total gradient norm: 0.243494
=== Actor Training Debug (Iteration 6409) ===
Q mean: -70.631096
Q std: 26.791996
Actor loss: 70.635086
Action reg: 0.003993
  l1.weight: grad_norm = 0.225725
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.443922
Total gradient norm: 0.902002
=== Actor Training Debug (Iteration 6410) ===
Q mean: -73.263969
Q std: 28.007021
Actor loss: 73.267952
Action reg: 0.003980
  l1.weight: grad_norm = 0.118771
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.257952
Total gradient norm: 0.469777
=== Actor Training Debug (Iteration 6411) ===
Q mean: -74.085709
Q std: 27.799587
Actor loss: 74.089699
Action reg: 0.003988
  l1.weight: grad_norm = 0.106904
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.258323
Total gradient norm: 0.494953
=== Actor Training Debug (Iteration 6412) ===
Q mean: -72.656448
Q std: 28.676342
Actor loss: 72.660439
Action reg: 0.003991
  l1.weight: grad_norm = 0.036036
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.082402
Total gradient norm: 0.164219
=== Actor Training Debug (Iteration 6413) ===
Q mean: -71.676598
Q std: 27.157801
Actor loss: 71.680588
Action reg: 0.003986
  l1.weight: grad_norm = 0.188915
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.399596
Total gradient norm: 0.869588
=== Actor Training Debug (Iteration 6414) ===
Q mean: -74.269951
Q std: 26.592628
Actor loss: 74.273941
Action reg: 0.003990
  l1.weight: grad_norm = 0.157183
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.326988
Total gradient norm: 0.648188
=== Actor Training Debug (Iteration 6415) ===
Q mean: -69.519043
Q std: 26.179913
Actor loss: 69.523033
Action reg: 0.003992
  l1.weight: grad_norm = 0.178907
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.320485
Total gradient norm: 0.563162
=== Actor Training Debug (Iteration 6416) ===
Q mean: -70.612076
Q std: 26.932205
Actor loss: 70.616074
Action reg: 0.003994
  l1.weight: grad_norm = 0.005339
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.011269
Total gradient norm: 0.021760
=== Actor Training Debug (Iteration 6417) ===
Q mean: -71.109741
Q std: 27.041174
Actor loss: 71.113731
Action reg: 0.003992
  l1.weight: grad_norm = 0.016980
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.031801
Total gradient norm: 0.066751
=== Actor Training Debug (Iteration 6418) ===
Q mean: -74.311005
Q std: 26.357998
Actor loss: 74.315002
Action reg: 0.003996
  l1.weight: grad_norm = 0.060851
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.128741
Total gradient norm: 0.248065
=== Actor Training Debug (Iteration 6419) ===
Q mean: -74.776329
Q std: 28.047188
Actor loss: 74.780312
Action reg: 0.003982
  l1.weight: grad_norm = 0.179633
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.397482
Total gradient norm: 0.609644
=== Actor Training Debug (Iteration 6420) ===
Q mean: -73.973701
Q std: 25.425137
Actor loss: 73.977692
Action reg: 0.003991
  l1.weight: grad_norm = 0.240695
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.659373
Total gradient norm: 1.119321
=== Actor Training Debug (Iteration 6421) ===
Q mean: -73.907272
Q std: 27.202682
Actor loss: 73.911255
Action reg: 0.003984
  l1.weight: grad_norm = 0.065719
  l1.bias: grad_norm = 0.000478
  l2.weight: grad_norm = 0.163384
Total gradient norm: 0.322414
=== Actor Training Debug (Iteration 6422) ===
Q mean: -71.600845
Q std: 26.188334
Actor loss: 71.604828
Action reg: 0.003985
  l1.weight: grad_norm = 0.427265
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.884379
Total gradient norm: 1.505419
=== Actor Training Debug (Iteration 6423) ===
Q mean: -72.198509
Q std: 28.058857
Actor loss: 72.202499
Action reg: 0.003992
  l1.weight: grad_norm = 0.152700
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.318401
Total gradient norm: 0.564008
=== Actor Training Debug (Iteration 6424) ===
Q mean: -73.014114
Q std: 27.744112
Actor loss: 73.018105
Action reg: 0.003987
  l1.weight: grad_norm = 0.058141
  l1.bias: grad_norm = 0.000794
  l2.weight: grad_norm = 0.153678
Total gradient norm: 0.340705
=== Actor Training Debug (Iteration 6425) ===
Q mean: -72.318199
Q std: 27.127888
Actor loss: 72.322182
Action reg: 0.003983
  l1.weight: grad_norm = 0.267318
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.659887
Total gradient norm: 1.174215
=== Actor Training Debug (Iteration 6426) ===
Q mean: -74.026466
Q std: 26.729679
Actor loss: 74.030449
Action reg: 0.003980
  l1.weight: grad_norm = 0.214277
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.456231
Total gradient norm: 0.920904
=== Actor Training Debug (Iteration 6427) ===
Q mean: -72.337502
Q std: 27.695288
Actor loss: 72.341484
Action reg: 0.003983
  l1.weight: grad_norm = 0.195256
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.464679
Total gradient norm: 0.811487
=== Actor Training Debug (Iteration 6428) ===
Q mean: -71.700653
Q std: 26.292494
Actor loss: 71.704636
Action reg: 0.003982
  l1.weight: grad_norm = 0.455184
  l1.bias: grad_norm = 0.000617
  l2.weight: grad_norm = 0.953703
Total gradient norm: 1.766269
=== Actor Training Debug (Iteration 6429) ===
Q mean: -77.939972
Q std: 25.995180
Actor loss: 77.943970
Action reg: 0.003995
  l1.weight: grad_norm = 0.076789
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.138613
Total gradient norm: 0.248894
=== Actor Training Debug (Iteration 6430) ===
Q mean: -75.419449
Q std: 26.493528
Actor loss: 75.423439
Action reg: 0.003993
  l1.weight: grad_norm = 0.257892
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.577230
Total gradient norm: 1.079436
=== Actor Training Debug (Iteration 6431) ===
Q mean: -70.045364
Q std: 28.265087
Actor loss: 70.049339
Action reg: 0.003978
  l1.weight: grad_norm = 0.433469
  l1.bias: grad_norm = 0.000760
  l2.weight: grad_norm = 0.881009
Total gradient norm: 1.768052
=== Actor Training Debug (Iteration 6432) ===
Q mean: -71.874771
Q std: 26.310518
Actor loss: 71.878761
Action reg: 0.003989
  l1.weight: grad_norm = 0.276383
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.665653
Total gradient norm: 1.366367
=== Actor Training Debug (Iteration 6433) ===
Q mean: -74.608246
Q std: 26.330816
Actor loss: 74.612183
Action reg: 0.003937
  l1.weight: grad_norm = 0.387423
  l1.bias: grad_norm = 0.000559
  l2.weight: grad_norm = 0.846056
Total gradient norm: 1.589417
=== Actor Training Debug (Iteration 6434) ===
Q mean: -73.661079
Q std: 27.718235
Actor loss: 73.665031
Action reg: 0.003952
  l1.weight: grad_norm = 0.130033
  l1.bias: grad_norm = 0.000768
  l2.weight: grad_norm = 0.260987
Total gradient norm: 0.476939
=== Actor Training Debug (Iteration 6435) ===
Q mean: -74.762192
Q std: 27.370380
Actor loss: 74.766167
Action reg: 0.003975
  l1.weight: grad_norm = 0.291480
  l1.bias: grad_norm = 0.000774
  l2.weight: grad_norm = 0.642208
Total gradient norm: 1.103159
=== Actor Training Debug (Iteration 6436) ===
Q mean: -73.639153
Q std: 26.644444
Actor loss: 73.643143
Action reg: 0.003991
  l1.weight: grad_norm = 0.053352
  l1.bias: grad_norm = 0.000661
  l2.weight: grad_norm = 0.110979
Total gradient norm: 0.204835
=== Actor Training Debug (Iteration 6437) ===
Q mean: -73.742905
Q std: 26.184320
Actor loss: 73.746895
Action reg: 0.003988
  l1.weight: grad_norm = 0.128491
  l1.bias: grad_norm = 0.000620
  l2.weight: grad_norm = 0.280588
Total gradient norm: 0.561937
=== Actor Training Debug (Iteration 6438) ===
Q mean: -72.165291
Q std: 26.114340
Actor loss: 72.169289
Action reg: 0.003997
  l1.weight: grad_norm = 0.099601
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.227248
Total gradient norm: 0.415085
=== Actor Training Debug (Iteration 6439) ===
Q mean: -71.660477
Q std: 26.292208
Actor loss: 71.664474
Action reg: 0.003995
  l1.weight: grad_norm = 0.078299
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.172075
Total gradient norm: 0.338908
=== Actor Training Debug (Iteration 6440) ===
Q mean: -72.495087
Q std: 27.705496
Actor loss: 72.499077
Action reg: 0.003992
  l1.weight: grad_norm = 0.095514
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.191829
Total gradient norm: 0.385765
=== Actor Training Debug (Iteration 6441) ===
Q mean: -74.388817
Q std: 28.403048
Actor loss: 74.392807
Action reg: 0.003991
  l1.weight: grad_norm = 0.205733
  l1.bias: grad_norm = 0.000689
  l2.weight: grad_norm = 0.425426
Total gradient norm: 0.807197
=== Actor Training Debug (Iteration 6442) ===
Q mean: -73.614204
Q std: 27.797747
Actor loss: 73.618187
Action reg: 0.003979
  l1.weight: grad_norm = 0.171244
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.403651
Total gradient norm: 0.846100
=== Actor Training Debug (Iteration 6443) ===
Q mean: -71.602951
Q std: 26.485325
Actor loss: 71.606941
Action reg: 0.003991
  l1.weight: grad_norm = 0.047743
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.129992
Total gradient norm: 0.280105
=== Actor Training Debug (Iteration 6444) ===
Q mean: -72.998940
Q std: 27.189566
Actor loss: 73.002930
Action reg: 0.003991
  l1.weight: grad_norm = 0.272971
  l1.bias: grad_norm = 0.000569
  l2.weight: grad_norm = 0.537837
Total gradient norm: 1.035059
=== Actor Training Debug (Iteration 6445) ===
Q mean: -74.532257
Q std: 25.431421
Actor loss: 74.536247
Action reg: 0.003988
  l1.weight: grad_norm = 0.615748
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 1.353882
Total gradient norm: 2.953234
=== Actor Training Debug (Iteration 6446) ===
Q mean: -76.469826
Q std: 26.097330
Actor loss: 76.473816
Action reg: 0.003991
  l1.weight: grad_norm = 0.173514
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.469846
Total gradient norm: 0.891640
=== Actor Training Debug (Iteration 6447) ===
Q mean: -71.473785
Q std: 27.472183
Actor loss: 71.477776
Action reg: 0.003987
  l1.weight: grad_norm = 0.036053
  l1.bias: grad_norm = 0.000552
  l2.weight: grad_norm = 0.086017
Total gradient norm: 0.194796
=== Actor Training Debug (Iteration 6448) ===
Q mean: -68.631226
Q std: 27.277039
Actor loss: 68.635208
Action reg: 0.003983
  l1.weight: grad_norm = 0.096324
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.171545
Total gradient norm: 0.321506
=== Actor Training Debug (Iteration 6449) ===
Q mean: -72.366333
Q std: 26.010975
Actor loss: 72.370323
Action reg: 0.003988
  l1.weight: grad_norm = 0.132573
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.280178
Total gradient norm: 0.565298
=== Actor Training Debug (Iteration 6450) ===
Q mean: -71.934631
Q std: 27.265850
Actor loss: 71.938622
Action reg: 0.003991
  l1.weight: grad_norm = 0.046661
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.105081
Total gradient norm: 0.189898
=== Actor Training Debug (Iteration 6451) ===
Q mean: -72.684860
Q std: 27.280666
Actor loss: 72.688850
Action reg: 0.003989
  l1.weight: grad_norm = 0.163969
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.319500
Total gradient norm: 0.521416
=== Actor Training Debug (Iteration 6452) ===
Q mean: -70.076958
Q std: 27.062534
Actor loss: 70.080948
Action reg: 0.003991
  l1.weight: grad_norm = 0.119742
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.282708
Total gradient norm: 0.535911
=== Actor Training Debug (Iteration 6453) ===
Q mean: -73.808861
Q std: 27.925314
Actor loss: 73.812843
Action reg: 0.003983
  l1.weight: grad_norm = 0.205866
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.479138
Total gradient norm: 1.045236
=== Actor Training Debug (Iteration 6454) ===
Q mean: -75.825485
Q std: 28.181019
Actor loss: 75.829475
Action reg: 0.003988
  l1.weight: grad_norm = 0.123867
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.271085
Total gradient norm: 0.500006
=== Actor Training Debug (Iteration 6455) ===
Q mean: -71.842987
Q std: 24.940506
Actor loss: 71.846970
Action reg: 0.003984
  l1.weight: grad_norm = 0.245547
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.525163
Total gradient norm: 0.995598
=== Actor Training Debug (Iteration 6456) ===
Q mean: -71.202866
Q std: 27.251453
Actor loss: 71.206848
Action reg: 0.003985
  l1.weight: grad_norm = 0.235844
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.528022
Total gradient norm: 0.926117
=== Actor Training Debug (Iteration 6457) ===
Q mean: -74.864922
Q std: 27.492094
Actor loss: 74.868912
Action reg: 0.003993
  l1.weight: grad_norm = 0.276340
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.707772
Total gradient norm: 1.294610
=== Actor Training Debug (Iteration 6458) ===
Q mean: -75.991295
Q std: 27.087526
Actor loss: 75.995285
Action reg: 0.003993
  l1.weight: grad_norm = 0.028167
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.059911
Total gradient norm: 0.114547
=== Actor Training Debug (Iteration 6459) ===
Q mean: -72.899826
Q std: 27.361332
Actor loss: 72.903816
Action reg: 0.003987
  l1.weight: grad_norm = 0.098300
  l1.bias: grad_norm = 0.000577
  l2.weight: grad_norm = 0.224112
Total gradient norm: 0.417189
=== Actor Training Debug (Iteration 6460) ===
Q mean: -73.491318
Q std: 27.559772
Actor loss: 73.495300
Action reg: 0.003979
  l1.weight: grad_norm = 0.166193
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.361651
Total gradient norm: 0.721220
=== Actor Training Debug (Iteration 6461) ===
Q mean: -71.189384
Q std: 28.018501
Actor loss: 71.193375
Action reg: 0.003987
  l1.weight: grad_norm = 0.179247
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.395411
Total gradient norm: 0.753803
=== Actor Training Debug (Iteration 6462) ===
Q mean: -75.355499
Q std: 26.897160
Actor loss: 75.359474
Action reg: 0.003978
  l1.weight: grad_norm = 0.199010
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.490815
Total gradient norm: 1.031553
=== Actor Training Debug (Iteration 6463) ===
Q mean: -73.878815
Q std: 29.113182
Actor loss: 73.882782
Action reg: 0.003970
  l1.weight: grad_norm = 0.117690
  l1.bias: grad_norm = 0.000862
  l2.weight: grad_norm = 0.259492
Total gradient norm: 0.471197
=== Actor Training Debug (Iteration 6464) ===
Q mean: -74.014801
Q std: 28.796326
Actor loss: 74.018776
Action reg: 0.003977
  l1.weight: grad_norm = 0.212896
  l1.bias: grad_norm = 0.000858
  l2.weight: grad_norm = 0.462708
Total gradient norm: 0.744338
=== Actor Training Debug (Iteration 6465) ===
Q mean: -73.269852
Q std: 26.084307
Actor loss: 73.273834
Action reg: 0.003984
  l1.weight: grad_norm = 0.625894
  l1.bias: grad_norm = 0.000706
  l2.weight: grad_norm = 1.260898
Total gradient norm: 2.322260
=== Actor Training Debug (Iteration 6466) ===
Q mean: -73.514847
Q std: 25.272985
Actor loss: 73.518837
Action reg: 0.003994
  l1.weight: grad_norm = 0.191380
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.412605
Total gradient norm: 0.900008
=== Actor Training Debug (Iteration 6467) ===
Q mean: -72.225922
Q std: 25.902924
Actor loss: 72.229904
Action reg: 0.003980
  l1.weight: grad_norm = 0.210530
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.474500
Total gradient norm: 0.879562
=== Actor Training Debug (Iteration 6468) ===
Q mean: -72.479050
Q std: 28.140118
Actor loss: 72.483040
Action reg: 0.003993
  l1.weight: grad_norm = 0.323763
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.657241
Total gradient norm: 1.155389
=== Actor Training Debug (Iteration 6469) ===
Q mean: -72.587067
Q std: 26.943810
Actor loss: 72.591057
Action reg: 0.003989
  l1.weight: grad_norm = 0.182183
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.459249
Total gradient norm: 0.928226
=== Actor Training Debug (Iteration 6470) ===
Q mean: -72.205574
Q std: 27.478008
Actor loss: 72.209557
Action reg: 0.003982
  l1.weight: grad_norm = 0.167867
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.430485
Total gradient norm: 0.786490
=== Actor Training Debug (Iteration 6471) ===
Q mean: -71.569542
Q std: 27.046360
Actor loss: 71.573524
Action reg: 0.003983
  l1.weight: grad_norm = 0.274717
  l1.bias: grad_norm = 0.000736
  l2.weight: grad_norm = 0.627077
Total gradient norm: 1.178197
=== Actor Training Debug (Iteration 6472) ===
Q mean: -74.102661
Q std: 26.213957
Actor loss: 74.106651
Action reg: 0.003990
  l1.weight: grad_norm = 0.071896
  l1.bias: grad_norm = 0.000414
  l2.weight: grad_norm = 0.152291
Total gradient norm: 0.262255
=== Actor Training Debug (Iteration 6473) ===
Q mean: -74.546112
Q std: 27.344652
Actor loss: 74.550095
Action reg: 0.003984
  l1.weight: grad_norm = 0.058631
  l1.bias: grad_norm = 0.000803
  l2.weight: grad_norm = 0.136596
Total gradient norm: 0.286230
=== Actor Training Debug (Iteration 6474) ===
Q mean: -75.231369
Q std: 26.442009
Actor loss: 75.235359
Action reg: 0.003990
  l1.weight: grad_norm = 0.135459
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.296258
Total gradient norm: 0.608600
=== Actor Training Debug (Iteration 6475) ===
Q mean: -73.309845
Q std: 26.722540
Actor loss: 73.313828
Action reg: 0.003983
  l1.weight: grad_norm = 0.208261
  l1.bias: grad_norm = 0.001502
  l2.weight: grad_norm = 0.427212
Total gradient norm: 0.724654
=== Actor Training Debug (Iteration 6476) ===
Q mean: -71.685181
=== Actor Training Debug (Iteration 6486) ===
Q mean: -72.480911
Q std: 27.650473
Actor loss: 72.484901
Action reg: 0.003992
  l1.weight: grad_norm = 0.116107
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.225245
Total gradient norm: 0.399754
=== Actor Training Debug (Iteration 6487) ===
Q mean: -71.202393
Q std: 26.462389
Actor loss: 71.206383
Action reg: 0.003989
  l1.weight: grad_norm = 0.109482
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.217826
Total gradient norm: 0.346431
=== Actor Training Debug (Iteration 6488) ===
Q mean: -75.315170
Q std: 27.148550
Actor loss: 75.319160
Action reg: 0.003990
  l1.weight: grad_norm = 1.115021
  l1.bias: grad_norm = 0.000484
  l2.weight: grad_norm = 2.360142
Total gradient norm: 4.202942
=== Actor Training Debug (Iteration 6489) ===
Q mean: -72.691193
Q std: 27.728428
Actor loss: 72.695175
Action reg: 0.003979
  l1.weight: grad_norm = 0.253843
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.530839
Total gradient norm: 1.008573
=== Actor Training Debug (Iteration 6490) ===
Q mean: -74.499863
Q std: 27.222116
Actor loss: 74.503860
Action reg: 0.003994
  l1.weight: grad_norm = 0.005653
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.012470
Total gradient norm: 0.025102
=== Actor Training Debug (Iteration 6491) ===
Q mean: -72.198204
Q std: 27.625177
Actor loss: 72.202194
Action reg: 0.003990
  l1.weight: grad_norm = 0.145256
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.346937
Total gradient norm: 0.723095
=== Actor Training Debug (Iteration 6492) ===
Q mean: -73.062004
Q std: 28.219889
Actor loss: 73.065987
Action reg: 0.003985
  l1.weight: grad_norm = 0.118766
  l1.bias: grad_norm = 0.000654
  l2.weight: grad_norm = 0.225593
Total gradient norm: 0.431690
=== Actor Training Debug (Iteration 6493) ===
Q mean: -72.642715
Q std: 27.517229
Actor loss: 72.646706
Action reg: 0.003988
  l1.weight: grad_norm = 0.181196
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.421488
Total gradient norm: 0.793505
=== Actor Training Debug (Iteration 6494) ===
Q mean: -75.149811
Q std: 27.067976
Actor loss: 75.153793
Action reg: 0.003983
  l1.weight: grad_norm = 0.059823
  l1.bias: grad_norm = 0.000885
  l2.weight: grad_norm = 0.124764
Total gradient norm: 0.208466
=== Actor Training Debug (Iteration 6495) ===
Q mean: -73.967865
Q std: 27.615799
Actor loss: 73.971855
Action reg: 0.003988
  l1.weight: grad_norm = 0.272309
  l1.bias: grad_norm = 0.000658
  l2.weight: grad_norm = 0.516233
Total gradient norm: 0.949723
=== Actor Training Debug (Iteration 6496) ===
Q mean: -75.409576
Q std: 27.437637
Actor loss: 75.413559
Action reg: 0.003985
  l1.weight: grad_norm = 0.096457
  l1.bias: grad_norm = 0.000865
  l2.weight: grad_norm = 0.226827
Total gradient norm: 0.416426
=== Actor Training Debug (Iteration 6497) ===
Q mean: -71.533417
Q std: 28.496122
Actor loss: 71.537392
Action reg: 0.003975
  l1.weight: grad_norm = 0.063500
  l1.bias: grad_norm = 0.001805
  l2.weight: grad_norm = 0.139848
Total gradient norm: 0.280990
=== Actor Training Debug (Iteration 6498) ===
Q mean: -72.128334
Q std: 28.050320
Actor loss: 72.132317
Action reg: 0.003981
  l1.weight: grad_norm = 0.109536
  l1.bias: grad_norm = 0.000971
  l2.weight: grad_norm = 0.191105
Total gradient norm: 0.394415
=== Actor Training Debug (Iteration 6499) ===
Q mean: -72.354881
Q std: 26.618353
Actor loss: 72.358864
Action reg: 0.003984
  l1.weight: grad_norm = 0.149434
  l1.bias: grad_norm = 0.000828
  l2.weight: grad_norm = 0.294770
Total gradient norm: 0.586688
=== Actor Training Debug (Iteration 6500) ===
Q mean: -73.492172
Q std: 26.360735
Actor loss: 73.496155
Action reg: 0.003986
  l1.weight: grad_norm = 0.619925
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 1.153451
Total gradient norm: 2.092763
  Average reward: -335.820 | Average length: 100.0
Evaluation at episode 115: -335.820
=== Actor Training Debug (Iteration 6501) ===
Q mean: -73.398537
Q std: 25.462557
Actor loss: 73.402527
Action reg: 0.003992
  l1.weight: grad_norm = 0.063981
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.139487
Total gradient norm: 0.259633
=== Actor Training Debug (Iteration 6502) ===
Q mean: -73.440292
Q std: 28.227350
Actor loss: 73.444275
Action reg: 0.003986
  l1.weight: grad_norm = 0.056525
  l1.bias: grad_norm = 0.000658
  l2.weight: grad_norm = 0.122198
Total gradient norm: 0.186173
=== Actor Training Debug (Iteration 6503) ===
Q mean: -71.640854
Q std: 27.816847
Actor loss: 71.644852
Action reg: 0.003995
  l1.weight: grad_norm = 0.179279
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.407182
Total gradient norm: 0.775890
=== Actor Training Debug (Iteration 6504) ===
Q mean: -71.343498
Q std: 27.846613
Actor loss: 71.347481
Action reg: 0.003982
  l1.weight: grad_norm = 2.188075
  l1.bias: grad_norm = 0.002659
  l2.weight: grad_norm = 5.315477
Total gradient norm: 10.689522
=== Actor Training Debug (Iteration 6505) ===
Q mean: -72.898148
Q std: 26.870985
Actor loss: 72.902138
Action reg: 0.003988
  l1.weight: grad_norm = 0.413434
  l1.bias: grad_norm = 0.000939
  l2.weight: grad_norm = 0.799865
Total gradient norm: 1.412768
=== Actor Training Debug (Iteration 6506) ===
Q mean: -73.777634
Q std: 26.283789
Actor loss: 73.781624
Action reg: 0.003988
  l1.weight: grad_norm = 0.233851
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.623454
Total gradient norm: 1.253705
=== Actor Training Debug (Iteration 6507) ===
Q mean: -74.337128
Q std: 27.654478
Actor loss: 74.341118
Action reg: 0.003989
  l1.weight: grad_norm = 0.159107
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.337202
Total gradient norm: 0.662201
=== Actor Training Debug (Iteration 6508) ===
Q mean: -71.514824
Q std: 26.608412
Actor loss: 71.518814
Action reg: 0.003991
  l1.weight: grad_norm = 0.062585
  l1.bias: grad_norm = 0.000494
  l2.weight: grad_norm = 0.146651
Total gradient norm: 0.280148
=== Actor Training Debug (Iteration 6509) ===
Q mean: -74.159019
Q std: 27.145767
Actor loss: 74.163010
Action reg: 0.003990
  l1.weight: grad_norm = 0.410507
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.872426
Total gradient norm: 1.493170
=== Actor Training Debug (Iteration 6510) ===
Q mean: -73.714798
Q std: 27.752470
Actor loss: 73.718788
Action reg: 0.003988
  l1.weight: grad_norm = 0.322471
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.697989
Total gradient norm: 1.298919
=== Actor Training Debug (Iteration 6511) ===
Q mean: -72.718536
Q std: 27.013977
Actor loss: 72.722527
Action reg: 0.003987
  l1.weight: grad_norm = 0.188887
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.374353
Total gradient norm: 0.700449
=== Actor Training Debug (Iteration 6512) ===
Q mean: -72.851318
Q std: 27.444105
Actor loss: 72.855309
Action reg: 0.003988
  l1.weight: grad_norm = 0.290117
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.718285
Total gradient norm: 1.352582
=== Actor Training Debug (Iteration 6513) ===
Q mean: -72.849869
Q std: 27.732601
Actor loss: 72.853851
Action reg: 0.003985
  l1.weight: grad_norm = 0.133955
  l1.bias: grad_norm = 0.000853
  l2.weight: grad_norm = 0.322504
Total gradient norm: 0.586677
=== Actor Training Debug (Iteration 6514) ===
Q mean: -72.992340
Q std: 26.661980
Actor loss: 72.996338
Action reg: 0.003995
  l1.weight: grad_norm = 0.279465
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.521781
Total gradient norm: 0.744666
=== Actor Training Debug (Iteration 6515) ===
Q mean: -76.289474
Q std: 25.747112
Actor loss: 76.293472
Action reg: 0.003994
  l1.weight: grad_norm = 0.148426
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.312210
Total gradient norm: 0.577841
=== Actor Training Debug (Iteration 6516) ===
Q mean: -72.747864
Q std: 26.307695
Actor loss: 72.751854
Action reg: 0.003987
  l1.weight: grad_norm = 0.162492
  l1.bias: grad_norm = 0.000671
  l2.weight: grad_norm = 0.341833
Total gradient norm: 0.761100
=== Actor Training Debug (Iteration 6517) ===
Q mean: -74.009995
Q std: 26.956480
Actor loss: 74.013985
Action reg: 0.003992
  l1.weight: grad_norm = 0.017109
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.042064
Total gradient norm: 0.077110
=== Actor Training Debug (Iteration 6518) ===
Q mean: -75.496811
Q std: 25.270658
Actor loss: 75.500809
Action reg: 0.003995
  l1.weight: grad_norm = 0.130355
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.265399
Total gradient norm: 0.406816
=== Actor Training Debug (Iteration 6519) ===
Q mean: -74.416336
Q std: 26.923304
Actor loss: 74.420326
Action reg: 0.003993
  l1.weight: grad_norm = 0.051019
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.115505
Total gradient norm: 0.213186
=== Actor Training Debug (Iteration 6520) ===
Q mean: -71.515358
Q std: 28.217464
Actor loss: 71.519348
Action reg: 0.003987
  l1.weight: grad_norm = 0.193613
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.419474
Total gradient norm: 0.646408
=== Actor Training Debug (Iteration 6521) ===
Q mean: -74.378952
Q std: 28.377684
Actor loss: 74.382935
Action reg: 0.003986
  l1.weight: grad_norm = 0.214482
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.507133
Total gradient norm: 1.094520
=== Actor Training Debug (Iteration 6522) ===
Q mean: -72.280952
Q std: 27.171160
Actor loss: 72.284935
Action reg: 0.003984
  l1.weight: grad_norm = 0.240724
  l1.bias: grad_norm = 0.000602
  l2.weight: grad_norm = 0.634701
Total gradient norm: 1.235936
=== Actor Training Debug (Iteration 6523) ===
Q mean: -73.577362
Q std: 28.019407
Actor loss: 73.581345
Action reg: 0.003983
  l1.weight: grad_norm = 0.096039
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.189871
Total gradient norm: 0.345608
=== Actor Training Debug (Iteration 6524) ===
Q mean: -72.508514
Q std: 25.876989
Actor loss: 72.512505
Action reg: 0.003991
  l1.weight: grad_norm = 0.138759
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.238236
Total gradient norm: 0.467645
=== Actor Training Debug (Iteration 6525) ===
Q mean: -73.214462
Q std: 28.082211
Actor loss: 73.218445
Action reg: 0.003985
  l1.weight: grad_norm = 0.123118
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.249148
Total gradient norm: 0.470752
=== Actor Training Debug (Iteration 6526) ===
Q mean: -75.596359
Q std: 27.622667
Actor loss: 75.600342
Action reg: 0.003981
  l1.weight: grad_norm = 0.174523
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.395638
Total gradient norm: 0.710793
=== Actor Training Debug (Iteration 6527) ===
Q mean: -73.702606
Q std: 29.010445
Actor loss: 73.706589
Action reg: 0.003984
  l1.weight: grad_norm = 0.421671
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.902172
Total gradient norm: 1.780330
=== Actor Training Debug (Iteration 6528) ===
Q mean: -71.452011
Q std: 26.876928
Actor loss: 71.456001
Action reg: 0.003991
  l1.weight: grad_norm = 0.136864
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.299785
Total gradient norm: 0.511532
=== Actor Training Debug (Iteration 6529) ===
Q mean: -68.533936
Q std: 27.505188
Actor loss: 68.537918
Action reg: 0.003985
  l1.weight: grad_norm = 0.016202
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.038296
Total gradient norm: 0.075183
=== Actor Training Debug (Iteration 6530) ===
Q mean: -73.025131
Q std: 26.580330
Actor loss: 73.029121
Action reg: 0.003987
  l1.weight: grad_norm = 0.148763
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.318966
Total gradient norm: 0.604839
=== Actor Training Debug (Iteration 6531) ===
Q mean: -74.446381
Q std: 27.039032
Actor loss: 74.450371
Action reg: 0.003990
  l1.weight: grad_norm = 0.333315
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.697306
Total gradient norm: 1.144216
=== Actor Training Debug (Iteration 6532) ===
Q mean: -75.806923
Q std: 27.322872
Actor loss: 75.810905
Action reg: 0.003981
  l1.weight: grad_norm = 0.373545
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.731830
Total gradient norm: 1.259410
=== Actor Training Debug (Iteration 6533) ===
Q mean: -74.482689
Q std: 28.028996
Actor loss: 74.486671
Action reg: 0.003986
  l1.weight: grad_norm = 0.223068
  l1.bias: grad_norm = 0.000529
  l2.weight: grad_norm = 0.473211
Total gradient norm: 0.910945
=== Actor Training Debug (Iteration 6534) ===
Q mean: -71.878525
Q std: 25.859457
Actor loss: 71.882515
Action reg: 0.003992
  l1.weight: grad_norm = 0.086172
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.205187
Total gradient norm: 0.382269
=== Actor Training Debug (Iteration 6535) ===
Q mean: -72.253922
Q std: 26.976212
Actor loss: 72.257904
Action reg: 0.003982
  l1.weight: grad_norm = 0.362580
  l1.bias: grad_norm = 0.000712
  l2.weight: grad_norm = 0.775482
Total gradient norm: 1.456591
=== Actor Training Debug (Iteration 6536) ===
Q mean: -72.847038
Q std: 26.573065
Actor loss: 72.851036
Action reg: 0.003994
  l1.weight: grad_norm = 0.069828
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.166753
Total gradient norm: 0.357508
=== Actor Training Debug (Iteration 6537) ===
Q mean: -74.988289
Q std: 28.941362
Actor loss: 74.992279
Action reg: 0.003990
  l1.weight: grad_norm = 0.364525
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.753188
Total gradient norm: 1.391767
=== Actor Training Debug (Iteration 6538) ===
Q mean: -73.344284
Q std: 29.390142
Actor loss: 73.348267
Action reg: 0.003983
  l1.weight: grad_norm = 0.009488
  l1.bias: grad_norm = 0.000955
  l2.weight: grad_norm = 0.022641
Total gradient norm: 0.051289
=== Actor Training Debug (Iteration 6539) ===
Q mean: -74.249863
Q std: 29.565052
Actor loss: 74.253853
Action reg: 0.003992
  l1.weight: grad_norm = 0.004973
  l1.bias: grad_norm = 0.000626
  l2.weight: grad_norm = 0.013366
Total gradient norm: 0.032713
=== Actor Training Debug (Iteration 6540) ===
Q mean: -76.769867
Q std: 28.328146
Actor loss: 76.773857
Action reg: 0.003990
  l1.weight: grad_norm = 0.131063
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.293615
Total gradient norm: 0.590566
=== Actor Training Debug (Iteration 6541) ===
Q mean: -73.424316
Q std: 27.278414
Actor loss: 73.428307
Action reg: 0.003987
  l1.weight: grad_norm = 0.460967
  l1.bias: grad_norm = 0.000608
  l2.weight: grad_norm = 1.071669
Total gradient norm: 2.121082
=== Actor Training Debug (Iteration 6542) ===
Q mean: -72.196411
Q std: 26.598501
Actor loss: 72.200401
Action reg: 0.003992
  l1.weight: grad_norm = 0.153016
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.368382
Total gradient norm: 0.699835
=== Actor Training Debug (Iteration 6543) ===
Q mean: -70.206314
Q std: 27.961863
Actor loss: 70.210289
Action reg: 0.003975
  l1.weight: grad_norm = 0.277319
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.444151
Total gradient norm: 0.809396
=== Actor Training Debug (Iteration 6544) ===
Q mean: -72.798416
Q std: 26.475615
Actor loss: 72.802399
Action reg: 0.003982
  l1.weight: grad_norm = 0.163085
  l1.bias: grad_norm = 0.000662
  l2.weight: grad_norm = 0.295041
Total gradient norm: 0.610179
=== Actor Training Debug (Iteration 6545) ===
Q mean: -76.323883
Q std: 25.676023
Actor loss: 76.327873
Action reg: 0.003991
  l1.weight: grad_norm = 0.133960
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.299216
Total gradient norm: 0.598090
=== Actor Training Debug (Iteration 6546) ===
Q mean: -71.056458
Q std: 27.034128
Actor loss: 71.060448
Action reg: 0.003988
  l1.weight: grad_norm = 0.139629
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.294046
Total gradient norm: 0.539723
=== Actor Training Debug (Iteration 6547) ===
Q mean: -73.103622
Q std: 27.164875
Actor loss: 73.107605
Action reg: 0.003986
  l1.weight: grad_norm = 0.284978
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.681691
Total gradient norm: 1.519750
=== Actor Training Debug (Iteration 6548) ===
Q mean: -72.314049
Q std: 27.068127
Actor loss: 72.318039
Action reg: 0.003993
  l1.weight: grad_norm = 1.701172
  l1.bias: grad_norm = 0.002221
  l2.weight: grad_norm = 3.561159
Total gradient norm: 6.167154
=== Actor Training Debug (Iteration 6549) ===
Q mean: -71.552834
Q std: 26.657621
Actor loss: 71.556824
Action reg: 0.003989
  l1.weight: grad_norm = 0.257483
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.675680
Total gradient norm: 1.395671
=== Actor Training Debug (Iteration 6550) ===
Q mean: -75.497070
Q std: 26.634111
Actor loss: 75.501060
Action reg: 0.003989
  l1.weight: grad_norm = 0.067043
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.153480
Total gradient norm: 0.298275
=== Actor Training Debug (Iteration 6551) ===
Q mean: -71.511551
Q std: 27.407822
Actor loss: 71.515549
Action reg: 0.003995
  l1.weight: grad_norm = 0.072627
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.159929
Total gradient norm: 0.273998
=== Actor Training Debug (Iteration 6552) ===
Q mean: -71.106552
Q std: 28.213385
Actor loss: 71.110550
Action reg: 0.003998
  l1.weight: grad_norm = 0.102190
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.218223
Total gradient norm: 0.455927
=== Actor Training Debug (Iteration 6553) ===
Q mean: -74.020569
Q std: 27.046062
Actor loss: 74.024559
Action reg: 0.003987
  l1.weight: grad_norm = 0.135108
  l1.bias: grad_norm = 0.000526
  l2.weight: grad_norm = 0.299526
Total gradient norm: 0.616605
=== Actor Training Debug (Iteration 6554) ===
Q mean: -73.014618
Q std: 25.942829
Actor loss: 73.018616
Action reg: 0.003995
  l1.weight: grad_norm = 0.112790
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.225706
Total gradient norm: 0.425190
=== Actor Training Debug (Iteration 6555) ===
Q mean: -73.764549
Q std: 25.280561
Actor loss: 73.768539
Action reg: 0.003991
  l1.weight: grad_norm = 0.072137
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.161363
Total gradient norm: 0.299298
=== Actor Training Debug (Iteration 6556) ===
Q mean: -75.548035
Q std: 26.384619
Actor loss: 75.552032
Action reg: 0.003995
  l1.weight: grad_norm = 0.030235
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.068445
Total gradient norm: 0.115320
=== Actor Training Debug (Iteration 6557) ===
Q mean: -72.778984
Q std: 26.611246
Actor loss: 72.782974
Action reg: 0.003992
  l1.weight: grad_norm = 0.092711
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.223039
Total gradient norm: 0.498743
=== Actor Training Debug (Iteration 6558) ===
Q mean: -73.304886
Q std: 27.967966
Actor loss: 73.308876
Action reg: 0.003992
  l1.weight: grad_norm = 0.170349
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.434058
Total gradient norm: 0.784951
=== Actor Training Debug (Iteration 6559) ===
Q mean: -74.497612
Q std: 26.053539
Actor loss: 74.501602
Action reg: 0.003994
  l1.weight: grad_norm = 0.116122
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.247112
Total gradient norm: 0.507495
=== Actor Training Debug (Iteration 6560) ===
Q mean: -74.862137
Q std: 27.430016
Actor loss: 74.866119
Action reg: 0.003986
  l1.weight: grad_norm = 0.290442
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.691773
Total gradient norm: 1.392435
=== Actor Training Debug (Iteration 6561) ===
Q mean: -73.273102
Q std: 28.925701
Actor loss: 73.277084
Action reg: 0.003983
  l1.weight: grad_norm = 0.129487
  l1.bias: grad_norm = 0.000892
  l2.weight: grad_norm = 0.259179
Total gradient norm: 0.509255
=== Actor Training Debug (Iteration 6562) ===
Q mean: -74.767632
Q std: 27.639977
Actor loss: 74.771622
Action reg: 0.003987
  l1.weight: grad_norm = 0.109552
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.253408
Total gradient norm: 0.416670
=== Actor Training Debug (Iteration 6563) ===
Q mean: -71.817757
Q std: 27.100416
Actor loss: 71.821747
Action reg: 0.003990
  l1.weight: grad_norm = 0.171448
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.373558
Total gradient norm: 0.735473
=== Actor Training Debug (Iteration 6564) ===
Q mean: -75.825218
Q std: 27.147963
Actor loss: 75.829208
Action reg: 0.003987
  l1.weight: grad_norm = 0.868183
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 2.094250
Total gradient norm: 4.082626
=== Actor Training Debug (Iteration 6565) ===
Q mean: -73.361832
Q std: 28.552711
Actor loss: 73.365822
Action reg: 0.003990
  l1.weight: grad_norm = 0.112899
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.232262
Total gradient norm: 0.424067
=== Actor Training Debug (Iteration 6566) ===
Q mean: -71.702896
Q std: 26.605242
Actor loss: 71.706871
Action reg: 0.003973
  l1.weight: grad_norm = 0.265380
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.619523
Total gradient norm: 1.211218
=== Actor Training Debug (Iteration 6567) ===
Q mean: -71.394638
Q std: 28.154575
Actor loss: 71.398613
Action reg: 0.003978
  l1.weight: grad_norm = 0.278491
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.516254
Total gradient norm: 0.898710
=== Actor Training Debug (Iteration 6568) ===
Q mean: -75.778053
Q std: 26.952787
Actor loss: 75.782043
Action reg: 0.003987
  l1.weight: grad_norm = 0.103471
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.285433
Total gradient norm: 0.545112
=== Actor Training Debug (Iteration 6569) ===
Q mean: -74.396027
Q std: 29.309080
Actor loss: 74.400002
Action reg: 0.003976
  l1.weight: grad_norm = 0.209732
  l1.bias: grad_norm = 0.000997
  l2.weight: grad_norm = 0.434297
Total gradient norm: 0.949737
=== Actor Training Debug (Iteration 6570) ===
Q mean: -76.212936
Q std: 25.348040
Actor loss: 76.216934
Action reg: 0.003994
  l1.weight: grad_norm = 0.197948
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.438544
Total gradient norm: 0.809523
=== Actor Training Debug (Iteration 6571) ===
Q mean: -69.898361
Q std: 27.600929
Actor loss: 69.902344
Action reg: 0.003981
  l1.weight: grad_norm = 0.203073
  l1.bias: grad_norm = 0.000990
  l2.weight: grad_norm = 0.382044
Total gradient norm: 0.903930
=== Actor Training Debug (Iteration 6572) ===
Q mean: -72.366562
Q std: 27.744307
Actor loss: 72.370544
Action reg: 0.003985
  l1.weight: grad_norm = 0.266239
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.599146
Total gradient norm: 1.041968
=== Actor Training Debug (Iteration 6573) ===
Q mean: -75.930252
Q std: 26.515388
Actor loss: 75.934242
Action reg: 0.003992
  l1.weight: grad_norm = 0.187049
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.339390
Total gradient norm: 0.573095
=== Actor Training Debug (Iteration 6574) ===
Q mean: -70.364792
Q std: 27.465071
Actor loss: 70.368774
Action reg: 0.003983
  l1.weight: grad_norm = 0.149530
  l1.bias: grad_norm = 0.000627
  l2.weight: grad_norm = 0.268615
Total gradient norm: 0.419673
=== Actor Training Debug (Iteration 6575) ===
Q mean: -73.182007
Q std: 25.759434
Actor loss: 73.185997
Action reg: 0.003993
  l1.weight: grad_norm = 0.232384
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.502587
Total gradient norm: 0.964341
=== Actor Training Debug (Iteration 6576) ===
Q mean: -77.322601
Q std: 27.539616
Actor loss: 77.326584
Action reg: 0.003983
  l1.weight: grad_norm = 0.199441
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.462533
Total gradient norm: 0.903697
=== Actor Training Debug (Iteration 6577) ===
Q mean: -71.672913
Q std: 27.579285
Actor loss: 71.676903
Action reg: 0.003988
  l1.weight: grad_norm = 0.198413
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.418011
Total gradient norm: 0.815783
=== Actor Training Debug (Iteration 6578) ===
Q mean: -73.790771
Q std: 26.283058
Actor loss: 73.794762
Action reg: 0.003989
  l1.weight: grad_norm = 0.180949
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.419474
Total gradient norm: 0.771529
=== Actor Training Debug (Iteration 6579) ===
Q mean: -71.160522
Q std: 27.069864
Actor loss: 71.164520
Action reg: 0.003996
  l1.weight: grad_norm = 0.131420
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.275411
Total gradient norm: 0.487627
=== Actor Training Debug (Iteration 6580) ===
Q mean: -71.687881
Q std: 27.190296
Actor loss: 71.691872
Action reg: 0.003987
  l1.weight: grad_norm = 0.131547
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.315371
Total gradient norm: 0.645365
=== Actor Training Debug (Iteration 6581) ===
Q mean: -77.128151
Q std: 27.489250
Actor loss: 77.132149
Action reg: 0.003995
  l1.weight: grad_norm = 0.158405
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.351104
Total gradient norm: 0.614202
=== Actor Training Debug (Iteration 6582) ===
Q mean: -71.861839
Q std: 26.722073
Actor loss: 71.865837
Action reg: 0.003995
  l1.weight: grad_norm = 0.215573
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.468775
Total gradient norm: 1.035904
=== Actor Training Debug (Iteration 6583) ===
Q mean: -73.903023
Q std: 27.527138
Actor loss: 73.907013
Action reg: 0.003991
  l1.weight: grad_norm = 0.119817
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.250436
Total gradient norm: 0.438801
=== Actor Training Debug (Iteration 6584) ===
Q mean: -74.044678
Q std: 27.240973
Actor loss: 74.048668
Action reg: 0.003991
  l1.weight: grad_norm = 0.076595
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.170214
Total gradient norm: 0.329505
=== Actor Training Debug (Iteration 6585) ===
Q mean: -73.933289
Q std: 26.542950
Actor loss: 73.937279
Action reg: 0.003993
  l1.weight: grad_norm = 0.067551
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.156255
Total gradient norm: 0.331777
=== Actor Training Debug (Iteration 6586) ===
Q mean: -74.014801
Q std: 29.480614
Actor loss: 74.018784
Action reg: 0.003983
  l1.weight: grad_norm = 0.155012
  l1.bias: grad_norm = 0.000481
  l2.weight: grad_norm = 0.404590
Total gradient norm: 0.871016
=== Actor Training Debug (Iteration 6587) ===
Q mean: -73.752602
Q std: 27.654131
Actor loss: 73.756592
Action reg: 0.003990
  l1.weight: grad_norm = 0.081257
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.188412
Total gradient norm: 0.350872
Total gradient norm: 0.129099ration 6486) ===
=== Actor Training Debug (Iteration 6598) ===
Q mean: -71.522575
Q std: 27.519405
Actor loss: 71.526573
Action reg: 0.003996
  l1.weight: grad_norm = 0.016119
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.032999
Total gradient norm: 0.063375
=== Actor Training Debug (Iteration 6599) ===
Q mean: -72.389694
Q std: 27.382608
Actor loss: 72.393684
Action reg: 0.003989
  l1.weight: grad_norm = 0.031810
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.069985
Total gradient norm: 0.140596
=== Actor Training Debug (Iteration 6600) ===
Q mean: -74.371170
Q std: 27.168530
Actor loss: 74.375160
Action reg: 0.003992
  l1.weight: grad_norm = 0.080279
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.174016
Total gradient norm: 0.290129
=== Actor Training Debug (Iteration 6601) ===
Q mean: -73.029327
Q std: 29.092318
Actor loss: 73.033310
Action reg: 0.003985
  l1.weight: grad_norm = 0.130221
  l1.bias: grad_norm = 0.000854
  l2.weight: grad_norm = 0.316933
Total gradient norm: 0.594359
=== Actor Training Debug (Iteration 6602) ===
Q mean: -72.483498
Q std: 27.537170
Actor loss: 72.487488
Action reg: 0.003993
  l1.weight: grad_norm = 0.161727
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.303840
Total gradient norm: 0.451406
=== Actor Training Debug (Iteration 6603) ===
Q mean: -71.392548
Q std: 27.422537
Actor loss: 71.396538
Action reg: 0.003991
  l1.weight: grad_norm = 0.008624
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.022176
Total gradient norm: 0.039270
=== Actor Training Debug (Iteration 6604) ===
Q mean: -74.266075
Q std: 26.918470
Actor loss: 74.270065
Action reg: 0.003991
  l1.weight: grad_norm = 0.094859
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.221259
Total gradient norm: 0.424619
=== Actor Training Debug (Iteration 6605) ===
Q mean: -71.929749
Q std: 28.673145
Actor loss: 71.933731
Action reg: 0.003985
  l1.weight: grad_norm = 0.070717
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.150902
Total gradient norm: 0.270857
=== Actor Training Debug (Iteration 6606) ===
Q mean: -75.301010
Q std: 27.129883
Actor loss: 75.304993
Action reg: 0.003985
  l1.weight: grad_norm = 0.240843
  l1.bias: grad_norm = 0.000590
  l2.weight: grad_norm = 0.414729
Total gradient norm: 0.717921
=== Actor Training Debug (Iteration 6607) ===
Q mean: -70.823105
Q std: 25.102005
Actor loss: 70.827103
Action reg: 0.003996
  l1.weight: grad_norm = 0.330742
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.606604
Total gradient norm: 1.199829
=== Actor Training Debug (Iteration 6608) ===
Q mean: -71.739792
Q std: 28.120251
Actor loss: 71.743767
Action reg: 0.003973
  l1.weight: grad_norm = 0.415829
  l1.bias: grad_norm = 0.000859
  l2.weight: grad_norm = 0.895901
Total gradient norm: 1.541330
=== Actor Training Debug (Iteration 6609) ===
Q mean: -70.440659
Q std: 29.770500
Actor loss: 70.444633
Action reg: 0.003977
  l1.weight: grad_norm = 0.193346
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.430883
Total gradient norm: 0.703829
=== Actor Training Debug (Iteration 6610) ===
Q mean: -74.673843
Q std: 27.329618
Actor loss: 74.677834
Action reg: 0.003992
  l1.weight: grad_norm = 0.350019
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.747229
Total gradient norm: 1.434260
=== Actor Training Debug (Iteration 6611) ===
Q mean: -72.678513
Q std: 28.027662
Actor loss: 72.682495
Action reg: 0.003983
  l1.weight: grad_norm = 0.114073
  l1.bias: grad_norm = 0.000524
  l2.weight: grad_norm = 0.273195
Total gradient norm: 0.522341
=== Actor Training Debug (Iteration 6612) ===
Q mean: -74.404526
Q std: 26.750656
Actor loss: 74.408508
Action reg: 0.003983
  l1.weight: grad_norm = 0.185071
  l1.bias: grad_norm = 0.000649
  l2.weight: grad_norm = 0.405020
Total gradient norm: 0.755523
=== Actor Training Debug (Iteration 6613) ===
Q mean: -72.093567
Q std: 26.761713
Actor loss: 72.097557
Action reg: 0.003990
  l1.weight: grad_norm = 0.004799
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.010975
Total gradient norm: 0.021220
=== Actor Training Debug (Iteration 6614) ===
Q mean: -74.526718
Q std: 27.303020
Actor loss: 74.530708
Action reg: 0.003989
  l1.weight: grad_norm = 0.204146
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.417516
Total gradient norm: 0.749305
=== Actor Training Debug (Iteration 6615) ===
Q mean: -73.193810
Q std: 28.137367
Actor loss: 73.197792
Action reg: 0.003981
  l1.weight: grad_norm = 0.259503
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.584197
Total gradient norm: 1.038158
=== Actor Training Debug (Iteration 6616) ===
Q mean: -72.612526
Q std: 29.926605
Actor loss: 72.616508
Action reg: 0.003985
  l1.weight: grad_norm = 0.077152
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.170121
Total gradient norm: 0.320558
=== Actor Training Debug (Iteration 6617) ===
Q mean: -76.508575
Q std: 27.103018
Actor loss: 76.512566
Action reg: 0.003989
  l1.weight: grad_norm = 0.367927
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.765188
Total gradient norm: 1.399642
=== Actor Training Debug (Iteration 6618) ===
Q mean: -76.314903
Q std: 27.830212
Actor loss: 76.318893
Action reg: 0.003990
  l1.weight: grad_norm = 0.060389
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.151835
Total gradient norm: 0.296641
=== Actor Training Debug (Iteration 6619) ===
Q mean: -76.976166
Q std: 27.131971
Actor loss: 76.980148
Action reg: 0.003985
  l1.weight: grad_norm = 0.114445
  l1.bias: grad_norm = 0.000526
  l2.weight: grad_norm = 0.223633
Total gradient norm: 0.342189
=== Actor Training Debug (Iteration 6620) ===
Q mean: -71.260101
Q std: 26.636473
Actor loss: 71.264091
Action reg: 0.003991
  l1.weight: grad_norm = 0.073882
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.186362
Total gradient norm: 0.398955
=== Actor Training Debug (Iteration 6621) ===
Q mean: -73.750427
Q std: 29.145203
Actor loss: 73.754417
Action reg: 0.003990
  l1.weight: grad_norm = 0.128429
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.337944
Total gradient norm: 0.712374
=== Actor Training Debug (Iteration 6622) ===
Q mean: -75.083168
Q std: 27.930817
Actor loss: 75.087158
Action reg: 0.003990
  l1.weight: grad_norm = 0.105379
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.215926
Total gradient norm: 0.423769
=== Actor Training Debug (Iteration 6623) ===
Q mean: -74.352234
Q std: 29.333410
Actor loss: 74.356224
Action reg: 0.003991
  l1.weight: grad_norm = 0.229219
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.514045
Total gradient norm: 1.027830
=== Actor Training Debug (Iteration 6624) ===
Q mean: -77.009583
Q std: 25.240210
Actor loss: 77.013580
Action reg: 0.003995
  l1.weight: grad_norm = 0.189732
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.391156
Total gradient norm: 0.674888
=== Actor Training Debug (Iteration 6625) ===
Q mean: -74.752739
Q std: 27.045856
Actor loss: 74.756729
Action reg: 0.003991
  l1.weight: grad_norm = 0.034897
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.069908
Total gradient norm: 0.127186
=== Actor Training Debug (Iteration 6626) ===
Q mean: -72.584389
Q std: 27.444952
Actor loss: 72.588364
Action reg: 0.003977
  l1.weight: grad_norm = 0.248409
  l1.bias: grad_norm = 0.000713
  l2.weight: grad_norm = 0.663306
Total gradient norm: 1.445509
=== Actor Training Debug (Iteration 6627) ===
Q mean: -74.425461
Q std: 26.890158
Actor loss: 74.429451
Action reg: 0.003990
  l1.weight: grad_norm = 0.242396
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 0.473565
Total gradient norm: 0.988623
=== Actor Training Debug (Iteration 6628) ===
Q mean: -72.952652
Q std: 26.821970
Actor loss: 72.956650
Action reg: 0.003996
  l1.weight: grad_norm = 0.144257
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.298322
Total gradient norm: 0.560401
=== Actor Training Debug (Iteration 6629) ===
Q mean: -72.181000
Q std: 26.901299
Actor loss: 72.184982
Action reg: 0.003983
  l1.weight: grad_norm = 0.216240
  l1.bias: grad_norm = 0.000529
  l2.weight: grad_norm = 0.460056
Total gradient norm: 0.771579
=== Actor Training Debug (Iteration 6630) ===
Q mean: -73.180710
Q std: 25.458895
Actor loss: 73.184700
Action reg: 0.003993
  l1.weight: grad_norm = 0.249643
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.541130
Total gradient norm: 0.966767
=== Actor Training Debug (Iteration 6631) ===
Q mean: -74.286469
Q std: 27.204651
Actor loss: 74.290459
Action reg: 0.003989
  l1.weight: grad_norm = 0.336168
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.730840
Total gradient norm: 1.427254
=== Actor Training Debug (Iteration 6632) ===
Q mean: -72.156174
Q std: 28.215275
Actor loss: 72.160156
Action reg: 0.003981
  l1.weight: grad_norm = 0.148740
  l1.bias: grad_norm = 0.000746
  l2.weight: grad_norm = 0.297517
Total gradient norm: 0.612846
=== Actor Training Debug (Iteration 6633) ===
Q mean: -73.138489
Q std: 27.601921
Actor loss: 73.142487
Action reg: 0.003995
  l1.weight: grad_norm = 0.068696
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.139529
Total gradient norm: 0.257182
=== Actor Training Debug (Iteration 6634) ===
Q mean: -73.890686
Q std: 29.343281
Actor loss: 73.894661
Action reg: 0.003979
  l1.weight: grad_norm = 0.120121
  l1.bias: grad_norm = 0.000781
  l2.weight: grad_norm = 0.273264
Total gradient norm: 0.526835
=== Actor Training Debug (Iteration 6635) ===
Q mean: -73.574867
Q std: 26.871086
Actor loss: 73.578857
Action reg: 0.003990
  l1.weight: grad_norm = 0.109571
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.256591
Total gradient norm: 0.510930
=== Actor Training Debug (Iteration 6636) ===
Q mean: -73.847687
Q std: 28.071091
Actor loss: 73.851677
Action reg: 0.003988
  l1.weight: grad_norm = 0.205495
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.481059
Total gradient norm: 1.011281
=== Actor Training Debug (Iteration 6637) ===
Q mean: -71.569717
Q std: 26.598423
Actor loss: 71.573708
Action reg: 0.003989
  l1.weight: grad_norm = 0.235869
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.484787
Total gradient norm: 0.910657
=== Actor Training Debug (Iteration 6638) ===
Q mean: -76.258026
Q std: 27.387222
Actor loss: 76.262016
Action reg: 0.003989
  l1.weight: grad_norm = 0.333663
  l1.bias: grad_norm = 0.001002
  l2.weight: grad_norm = 0.605317
Total gradient norm: 1.264176
=== Actor Training Debug (Iteration 6639) ===
Q mean: -76.136253
Q std: 27.644262
Actor loss: 76.140244
Action reg: 0.003991
  l1.weight: grad_norm = 0.108355
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.228159
Total gradient norm: 0.420452
=== Actor Training Debug (Iteration 6640) ===
Q mean: -75.451111
Q std: 26.679689
Actor loss: 75.455109
Action reg: 0.003996
  l1.weight: grad_norm = 0.166801
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.323534
Total gradient norm: 0.650055
=== Actor Training Debug (Iteration 6641) ===
Q mean: -72.638733
Q std: 26.110661
Actor loss: 72.642723
Action reg: 0.003991
  l1.weight: grad_norm = 0.266959
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.645688
Total gradient norm: 1.259226
=== Actor Training Debug (Iteration 6642) ===
Q mean: -73.972542
Q std: 28.059374
Actor loss: 73.976524
Action reg: 0.003985
  l1.weight: grad_norm = 0.469193
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.887711
Total gradient norm: 1.434464
=== Actor Training Debug (Iteration 6643) ===
Q mean: -77.689850
Q std: 27.663542
Actor loss: 77.693832
Action reg: 0.003984
  l1.weight: grad_norm = 0.244777
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.585579
Total gradient norm: 1.130211
=== Actor Training Debug (Iteration 6644) ===
Q mean: -72.875984
Q std: 27.092291
Actor loss: 72.879967
Action reg: 0.003981
  l1.weight: grad_norm = 0.165474
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.307319
Total gradient norm: 0.516261
=== Actor Training Debug (Iteration 6645) ===
Q mean: -74.200378
Q std: 27.981441
Actor loss: 74.204369
Action reg: 0.003988
  l1.weight: grad_norm = 0.177921
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.388274
Total gradient norm: 0.694798
=== Actor Training Debug (Iteration 6646) ===
Q mean: -72.630638
Q std: 27.924713
Actor loss: 72.634628
Action reg: 0.003993
  l1.weight: grad_norm = 0.006420
  l1.bias: grad_norm = 0.000721
  l2.weight: grad_norm = 0.013713
Total gradient norm: 0.027847
=== Actor Training Debug (Iteration 6647) ===
Q mean: -71.804008
Q std: 28.249714
Actor loss: 71.807991
Action reg: 0.003984
  l1.weight: grad_norm = 0.145105
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.268328
Total gradient norm: 0.509435
=== Actor Training Debug (Iteration 6648) ===
Q mean: -72.342987
Q std: 28.833080
Actor loss: 72.346970
Action reg: 0.003983
  l1.weight: grad_norm = 0.334685
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.746046
Total gradient norm: 1.405673
=== Actor Training Debug (Iteration 6649) ===
Q mean: -75.935394
Q std: 27.057976
Actor loss: 75.939392
Action reg: 0.003996
  l1.weight: grad_norm = 0.001740
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.005104
Total gradient norm: 0.014638
=== Actor Training Debug (Iteration 6650) ===
Q mean: -73.918015
Q std: 26.987415
Actor loss: 73.922005
Action reg: 0.003987
  l1.weight: grad_norm = 0.196379
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.438716
Total gradient norm: 0.711233
=== Actor Training Debug (Iteration 6651) ===
Q mean: -73.504456
Q std: 27.648066
Actor loss: 73.508446
Action reg: 0.003987
  l1.weight: grad_norm = 0.131122
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.273212
Total gradient norm: 0.478411
=== Actor Training Debug (Iteration 6652) ===
Q mean: -73.861267
Q std: 25.934742
Actor loss: 73.865257
Action reg: 0.003992
  l1.weight: grad_norm = 0.154129
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.347336
Total gradient norm: 0.660799
=== Actor Training Debug (Iteration 6653) ===
Q mean: -74.830170
Q std: 28.606764
Actor loss: 74.834160
Action reg: 0.003992
  l1.weight: grad_norm = 0.079868
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.195810
Total gradient norm: 0.360479
=== Actor Training Debug (Iteration 6654) ===
Q mean: -75.684677
Q std: 26.456541
Actor loss: 75.688667
Action reg: 0.003993
  l1.weight: grad_norm = 0.276608
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.539082
Total gradient norm: 1.155586
=== Actor Training Debug (Iteration 6655) ===
Q mean: -74.263290
Q std: 29.114861
Actor loss: 74.267281
Action reg: 0.003987
  l1.weight: grad_norm = 0.059237
  l1.bias: grad_norm = 0.000658
  l2.weight: grad_norm = 0.128539
Total gradient norm: 0.256927
=== Actor Training Debug (Iteration 6656) ===
Q mean: -73.413086
Q std: 27.021931
Actor loss: 73.417076
Action reg: 0.003988
  l1.weight: grad_norm = 0.413206
  l1.bias: grad_norm = 0.001259
  l2.weight: grad_norm = 0.838712
Total gradient norm: 1.724929
=== Actor Training Debug (Iteration 6657) ===
Q mean: -75.502945
Q std: 29.305668
Actor loss: 75.506927
Action reg: 0.003984
  l1.weight: grad_norm = 0.274835
  l1.bias: grad_norm = 0.001201
  l2.weight: grad_norm = 0.657371
Total gradient norm: 1.552718
=== Actor Training Debug (Iteration 6658) ===
Q mean: -74.699509
Q std: 27.350048
Actor loss: 74.703506
Action reg: 0.003998
  l1.weight: grad_norm = 0.270591
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.561189
Total gradient norm: 1.079188
=== Actor Training Debug (Iteration 6659) ===
Q mean: -73.492416
Q std: 27.585672
Actor loss: 73.496399
Action reg: 0.003985
  l1.weight: grad_norm = 0.202461
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.464381
Total gradient norm: 0.762451
=== Actor Training Debug (Iteration 6660) ===
Q mean: -70.357140
Q std: 28.061211
Actor loss: 70.361130
Action reg: 0.003989
  l1.weight: grad_norm = 0.122906
  l1.bias: grad_norm = 0.000538
  l2.weight: grad_norm = 0.303382
Total gradient norm: 0.565858
=== Actor Training Debug (Iteration 6661) ===
Q mean: -74.260803
Q std: 26.739733
Actor loss: 74.264801
Action reg: 0.003998
  l1.weight: grad_norm = 0.089786
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.210172
Total gradient norm: 0.439166
=== Actor Training Debug (Iteration 6662) ===
Q mean: -75.068214
Q std: 26.710293
Actor loss: 75.072205
Action reg: 0.003988
  l1.weight: grad_norm = 0.255924
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.536162
Total gradient norm: 1.082029
=== Actor Training Debug (Iteration 6663) ===
Q mean: -73.849236
Q std: 28.671211
Actor loss: 73.853226
Action reg: 0.003992
  l1.weight: grad_norm = 0.089013
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.216041
Total gradient norm: 0.434427
=== Actor Training Debug (Iteration 6664) ===
Q mean: -71.187714
Q std: 27.573980
Actor loss: 71.191696
Action reg: 0.003984
  l1.weight: grad_norm = 0.211571
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.397324
Total gradient norm: 0.739339
=== Actor Training Debug (Iteration 6665) ===
Q mean: -74.939713
Q std: 28.888451
Actor loss: 74.943703
Action reg: 0.003991
  l1.weight: grad_norm = 0.059117
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.139685
Total gradient norm: 0.262860
=== Actor Training Debug (Iteration 6666) ===
Q mean: -72.820656
Q std: 28.185650
Actor loss: 72.824654
Action reg: 0.003995
  l1.weight: grad_norm = 0.032897
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.077545
Total gradient norm: 0.143554
=== Actor Training Debug (Iteration 6667) ===
Q mean: -74.897797
Q std: 27.468962
Actor loss: 74.901779
Action reg: 0.003986
  l1.weight: grad_norm = 0.237029
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.591247
Total gradient norm: 1.198082
=== Actor Training Debug (Iteration 6668) ===
Q mean: -71.184021
Q std: 26.371439
Actor loss: 71.188004
Action reg: 0.003986
  l1.weight: grad_norm = 0.130904
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.264683
Total gradient norm: 0.433867
=== Actor Training Debug (Iteration 6669) ===
Q mean: -75.331543
Q std: 27.522278
Actor loss: 75.335533
Action reg: 0.003991
  l1.weight: grad_norm = 0.725404
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 1.316079
Total gradient norm: 2.266908
=== Actor Training Debug (Iteration 6670) ===
Q mean: -73.504669
Q std: 27.776747
Actor loss: 73.508667
Action reg: 0.003994
  l1.weight: grad_norm = 0.008251
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.020047
Total gradient norm: 0.044239
=== Actor Training Debug (Iteration 6671) ===
Q mean: -74.371674
Q std: 28.647667
Actor loss: 74.375664
Action reg: 0.003990
  l1.weight: grad_norm = 0.115336
  l1.bias: grad_norm = 0.000544
  l2.weight: grad_norm = 0.254286
Total gradient norm: 0.471052
=== Actor Training Debug (Iteration 6672) ===
Q mean: -74.490097
Q std: 26.326117
Actor loss: 74.494087
Action reg: 0.003991
  l1.weight: grad_norm = 0.245417
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.529465
Total gradient norm: 0.942969
=== Actor Training Debug (Iteration 6673) ===
Q mean: -74.066994
Q std: 28.357948
Actor loss: 74.070984
Action reg: 0.003987
  l1.weight: grad_norm = 0.431182
  l1.bias: grad_norm = 0.000690
  l2.weight: grad_norm = 0.855800
Total gradient norm: 1.703019
=== Actor Training Debug (Iteration 6674) ===
Q mean: -72.843895
Q std: 26.461338
Actor loss: 72.847885
Action reg: 0.003993
  l1.weight: grad_norm = 0.216584
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.461192
Total gradient norm: 0.868065
=== Actor Training Debug (Iteration 6675) ===
Q mean: -73.255089
Q std: 29.150511
Actor loss: 73.259071
Action reg: 0.003984
  l1.weight: grad_norm = 0.597002
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 1.036411
Total gradient norm: 1.955799
=== Actor Training Debug (Iteration 6676) ===
Q mean: -70.804626
Q std: 29.794081
Actor loss: 70.808609
Action reg: 0.003984
  l1.weight: grad_norm = 0.134232
  l1.bias: grad_norm = 0.000738
  l2.weight: grad_norm = 0.314529
Total gradient norm: 0.629852
=== Actor Training Debug (Iteration 6677) ===
Q mean: -75.586166
Q std: 28.139486
Actor loss: 75.590164
Action reg: 0.003997
  l1.weight: grad_norm = 0.013598
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.024945
Total gradient norm: 0.054580
=== Actor Training Debug (Iteration 6678) ===
Q mean: -70.614830
Q std: 26.676495
Actor loss: 70.618813
Action reg: 0.003986
  l1.weight: grad_norm = 0.349841
  l1.bias: grad_norm = 0.000725
  l2.weight: grad_norm = 0.741969
Total gradient norm: 1.325793
=== Actor Training Debug (Iteration 6679) ===
Q mean: -71.846519
Q std: 24.355141
Actor loss: 71.850517
Action reg: 0.003997
  l1.weight: grad_norm = 0.306488
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.658413
Total gradient norm: 1.163605
=== Actor Training Debug (Iteration 6680) ===
Q mean: -74.708580
Q std: 26.311197
Actor loss: 74.712563
Action reg: 0.003986
  l1.weight: grad_norm = 0.154932
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.380582
Total gradient norm: 0.799809
=== Actor Training Debug (Iteration 6681) ===
Q mean: -71.062134
Q std: 29.059174
Actor loss: 71.066116
Action reg: 0.003980
  l1.weight: grad_norm = 0.055101
  l1.bias: grad_norm = 0.001075
  l2.weight: grad_norm = 0.105214
Total gradient norm: 0.227239
=== Actor Training Debug (Iteration 6682) ===
Q mean: -73.801147
Q std: 29.690004
Actor loss: 73.805130
Action reg: 0.003981
  l1.weight: grad_norm = 0.442417
  l1.bias: grad_norm = 0.000695
  l2.weight: grad_norm = 0.788313
Total gradient norm: 1.529061
=== Actor Training Debug (Iteration 6683) ===
Q mean: -71.312698
Q std: 29.148134
Actor loss: 71.316673
Action reg: 0.003976
  l1.weight: grad_norm = 0.247139
  l1.bias: grad_norm = 0.000765
  l2.weight: grad_norm = 0.617420
Total gradient norm: 1.288192
=== Actor Training Debug (Iteration 6684) ===
Q mean: -72.313919
Q std: 28.911776
Actor loss: 72.317909
Action reg: 0.003987
  l1.weight: grad_norm = 0.203736
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.388140
Total gradient norm: 0.710123
=== Actor Training Debug (Iteration 6685) ===
Q mean: -73.368134
Q std: 27.664600
Actor loss: 73.372124
Action reg: 0.003988
  l1.weight: grad_norm = 0.188569
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.439730
Total gradient norm: 0.805081
=== Actor Training Debug (Iteration 6686) ===
Q mean: -73.203072
Q std: 27.814718
Actor loss: 73.207062
Action reg: 0.003987
  l1.weight: grad_norm = 0.137737
  l1.bias: grad_norm = 0.000476
  l2.weight: grad_norm = 0.292782
Total gradient norm: 0.598625
=== Actor Training Debug (Iteration 6687) ===
Q mean: -76.694649
Q std: 26.956045
Actor loss: 76.698647
Action reg: 0.003994
  l1.weight: grad_norm = 0.054523
  l1.bias: grad_norm = 0.000609
  l2.weight: grad_norm = 0.107056
Total gradient norm: 0.223107
=== Actor Training Debug (Iteration 6688) ===
Q mean: -72.622368
Q std: 26.946960
Actor loss: 72.626358
Action reg: 0.003992
  l1.weight: grad_norm = 0.240497
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.556086
Total gradient norm: 1.030184
=== Actor Training Debug (Iteration 6689) ===
Q mean: -71.835869
Q std: 29.135103
Actor loss: 71.839859
Action reg: 0.003987
  l1.weight: grad_norm = 0.472546
  l1.bias: grad_norm = 0.000540
  l2.weight: grad_norm = 0.942099
Total gradient norm: 1.746191
=== Actor Training Debug (Iteration 6690) ===
Q mean: -76.341240
Q std: 29.035686
Actor loss: 76.345230
Action reg: 0.003994
  l1.weight: grad_norm = 0.061882
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.120923
Total gradient norm: 0.227760
=== Actor Training Debug (Iteration 6691) ===
Q mean: -76.749413
Q std: 29.664623
Actor loss: 76.753410
Action reg: 0.003996
  l1.weight: grad_norm = 0.003539
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.008930
Total gradient norm: 0.021192
=== Actor Training Debug (Iteration 6692) ===
Q mean: -71.518089
Q std: 28.625174
Actor loss: 71.522072
Action reg: 0.003983
  l1.weight: grad_norm = 0.087249
  l1.bias: grad_norm = 0.000540
  l2.weight: grad_norm = 0.205833
Total gradient norm: 0.397520
=== Actor Training Debug (Iteration 6693) ===
Q mean: -73.058479
Q std: 27.454861
Actor loss: 73.062469
Action reg: 0.003991
  l1.weight: grad_norm = 0.212589
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.450506
Total gradient norm: 0.805049
=== Actor Training Debug (Iteration 6694) ===
Q mean: -75.951439
Q std: 27.589231
Actor loss: 75.955429
Action reg: 0.003994
  l1.weight: grad_norm = 0.158747
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.321530
Total gradient norm: 0.615555
=== Actor Training Debug (Iteration 6695) ===
Q mean: -73.305130
Q std: 27.586170
Actor loss: 73.309120
Action reg: 0.003988
  l1.weight: grad_norm = 0.067558
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.135528
Total gradient norm: 0.258867
=== Actor Training Debug (Iteration 6696) ===
Q mean: -71.659943
Q std: 28.932537
Actor loss: 71.663925
Action reg: 0.003986
  l1.weight: grad_norm = 0.191796
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.485316
Total gradient norm: 1.032724
=== Actor Training Debug (Iteration 6697) ===
Q mean: -75.635475
Q std: 28.055355
Actor loss: 75.639465
Action reg: 0.003993
  l1.weight: grad_norm = 0.020003
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.041716
Total gradient norm: 0.074126
=== Actor Training Debug (Iteration 6698) ===
Q mean: -72.589745
Q std: 28.710249
Actor loss: 72.593727
Action reg: 0.003980
  l1.weight: grad_norm = 0.106534
  l1.bias: grad_norm = 0.000787
  l2.weight: grad_norm = 0.242138
Total gradient norm: 0.451542
=== Actor Training Debug (Iteration 6699) ===
Q mean: -74.909134
Q std: 27.955603
Actor loss: 74.913124
Action reg: 0.003991
  l1.weight: grad_norm = 0.265844
  l1.bias: grad_norm = 0.000728
  l2.weight: grad_norm = 0.481673
Total gradient norm: 1.055963
=== Actor Training Debug (Iteration 6700) ===
Q mean: -74.096596
Q std: 26.380997
Actor loss: 74.100586
Action reg: 0.003992
  l1.weight: grad_norm = 0.120676
  l1.bias: grad_norm = 0.000685
  l2.weight: grad_norm = 0.258733
Total gradient norm: 0.487495
=== Actor Training Debug (Iteration 6701) ===
Q mean: -73.218796
Q std: 26.776920
Actor loss: 73.222786
Action reg: 0.003990
  l1.weight: grad_norm = 0.166101
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.370404
Total gradient norm: 0.730517
=== Actor Training Debug (Iteration 6702) ===
Q mean: -75.941277
Q std: 27.560047
Actor loss: 75.945267
Action reg: 0.003988
  l1.weight: grad_norm = 0.208298
  l1.bias: grad_norm = 0.000820
  l2.weight: grad_norm = 0.411325
Total gradient norm: 0.781648
=== Actor Training Debug (Iteration 6703) ===
Q mean: -72.215477
Q std: 27.076948
Actor loss: 72.219475
Action reg: 0.003995
  l1.weight: grad_norm = 0.185793
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.383088
Total gradient norm: 0.700905
=== Actor Training Debug (Iteration 6704) ===
Q mean: -75.170212
Q std: 29.080622
Actor loss: 75.174202
Action reg: 0.003991
  l1.weight: grad_norm = 0.397283
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.896470
Total gradient norm: 1.462712
=== Actor Training Debug (Iteration 6705) ===
Q mean: -74.087914
Q std: 28.548079
Actor loss: 74.091904
Action reg: 0.003988
  l1.weight: grad_norm = 0.483054
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 1.152557
Total gradient norm: 1.934327
=== Actor Training Debug (Iteration 6706) ===
Q mean: -76.239487
Q std: 28.135593
Actor loss: 76.243477
Action reg: 0.003989
  l1.weight: grad_norm = 0.063368
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.127966
Total gradient norm: 0.210968
=== Actor Training Debug (Iteration 6707) ===
Q mean: -75.809738
Q std: 27.549917
Actor loss: 75.813728
Action reg: 0.003988
  l1.weight: grad_norm = 0.143734
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.346381
Total gradient norm: 0.631691
=== Actor Training Debug (Iteration 6708) ===
Q mean: -74.428925
Q std: 26.683083
Actor loss: 74.432915
Action reg: 0.003993
  l1.weight: grad_norm = 0.115563
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.237085
Total gradient norm: 0.438159
=== Actor Training Debug (Iteration 6709) ===
Q mean: -75.371979
Q std: 27.188517
Actor loss: 75.375969
Action reg: 0.003991
  l1.weight: grad_norm = 0.318145
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.628319
Total gradient norm: 1.221313
=== Actor Training Debug (Iteration 6710) ===
Q mean: -72.490860
Q std: 26.790190
Actor loss: 72.494850
Action reg: 0.003989
  l1.weight: grad_norm = 0.331950
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.722596
Total gradient norm: 1.263040
=== Actor Training Debug (Iteration 6711) ===
Q mean: -75.975128
Q std: 28.589706
Actor loss: 75.979118
Action reg: 0.003991
  l1.weight: grad_norm = 0.320538
  l1.bias: grad_norm = 0.000884
  l2.weight: grad_norm = 0.609466
Total gradient norm: 1.307933
=== Actor Training Debug (Iteration 6712) ===
Q mean: -71.653320
Q std: 29.255220
Actor loss: 71.657303
Action reg: 0.003985
  l1.weight: grad_norm = 0.169866
  l1.bias: grad_norm = 0.000589
  l2.weight: grad_norm = 0.335607
Total gradient norm: 0.625047
=== Actor Training Debug (Iteration 6713) ===
Q mean: -75.012672
Q std: 28.897963
Actor loss: 75.016663
Action reg: 0.003990
  l1.weight: grad_norm = 0.348351
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.728390
Total gradient norm: 1.473813
=== Actor Training Debug (Iteration 6714) ===
Q mean: -75.166962
Q std: 28.349758
Actor loss: 75.170952
Action reg: 0.003989
  l1.weight: grad_norm = 0.023304
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.048110
Total gradient norm: 0.097511
=== Actor Training Debug (Iteration 6715) ===
Q mean: -77.986900
Q std: 26.965868
Actor loss: 77.990898
Action reg: 0.003994
  l1.weight: grad_norm = 0.079853
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.205953
Total gradient norm: 0.418504
=== Actor Training Debug (Iteration 6716) ===
Q mean: -76.306557
Q std: 27.135216
Actor loss: 76.310547
Action reg: 0.003991
  l1.weight: grad_norm = 0.103417
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.210742
Total gradient norm: 0.344123
=== Actor Training Debug (Iteration 6717) ===
Q mean: -74.178383
Q std: 28.016010
Actor loss: 74.182381
Action reg: 0.003995
  l1.weight: grad_norm = 0.155314
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.349492
Total gradient norm: 0.672281
=== Actor Training Debug (Iteration 6718) ===
Q mean: -74.460693
Q std: 28.186905
Actor loss: 74.464668
Action reg: 0.003976
  l1.weight: grad_norm = 0.191949
  l1.bias: grad_norm = 0.001212
  l2.weight: grad_norm = 0.393659
Total gradient norm: 0.714031
=== Actor Training Debug (Iteration 6719) ===
Q mean: -74.847183
Q std: 28.015362
Actor loss: 74.851166
Action reg: 0.003986
  l1.weight: grad_norm = 0.206996
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.512434
Total gradient norm: 1.012661
=== Actor Training Debug (Iteration 6720) ===
Q mean: -71.980164
Q std: 26.483677
Actor loss: 71.984146
Action reg: 0.003986
  l1.weight: grad_norm = 0.308922
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.677460
Total gradient norm: 1.285315
=== Actor Training Debug (Iteration 6721) ===
Q mean: -70.582512
Q std: 29.672518
Actor loss: 70.586494
Action reg: 0.003982
  l1.weight: grad_norm = 0.393183
  l1.bias: grad_norm = 0.001098
  l2.weight: grad_norm = 0.858524
Total gradient norm: 1.546665
=== Actor Training Debug (Iteration 6722) ===
Q mean: -75.936890
Q std: 26.763315
Actor loss: 75.940872
Action reg: 0.003986
  l1.weight: grad_norm = 0.168429
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.382522
Total gradient norm: 0.740040
=== Actor Training Debug (Iteration 6723) ===
Q mean: -73.520340
Q std: 27.593435
Actor loss: 73.524323
Action reg: 0.003986
  l1.weight: grad_norm = 0.272710
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.528219
Total gradient norm: 0.893551
=== Actor Training Debug (Iteration 6724) ===
Q mean: -72.017899
Q std: 28.817770
Actor loss: 72.021889
Action reg: 0.003987
  l1.weight: grad_norm = 0.311315
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.731755
Total gradient norm: 1.360131
=== Actor Training Debug (Iteration 6725) ===
Q mean: -75.257545
Q std: 28.303829
Actor loss: 75.261536
Action reg: 0.003989
  l1.weight: grad_norm = 1.563946
  l1.bias: grad_norm = 0.002626
  l2.weight: grad_norm = 3.517582
Total gradient norm: 6.985867
=== Actor Training Debug (Iteration 6726) ===
Q mean: -74.618668
Q std: 27.015743
Actor loss: 74.622650
Action reg: 0.003979
  l1.weight: grad_norm = 0.201356
  l1.bias: grad_norm = 0.000552
  l2.weight: grad_norm = 0.382062
Total gradient norm: 0.645955
=== Actor Training Debug (Iteration 6727) ===
Q mean: -76.574966
Q std: 27.599981
Actor loss: 76.578949
Action reg: 0.003984
  l1.weight: grad_norm = 0.210427
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.461348
Total gradient norm: 0.898595
=== Actor Training Debug (Iteration 6728) ===
Q mean: -73.930191
Q std: 26.838751
Actor loss: 73.934174
Action reg: 0.003985
  l1.weight: grad_norm = 0.232994
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.472798
Total gradient norm: 0.871258
=== Actor Training Debug (Iteration 6729) ===
Q mean: -71.985992
Q std: 28.257286
Actor loss: 71.989983
Action reg: 0.003989
  l1.weight: grad_norm = 0.294203
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.715670
Total gradient norm: 1.220530
=== Actor Training Debug (Iteration 6730) ===
Q mean: -74.371628
Q std: 27.057209
Actor loss: 74.375618
Action reg: 0.003988
  l1.weight: grad_norm = 0.337961
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.738713
Total gradient norm: 1.501903
=== Actor Training Debug (Iteration 6731) ===
Q mean: -74.481186
Q std: 29.030474
Actor loss: 74.485176
Action reg: 0.003990
  l1.weight: grad_norm = 0.085160
  l1.bias: grad_norm = 0.000681
  l2.weight: grad_norm = 0.162571
Total gradient norm: 0.305116
Total gradient norm: 1.307274ration 6486) ===
=== Actor Training Debug (Iteration 6742) ===
Q mean: -77.311829
Q std: 27.382896
Actor loss: 77.315819
Action reg: 0.003993
  l1.weight: grad_norm = 0.256642
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.536884
Total gradient norm: 0.926550
=== Actor Training Debug (Iteration 6743) ===
Q mean: -75.805748
Q std: 28.467634
Actor loss: 75.809731
Action reg: 0.003981
  l1.weight: grad_norm = 0.247240
  l1.bias: grad_norm = 0.001395
  l2.weight: grad_norm = 0.607977
Total gradient norm: 1.158150
=== Actor Training Debug (Iteration 6744) ===
Q mean: -74.882904
Q std: 27.613941
Actor loss: 74.886894
Action reg: 0.003991
  l1.weight: grad_norm = 0.112298
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.245526
Total gradient norm: 0.525185
=== Actor Training Debug (Iteration 6745) ===
Q mean: -75.038017
Q std: 27.702774
Actor loss: 75.042007
Action reg: 0.003993
  l1.weight: grad_norm = 0.177983
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.401267
Total gradient norm: 0.667984
=== Actor Training Debug (Iteration 6746) ===
Q mean: -78.230164
Q std: 27.999357
Actor loss: 78.234154
Action reg: 0.003989
  l1.weight: grad_norm = 0.040530
  l1.bias: grad_norm = 0.000895
  l2.weight: grad_norm = 0.083471
Total gradient norm: 0.167188
=== Actor Training Debug (Iteration 6747) ===
Q mean: -77.896225
Q std: 28.692844
Actor loss: 77.900208
Action reg: 0.003985
  l1.weight: grad_norm = 0.534904
  l1.bias: grad_norm = 0.000559
  l2.weight: grad_norm = 1.203741
Total gradient norm: 1.955150
=== Actor Training Debug (Iteration 6748) ===
Q mean: -74.541824
Q std: 28.640728
Actor loss: 74.545807
Action reg: 0.003986
  l1.weight: grad_norm = 0.451774
  l1.bias: grad_norm = 0.000742
  l2.weight: grad_norm = 0.946067
Total gradient norm: 1.673210
=== Actor Training Debug (Iteration 6749) ===
Q mean: -70.117966
Q std: 28.779518
Actor loss: 70.121956
Action reg: 0.003991
  l1.weight: grad_norm = 0.253628
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.559667
Total gradient norm: 1.042910
=== Actor Training Debug (Iteration 6750) ===
Q mean: -75.538055
Q std: 29.811419
Actor loss: 75.542030
Action reg: 0.003978
  l1.weight: grad_norm = 0.222229
  l1.bias: grad_norm = 0.001261
  l2.weight: grad_norm = 0.405120
Total gradient norm: 0.841956
=== Actor Training Debug (Iteration 6751) ===
Q mean: -70.491196
Q std: 28.418661
Actor loss: 70.495178
Action reg: 0.003986
  l1.weight: grad_norm = 0.081289
  l1.bias: grad_norm = 0.000835
  l2.weight: grad_norm = 0.193288
Total gradient norm: 0.396181
=== Actor Training Debug (Iteration 6752) ===
Q mean: -72.807053
Q std: 28.252077
Actor loss: 72.811043
Action reg: 0.003990
  l1.weight: grad_norm = 0.247370
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.563335
Total gradient norm: 1.149800
=== Actor Training Debug (Iteration 6753) ===
Q mean: -73.796669
Q std: 27.118395
Actor loss: 73.800652
Action reg: 0.003985
  l1.weight: grad_norm = 0.209513
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.454101
Total gradient norm: 0.845763
=== Actor Training Debug (Iteration 6754) ===
Q mean: -73.126953
Q std: 27.780724
Actor loss: 73.130943
Action reg: 0.003987
  l1.weight: grad_norm = 0.165216
  l1.bias: grad_norm = 0.000882
  l2.weight: grad_norm = 0.330800
Total gradient norm: 0.595856
=== Actor Training Debug (Iteration 6755) ===
Q mean: -73.976456
Q std: 27.179476
Actor loss: 73.980453
Action reg: 0.003996
  l1.weight: grad_norm = 0.089492
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.163223
Total gradient norm: 0.286942
=== Actor Training Debug (Iteration 6756) ===
Q mean: -71.121544
Q std: 28.773384
Actor loss: 71.125526
Action reg: 0.003985
  l1.weight: grad_norm = 0.157928
  l1.bias: grad_norm = 0.000830
  l2.weight: grad_norm = 0.370710
Total gradient norm: 0.734343
=== Actor Training Debug (Iteration 6757) ===
Q mean: -71.743149
Q std: 29.704092
Actor loss: 71.747131
Action reg: 0.003980
  l1.weight: grad_norm = 0.278189
  l1.bias: grad_norm = 0.001085
  l2.weight: grad_norm = 0.589632
Total gradient norm: 1.073675
=== Actor Training Debug (Iteration 6758) ===
Q mean: -72.596672
Q std: 29.636436
Actor loss: 72.600662
Action reg: 0.003987
  l1.weight: grad_norm = 0.223038
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.496509
Total gradient norm: 0.922996
=== Actor Training Debug (Iteration 6759) ===
Q mean: -74.245369
Q std: 29.572182
Actor loss: 74.249359
Action reg: 0.003991
  l1.weight: grad_norm = 0.151527
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.262666
Total gradient norm: 0.409099
=== Actor Training Debug (Iteration 6760) ===
Q mean: -71.987350
Q std: 27.815401
Actor loss: 71.991341
Action reg: 0.003988
  l1.weight: grad_norm = 0.333245
  l1.bias: grad_norm = 0.000798
  l2.weight: grad_norm = 0.759548
Total gradient norm: 1.496608
=== Actor Training Debug (Iteration 6761) ===
Q mean: -72.230614
Q std: 26.141518
Actor loss: 72.234604
Action reg: 0.003993
  l1.weight: grad_norm = 0.110241
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.257936
Total gradient norm: 0.526601
=== Actor Training Debug (Iteration 6762) ===
Q mean: -71.995865
Q std: 28.006004
Actor loss: 71.999855
Action reg: 0.003988
  l1.weight: grad_norm = 0.201460
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.391196
Total gradient norm: 0.670114
=== Actor Training Debug (Iteration 6763) ===
Q mean: -76.052338
Q std: 26.844305
Actor loss: 76.056328
Action reg: 0.003991
  l1.weight: grad_norm = 0.522833
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 1.115101
Total gradient norm: 1.948837
=== Actor Training Debug (Iteration 6764) ===
Q mean: -74.253677
Q std: 27.875002
Actor loss: 74.257668
Action reg: 0.003992
  l1.weight: grad_norm = 0.065736
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.143793
Total gradient norm: 0.269624
=== Actor Training Debug (Iteration 6765) ===
Q mean: -74.868591
Q std: 28.080143
Actor loss: 74.872581
Action reg: 0.003991
  l1.weight: grad_norm = 0.933989
  l1.bias: grad_norm = 0.001445
  l2.weight: grad_norm = 1.675057
Total gradient norm: 3.875106
=== Actor Training Debug (Iteration 6766) ===
Q mean: -77.899292
Q std: 26.946188
Actor loss: 77.903282
Action reg: 0.003989
  l1.weight: grad_norm = 0.236943
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.588451
Total gradient norm: 1.100220
=== Actor Training Debug (Iteration 6767) ===
Q mean: -74.796844
Q std: 27.947720
Actor loss: 74.800835
Action reg: 0.003992
  l1.weight: grad_norm = 0.063466
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.134421
Total gradient norm: 0.244402
=== Actor Training Debug (Iteration 6768) ===
Q mean: -72.879074
Q std: 27.730478
Actor loss: 72.883064
Action reg: 0.003988
  l1.weight: grad_norm = 0.122876
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.268105
Total gradient norm: 0.446742
=== Actor Training Debug (Iteration 6769) ===
Q mean: -75.028999
Q std: 30.653946
Actor loss: 75.032982
Action reg: 0.003984
  l1.weight: grad_norm = 0.010378
  l1.bias: grad_norm = 0.000991
  l2.weight: grad_norm = 0.029069
Total gradient norm: 0.070372
=== Actor Training Debug (Iteration 6770) ===
Q mean: -76.241508
Q std: 27.910986
Actor loss: 76.245499
Action reg: 0.003993
  l1.weight: grad_norm = 0.100151
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.221500
Total gradient norm: 0.394534
=== Actor Training Debug (Iteration 6771) ===
Q mean: -70.905151
Q std: 28.250399
Actor loss: 70.909134
Action reg: 0.003983
  l1.weight: grad_norm = 0.251085
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.553302
Total gradient norm: 1.032732
=== Actor Training Debug (Iteration 6772) ===
Q mean: -74.936913
Q std: 28.375120
Actor loss: 74.940895
Action reg: 0.003983
  l1.weight: grad_norm = 0.436490
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.963619
Total gradient norm: 1.607056
=== Actor Training Debug (Iteration 6773) ===
Q mean: -77.953766
Q std: 27.605909
Actor loss: 77.957756
Action reg: 0.003988
  l1.weight: grad_norm = 0.149086
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.319746
Total gradient norm: 0.589578
=== Actor Training Debug (Iteration 6774) ===
Q mean: -74.923988
Q std: 27.470497
Actor loss: 74.927979
Action reg: 0.003988
  l1.weight: grad_norm = 0.018232
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.028737
Total gradient norm: 0.043628
=== Actor Training Debug (Iteration 6775) ===
Q mean: -75.143196
Q std: 28.512354
Actor loss: 75.147186
Action reg: 0.003991
  l1.weight: grad_norm = 0.158876
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.435363
Total gradient norm: 0.871696
=== Actor Training Debug (Iteration 6776) ===
Q mean: -70.569389
Q std: 29.339111
Actor loss: 70.573380
Action reg: 0.003990
  l1.weight: grad_norm = 0.071485
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.168312
Total gradient norm: 0.363693
=== Actor Training Debug (Iteration 6777) ===
Q mean: -76.394272
Q std: 27.625212
Actor loss: 76.398262
Action reg: 0.003994
  l1.weight: grad_norm = 0.291991
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.693587
Total gradient norm: 1.380288
=== Actor Training Debug (Iteration 6778) ===
Q mean: -74.873779
Q std: 30.098106
Actor loss: 74.877762
Action reg: 0.003981
  l1.weight: grad_norm = 0.063968
  l1.bias: grad_norm = 0.000848
  l2.weight: grad_norm = 0.168358
Total gradient norm: 0.385561
=== Actor Training Debug (Iteration 6779) ===
Q mean: -72.697144
Q std: 27.782974
Actor loss: 72.701126
Action reg: 0.003984
  l1.weight: grad_norm = 0.203858
  l1.bias: grad_norm = 0.000835
  l2.weight: grad_norm = 0.454175
Total gradient norm: 0.886126
=== Actor Training Debug (Iteration 6780) ===
Q mean: -74.229881
Q std: 27.704485
Actor loss: 74.233879
Action reg: 0.003996
  l1.weight: grad_norm = 0.148348
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.352897
Total gradient norm: 0.666495
=== Actor Training Debug (Iteration 6781) ===
Q mean: -74.447403
Q std: 28.502380
Actor loss: 74.451393
Action reg: 0.003989
  l1.weight: grad_norm = 0.070877
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.176683
Total gradient norm: 0.379720
=== Actor Training Debug (Iteration 6782) ===
Q mean: -74.234070
Q std: 28.671398
Actor loss: 74.238060
Action reg: 0.003990
  l1.weight: grad_norm = 0.088333
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.182135
Total gradient norm: 0.333621
=== Actor Training Debug (Iteration 6783) ===
Q mean: -73.480667
Q std: 28.573063
Actor loss: 73.484657
Action reg: 0.003987
  l1.weight: grad_norm = 0.294460
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.666291
Total gradient norm: 1.326885
=== Actor Training Debug (Iteration 6784) ===
Q mean: -73.376419
Q std: 28.044069
Actor loss: 73.380402
Action reg: 0.003982
  l1.weight: grad_norm = 0.307217
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.628298
Total gradient norm: 1.242453
=== Actor Training Debug (Iteration 6785) ===
Q mean: -75.159576
Q std: 29.016624
Actor loss: 75.163567
Action reg: 0.003989
  l1.weight: grad_norm = 0.211474
  l1.bias: grad_norm = 0.000728
  l2.weight: grad_norm = 0.375359
Total gradient norm: 0.680224
=== Actor Training Debug (Iteration 6786) ===
Q mean: -70.439491
Q std: 28.629753
Actor loss: 70.443481
Action reg: 0.003993
  l1.weight: grad_norm = 0.144450
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.332801
Total gradient norm: 0.588387
=== Actor Training Debug (Iteration 6787) ===
Q mean: -74.543274
Q std: 27.100990
Actor loss: 74.547272
Action reg: 0.003996
  l1.weight: grad_norm = 0.077557
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.169478
Total gradient norm: 0.343248
=== Actor Training Debug (Iteration 6788) ===
Q mean: -72.060349
Q std: 28.267996
Actor loss: 72.064339
Action reg: 0.003988
  l1.weight: grad_norm = 0.277338
  l1.bias: grad_norm = 0.000693
  l2.weight: grad_norm = 0.529335
Total gradient norm: 0.968542
=== Actor Training Debug (Iteration 6789) ===
Q mean: -75.538467
Q std: 28.217319
Actor loss: 75.542458
Action reg: 0.003993
  l1.weight: grad_norm = 0.126755
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.321010
Total gradient norm: 0.642999
=== Actor Training Debug (Iteration 6790) ===
Q mean: -73.446976
Q std: 28.562004
Actor loss: 73.450966
Action reg: 0.003987
  l1.weight: grad_norm = 0.088327
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.171194
Total gradient norm: 0.323377
=== Actor Training Debug (Iteration 6791) ===
Q mean: -73.641159
Q std: 29.035885
Actor loss: 73.645149
Action reg: 0.003991
  l1.weight: grad_norm = 0.082132
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.192293
Total gradient norm: 0.323564
=== Actor Training Debug (Iteration 6792) ===
Q mean: -72.393074
Q std: 29.381264
Actor loss: 72.397057
Action reg: 0.003986
  l1.weight: grad_norm = 0.212424
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.427108
Total gradient norm: 0.786356
=== Actor Training Debug (Iteration 6793) ===
Q mean: -76.794762
Q std: 27.391205
Actor loss: 76.798752
Action reg: 0.003987
  l1.weight: grad_norm = 0.164613
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.398025
Total gradient norm: 0.801198
=== Actor Training Debug (Iteration 6794) ===
Q mean: -72.669739
Q std: 27.306776
Actor loss: 72.673729
Action reg: 0.003990
  l1.weight: grad_norm = 0.110573
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.258265
Total gradient norm: 0.503349
=== Actor Training Debug (Iteration 6795) ===
Q mean: -74.767822
Q std: 27.552759
Actor loss: 74.771812
Action reg: 0.003993
  l1.weight: grad_norm = 0.400393
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.863067
Total gradient norm: 1.531976
=== Actor Training Debug (Iteration 6796) ===
Q mean: -72.431259
Q std: 28.806280
Actor loss: 72.435242
Action reg: 0.003984
  l1.weight: grad_norm = 0.343394
  l1.bias: grad_norm = 0.001135
  l2.weight: grad_norm = 0.649954
Total gradient norm: 1.261451
=== Actor Training Debug (Iteration 6797) ===
Q mean: -74.576767
Q std: 28.209002
Actor loss: 74.580757
Action reg: 0.003992
  l1.weight: grad_norm = 0.427754
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.781024
Total gradient norm: 1.338711
=== Actor Training Debug (Iteration 6798) ===
Q mean: -71.989159
Q std: 28.141247
Actor loss: 71.993149
Action reg: 0.003990
  l1.weight: grad_norm = 0.145131
  l1.bias: grad_norm = 0.000707
  l2.weight: grad_norm = 0.313191
Total gradient norm: 0.588518
=== Actor Training Debug (Iteration 6799) ===
Q mean: -74.418701
Q std: 28.714653
Actor loss: 74.422691
Action reg: 0.003990
  l1.weight: grad_norm = 0.330279
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.774339
Total gradient norm: 1.437598
=== Actor Training Debug (Iteration 6800) ===
Q mean: -73.480156
Q std: 29.314339
Actor loss: 73.484138
Action reg: 0.003984
  l1.weight: grad_norm = 0.183599
  l1.bias: grad_norm = 0.000798
  l2.weight: grad_norm = 0.392108
Total gradient norm: 0.868719
=== Actor Training Debug (Iteration 6801) ===
Q mean: -72.235985
Q std: 27.989660
Actor loss: 72.239975
Action reg: 0.003987
  l1.weight: grad_norm = 0.306200
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.602547
Total gradient norm: 1.052384
=== Actor Training Debug (Iteration 6802) ===
Q mean: -71.297470
Q std: 28.146152
Actor loss: 71.301460
Action reg: 0.003994
  l1.weight: grad_norm = 0.129787
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.244392
Total gradient norm: 0.412935
=== Actor Training Debug (Iteration 6803) ===
Q mean: -75.681656
Q std: 27.372068
Actor loss: 75.685646
Action reg: 0.003991
  l1.weight: grad_norm = 0.301098
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.514515
Total gradient norm: 1.059896
=== Actor Training Debug (Iteration 6804) ===
Q mean: -75.371262
Q std: 26.879217
Actor loss: 75.375252
Action reg: 0.003987
  l1.weight: grad_norm = 0.170775
  l1.bias: grad_norm = 0.000494
  l2.weight: grad_norm = 0.396825
Total gradient norm: 0.739902
=== Actor Training Debug (Iteration 6805) ===
Q mean: -71.458115
Q std: 27.814714
Actor loss: 71.462105
Action reg: 0.003992
  l1.weight: grad_norm = 0.237741
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.536942
Total gradient norm: 0.891077
=== Actor Training Debug (Iteration 6806) ===
Q mean: -74.788116
Q std: 29.625217
Actor loss: 74.792107
Action reg: 0.003989
  l1.weight: grad_norm = 0.232119
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.486868
Total gradient norm: 0.847856
=== Actor Training Debug (Iteration 6807) ===
Q mean: -75.044411
Q std: 27.354881
Actor loss: 75.048401
Action reg: 0.003992
  l1.weight: grad_norm = 0.196382
  l1.bias: grad_norm = 0.000546
  l2.weight: grad_norm = 0.384945
Total gradient norm: 0.693509
=== Actor Training Debug (Iteration 6808) ===
Q mean: -73.691734
Q std: 28.352657
Actor loss: 73.695724
Action reg: 0.003994
  l1.weight: grad_norm = 0.079037
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.187678
Total gradient norm: 0.459450
=== Actor Training Debug (Iteration 6809) ===
Q mean: -71.622009
Q std: 28.373892
Actor loss: 71.625992
Action reg: 0.003986
  l1.weight: grad_norm = 0.119450
  l1.bias: grad_norm = 0.001080
  l2.weight: grad_norm = 0.260018
Total gradient norm: 0.514437
=== Actor Training Debug (Iteration 6810) ===
Q mean: -75.097862
Q std: 27.807175
Actor loss: 75.101852
Action reg: 0.003988
  l1.weight: grad_norm = 0.294300
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.620416
Total gradient norm: 1.112700
=== Actor Training Debug (Iteration 6811) ===
Q mean: -74.474426
Q std: 27.180794
Actor loss: 74.478416
Action reg: 0.003989
  l1.weight: grad_norm = 0.127698
  l1.bias: grad_norm = 0.000660
  l2.weight: grad_norm = 0.286102
Total gradient norm: 0.518701
=== Actor Training Debug (Iteration 6812) ===
Q mean: -74.429016
Q std: 29.867090
Actor loss: 74.432999
Action reg: 0.003982
  l1.weight: grad_norm = 0.233838
  l1.bias: grad_norm = 0.001487
  l2.weight: grad_norm = 0.545562
Total gradient norm: 1.086505
=== Actor Training Debug (Iteration 6813) ===
Q mean: -73.251511
Q std: 28.471798
Actor loss: 73.255501
Action reg: 0.003988
  l1.weight: grad_norm = 0.181784
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.431391
Total gradient norm: 0.823407
=== Actor Training Debug (Iteration 6814) ===
Q mean: -69.231621
Q std: 28.412687
Actor loss: 69.235611
Action reg: 0.003993
  l1.weight: grad_norm = 0.062024
  l1.bias: grad_norm = 0.000536
  l2.weight: grad_norm = 0.120524
Total gradient norm: 0.210613
=== Actor Training Debug (Iteration 6815) ===
Q mean: -71.735970
Q std: 27.407850
Actor loss: 71.739960
Action reg: 0.003989
  l1.weight: grad_norm = 0.080199
  l1.bias: grad_norm = 0.000677
  l2.weight: grad_norm = 0.165263
Total gradient norm: 0.328790
=== Actor Training Debug (Iteration 6816) ===
Q mean: -76.417267
Q std: 27.657587
Actor loss: 76.421257
Action reg: 0.003988
  l1.weight: grad_norm = 0.231867
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.427412
Total gradient norm: 0.738420
=== Actor Training Debug (Iteration 6817) ===
Q mean: -72.867050
Q std: 27.649899
Actor loss: 72.871040
Action reg: 0.003990
  l1.weight: grad_norm = 0.191108
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.418658
Total gradient norm: 0.870000
=== Actor Training Debug (Iteration 6818) ===
Q mean: -73.066986
Q std: 29.162064
Actor loss: 73.070976
Action reg: 0.003988
  l1.weight: grad_norm = 0.073746
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.167378
Total gradient norm: 0.377804
=== Actor Training Debug (Iteration 6819) ===
Q mean: -76.068970
Q std: 28.572660
Actor loss: 76.072960
Action reg: 0.003989
  l1.weight: grad_norm = 0.309403
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.574564
Total gradient norm: 1.000976
=== Actor Training Debug (Iteration 6820) ===
Q mean: -75.713196
Q std: 28.428135
Actor loss: 75.717186
Action reg: 0.003988
  l1.weight: grad_norm = 0.123849
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.344228
Total gradient norm: 0.801343
=== Actor Training Debug (Iteration 6821) ===
Q mean: -73.713669
Q std: 27.917019
Actor loss: 73.717651
Action reg: 0.003984
  l1.weight: grad_norm = 0.179156
  l1.bias: grad_norm = 0.000640
  l2.weight: grad_norm = 0.386812
Total gradient norm: 0.710517
=== Actor Training Debug (Iteration 6822) ===
Q mean: -74.137550
Q std: 28.491014
Actor loss: 74.141541
Action reg: 0.003991
  l1.weight: grad_norm = 0.089182
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.172049
Total gradient norm: 0.303594
=== Actor Training Debug (Iteration 6823) ===
Q mean: -76.824387
Q std: 28.079767
Actor loss: 76.828377
Action reg: 0.003991
  l1.weight: grad_norm = 0.078050
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.166241
Total gradient norm: 0.299857
=== Actor Training Debug (Iteration 6824) ===
Q mean: -77.176666
Q std: 26.380268
Actor loss: 77.180656
Action reg: 0.003992
  l1.weight: grad_norm = 0.077685
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.187966
Total gradient norm: 0.373657
=== Actor Training Debug (Iteration 6825) ===
Q mean: -76.079834
Q std: 28.297432
Actor loss: 76.083817
Action reg: 0.003986
  l1.weight: grad_norm = 0.324369
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.652753
Total gradient norm: 1.370274
=== Actor Training Debug (Iteration 6826) ===
Q mean: -74.480911
Q std: 27.877409
Actor loss: 74.484894
Action reg: 0.003986
  l1.weight: grad_norm = 0.295430
  l1.bias: grad_norm = 0.000498
  l2.weight: grad_norm = 0.657095
Total gradient norm: 1.348161
=== Actor Training Debug (Iteration 6827) ===
Q mean: -76.001259
Q std: 28.972178
Actor loss: 76.005241
Action reg: 0.003979
  l1.weight: grad_norm = 0.241224
  l1.bias: grad_norm = 0.000906
  l2.weight: grad_norm = 0.561980
Total gradient norm: 1.094444
=== Actor Training Debug (Iteration 6828) ===
Q mean: -78.216896
Q std: 28.127697
Actor loss: 78.220886
Action reg: 0.003987
  l1.weight: grad_norm = 0.270627
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.608189
Total gradient norm: 1.308459
=== Actor Training Debug (Iteration 6829) ===
Q mean: -73.223450
Q std: 27.092823
Actor loss: 73.227440
Action reg: 0.003989
  l1.weight: grad_norm = 0.262801
  l1.bias: grad_norm = 0.000652
  l2.weight: grad_norm = 0.453480
Total gradient norm: 0.680229
=== Actor Training Debug (Iteration 6830) ===
Q mean: -77.688812
Q std: 27.618750
Actor loss: 77.692802
Action reg: 0.003989
  l1.weight: grad_norm = 0.506781
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 1.153801
Total gradient norm: 2.285367
=== Actor Training Debug (Iteration 6831) ===
Q mean: -75.309189
Q std: 29.668997
Actor loss: 75.313179
Action reg: 0.003988
  l1.weight: grad_norm = 0.069137
  l1.bias: grad_norm = 0.000973
  l2.weight: grad_norm = 0.147246
Total gradient norm: 0.270474
=== Actor Training Debug (Iteration 6832) ===
Q mean: -73.554405
Q std: 28.405975
Actor loss: 73.558395
Action reg: 0.003989
  l1.weight: grad_norm = 0.228742
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.498940
Total gradient norm: 0.924918
=== Actor Training Debug (Iteration 6833) ===
Q mean: -72.572098
Q std: 26.579294
Actor loss: 72.576088
Action reg: 0.003993
  l1.weight: grad_norm = 0.085419
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.202494
Total gradient norm: 0.385543
=== Actor Training Debug (Iteration 6834) ===
Q mean: -72.546715
Q std: 27.945677
Actor loss: 72.550713
Action reg: 0.003997
  l1.weight: grad_norm = 0.014177
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.035968
Total gradient norm: 0.070656
=== Actor Training Debug (Iteration 6835) ===
Q mean: -77.627556
Q std: 28.328360
Actor loss: 77.631546
Action reg: 0.003992
  l1.weight: grad_norm = 0.020609
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.049413
Total gradient norm: 0.095793
=== Actor Training Debug (Iteration 6836) ===
Q mean: -78.022232
Q std: 26.792845
Actor loss: 78.026222
Action reg: 0.003988
  l1.weight: grad_norm = 0.252254
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.638261
Total gradient norm: 1.332293
=== Actor Training Debug (Iteration 6837) ===
Q mean: -73.809265
Q std: 27.010719
Actor loss: 73.813255
Action reg: 0.003990
  l1.weight: grad_norm = 0.210763
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.495161
Total gradient norm: 0.958652
=== Actor Training Debug (Iteration 6838) ===
Q mean: -76.877396
Q std: 29.773401
Actor loss: 76.881378
Action reg: 0.003984
  l1.weight: grad_norm = 0.391876
  l1.bias: grad_norm = 0.000606
  l2.weight: grad_norm = 0.847251
Total gradient norm: 1.876527
=== Actor Training Debug (Iteration 6839) ===
Q mean: -71.956856
Q std: 27.328669
Actor loss: 71.960838
Action reg: 0.003984
  l1.weight: grad_norm = 0.290150
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.590411
Total gradient norm: 1.152811
=== Actor Training Debug (Iteration 6840) ===
Q mean: -74.282448
Q std: 28.885891
Actor loss: 74.286430
Action reg: 0.003983
  l1.weight: grad_norm = 0.333505
  l1.bias: grad_norm = 0.000659
  l2.weight: grad_norm = 0.662161
Total gradient norm: 1.236829
=== Actor Training Debug (Iteration 6841) ===
Q mean: -74.927063
Q std: 29.658461
Actor loss: 74.931046
Action reg: 0.003984
  l1.weight: grad_norm = 0.130139
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.261998
Total gradient norm: 0.504733
=== Actor Training Debug (Iteration 6842) ===
Q mean: -72.478806
Q std: 27.766895
Actor loss: 72.482796
Action reg: 0.003992
  l1.weight: grad_norm = 0.007112
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.015260
Total gradient norm: 0.028513
=== Actor Training Debug (Iteration 6843) ===
Q mean: -71.582962
Q std: 28.240971
Actor loss: 71.586945
Action reg: 0.003980
  l1.weight: grad_norm = 0.211012
  l1.bias: grad_norm = 0.000904
  l2.weight: grad_norm = 0.369260
Total gradient norm: 0.636640
=== Actor Training Debug (Iteration 6844) ===
Q mean: -75.338371
Q std: 28.780405
Actor loss: 75.342361
Action reg: 0.003987
  l1.weight: grad_norm = 0.505193
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 1.126152
Total gradient norm: 1.777316
=== Actor Training Debug (Iteration 6845) ===
Q mean: -77.027939
Q std: 28.303774
Actor loss: 77.031929
Action reg: 0.003988
  l1.weight: grad_norm = 0.131678
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.301698
Total gradient norm: 0.610612
=== Actor Training Debug (Iteration 6846) ===
Q mean: -76.818764
Q std: 27.356190
Actor loss: 76.822762
Action reg: 0.003995
  l1.weight: grad_norm = 0.008141
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.016436
Total gradient norm: 0.033586
=== Actor Training Debug (Iteration 6847) ===
Q mean: -72.366867
Q std: 29.206402
Actor loss: 72.370857
Action reg: 0.003989
  l1.weight: grad_norm = 0.152484
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.252808
Total gradient norm: 0.439364
=== Actor Training Debug (Iteration 6848) ===
Q mean: -74.534561
Q std: 27.470140
Actor loss: 74.538551
Action reg: 0.003990
  l1.weight: grad_norm = 0.214844
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.420978
Total gradient norm: 0.821430
=== Actor Training Debug (Iteration 6849) ===
Q mean: -78.472145
Q std: 27.277336
Actor loss: 78.476135
Action reg: 0.003991
  l1.weight: grad_norm = 0.085445
  l1.bias: grad_norm = 0.000425
  l2.weight: grad_norm = 0.218726
Total gradient norm: 0.461421
=== Actor Training Debug (Iteration 6850) ===
Q mean: -72.940903
Q std: 27.657272
Actor loss: 72.944893
Action reg: 0.003991
  l1.weight: grad_norm = 0.168851
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.353967
Total gradient norm: 0.542935
=== Actor Training Debug (Iteration 6851) ===
Q mean: -80.262497
Q std: 27.910387
Actor loss: 80.266487
Action reg: 0.003990
  l1.weight: grad_norm = 0.299746
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.748118
Total gradient norm: 1.296288
=== Actor Training Debug (Iteration 6852) ===
Q mean: -74.312973
Q std: 27.277241
Actor loss: 74.316963
Action reg: 0.003993
  l1.weight: grad_norm = 0.155822
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.364916
Total gradient norm: 0.674685
=== Actor Training Debug (Iteration 6853) ===
Q mean: -73.709259
Q std: 28.854189
Actor loss: 73.713249
Action reg: 0.003988
  l1.weight: grad_norm = 0.116741
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.252117
Total gradient norm: 0.505521
=== Actor Training Debug (Iteration 6854) ===
Q mean: -76.168304
Q std: 27.895222
Actor loss: 76.172295
Action reg: 0.003994
  l1.weight: grad_norm = 0.144544
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.283093
Total gradient norm: 0.511532
=== Actor Training Debug (Iteration 6855) ===
Q mean: -74.293694
Q std: 29.203583
Actor loss: 74.297676
Action reg: 0.003986
  l1.weight: grad_norm = 0.287962
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.540765
Total gradient norm: 1.148851
=== Actor Training Debug (Iteration 6856) ===
Q mean: -74.705170
Q std: 28.159168
Actor loss: 74.709160
Action reg: 0.003989
  l1.weight: grad_norm = 0.160865
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.384575
Total gradient norm: 0.668876
=== Actor Training Debug (Iteration 6857) ===
Q mean: -70.987518
Q std: 27.975832
Actor loss: 70.991501
Action reg: 0.003981
  l1.weight: grad_norm = 0.346082
  l1.bias: grad_norm = 0.000609
  l2.weight: grad_norm = 0.694745
Total gradient norm: 1.169373
=== Actor Training Debug (Iteration 6858) ===
Q mean: -77.726837
Q std: 29.248808
Actor loss: 77.730820
Action reg: 0.003984
  l1.weight: grad_norm = 0.265688
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.533016
Total gradient norm: 0.981201
=== Actor Training Debug (Iteration 6859) ===
Q mean: -77.404617
Q std: 28.597519
Actor loss: 77.408600
Action reg: 0.003985
  l1.weight: grad_norm = 0.187736
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.420185
Total gradient norm: 0.743923
=== Actor Training Debug (Iteration 6860) ===
Q mean: -74.912422
Q std: 27.976227
Actor loss: 74.916420
Action reg: 0.003997
  l1.weight: grad_norm = 0.069308
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.154907
Total gradient norm: 0.261852
=== Actor Training Debug (Iteration 6861) ===
Q mean: -76.313080
Q std: 28.231390
Actor loss: 76.317062
Action reg: 0.003980
  l1.weight: grad_norm = 0.379089
  l1.bias: grad_norm = 0.000679
  l2.weight: grad_norm = 0.813122
Total gradient norm: 1.500046
=== Actor Training Debug (Iteration 6862) ===
Q mean: -72.464798
Q std: 28.396227
Actor loss: 72.468788
Action reg: 0.003993
  l1.weight: grad_norm = 0.105830
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.215179
Total gradient norm: 0.404376
=== Actor Training Debug (Iteration 6863) ===
Q mean: -74.693253
Q std: 28.117146
Actor loss: 74.697235
Action reg: 0.003984
  l1.weight: grad_norm = 0.502138
  l1.bias: grad_norm = 0.000461
  l2.weight: grad_norm = 0.976696
Total gradient norm: 1.686205
=== Actor Training Debug (Iteration 6864) ===
Q mean: -74.929207
Q std: 29.462479
Actor loss: 74.933197
Action reg: 0.003991
  l1.weight: grad_norm = 0.131182
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.268113
Total gradient norm: 0.495472
=== Actor Training Debug (Iteration 6865) ===
Q mean: -78.095825
Q std: 28.587917
Actor loss: 78.099823
Action reg: 0.003996
  l1.weight: grad_norm = 0.071659
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.158722
Total gradient norm: 0.289122
=== Actor Training Debug (Iteration 6866) ===
Q mean: -76.288086
Q std: 27.966440
Actor loss: 76.292076
Action reg: 0.003992
  l1.weight: grad_norm = 0.386742
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.946839
Total gradient norm: 1.588317
=== Actor Training Debug (Iteration 6867) ===
Q mean: -74.211525
Q std: 28.285732
Actor loss: 74.215515
Action reg: 0.003991
  l1.weight: grad_norm = 0.286649
  l1.bias: grad_norm = 0.000505
  l2.weight: grad_norm = 0.641958
Total gradient norm: 1.694829
=== Actor Training Debug (Iteration 6868) ===
Q mean: -74.748352
Q std: 27.817558
Actor loss: 74.752350
Action reg: 0.003995
  l1.weight: grad_norm = 0.082941
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.160493
Total gradient norm: 0.277380
=== Actor Training Debug (Iteration 6869) ===
Q mean: -73.226791
Q std: 30.583656
Actor loss: 73.230782
Action reg: 0.003987
  l1.weight: grad_norm = 0.295089
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.613810
Total gradient norm: 1.093422
=== Actor Training Debug (Iteration 6870) ===
Q mean: -74.107986
Q std: 28.388334
Actor loss: 74.111969
Action reg: 0.003983
  l1.weight: grad_norm = 0.139090
  l1.bias: grad_norm = 0.000536
  l2.weight: grad_norm = 0.332696
Total gradient norm: 0.637542
=== Actor Training Debug (Iteration 6871) ===
Q mean: -73.914062
Q std: 28.514147
Actor loss: 73.918053
Action reg: 0.003989
  l1.weight: grad_norm = 0.231091
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.469434
Total gradient norm: 0.851620
=== Actor Training Debug (Iteration 6872) ===
Q mean: -76.522659
Q std: 26.163195
Actor loss: 76.526649
Action reg: 0.003988
  l1.weight: grad_norm = 0.337717
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.688829
Total gradient norm: 1.317064
=== Actor Training Debug (Iteration 6873) ===
Q mean: -74.185410
Q std: 27.666445
Actor loss: 74.189400
Action reg: 0.003989
  l1.weight: grad_norm = 0.237839
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.456116
Total gradient norm: 0.826401
=== Actor Training Debug (Iteration 6874) ===
Q mean: -72.700897
Q std: 29.092018
Actor loss: 72.704887
Action reg: 0.003991
  l1.weight: grad_norm = 0.147158
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.285527
Total gradient norm: 0.464773
=== Actor Training Debug (Iteration 6875) ===
Q mean: -71.100845
Q std: 28.158709
Actor loss: 71.104836
Action reg: 0.003990
  l1.weight: grad_norm = 0.094886
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.227114
Total gradient norm: 0.432541
=== Actor Training Debug (Iteration 6876) ===
Q mean: -74.085922
Q std: 29.819939
Actor loss: 74.089912
Action reg: 0.003994
  l1.weight: grad_norm = 0.200463
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.360119
Total gradient norm: 0.635219
=== Actor Training Debug (Iteration 6877) ===
Q mean: -72.871521
Q std: 27.929190
Actor loss: 72.875504
Action reg: 0.003984
  l1.weight: grad_norm = 0.469820
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 1.084271
Total gradient norm: 2.351586
=== Actor Training Debug (Iteration 6878) ===
Q mean: -75.249855
Q std: 27.679752
Actor loss: 75.253845
Action reg: 0.003987
  l1.weight: grad_norm = 0.159736
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.352894
Total gradient norm: 0.628751
=== Actor Training Debug (Iteration 6879) ===
Q mean: -76.531555
Q std: 28.712612
Actor loss: 76.535545
Action reg: 0.003986
  l1.weight: grad_norm = 0.165489
Action reg: 0.003991 1.307274ration 6486) ===
  l1.weight: grad_norm = 0.184534
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.419096
Total gradient norm: 0.769136
=== Actor Training Debug (Iteration 6890) ===
Q mean: -75.709854
Q std: 27.643684
Actor loss: 75.713844
Action reg: 0.003988
  l1.weight: grad_norm = 0.344860
  l1.bias: grad_norm = 0.000492
  l2.weight: grad_norm = 0.634273
Total gradient norm: 1.167181
=== Actor Training Debug (Iteration 6891) ===
Q mean: -73.886475
Q std: 28.275204
Actor loss: 73.890465
Action reg: 0.003993
  l1.weight: grad_norm = 0.157674
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.344144
Total gradient norm: 0.551783
=== Actor Training Debug (Iteration 6892) ===
Q mean: -73.910805
Q std: 28.671707
Actor loss: 73.914795
Action reg: 0.003987
  l1.weight: grad_norm = 0.217971
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.431160
Total gradient norm: 0.868863
=== Actor Training Debug (Iteration 6893) ===
Q mean: -72.800941
Q std: 27.370966
Actor loss: 72.804924
Action reg: 0.003983
  l1.weight: grad_norm = 0.264560
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.555071
Total gradient norm: 1.206030
=== Actor Training Debug (Iteration 6894) ===
Q mean: -74.300850
Q std: 28.720179
Actor loss: 74.304840
Action reg: 0.003993
  l1.weight: grad_norm = 0.072190
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.164464
Total gradient norm: 0.314317
=== Actor Training Debug (Iteration 6895) ===
Q mean: -71.066971
Q std: 27.486734
Actor loss: 71.070961
Action reg: 0.003989
  l1.weight: grad_norm = 0.203433
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.424890
Total gradient norm: 0.829905
=== Actor Training Debug (Iteration 6896) ===
Q mean: -72.979050
Q std: 27.827765
Actor loss: 72.983040
Action reg: 0.003987
  l1.weight: grad_norm = 0.301112
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.721827
Total gradient norm: 1.319472
=== Actor Training Debug (Iteration 6897) ===
Q mean: -76.843613
Q std: 27.279671
Actor loss: 76.847603
Action reg: 0.003992
  l1.weight: grad_norm = 0.142128
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.287989
Total gradient norm: 0.507660
=== Actor Training Debug (Iteration 6898) ===
Q mean: -76.494629
Q std: 28.795626
Actor loss: 76.498627
Action reg: 0.003995
  l1.weight: grad_norm = 0.164369
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.297017
Total gradient norm: 0.607348
=== Actor Training Debug (Iteration 6899) ===
Q mean: -73.836853
Q std: 28.790760
Actor loss: 73.840843
Action reg: 0.003987
  l1.weight: grad_norm = 0.151568
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.302413
Total gradient norm: 0.537079
=== Actor Training Debug (Iteration 6900) ===
Q mean: -73.909752
Q std: 29.665628
Actor loss: 73.913742
Action reg: 0.003989
  l1.weight: grad_norm = 0.329412
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.621379
Total gradient norm: 1.268090
=== Actor Training Debug (Iteration 6901) ===
Q mean: -75.878433
Q std: 28.483633
Actor loss: 75.882423
Action reg: 0.003987
  l1.weight: grad_norm = 0.069425
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.128833
Total gradient norm: 0.227974
=== Actor Training Debug (Iteration 6902) ===
Q mean: -76.800407
Q std: 27.850800
Actor loss: 76.804398
Action reg: 0.003988
  l1.weight: grad_norm = 0.251215
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.553125
Total gradient norm: 1.077600
=== Actor Training Debug (Iteration 6903) ===
Q mean: -74.511078
Q std: 28.726101
Actor loss: 74.515053
Action reg: 0.003978
  l1.weight: grad_norm = 0.392707
  l1.bias: grad_norm = 0.000808
  l2.weight: grad_norm = 0.868404
Total gradient norm: 1.581111
=== Actor Training Debug (Iteration 6904) ===
Q mean: -72.910301
Q std: 27.359056
Actor loss: 72.914291
Action reg: 0.003992
  l1.weight: grad_norm = 0.613248
  l1.bias: grad_norm = 0.000781
  l2.weight: grad_norm = 1.221946
Total gradient norm: 2.166083
=== Actor Training Debug (Iteration 6905) ===
Q mean: -74.607010
Q std: 27.346336
Actor loss: 74.610992
Action reg: 0.003979
  l1.weight: grad_norm = 0.482602
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 1.076825
Total gradient norm: 1.979554
=== Actor Training Debug (Iteration 6906) ===
Q mean: -75.703163
Q std: 27.763876
Actor loss: 75.707153
Action reg: 0.003993
  l1.weight: grad_norm = 0.099650
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.223498
Total gradient norm: 0.428530
=== Actor Training Debug (Iteration 6907) ===
Q mean: -74.915833
Q std: 27.072729
Actor loss: 74.919823
Action reg: 0.003986
  l1.weight: grad_norm = 0.492318
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 1.198040
Total gradient norm: 2.516385
=== Actor Training Debug (Iteration 6908) ===
Q mean: -74.186478
Q std: 28.619316
Actor loss: 74.190460
Action reg: 0.003981
  l1.weight: grad_norm = 0.382435
  l1.bias: grad_norm = 0.000732
  l2.weight: grad_norm = 0.792581
Total gradient norm: 1.616025
=== Actor Training Debug (Iteration 6909) ===
Q mean: -75.319817
Q std: 28.448818
Actor loss: 75.323799
Action reg: 0.003985
  l1.weight: grad_norm = 0.191110
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.372317
Total gradient norm: 0.641712
=== Actor Training Debug (Iteration 6910) ===
Q mean: -76.405823
Q std: 29.617537
Actor loss: 76.409805
Action reg: 0.003986
  l1.weight: grad_norm = 0.057693
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.119222
Total gradient norm: 0.210211
=== Actor Training Debug (Iteration 6911) ===
Q mean: -72.672218
Q std: 28.188242
Actor loss: 72.676208
Action reg: 0.003991
  l1.weight: grad_norm = 0.190344
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.462358
Total gradient norm: 0.924682
=== Actor Training Debug (Iteration 6912) ===
Q mean: -72.416283
Q std: 28.595219
Actor loss: 72.420273
Action reg: 0.003992
  l1.weight: grad_norm = 0.305420
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.547101
Total gradient norm: 1.126318
=== Actor Training Debug (Iteration 6913) ===
Q mean: -75.217087
Q std: 28.764601
Actor loss: 75.221069
Action reg: 0.003984
  l1.weight: grad_norm = 0.585843
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 1.121840
Total gradient norm: 1.904487
=== Actor Training Debug (Iteration 6914) ===
Q mean: -73.587631
Q std: 27.653225
Actor loss: 73.591614
Action reg: 0.003986
  l1.weight: grad_norm = 0.403735
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.882508
Total gradient norm: 1.541071
=== Actor Training Debug (Iteration 6915) ===
Q mean: -75.454514
Q std: 28.767059
Actor loss: 75.458496
Action reg: 0.003982
  l1.weight: grad_norm = 0.212217
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.518065
Total gradient norm: 1.064949
=== Actor Training Debug (Iteration 6916) ===
Q mean: -73.746590
Q std: 27.741610
Actor loss: 73.750580
Action reg: 0.003988
  l1.weight: grad_norm = 0.225584
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.530627
Total gradient norm: 0.885828
=== Actor Training Debug (Iteration 6917) ===
Q mean: -73.038200
Q std: 28.733906
Actor loss: 73.042175
Action reg: 0.003978
  l1.weight: grad_norm = 0.165141
  l1.bias: grad_norm = 0.000609
  l2.weight: grad_norm = 0.367019
Total gradient norm: 0.707684
=== Actor Training Debug (Iteration 6918) ===
Q mean: -72.059647
Q std: 27.984480
Actor loss: 72.063629
Action reg: 0.003984
  l1.weight: grad_norm = 0.127158
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.260886
Total gradient norm: 0.457785
=== Actor Training Debug (Iteration 6919) ===
Q mean: -73.303711
Q std: 28.725262
Actor loss: 73.307701
Action reg: 0.003988
  l1.weight: grad_norm = 0.154557
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.380685
Total gradient norm: 0.744802
=== Actor Training Debug (Iteration 6920) ===
Q mean: -77.068810
Q std: 27.977638
Actor loss: 77.072800
Action reg: 0.003988
  l1.weight: grad_norm = 0.181352
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.433669
Total gradient norm: 0.899933
=== Actor Training Debug (Iteration 6921) ===
Q mean: -75.231750
Q std: 27.364292
Actor loss: 75.235741
Action reg: 0.003994
  l1.weight: grad_norm = 0.106050
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.207509
Total gradient norm: 0.325833
=== Actor Training Debug (Iteration 6922) ===
Q mean: -71.668121
Q std: 28.989954
Actor loss: 71.672104
Action reg: 0.003984
  l1.weight: grad_norm = 0.208946
  l1.bias: grad_norm = 0.000625
  l2.weight: grad_norm = 0.446376
Total gradient norm: 0.925553
=== Actor Training Debug (Iteration 6923) ===
Q mean: -76.103241
Q std: 26.970419
Actor loss: 76.107239
Action reg: 0.003996
  l1.weight: grad_norm = 0.077122
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.161338
Total gradient norm: 0.285914
=== Actor Training Debug (Iteration 6924) ===
Q mean: -73.131294
Q std: 28.584591
Actor loss: 73.135277
Action reg: 0.003979
  l1.weight: grad_norm = 0.259120
  l1.bias: grad_norm = 0.000516
  l2.weight: grad_norm = 0.439918
Total gradient norm: 0.723917
=== Actor Training Debug (Iteration 6925) ===
Q mean: -73.181137
Q std: 29.493019
Actor loss: 73.185127
Action reg: 0.003987
  l1.weight: grad_norm = 0.244436
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.444562
Total gradient norm: 0.672060
=== Actor Training Debug (Iteration 6926) ===
Q mean: -74.512405
Q std: 27.341251
Actor loss: 74.516396
Action reg: 0.003988
  l1.weight: grad_norm = 0.432551
  l1.bias: grad_norm = 0.000678
  l2.weight: grad_norm = 0.782316
Total gradient norm: 1.597949
=== Actor Training Debug (Iteration 6927) ===
Q mean: -76.097679
Q std: 27.075468
Actor loss: 76.101677
Action reg: 0.003995
  l1.weight: grad_norm = 0.079867
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.175222
Total gradient norm: 0.332156
=== Actor Training Debug (Iteration 6928) ===
Q mean: -78.241417
Q std: 27.793238
Actor loss: 78.245407
Action reg: 0.003991
  l1.weight: grad_norm = 0.050499
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.108898
Total gradient norm: 0.186147
=== Actor Training Debug (Iteration 6929) ===
Q mean: -76.928986
Q std: 26.910622
Actor loss: 76.932976
Action reg: 0.003994
  l1.weight: grad_norm = 0.136697
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.296665
Total gradient norm: 0.544762
=== Actor Training Debug (Iteration 6930) ===
Q mean: -76.342247
Q std: 28.561163
Actor loss: 76.346230
Action reg: 0.003982
  l1.weight: grad_norm = 0.428783
  l1.bias: grad_norm = 0.000819
  l2.weight: grad_norm = 0.950582
Total gradient norm: 1.770821
=== Actor Training Debug (Iteration 6931) ===
Q mean: -75.307701
Q std: 28.774605
Actor loss: 75.311691
Action reg: 0.003988
  l1.weight: grad_norm = 0.201432
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.515624
Total gradient norm: 1.021431
=== Actor Training Debug (Iteration 6932) ===
Q mean: -74.251526
Q std: 28.648270
Actor loss: 74.255516
Action reg: 0.003993
  l1.weight: grad_norm = 0.187440
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.448670
Total gradient norm: 0.922499
=== Actor Training Debug (Iteration 6933) ===
Q mean: -74.047462
Q std: 29.834127
Actor loss: 74.051453
Action reg: 0.003990
  l1.weight: grad_norm = 0.013681
  l1.bias: grad_norm = 0.000829
  l2.weight: grad_norm = 0.034962
Total gradient norm: 0.078220
=== Actor Training Debug (Iteration 6934) ===
Q mean: -77.781433
Q std: 27.121891
Actor loss: 77.785423
Action reg: 0.003993
  l1.weight: grad_norm = 0.183415
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.331633
Total gradient norm: 0.553835
=== Actor Training Debug (Iteration 6935) ===
Q mean: -75.008774
Q std: 28.290960
Actor loss: 75.012764
Action reg: 0.003988
  l1.weight: grad_norm = 0.399115
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.857659
Total gradient norm: 1.669470
=== Actor Training Debug (Iteration 6936) ===
Q mean: -74.405746
Q std: 28.091724
Actor loss: 74.409737
Action reg: 0.003987
  l1.weight: grad_norm = 0.425841
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.853074
Total gradient norm: 1.658278
=== Actor Training Debug (Iteration 6937) ===
Q mean: -75.494049
Q std: 28.853704
Actor loss: 75.498032
Action reg: 0.003985
  l1.weight: grad_norm = 0.063246
  l1.bias: grad_norm = 0.000715
  l2.weight: grad_norm = 0.104561
Total gradient norm: 0.218440
=== Actor Training Debug (Iteration 6938) ===
Q mean: -76.029648
Q std: 27.960148
Actor loss: 76.033638
Action reg: 0.003992
  l1.weight: grad_norm = 0.039240
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.083683
Total gradient norm: 0.173751
=== Actor Training Debug (Iteration 6939) ===
Q mean: -71.995651
Q std: 28.472616
Actor loss: 71.999626
Action reg: 0.003977
  l1.weight: grad_norm = 0.185207
  l1.bias: grad_norm = 0.000831
  l2.weight: grad_norm = 0.451526
Total gradient norm: 0.795307
=== Actor Training Debug (Iteration 6940) ===
Q mean: -74.069153
Q std: 29.266836
Actor loss: 74.073143
Action reg: 0.003990
  l1.weight: grad_norm = 0.028652
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.066411
Total gradient norm: 0.146738
=== Actor Training Debug (Iteration 6941) ===
Q mean: -74.506470
Q std: 28.399078
Actor loss: 74.510452
Action reg: 0.003981
  l1.weight: grad_norm = 0.268170
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.531592
Total gradient norm: 0.963823
=== Actor Training Debug (Iteration 6942) ===
Q mean: -75.747543
Q std: 27.795996
Actor loss: 75.751534
Action reg: 0.003992
  l1.weight: grad_norm = 0.834332
  l1.bias: grad_norm = 0.001173
  l2.weight: grad_norm = 1.459670
Total gradient norm: 2.877618
=== Actor Training Debug (Iteration 6943) ===
Q mean: -75.758011
Q std: 28.455326
Actor loss: 75.762001
Action reg: 0.003991
  l1.weight: grad_norm = 0.364296
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.874225
Total gradient norm: 1.502434
=== Actor Training Debug (Iteration 6944) ===
Q mean: -75.574738
Q std: 27.490534
Actor loss: 75.578728
Action reg: 0.003987
  l1.weight: grad_norm = 0.254485
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.620809
Total gradient norm: 1.447189
=== Actor Training Debug (Iteration 6945) ===
Q mean: -73.351242
Q std: 27.598183
Actor loss: 73.355232
Action reg: 0.003990
  l1.weight: grad_norm = 0.069235
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.162112
Total gradient norm: 0.305757
=== Actor Training Debug (Iteration 6946) ===
Q mean: -74.153938
Q std: 28.535011
Actor loss: 74.157921
Action reg: 0.003984
  l1.weight: grad_norm = 0.192692
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.410213
Total gradient norm: 0.769715
=== Actor Training Debug (Iteration 6947) ===
Q mean: -74.396996
Q std: 29.110966
Actor loss: 74.400978
Action reg: 0.003984
  l1.weight: grad_norm = 0.195079
  l1.bias: grad_norm = 0.000877
  l2.weight: grad_norm = 0.491440
Total gradient norm: 0.837999
=== Actor Training Debug (Iteration 6948) ===
Q mean: -74.361237
Q std: 29.282906
Actor loss: 74.365219
Action reg: 0.003986
  l1.weight: grad_norm = 0.400660
  l1.bias: grad_norm = 0.000693
  l2.weight: grad_norm = 0.797838
Total gradient norm: 1.447670
=== Actor Training Debug (Iteration 6949) ===
Q mean: -71.088226
Q std: 28.298090
Actor loss: 71.092216
Action reg: 0.003987
  l1.weight: grad_norm = 0.259784
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.520266
Total gradient norm: 0.934047
=== Actor Training Debug (Iteration 6950) ===
Q mean: -72.989822
Q std: 29.961809
Actor loss: 72.993805
Action reg: 0.003984
  l1.weight: grad_norm = 0.232839
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.430113
Total gradient norm: 0.803881
=== Actor Training Debug (Iteration 6951) ===
Q mean: -75.612396
Q std: 29.237541
Actor loss: 75.616379
Action reg: 0.003982
  l1.weight: grad_norm = 0.178746
  l1.bias: grad_norm = 0.000585
  l2.weight: grad_norm = 0.421760
Total gradient norm: 0.925808
=== Actor Training Debug (Iteration 6952) ===
Q mean: -75.556358
Q std: 29.868307
Actor loss: 75.560341
Action reg: 0.003982
  l1.weight: grad_norm = 0.205327
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.386767
Total gradient norm: 0.717257
=== Actor Training Debug (Iteration 6953) ===
Q mean: -76.044022
Q std: 27.402620
Actor loss: 76.048019
Action reg: 0.003997
  l1.weight: grad_norm = 0.275147
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.644471
Total gradient norm: 1.311112
=== Actor Training Debug (Iteration 6954) ===
Q mean: -71.536285
Q std: 29.429285
Actor loss: 71.540276
Action reg: 0.003989
  l1.weight: grad_norm = 0.220040
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.477884
Total gradient norm: 0.801268
=== Actor Training Debug (Iteration 6955) ===
Q mean: -73.927429
Q std: 28.742565
Actor loss: 73.931412
Action reg: 0.003983
  l1.weight: grad_norm = 0.273002
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.563351
Total gradient norm: 1.024536
=== Actor Training Debug (Iteration 6956) ===
Q mean: -72.343048
Q std: 29.139427
Actor loss: 72.347038
Action reg: 0.003992
  l1.weight: grad_norm = 0.198156
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.498422
Total gradient norm: 1.138716
=== Actor Training Debug (Iteration 6957) ===
Q mean: -77.691971
Q std: 27.861191
Actor loss: 77.695969
Action reg: 0.003997
  l1.weight: grad_norm = 0.061930
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.130860
Total gradient norm: 0.295868
=== Actor Training Debug (Iteration 6958) ===
Q mean: -77.121895
Q std: 30.009426
Actor loss: 77.125885
Action reg: 0.003991
  l1.weight: grad_norm = 0.103288
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.227757
Total gradient norm: 0.442979
=== Actor Training Debug (Iteration 6959) ===
Q mean: -76.731934
Q std: 27.720253
Actor loss: 76.735931
Action reg: 0.003999
  l1.weight: grad_norm = 0.017404
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.038840
Total gradient norm: 0.075477
=== Actor Training Debug (Iteration 6960) ===
Q mean: -72.133163
Q std: 27.768570
Actor loss: 72.137154
Action reg: 0.003988
  l1.weight: grad_norm = 0.133076
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.313293
Total gradient norm: 0.595309
=== Actor Training Debug (Iteration 6961) ===
Q mean: -72.701477
Q std: 28.117167
Actor loss: 72.705467
Action reg: 0.003987
  l1.weight: grad_norm = 0.112283
  l1.bias: grad_norm = 0.000569
  l2.weight: grad_norm = 0.234111
Total gradient norm: 0.472676
=== Actor Training Debug (Iteration 6962) ===
Q mean: -71.711342
Q std: 29.730371
Actor loss: 71.715332
Action reg: 0.003993
  l1.weight: grad_norm = 0.095351
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.165697
Total gradient norm: 0.332481
=== Actor Training Debug (Iteration 6963) ===
Q mean: -76.804970
Q std: 29.585238
Actor loss: 76.808952
Action reg: 0.003983
  l1.weight: grad_norm = 0.130582
  l1.bias: grad_norm = 0.000635
  l2.weight: grad_norm = 0.302776
Total gradient norm: 0.562587
=== Actor Training Debug (Iteration 6964) ===
Q mean: -77.214035
Q std: 28.730656
Actor loss: 77.218018
Action reg: 0.003982
  l1.weight: grad_norm = 0.103501
  l1.bias: grad_norm = 0.000746
  l2.weight: grad_norm = 0.246561
Total gradient norm: 0.504220
=== Actor Training Debug (Iteration 6965) ===
Q mean: -75.527618
Q std: 28.535036
Actor loss: 75.531601
Action reg: 0.003986
  l1.weight: grad_norm = 0.048756
  l1.bias: grad_norm = 0.000653
  l2.weight: grad_norm = 0.110003
Total gradient norm: 0.226381
=== Actor Training Debug (Iteration 6966) ===
Q mean: -71.960854
Q std: 28.199263
Actor loss: 71.964851
Action reg: 0.003997
  l1.weight: grad_norm = 0.025556
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.058630
Total gradient norm: 0.108420
=== Actor Training Debug (Iteration 6967) ===
Q mean: -74.131989
Q std: 29.203197
Actor loss: 74.135979
Action reg: 0.003989
  l1.weight: grad_norm = 0.751931
  l1.bias: grad_norm = 0.001020
  l2.weight: grad_norm = 1.377369
Total gradient norm: 3.047982
=== Actor Training Debug (Iteration 6968) ===
Q mean: -75.644417
Q std: 28.210823
Actor loss: 75.648407
Action reg: 0.003988
  l1.weight: grad_norm = 0.299742
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.612362
Total gradient norm: 1.147978
=== Actor Training Debug (Iteration 6969) ===
Q mean: -74.258652
Q std: 27.528978
Actor loss: 74.262650
Action reg: 0.003995
  l1.weight: grad_norm = 0.097411
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.211106
Total gradient norm: 0.422807
=== Actor Training Debug (Iteration 6970) ===
Q mean: -72.385193
Q std: 28.825047
Actor loss: 72.389168
Action reg: 0.003975
  l1.weight: grad_norm = 0.133335
  l1.bias: grad_norm = 0.000861
  l2.weight: grad_norm = 0.278236
Total gradient norm: 0.568315
=== Actor Training Debug (Iteration 6971) ===
Q mean: -74.720108
Q std: 29.165718
Actor loss: 74.724091
Action reg: 0.003983
  l1.weight: grad_norm = 0.117734
  l1.bias: grad_norm = 0.000631
  l2.weight: grad_norm = 0.257218
Total gradient norm: 0.516571
=== Actor Training Debug (Iteration 6972) ===
Q mean: -72.808296
Q std: 27.974630
Actor loss: 72.812279
Action reg: 0.003985
  l1.weight: grad_norm = 0.143973
  l1.bias: grad_norm = 0.000617
  l2.weight: grad_norm = 0.353213
Total gradient norm: 0.697229
=== Actor Training Debug (Iteration 6973) ===
Q mean: -76.853439
Q std: 28.643925
Actor loss: 76.857430
Action reg: 0.003987
  l1.weight: grad_norm = 0.074573
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.167080
Total gradient norm: 0.314757
=== Actor Training Debug (Iteration 6974) ===
Q mean: -74.943214
Q std: 28.155935
Actor loss: 74.947205
Action reg: 0.003992
  l1.weight: grad_norm = 0.079143
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.171989
Total gradient norm: 0.339408
=== Actor Training Debug (Iteration 6975) ===
Q mean: -73.846405
Q std: 28.708658
Actor loss: 73.850388
Action reg: 0.003981
  l1.weight: grad_norm = 0.099956
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.221663
Total gradient norm: 0.366540
=== Actor Training Debug (Iteration 6976) ===
Q mean: -73.436081
Q std: 28.936558
Actor loss: 73.440063
Action reg: 0.003986
  l1.weight: grad_norm = 0.113691
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.204871
Total gradient norm: 0.337557
=== Actor Training Debug (Iteration 6977) ===
Q mean: -76.584770
Q std: 28.441652
Actor loss: 76.588768
Action reg: 0.003995
  l1.weight: grad_norm = 0.224366
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.391851
Total gradient norm: 0.793539
Total gradient norm: 1.369559ration 6486) ===
=== Actor Training Debug (Iteration 6988) ===
Q mean: -79.229988
Q std: 28.130178
Actor loss: 79.233971
Action reg: 0.003985
  l1.weight: grad_norm = 0.258641
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.565384
Total gradient norm: 1.108978
=== Actor Training Debug (Iteration 6989) ===
Q mean: -76.177429
Q std: 28.904991
Actor loss: 76.181419
Action reg: 0.003993
  l1.weight: grad_norm = 0.264089
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.565358
Total gradient norm: 0.974514
=== Actor Training Debug (Iteration 6990) ===
Q mean: -75.645111
Q std: 28.011375
Actor loss: 75.649101
Action reg: 0.003988
  l1.weight: grad_norm = 0.247077
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.505682
Total gradient norm: 1.025328
=== Actor Training Debug (Iteration 6991) ===
Q mean: -76.374115
Q std: 30.246580
Actor loss: 76.378098
Action reg: 0.003984
  l1.weight: grad_norm = 0.238905
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.512521
Total gradient norm: 1.029267
=== Actor Training Debug (Iteration 6992) ===
Q mean: -75.078598
Q std: 28.662720
Actor loss: 75.082581
Action reg: 0.003984
  l1.weight: grad_norm = 0.206235
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.464017
Total gradient norm: 0.862947
=== Actor Training Debug (Iteration 6993) ===
Q mean: -77.664085
Q std: 26.844309
Actor loss: 77.668076
Action reg: 0.003988
  l1.weight: grad_norm = 0.238377
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.462398
Total gradient norm: 0.863982
=== Actor Training Debug (Iteration 6994) ===
Q mean: -74.112106
Q std: 27.644279
Actor loss: 74.116089
Action reg: 0.003984
  l1.weight: grad_norm = 0.092088
  l1.bias: grad_norm = 0.000765
  l2.weight: grad_norm = 0.190851
Total gradient norm: 0.382433
=== Actor Training Debug (Iteration 6995) ===
Q mean: -76.592888
Q std: 29.114258
Actor loss: 76.596878
Action reg: 0.003986
  l1.weight: grad_norm = 0.160675
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.332498
Total gradient norm: 0.644555
=== Actor Training Debug (Iteration 6996) ===
Q mean: -74.711060
Q std: 28.041216
Actor loss: 74.715057
Action reg: 0.003997
  l1.weight: grad_norm = 0.006510
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.012119
Total gradient norm: 0.019978
=== Actor Training Debug (Iteration 6997) ===
Q mean: -71.118317
Q std: 26.931402
Actor loss: 71.122299
Action reg: 0.003981
  l1.weight: grad_norm = 0.373940
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.771055
Total gradient norm: 1.435923
=== Actor Training Debug (Iteration 6998) ===
Q mean: -74.953354
Q std: 28.748220
Actor loss: 74.957336
Action reg: 0.003985
  l1.weight: grad_norm = 0.318887
  l1.bias: grad_norm = 0.000979
  l2.weight: grad_norm = 0.702006
Total gradient norm: 1.255072
=== Actor Training Debug (Iteration 6999) ===
Q mean: -76.384636
Q std: 29.352684
Actor loss: 76.388626
Action reg: 0.003988
  l1.weight: grad_norm = 0.114398
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.287186
Total gradient norm: 0.596600
=== Actor Training Debug (Iteration 7000) ===
Q mean: -74.652298
Q std: 27.152113
Actor loss: 74.656288
Action reg: 0.003986
  l1.weight: grad_norm = 0.295213
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.690581
Total gradient norm: 1.373818
Step 12000: Critic Loss: 5.0193, Actor Loss: 74.6563, Q Value: -74.6523
  Average reward: -335.573 | Average length: 100.0
Evaluation at episode 120: -335.573
=== Actor Training Debug (Iteration 7001) ===
Q mean: -74.209900
Q std: 27.411037
Actor loss: 74.213882
Action reg: 0.003982
  l1.weight: grad_norm = 0.291289
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.588990
Total gradient norm: 0.945987
=== Actor Training Debug (Iteration 7002) ===
Q mean: -77.191231
Q std: 27.890493
Actor loss: 77.195221
Action reg: 0.003989
  l1.weight: grad_norm = 0.107755
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.221725
Total gradient norm: 0.443533
=== Actor Training Debug (Iteration 7003) ===
Q mean: -75.531433
Q std: 30.108467
Actor loss: 75.535408
Action reg: 0.003975
  l1.weight: grad_norm = 0.266617
  l1.bias: grad_norm = 0.000932
  l2.weight: grad_norm = 0.573650
Total gradient norm: 1.131977
=== Actor Training Debug (Iteration 7004) ===
Q mean: -73.581848
Q std: 28.195290
Actor loss: 73.585838
Action reg: 0.003991
  l1.weight: grad_norm = 0.230976
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.492119
Total gradient norm: 0.942324
=== Actor Training Debug (Iteration 7005) ===
Q mean: -73.803009
Q std: 28.013132
Actor loss: 73.806992
Action reg: 0.003981
  l1.weight: grad_norm = 0.157728
  l1.bias: grad_norm = 0.000676
  l2.weight: grad_norm = 0.359419
Total gradient norm: 0.704490
=== Actor Training Debug (Iteration 7006) ===
Q mean: -75.156059
Q std: 27.315897
Actor loss: 75.160057
Action reg: 0.003994
  l1.weight: grad_norm = 0.085756
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.207687
Total gradient norm: 0.437347
=== Actor Training Debug (Iteration 7007) ===
Q mean: -74.823990
Q std: 27.444223
Actor loss: 74.827972
Action reg: 0.003984
  l1.weight: grad_norm = 0.162236
  l1.bias: grad_norm = 0.000598
  l2.weight: grad_norm = 0.343243
Total gradient norm: 0.672941
=== Actor Training Debug (Iteration 7008) ===
Q mean: -73.323151
Q std: 28.369114
Actor loss: 73.327141
Action reg: 0.003990
  l1.weight: grad_norm = 0.152214
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.330036
Total gradient norm: 0.567559
=== Actor Training Debug (Iteration 7009) ===
Q mean: -73.599327
Q std: 29.565853
Actor loss: 73.603310
Action reg: 0.003982
  l1.weight: grad_norm = 0.320439
  l1.bias: grad_norm = 0.000802
  l2.weight: grad_norm = 0.783693
Total gradient norm: 1.512795
=== Actor Training Debug (Iteration 7010) ===
Q mean: -70.951584
Q std: 28.744320
Actor loss: 70.955566
Action reg: 0.003979
  l1.weight: grad_norm = 0.273316
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.595239
Total gradient norm: 0.998842
=== Actor Training Debug (Iteration 7011) ===
Q mean: -75.899284
Q std: 28.985991
Actor loss: 75.903275
Action reg: 0.003989
  l1.weight: grad_norm = 0.086583
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.201365
Total gradient norm: 0.344138
=== Actor Training Debug (Iteration 7012) ===
Q mean: -74.249435
Q std: 27.938461
Actor loss: 74.253426
Action reg: 0.003990
  l1.weight: grad_norm = 0.643677
  l1.bias: grad_norm = 0.000873
  l2.weight: grad_norm = 1.279824
Total gradient norm: 2.350217
=== Actor Training Debug (Iteration 7013) ===
Q mean: -74.716461
Q std: 28.325071
Actor loss: 74.720444
Action reg: 0.003984
  l1.weight: grad_norm = 0.142116
  l1.bias: grad_norm = 0.000542
  l2.weight: grad_norm = 0.334669
Total gradient norm: 0.680257
=== Actor Training Debug (Iteration 7014) ===
Q mean: -74.790604
Q std: 28.406797
Actor loss: 74.794586
Action reg: 0.003983
  l1.weight: grad_norm = 0.066858
  l1.bias: grad_norm = 0.000615
  l2.weight: grad_norm = 0.148512
Total gradient norm: 0.249970
=== Actor Training Debug (Iteration 7015) ===
Q mean: -76.481857
Q std: 30.368874
Actor loss: 76.485840
Action reg: 0.003983
  l1.weight: grad_norm = 0.206938
  l1.bias: grad_norm = 0.001322
  l2.weight: grad_norm = 0.382677
Total gradient norm: 0.654550
=== Actor Training Debug (Iteration 7016) ===
Q mean: -75.136574
Q std: 31.162466
Actor loss: 75.140556
Action reg: 0.003979
  l1.weight: grad_norm = 0.117130
  l1.bias: grad_norm = 0.000829
  l2.weight: grad_norm = 0.283686
Total gradient norm: 0.572197
=== Actor Training Debug (Iteration 7017) ===
Q mean: -76.757492
Q std: 27.751617
Actor loss: 76.761482
Action reg: 0.003993
  l1.weight: grad_norm = 0.152112
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.366313
Total gradient norm: 0.691578
=== Actor Training Debug (Iteration 7018) ===
Q mean: -72.889351
Q std: 27.902084
Actor loss: 72.893326
Action reg: 0.003978
  l1.weight: grad_norm = 0.201296
  l1.bias: grad_norm = 0.000793
  l2.weight: grad_norm = 0.375546
Total gradient norm: 0.614128
=== Actor Training Debug (Iteration 7019) ===
Q mean: -75.022469
Q std: 28.452515
Actor loss: 75.026451
Action reg: 0.003981
  l1.weight: grad_norm = 0.150132
  l1.bias: grad_norm = 0.000896
  l2.weight: grad_norm = 0.269960
Total gradient norm: 0.429588
=== Actor Training Debug (Iteration 7020) ===
Q mean: -79.661438
Q std: 30.168766
Actor loss: 79.665428
Action reg: 0.003987
  l1.weight: grad_norm = 0.202512
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.441913
Total gradient norm: 0.861097
=== Actor Training Debug (Iteration 7021) ===
Q mean: -74.349182
Q std: 28.350121
Actor loss: 74.353172
Action reg: 0.003991
  l1.weight: grad_norm = 0.093996
  l1.bias: grad_norm = 0.000513
  l2.weight: grad_norm = 0.202305
Total gradient norm: 0.397316
=== Actor Training Debug (Iteration 7022) ===
Q mean: -74.093193
Q std: 27.906923
Actor loss: 74.097183
Action reg: 0.003992
  l1.weight: grad_norm = 0.007134
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.018467
Total gradient norm: 0.043718
=== Actor Training Debug (Iteration 7023) ===
Q mean: -76.342720
Q std: 26.568773
Actor loss: 76.346718
Action reg: 0.003996
  l1.weight: grad_norm = 0.124382
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.280966
Total gradient norm: 0.553715
=== Actor Training Debug (Iteration 7024) ===
Q mean: -76.514114
Q std: 27.637215
Actor loss: 76.518105
Action reg: 0.003988
  l1.weight: grad_norm = 0.194618
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.460912
Total gradient norm: 0.812468
=== Actor Training Debug (Iteration 7025) ===
Q mean: -74.837181
Q std: 26.893164
Actor loss: 74.841179
Action reg: 0.003996
  l1.weight: grad_norm = 0.014481
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.036842
Total gradient norm: 0.073795
=== Actor Training Debug (Iteration 7026) ===
Q mean: -71.935471
Q std: 30.571781
Actor loss: 71.939445
Action reg: 0.003974
  l1.weight: grad_norm = 0.208420
  l1.bias: grad_norm = 0.001499
  l2.weight: grad_norm = 0.487788
Total gradient norm: 0.966339
=== Actor Training Debug (Iteration 7027) ===
Q mean: -73.296280
Q std: 28.352180
Actor loss: 73.300262
Action reg: 0.003984
  l1.weight: grad_norm = 0.139165
  l1.bias: grad_norm = 0.000781
  l2.weight: grad_norm = 0.327061
Total gradient norm: 0.581378
=== Actor Training Debug (Iteration 7028) ===
Q mean: -77.562241
Q std: 27.714434
Actor loss: 77.566223
Action reg: 0.003983
  l1.weight: grad_norm = 0.417940
  l1.bias: grad_norm = 0.001263
  l2.weight: grad_norm = 0.988572
Total gradient norm: 2.148488
=== Actor Training Debug (Iteration 7029) ===
Q mean: -78.645859
Q std: 26.542088
Actor loss: 78.649849
Action reg: 0.003989
  l1.weight: grad_norm = 0.172005
  l1.bias: grad_norm = 0.000780
  l2.weight: grad_norm = 0.350895
Total gradient norm: 0.500409
=== Actor Training Debug (Iteration 7030) ===
Q mean: -74.333939
Q std: 27.461611
Actor loss: 74.337929
Action reg: 0.003992
  l1.weight: grad_norm = 0.060905
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.127493
Total gradient norm: 0.230736
=== Actor Training Debug (Iteration 7031) ===
Q mean: -77.348129
Q std: 27.014593
Actor loss: 77.352112
Action reg: 0.003985
  l1.weight: grad_norm = 0.213500
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.508014
Total gradient norm: 0.999781
=== Actor Training Debug (Iteration 7032) ===
Q mean: -78.928238
Q std: 28.596300
Actor loss: 78.932220
Action reg: 0.003981
  l1.weight: grad_norm = 0.173717
  l1.bias: grad_norm = 0.001159
  l2.weight: grad_norm = 0.357443
Total gradient norm: 0.669566
=== Actor Training Debug (Iteration 7033) ===
Q mean: -77.441116
Q std: 28.813671
Actor loss: 77.445107
Action reg: 0.003989
  l1.weight: grad_norm = 0.046396
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.106576
Total gradient norm: 0.194083
=== Actor Training Debug (Iteration 7034) ===
Q mean: -77.147392
Q std: 29.063120
Actor loss: 77.151382
Action reg: 0.003989
  l1.weight: grad_norm = 0.425749
  l1.bias: grad_norm = 0.000649
  l2.weight: grad_norm = 0.876993
Total gradient norm: 1.874691
=== Actor Training Debug (Iteration 7035) ===
Q mean: -73.535805
Q std: 29.829758
Actor loss: 73.539795
Action reg: 0.003990
  l1.weight: grad_norm = 0.227602
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.437481
Total gradient norm: 0.756597
=== Actor Training Debug (Iteration 7036) ===
Q mean: -74.111618
Q std: 28.680523
Actor loss: 74.115608
Action reg: 0.003993
  l1.weight: grad_norm = 0.046882
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.084282
Total gradient norm: 0.146211
=== Actor Training Debug (Iteration 7037) ===
Q mean: -77.390762
Q std: 29.303833
Actor loss: 77.394745
Action reg: 0.003984
  l1.weight: grad_norm = 0.094673
  l1.bias: grad_norm = 0.001245
  l2.weight: grad_norm = 0.195414
Total gradient norm: 0.363628
=== Actor Training Debug (Iteration 7038) ===
Q mean: -77.362579
Q std: 28.507303
Actor loss: 77.366562
Action reg: 0.003986
  l1.weight: grad_norm = 0.179932
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.430339
Total gradient norm: 0.925860
=== Actor Training Debug (Iteration 7039) ===
Q mean: -73.040085
Q std: 27.058170
Actor loss: 73.044060
Action reg: 0.003975
  l1.weight: grad_norm = 0.132310
  l1.bias: grad_norm = 0.000835
  l2.weight: grad_norm = 0.312775
Total gradient norm: 0.615162
=== Actor Training Debug (Iteration 7040) ===
Q mean: -73.387817
Q std: 29.659475
Actor loss: 73.391800
Action reg: 0.003980
  l1.weight: grad_norm = 0.107918
  l1.bias: grad_norm = 0.000841
  l2.weight: grad_norm = 0.245421
Total gradient norm: 0.438656
=== Actor Training Debug (Iteration 7041) ===
Q mean: -75.824532
Q std: 27.730131
Actor loss: 75.828529
Action reg: 0.003995
  l1.weight: grad_norm = 0.043447
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.092064
Total gradient norm: 0.175316
=== Actor Training Debug (Iteration 7042) ===
Q mean: -74.313110
Q std: 27.214439
Actor loss: 74.317101
Action reg: 0.003990
  l1.weight: grad_norm = 0.180471
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.355507
Total gradient norm: 0.601114
=== Actor Training Debug (Iteration 7043) ===
Q mean: -76.512978
Q std: 28.182344
Actor loss: 76.516960
Action reg: 0.003980
  l1.weight: grad_norm = 0.082550
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.200930
Total gradient norm: 0.382364
=== Actor Training Debug (Iteration 7044) ===
Q mean: -74.723572
Q std: 26.866621
Actor loss: 74.727570
Action reg: 0.003994
  l1.weight: grad_norm = 0.277817
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.552912
Total gradient norm: 0.937069
=== Actor Training Debug (Iteration 7045) ===
Q mean: -74.980385
Q std: 28.773512
Actor loss: 74.984360
Action reg: 0.003976
  l1.weight: grad_norm = 0.313024
  l1.bias: grad_norm = 0.000729
  l2.weight: grad_norm = 0.605525
Total gradient norm: 0.916884
=== Actor Training Debug (Iteration 7046) ===
Q mean: -74.689240
Q std: 28.698296
Actor loss: 74.693222
Action reg: 0.003984
  l1.weight: grad_norm = 0.240329
  l1.bias: grad_norm = 0.001192
  l2.weight: grad_norm = 0.404559
Total gradient norm: 0.827359
=== Actor Training Debug (Iteration 7047) ===
Q mean: -76.583397
Q std: 29.405071
Actor loss: 76.587379
Action reg: 0.003986
  l1.weight: grad_norm = 0.024066
  l1.bias: grad_norm = 0.000833
  l2.weight: grad_norm = 0.058942
Total gradient norm: 0.107791
=== Actor Training Debug (Iteration 7048) ===
Q mean: -75.675270
Q std: 28.681789
Actor loss: 75.679260
Action reg: 0.003989
  l1.weight: grad_norm = 0.047519
  l1.bias: grad_norm = 0.001154
  l2.weight: grad_norm = 0.108022
Total gradient norm: 0.227750
=== Actor Training Debug (Iteration 7049) ===
Q mean: -77.592819
Q std: 28.354328
Actor loss: 77.596809
Action reg: 0.003989
  l1.weight: grad_norm = 0.184519
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.378448
Total gradient norm: 0.620748
=== Actor Training Debug (Iteration 7050) ===
Q mean: -75.552910
Q std: 28.625727
Actor loss: 75.556900
Action reg: 0.003991
  l1.weight: grad_norm = 0.201290
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.445401
Total gradient norm: 0.882790
=== Actor Training Debug (Iteration 7051) ===
Q mean: -76.556267
Q std: 28.984499
Actor loss: 76.560242
Action reg: 0.003978
  l1.weight: grad_norm = 0.256673
  l1.bias: grad_norm = 0.000984
  l2.weight: grad_norm = 0.490818
Total gradient norm: 0.875649
=== Actor Training Debug (Iteration 7052) ===
Q mean: -77.017738
Q std: 27.669310
Actor loss: 77.021721
Action reg: 0.003985
  l1.weight: grad_norm = 0.192147
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.375749
Total gradient norm: 0.683756
=== Actor Training Debug (Iteration 7053) ===
Q mean: -77.763550
Q std: 27.801060
Actor loss: 77.767548
Action reg: 0.003997
  l1.weight: grad_norm = 0.044608
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.106524
Total gradient norm: 0.190578
=== Actor Training Debug (Iteration 7054) ===
Q mean: -72.997574
Q std: 28.073805
Actor loss: 73.001556
Action reg: 0.003981
  l1.weight: grad_norm = 0.137072
  l1.bias: grad_norm = 0.001448
  l2.weight: grad_norm = 0.271728
Total gradient norm: 0.477747
=== Actor Training Debug (Iteration 7055) ===
Q mean: -75.051392
Q std: 27.961014
Actor loss: 75.055374
Action reg: 0.003984
  l1.weight: grad_norm = 0.217897
  l1.bias: grad_norm = 0.000984
  l2.weight: grad_norm = 0.452839
Total gradient norm: 0.826538
=== Actor Training Debug (Iteration 7056) ===
Q mean: -72.486908
Q std: 27.737637
Actor loss: 72.490898
Action reg: 0.003988
  l1.weight: grad_norm = 0.271077
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.490279
Total gradient norm: 0.851328
=== Actor Training Debug (Iteration 7057) ===
Q mean: -76.670616
Q std: 27.800730
Actor loss: 76.674606
Action reg: 0.003992
  l1.weight: grad_norm = 0.020606
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.039600
Total gradient norm: 0.072232
=== Actor Training Debug (Iteration 7058) ===
Q mean: -75.319901
Q std: 28.816031
Actor loss: 75.323883
Action reg: 0.003982
  l1.weight: grad_norm = 0.032422
  l1.bias: grad_norm = 0.001151
  l2.weight: grad_norm = 0.070466
Total gradient norm: 0.134625
=== Actor Training Debug (Iteration 7059) ===
Q mean: -75.371994
Q std: 29.342686
Actor loss: 75.375977
Action reg: 0.003986
  l1.weight: grad_norm = 0.357519
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.761337
Total gradient norm: 1.501763
=== Actor Training Debug (Iteration 7060) ===
Q mean: -73.332443
Q std: 28.498785
Actor loss: 73.336433
Action reg: 0.003989
  l1.weight: grad_norm = 0.185508
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.342977
Total gradient norm: 0.663158
=== Actor Training Debug (Iteration 7061) ===
Q mean: -75.033707
Q std: 28.658537
Actor loss: 75.037697
Action reg: 0.003992
  l1.weight: grad_norm = 0.205298
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.499062
Total gradient norm: 0.930120
=== Actor Training Debug (Iteration 7062) ===
Q mean: -76.255356
Q std: 28.726851
Actor loss: 76.259331
Action reg: 0.003973
  l1.weight: grad_norm = 0.172654
  l1.bias: grad_norm = 0.002765
  l2.weight: grad_norm = 0.319099
Total gradient norm: 0.741247
=== Actor Training Debug (Iteration 7063) ===
Q mean: -76.625092
Q std: 29.304718
Actor loss: 76.629074
Action reg: 0.003983
  l1.weight: grad_norm = 0.188730
  l1.bias: grad_norm = 0.001632
  l2.weight: grad_norm = 0.349999
Total gradient norm: 0.654033
=== Actor Training Debug (Iteration 7064) ===
Q mean: -75.709961
Q std: 29.544889
Actor loss: 75.713943
Action reg: 0.003986
  l1.weight: grad_norm = 0.159068
  l1.bias: grad_norm = 0.001555
  l2.weight: grad_norm = 0.280087
Total gradient norm: 0.658585
=== Actor Training Debug (Iteration 7065) ===
Q mean: -76.341423
Q std: 27.486752
Actor loss: 76.345413
Action reg: 0.003989
  l1.weight: grad_norm = 0.348439
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.657883
Total gradient norm: 1.078428
=== Actor Training Debug (Iteration 7066) ===
Q mean: -75.509827
Q std: 28.693390
Actor loss: 75.513809
Action reg: 0.003981
  l1.weight: grad_norm = 0.083651
  l1.bias: grad_norm = 0.000840
  l2.weight: grad_norm = 0.170249
Total gradient norm: 0.358047
=== Actor Training Debug (Iteration 7067) ===
Q mean: -77.273483
Q std: 28.598356
Actor loss: 77.277473
Action reg: 0.003986
  l1.weight: grad_norm = 0.268284
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.564005
Total gradient norm: 0.898537
=== Actor Training Debug (Iteration 7068) ===
Q mean: -73.822815
Q std: 28.821892
Actor loss: 73.826805
Action reg: 0.003988
  l1.weight: grad_norm = 0.159267
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.309364
Total gradient norm: 0.539482
=== Actor Training Debug (Iteration 7069) ===
Q mean: -76.211403
Q std: 28.345493
Actor loss: 76.215393
Action reg: 0.003990
  l1.weight: grad_norm = 0.117549
  l1.bias: grad_norm = 0.000814
  l2.weight: grad_norm = 0.219201
Total gradient norm: 0.378276
=== Actor Training Debug (Iteration 7070) ===
Q mean: -78.589264
Q std: 29.475998
Actor loss: 78.593254
Action reg: 0.003992
  l1.weight: grad_norm = 0.219455
  l1.bias: grad_norm = 0.000837
  l2.weight: grad_norm = 0.401704
Total gradient norm: 0.864393
=== Actor Training Debug (Iteration 7071) ===
Q mean: -72.903564
Q std: 28.647165
Actor loss: 72.907562
Action reg: 0.003994
  l1.weight: grad_norm = 0.123512
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.265572
Total gradient norm: 0.496815
=== Actor Training Debug (Iteration 7072) ===
Q mean: -73.443138
Q std: 29.707867
Actor loss: 73.447128
Action reg: 0.003990
  l1.weight: grad_norm = 0.034882
  l1.bias: grad_norm = 0.000935
  l2.weight: grad_norm = 0.075934
Total gradient norm: 0.153341
=== Actor Training Debug (Iteration 7073) ===
Q mean: -75.115715
Q std: 28.427120
Actor loss: 75.119690
Action reg: 0.003978
  l1.weight: grad_norm = 0.175415
  l1.bias: grad_norm = 0.001487
  l2.weight: grad_norm = 0.395717
Total gradient norm: 0.783004
=== Actor Training Debug (Iteration 7074) ===
Q mean: -74.694664
Q std: 29.382082
Actor loss: 74.698654
Action reg: 0.003993
  l1.weight: grad_norm = 0.058430
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.142994
Total gradient norm: 0.316974
=== Actor Training Debug (Iteration 7075) ===
Q mean: -74.059242
Q std: 28.167936
Actor loss: 74.063225
Action reg: 0.003981
  l1.weight: grad_norm = 0.523968
  l1.bias: grad_norm = 0.000754
  l2.weight: grad_norm = 1.102588
Total gradient norm: 2.012745
=== Actor Training Debug (Iteration 7076) ===
Q mean: -73.690071
Q std: 29.582657
Actor loss: 73.694054
Action reg: 0.003982
  l1.weight: grad_norm = 0.083302
  l1.bias: grad_norm = 0.001350
  l2.weight: grad_norm = 0.192055
Total gradient norm: 0.369701
=== Actor Training Debug (Iteration 7077) ===
Q mean: -77.252579
Q std: 28.346134
Actor loss: 77.256569
Action reg: 0.003989
  l1.weight: grad_norm = 0.193384
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.432254
Total gradient norm: 0.919214
=== Actor Training Debug (Iteration 7078) ===
Q mean: -74.155228
Q std: 30.301746
Actor loss: 74.159210
Action reg: 0.003984
  l1.weight: grad_norm = 0.275837
  l1.bias: grad_norm = 0.000631
  l2.weight: grad_norm = 0.564720
Total gradient norm: 1.083824
=== Actor Training Debug (Iteration 7079) ===
Q mean: -77.346321
Q std: 28.692844
Actor loss: 77.350311
Action reg: 0.003988
  l1.weight: grad_norm = 0.193563
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.357803
Total gradient norm: 0.592594
=== Actor Training Debug (Iteration 7080) ===
Q mean: -76.576340
Q std: 30.678856
Actor loss: 76.580322
Action reg: 0.003983
  l1.weight: grad_norm = 0.289095
  l1.bias: grad_norm = 0.001024
  l2.weight: grad_norm = 0.577866
Total gradient norm: 1.044550
=== Actor Training Debug (Iteration 7081) ===
Q mean: -74.939636
Q std: 27.355576
Actor loss: 74.943626
Action reg: 0.003990
  l1.weight: grad_norm = 0.132732
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.313382
Total gradient norm: 0.701750
=== Actor Training Debug (Iteration 7082) ===
Q mean: -76.305717
Q std: 30.130968
Actor loss: 76.309700
Action reg: 0.003981
  l1.weight: grad_norm = 0.209979
  l1.bias: grad_norm = 0.001601
  l2.weight: grad_norm = 0.441304
Total gradient norm: 0.778326
=== Actor Training Debug (Iteration 7083) ===
Q mean: -74.865891
Q std: 28.712269
Actor loss: 74.869881
Action reg: 0.003987
  l1.weight: grad_norm = 0.228900
  l1.bias: grad_norm = 0.000819
  l2.weight: grad_norm = 0.567033
Total gradient norm: 1.088518
=== Actor Training Debug (Iteration 7084) ===
Q mean: -79.028900
Q std: 28.213163
Actor loss: 79.032883
Action reg: 0.003984
  l1.weight: grad_norm = 0.152108
  l1.bias: grad_norm = 0.000602
  l2.weight: grad_norm = 0.309865
Total gradient norm: 0.636037
=== Actor Training Debug (Iteration 7085) ===
Q mean: -77.669952
Q std: 28.990185
Actor loss: 77.673943
Action reg: 0.003989
  l1.weight: grad_norm = 0.270485
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.614774
Total gradient norm: 1.034350
=== Actor Training Debug (Iteration 7086) ===
Q mean: -76.030365
Q std: 27.873253
Actor loss: 76.034355
Action reg: 0.003989
  l1.weight: grad_norm = 0.229733
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.576703
Total gradient norm: 0.937804
=== Actor Training Debug (Iteration 7087) ===
Q mean: -77.806122
Q std: 28.747662
Actor loss: 77.810112
Action reg: 0.003993
  l1.weight: grad_norm = 0.101255
  l1.bias: grad_norm = 0.000666
  l2.weight: grad_norm = 0.218361
Total gradient norm: 0.442905
=== Actor Training Debug (Iteration 7088) ===
Q mean: -78.338737
Q std: 30.567715
Actor loss: 78.342712
Action reg: 0.003974
  l1.weight: grad_norm = 0.220450
  l1.bias: grad_norm = 0.001497
  l2.weight: grad_norm = 0.414532
Total gradient norm: 0.739720
=== Actor Training Debug (Iteration 7089) ===
Q mean: -74.356323
Q std: 30.334312
Actor loss: 74.360291
Action reg: 0.003965
  l1.weight: grad_norm = 0.189256
  l1.bias: grad_norm = 0.001558
  l2.weight: grad_norm = 0.425276
Total gradient norm: 0.736318
=== Actor Training Debug (Iteration 7090) ===
Q mean: -74.838417
Q std: 29.878641
Actor loss: 74.842400
Action reg: 0.003984
  l1.weight: grad_norm = 0.122980
  l1.bias: grad_norm = 0.001435
  l2.weight: grad_norm = 0.242374
Total gradient norm: 0.453426
=== Actor Training Debug (Iteration 7091) ===
Q mean: -75.517014
Q std: 29.798939
Actor loss: 75.520996
Action reg: 0.003982
  l1.weight: grad_norm = 0.356597
  l1.bias: grad_norm = 0.001033
  l2.weight: grad_norm = 0.953263
Total gradient norm: 2.394348
=== Actor Training Debug (Iteration 7092) ===
Q mean: -74.505753
Q std: 28.429604
Actor loss: 74.509735
Action reg: 0.003981
  l1.weight: grad_norm = 0.216233
  l1.bias: grad_norm = 0.000944
  l2.weight: grad_norm = 0.425104
Total gradient norm: 0.829860
=== Actor Training Debug (Iteration 7093) ===
Q mean: -72.175453
Q std: 29.782038
Actor loss: 72.179436
Action reg: 0.003982
  l1.weight: grad_norm = 0.172353
  l1.bias: grad_norm = 0.001927
  l2.weight: grad_norm = 0.298835
Total gradient norm: 0.479311
=== Actor Training Debug (Iteration 7094) ===
Q mean: -75.993515
Q std: 27.458130
Actor loss: 75.997498
Action reg: 0.003985
  l1.weight: grad_norm = 0.228279
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.470392
Total gradient norm: 0.916114
=== Actor Training Debug (Iteration 7095) ===
Q mean: -79.671242
Q std: 28.797884
Actor loss: 79.675217
Action reg: 0.003975
  l1.weight: grad_norm = 0.247313
  l1.bias: grad_norm = 0.001133
  l2.weight: grad_norm = 0.580855
Total gradient norm: 1.125777
=== Actor Training Debug (Iteration 7096) ===
Q mean: -74.070480
Q std: 28.786983
Actor loss: 74.074463
Action reg: 0.003983
  l1.weight: grad_norm = 0.206910
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.441652
Total gradient norm: 0.770367
=== Actor Training Debug (Iteration 7097) ===
Q mean: -76.588531
Q std: 28.597401
Actor loss: 76.592514
Action reg: 0.003985
  l1.weight: grad_norm = 0.011350
  l1.bias: grad_norm = 0.000856
  l2.weight: grad_norm = 0.024022
Total gradient norm: 0.047985
=== Actor Training Debug (Iteration 7098) ===
Q mean: -75.432053
Q std: 29.860674
Actor loss: 75.436043
Action reg: 0.003988
  l1.weight: grad_norm = 0.229135
  l1.bias: grad_norm = 0.000858
  l2.weight: grad_norm = 0.554147
Total gradient norm: 1.068653
=== Actor Training Debug (Iteration 7099) ===
Q mean: -74.142639
Q std: 29.357618
Actor loss: 74.146629
Action reg: 0.003987
  l1.weight: grad_norm = 0.177067
  l1.bias: grad_norm = 0.001210
  l2.weight: grad_norm = 0.379079
Total gradient norm: 0.775535
=== Actor Training Debug (Iteration 7100) ===
Q mean: -76.564911
Q std: 27.727781
Actor loss: 76.568893
Action reg: 0.003984
  l1.weight: grad_norm = 0.101229
  l1.bias: grad_norm = 0.001008
  l2.weight: grad_norm = 0.194318
Total gradient norm: 0.411094
Episode 121: Steps=100, Reward=-277.612, Buffer_size=12100
=== Actor Training Debug (Iteration 7101) ===
Q mean: -75.039558
Q std: 29.357222
Actor loss: 75.043556
Action reg: 0.003994
  l1.weight: grad_norm = 0.027268
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.058691
Total gradient norm: 0.125168
Total gradient norm: 0.481384ration 6486) ===
=== Actor Training Debug (Iteration 7112) ===
Q mean: -75.857315
Q std: 29.549435
Actor loss: 75.861298
Action reg: 0.003981
  l1.weight: grad_norm = 0.307108
  l1.bias: grad_norm = 0.002049
  l2.weight: grad_norm = 0.661260
Total gradient norm: 1.354725
=== Actor Training Debug (Iteration 7113) ===
Q mean: -75.657974
Q std: 28.592068
Actor loss: 75.661964
Action reg: 0.003989
  l1.weight: grad_norm = 0.058967
  l1.bias: grad_norm = 0.000891
  l2.weight: grad_norm = 0.111395
Total gradient norm: 0.203483
=== Actor Training Debug (Iteration 7114) ===
Q mean: -75.739647
Q std: 27.143007
Actor loss: 75.743637
Action reg: 0.003990
  l1.weight: grad_norm = 0.183050
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.363626
Total gradient norm: 0.637381
=== Actor Training Debug (Iteration 7115) ===
Q mean: -77.938248
Q std: 28.389067
Actor loss: 77.942230
Action reg: 0.003984
  l1.weight: grad_norm = 0.225976
  l1.bias: grad_norm = 0.001458
  l2.weight: grad_norm = 0.508393
Total gradient norm: 0.980824
=== Actor Training Debug (Iteration 7116) ===
Q mean: -75.418701
Q std: 28.928888
Actor loss: 75.422691
Action reg: 0.003990
  l1.weight: grad_norm = 0.076016
  l1.bias: grad_norm = 0.001110
  l2.weight: grad_norm = 0.163256
Total gradient norm: 0.336528
=== Actor Training Debug (Iteration 7117) ===
Q mean: -74.185333
Q std: 29.151503
Actor loss: 74.189308
Action reg: 0.003974
  l1.weight: grad_norm = 0.227006
  l1.bias: grad_norm = 0.001759
  l2.weight: grad_norm = 0.484285
Total gradient norm: 0.968264
=== Actor Training Debug (Iteration 7118) ===
Q mean: -75.952972
Q std: 30.326426
Actor loss: 75.956955
Action reg: 0.003981
  l1.weight: grad_norm = 0.115594
  l1.bias: grad_norm = 0.001409
  l2.weight: grad_norm = 0.213703
Total gradient norm: 0.439300
=== Actor Training Debug (Iteration 7119) ===
Q mean: -72.699638
Q std: 27.755074
Actor loss: 72.703629
Action reg: 0.003991
  l1.weight: grad_norm = 0.086963
  l1.bias: grad_norm = 0.000801
  l2.weight: grad_norm = 0.221634
Total gradient norm: 0.389086
=== Actor Training Debug (Iteration 7120) ===
Q mean: -76.809448
Q std: 30.140320
Actor loss: 76.813438
Action reg: 0.003988
  l1.weight: grad_norm = 0.065271
  l1.bias: grad_norm = 0.000695
  l2.weight: grad_norm = 0.137432
Total gradient norm: 0.242925
=== Actor Training Debug (Iteration 7121) ===
Q mean: -70.842499
Q std: 29.177879
Actor loss: 70.846474
Action reg: 0.003976
  l1.weight: grad_norm = 0.080719
  l1.bias: grad_norm = 0.001895
  l2.weight: grad_norm = 0.176541
Total gradient norm: 0.301352
=== Actor Training Debug (Iteration 7122) ===
Q mean: -76.167816
Q std: 28.421232
Actor loss: 76.171806
Action reg: 0.003990
  l1.weight: grad_norm = 0.047583
  l1.bias: grad_norm = 0.001051
  l2.weight: grad_norm = 0.102658
Total gradient norm: 0.184618
=== Actor Training Debug (Iteration 7123) ===
Q mean: -77.707329
Q std: 30.045338
Actor loss: 77.711311
Action reg: 0.003982
  l1.weight: grad_norm = 0.189082
  l1.bias: grad_norm = 0.001581
  l2.weight: grad_norm = 0.428924
Total gradient norm: 0.825567
=== Actor Training Debug (Iteration 7124) ===
Q mean: -77.327103
Q std: 29.176510
Actor loss: 77.331078
Action reg: 0.003979
  l1.weight: grad_norm = 0.197394
  l1.bias: grad_norm = 0.002106
  l2.weight: grad_norm = 0.469739
Total gradient norm: 0.993353
=== Actor Training Debug (Iteration 7125) ===
Q mean: -73.609421
Q std: 28.369593
Actor loss: 73.613396
Action reg: 0.003974
  l1.weight: grad_norm = 0.427170
  l1.bias: grad_norm = 0.001738
  l2.weight: grad_norm = 0.976401
Total gradient norm: 2.055037
=== Actor Training Debug (Iteration 7126) ===
Q mean: -76.799759
Q std: 29.133516
Actor loss: 76.803741
Action reg: 0.003980
  l1.weight: grad_norm = 0.038526
  l1.bias: grad_norm = 0.001615
  l2.weight: grad_norm = 0.085601
Total gradient norm: 0.160926
=== Actor Training Debug (Iteration 7127) ===
Q mean: -75.253708
Q std: 28.207502
Actor loss: 75.257683
Action reg: 0.003973
  l1.weight: grad_norm = 0.120233
  l1.bias: grad_norm = 0.001857
  l2.weight: grad_norm = 0.260001
Total gradient norm: 0.531343
=== Actor Training Debug (Iteration 7128) ===
Q mean: -74.399307
Q std: 28.575260
Actor loss: 74.403297
Action reg: 0.003991
  l1.weight: grad_norm = 0.104122
  l1.bias: grad_norm = 0.000702
  l2.weight: grad_norm = 0.235589
Total gradient norm: 0.469711
=== Actor Training Debug (Iteration 7129) ===
Q mean: -77.585426
Q std: 28.058447
Actor loss: 77.589401
Action reg: 0.003976
  l1.weight: grad_norm = 0.196979
  l1.bias: grad_norm = 0.001592
  l2.weight: grad_norm = 0.423158
Total gradient norm: 0.870984
=== Actor Training Debug (Iteration 7130) ===
Q mean: -77.210312
Q std: 27.653284
Actor loss: 77.214302
Action reg: 0.003990
  l1.weight: grad_norm = 0.147548
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.289896
Total gradient norm: 0.540535
=== Actor Training Debug (Iteration 7131) ===
Q mean: -77.872208
Q std: 27.433420
Actor loss: 77.876205
Action reg: 0.003997
  l1.weight: grad_norm = 0.039951
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.092228
Total gradient norm: 0.182785
=== Actor Training Debug (Iteration 7132) ===
Q mean: -75.824127
Q std: 29.013575
Actor loss: 75.828110
Action reg: 0.003981
  l1.weight: grad_norm = 0.340903
  l1.bias: grad_norm = 0.001337
  l2.weight: grad_norm = 0.781497
Total gradient norm: 1.657944
=== Actor Training Debug (Iteration 7133) ===
Q mean: -73.760635
Q std: 29.033842
Actor loss: 73.764610
Action reg: 0.003976
  l1.weight: grad_norm = 0.377772
  l1.bias: grad_norm = 0.001789
  l2.weight: grad_norm = 0.793255
Total gradient norm: 1.341839
=== Actor Training Debug (Iteration 7134) ===
Q mean: -75.427849
Q std: 28.738195
Actor loss: 75.431824
Action reg: 0.003978
  l1.weight: grad_norm = 0.605177
  l1.bias: grad_norm = 0.001593
  l2.weight: grad_norm = 1.339493
Total gradient norm: 2.649538
=== Actor Training Debug (Iteration 7135) ===
Q mean: -75.287689
Q std: 28.893654
Actor loss: 75.291679
Action reg: 0.003992
  l1.weight: grad_norm = 0.105992
  l1.bias: grad_norm = 0.000856
  l2.weight: grad_norm = 0.225920
Total gradient norm: 0.445170
=== Actor Training Debug (Iteration 7136) ===
Q mean: -77.947281
Q std: 28.322891
Actor loss: 77.951271
Action reg: 0.003989
  l1.weight: grad_norm = 0.194768
  l1.bias: grad_norm = 0.001310
  l2.weight: grad_norm = 0.358502
Total gradient norm: 0.575494
=== Actor Training Debug (Iteration 7137) ===
Q mean: -74.934509
Q std: 27.947407
Actor loss: 74.938499
Action reg: 0.003992
  l1.weight: grad_norm = 0.239306
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.485898
Total gradient norm: 0.979264
=== Actor Training Debug (Iteration 7138) ===
Q mean: -73.105492
Q std: 30.094255
Actor loss: 73.109467
Action reg: 0.003978
  l1.weight: grad_norm = 0.181174
  l1.bias: grad_norm = 0.001206
  l2.weight: grad_norm = 0.350256
Total gradient norm: 0.608910
=== Actor Training Debug (Iteration 7139) ===
Q mean: -73.772041
Q std: 29.718769
Actor loss: 73.776024
Action reg: 0.003985
  l1.weight: grad_norm = 0.145889
  l1.bias: grad_norm = 0.001123
  l2.weight: grad_norm = 0.290191
Total gradient norm: 0.596170
=== Actor Training Debug (Iteration 7140) ===
Q mean: -76.746109
Q std: 29.319963
Actor loss: 76.750107
Action reg: 0.003997
  l1.weight: grad_norm = 0.134295
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.358537
Total gradient norm: 0.874797
=== Actor Training Debug (Iteration 7141) ===
Q mean: -79.164810
Q std: 30.132950
Actor loss: 79.168793
Action reg: 0.003984
  l1.weight: grad_norm = 0.149648
  l1.bias: grad_norm = 0.001394
  l2.weight: grad_norm = 0.410716
Total gradient norm: 0.998610
=== Actor Training Debug (Iteration 7142) ===
Q mean: -76.272415
Q std: 28.349802
Actor loss: 76.276405
Action reg: 0.003989
  l1.weight: grad_norm = 0.198074
  l1.bias: grad_norm = 0.000837
  l2.weight: grad_norm = 0.411454
Total gradient norm: 0.801647
=== Actor Training Debug (Iteration 7143) ===
Q mean: -75.692581
Q std: 28.358040
Actor loss: 75.696571
Action reg: 0.003992
  l1.weight: grad_norm = 0.037249
  l1.bias: grad_norm = 0.000858
  l2.weight: grad_norm = 0.088016
Total gradient norm: 0.164506
=== Actor Training Debug (Iteration 7144) ===
Q mean: -76.175034
Q std: 27.577223
Actor loss: 76.179024
Action reg: 0.003993
  l1.weight: grad_norm = 0.200096
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.438276
Total gradient norm: 0.850618
=== Actor Training Debug (Iteration 7145) ===
Q mean: -75.206848
Q std: 28.416950
Actor loss: 75.210831
Action reg: 0.003986
  l1.weight: grad_norm = 0.099894
  l1.bias: grad_norm = 0.001406
  l2.weight: grad_norm = 0.205117
Total gradient norm: 0.466242
=== Actor Training Debug (Iteration 7146) ===
Q mean: -73.975067
Q std: 28.389524
Actor loss: 73.979057
Action reg: 0.003990
  l1.weight: grad_norm = 0.100888
  l1.bias: grad_norm = 0.000992
  l2.weight: grad_norm = 0.206543
Total gradient norm: 0.394695
=== Actor Training Debug (Iteration 7147) ===
Q mean: -74.614471
Q std: 28.887613
Actor loss: 74.618454
Action reg: 0.003984
  l1.weight: grad_norm = 0.090679
  l1.bias: grad_norm = 0.001158
  l2.weight: grad_norm = 0.200658
Total gradient norm: 0.361359
=== Actor Training Debug (Iteration 7148) ===
Q mean: -74.558167
Q std: 29.663227
Actor loss: 74.562157
Action reg: 0.003987
  l1.weight: grad_norm = 0.016278
  l1.bias: grad_norm = 0.001038
  l2.weight: grad_norm = 0.040402
Total gradient norm: 0.081742
=== Actor Training Debug (Iteration 7149) ===
Q mean: -74.744003
Q std: 28.769506
Actor loss: 74.747986
Action reg: 0.003983
  l1.weight: grad_norm = 0.108788
  l1.bias: grad_norm = 0.001152
  l2.weight: grad_norm = 0.217278
Total gradient norm: 0.393644
=== Actor Training Debug (Iteration 7150) ===
Q mean: -72.891342
Q std: 29.156788
Actor loss: 72.895325
Action reg: 0.003983
  l1.weight: grad_norm = 0.168769
  l1.bias: grad_norm = 0.001081
  l2.weight: grad_norm = 0.375171
Total gradient norm: 0.707413
=== Actor Training Debug (Iteration 7151) ===
Q mean: -72.250908
Q std: 29.807064
Actor loss: 72.254875
Action reg: 0.003969
  l1.weight: grad_norm = 0.258961
  l1.bias: grad_norm = 0.001736
  l2.weight: grad_norm = 0.627905
Total gradient norm: 1.482928
=== Actor Training Debug (Iteration 7152) ===
Q mean: -78.402748
Q std: 28.445625
Actor loss: 78.406715
Action reg: 0.003969
  l1.weight: grad_norm = 0.192076
  l1.bias: grad_norm = 0.002132
  l2.weight: grad_norm = 0.406828
Total gradient norm: 0.775037
=== Actor Training Debug (Iteration 7153) ===
Q mean: -78.960258
Q std: 28.952641
Actor loss: 78.964249
Action reg: 0.003991
  l1.weight: grad_norm = 0.006567
  l1.bias: grad_norm = 0.000863
  l2.weight: grad_norm = 0.014583
Total gradient norm: 0.028247
=== Actor Training Debug (Iteration 7154) ===
Q mean: -76.123283
Q std: 28.704741
Actor loss: 76.127266
Action reg: 0.003986
  l1.weight: grad_norm = 0.022376
  l1.bias: grad_norm = 0.001075
  l2.weight: grad_norm = 0.045626
Total gradient norm: 0.101794
=== Actor Training Debug (Iteration 7155) ===
Q mean: -76.623917
Q std: 27.582029
Actor loss: 76.627892
Action reg: 0.003977
  l1.weight: grad_norm = 0.396313
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.846351
Total gradient norm: 1.507627
=== Actor Training Debug (Iteration 7156) ===
Q mean: -73.900467
Q std: 28.777641
Actor loss: 73.904457
Action reg: 0.003993
  l1.weight: grad_norm = 0.179571
  l1.bias: grad_norm = 0.000454
  l2.weight: grad_norm = 0.429854
Total gradient norm: 0.890329
=== Actor Training Debug (Iteration 7157) ===
Q mean: -80.171860
Q std: 29.580505
Actor loss: 80.175850
Action reg: 0.003989
  l1.weight: grad_norm = 0.143855
  l1.bias: grad_norm = 0.001540
  l2.weight: grad_norm = 0.346927
Total gradient norm: 0.666924
=== Actor Training Debug (Iteration 7158) ===
Q mean: -76.667557
Q std: 28.441605
Actor loss: 76.671547
Action reg: 0.003988
  l1.weight: grad_norm = 0.050176
  l1.bias: grad_norm = 0.000832
  l2.weight: grad_norm = 0.128820
Total gradient norm: 0.236381
=== Actor Training Debug (Iteration 7159) ===
Q mean: -74.267792
Q std: 29.094007
Actor loss: 74.271774
Action reg: 0.003986
  l1.weight: grad_norm = 0.186122
  l1.bias: grad_norm = 0.001207
  l2.weight: grad_norm = 0.410348
Total gradient norm: 0.730093
=== Actor Training Debug (Iteration 7160) ===
Q mean: -74.633713
Q std: 27.459444
Actor loss: 74.637695
Action reg: 0.003986
  l1.weight: grad_norm = 0.461631
  l1.bias: grad_norm = 0.001534
  l2.weight: grad_norm = 0.948709
Total gradient norm: 1.864156
=== Actor Training Debug (Iteration 7161) ===
Q mean: -77.534836
Q std: 28.663887
Actor loss: 77.538818
Action reg: 0.003986
  l1.weight: grad_norm = 0.245590
  l1.bias: grad_norm = 0.001193
  l2.weight: grad_norm = 0.475199
Total gradient norm: 0.835472
=== Actor Training Debug (Iteration 7162) ===
Q mean: -75.899200
Q std: 27.735062
Actor loss: 75.903198
Action reg: 0.003995
  l1.weight: grad_norm = 0.094031
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.232851
Total gradient norm: 0.472014
=== Actor Training Debug (Iteration 7163) ===
Q mean: -73.660988
Q std: 30.027086
Actor loss: 73.664970
Action reg: 0.003980
  l1.weight: grad_norm = 0.124804
  l1.bias: grad_norm = 0.001093
  l2.weight: grad_norm = 0.252811
Total gradient norm: 0.425875
=== Actor Training Debug (Iteration 7164) ===
Q mean: -75.439278
Q std: 27.989923
Actor loss: 75.443268
Action reg: 0.003989
  l1.weight: grad_norm = 0.176488
  l1.bias: grad_norm = 0.001401
  l2.weight: grad_norm = 0.379453
Total gradient norm: 0.767620
=== Actor Training Debug (Iteration 7165) ===
Q mean: -76.963013
Q std: 28.609209
Actor loss: 76.967003
Action reg: 0.003989
  l1.weight: grad_norm = 0.250852
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.503186
Total gradient norm: 0.896666
=== Actor Training Debug (Iteration 7166) ===
Q mean: -76.818115
Q std: 28.391012
Actor loss: 76.822090
Action reg: 0.003972
  l1.weight: grad_norm = 0.121662
  l1.bias: grad_norm = 0.002580
  l2.weight: grad_norm = 0.237950
Total gradient norm: 0.421129
=== Actor Training Debug (Iteration 7167) ===
Q mean: -75.722885
Q std: 29.464712
Actor loss: 75.726875
Action reg: 0.003991
  l1.weight: grad_norm = 0.125090
  l1.bias: grad_norm = 0.000883
  l2.weight: grad_norm = 0.266024
Total gradient norm: 0.505476
=== Actor Training Debug (Iteration 7168) ===
Q mean: -73.128967
Q std: 28.528046
Actor loss: 73.132950
Action reg: 0.003981
  l1.weight: grad_norm = 0.133780
  l1.bias: grad_norm = 0.001183
  l2.weight: grad_norm = 0.276880
Total gradient norm: 0.532905
=== Actor Training Debug (Iteration 7169) ===
Q mean: -73.957031
Q std: 28.124258
Actor loss: 73.961014
Action reg: 0.003983
  l1.weight: grad_norm = 0.305765
  l1.bias: grad_norm = 0.001042
  l2.weight: grad_norm = 0.696053
Total gradient norm: 1.394581
=== Actor Training Debug (Iteration 7170) ===
Q mean: -76.496460
Q std: 29.147673
Actor loss: 76.500450
Action reg: 0.003994
  l1.weight: grad_norm = 0.105574
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.232722
Total gradient norm: 0.445086
=== Actor Training Debug (Iteration 7171) ===
Q mean: -77.620041
Q std: 29.145041
Actor loss: 77.624031
Action reg: 0.003989
  l1.weight: grad_norm = 0.106296
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.242802
Total gradient norm: 0.480181
=== Actor Training Debug (Iteration 7172) ===
Q mean: -73.871391
Q std: 30.152233
Actor loss: 73.875366
Action reg: 0.003977
  l1.weight: grad_norm = 0.084444
  l1.bias: grad_norm = 0.002896
  l2.weight: grad_norm = 0.186726
Total gradient norm: 0.388759
=== Actor Training Debug (Iteration 7173) ===
Q mean: -72.538513
Q std: 27.806353
Actor loss: 72.542496
Action reg: 0.003979
  l1.weight: grad_norm = 0.637266
  l1.bias: grad_norm = 0.002247
  l2.weight: grad_norm = 1.390477
Total gradient norm: 2.598518
=== Actor Training Debug (Iteration 7174) ===
Q mean: -74.779007
Q std: 27.330772
Actor loss: 74.782990
Action reg: 0.003982
  l1.weight: grad_norm = 0.197889
  l1.bias: grad_norm = 0.000659
  l2.weight: grad_norm = 0.367843
Total gradient norm: 0.782436
=== Actor Training Debug (Iteration 7175) ===
Q mean: -79.020950
Q std: 28.041586
Actor loss: 79.024940
Action reg: 0.003990
  l1.weight: grad_norm = 0.078645
  l1.bias: grad_norm = 0.000799
  l2.weight: grad_norm = 0.189845
Total gradient norm: 0.490591
=== Actor Training Debug (Iteration 7176) ===
Q mean: -77.199417
Q std: 28.554131
Actor loss: 77.203400
Action reg: 0.003982
  l1.weight: grad_norm = 0.185103
  l1.bias: grad_norm = 0.001185
  l2.weight: grad_norm = 0.388600
Total gradient norm: 0.776063
=== Actor Training Debug (Iteration 7177) ===
Q mean: -75.714516
Q std: 30.394623
Actor loss: 75.718491
Action reg: 0.003976
  l1.weight: grad_norm = 0.354061
  l1.bias: grad_norm = 0.001431
  l2.weight: grad_norm = 0.679488
Total gradient norm: 1.275061
=== Actor Training Debug (Iteration 7178) ===
Q mean: -76.661850
Q std: 29.761665
Actor loss: 76.665833
Action reg: 0.003980
  l1.weight: grad_norm = 0.481639
  l1.bias: grad_norm = 0.002427
  l2.weight: grad_norm = 0.905615
Total gradient norm: 1.607283
=== Actor Training Debug (Iteration 7179) ===
Q mean: -77.488022
Q std: 29.093899
Actor loss: 77.492012
Action reg: 0.003993
  l1.weight: grad_norm = 0.220115
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.447577
Total gradient norm: 0.766924
=== Actor Training Debug (Iteration 7180) ===
Q mean: -77.116508
Q std: 27.672163
Actor loss: 77.120491
Action reg: 0.003986
  l1.weight: grad_norm = 0.221276
  l1.bias: grad_norm = 0.000829
  l2.weight: grad_norm = 0.537777
Total gradient norm: 1.078788
=== Actor Training Debug (Iteration 7181) ===
Q mean: -80.501640
Q std: 27.335144
Actor loss: 80.505623
Action reg: 0.003986
  l1.weight: grad_norm = 0.328768
  l1.bias: grad_norm = 0.001311
  l2.weight: grad_norm = 0.748518
Total gradient norm: 1.557755
=== Actor Training Debug (Iteration 7182) ===
Q mean: -77.126511
Q std: 27.613661
Actor loss: 77.130493
Action reg: 0.003980
  l1.weight: grad_norm = 0.120957
  l1.bias: grad_norm = 0.002113
  l2.weight: grad_norm = 0.236026
Total gradient norm: 0.436090
=== Actor Training Debug (Iteration 7183) ===
Q mean: -78.732147
Q std: 29.237938
Actor loss: 78.736130
Action reg: 0.003979
  l1.weight: grad_norm = 0.096953
  l1.bias: grad_norm = 0.001917
  l2.weight: grad_norm = 0.193088
Total gradient norm: 0.358443
=== Actor Training Debug (Iteration 7184) ===
Q mean: -75.574097
Q std: 30.469328
Actor loss: 75.578079
Action reg: 0.003980
  l1.weight: grad_norm = 0.455172
  l1.bias: grad_norm = 0.002073
  l2.weight: grad_norm = 1.009878
Total gradient norm: 1.740538
=== Actor Training Debug (Iteration 7185) ===
Q mean: -76.278389
Q std: 29.199949
Actor loss: 76.282356
Action reg: 0.003970
  l1.weight: grad_norm = 0.216598
  l1.bias: grad_norm = 0.001739
  l2.weight: grad_norm = 0.434387
Total gradient norm: 0.886562
=== Actor Training Debug (Iteration 7186) ===
Q mean: -75.812485
Q std: 27.815498
Actor loss: 75.816467
Action reg: 0.003981
  l1.weight: grad_norm = 0.294874
  l1.bias: grad_norm = 0.001684
  l2.weight: grad_norm = 0.625853
Total gradient norm: 1.077274
=== Actor Training Debug (Iteration 7187) ===
Q mean: -75.927795
Q std: 29.542915
Actor loss: 75.931770
Action reg: 0.003974
  l1.weight: grad_norm = 0.351954
  l1.bias: grad_norm = 0.004017
  l2.weight: grad_norm = 0.648573
Total gradient norm: 1.228649
=== Actor Training Debug (Iteration 7188) ===
Q mean: -71.991287
Q std: 28.433086
Actor loss: 71.995270
Action reg: 0.003981
  l1.weight: grad_norm = 0.248465
  l1.bias: grad_norm = 0.002801
  l2.weight: grad_norm = 0.559697
Total gradient norm: 0.895600
=== Actor Training Debug (Iteration 7189) ===
Q mean: -71.564072
Q std: 30.808517
Actor loss: 71.568039
Action reg: 0.003968
  l1.weight: grad_norm = 0.131643
  l1.bias: grad_norm = 0.003685
  l2.weight: grad_norm = 0.242539
Total gradient norm: 0.417782
=== Actor Training Debug (Iteration 7190) ===
Q mean: -75.818443
Q std: 28.426168
Actor loss: 75.822426
Action reg: 0.003984
  l1.weight: grad_norm = 0.245795
  l1.bias: grad_norm = 0.001054
  l2.weight: grad_norm = 0.515905
Total gradient norm: 0.945601
=== Actor Training Debug (Iteration 7191) ===
Q mean: -73.218742
Q std: 28.268442
Actor loss: 73.222733
Action reg: 0.003986
  l1.weight: grad_norm = 0.078598
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.194751
Total gradient norm: 0.381523
=== Actor Training Debug (Iteration 7192) ===
Q mean: -75.292564
Q std: 28.944637
Actor loss: 75.296547
Action reg: 0.003984
  l1.weight: grad_norm = 0.220050
  l1.bias: grad_norm = 0.001672
  l2.weight: grad_norm = 0.589193
Total gradient norm: 1.464043
=== Actor Training Debug (Iteration 7193) ===
Q mean: -81.152519
Q std: 29.820280
Actor loss: 81.156487
Action reg: 0.003971
  l1.weight: grad_norm = 0.377427
  l1.bias: grad_norm = 0.002353
  l2.weight: grad_norm = 0.861799
Total gradient norm: 1.812381
=== Actor Training Debug (Iteration 7194) ===
Q mean: -77.566025
Q std: 28.832775
Actor loss: 77.570023
Action reg: 0.003996
  l1.weight: grad_norm = 0.156122
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.347018
Total gradient norm: 0.690636
=== Actor Training Debug (Iteration 7195) ===
Q mean: -74.061859
Q std: 29.676970
Actor loss: 74.065842
Action reg: 0.003986
  l1.weight: grad_norm = 0.134085
  l1.bias: grad_norm = 0.001053
  l2.weight: grad_norm = 0.337609
Total gradient norm: 0.658800
=== Actor Training Debug (Iteration 7196) ===
Q mean: -73.849014
Q std: 29.428493
Actor loss: 73.852989
Action reg: 0.003972
  l1.weight: grad_norm = 0.138847
  l1.bias: grad_norm = 0.001441
  l2.weight: grad_norm = 0.311927
Total gradient norm: 0.525377
=== Actor Training Debug (Iteration 7197) ===
Q mean: -79.261284
Q std: 27.827394
Actor loss: 79.265274
Action reg: 0.003990
  l1.weight: grad_norm = 0.262517
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.504470
Total gradient norm: 0.893708
=== Actor Training Debug (Iteration 7198) ===
Q mean: -78.829636
Q std: 29.075554
Actor loss: 78.833618
Action reg: 0.003984
  l1.weight: grad_norm = 0.238416
  l1.bias: grad_norm = 0.001390
  l2.weight: grad_norm = 0.491883
Total gradient norm: 0.886374
=== Actor Training Debug (Iteration 7199) ===
Q mean: -75.644318
Q std: 30.367199
Actor loss: 75.648293
Action reg: 0.003978
  l1.weight: grad_norm = 0.249485
  l1.bias: grad_norm = 0.001487
  l2.weight: grad_norm = 0.578421
Total gradient norm: 1.078117
=== Actor Training Debug (Iteration 7200) ===
Q mean: -77.908463
Q std: 29.027451
Actor loss: 77.912453
Action reg: 0.003993
  l1.weight: grad_norm = 0.099962
  l1.bias: grad_norm = 0.000706
  l2.weight: grad_norm = 0.254274
Total gradient norm: 0.434381
=== Actor Training Debug (Iteration 7201) ===
Q mean: -75.071808
Q std: 28.291271
Actor loss: 75.075790
Action reg: 0.003984
  l1.weight: grad_norm = 0.194809
  l1.bias: grad_norm = 0.000904
  l2.weight: grad_norm = 0.552089
Total gradient norm: 1.213218
=== Actor Training Debug (Iteration 7202) ===
Q mean: -75.605980
Q std: 28.585621
Actor loss: 75.609970
Action reg: 0.003991
  l1.weight: grad_norm = 0.089699
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.201942
Total gradient norm: 0.436775
=== Actor Training Debug (Iteration 7203) ===
Q mean: -76.595520
Q std: 29.556936
Actor loss: 76.599503
Action reg: 0.003984
  l1.weight: grad_norm = 0.210756
  l1.bias: grad_norm = 0.000889
  l2.weight: grad_norm = 0.415986
Total gradient norm: 0.740456
=== Actor Training Debug (Iteration 7204) ===
Q mean: -76.876190
Q std: 28.811337
Actor loss: 76.880173
Action reg: 0.003981
  l1.weight: grad_norm = 0.189778
  l1.bias: grad_norm = 0.002121
  l2.weight: grad_norm = 0.438277
Total gradient norm: 0.844949
=== Actor Training Debug (Iteration 7205) ===
Q mean: -74.407341
Q std: 29.002607
Actor loss: 74.411331
Action reg: 0.003994
  l1.weight: grad_norm = 0.134153
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.297769
Total gradient norm: 0.685123
=== Actor Training Debug (Iteration 7206) ===
Q mean: -74.772552
Q std: 28.770411
Actor loss: 74.776535
Action reg: 0.003981
  l1.weight: grad_norm = 0.168649
  l1.bias: grad_norm = 0.001882
  l2.weight: grad_norm = 0.401936
Total gradient norm: 0.866687
=== Actor Training Debug (Iteration 7207) ===
Q mean: -76.272163
Q std: 29.769310
Actor loss: 76.276138
Action reg: 0.003976
  l1.weight: grad_norm = 0.422152
  l1.bias: grad_norm = 0.000882
  l2.weight: grad_norm = 0.959734
Total gradient norm: 1.985678
=== Actor Training Debug (Iteration 7208) ===
Q mean: -75.753365
Q std: 28.652893
Actor loss: 75.757339
Action reg: 0.003979
  l1.weight: grad_norm = 0.219542
  l1.bias: grad_norm = 0.001932
  l2.weight: grad_norm = 0.477327
Total gradient norm: 1.054737
=== Actor Training Debug (Iteration 7209) ===
Q mean: -75.063889
Q std: 28.640446
Actor loss: 75.067879
Action reg: 0.003989
  l1.weight: grad_norm = 0.226356
  l1.bias: grad_norm = 0.001047
  l2.weight: grad_norm = 0.489428
Total gradient norm: 0.856483
=== Actor Training Debug (Iteration 7210) ===
Q mean: -78.879288
Q std: 28.409607
Actor loss: 78.883278
Action reg: 0.003989
  l1.weight: grad_norm = 0.249520
  l1.bias: grad_norm = 0.000542
  l2.weight: grad_norm = 0.556902
Total gradient norm: 1.098888
=== Actor Training Debug (Iteration 7211) ===
Q mean: -79.831833
Q std: 28.479017
Actor loss: 79.835823
Action reg: 0.003987
  l1.weight: grad_norm = 0.210897
  l1.bias: grad_norm = 0.000567
  l2.weight: grad_norm = 0.448156
Total gradient norm: 0.832423
=== Actor Training Debug (Iteration 7212) ===
Q mean: -73.237526
Q std: 29.907471
Actor loss: 73.241508
Action reg: 0.003983
  l1.weight: grad_norm = 0.312525
  l1.bias: grad_norm = 0.001699
  l2.weight: grad_norm = 0.590268
Total gradient norm: 1.001206
=== Actor Training Debug (Iteration 7213) ===
Q mean: -76.023293
Q std: 28.732176
Actor loss: 76.027290
Action reg: 0.003998
  l1.weight: grad_norm = 0.071155
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.179115
Total gradient norm: 0.322272
=== Actor Training Debug (Iteration 7214) ===
Q mean: -73.429443
Q std: 29.977386
Actor loss: 73.433426
Action reg: 0.003982
  l1.weight: grad_norm = 0.087011
  l1.bias: grad_norm = 0.002030
  l2.weight: grad_norm = 0.207779
Total gradient norm: 0.453683
=== Actor Training Debug (Iteration 7215) ===
Q mean: -77.131424
Q std: 28.200485
Actor loss: 77.135406
Action reg: 0.003985
  l1.weight: grad_norm = 2.486723
  l1.bias: grad_norm = 0.003218
  l2.weight: grad_norm = 4.680124
Total gradient norm: 7.740805
=== Actor Training Debug (Iteration 7216) ===
Q mean: -78.668823
Q std: 27.690207
Actor loss: 78.672813
Action reg: 0.003987
  l1.weight: grad_norm = 0.378586
  l1.bias: grad_norm = 0.002099
  l2.weight: grad_norm = 0.863058
Total gradient norm: 1.625685
=== Actor Training Debug (Iteration 7217) ===
Q mean: -75.446701
Q std: 28.466139
Actor loss: 75.450684
Action reg: 0.003985
  l1.weight: grad_norm = 0.172454
  l1.bias: grad_norm = 0.001505
  l2.weight: grad_norm = 0.322529
Total gradient norm: 0.517087
=== Actor Training Debug (Iteration 7218) ===
Q mean: -78.574883
Q std: 29.549179
Actor loss: 78.578857
Action reg: 0.003975
  l1.weight: grad_norm = 0.680572
  l1.bias: grad_norm = 0.001223
  l2.weight: grad_norm = 1.338126
Total gradient norm: 2.502425
=== Actor Training Debug (Iteration 7219) ===
Q mean: -76.549019
Q std: 28.906925
Actor loss: 76.553001
Action reg: 0.003982
  l1.weight: grad_norm = 0.120280
  l1.bias: grad_norm = 0.001350
  l2.weight: grad_norm = 0.277342
Total gradient norm: 0.547619
=== Actor Training Debug (Iteration 7220) ===
Q mean: -74.797806
Q std: 28.379900
Actor loss: 74.801773
Action reg: 0.003971
  l1.weight: grad_norm = 0.692572
  l1.bias: grad_norm = 0.003016
  l2.weight: grad_norm = 1.440268
Total gradient norm: 3.201489
=== Actor Training Debug (Iteration 7221) ===
Q mean: -76.929367
Q std: 29.218597
Actor loss: 76.933350
Action reg: 0.003983
  l1.weight: grad_norm = 0.076278
  l1.bias: grad_norm = 0.002147
  l2.weight: grad_norm = 0.157135
Total gradient norm: 0.332094
=== Actor Training Debug (Iteration 7222) ===
Q mean: -73.156113
Q std: 30.084509
Actor loss: 73.160095
Action reg: 0.003981
  l1.weight: grad_norm = 0.431428
  l1.bias: grad_norm = 0.002919
  l2.weight: grad_norm = 0.903367
Total gradient norm: 1.555073
=== Actor Training Debug (Iteration 7223) ===
Q mean: -73.741074
Q std: 29.208279
Actor loss: 73.745056
Action reg: 0.003984
  l1.weight: grad_norm = 0.082590
  l1.bias: grad_norm = 0.001179
  l2.weight: grad_norm = 0.160308
Total gradient norm: 0.267576
=== Actor Training Debug (Iteration 7224) ===
Q mean: -74.713799
Q std: 28.881773
Actor loss: 74.717773
Action reg: 0.003978
  l1.weight: grad_norm = 0.076835
  l1.bias: grad_norm = 0.002039
  l2.weight: grad_norm = 0.172219
Total gradient norm: 0.329498
=== Actor Training Debug (Iteration 7225) ===
Q mean: -76.385117
Q std: 29.347233
Actor loss: 76.389107
Action reg: 0.003993
  l1.weight: grad_norm = 0.136899
  l1.bias: grad_norm = 0.000478
  l2.weight: grad_norm = 0.311456
Total gradient norm: 0.634212
=== Actor Training Debug (Iteration 7226) ===
Q mean: -76.182693
Q std: 27.430408
Actor loss: 76.186676
Action reg: 0.003983
  l1.weight: grad_norm = 0.310667
  l1.bias: grad_norm = 0.001457
  l2.weight: grad_norm = 0.606173
Total gradient norm: 1.086613
=== Actor Training Debug (Iteration 7227) ===
Q mean: -72.568489
Q std: 28.031055
Actor loss: 72.572479
Action reg: 0.003989
  l1.weight: grad_norm = 0.056266
  l1.bias: grad_norm = 0.001866
  l2.weight: grad_norm = 0.127912
Total gradient norm: 0.267303
=== Actor Training Debug (Iteration 7228) ===
Q mean: -76.687553
Q std: 30.480598
Actor loss: 76.691544
Action reg: 0.003993
  l1.weight: grad_norm = 0.243169
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.508205
Total gradient norm: 1.156629
=== Actor Training Debug (Iteration 7229) ===
Q mean: -77.374191
Q std: 29.631998
Actor loss: 77.378166
Action reg: 0.003971
  l1.weight: grad_norm = 0.169802
  l1.bias: grad_norm = 0.004761
  l2.weight: grad_norm = 0.411611
Total gradient norm: 0.729897
=== Actor Training Debug (Iteration 7230) ===
Q mean: -77.051941
Q std: 27.746746
Actor loss: 77.055939
Action reg: 0.003995
  l1.weight: grad_norm = 0.071799
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.136486
Total gradient norm: 0.259748
=== Actor Training Debug (Iteration 7231) ===
Q mean: -75.613251
Q std: 29.358974
Actor loss: 75.617233
Action reg: 0.003982
  l1.weight: grad_norm = 0.492916
  l1.bias: grad_norm = 0.001228
  l2.weight: grad_norm = 0.987110
Total gradient norm: 1.782566
=== Actor Training Debug (Iteration 7232) ===
Q mean: -76.416763
Q std: 28.743023
Actor loss: 76.420753
Action reg: 0.003990
  l1.weight: grad_norm = 0.009094
  l1.bias: grad_norm = 0.002270
  l2.weight: grad_norm = 0.028303
Total gradient norm: 0.079391
=== Actor Training Debug (Iteration 7233) ===
Q mean: -72.650955
Q std: 28.509979
Actor loss: 72.654922
Action reg: 0.003964
  l1.weight: grad_norm = 0.327862
  l1.bias: grad_norm = 0.006157
  l2.weight: grad_norm = 0.633113
Total gradient norm: 1.168577
=== Actor Training Debug (Iteration 7234) ===
Q mean: -79.496704
Q std: 28.354172
Actor loss: 79.500694
Action reg: 0.003990
  l1.weight: grad_norm = 0.257651
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.525703
Total gradient norm: 0.998559
=== Actor Training Debug (Iteration 7235) ===
Q mean: -76.653893
Q std: 29.006031
Actor loss: 76.657875
Action reg: 0.003984
  l1.weight: grad_norm = 0.146947
  l1.bias: grad_norm = 0.003399
  l2.weight: grad_norm = 0.338493
Total gradient norm: 0.687444
=== Actor Training Debug (Iteration 7236) ===
Q mean: -74.750549
Q std: 29.849751
Actor loss: 74.754532
Action reg: 0.003986
  l1.weight: grad_norm = 0.224954
  l1.bias: grad_norm = 0.001448
  l2.weight: grad_norm = 0.466071
Total gradient norm: 0.994887
=== Actor Training Debug (Iteration 7237) ===
Q mean: -74.366196
Q std: 29.636976
Actor loss: 74.370178
Action reg: 0.003985
  l1.weight: grad_norm = 0.222475
  l1.bias: grad_norm = 0.001274
  l2.weight: grad_norm = 0.449221
Total gradient norm: 0.668383
=== Actor Training Debug (Iteration 7238) ===
Q mean: -80.604393
Q std: 29.728096
Actor loss: 80.608368
Action reg: 0.003973
  l1.weight: grad_norm = 0.342768
  l1.bias: grad_norm = 0.003227
  l2.weight: grad_norm = 0.724151
Total gradient norm: 1.446656
=== Actor Training Debug (Iteration 7239) ===
Q mean: -79.015755
Q std: 26.517895
Actor loss: 79.019745
Action reg: 0.003990
  l1.weight: grad_norm = 0.417100
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.914478
Total gradient norm: 2.059985
=== Actor Training Debug (Iteration 7240) ===
Q mean: -76.886765
Q std: 28.060951
Actor loss: 76.890755
Action reg: 0.003987
  l1.weight: grad_norm = 0.242285
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.526802
Total gradient norm: 1.011619
=== Actor Training Debug (Iteration 7241) ===
Q mean: -72.769318
Q std: 27.346161
Actor loss: 72.773300
Action reg: 0.003983
  l1.weight: grad_norm = 0.121120
  l1.bias: grad_norm = 0.001655
  l2.weight: grad_norm = 0.247920
Total gradient norm: 0.497340
=== Actor Training Debug (Iteration 7242) ===
Q mean: -77.304443
Q std: 27.708778
Actor loss: 77.308434
Action reg: 0.003991
  l1.weight: grad_norm = 0.062702
  l1.bias: grad_norm = 0.001288
  l2.weight: grad_norm = 0.121192
Total gradient norm: 0.196227
=== Actor Training Debug (Iteration 7243) ===
Q mean: -73.710320
Q std: 29.454845
Actor loss: 73.714302
Action reg: 0.003980
  l1.weight: grad_norm = 0.142541
  l1.bias: grad_norm = 0.002117
  l2.weight: grad_norm = 0.320644
Total gradient norm: 0.654462
=== Actor Training Debug (Iteration 7244) ===
Q mean: -73.814651
Q std: 30.137089
Actor loss: 73.818634
Action reg: 0.003982
  l1.weight: grad_norm = 0.043535
  l1.bias: grad_norm = 0.003404
  l2.weight: grad_norm = 0.104113
Total gradient norm: 0.230717
=== Actor Training Debug (Iteration 7245) ===
Q mean: -73.006104
Q std: 29.815220
Actor loss: 73.010078
Action reg: 0.003975
  l1.weight: grad_norm = 0.174787
  l1.bias: grad_norm = 0.002623
  l2.weight: grad_norm = 0.379052
Total gradient norm: 0.784545
Total gradient norm: 0.852500ration 6486) ===
=== Actor Training Debug (Iteration 7256) ===
Q mean: -74.538666
Q std: 27.915194
Actor loss: 74.542648
Action reg: 0.003979
  l1.weight: grad_norm = 0.244853
  l1.bias: grad_norm = 0.002640
  l2.weight: grad_norm = 0.501741
Total gradient norm: 0.997102
=== Actor Training Debug (Iteration 7257) ===
Q mean: -75.454391
Q std: 28.910894
Actor loss: 75.458374
Action reg: 0.003984
  l1.weight: grad_norm = 0.187527
  l1.bias: grad_norm = 0.002059
  l2.weight: grad_norm = 0.419716
Total gradient norm: 1.001080
=== Actor Training Debug (Iteration 7258) ===
Q mean: -75.861816
Q std: 28.502041
Actor loss: 75.865807
Action reg: 0.003988
  l1.weight: grad_norm = 0.195783
  l1.bias: grad_norm = 0.001690
  l2.weight: grad_norm = 0.407721
Total gradient norm: 0.718102
=== Actor Training Debug (Iteration 7259) ===
Q mean: -76.800415
Q std: 28.078604
Actor loss: 76.804398
Action reg: 0.003983
  l1.weight: grad_norm = 0.307377
  l1.bias: grad_norm = 0.001851
  l2.weight: grad_norm = 0.579516
Total gradient norm: 1.131081
=== Actor Training Debug (Iteration 7260) ===
Q mean: -76.467422
Q std: 28.491459
Actor loss: 76.471397
Action reg: 0.003978
  l1.weight: grad_norm = 0.158925
  l1.bias: grad_norm = 0.001800
  l2.weight: grad_norm = 0.313098
Total gradient norm: 0.590899
=== Actor Training Debug (Iteration 7261) ===
Q mean: -74.928658
Q std: 28.419792
Actor loss: 74.932640
Action reg: 0.003980
  l1.weight: grad_norm = 0.468400
  l1.bias: grad_norm = 0.002812
  l2.weight: grad_norm = 0.885507
Total gradient norm: 1.518268
=== Actor Training Debug (Iteration 7262) ===
Q mean: -76.575577
Q std: 29.432188
Actor loss: 76.579552
Action reg: 0.003974
  l1.weight: grad_norm = 0.352827
  l1.bias: grad_norm = 0.003902
  l2.weight: grad_norm = 0.665057
Total gradient norm: 1.171877
=== Actor Training Debug (Iteration 7263) ===
Q mean: -77.001442
Q std: 29.164356
Actor loss: 77.005432
Action reg: 0.003987
  l1.weight: grad_norm = 0.103986
  l1.bias: grad_norm = 0.001356
  l2.weight: grad_norm = 0.248184
Total gradient norm: 0.451075
=== Actor Training Debug (Iteration 7264) ===
Q mean: -74.875412
Q std: 29.129044
Actor loss: 74.879387
Action reg: 0.003972
  l1.weight: grad_norm = 0.195901
  l1.bias: grad_norm = 0.004164
  l2.weight: grad_norm = 0.522945
Total gradient norm: 1.023051
=== Actor Training Debug (Iteration 7265) ===
Q mean: -76.706520
Q std: 28.504642
Actor loss: 76.710480
Action reg: 0.003958
  l1.weight: grad_norm = 0.182811
  l1.bias: grad_norm = 0.005579
  l2.weight: grad_norm = 0.403759
Total gradient norm: 0.856888
=== Actor Training Debug (Iteration 7266) ===
Q mean: -75.589317
Q std: 29.877470
Actor loss: 75.593292
Action reg: 0.003971
  l1.weight: grad_norm = 0.054840
  l1.bias: grad_norm = 0.005112
  l2.weight: grad_norm = 0.126975
Total gradient norm: 0.256988
=== Actor Training Debug (Iteration 7267) ===
Q mean: -74.783600
Q std: 28.578217
Actor loss: 74.787582
Action reg: 0.003983
  l1.weight: grad_norm = 0.266561
  l1.bias: grad_norm = 0.002281
  l2.weight: grad_norm = 0.599720
Total gradient norm: 1.125516
=== Actor Training Debug (Iteration 7268) ===
Q mean: -75.745605
Q std: 29.473063
Actor loss: 75.749596
Action reg: 0.003987
  l1.weight: grad_norm = 0.239354
  l1.bias: grad_norm = 0.000906
  l2.weight: grad_norm = 0.520273
Total gradient norm: 1.106294
=== Actor Training Debug (Iteration 7269) ===
Q mean: -75.199623
Q std: 28.599791
Actor loss: 75.203606
Action reg: 0.003984
  l1.weight: grad_norm = 0.060495
  l1.bias: grad_norm = 0.003596
  l2.weight: grad_norm = 0.113841
Total gradient norm: 0.214685
=== Actor Training Debug (Iteration 7270) ===
Q mean: -75.805557
Q std: 27.285082
Actor loss: 75.809540
Action reg: 0.003982
  l1.weight: grad_norm = 0.266738
  l1.bias: grad_norm = 0.001819
  l2.weight: grad_norm = 0.540935
Total gradient norm: 1.021524
=== Actor Training Debug (Iteration 7271) ===
Q mean: -75.931931
Q std: 28.148180
Actor loss: 75.935913
Action reg: 0.003986
  l1.weight: grad_norm = 0.303411
  l1.bias: grad_norm = 0.002439
  l2.weight: grad_norm = 0.658343
Total gradient norm: 1.095292
=== Actor Training Debug (Iteration 7272) ===
Q mean: -74.313622
Q std: 29.021105
Actor loss: 74.317596
Action reg: 0.003975
  l1.weight: grad_norm = 0.261367
  l1.bias: grad_norm = 0.003762
  l2.weight: grad_norm = 0.536034
Total gradient norm: 0.909678
=== Actor Training Debug (Iteration 7273) ===
Q mean: -78.699646
Q std: 27.115860
Actor loss: 78.703644
Action reg: 0.003999
  l1.weight: grad_norm = 0.115097
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.243469
Total gradient norm: 0.468744
=== Actor Training Debug (Iteration 7274) ===
Q mean: -76.749397
Q std: 30.082922
Actor loss: 76.753372
Action reg: 0.003972
  l1.weight: grad_norm = 0.223975
  l1.bias: grad_norm = 0.006001
  l2.weight: grad_norm = 0.433612
Total gradient norm: 0.804692
=== Actor Training Debug (Iteration 7275) ===
Q mean: -77.160934
Q std: 30.102436
Actor loss: 77.164917
Action reg: 0.003983
  l1.weight: grad_norm = 0.153722
  l1.bias: grad_norm = 0.002301
  l2.weight: grad_norm = 0.335376
Total gradient norm: 0.690266
=== Actor Training Debug (Iteration 7276) ===
Q mean: -78.006531
Q std: 30.807215
Actor loss: 78.010506
Action reg: 0.003973
  l1.weight: grad_norm = 0.053414
  l1.bias: grad_norm = 0.006120
  l2.weight: grad_norm = 0.120085
Total gradient norm: 0.261854
=== Actor Training Debug (Iteration 7277) ===
Q mean: -74.250580
Q std: 28.989061
Actor loss: 74.254547
Action reg: 0.003966
  l1.weight: grad_norm = 0.079777
  l1.bias: grad_norm = 0.007908
  l2.weight: grad_norm = 0.185265
Total gradient norm: 0.443360
=== Actor Training Debug (Iteration 7278) ===
Q mean: -78.934204
Q std: 28.378632
Actor loss: 78.938179
Action reg: 0.003977
  l1.weight: grad_norm = 0.139977
  l1.bias: grad_norm = 0.003912
  l2.weight: grad_norm = 0.310173
Total gradient norm: 0.551367
=== Actor Training Debug (Iteration 7279) ===
Q mean: -74.660645
Q std: 28.983913
Actor loss: 74.664627
Action reg: 0.003982
  l1.weight: grad_norm = 0.020596
  l1.bias: grad_norm = 0.003745
  l2.weight: grad_norm = 0.049236
Total gradient norm: 0.128997
=== Actor Training Debug (Iteration 7280) ===
Q mean: -74.912399
Q std: 29.565578
Actor loss: 74.916382
Action reg: 0.003981
  l1.weight: grad_norm = 0.037336
  l1.bias: grad_norm = 0.003653
  l2.weight: grad_norm = 0.082480
Total gradient norm: 0.191453
=== Actor Training Debug (Iteration 7281) ===
Q mean: -78.474472
Q std: 28.901463
Actor loss: 78.478462
Action reg: 0.003988
  l1.weight: grad_norm = 0.105414
  l1.bias: grad_norm = 0.001404
  l2.weight: grad_norm = 0.220846
Total gradient norm: 0.397955
=== Actor Training Debug (Iteration 7282) ===
Q mean: -77.519737
Q std: 29.753849
Actor loss: 77.523735
Action reg: 0.003995
  l1.weight: grad_norm = 0.297431
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.617292
Total gradient norm: 1.256681
=== Actor Training Debug (Iteration 7283) ===
Q mean: -75.664413
Q std: 29.846003
Actor loss: 75.668396
Action reg: 0.003980
  l1.weight: grad_norm = 0.076172
  l1.bias: grad_norm = 0.004269
  l2.weight: grad_norm = 0.177337
Total gradient norm: 0.374859
=== Actor Training Debug (Iteration 7284) ===
Q mean: -75.214775
Q std: 28.340721
Actor loss: 75.218758
Action reg: 0.003982
  l1.weight: grad_norm = 0.084560
  l1.bias: grad_norm = 0.002478
  l2.weight: grad_norm = 0.173348
Total gradient norm: 0.329148
=== Actor Training Debug (Iteration 7285) ===
Q mean: -78.360794
Q std: 29.791952
Actor loss: 78.364777
Action reg: 0.003982
  l1.weight: grad_norm = 0.263684
  l1.bias: grad_norm = 0.001692
  l2.weight: grad_norm = 0.491882
Total gradient norm: 0.838024
=== Actor Training Debug (Iteration 7286) ===
Q mean: -79.560600
Q std: 28.519690
Actor loss: 79.564590
Action reg: 0.003988
  l1.weight: grad_norm = 0.210581
  l1.bias: grad_norm = 0.001483
  l2.weight: grad_norm = 0.488312
Total gradient norm: 0.883323
=== Actor Training Debug (Iteration 7287) ===
Q mean: -72.991440
Q std: 27.979761
Actor loss: 72.995415
Action reg: 0.003974
  l1.weight: grad_norm = 0.905507
  l1.bias: grad_norm = 0.003733
  l2.weight: grad_norm = 1.660481
Total gradient norm: 3.539107
=== Actor Training Debug (Iteration 7288) ===
Q mean: -75.597900
Q std: 28.602028
Actor loss: 75.601868
Action reg: 0.003971
  l1.weight: grad_norm = 0.269858
  l1.bias: grad_norm = 0.003945
  l2.weight: grad_norm = 0.655684
Total gradient norm: 1.264662
=== Actor Training Debug (Iteration 7289) ===
Q mean: -73.916290
Q std: 30.658228
Actor loss: 73.920258
Action reg: 0.003964
  l1.weight: grad_norm = 0.251001
  l1.bias: grad_norm = 0.007035
  l2.weight: grad_norm = 0.573181
Total gradient norm: 1.085634
=== Actor Training Debug (Iteration 7290) ===
Q mean: -76.907944
Q std: 30.843081
Actor loss: 76.911911
Action reg: 0.003966
  l1.weight: grad_norm = 0.299652
  l1.bias: grad_norm = 0.006179
  l2.weight: grad_norm = 0.636405
Total gradient norm: 1.201233
=== Actor Training Debug (Iteration 7291) ===
Q mean: -76.028366
Q std: 29.251593
Actor loss: 76.032349
Action reg: 0.003981
  l1.weight: grad_norm = 0.285218
  l1.bias: grad_norm = 0.001763
  l2.weight: grad_norm = 0.633352
Total gradient norm: 1.301458
=== Actor Training Debug (Iteration 7292) ===
Q mean: -75.801315
Q std: 29.357491
Actor loss: 75.805298
Action reg: 0.003986
  l1.weight: grad_norm = 0.106881
  l1.bias: grad_norm = 0.003485
  l2.weight: grad_norm = 0.241392
Total gradient norm: 0.493486
=== Actor Training Debug (Iteration 7293) ===
Q mean: -78.932190
Q std: 28.780655
Actor loss: 78.936165
Action reg: 0.003977
  l1.weight: grad_norm = 0.230038
  l1.bias: grad_norm = 0.003703
  l2.weight: grad_norm = 0.514227
Total gradient norm: 1.053470
=== Actor Training Debug (Iteration 7294) ===
Q mean: -76.121231
Q std: 29.429779
Actor loss: 76.125214
Action reg: 0.003985
  l1.weight: grad_norm = 0.184737
  l1.bias: grad_norm = 0.001729
  l2.weight: grad_norm = 0.442934
Total gradient norm: 0.974348
=== Actor Training Debug (Iteration 7295) ===
Q mean: -77.282898
Q std: 28.614437
Actor loss: 77.286880
Action reg: 0.003980
  l1.weight: grad_norm = 0.049388
  l1.bias: grad_norm = 0.004702
  l2.weight: grad_norm = 0.126226
Total gradient norm: 0.293625
=== Actor Training Debug (Iteration 7296) ===
Q mean: -78.453583
Q std: 30.110788
Actor loss: 78.457565
Action reg: 0.003985
  l1.weight: grad_norm = 0.113534
  l1.bias: grad_norm = 0.001987
  l2.weight: grad_norm = 0.243185
Total gradient norm: 0.467952
=== Actor Training Debug (Iteration 7297) ===
Q mean: -77.312531
Q std: 27.794477
Actor loss: 77.316513
Action reg: 0.003980
  l1.weight: grad_norm = 0.188125
  l1.bias: grad_norm = 0.003409
  l2.weight: grad_norm = 0.373908
Total gradient norm: 0.638061
=== Actor Training Debug (Iteration 7298) ===
Q mean: -75.636581
Q std: 29.092630
Actor loss: 75.640579
Action reg: 0.003997
  l1.weight: grad_norm = 0.090656
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.171266
Total gradient norm: 0.310800
=== Actor Training Debug (Iteration 7299) ===
Q mean: -76.538788
Q std: 29.325445
Actor loss: 76.542778
Action reg: 0.003989
  l1.weight: grad_norm = 0.419203
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 1.004129
Total gradient norm: 2.225528
=== Actor Training Debug (Iteration 7300) ===
Q mean: -76.513969
Q std: 29.456593
Actor loss: 76.517960
Action reg: 0.003989
  l1.weight: grad_norm = 0.098058
  l1.bias: grad_norm = 0.003327
  l2.weight: grad_norm = 0.193799
Total gradient norm: 0.386391
=== Actor Training Debug (Iteration 7301) ===
Q mean: -77.768921
Q std: 29.113768
Actor loss: 77.772903
Action reg: 0.003980
  l1.weight: grad_norm = 0.060763
  l1.bias: grad_norm = 0.003893
  l2.weight: grad_norm = 0.145309
Total gradient norm: 0.347175
=== Actor Training Debug (Iteration 7302) ===
Q mean: -77.501488
Q std: 28.203913
Actor loss: 77.505470
Action reg: 0.003980
  l1.weight: grad_norm = 0.372499
  l1.bias: grad_norm = 0.002677
  l2.weight: grad_norm = 0.733063
Total gradient norm: 1.415932
=== Actor Training Debug (Iteration 7303) ===
Q mean: -76.763611
Q std: 29.714842
Actor loss: 76.767586
Action reg: 0.003972
  l1.weight: grad_norm = 0.132749
  l1.bias: grad_norm = 0.004103
  l2.weight: grad_norm = 0.316642
Total gradient norm: 0.550692
=== Actor Training Debug (Iteration 7304) ===
Q mean: -78.982086
Q std: 30.539736
Actor loss: 78.986069
Action reg: 0.003984
  l1.weight: grad_norm = 0.246049
  l1.bias: grad_norm = 0.002212
  l2.weight: grad_norm = 0.505145
Total gradient norm: 0.974078
=== Actor Training Debug (Iteration 7305) ===
Q mean: -74.055496
Q std: 29.783030
Actor loss: 74.059479
Action reg: 0.003982
  l1.weight: grad_norm = 0.043701
  l1.bias: grad_norm = 0.003265
  l2.weight: grad_norm = 0.096475
Total gradient norm: 0.200894
=== Actor Training Debug (Iteration 7306) ===
Q mean: -75.519066
Q std: 27.681562
Actor loss: 75.523041
Action reg: 0.003977
  l1.weight: grad_norm = 0.368568
  l1.bias: grad_norm = 0.003935
  l2.weight: grad_norm = 0.863414
Total gradient norm: 1.854159
=== Actor Training Debug (Iteration 7307) ===
Q mean: -75.507881
Q std: 30.698673
Actor loss: 75.511856
Action reg: 0.003974
  l1.weight: grad_norm = 0.281018
  l1.bias: grad_norm = 0.004983
  l2.weight: grad_norm = 0.551883
Total gradient norm: 0.966707
=== Actor Training Debug (Iteration 7308) ===
Q mean: -74.473343
Q std: 29.544308
Actor loss: 74.477318
Action reg: 0.003973
  l1.weight: grad_norm = 0.013472
  l1.bias: grad_norm = 0.008666
  l2.weight: grad_norm = 0.060913
Total gradient norm: 0.202456
=== Actor Training Debug (Iteration 7309) ===
Q mean: -75.294708
Q std: 30.376400
Actor loss: 75.298698
Action reg: 0.003992
  l1.weight: grad_norm = 0.049029
  l1.bias: grad_norm = 0.002872
  l2.weight: grad_norm = 0.128151
Total gradient norm: 0.261494
=== Actor Training Debug (Iteration 7310) ===
Q mean: -74.162674
Q std: 29.058636
Actor loss: 74.166664
Action reg: 0.003987
  l1.weight: grad_norm = 0.298668
  l1.bias: grad_norm = 0.001656
  l2.weight: grad_norm = 0.618098
Total gradient norm: 1.206820
=== Actor Training Debug (Iteration 7311) ===
Q mean: -75.876419
Q std: 28.957367
Actor loss: 75.880394
Action reg: 0.003974
  l1.weight: grad_norm = 0.216339
  l1.bias: grad_norm = 0.005020
  l2.weight: grad_norm = 0.547474
Total gradient norm: 1.150789
=== Actor Training Debug (Iteration 7312) ===
Q mean: -74.125130
Q std: 28.595402
Actor loss: 74.129105
Action reg: 0.003975
  l1.weight: grad_norm = 0.183908
  l1.bias: grad_norm = 0.006028
  l2.weight: grad_norm = 0.371996
Total gradient norm: 0.712887
=== Actor Training Debug (Iteration 7313) ===
Q mean: -79.016174
Q std: 30.354408
Actor loss: 79.020134
Action reg: 0.003956
  l1.weight: grad_norm = 0.075047
  l1.bias: grad_norm = 0.011357
  l2.weight: grad_norm = 0.177926
Total gradient norm: 0.415950
=== Actor Training Debug (Iteration 7314) ===
Q mean: -74.908791
Q std: 30.631754
Actor loss: 74.912766
Action reg: 0.003978
  l1.weight: grad_norm = 0.771961
  l1.bias: grad_norm = 0.003435
  l2.weight: grad_norm = 1.416725
Total gradient norm: 2.335041
=== Actor Training Debug (Iteration 7315) ===
Q mean: -79.191238
Q std: 29.046955
Actor loss: 79.195229
Action reg: 0.003989
  l1.weight: grad_norm = 0.183803
  l1.bias: grad_norm = 0.001755
  l2.weight: grad_norm = 0.336226
Total gradient norm: 0.729456
=== Actor Training Debug (Iteration 7316) ===
Q mean: -76.155426
Q std: 29.510744
Actor loss: 76.159416
Action reg: 0.003987
  l1.weight: grad_norm = 0.159868
  l1.bias: grad_norm = 0.001450
  l2.weight: grad_norm = 0.407266
Total gradient norm: 0.833554
=== Actor Training Debug (Iteration 7317) ===
Q mean: -74.647537
Q std: 27.469254
Actor loss: 74.651535
Action reg: 0.003997
  l1.weight: grad_norm = 0.116396
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.224481
Total gradient norm: 0.421472
=== Actor Training Debug (Iteration 7318) ===
Q mean: -74.901596
Q std: 28.282171
Actor loss: 74.905571
Action reg: 0.003978
  l1.weight: grad_norm = 0.288096
  l1.bias: grad_norm = 0.003842
  l2.weight: grad_norm = 0.547612
Total gradient norm: 0.935368
=== Actor Training Debug (Iteration 7319) ===
Q mean: -80.727005
Q std: 28.870380
Actor loss: 80.731003
Action reg: 0.003997
  l1.weight: grad_norm = 0.057639
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.119305
Total gradient norm: 0.203672
=== Actor Training Debug (Iteration 7320) ===
Q mean: -74.813972
Q std: 30.015402
Actor loss: 74.817947
Action reg: 0.003978
  l1.weight: grad_norm = 0.449264
  l1.bias: grad_norm = 0.004793
  l2.weight: grad_norm = 1.017648
Total gradient norm: 1.852987
=== Actor Training Debug (Iteration 7321) ===
Q mean: -73.813004
Q std: 28.208931
Actor loss: 73.816978
Action reg: 0.003976
  l1.weight: grad_norm = 0.388976
  l1.bias: grad_norm = 0.004984
  l2.weight: grad_norm = 0.791329
Total gradient norm: 1.430554
=== Actor Training Debug (Iteration 7322) ===
Q mean: -74.215347
Q std: 27.410492
Actor loss: 74.219330
Action reg: 0.003985
  l1.weight: grad_norm = 0.150739
  l1.bias: grad_norm = 0.002671
  l2.weight: grad_norm = 0.381259
Total gradient norm: 0.799973
=== Actor Training Debug (Iteration 7323) ===
Q mean: -74.736908
Q std: 28.087660
Actor loss: 74.740891
Action reg: 0.003986
  l1.weight: grad_norm = 0.197953
  l1.bias: grad_norm = 0.002309
  l2.weight: grad_norm = 0.413227
Total gradient norm: 0.968938
=== Actor Training Debug (Iteration 7324) ===
Q mean: -77.388321
Q std: 30.991470
Actor loss: 77.392296
Action reg: 0.003978
  l1.weight: grad_norm = 0.366938
  l1.bias: grad_norm = 0.004245
  l2.weight: grad_norm = 0.756137
Total gradient norm: 1.523893
=== Actor Training Debug (Iteration 7325) ===
Q mean: -75.814194
Q std: 30.699575
Actor loss: 75.818176
Action reg: 0.003982
  l1.weight: grad_norm = 0.211835
  l1.bias: grad_norm = 0.004122
  l2.weight: grad_norm = 0.454234
Total gradient norm: 0.912774
=== Actor Training Debug (Iteration 7326) ===
Q mean: -74.153755
Q std: 29.480648
Actor loss: 74.157745
Action reg: 0.003990
  l1.weight: grad_norm = 0.178794
  l1.bias: grad_norm = 0.002251
  l2.weight: grad_norm = 0.444592
Total gradient norm: 0.899190
=== Actor Training Debug (Iteration 7327) ===
Q mean: -73.170639
Q std: 29.450123
Actor loss: 73.174622
Action reg: 0.003979
  l1.weight: grad_norm = 1.132713
  l1.bias: grad_norm = 0.005010
  l2.weight: grad_norm = 2.191295
Total gradient norm: 4.417650
=== Actor Training Debug (Iteration 7328) ===
Q mean: -76.281303
Q std: 29.361814
Actor loss: 76.285286
Action reg: 0.003981
  l1.weight: grad_norm = 0.192576
  l1.bias: grad_norm = 0.003145
  l2.weight: grad_norm = 0.426736
Total gradient norm: 0.831716
=== Actor Training Debug (Iteration 7329) ===
Q mean: -76.541061
Q std: 29.126747
Actor loss: 76.545036
Action reg: 0.003976
  l1.weight: grad_norm = 0.151526
  l1.bias: grad_norm = 0.004484
  l2.weight: grad_norm = 0.384538
Total gradient norm: 0.723722
=== Actor Training Debug (Iteration 7330) ===
Q mean: -75.514175
Q std: 28.664112
Actor loss: 75.518150
Action reg: 0.003972
  l1.weight: grad_norm = 0.271038
  l1.bias: grad_norm = 0.006348
  l2.weight: grad_norm = 0.580876
Total gradient norm: 1.121576
=== Actor Training Debug (Iteration 7331) ===
Q mean: -76.789581
Q std: 29.118639
Actor loss: 76.793556
Action reg: 0.003974
  l1.weight: grad_norm = 0.362237
  l1.bias: grad_norm = 0.005529
  l2.weight: grad_norm = 0.890362
Total gradient norm: 1.929253
=== Actor Training Debug (Iteration 7332) ===
Q mean: -75.443153
Q std: 28.329519
Actor loss: 75.447136
Action reg: 0.003980
  l1.weight: grad_norm = 0.349224
  l1.bias: grad_norm = 0.002423
  l2.weight: grad_norm = 0.803354
Total gradient norm: 1.442433
=== Actor Training Debug (Iteration 7333) ===
Q mean: -76.026031
Q std: 29.163839
Actor loss: 76.030006
Action reg: 0.003975
  l1.weight: grad_norm = 0.582823
  l1.bias: grad_norm = 0.004430
  l2.weight: grad_norm = 1.196207
Total gradient norm: 2.150871
=== Actor Training Debug (Iteration 7334) ===
Q mean: -77.385544
Q std: 28.333239
Actor loss: 77.389534
Action reg: 0.003991
  l1.weight: grad_norm = 0.310847
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.643976
Total gradient norm: 1.289280
=== Actor Training Debug (Iteration 7335) ===
Q mean: -75.707489
Q std: 29.662027
Actor loss: 75.711464
Action reg: 0.003973
  l1.weight: grad_norm = 0.181826
  l1.bias: grad_norm = 0.005303
  l2.weight: grad_norm = 0.384348
Total gradient norm: 0.734256
=== Actor Training Debug (Iteration 7336) ===
Q mean: -78.248558
Q std: 28.506855
Actor loss: 78.252525
Action reg: 0.003969
  l1.weight: grad_norm = 0.203745
  l1.bias: grad_norm = 0.007553
  l2.weight: grad_norm = 0.365285
Total gradient norm: 0.668184
=== Actor Training Debug (Iteration 7337) ===
Q mean: -77.310669
Q std: 29.534636
Actor loss: 77.314644
Action reg: 0.003972
  l1.weight: grad_norm = 0.230584
  l1.bias: grad_norm = 0.005061
  l2.weight: grad_norm = 0.479126
Total gradient norm: 0.733946
=== Actor Training Debug (Iteration 7338) ===
Q mean: -77.248230
Q std: 29.440149
Actor loss: 77.252205
Action reg: 0.003977
  l1.weight: grad_norm = 0.148373
  l1.bias: grad_norm = 0.004951
  l2.weight: grad_norm = 0.267309
Total gradient norm: 0.502652
=== Actor Training Debug (Iteration 7339) ===
Q mean: -77.868446
Q std: 29.201429
Actor loss: 77.872437
Action reg: 0.003990
  l1.weight: grad_norm = 0.022665
  l1.bias: grad_norm = 0.002365
  l2.weight: grad_norm = 0.058406
Total gradient norm: 0.117136
=== Actor Training Debug (Iteration 7340) ===
Q mean: -76.400314
Q std: 28.337219
Actor loss: 76.404289
Action reg: 0.003976
  l1.weight: grad_norm = 0.232379
  l1.bias: grad_norm = 0.006034
  l2.weight: grad_norm = 0.638571
Total gradient norm: 1.535652
=== Actor Training Debug (Iteration 7341) ===
Q mean: -77.182373
Q std: 27.425848
Actor loss: 77.186363
Action reg: 0.003990
  l1.weight: grad_norm = 0.035635
  l1.bias: grad_norm = 0.003085
  l2.weight: grad_norm = 0.073638
Total gradient norm: 0.137929
=== Actor Training Debug (Iteration 7342) ===
Q mean: -77.128586
Q std: 27.877903
Actor loss: 77.132576
Action reg: 0.003988
  l1.weight: grad_norm = 0.148867
  l1.bias: grad_norm = 0.003502
  l2.weight: grad_norm = 0.365579
Total gradient norm: 0.728064
=== Actor Training Debug (Iteration 7343) ===
Q mean: -73.383453
Q std: 28.458139
Actor loss: 73.387436
Action reg: 0.003985
  l1.weight: grad_norm = 0.317036
  l1.bias: grad_norm = 0.002665
  l2.weight: grad_norm = 0.721619
Total gradient norm: 1.745266
=== Actor Training Debug (Iteration 7344) ===
Q mean: -75.856201
Q std: 29.214130
Actor loss: 75.860184
Action reg: 0.003980
  l1.weight: grad_norm = 0.131748
  l1.bias: grad_norm = 0.003922
  l2.weight: grad_norm = 0.278383
Total gradient norm: 0.517774
=== Actor Training Debug (Iteration 7345) ===
Q mean: -79.062592
Q std: 30.367880
Actor loss: 79.066559
Action reg: 0.003966
  l1.weight: grad_norm = 0.316039
  l1.bias: grad_norm = 0.007555
  l2.weight: grad_norm = 0.700782
Total gradient norm: 1.447518
=== Actor Training Debug (Iteration 7346) ===
Q mean: -73.774719
Q std: 30.214394
Actor loss: 73.778679
Action reg: 0.003961
  l1.weight: grad_norm = 0.555629
  l1.bias: grad_norm = 0.008515
  l2.weight: grad_norm = 1.168037
Total gradient norm: 2.209224
=== Actor Training Debug (Iteration 7347) ===
Q mean: -79.591141
Q std: 28.972502
Actor loss: 79.595139
Action reg: 0.003995
  l1.weight: grad_norm = 0.395094
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.777077
Total gradient norm: 1.369535
=== Actor Training Debug (Iteration 7348) ===
Q mean: -75.333633
Q std: 27.853445
Actor loss: 75.337624
Action reg: 0.003988
  l1.weight: grad_norm = 0.195443
  l1.bias: grad_norm = 0.001360
  l2.weight: grad_norm = 0.469576
Total gradient norm: 1.066354
=== Actor Training Debug (Iteration 7349) ===
Q mean: -76.799957
Q std: 30.394741
Actor loss: 76.803940
Action reg: 0.003979
  l1.weight: grad_norm = 0.340325
  l1.bias: grad_norm = 0.003199
  l2.weight: grad_norm = 0.766822
Total gradient norm: 1.404348
=== Actor Training Debug (Iteration 7350) ===
Q mean: -74.884735
Q std: 28.503136
Actor loss: 74.888710
Action reg: 0.003977
  l1.weight: grad_norm = 0.232997
  l1.bias: grad_norm = 0.004109
  l2.weight: grad_norm = 0.432993
Total gradient norm: 0.789656
=== Actor Training Debug (Iteration 7351) ===
Q mean: -78.554565
Q std: 29.084846
Actor loss: 78.558548
Action reg: 0.003986
  l1.weight: grad_norm = 0.087273
  l1.bias: grad_norm = 0.002611
  l2.weight: grad_norm = 0.180911
Total gradient norm: 0.417700
=== Actor Training Debug (Iteration 7352) ===
Q mean: -76.048172
Q std: 30.871271
Actor loss: 76.052155
Action reg: 0.003981
  l1.weight: grad_norm = 0.231466
  l1.bias: grad_norm = 0.004565
  l2.weight: grad_norm = 0.507216
Total gradient norm: 0.869790
=== Actor Training Debug (Iteration 7353) ===
Q mean: -81.625725
Q std: 29.083055
Actor loss: 81.629700
Action reg: 0.003978
  l1.weight: grad_norm = 0.134803
  l1.bias: grad_norm = 0.003376
  l2.weight: grad_norm = 0.314782
Total gradient norm: 0.692569
=== Actor Training Debug (Iteration 7354) ===
Q mean: -75.363968
Q std: 29.562649
Actor loss: 75.367935
Action reg: 0.003971
  l1.weight: grad_norm = 0.431754
  l1.bias: grad_norm = 0.003975
  l2.weight: grad_norm = 1.023562
Total gradient norm: 1.967691
=== Actor Training Debug (Iteration 7355) ===
Q mean: -76.510307
Q std: 28.304590
Actor loss: 76.514282
Action reg: 0.003977
  l1.weight: grad_norm = 0.141879
  l1.bias: grad_norm = 0.005420
  l2.weight: grad_norm = 0.309982
Total gradient norm: 0.720484
=== Actor Training Debug (Iteration 7356) ===
Q mean: -79.695938
Q std: 28.552141
Actor loss: 79.699905
Action reg: 0.003970
  l1.weight: grad_norm = 0.878656
  l1.bias: grad_norm = 0.005355
  l2.weight: grad_norm = 1.880329
Total gradient norm: 4.296075
=== Actor Training Debug (Iteration 7357) ===
Q mean: -76.360970
Q std: 27.508242
Actor loss: 76.364952
Action reg: 0.003980
  l1.weight: grad_norm = 0.440902
  l1.bias: grad_norm = 0.004190
  l2.weight: grad_norm = 1.045336
Total gradient norm: 2.041248
=== Actor Training Debug (Iteration 7358) ===
Q mean: -74.539703
Q std: 28.342184
Actor loss: 74.543686
Action reg: 0.003983
  l1.weight: grad_norm = 0.097958
  l1.bias: grad_norm = 0.005040
  l2.weight: grad_norm = 0.207360
Total gradient norm: 0.385572
=== Actor Training Debug (Iteration 7359) ===
Q mean: -75.590485
Q std: 30.557802
Actor loss: 75.594444
Action reg: 0.003962
  l1.weight: grad_norm = 0.057579
  l1.bias: grad_norm = 0.010798
  l2.weight: grad_norm = 0.145966
Total gradient norm: 0.349079
=== Actor Training Debug (Iteration 7360) ===
Q mean: -79.189590
Q std: 29.465410
Actor loss: 79.193573
Action reg: 0.003984
  l1.weight: grad_norm = 0.115174
  l1.bias: grad_norm = 0.004725
  l2.weight: grad_norm = 0.256023
Total gradient norm: 0.458756
=== Actor Training Debug (Iteration 7361) ===
Q mean: -74.946564
Q std: 27.309795
Actor loss: 74.950546
Action reg: 0.003982
  l1.weight: grad_norm = 0.212433
  l1.bias: grad_norm = 0.003438
  l2.weight: grad_norm = 0.352732
Total gradient norm: 0.592955
=== Actor Training Debug (Iteration 7362) ===
Q mean: -78.611633
Q std: 29.179583
Actor loss: 78.615608
Action reg: 0.003974
  l1.weight: grad_norm = 0.383740
  l1.bias: grad_norm = 0.006432
  l2.weight: grad_norm = 0.848142
Total gradient norm: 1.677601
=== Actor Training Debug (Iteration 7363) ===
Q mean: -73.117920
Q std: 27.981647
Actor loss: 73.121902
Action reg: 0.003981
  l1.weight: grad_norm = 0.257159
  l1.bias: grad_norm = 0.005339
  l2.weight: grad_norm = 0.496968
Total gradient norm: 0.918569
=== Actor Training Debug (Iteration 7364) ===
Q mean: -76.307777
Q std: 28.143421
Actor loss: 76.311760
Action reg: 0.003984
  l1.weight: grad_norm = 0.250664
  l1.bias: grad_norm = 0.002848
  l2.weight: grad_norm = 0.471953
Total gradient norm: 0.805690
=== Actor Training Debug (Iteration 7365) ===
Q mean: -75.592712
Q std: 30.107286
Actor loss: 75.596695
Action reg: 0.003985
  l1.weight: grad_norm = 0.104569
  l1.bias: grad_norm = 0.003483
  l2.weight: grad_norm = 0.212159
Total gradient norm: 0.409327
=== Actor Training Debug (Iteration 7366) ===
Q mean: -75.207733
Q std: 29.850641
Actor loss: 75.211708
Action reg: 0.003976
  l1.weight: grad_norm = 0.130239
  l1.bias: grad_norm = 0.006741
  l2.weight: grad_norm = 0.268982
Total gradient norm: 0.505068
=== Actor Training Debug (Iteration 7367) ===
Q mean: -74.264259
Q std: 29.832352
Actor loss: 74.268219
Action reg: 0.003960
  l1.weight: grad_norm = 0.202953
  l1.bias: grad_norm = 0.010456
  l2.weight: grad_norm = 0.530842
Total gradient norm: 1.087351
=== Actor Training Debug (Iteration 7368) ===
Q mean: -76.981171
Q std: 29.497473
Actor loss: 76.985153
Action reg: 0.003979
  l1.weight: grad_norm = 0.247688
  l1.bias: grad_norm = 0.004811
  l2.weight: grad_norm = 0.551451
Total gradient norm: 0.918204
=== Actor Training Debug (Iteration 7369) ===
Q mean: -80.711983
Q std: 29.786186
Actor loss: 80.715981
Action reg: 0.003995
  l1.weight: grad_norm = 0.338038
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.683767
Total gradient norm: 1.169149
=== Actor Training Debug (Iteration 7370) ===
Q mean: -73.050827
Q std: 29.913527
Q std: 30.574732orm: 0.852500ration 6486) ===
Actor loss: 75.724510
Action reg: 0.003971
  l1.weight: grad_norm = 0.307905
  l1.bias: grad_norm = 0.010591
  l2.weight: grad_norm = 0.635647
Total gradient norm: 1.229986
=== Actor Training Debug (Iteration 7381) ===
Q mean: -78.363647
Q std: 28.676600
Actor loss: 78.367638
Action reg: 0.003988
  l1.weight: grad_norm = 0.182567
  l1.bias: grad_norm = 0.002541
  l2.weight: grad_norm = 0.433045
Total gradient norm: 0.786527
=== Actor Training Debug (Iteration 7382) ===
Q mean: -76.788429
Q std: 29.407061
Actor loss: 76.792412
Action reg: 0.003981
  l1.weight: grad_norm = 0.358432
  l1.bias: grad_norm = 0.006713
  l2.weight: grad_norm = 0.695410
Total gradient norm: 1.499371
=== Actor Training Debug (Iteration 7383) ===
Q mean: -77.230484
Q std: 27.723686
Actor loss: 77.234474
Action reg: 0.003987
  l1.weight: grad_norm = 0.110233
  l1.bias: grad_norm = 0.003851
  l2.weight: grad_norm = 0.244662
Total gradient norm: 0.489983
=== Actor Training Debug (Iteration 7384) ===
Q mean: -75.995522
Q std: 28.870132
Actor loss: 75.999496
Action reg: 0.003977
  l1.weight: grad_norm = 0.439599
  l1.bias: grad_norm = 0.008373
  l2.weight: grad_norm = 1.328945
Total gradient norm: 2.862504
=== Actor Training Debug (Iteration 7385) ===
Q mean: -77.478340
Q std: 29.952080
Actor loss: 77.482315
Action reg: 0.003971
  l1.weight: grad_norm = 0.083633
  l1.bias: grad_norm = 0.011535
  l2.weight: grad_norm = 0.163973
Total gradient norm: 0.393021
=== Actor Training Debug (Iteration 7386) ===
Q mean: -73.182220
Q std: 29.433338
Actor loss: 73.186188
Action reg: 0.003968
  l1.weight: grad_norm = 0.273874
  l1.bias: grad_norm = 0.009878
  l2.weight: grad_norm = 0.626794
Total gradient norm: 1.248345
=== Actor Training Debug (Iteration 7387) ===
Q mean: -78.900681
Q std: 28.792414
Actor loss: 78.904678
Action reg: 0.003997
  l1.weight: grad_norm = 0.163595
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.375012
Total gradient norm: 0.734346
=== Actor Training Debug (Iteration 7388) ===
Q mean: -78.495247
Q std: 30.383030
Actor loss: 78.499229
Action reg: 0.003982
  l1.weight: grad_norm = 0.038979
  l1.bias: grad_norm = 0.004906
  l2.weight: grad_norm = 0.093260
Total gradient norm: 0.215249
=== Actor Training Debug (Iteration 7389) ===
Q mean: -75.669144
Q std: 28.953039
Actor loss: 75.673111
Action reg: 0.003970
  l1.weight: grad_norm = 0.222110
  l1.bias: grad_norm = 0.009077
  l2.weight: grad_norm = 0.492430
Total gradient norm: 0.834055
=== Actor Training Debug (Iteration 7390) ===
Q mean: -72.771461
Q std: 29.768742
Actor loss: 72.775436
Action reg: 0.003972
  l1.weight: grad_norm = 0.230848
  l1.bias: grad_norm = 0.007789
  l2.weight: grad_norm = 0.468931
Total gradient norm: 1.060766
=== Actor Training Debug (Iteration 7391) ===
Q mean: -72.143341
Q std: 30.048674
Actor loss: 72.147308
Action reg: 0.003965
  l1.weight: grad_norm = 0.224374
  l1.bias: grad_norm = 0.010543
  l2.weight: grad_norm = 0.355827
Total gradient norm: 0.629446
=== Actor Training Debug (Iteration 7392) ===
Q mean: -73.473747
Q std: 29.656931
Actor loss: 73.477730
Action reg: 0.003983
  l1.weight: grad_norm = 0.078991
  l1.bias: grad_norm = 0.005510
  l2.weight: grad_norm = 0.201145
Total gradient norm: 0.469507
=== Actor Training Debug (Iteration 7393) ===
Q mean: -77.213272
Q std: 28.070156
Actor loss: 77.217247
Action reg: 0.003978
  l1.weight: grad_norm = 0.165896
  l1.bias: grad_norm = 0.006353
  l2.weight: grad_norm = 0.342236
Total gradient norm: 0.643122
=== Actor Training Debug (Iteration 7394) ===
Q mean: -80.735786
Q std: 27.015793
Actor loss: 80.739777
Action reg: 0.003987
  l1.weight: grad_norm = 0.230340
  l1.bias: grad_norm = 0.003054
  l2.weight: grad_norm = 0.471378
Total gradient norm: 0.941625
=== Actor Training Debug (Iteration 7395) ===
Q mean: -75.112915
Q std: 28.420179
Actor loss: 75.116905
Action reg: 0.003991
  l1.weight: grad_norm = 0.080981
  l1.bias: grad_norm = 0.002920
  l2.weight: grad_norm = 0.164690
Total gradient norm: 0.274303
=== Actor Training Debug (Iteration 7396) ===
Q mean: -75.100433
Q std: 29.928537
Actor loss: 75.104401
Action reg: 0.003967
  l1.weight: grad_norm = 0.193547
  l1.bias: grad_norm = 0.011330
  l2.weight: grad_norm = 0.422922
Total gradient norm: 0.915659
=== Actor Training Debug (Iteration 7397) ===
Q mean: -76.984612
Q std: 29.786297
Actor loss: 76.988586
Action reg: 0.003971
  l1.weight: grad_norm = 0.033936
  l1.bias: grad_norm = 0.011642
  l2.weight: grad_norm = 0.118865
Total gradient norm: 0.359327
=== Actor Training Debug (Iteration 7398) ===
Q mean: -74.100166
Q std: 30.953365
Actor loss: 74.104126
Action reg: 0.003958
  l1.weight: grad_norm = 0.109188
  l1.bias: grad_norm = 0.015010
  l2.weight: grad_norm = 0.265453
Total gradient norm: 0.607340
=== Actor Training Debug (Iteration 7399) ===
Q mean: -76.316605
Q std: 29.881521
Actor loss: 76.320587
Action reg: 0.003984
  l1.weight: grad_norm = 0.071122
  l1.bias: grad_norm = 0.005480
  l2.weight: grad_norm = 0.154976
Total gradient norm: 0.263364
=== Actor Training Debug (Iteration 7400) ===
Q mean: -76.350861
Q std: 28.674738
Actor loss: 76.354843
Action reg: 0.003984
  l1.weight: grad_norm = 0.281838
  l1.bias: grad_norm = 0.003731
  l2.weight: grad_norm = 0.489478
Total gradient norm: 0.876895
=== Actor Training Debug (Iteration 7401) ===
Q mean: -76.617821
Q std: 28.490803
Actor loss: 76.621803
Action reg: 0.003985
  l1.weight: grad_norm = 0.253669
  l1.bias: grad_norm = 0.004915
  l2.weight: grad_norm = 0.500695
Total gradient norm: 0.958721
=== Actor Training Debug (Iteration 7402) ===
Q mean: -75.083008
Q std: 29.713808
Actor loss: 75.086960
Action reg: 0.003948
  l1.weight: grad_norm = 0.194958
  l1.bias: grad_norm = 0.017316
  l2.weight: grad_norm = 0.367284
Total gradient norm: 0.666138
=== Actor Training Debug (Iteration 7403) ===
Q mean: -78.352074
Q std: 30.377728
Actor loss: 78.356026
Action reg: 0.003954
  l1.weight: grad_norm = 0.268744
  l1.bias: grad_norm = 0.017031
  l2.weight: grad_norm = 0.495337
Total gradient norm: 0.968542
=== Actor Training Debug (Iteration 7404) ===
Q mean: -78.504288
Q std: 27.646780
Actor loss: 78.508263
Action reg: 0.003976
  l1.weight: grad_norm = 0.421251
  l1.bias: grad_norm = 0.003711
  l2.weight: grad_norm = 0.745532
Total gradient norm: 1.399630
=== Actor Training Debug (Iteration 7405) ===
Q mean: -78.943863
Q std: 28.713783
Actor loss: 78.947845
Action reg: 0.003984
  l1.weight: grad_norm = 0.189173
  l1.bias: grad_norm = 0.003262
  l2.weight: grad_norm = 0.378227
Total gradient norm: 0.742816
=== Actor Training Debug (Iteration 7406) ===
Q mean: -78.293732
Q std: 31.410172
Actor loss: 78.297691
Action reg: 0.003962
  l1.weight: grad_norm = 0.069738
  l1.bias: grad_norm = 0.015878
  l2.weight: grad_norm = 0.184331
Total gradient norm: 0.474797
=== Actor Training Debug (Iteration 7407) ===
Q mean: -73.775040
Q std: 29.127516
Actor loss: 73.779015
Action reg: 0.003974
  l1.weight: grad_norm = 0.131822
  l1.bias: grad_norm = 0.008104
  l2.weight: grad_norm = 0.293093
Total gradient norm: 0.634046
=== Actor Training Debug (Iteration 7408) ===
Q mean: -72.484138
Q std: 30.352198
Actor loss: 72.488106
Action reg: 0.003971
  l1.weight: grad_norm = 0.164657
  l1.bias: grad_norm = 0.008678
  l2.weight: grad_norm = 0.352562
Total gradient norm: 0.616605
=== Actor Training Debug (Iteration 7409) ===
Q mean: -77.335968
Q std: 28.976828
Actor loss: 77.339943
Action reg: 0.003973
  l1.weight: grad_norm = 1.677713
  l1.bias: grad_norm = 0.008865
  l2.weight: grad_norm = 3.806440
Total gradient norm: 6.880255
=== Actor Training Debug (Iteration 7410) ===
Q mean: -75.782272
Q std: 29.961613
Actor loss: 75.786247
Action reg: 0.003976
  l1.weight: grad_norm = 0.060699
  l1.bias: grad_norm = 0.010564
  l2.weight: grad_norm = 0.150624
Total gradient norm: 0.333350
=== Actor Training Debug (Iteration 7411) ===
Q mean: -78.065514
Q std: 28.047596
Actor loss: 78.069504
Action reg: 0.003992
  l1.weight: grad_norm = 0.146123
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.313782
Total gradient norm: 0.546700
=== Actor Training Debug (Iteration 7412) ===
Q mean: -74.391884
Q std: 28.039513
Actor loss: 74.395844
Action reg: 0.003962
  l1.weight: grad_norm = 0.375565
  l1.bias: grad_norm = 0.013717
  l2.weight: grad_norm = 0.811100
Total gradient norm: 1.561727
=== Actor Training Debug (Iteration 7413) ===
Q mean: -77.893196
Q std: 28.915260
Actor loss: 77.897179
Action reg: 0.003986
  l1.weight: grad_norm = 0.187588
  l1.bias: grad_norm = 0.003558
  l2.weight: grad_norm = 0.307664
Total gradient norm: 0.584650
=== Actor Training Debug (Iteration 7414) ===
Q mean: -76.949409
Q std: 29.806690
Actor loss: 76.953384
Action reg: 0.003978
  l1.weight: grad_norm = 0.254546
  l1.bias: grad_norm = 0.008322
  l2.weight: grad_norm = 0.744984
Total gradient norm: 1.826046
=== Actor Training Debug (Iteration 7415) ===
Q mean: -77.567322
Q std: 29.297787
Actor loss: 77.571297
Action reg: 0.003972
  l1.weight: grad_norm = 0.419732
  l1.bias: grad_norm = 0.005400
  l2.weight: grad_norm = 0.802205
Total gradient norm: 1.358377
=== Actor Training Debug (Iteration 7416) ===
Q mean: -74.436165
Q std: 30.031645
Actor loss: 74.440155
Action reg: 0.003989
  l1.weight: grad_norm = 0.680637
  l1.bias: grad_norm = 0.004299
  l2.weight: grad_norm = 1.502684
Total gradient norm: 2.836502
=== Actor Training Debug (Iteration 7417) ===
Q mean: -78.294907
Q std: 29.242212
Actor loss: 78.298889
Action reg: 0.003981
  l1.weight: grad_norm = 0.582046
  l1.bias: grad_norm = 0.001954
  l2.weight: grad_norm = 1.105565
Total gradient norm: 1.951354
=== Actor Training Debug (Iteration 7418) ===
Q mean: -76.546921
Q std: 29.058012
Actor loss: 76.550888
Action reg: 0.003967
  l1.weight: grad_norm = 0.063999
  l1.bias: grad_norm = 0.014357
  l2.weight: grad_norm = 0.173131
Total gradient norm: 0.407386
=== Actor Training Debug (Iteration 7419) ===
Q mean: -73.591614
Q std: 27.974100
Actor loss: 73.595589
Action reg: 0.003972
  l1.weight: grad_norm = 0.071532
  l1.bias: grad_norm = 0.008595
  l2.weight: grad_norm = 0.140183
Total gradient norm: 0.267882
=== Actor Training Debug (Iteration 7420) ===
Q mean: -76.432045
Q std: 29.216143
Actor loss: 76.436028
Action reg: 0.003984
  l1.weight: grad_norm = 0.234169
  l1.bias: grad_norm = 0.003653
  l2.weight: grad_norm = 0.542594
Total gradient norm: 0.956729
=== Actor Training Debug (Iteration 7421) ===
Q mean: -75.735687
Q std: 30.286564
Actor loss: 75.739632
Action reg: 0.003945
  l1.weight: grad_norm = 3.586720
  l1.bias: grad_norm = 0.012955
  l2.weight: grad_norm = 7.100345
Total gradient norm: 13.327507
=== Actor Training Debug (Iteration 7422) ===
Q mean: -73.525322
Q std: 28.874237
Actor loss: 73.529305
Action reg: 0.003986
  l1.weight: grad_norm = 0.251211
  l1.bias: grad_norm = 0.002050
  l2.weight: grad_norm = 0.577351
Total gradient norm: 1.212129
=== Actor Training Debug (Iteration 7423) ===
Q mean: -73.615929
Q std: 30.086187
Actor loss: 73.619888
Action reg: 0.003959
  l1.weight: grad_norm = 0.064239
  l1.bias: grad_norm = 0.019085
  l2.weight: grad_norm = 0.183753
Total gradient norm: 0.513543
=== Actor Training Debug (Iteration 7424) ===
Q mean: -74.342804
Q std: 29.269434
Actor loss: 74.346779
Action reg: 0.003978
  l1.weight: grad_norm = 0.445842
  l1.bias: grad_norm = 0.003897
  l2.weight: grad_norm = 1.132099
Total gradient norm: 2.546871
=== Actor Training Debug (Iteration 7425) ===
Q mean: -77.781288
Q std: 29.593109
Actor loss: 77.785263
Action reg: 0.003977
  l1.weight: grad_norm = 0.464727
  l1.bias: grad_norm = 0.005931
  l2.weight: grad_norm = 0.908189
Total gradient norm: 1.700638
=== Actor Training Debug (Iteration 7426) ===
Q mean: -77.954651
Q std: 31.111099
Actor loss: 77.958633
Action reg: 0.003983
  l1.weight: grad_norm = 0.148820
  l1.bias: grad_norm = 0.007036
  l2.weight: grad_norm = 0.315034
Total gradient norm: 0.657847
=== Actor Training Debug (Iteration 7427) ===
Q mean: -78.740318
Q std: 29.417685
Actor loss: 78.744286
Action reg: 0.003970
  l1.weight: grad_norm = 0.102699
  l1.bias: grad_norm = 0.011702
  l2.weight: grad_norm = 0.223847
Total gradient norm: 0.475437
=== Actor Training Debug (Iteration 7428) ===
Q mean: -77.775322
Q std: 29.620617
Actor loss: 77.779305
Action reg: 0.003984
  l1.weight: grad_norm = 0.099471
  l1.bias: grad_norm = 0.006797
  l2.weight: grad_norm = 0.206738
Total gradient norm: 0.388133
=== Actor Training Debug (Iteration 7429) ===
Q mean: -77.645294
Q std: 30.305286
Actor loss: 77.649261
Action reg: 0.003965
  l1.weight: grad_norm = 0.611833
  l1.bias: grad_norm = 0.012558
  l2.weight: grad_norm = 1.092231
Total gradient norm: 1.897473
=== Actor Training Debug (Iteration 7430) ===
Q mean: -77.100403
Q std: 29.885605
Actor loss: 77.104370
Action reg: 0.003969
  l1.weight: grad_norm = 0.102933
  l1.bias: grad_norm = 0.012946
  l2.weight: grad_norm = 0.231687
Total gradient norm: 0.557849
=== Actor Training Debug (Iteration 7431) ===
Q mean: -78.938072
Q std: 30.068665
Actor loss: 78.942047
Action reg: 0.003972
  l1.weight: grad_norm = 0.076151
  l1.bias: grad_norm = 0.011744
  l2.weight: grad_norm = 0.161132
Total gradient norm: 0.338974
=== Actor Training Debug (Iteration 7432) ===
Q mean: -75.625244
Q std: 27.008371
Actor loss: 75.629219
Action reg: 0.003971
  l1.weight: grad_norm = 1.435833
  l1.bias: grad_norm = 0.007465
  l2.weight: grad_norm = 2.542456
Total gradient norm: 4.782983
=== Actor Training Debug (Iteration 7433) ===
Q mean: -75.044525
Q std: 30.178495
Actor loss: 75.048500
Action reg: 0.003975
  l1.weight: grad_norm = 0.033712
  l1.bias: grad_norm = 0.010185
  l2.weight: grad_norm = 0.088442
Total gradient norm: 0.235465
=== Actor Training Debug (Iteration 7434) ===
Q mean: -76.247818
Q std: 28.775545
Actor loss: 76.251801
Action reg: 0.003982
  l1.weight: grad_norm = 0.678031
  l1.bias: grad_norm = 0.005695
  l2.weight: grad_norm = 1.497208
Total gradient norm: 3.512111
=== Actor Training Debug (Iteration 7435) ===
Q mean: -78.980347
Q std: 29.617790
Actor loss: 78.984337
Action reg: 0.003986
  l1.weight: grad_norm = 0.140431
  l1.bias: grad_norm = 0.004408
  l2.weight: grad_norm = 0.305159
Total gradient norm: 0.580111
=== Actor Training Debug (Iteration 7436) ===
Q mean: -77.924355
Q std: 29.345358
Actor loss: 77.928329
Action reg: 0.003975
  l1.weight: grad_norm = 0.331821
  l1.bias: grad_norm = 0.009291
  l2.weight: grad_norm = 0.621204
Total gradient norm: 1.218449
=== Actor Training Debug (Iteration 7437) ===
Q mean: -78.582008
Q std: 29.660189
Actor loss: 78.585991
Action reg: 0.003983
  l1.weight: grad_norm = 0.026217
  l1.bias: grad_norm = 0.007753
  l2.weight: grad_norm = 0.065907
Total gradient norm: 0.156955
=== Actor Training Debug (Iteration 7438) ===
Q mean: -79.942642
Q std: 29.707676
Actor loss: 79.946617
Action reg: 0.003975
  l1.weight: grad_norm = 0.079862
  l1.bias: grad_norm = 0.008756
  l2.weight: grad_norm = 0.157598
Total gradient norm: 0.299221
=== Actor Training Debug (Iteration 7439) ===
Q mean: -80.428673
Q std: 27.832176
Actor loss: 80.432655
Action reg: 0.003986
  l1.weight: grad_norm = 0.453399
  l1.bias: grad_norm = 0.001512
  l2.weight: grad_norm = 1.195438
Total gradient norm: 2.689752
=== Actor Training Debug (Iteration 7440) ===
Q mean: -78.114357
Q std: 28.977358
Actor loss: 78.118340
Action reg: 0.003985
  l1.weight: grad_norm = 0.133695
  l1.bias: grad_norm = 0.003213
  l2.weight: grad_norm = 0.283567
Total gradient norm: 0.525460
=== Actor Training Debug (Iteration 7441) ===
Q mean: -78.184952
Q std: 29.089508
Actor loss: 78.188934
Action reg: 0.003980
  l1.weight: grad_norm = 0.141641
  l1.bias: grad_norm = 0.007977
  l2.weight: grad_norm = 0.293488
Total gradient norm: 0.602697
=== Actor Training Debug (Iteration 7442) ===
Q mean: -77.288742
Q std: 29.179886
Actor loss: 77.292725
Action reg: 0.003984
  l1.weight: grad_norm = 0.213099
  l1.bias: grad_norm = 0.005774
  l2.weight: grad_norm = 0.366916
Total gradient norm: 0.629029
=== Actor Training Debug (Iteration 7443) ===
Q mean: -75.446381
Q std: 29.340246
Actor loss: 75.450371
Action reg: 0.003990
  l1.weight: grad_norm = 0.230702
  l1.bias: grad_norm = 0.001617
  l2.weight: grad_norm = 0.429662
Total gradient norm: 0.816653
=== Actor Training Debug (Iteration 7444) ===
Q mean: -72.078773
Q std: 30.180365
Actor loss: 72.082741
Action reg: 0.003969
  l1.weight: grad_norm = 0.267637
  l1.bias: grad_norm = 0.011339
  l2.weight: grad_norm = 0.577618
Total gradient norm: 1.199891
=== Actor Training Debug (Iteration 7445) ===
Q mean: -76.656647
Q std: 29.896585
Actor loss: 76.660645
Action reg: 0.003995
  l1.weight: grad_norm = 0.036872
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.084274
Total gradient norm: 0.158356
=== Actor Training Debug (Iteration 7446) ===
Q mean: -76.445587
Q std: 30.477808
Actor loss: 76.449562
Action reg: 0.003975
  l1.weight: grad_norm = 0.141127
  l1.bias: grad_norm = 0.010625
  l2.weight: grad_norm = 0.311809
Total gradient norm: 0.585107
=== Actor Training Debug (Iteration 7447) ===
Q mean: -78.687973
Q std: 27.771099
Actor loss: 78.691971
Action reg: 0.003998
  l1.weight: grad_norm = 0.069870
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.141193
Total gradient norm: 0.279567
=== Actor Training Debug (Iteration 7448) ===
Q mean: -76.137131
Q std: 29.107979
Actor loss: 76.141106
Action reg: 0.003978
  l1.weight: grad_norm = 0.071330
  l1.bias: grad_norm = 0.008379
  l2.weight: grad_norm = 0.167854
Total gradient norm: 0.333717
=== Actor Training Debug (Iteration 7449) ===
Q mean: -76.316742
Q std: 29.731047
Actor loss: 76.320702
Action reg: 0.003957
  l1.weight: grad_norm = 0.362050
  l1.bias: grad_norm = 0.014023
  l2.weight: grad_norm = 0.744312
Total gradient norm: 1.396603
=== Actor Training Debug (Iteration 7450) ===
Q mean: -79.247787
Q std: 28.973150
Actor loss: 79.251770
Action reg: 0.003986
  l1.weight: grad_norm = 0.131007
  l1.bias: grad_norm = 0.005678
  l2.weight: grad_norm = 0.282919
Total gradient norm: 0.598590
=== Actor Training Debug (Iteration 7451) ===
Q mean: -76.879272
Q std: 29.239113
Actor loss: 76.883247
Action reg: 0.003976
  l1.weight: grad_norm = 0.125935
  l1.bias: grad_norm = 0.008268
  l2.weight: grad_norm = 0.282212
Total gradient norm: 0.523339
=== Actor Training Debug (Iteration 7452) ===
Q mean: -75.344139
Q std: 29.422529
Actor loss: 75.348114
Action reg: 0.003973
  l1.weight: grad_norm = 0.303553
  l1.bias: grad_norm = 0.009230
  l2.weight: grad_norm = 0.679913
Total gradient norm: 1.330282
=== Actor Training Debug (Iteration 7453) ===
Q mean: -77.074722
Q std: 30.339668
Actor loss: 77.078697
Action reg: 0.003977
  l1.weight: grad_norm = 0.084006
  l1.bias: grad_norm = 0.011716
  l2.weight: grad_norm = 0.196566
Total gradient norm: 0.403760
=== Actor Training Debug (Iteration 7454) ===
Q mean: -80.727127
Q std: 30.085232
Actor loss: 80.731110
Action reg: 0.003985
  l1.weight: grad_norm = 0.103623
  l1.bias: grad_norm = 0.003835
  l2.weight: grad_norm = 0.217224
Total gradient norm: 0.404676
=== Actor Training Debug (Iteration 7455) ===
Q mean: -78.823906
Q std: 28.733419
Actor loss: 78.827881
Action reg: 0.003979
  l1.weight: grad_norm = 0.263235
  l1.bias: grad_norm = 0.008475
  l2.weight: grad_norm = 0.721910
Total gradient norm: 1.422456
=== Actor Training Debug (Iteration 7456) ===
Q mean: -78.158630
Q std: 29.529640
Actor loss: 78.162613
Action reg: 0.003985
  l1.weight: grad_norm = 0.111328
  l1.bias: grad_norm = 0.004307
  l2.weight: grad_norm = 0.242423
Total gradient norm: 0.458062
=== Actor Training Debug (Iteration 7457) ===
Q mean: -76.980721
Q std: 29.479296
Actor loss: 76.984695
Action reg: 0.003973
  l1.weight: grad_norm = 0.562147
  l1.bias: grad_norm = 0.010485
  l2.weight: grad_norm = 1.477838
Total gradient norm: 3.135479
=== Actor Training Debug (Iteration 7458) ===
Q mean: -79.134186
Q std: 29.691748
Actor loss: 79.138153
Action reg: 0.003970
  l1.weight: grad_norm = 0.213267
  l1.bias: grad_norm = 0.010535
  l2.weight: grad_norm = 0.465092
Total gradient norm: 1.049770
=== Actor Training Debug (Iteration 7459) ===
Q mean: -76.015884
Q std: 29.933416
Actor loss: 76.019867
Action reg: 0.003981
  l1.weight: grad_norm = 0.144764
  l1.bias: grad_norm = 0.005866
  l2.weight: grad_norm = 0.299254
Total gradient norm: 0.732871
=== Actor Training Debug (Iteration 7460) ===
Q mean: -75.507080
Q std: 28.702370
Actor loss: 75.511063
Action reg: 0.003983
  l1.weight: grad_norm = 0.224177
  l1.bias: grad_norm = 0.004961
  l2.weight: grad_norm = 0.610394
Total gradient norm: 1.168423
=== Actor Training Debug (Iteration 7461) ===
Q mean: -75.760803
Q std: 30.030376
Actor loss: 75.764778
Action reg: 0.003973
  l1.weight: grad_norm = 1.817748
  l1.bias: grad_norm = 0.009130
  l2.weight: grad_norm = 4.749686
Total gradient norm: 9.873492
=== Actor Training Debug (Iteration 7462) ===
Q mean: -77.879234
Q std: 28.291592
Actor loss: 77.883209
Action reg: 0.003975
  l1.weight: grad_norm = 0.145898
  l1.bias: grad_norm = 0.009519
  l2.weight: grad_norm = 0.304336
Total gradient norm: 0.534578
=== Actor Training Debug (Iteration 7463) ===
Q mean: -76.341705
Q std: 28.592430
Actor loss: 76.345695
Action reg: 0.003991
  l1.weight: grad_norm = 0.742930
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 1.968891
Total gradient norm: 4.250666
=== Actor Training Debug (Iteration 7464) ===
Q mean: -80.369339
Q std: 29.718216
Actor loss: 80.373322
Action reg: 0.003980
  l1.weight: grad_norm = 0.509801
  l1.bias: grad_norm = 0.006463
  l2.weight: grad_norm = 1.221364
Total gradient norm: 2.261037
=== Actor Training Debug (Iteration 7465) ===
Q mean: -77.662506
Q std: 30.235580
Actor loss: 77.666481
Action reg: 0.003975
  l1.weight: grad_norm = 0.353035
  l1.bias: grad_norm = 0.007714
  l2.weight: grad_norm = 0.777878
Total gradient norm: 1.392220
=== Actor Training Debug (Iteration 7466) ===
Q mean: -80.049179
Q std: 30.103073
Actor loss: 80.053169
Action reg: 0.003991
  l1.weight: grad_norm = 0.447561
  l1.bias: grad_norm = 0.002995
  l2.weight: grad_norm = 0.901428
Total gradient norm: 1.785580
=== Actor Training Debug (Iteration 7467) ===
Q mean: -80.283516
Q std: 29.096249
Actor loss: 80.287506
Action reg: 0.003994
  l1.weight: grad_norm = 0.023279
  l1.bias: grad_norm = 0.003902
  l2.weight: grad_norm = 0.056919
Total gradient norm: 0.128768
=== Actor Training Debug (Iteration 7468) ===
Q mean: -78.013664
Q std: 29.478561
Actor loss: 78.017654
Action reg: 0.003990
  l1.weight: grad_norm = 0.180253
  l1.bias: grad_norm = 0.001053
  l2.weight: grad_norm = 0.391158
Total gradient norm: 0.632515
=== Actor Training Debug (Iteration 7469) ===
Q mean: -75.773132
Q std: 29.735882
Actor loss: 75.777115
Action reg: 0.003983
  l1.weight: grad_norm = 0.216290
  l1.bias: grad_norm = 0.006503
  l2.weight: grad_norm = 0.383175
Total gradient norm: 0.792668
=== Actor Training Debug (Iteration 7470) ===
Q mean: -80.213951
Q std: 29.400982
Actor loss: 80.217941
Action reg: 0.003987
  l1.weight: grad_norm = 0.548001
  l1.bias: grad_norm = 0.002924
  l2.weight: grad_norm = 1.086304
Total gradient norm: 1.926002
=== Actor Training Debug (Iteration 7471) ===
Q mean: -77.759796
Q std: 30.326729
Actor loss: 77.763763
Action reg: 0.003967
  l1.weight: grad_norm = 0.528005
  l1.bias: grad_norm = 0.010346
  l2.weight: grad_norm = 1.051065
Total gradient norm: 2.052384
=== Actor Training Debug (Iteration 7472) ===
Q mean: -75.493790
Q std: 28.385702
Actor loss: 75.497780
Action reg: 0.003991
  l1.weight: grad_norm = 0.119111
  l1.bias: grad_norm = 0.003904
  l2.weight: grad_norm = 0.353423
Total gradient norm: 0.670376
=== Actor Training Debug (Iteration 7473) ===
Q mean: -75.508972
Q std: 28.375357
Actor loss: 75.512955
Action reg: 0.003979
  l1.weight: grad_norm = 0.339961
  l1.bias: grad_norm = 0.003493
  l2.weight: grad_norm = 0.712394
Total gradient norm: 1.276994
=== Actor Training Debug (Iteration 7474) ===
Q mean: -81.471886
Q std: 29.095896
Actor loss: 81.475876
Action reg: 0.003988
  l1.weight: grad_norm = 0.145550
  l1.bias: grad_norm = 0.002928
  l2.weight: grad_norm = 0.310776
Total gradient norm: 0.613538
=== Actor Training Debug (Iteration 7475) ===
Q mean: -76.946388
Q std: 30.088358
Actor loss: 76.950363
Action reg: 0.003974
  l1.weight: grad_norm = 0.405401
  l1.bias: grad_norm = 0.012297
  l2.weight: grad_norm = 0.992671
Total gradient norm: 2.177332
=== Actor Training Debug (Iteration 7476) ===
Q mean: -76.933609
Q std: 31.345272
Actor loss: 76.937584
Action reg: 0.003974
  l1.weight: grad_norm = 0.097720
  l1.bias: grad_norm = 0.011469
  l2.weight: grad_norm = 0.238419
Total gradient norm: 0.450318
=== Actor Training Debug (Iteration 7477) ===
Q mean: -76.368149
Q std: 28.955542
Actor loss: 76.372124
Action reg: 0.003976
  l1.weight: grad_norm = 0.078983
  l1.bias: grad_norm = 0.011262
  l2.weight: grad_norm = 0.176156
Total gradient norm: 0.365642
=== Actor Training Debug (Iteration 7478) ===
Q mean: -78.583542
Q std: 29.059013
Actor loss: 78.587509
Action reg: 0.003965
  l1.weight: grad_norm = 0.557272
  l1.bias: grad_norm = 0.012001
  l2.weight: grad_norm = 1.065420
Total gradient norm: 1.653526
=== Actor Training Debug (Iteration 7479) ===
Q mean: -74.687187
Q std: 30.109837
Actor loss: 74.691170
Action reg: 0.003981
  l1.weight: grad_norm = 0.248924
  l1.bias: grad_norm = 0.008543
  l2.weight: grad_norm = 0.584449
Total gradient norm: 1.102769
=== Actor Training Debug (Iteration 7480) ===
Q mean: -76.203415
Q std: 29.497864
Actor loss: 76.207397
Action reg: 0.003979
  l1.weight: grad_norm = 2.377416
  l1.bias: grad_norm = 0.010035
  l2.weight: grad_norm = 4.697819
Total gradient norm: 9.229271
=== Actor Training Debug (Iteration 7481) ===
Q mean: -73.327362
Q std: 29.910151
Actor loss: 73.331345
Action reg: 0.003984
  l1.weight: grad_norm = 0.085468
  l1.bias: grad_norm = 0.008066
  l2.weight: grad_norm = 0.238940
Total gradient norm: 0.458492
=== Actor Training Debug (Iteration 7482) ===
Q mean: -78.635513
Q std: 29.574129
Actor loss: 78.639488
Action reg: 0.003978
  l1.weight: grad_norm = 0.441518
  l1.bias: grad_norm = 0.005698
  l2.weight: grad_norm = 1.032244
Total gradient norm: 1.755308
=== Actor Training Debug (Iteration 7483) ===
Q mean: -79.149246
Q std: 29.142059
Actor loss: 79.153236
Action reg: 0.003988
  l1.weight: grad_norm = 0.052604
  l1.bias: grad_norm = 0.004797
  l2.weight: grad_norm = 0.148563
Total gradient norm: 0.294713
=== Actor Training Debug (Iteration 7484) ===
Q mean: -79.947235
Q std: 29.788803
Actor loss: 79.951218
Action reg: 0.003986
  l1.weight: grad_norm = 0.248672
  l1.bias: grad_norm = 0.006176
  l2.weight: grad_norm = 0.531966
Total gradient norm: 1.003280
=== Actor Training Debug (Iteration 7485) ===
Q mean: -76.150208
Q std: 29.592699
Actor loss: 76.154182
Action reg: 0.003978
  l1.weight: grad_norm = 0.067892
  l1.bias: grad_norm = 0.007999
  l2.weight: grad_norm = 0.138993
Total gradient norm: 0.259695
=== Actor Training Debug (Iteration 7486) ===
Q mean: -75.909576
Q std: 29.416248
Actor loss: 75.913559
Action reg: 0.003984
  l1.weight: grad_norm = 0.077162
  l1.bias: grad_norm = 0.007897
  l2.weight: grad_norm = 0.156221
Total gradient norm: 0.336049
=== Actor Training Debug (Iteration 7487) ===
Q mean: -77.975632
Q std: 31.570139
Actor loss: 77.979599
Action reg: 0.003970
  l1.weight: grad_norm = 0.501348
  l1.bias: grad_norm = 0.012403
  l2.weight: grad_norm = 0.908142
Total gradient norm: 1.330604
=== Actor Training Debug (Iteration 7488) ===
Q mean: -75.198021
Q std: 28.897642
Actor loss: 75.201996
Action reg: 0.003976
  l1.weight: grad_norm = 0.138171
  l1.bias: grad_norm = 0.007525
  l2.weight: grad_norm = 0.289762
Total gradient norm: 0.617193
=== Actor Training Debug (Iteration 7489) ===
Q mean: -76.625526
Q std: 31.638477
Actor loss: 76.629509
Action reg: 0.003984
  l1.weight: grad_norm = 0.569537
  l1.bias: grad_norm = 0.006447
  l2.weight: grad_norm = 1.577568
Total gradient norm: 3.174255
=== Actor Training Debug (Iteration 7490) ===
Q mean: -77.215538
Q std: 30.425352
Actor loss: 77.219513
Action reg: 0.003979
  l1.weight: grad_norm = 0.117521
  l1.bias: grad_norm = 0.009882
  l2.weight: grad_norm = 0.254793
Total gradient norm: 0.469051
=== Actor Training Debug (Iteration 7491) ===
Q mean: -75.401794
Q std: 28.632273
Actor loss: 75.405785
Action reg: 0.003990
  l1.weight: grad_norm = 0.281215
  l1.bias: grad_norm = 0.002658
  l2.weight: grad_norm = 0.655646
Total gradient norm: 1.267957
=== Actor Training Debug (Iteration 7492) ===
Q mean: -79.233292
Q std: 30.406136
Actor loss: 79.237274
Action reg: 0.003983
  l1.weight: grad_norm = 0.034674
  l1.bias: grad_norm = 0.008427
  l2.weight: grad_norm = 0.086772
Total gradient norm: 0.209905
=== Actor Training Debug (Iteration 7493) ===
Q mean: -76.732368
Q std: 28.970348
Actor loss: 76.736351
Action reg: 0.003979
  l1.weight: grad_norm = 0.378682
  l1.bias: grad_norm = 0.006786
  l2.weight: grad_norm = 0.759423
Total gradient norm: 1.368392
=== Actor Training Debug (Iteration 7494) ===
Q mean: -77.347595
Q std: 29.993073
Actor loss: 77.351585
Action reg: 0.003994
  l1.weight: grad_norm = 0.278267
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.659275
Total gradient norm: 1.501484
=== Actor Training Debug (Iteration 7495) ===
Q mean: -81.060211
Q std: 28.150652
Actor loss: 81.064209
Action reg: 0.003999
  l1.weight: grad_norm = 0.061941
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.151251
Total gradient norm: 0.296195
=== Actor Training Debug (Iteration 7496) ===
Q mean: -78.538025
Q std: 30.106344
Actor loss: 78.542007
Action reg: 0.003985
  l1.weight: grad_norm = 0.032495
  l1.bias: grad_norm = 0.006256
  l2.weight: grad_norm = 0.069048
Total gradient norm: 0.157817
=== Actor Training Debug (Iteration 7497) ===
Q mean: -76.944908
Q std: 29.117220
Actor loss: 76.948883
Action reg: 0.003972
  l1.weight: grad_norm = 0.096006
  l1.bias: grad_norm = 0.010303
  l2.weight: grad_norm = 0.204067
Total gradient norm: 0.360309
=== Actor Training Debug (Iteration 7498) ===
Q mean: -78.040588
Q std: 29.639194
Actor loss: 78.044579
Action reg: 0.003988
  l1.weight: grad_norm = 0.473966
  l1.bias: grad_norm = 0.003971
  l2.weight: grad_norm = 1.150836
Total gradient norm: 2.239482
=== Actor Training Debug (Iteration 7499) ===
Q mean: -75.597656
Q std: 27.978180
Actor loss: 75.601631
Action reg: 0.003975
  l1.weight: grad_norm = 0.170789
  l1.bias: grad_norm = 0.008994
  l2.weight: grad_norm = 0.336524
Total gradient norm: 0.609688
=== Actor Training Debug (Iteration 7500) ===
Q mean: -78.358414
Q std: 29.749544
Actor loss: 78.362404
Action reg: 0.003994
  l1.weight: grad_norm = 0.185919
  l1.bias: grad_norm = 0.002363
  l2.weight: grad_norm = 0.398600
Total gradient norm: 0.783521
  l1.bias: grad_norm = 0.006290tion 6486) ===
  l2.weight: grad_norm = 1.044671
Total gradient norm: 2.074995
=== Actor Training Debug (Iteration 7511) ===
Q mean: -75.537552
Q std: 29.875778
Actor loss: 75.541542
Action reg: 0.003989
  l1.weight: grad_norm = 0.234936
  l1.bias: grad_norm = 0.004457
  l2.weight: grad_norm = 0.591793
Total gradient norm: 1.143413
=== Actor Training Debug (Iteration 7512) ===
Q mean: -76.407364
Q std: 29.311609
Actor loss: 76.411339
Action reg: 0.003973
  l1.weight: grad_norm = 0.197751
  l1.bias: grad_norm = 0.012077
  l2.weight: grad_norm = 0.367063
Total gradient norm: 0.742944
=== Actor Training Debug (Iteration 7513) ===
Q mean: -77.523796
Q std: 28.770124
Actor loss: 77.527779
Action reg: 0.003982
  l1.weight: grad_norm = 0.356739
  l1.bias: grad_norm = 0.004593
  l2.weight: grad_norm = 0.764850
Total gradient norm: 1.559554
=== Actor Training Debug (Iteration 7514) ===
Q mean: -73.609825
Q std: 28.817671
Actor loss: 73.613792
Action reg: 0.003967
  l1.weight: grad_norm = 0.547704
  l1.bias: grad_norm = 0.007434
  l2.weight: grad_norm = 1.163082
Total gradient norm: 2.415513
=== Actor Training Debug (Iteration 7515) ===
Q mean: -78.574669
Q std: 29.710403
Actor loss: 78.578659
Action reg: 0.003992
  l1.weight: grad_norm = 0.167771
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.287271
Total gradient norm: 0.453442
=== Actor Training Debug (Iteration 7516) ===
Q mean: -77.684540
Q std: 29.851866
Actor loss: 77.688538
Action reg: 0.003996
  l1.weight: grad_norm = 0.084122
  l1.bias: grad_norm = 0.000713
  l2.weight: grad_norm = 0.169356
Total gradient norm: 0.318292
=== Actor Training Debug (Iteration 7517) ===
Q mean: -74.844170
Q std: 30.826141
Actor loss: 74.848145
Action reg: 0.003978
  l1.weight: grad_norm = 0.203579
  l1.bias: grad_norm = 0.009154
  l2.weight: grad_norm = 0.389212
Total gradient norm: 0.703306
=== Actor Training Debug (Iteration 7518) ===
Q mean: -73.431458
Q std: 29.477203
Actor loss: 73.435440
Action reg: 0.003983
  l1.weight: grad_norm = 0.246568
  l1.bias: grad_norm = 0.003249
  l2.weight: grad_norm = 0.632407
Total gradient norm: 1.230241
=== Actor Training Debug (Iteration 7519) ===
Q mean: -74.701897
Q std: 28.325558
Actor loss: 74.705887
Action reg: 0.003989
  l1.weight: grad_norm = 0.233055
  l1.bias: grad_norm = 0.004837
  l2.weight: grad_norm = 0.592376
Total gradient norm: 1.094393
=== Actor Training Debug (Iteration 7520) ===
Q mean: -79.342354
Q std: 29.458149
Actor loss: 79.346344
Action reg: 0.003991
  l1.weight: grad_norm = 0.170120
  l1.bias: grad_norm = 0.002163
  l2.weight: grad_norm = 0.463910
Total gradient norm: 0.884355
=== Actor Training Debug (Iteration 7521) ===
Q mean: -77.520760
Q std: 29.144630
Actor loss: 77.524750
Action reg: 0.003989
  l1.weight: grad_norm = 0.242029
  l1.bias: grad_norm = 0.004505
  l2.weight: grad_norm = 0.584017
Total gradient norm: 1.148406
=== Actor Training Debug (Iteration 7522) ===
Q mean: -77.743111
Q std: 29.295052
Actor loss: 77.747101
Action reg: 0.003993
  l1.weight: grad_norm = 0.140850
  l1.bias: grad_norm = 0.002044
  l2.weight: grad_norm = 0.278828
Total gradient norm: 0.526340
=== Actor Training Debug (Iteration 7523) ===
Q mean: -74.949997
Q std: 29.898380
Actor loss: 74.953972
Action reg: 0.003978
  l1.weight: grad_norm = 0.062822
  l1.bias: grad_norm = 0.013399
  l2.weight: grad_norm = 0.166055
Total gradient norm: 0.393758
=== Actor Training Debug (Iteration 7524) ===
Q mean: -75.948997
Q std: 28.508476
Actor loss: 75.952988
Action reg: 0.003991
  l1.weight: grad_norm = 0.659925
  l1.bias: grad_norm = 0.002684
  l2.weight: grad_norm = 1.425883
Total gradient norm: 2.923487
=== Actor Training Debug (Iteration 7525) ===
Q mean: -78.028954
Q std: 30.153681
Actor loss: 78.032936
Action reg: 0.003983
  l1.weight: grad_norm = 0.599919
  l1.bias: grad_norm = 0.004633
  l2.weight: grad_norm = 1.285715
Total gradient norm: 2.589822
=== Actor Training Debug (Iteration 7526) ===
Q mean: -78.448364
Q std: 29.953819
Actor loss: 78.452339
Action reg: 0.003978
  l1.weight: grad_norm = 0.040559
  l1.bias: grad_norm = 0.010579
  l2.weight: grad_norm = 0.116491
Total gradient norm: 0.282036
=== Actor Training Debug (Iteration 7527) ===
Q mean: -73.381363
Q std: 29.898994
Actor loss: 73.385353
Action reg: 0.003989
  l1.weight: grad_norm = 0.118233
  l1.bias: grad_norm = 0.003945
  l2.weight: grad_norm = 0.228038
Total gradient norm: 0.431712
=== Actor Training Debug (Iteration 7528) ===
Q mean: -75.801239
Q std: 30.692314
Actor loss: 75.805214
Action reg: 0.003976
  l1.weight: grad_norm = 0.117767
  l1.bias: grad_norm = 0.009118
  l2.weight: grad_norm = 0.226918
Total gradient norm: 0.394723
=== Actor Training Debug (Iteration 7529) ===
Q mean: -78.415039
Q std: 28.862663
Actor loss: 78.419029
Action reg: 0.003991
  l1.weight: grad_norm = 0.191150
  l1.bias: grad_norm = 0.001120
  l2.weight: grad_norm = 0.423220
Total gradient norm: 0.791795
=== Actor Training Debug (Iteration 7530) ===
Q mean: -79.391983
Q std: 28.749739
Actor loss: 79.395966
Action reg: 0.003982
  l1.weight: grad_norm = 0.540097
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 1.121320
Total gradient norm: 2.049782
=== Actor Training Debug (Iteration 7531) ===
Q mean: -74.705589
Q std: 28.789186
Actor loss: 74.709572
Action reg: 0.003982
  l1.weight: grad_norm = 0.333239
  l1.bias: grad_norm = 0.005384
  l2.weight: grad_norm = 0.604585
Total gradient norm: 1.173113
=== Actor Training Debug (Iteration 7532) ===
Q mean: -78.711670
Q std: 29.247152
Actor loss: 78.715652
Action reg: 0.003985
  l1.weight: grad_norm = 0.043808
  l1.bias: grad_norm = 0.003891
  l2.weight: grad_norm = 0.086930
Total gradient norm: 0.178284
=== Actor Training Debug (Iteration 7533) ===
Q mean: -75.433746
Q std: 30.483377
Actor loss: 75.437721
Action reg: 0.003978
  l1.weight: grad_norm = 0.271555
  l1.bias: grad_norm = 0.007203
  l2.weight: grad_norm = 0.573498
Total gradient norm: 1.034384
=== Actor Training Debug (Iteration 7534) ===
Q mean: -80.480835
Q std: 29.422401
Actor loss: 80.484825
Action reg: 0.003989
  l1.weight: grad_norm = 0.469507
  l1.bias: grad_norm = 0.003786
  l2.weight: grad_norm = 0.741343
Total gradient norm: 1.270309
=== Actor Training Debug (Iteration 7535) ===
Q mean: -76.700699
Q std: 29.017300
Actor loss: 76.704689
Action reg: 0.003988
  l1.weight: grad_norm = 0.627076
  l1.bias: grad_norm = 0.002735
  l2.weight: grad_norm = 1.652375
Total gradient norm: 3.356273
=== Actor Training Debug (Iteration 7536) ===
Q mean: -74.934738
Q std: 30.227970
Actor loss: 74.938721
Action reg: 0.003982
  l1.weight: grad_norm = 0.276925
  l1.bias: grad_norm = 0.003530
  l2.weight: grad_norm = 0.554664
Total gradient norm: 0.951085
=== Actor Training Debug (Iteration 7537) ===
Q mean: -79.966583
Q std: 29.275007
Actor loss: 79.970573
Action reg: 0.003991
  l1.weight: grad_norm = 0.039665
  l1.bias: grad_norm = 0.002555
  l2.weight: grad_norm = 0.078143
Total gradient norm: 0.140779
=== Actor Training Debug (Iteration 7538) ===
Q mean: -74.407753
Q std: 28.718479
Actor loss: 74.411743
Action reg: 0.003989
  l1.weight: grad_norm = 0.054604
  l1.bias: grad_norm = 0.005375
  l2.weight: grad_norm = 0.103009
Total gradient norm: 0.195780
=== Actor Training Debug (Iteration 7539) ===
Q mean: -76.829147
Q std: 30.484663
Actor loss: 76.833138
Action reg: 0.003987
  l1.weight: grad_norm = 0.793146
  l1.bias: grad_norm = 0.001971
  l2.weight: grad_norm = 1.605374
Total gradient norm: 2.888439
=== Actor Training Debug (Iteration 7540) ===
Q mean: -81.068817
Q std: 28.859028
Actor loss: 81.072807
Action reg: 0.003993
  l1.weight: grad_norm = 0.240845
  l1.bias: grad_norm = 0.003143
  l2.weight: grad_norm = 0.498825
Total gradient norm: 0.923821
=== Actor Training Debug (Iteration 7541) ===
Q mean: -77.435852
Q std: 29.300480
Actor loss: 77.439827
Action reg: 0.003975
  l1.weight: grad_norm = 0.450146
  l1.bias: grad_norm = 0.010419
  l2.weight: grad_norm = 1.146455
Total gradient norm: 2.419626
=== Actor Training Debug (Iteration 7542) ===
Q mean: -73.707031
Q std: 29.794828
Actor loss: 73.711006
Action reg: 0.003974
  l1.weight: grad_norm = 0.233922
  l1.bias: grad_norm = 0.008664
  l2.weight: grad_norm = 0.467459
Total gradient norm: 0.844077
=== Actor Training Debug (Iteration 7543) ===
Q mean: -77.286026
Q std: 29.728857
Actor loss: 77.290001
Action reg: 0.003978
  l1.weight: grad_norm = 0.082161
  l1.bias: grad_norm = 0.010685
  l2.weight: grad_norm = 0.190849
Total gradient norm: 0.342782
=== Actor Training Debug (Iteration 7544) ===
Q mean: -76.965645
Q std: 28.821217
Actor loss: 76.969635
Action reg: 0.003992
  l1.weight: grad_norm = 0.364931
  l1.bias: grad_norm = 0.002861
  l2.weight: grad_norm = 0.759014
Total gradient norm: 1.505585
=== Actor Training Debug (Iteration 7545) ===
Q mean: -77.362854
Q std: 28.752979
Actor loss: 77.366837
Action reg: 0.003981
  l1.weight: grad_norm = 0.147851
  l1.bias: grad_norm = 0.005859
  l2.weight: grad_norm = 0.341493
Total gradient norm: 0.723174
=== Actor Training Debug (Iteration 7546) ===
Q mean: -78.694183
Q std: 29.499018
Actor loss: 78.698158
Action reg: 0.003977
  l1.weight: grad_norm = 0.059364
  l1.bias: grad_norm = 0.008048
  l2.weight: grad_norm = 0.134071
Total gradient norm: 0.250005
=== Actor Training Debug (Iteration 7547) ===
Q mean: -75.491852
Q std: 28.624844
Actor loss: 75.495834
Action reg: 0.003985
  l1.weight: grad_norm = 0.025513
  l1.bias: grad_norm = 0.006253
  l2.weight: grad_norm = 0.056072
Total gradient norm: 0.134004
=== Actor Training Debug (Iteration 7548) ===
Q mean: -77.620041
Q std: 30.138920
Actor loss: 77.624023
Action reg: 0.003985
  l1.weight: grad_norm = 0.080560
  l1.bias: grad_norm = 0.006346
  l2.weight: grad_norm = 0.205211
Total gradient norm: 0.390275
=== Actor Training Debug (Iteration 7549) ===
Q mean: -73.495285
Q std: 29.603287
Actor loss: 73.499268
Action reg: 0.003983
  l1.weight: grad_norm = 0.272514
  l1.bias: grad_norm = 0.003944
  l2.weight: grad_norm = 0.644710
Total gradient norm: 1.296368
=== Actor Training Debug (Iteration 7550) ===
Q mean: -77.844193
Q std: 30.277044
Actor loss: 77.848175
Action reg: 0.003982
  l1.weight: grad_norm = 0.206603
  l1.bias: grad_norm = 0.009240
  l2.weight: grad_norm = 0.512118
Total gradient norm: 1.044380
=== Actor Training Debug (Iteration 7551) ===
Q mean: -78.240768
Q std: 28.810472
Actor loss: 78.244759
Action reg: 0.003988
  l1.weight: grad_norm = 0.121999
  l1.bias: grad_norm = 0.003647
  l2.weight: grad_norm = 0.257563
Total gradient norm: 0.435226
=== Actor Training Debug (Iteration 7552) ===
Q mean: -77.332932
Q std: 28.306725
Actor loss: 77.336929
Action reg: 0.003994
  l1.weight: grad_norm = 0.170706
  l1.bias: grad_norm = 0.000813
  l2.weight: grad_norm = 0.385334
Total gradient norm: 0.627428
=== Actor Training Debug (Iteration 7553) ===
Q mean: -77.169144
Q std: 29.786219
Actor loss: 77.173134
Action reg: 0.003988
  l1.weight: grad_norm = 0.167617
  l1.bias: grad_norm = 0.006256
  l2.weight: grad_norm = 0.351602
Total gradient norm: 0.620627
=== Actor Training Debug (Iteration 7554) ===
Q mean: -77.157249
Q std: 30.253372
Actor loss: 77.161240
Action reg: 0.003987
  l1.weight: grad_norm = 0.131825
  l1.bias: grad_norm = 0.003502
  l2.weight: grad_norm = 0.309707
Total gradient norm: 0.591024
=== Actor Training Debug (Iteration 7555) ===
Q mean: -74.947922
Q std: 28.072836
Actor loss: 74.951904
Action reg: 0.003984
  l1.weight: grad_norm = 0.184641
  l1.bias: grad_norm = 0.008491
  l2.weight: grad_norm = 0.392308
Total gradient norm: 0.697562
=== Actor Training Debug (Iteration 7556) ===
Q mean: -77.099854
Q std: 29.937630
Actor loss: 77.103851
Action reg: 0.003996
  l1.weight: grad_norm = 0.216053
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.502945
Total gradient norm: 0.927660
=== Actor Training Debug (Iteration 7557) ===
Q mean: -77.948524
Q std: 29.987549
Actor loss: 77.952507
Action reg: 0.003980
  l1.weight: grad_norm = 0.304189
  l1.bias: grad_norm = 0.005307
  l2.weight: grad_norm = 0.818700
Total gradient norm: 1.706611
=== Actor Training Debug (Iteration 7558) ===
Q mean: -79.583389
Q std: 29.979374
Actor loss: 79.587372
Action reg: 0.003984
  l1.weight: grad_norm = 0.215092
  l1.bias: grad_norm = 0.001813
  l2.weight: grad_norm = 0.467224
Total gradient norm: 0.882283
=== Actor Training Debug (Iteration 7559) ===
Q mean: -72.921814
Q std: 28.704527
Actor loss: 72.925797
Action reg: 0.003982
  l1.weight: grad_norm = 0.160809
  l1.bias: grad_norm = 0.005446
  l2.weight: grad_norm = 0.343137
Total gradient norm: 0.612491
=== Actor Training Debug (Iteration 7560) ===
Q mean: -76.232025
Q std: 29.395224
Actor loss: 76.236023
Action reg: 0.003994
  l1.weight: grad_norm = 0.066771
  l1.bias: grad_norm = 0.001249
  l2.weight: grad_norm = 0.128510
Total gradient norm: 0.271765
=== Actor Training Debug (Iteration 7561) ===
Q mean: -77.793655
Q std: 28.804079
Actor loss: 77.797646
Action reg: 0.003987
  l1.weight: grad_norm = 0.148992
  l1.bias: grad_norm = 0.004151
  l2.weight: grad_norm = 0.278305
Total gradient norm: 0.515582
=== Actor Training Debug (Iteration 7562) ===
Q mean: -76.171333
Q std: 28.297377
Actor loss: 76.175323
Action reg: 0.003991
  l1.weight: grad_norm = 0.100708
  l1.bias: grad_norm = 0.002695
  l2.weight: grad_norm = 0.183680
Total gradient norm: 0.336864
=== Actor Training Debug (Iteration 7563) ===
Q mean: -77.042763
Q std: 28.387039
Actor loss: 77.046753
Action reg: 0.003989
  l1.weight: grad_norm = 0.266034
  l1.bias: grad_norm = 0.001739
  l2.weight: grad_norm = 0.517747
Total gradient norm: 1.015142
=== Actor Training Debug (Iteration 7564) ===
Q mean: -76.065323
Q std: 29.303087
Actor loss: 76.069305
Action reg: 0.003986
  l1.weight: grad_norm = 0.365228
  l1.bias: grad_norm = 0.002654
  l2.weight: grad_norm = 0.794421
Total gradient norm: 1.525835
=== Actor Training Debug (Iteration 7565) ===
Q mean: -75.249054
Q std: 28.213295
Actor loss: 75.253036
Action reg: 0.003986
  l1.weight: grad_norm = 0.262454
  l1.bias: grad_norm = 0.003054
  l2.weight: grad_norm = 0.482967
Total gradient norm: 0.847725
=== Actor Training Debug (Iteration 7566) ===
Q mean: -75.372650
Q std: 30.050196
Actor loss: 75.376633
Action reg: 0.003985
  l1.weight: grad_norm = 0.161098
  l1.bias: grad_norm = 0.005638
  l2.weight: grad_norm = 0.346794
Total gradient norm: 0.640779
=== Actor Training Debug (Iteration 7567) ===
Q mean: -77.507050
Q std: 30.736311
Actor loss: 77.511040
Action reg: 0.003989
  l1.weight: grad_norm = 0.227071
  l1.bias: grad_norm = 0.002020
  l2.weight: grad_norm = 0.503800
Total gradient norm: 0.868137
=== Actor Training Debug (Iteration 7568) ===
Q mean: -73.941185
Q std: 29.310997
Actor loss: 73.945168
Action reg: 0.003986
  l1.weight: grad_norm = 0.316668
  l1.bias: grad_norm = 0.003414
  l2.weight: grad_norm = 0.789702
Total gradient norm: 1.401106
=== Actor Training Debug (Iteration 7569) ===
Q mean: -79.336258
Q std: 30.812822
Actor loss: 79.340240
Action reg: 0.003983
  l1.weight: grad_norm = 0.335531
  l1.bias: grad_norm = 0.002923
  l2.weight: grad_norm = 0.752570
Total gradient norm: 1.878069
=== Actor Training Debug (Iteration 7570) ===
Q mean: -81.308922
Q std: 28.910053
Actor loss: 81.312920
Action reg: 0.003996
  l1.weight: grad_norm = 0.176285
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.316420
Total gradient norm: 0.556170
=== Actor Training Debug (Iteration 7571) ===
Q mean: -78.777969
Q std: 28.457779
Actor loss: 78.781952
Action reg: 0.003985
  l1.weight: grad_norm = 0.299171
  l1.bias: grad_norm = 0.002699
  l2.weight: grad_norm = 0.713724
Total gradient norm: 1.434698
=== Actor Training Debug (Iteration 7572) ===
Q mean: -79.437531
Q std: 28.952780
Actor loss: 79.441513
Action reg: 0.003983
  l1.weight: grad_norm = 0.183338
  l1.bias: grad_norm = 0.006742
  l2.weight: grad_norm = 0.374204
Total gradient norm: 0.786999
=== Actor Training Debug (Iteration 7573) ===
Q mean: -77.758636
Q std: 29.336740
Actor loss: 77.762619
Action reg: 0.003985
  l1.weight: grad_norm = 0.399986
  l1.bias: grad_norm = 0.004778
  l2.weight: grad_norm = 0.814948
Total gradient norm: 1.823338
=== Actor Training Debug (Iteration 7574) ===
Q mean: -78.304138
Q std: 29.730036
Actor loss: 78.308121
Action reg: 0.003983
  l1.weight: grad_norm = 0.498084
  l1.bias: grad_norm = 0.006177
  l2.weight: grad_norm = 0.801249
Total gradient norm: 1.542353
=== Actor Training Debug (Iteration 7575) ===
Q mean: -77.777313
Q std: 28.818699
Actor loss: 77.781303
Action reg: 0.003994
  l1.weight: grad_norm = 0.503093
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.912325
Total gradient norm: 1.590170
=== Actor Training Debug (Iteration 7576) ===
Q mean: -76.242859
Q std: 29.769222
Actor loss: 76.246849
Action reg: 0.003988
  l1.weight: grad_norm = 0.559841
  l1.bias: grad_norm = 0.003545
  l2.weight: grad_norm = 1.226910
Total gradient norm: 2.550356
=== Actor Training Debug (Iteration 7577) ===
Q mean: -75.150970
Q std: 31.737497
Actor loss: 75.154938
Action reg: 0.003969
  l1.weight: grad_norm = 0.134881
  l1.bias: grad_norm = 0.011658
  l2.weight: grad_norm = 0.258195
Total gradient norm: 0.442797
=== Actor Training Debug (Iteration 7578) ===
Q mean: -78.896461
Q std: 29.832760
Actor loss: 78.900459
Action reg: 0.003998
  l1.weight: grad_norm = 0.037519
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.080225
Total gradient norm: 0.171364
=== Actor Training Debug (Iteration 7579) ===
Q mean: -79.150375
Q std: 29.694422
Actor loss: 79.154366
Action reg: 0.003991
  l1.weight: grad_norm = 0.324474
  l1.bias: grad_norm = 0.001176
  l2.weight: grad_norm = 0.732638
Total gradient norm: 1.462648
=== Actor Training Debug (Iteration 7580) ===
Q mean: -76.240036
Q std: 28.440994
Actor loss: 76.244026
Action reg: 0.003991
  l1.weight: grad_norm = 0.148152
  l1.bias: grad_norm = 0.002309
  l2.weight: grad_norm = 0.321351
Total gradient norm: 0.577476
=== Actor Training Debug (Iteration 7581) ===
Q mean: -74.426140
Q std: 29.531115
Actor loss: 74.430130
Action reg: 0.003988
  l1.weight: grad_norm = 0.269187
  l1.bias: grad_norm = 0.000666
  l2.weight: grad_norm = 0.521846
Total gradient norm: 0.996448
=== Actor Training Debug (Iteration 7582) ===
Q mean: -75.444290
Q std: 28.754539
Actor loss: 75.448265
Action reg: 0.003973
  l1.weight: grad_norm = 0.143275
  l1.bias: grad_norm = 0.004514
  l2.weight: grad_norm = 0.315507
Total gradient norm: 0.567742
=== Actor Training Debug (Iteration 7583) ===
Q mean: -74.425354
Q std: 29.541731
Actor loss: 74.429337
Action reg: 0.003983
  l1.weight: grad_norm = 0.166750
  l1.bias: grad_norm = 0.001961
  l2.weight: grad_norm = 0.392478
Total gradient norm: 0.792452
=== Actor Training Debug (Iteration 7584) ===
Q mean: -77.914955
Q std: 29.311298
Actor loss: 77.918945
Action reg: 0.003993
  l1.weight: grad_norm = 0.061318
  l1.bias: grad_norm = 0.000627
  l2.weight: grad_norm = 0.132624
Total gradient norm: 0.278700
=== Actor Training Debug (Iteration 7585) ===
Q mean: -79.204636
Q std: 29.744652
Actor loss: 79.208626
Action reg: 0.003989
  l1.weight: grad_norm = 0.091888
  l1.bias: grad_norm = 0.004183
  l2.weight: grad_norm = 0.235401
Total gradient norm: 0.507637
=== Actor Training Debug (Iteration 7586) ===
Q mean: -77.302673
Q std: 30.565746
Actor loss: 77.306664
Action reg: 0.003986
  l1.weight: grad_norm = 0.363078
  l1.bias: grad_norm = 0.002159
  l2.weight: grad_norm = 0.841389
Total gradient norm: 1.637187
=== Actor Training Debug (Iteration 7587) ===
Q mean: -73.780792
Q std: 29.024654
Actor loss: 73.784775
Action reg: 0.003979
  l1.weight: grad_norm = 0.470956
  l1.bias: grad_norm = 0.006696
  l2.weight: grad_norm = 0.898335
Total gradient norm: 1.492658
=== Actor Training Debug (Iteration 7588) ===
Q mean: -76.391220
Q std: 28.825277
Actor loss: 76.395210
Action reg: 0.003991
  l1.weight: grad_norm = 0.041059
  l1.bias: grad_norm = 0.004395
  l2.weight: grad_norm = 0.111610
Total gradient norm: 0.234343
=== Actor Training Debug (Iteration 7589) ===
Q mean: -76.506470
Q std: 29.666775
Actor loss: 76.510452
Action reg: 0.003979
  l1.weight: grad_norm = 0.249649
  l1.bias: grad_norm = 0.005846
  l2.weight: grad_norm = 0.458752
Total gradient norm: 0.907530
=== Actor Training Debug (Iteration 7590) ===
Q mean: -79.897346
Q std: 30.706856
Actor loss: 79.901329
Action reg: 0.003981
  l1.weight: grad_norm = 0.533961
  l1.bias: grad_norm = 0.004048
  l2.weight: grad_norm = 1.075673
Total gradient norm: 2.033038
=== Actor Training Debug (Iteration 7591) ===
Q mean: -78.467110
Q std: 29.894785
Actor loss: 78.471100
Action reg: 0.003987
  l1.weight: grad_norm = 0.494227
  l1.bias: grad_norm = 0.001958
  l2.weight: grad_norm = 0.985985
Total gradient norm: 2.058133
=== Actor Training Debug (Iteration 7592) ===
Q mean: -77.313614
Q std: 30.531921
Actor loss: 77.317596
Action reg: 0.003983
  l1.weight: grad_norm = 0.059971
  l1.bias: grad_norm = 0.008618
  l2.weight: grad_norm = 0.158254
Total gradient norm: 0.321391
=== Actor Training Debug (Iteration 7593) ===
Q mean: -75.681961
Q std: 31.117977
Actor loss: 75.685959
Action reg: 0.003994
  l1.weight: grad_norm = 0.040827
  l1.bias: grad_norm = 0.001928
  l2.weight: grad_norm = 0.104348
Total gradient norm: 0.201482
=== Actor Training Debug (Iteration 7594) ===
Q mean: -80.545967
Q std: 30.315151
Actor loss: 80.549957
Action reg: 0.003986
  l1.weight: grad_norm = 0.320968
  l1.bias: grad_norm = 0.002411
  l2.weight: grad_norm = 0.791913
Total gradient norm: 1.464277
=== Actor Training Debug (Iteration 7595) ===
Q mean: -75.708649
Q std: 30.597345
Actor loss: 75.712639
Action reg: 0.003990
  l1.weight: grad_norm = 0.025226
  l1.bias: grad_norm = 0.005912
  l2.weight: grad_norm = 0.055308
Total gradient norm: 0.129877
=== Actor Training Debug (Iteration 7596) ===
Q mean: -75.831329
Q std: 29.354637
Actor loss: 75.835327
Action reg: 0.003996
  l1.weight: grad_norm = 0.011627
  l1.bias: grad_norm = 0.001449
  l2.weight: grad_norm = 0.024155
Total gradient norm: 0.042254
=== Actor Training Debug (Iteration 7597) ===
Q mean: -75.642715
Q std: 30.643950
Actor loss: 75.646706
Action reg: 0.003987
  l1.weight: grad_norm = 0.479894
  l1.bias: grad_norm = 0.001994
  l2.weight: grad_norm = 1.129431
Total gradient norm: 2.112501
=== Actor Training Debug (Iteration 7598) ===
Q mean: -73.416229
Q std: 29.995483
Actor loss: 73.420197
Action reg: 0.003971
  l1.weight: grad_norm = 0.238429
  l1.bias: grad_norm = 0.003176
  l2.weight: grad_norm = 0.544097
Total gradient norm: 0.992794
=== Actor Training Debug (Iteration 7599) ===
Q mean: -79.094315
Q std: 30.170464
Actor loss: 79.098282
Action reg: 0.003964
  l1.weight: grad_norm = 0.386369
  l1.bias: grad_norm = 0.004037
  l2.weight: grad_norm = 0.754963
Total gradient norm: 1.320640
=== Actor Training Debug (Iteration 7600) ===
Q mean: -77.767792
Q std: 29.665630
Actor loss: 77.771782
Action reg: 0.003991
  l1.weight: grad_norm = 0.154300
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.328282
Total gradient norm: 0.739935
=== Actor Training Debug (Iteration 7601) ===
Q mean: -71.478668
Q std: 30.873295
Actor loss: 71.482635
Action reg: 0.003968
  l1.weight: grad_norm = 0.396853
  l1.bias: grad_norm = 0.015892
  l2.weight: grad_norm = 0.857010
Total gradient norm: 1.416310
=== Actor Training Debug (Iteration 7602) ===
Q mean: -78.108047
Q std: 30.548761
Actor loss: 78.112030
Action reg: 0.003981
  l1.weight: grad_norm = 0.361593
  l1.bias: grad_norm = 0.005612
  l2.weight: grad_norm = 0.655850
Total gradient norm: 1.151285
=== Actor Training Debug (Iteration 7603) ===
Q mean: -76.266434
Q std: 29.775501
Actor loss: 76.270416
Action reg: 0.003980
  l1.weight: grad_norm = 0.672670
  l1.bias: grad_norm = 0.005886
  l2.weight: grad_norm = 1.125523
Total gradient norm: 2.293422
=== Actor Training Debug (Iteration 7604) ===
Q mean: -77.588203
Q std: 29.034565
Actor loss: 77.592194
Action reg: 0.003988
  l1.weight: grad_norm = 0.274088
  l1.bias: grad_norm = 0.001901
  l2.weight: grad_norm = 0.529208
Total gradient norm: 0.938870
=== Actor Training Debug (Iteration 7605) ===
Q mean: -79.632149
Q std: 30.125732
Actor loss: 79.636131
Action reg: 0.003984
  l1.weight: grad_norm = 0.086924
  l1.bias: grad_norm = 0.005344
  l2.weight: grad_norm = 0.176745
Total gradient norm: 0.346659
=== Actor Training Debug (Iteration 7606) ===
Q mean: -74.806236
Q std: 29.917000
Actor loss: 74.810219
Action reg: 0.003981
  l1.weight: grad_norm = 0.062095
  l1.bias: grad_norm = 0.009581
  l2.weight: grad_norm = 0.148966
Total gradient norm: 0.298025
=== Actor Training Debug (Iteration 7607) ===
Q mean: -76.807602
Q std: 30.566938
Actor loss: 76.811592
Action reg: 0.003987
  l1.weight: grad_norm = 0.673470
  l1.bias: grad_norm = 0.006315
  l2.weight: grad_norm = 1.338975
Total gradient norm: 2.810251
=== Actor Training Debug (Iteration 7608) ===
Q mean: -73.053162
Q std: 31.257732
Actor loss: 73.057137
Action reg: 0.003977
  l1.weight: grad_norm = 0.219696
  l1.bias: grad_norm = 0.006460
  l2.weight: grad_norm = 0.382719
Total gradient norm: 0.696345
=== Actor Training Debug (Iteration 7609) ===
Q mean: -75.549461
Q std: 29.066187
Actor loss: 75.553436
Action reg: 0.003973
  l1.weight: grad_norm = 0.219726
  l1.bias: grad_norm = 0.005641
  l2.weight: grad_norm = 0.605861
Total gradient norm: 1.251025
=== Actor Training Debug (Iteration 7610) ===
Q mean: -77.707207
Q std: 30.059372
Actor loss: 77.711189
Action reg: 0.003980
  l1.weight: grad_norm = 0.977512
  l1.bias: grad_norm = 0.005509
  l2.weight: grad_norm = 2.479559
Total gradient norm: 5.237380
=== Actor Training Debug (Iteration 7611) ===
Q mean: -80.716156
Q std: 31.003237
Actor loss: 80.720139
Action reg: 0.003981
  l1.weight: grad_norm = 0.244606
  l1.bias: grad_norm = 0.004568
  l2.weight: grad_norm = 0.516969
Total gradient norm: 0.948421
=== Actor Training Debug (Iteration 7612) ===
Q mean: -76.835312
Q std: 30.096079
Actor loss: 76.839294
Action reg: 0.003984
  l1.weight: grad_norm = 0.257594
  l1.bias: grad_norm = 0.002686
  l2.weight: grad_norm = 0.472462
Total gradient norm: 0.880912
=== Actor Training Debug (Iteration 7613) ===
Q mean: -79.088341
Q std: 29.446756
Actor loss: 79.092331
Action reg: 0.003987
  l1.weight: grad_norm = 0.251810
  l1.bias: grad_norm = 0.003070
  l2.weight: grad_norm = 0.474808
Total gradient norm: 0.751681
=== Actor Training Debug (Iteration 7614) ===
Q mean: -74.814941
Q std: 29.467129
Actor loss: 74.818924
Action reg: 0.003980
  l1.weight: grad_norm = 0.263318
  l1.bias: grad_norm = 0.004815
  l2.weight: grad_norm = 0.457713
Total gradient norm: 0.986367
=== Actor Training Debug (Iteration 7615) ===
Q mean: -75.281494
Q std: 29.951340
Actor loss: 75.285469
Action reg: 0.003978
  l1.weight: grad_norm = 0.285054
  l1.bias: grad_norm = 0.007461
  l2.weight: grad_norm = 0.686854
Total gradient norm: 1.338379
=== Actor Training Debug (Iteration 7616) ===
Q mean: -78.730553
Q std: 28.859669
Actor loss: 78.734543
Action reg: 0.003993
  l1.weight: grad_norm = 0.098354
  l1.bias: grad_norm = 0.002358
  l2.weight: grad_norm = 0.216385
Total gradient norm: 0.436177
=== Actor Training Debug (Iteration 7617) ===
Q mean: -77.584579
Q std: 28.547239
Actor loss: 77.588577
Action reg: 0.003997
  l1.weight: grad_norm = 0.008137
  l1.bias: grad_norm = 0.000957
  l2.weight: grad_norm = 0.019065
Total gradient norm: 0.042600
Total gradient norm: 0.62862690tion 6486) ===
=== Actor Training Debug (Iteration 7628) ===
Q mean: -76.424820
Q std: 29.681387
Actor loss: 76.428795
Action reg: 0.003978
  l1.weight: grad_norm = 0.487595
  l1.bias: grad_norm = 0.006360
  l2.weight: grad_norm = 0.851727
Total gradient norm: 1.515535
=== Actor Training Debug (Iteration 7629) ===
Q mean: -80.973854
Q std: 29.377556
Actor loss: 80.977837
Action reg: 0.003986
  l1.weight: grad_norm = 0.210196
  l1.bias: grad_norm = 0.004649
  l2.weight: grad_norm = 0.362824
Total gradient norm: 0.658026
=== Actor Training Debug (Iteration 7630) ===
Q mean: -77.269730
Q std: 30.279964
Actor loss: 77.273720
Action reg: 0.003987
  l1.weight: grad_norm = 0.119521
  l1.bias: grad_norm = 0.001522
  l2.weight: grad_norm = 0.286696
Total gradient norm: 0.563519
=== Actor Training Debug (Iteration 7631) ===
Q mean: -80.571945
Q std: 28.992195
Actor loss: 80.575935
Action reg: 0.003991
  l1.weight: grad_norm = 0.547855
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.976593
Total gradient norm: 1.676769
=== Actor Training Debug (Iteration 7632) ===
Q mean: -77.920586
Q std: 31.849579
Actor loss: 77.924568
Action reg: 0.003979
  l1.weight: grad_norm = 0.592123
  l1.bias: grad_norm = 0.007022
  l2.weight: grad_norm = 1.249132
Total gradient norm: 2.633854
=== Actor Training Debug (Iteration 7633) ===
Q mean: -77.532578
Q std: 30.117916
Actor loss: 77.536568
Action reg: 0.003992
  l1.weight: grad_norm = 0.283503
  l1.bias: grad_norm = 0.001865
  l2.weight: grad_norm = 0.697001
Total gradient norm: 1.316792
=== Actor Training Debug (Iteration 7634) ===
Q mean: -76.355103
Q std: 31.637983
Actor loss: 76.359093
Action reg: 0.003988
  l1.weight: grad_norm = 0.184523
  l1.bias: grad_norm = 0.003015
  l2.weight: grad_norm = 0.337876
Total gradient norm: 0.630116
=== Actor Training Debug (Iteration 7635) ===
Q mean: -79.484314
Q std: 29.463728
Actor loss: 79.488304
Action reg: 0.003994
  l1.weight: grad_norm = 0.097308
  l1.bias: grad_norm = 0.001082
  l2.weight: grad_norm = 0.208088
Total gradient norm: 0.388448
=== Actor Training Debug (Iteration 7636) ===
Q mean: -78.278320
Q std: 30.077044
Actor loss: 78.282303
Action reg: 0.003983
  l1.weight: grad_norm = 0.152677
  l1.bias: grad_norm = 0.005827
  l2.weight: grad_norm = 0.327976
Total gradient norm: 0.614771
=== Actor Training Debug (Iteration 7637) ===
Q mean: -81.795303
Q std: 28.396786
Actor loss: 81.799294
Action reg: 0.003990
  l1.weight: grad_norm = 0.214756
  l1.bias: grad_norm = 0.001069
  l2.weight: grad_norm = 0.469601
Total gradient norm: 0.833289
=== Actor Training Debug (Iteration 7638) ===
Q mean: -77.349442
Q std: 29.300440
Actor loss: 77.353432
Action reg: 0.003988
  l1.weight: grad_norm = 0.242959
  l1.bias: grad_norm = 0.002008
  l2.weight: grad_norm = 0.570764
Total gradient norm: 1.146840
=== Actor Training Debug (Iteration 7639) ===
Q mean: -74.049255
Q std: 30.204357
Actor loss: 74.053238
Action reg: 0.003980
  l1.weight: grad_norm = 0.349027
  l1.bias: grad_norm = 0.006611
  l2.weight: grad_norm = 0.678839
Total gradient norm: 1.324492
=== Actor Training Debug (Iteration 7640) ===
Q mean: -79.253860
Q std: 29.161489
Actor loss: 79.257851
Action reg: 0.003987
  l1.weight: grad_norm = 0.476990
  l1.bias: grad_norm = 0.002916
  l2.weight: grad_norm = 1.112002
Total gradient norm: 2.000088
=== Actor Training Debug (Iteration 7641) ===
Q mean: -79.313995
Q std: 29.236794
Actor loss: 79.317993
Action reg: 0.003995
  l1.weight: grad_norm = 0.159982
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.308199
Total gradient norm: 0.570125
=== Actor Training Debug (Iteration 7642) ===
Q mean: -77.961418
Q std: 29.977579
Actor loss: 77.965416
Action reg: 0.003995
  l1.weight: grad_norm = 0.080028
  l1.bias: grad_norm = 0.001522
  l2.weight: grad_norm = 0.153979
Total gradient norm: 0.294021
=== Actor Training Debug (Iteration 7643) ===
Q mean: -79.466606
Q std: 30.385324
Actor loss: 79.470589
Action reg: 0.003985
  l1.weight: grad_norm = 0.174164
  l1.bias: grad_norm = 0.001872
  l2.weight: grad_norm = 0.361597
Total gradient norm: 0.738202
=== Actor Training Debug (Iteration 7644) ===
Q mean: -76.128815
Q std: 30.168941
Actor loss: 76.132805
Action reg: 0.003987
  l1.weight: grad_norm = 0.214765
  l1.bias: grad_norm = 0.005039
  l2.weight: grad_norm = 0.361832
Total gradient norm: 0.677015
=== Actor Training Debug (Iteration 7645) ===
Q mean: -76.692497
Q std: 29.471945
Actor loss: 76.696487
Action reg: 0.003992
  l1.weight: grad_norm = 0.135574
  l1.bias: grad_norm = 0.004221
  l2.weight: grad_norm = 0.297436
Total gradient norm: 0.647153
=== Actor Training Debug (Iteration 7646) ===
Q mean: -76.130966
Q std: 28.898153
Actor loss: 76.134956
Action reg: 0.003988
  l1.weight: grad_norm = 0.746816
  l1.bias: grad_norm = 0.000691
  l2.weight: grad_norm = 1.290460
Total gradient norm: 2.624409
=== Actor Training Debug (Iteration 7647) ===
Q mean: -76.517212
Q std: 30.583773
Actor loss: 76.521194
Action reg: 0.003984
  l1.weight: grad_norm = 0.689186
  l1.bias: grad_norm = 0.004434
  l2.weight: grad_norm = 1.222526
Total gradient norm: 2.562400
=== Actor Training Debug (Iteration 7648) ===
Q mean: -78.756401
Q std: 29.654539
Actor loss: 78.760391
Action reg: 0.003992
  l1.weight: grad_norm = 0.502988
  l1.bias: grad_norm = 0.002644
  l2.weight: grad_norm = 1.073259
Total gradient norm: 2.142340
=== Actor Training Debug (Iteration 7649) ===
Q mean: -75.446426
Q std: 28.281164
Actor loss: 75.450424
Action reg: 0.003997
  l1.weight: grad_norm = 0.087090
  l1.bias: grad_norm = 0.000857
  l2.weight: grad_norm = 0.151747
Total gradient norm: 0.326377
=== Actor Training Debug (Iteration 7650) ===
Q mean: -76.762024
Q std: 28.949654
Actor loss: 76.766014
Action reg: 0.003991
  l1.weight: grad_norm = 0.127806
  l1.bias: grad_norm = 0.002564
  l2.weight: grad_norm = 0.264968
Total gradient norm: 0.458414
=== Actor Training Debug (Iteration 7651) ===
Q mean: -79.048126
Q std: 29.239695
Actor loss: 79.052124
Action reg: 0.003997
  l1.weight: grad_norm = 0.072277
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.170951
Total gradient norm: 0.363859
=== Actor Training Debug (Iteration 7652) ===
Q mean: -80.205803
Q std: 28.583906
Actor loss: 80.209801
Action reg: 0.003994
  l1.weight: grad_norm = 0.229974
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.415019
Total gradient norm: 0.846596
=== Actor Training Debug (Iteration 7653) ===
Q mean: -77.112167
Q std: 30.059395
Actor loss: 77.116150
Action reg: 0.003985
  l1.weight: grad_norm = 0.340457
  l1.bias: grad_norm = 0.005708
  l2.weight: grad_norm = 0.631458
Total gradient norm: 1.097839
=== Actor Training Debug (Iteration 7654) ===
Q mean: -76.907074
Q std: 27.938253
Actor loss: 76.911072
Action reg: 0.003997
  l1.weight: grad_norm = 0.140998
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.297433
Total gradient norm: 0.575717
=== Actor Training Debug (Iteration 7655) ===
Q mean: -76.083740
Q std: 29.054340
Actor loss: 76.087730
Action reg: 0.003992
  l1.weight: grad_norm = 0.411486
  l1.bias: grad_norm = 0.001545
  l2.weight: grad_norm = 0.900452
Total gradient norm: 1.692966
=== Actor Training Debug (Iteration 7656) ===
Q mean: -75.414177
Q std: 28.903315
Actor loss: 75.418167
Action reg: 0.003990
  l1.weight: grad_norm = 0.359489
  l1.bias: grad_norm = 0.002896
  l2.weight: grad_norm = 0.709589
Total gradient norm: 1.474053
=== Actor Training Debug (Iteration 7657) ===
Q mean: -77.669800
Q std: 28.838562
Actor loss: 77.673790
Action reg: 0.003991
  l1.weight: grad_norm = 0.278595
  l1.bias: grad_norm = 0.002737
  l2.weight: grad_norm = 0.536207
Total gradient norm: 0.968020
=== Actor Training Debug (Iteration 7658) ===
Q mean: -77.558365
Q std: 29.545826
Actor loss: 77.562363
Action reg: 0.003997
  l1.weight: grad_norm = 0.062126
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.162386
Total gradient norm: 0.322324
=== Actor Training Debug (Iteration 7659) ===
Q mean: -76.131439
Q std: 28.756741
Actor loss: 76.135437
Action reg: 0.003997
  l1.weight: grad_norm = 0.153184
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.393283
Total gradient norm: 0.879491
=== Actor Training Debug (Iteration 7660) ===
Q mean: -78.112076
Q std: 30.917917
Actor loss: 78.116058
Action reg: 0.003986
  l1.weight: grad_norm = 0.193958
  l1.bias: grad_norm = 0.005205
  l2.weight: grad_norm = 0.358199
Total gradient norm: 0.674457
=== Actor Training Debug (Iteration 7661) ===
Q mean: -78.539803
Q std: 28.630133
Actor loss: 78.543800
Action reg: 0.003996
  l1.weight: grad_norm = 0.136066
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.288438
Total gradient norm: 0.552865
=== Actor Training Debug (Iteration 7662) ===
Q mean: -75.983078
Q std: 28.791519
Actor loss: 75.987061
Action reg: 0.003985
  l1.weight: grad_norm = 0.794097
  l1.bias: grad_norm = 0.001696
  l2.weight: grad_norm = 1.188889
Total gradient norm: 2.191180
=== Actor Training Debug (Iteration 7663) ===
Q mean: -75.870613
Q std: 29.842243
Actor loss: 75.874603
Action reg: 0.003990
  l1.weight: grad_norm = 0.160433
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.369468
Total gradient norm: 0.749447
=== Actor Training Debug (Iteration 7664) ===
Q mean: -77.566383
Q std: 29.826151
Actor loss: 77.570374
Action reg: 0.003991
  l1.weight: grad_norm = 0.023230
  l1.bias: grad_norm = 0.004834
  l2.weight: grad_norm = 0.050485
Total gradient norm: 0.107898
=== Actor Training Debug (Iteration 7665) ===
Q mean: -76.485306
Q std: 29.994246
Actor loss: 76.489288
Action reg: 0.003986
  l1.weight: grad_norm = 0.176565
  l1.bias: grad_norm = 0.003889
  l2.weight: grad_norm = 0.337529
Total gradient norm: 0.597700
=== Actor Training Debug (Iteration 7666) ===
Q mean: -79.531036
Q std: 28.711273
Actor loss: 79.535034
Action reg: 0.003994
  l1.weight: grad_norm = 0.117274
  l1.bias: grad_norm = 0.001633
  l2.weight: grad_norm = 0.206838
Total gradient norm: 0.379607
=== Actor Training Debug (Iteration 7667) ===
Q mean: -77.914810
Q std: 31.547491
Actor loss: 77.918800
Action reg: 0.003991
  l1.weight: grad_norm = 0.388288
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 1.004242
Total gradient norm: 1.850249
=== Actor Training Debug (Iteration 7668) ===
Q mean: -75.835495
Q std: 29.006252
Actor loss: 75.839485
Action reg: 0.003992
  l1.weight: grad_norm = 0.104442
  l1.bias: grad_norm = 0.002176
  l2.weight: grad_norm = 0.187780
Total gradient norm: 0.361033
=== Actor Training Debug (Iteration 7669) ===
Q mean: -77.075241
Q std: 28.615137
Actor loss: 77.079239
Action reg: 0.003995
  l1.weight: grad_norm = 0.352207
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.549531
Total gradient norm: 1.063460
=== Actor Training Debug (Iteration 7670) ===
Q mean: -80.816376
Q std: 30.000700
Actor loss: 80.820358
Action reg: 0.003984
  l1.weight: grad_norm = 0.302528
  l1.bias: grad_norm = 0.004500
  l2.weight: grad_norm = 0.630016
Total gradient norm: 1.150397
=== Actor Training Debug (Iteration 7671) ===
Q mean: -78.021225
Q std: 29.089928
Actor loss: 78.025215
Action reg: 0.003993
  l1.weight: grad_norm = 0.243605
  l1.bias: grad_norm = 0.002617
  l2.weight: grad_norm = 0.366055
Total gradient norm: 0.681219
=== Actor Training Debug (Iteration 7672) ===
Q mean: -73.987900
Q std: 30.132242
Actor loss: 73.991898
Action reg: 0.003996
  l1.weight: grad_norm = 0.068169
  l1.bias: grad_norm = 0.001810
  l2.weight: grad_norm = 0.175908
Total gradient norm: 0.325517
=== Actor Training Debug (Iteration 7673) ===
Q mean: -81.544624
Q std: 29.371696
Actor loss: 81.548607
Action reg: 0.003984
  l1.weight: grad_norm = 0.410693
  l1.bias: grad_norm = 0.004645
  l2.weight: grad_norm = 0.924905
Total gradient norm: 1.684438
=== Actor Training Debug (Iteration 7674) ===
Q mean: -82.860229
Q std: 29.264864
Actor loss: 82.864227
Action reg: 0.003996
  l1.weight: grad_norm = 0.072668
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.153439
Total gradient norm: 0.300315
=== Actor Training Debug (Iteration 7675) ===
Q mean: -75.306885
Q std: 29.885548
Actor loss: 75.310867
Action reg: 0.003985
  l1.weight: grad_norm = 0.290781
  l1.bias: grad_norm = 0.006897
  l2.weight: grad_norm = 0.505617
Total gradient norm: 0.871206
=== Actor Training Debug (Iteration 7676) ===
Q mean: -77.201035
Q std: 30.177906
Actor loss: 77.205032
Action reg: 0.003995
  l1.weight: grad_norm = 0.099142
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.184156
Total gradient norm: 0.355589
=== Actor Training Debug (Iteration 7677) ===
Q mean: -76.920639
Q std: 28.079397
Actor loss: 76.924629
Action reg: 0.003987
  l1.weight: grad_norm = 0.103112
  l1.bias: grad_norm = 0.003116
  l2.weight: grad_norm = 0.246416
Total gradient norm: 0.552633
=== Actor Training Debug (Iteration 7678) ===
Q mean: -77.925575
Q std: 29.639978
Actor loss: 77.929565
Action reg: 0.003988
  l1.weight: grad_norm = 0.096767
  l1.bias: grad_norm = 0.004753
  l2.weight: grad_norm = 0.191778
Total gradient norm: 0.368479
=== Actor Training Debug (Iteration 7679) ===
Q mean: -77.667152
Q std: 30.767071
Actor loss: 77.671135
Action reg: 0.003980
  l1.weight: grad_norm = 0.200733
  l1.bias: grad_norm = 0.005728
  l2.weight: grad_norm = 0.423539
Total gradient norm: 0.758186
=== Actor Training Debug (Iteration 7680) ===
Q mean: -77.789078
Q std: 30.645430
Actor loss: 77.793060
Action reg: 0.003982
  l1.weight: grad_norm = 0.573230
  l1.bias: grad_norm = 0.004377
  l2.weight: grad_norm = 1.029440
Total gradient norm: 1.989510
=== Actor Training Debug (Iteration 7681) ===
Q mean: -77.296082
Q std: 30.835325
Actor loss: 77.300072
Action reg: 0.003989
  l1.weight: grad_norm = 0.323537
  l1.bias: grad_norm = 0.003946
  l2.weight: grad_norm = 0.800275
Total gradient norm: 1.485061
=== Actor Training Debug (Iteration 7682) ===
Q mean: -79.675179
Q std: 29.253744
Actor loss: 79.679169
Action reg: 0.003987
  l1.weight: grad_norm = 0.305267
  l1.bias: grad_norm = 0.001310
  l2.weight: grad_norm = 0.459058
Total gradient norm: 0.770086
=== Actor Training Debug (Iteration 7683) ===
Q mean: -76.807289
Q std: 31.113096
Actor loss: 76.811279
Action reg: 0.003989
  l1.weight: grad_norm = 0.123388
  l1.bias: grad_norm = 0.001992
  l2.weight: grad_norm = 0.271078
Total gradient norm: 0.511970
=== Actor Training Debug (Iteration 7684) ===
Q mean: -75.132538
Q std: 32.261181
Actor loss: 75.136520
Action reg: 0.003983
  l1.weight: grad_norm = 0.985180
  l1.bias: grad_norm = 0.001688
  l2.weight: grad_norm = 1.814105
Total gradient norm: 3.426890
=== Actor Training Debug (Iteration 7685) ===
Q mean: -75.614197
Q std: 29.182400
Actor loss: 75.618187
Action reg: 0.003993
  l1.weight: grad_norm = 0.134021
  l1.bias: grad_norm = 0.001681
  l2.weight: grad_norm = 0.318585
Total gradient norm: 0.746223
=== Actor Training Debug (Iteration 7686) ===
Q mean: -79.933434
Q std: 29.943800
Actor loss: 79.937431
Action reg: 0.003995
  l1.weight: grad_norm = 0.372785
  l1.bias: grad_norm = 0.002169
  l2.weight: grad_norm = 0.717400
Total gradient norm: 1.421055
=== Actor Training Debug (Iteration 7687) ===
Q mean: -78.342072
Q std: 30.472160
Actor loss: 78.346062
Action reg: 0.003989
  l1.weight: grad_norm = 0.070868
  l1.bias: grad_norm = 0.005980
  l2.weight: grad_norm = 0.129966
Total gradient norm: 0.272255
=== Actor Training Debug (Iteration 7688) ===
Q mean: -81.792709
Q std: 29.482693
Actor loss: 81.796700
Action reg: 0.003990
  l1.weight: grad_norm = 0.217610
  l1.bias: grad_norm = 0.002054
  l2.weight: grad_norm = 0.477652
Total gradient norm: 0.881493
=== Actor Training Debug (Iteration 7689) ===
Q mean: -74.668365
Q std: 29.484678
Actor loss: 74.672348
Action reg: 0.003985
  l1.weight: grad_norm = 0.320947
  l1.bias: grad_norm = 0.001965
  l2.weight: grad_norm = 0.675649
Total gradient norm: 1.166886
=== Actor Training Debug (Iteration 7690) ===
Q mean: -76.500122
Q std: 29.772446
Actor loss: 76.504105
Action reg: 0.003982
  l1.weight: grad_norm = 0.517628
  l1.bias: grad_norm = 0.006853
  l2.weight: grad_norm = 1.138017
Total gradient norm: 2.383184
=== Actor Training Debug (Iteration 7691) ===
Q mean: -79.981522
Q std: 28.534401
Actor loss: 79.985512
Action reg: 0.003991
  l1.weight: grad_norm = 0.121010
  l1.bias: grad_norm = 0.001755
  l2.weight: grad_norm = 0.271508
Total gradient norm: 0.481169
=== Actor Training Debug (Iteration 7692) ===
Q mean: -75.436043
Q std: 29.987020
Actor loss: 75.440033
Action reg: 0.003994
  l1.weight: grad_norm = 0.126447
  l1.bias: grad_norm = 0.001275
  l2.weight: grad_norm = 0.327348
Total gradient norm: 0.671362
=== Actor Training Debug (Iteration 7693) ===
Q mean: -77.068947
Q std: 31.615740
Actor loss: 77.072937
Action reg: 0.003991
  l1.weight: grad_norm = 0.352437
  l1.bias: grad_norm = 0.002360
  l2.weight: grad_norm = 0.682068
Total gradient norm: 1.329857
=== Actor Training Debug (Iteration 7694) ===
Q mean: -79.506668
Q std: 31.840364
Actor loss: 79.510651
Action reg: 0.003983
  l1.weight: grad_norm = 0.033859
  l1.bias: grad_norm = 0.008448
  l2.weight: grad_norm = 0.082578
Total gradient norm: 0.187522
=== Actor Training Debug (Iteration 7695) ===
Q mean: -81.408020
Q std: 32.177376
Actor loss: 81.412003
Action reg: 0.003984
  l1.weight: grad_norm = 0.086849
  l1.bias: grad_norm = 0.006792
  l2.weight: grad_norm = 0.198015
Total gradient norm: 0.385851
=== Actor Training Debug (Iteration 7696) ===
Q mean: -78.901039
Q std: 28.111118
Actor loss: 78.905022
Action reg: 0.003981
  l1.weight: grad_norm = 0.488221
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.958919
Total gradient norm: 1.797667
=== Actor Training Debug (Iteration 7697) ===
Q mean: -76.591179
Q std: 29.875608
Actor loss: 76.595161
Action reg: 0.003985
  l1.weight: grad_norm = 0.082874
  l1.bias: grad_norm = 0.001377
  l2.weight: grad_norm = 0.156575
Total gradient norm: 0.266627
=== Actor Training Debug (Iteration 7698) ===
Q mean: -75.991119
Q std: 31.203478
Actor loss: 75.995087
Action reg: 0.003969
  l1.weight: grad_norm = 0.485758
  l1.bias: grad_norm = 0.008394
  l2.weight: grad_norm = 1.170543
Total gradient norm: 2.138963
=== Actor Training Debug (Iteration 7699) ===
Q mean: -78.486061
Q std: 29.723114
Actor loss: 78.490044
Action reg: 0.003982
  l1.weight: grad_norm = 0.245134
  l1.bias: grad_norm = 0.003986
  l2.weight: grad_norm = 0.475015
Total gradient norm: 0.749210
=== Actor Training Debug (Iteration 7700) ===
Q mean: -76.601028
Q std: 29.752893
Actor loss: 76.605019
Action reg: 0.003988
  l1.weight: grad_norm = 0.361253
  l1.bias: grad_norm = 0.002269
  l2.weight: grad_norm = 0.771151
Total gradient norm: 1.453807
=== Actor Training Debug (Iteration 7701) ===
Q mean: -74.450745
Q std: 28.714905
Actor loss: 74.454735
Action reg: 0.003991
  l1.weight: grad_norm = 0.148156
  l1.bias: grad_norm = 0.001689
  l2.weight: grad_norm = 0.337146
Total gradient norm: 0.678217
=== Actor Training Debug (Iteration 7702) ===
Q mean: -77.859894
Q std: 30.707891
Actor loss: 77.863876
Action reg: 0.003982
  l1.weight: grad_norm = 0.300266
  l1.bias: grad_norm = 0.004910
  l2.weight: grad_norm = 0.697076
Total gradient norm: 1.525165
=== Actor Training Debug (Iteration 7703) ===
Q mean: -79.714142
Q std: 30.313456
Actor loss: 79.718117
Action reg: 0.003977
  l1.weight: grad_norm = 0.206718
  l1.bias: grad_norm = 0.007555
  l2.weight: grad_norm = 0.442981
Total gradient norm: 0.828593
=== Actor Training Debug (Iteration 7704) ===
Q mean: -75.251839
Q std: 29.162958
Actor loss: 75.255829
Action reg: 0.003990
  l1.weight: grad_norm = 0.337134
  l1.bias: grad_norm = 0.002954
  l2.weight: grad_norm = 0.767927
Total gradient norm: 1.355886
=== Actor Training Debug (Iteration 7705) ===
Q mean: -74.595848
Q std: 28.487907
Actor loss: 74.599831
Action reg: 0.003979
  l1.weight: grad_norm = 0.577792
  l1.bias: grad_norm = 0.001357
  l2.weight: grad_norm = 1.193385
Total gradient norm: 2.401058
=== Actor Training Debug (Iteration 7706) ===
Q mean: -79.225510
Q std: 31.552853
Actor loss: 79.229492
Action reg: 0.003981
  l1.weight: grad_norm = 0.179135
  l1.bias: grad_norm = 0.005736
  l2.weight: grad_norm = 0.352647
Total gradient norm: 0.738552
=== Actor Training Debug (Iteration 7707) ===
Q mean: -78.428696
Q std: 30.347696
Actor loss: 78.432686
Action reg: 0.003989
  l1.weight: grad_norm = 0.438983
  l1.bias: grad_norm = 0.002833
  l2.weight: grad_norm = 0.936250
Total gradient norm: 1.837725
=== Actor Training Debug (Iteration 7708) ===
Q mean: -77.075310
Q std: 29.255341
Actor loss: 77.079300
Action reg: 0.003988
  l1.weight: grad_norm = 0.181574
  l1.bias: grad_norm = 0.003787
  l2.weight: grad_norm = 0.368739
Total gradient norm: 0.730763
=== Actor Training Debug (Iteration 7709) ===
Q mean: -79.045547
Q std: 29.590523
Actor loss: 79.049538
Action reg: 0.003992
  l1.weight: grad_norm = 0.073020
  l1.bias: grad_norm = 0.001746
  l2.weight: grad_norm = 0.156085
Total gradient norm: 0.349571
=== Actor Training Debug (Iteration 7710) ===
Q mean: -73.690971
Q std: 29.731365
Actor loss: 73.694962
Action reg: 0.003988
  l1.weight: grad_norm = 0.034653
  l1.bias: grad_norm = 0.004314
  l2.weight: grad_norm = 0.073252
Total gradient norm: 0.163374
=== Actor Training Debug (Iteration 7711) ===
Q mean: -77.083244
Q std: 29.764395
Actor loss: 77.087234
Action reg: 0.003989
  l1.weight: grad_norm = 0.217905
  l1.bias: grad_norm = 0.002766
  l2.weight: grad_norm = 0.397978
Total gradient norm: 0.720646
=== Actor Training Debug (Iteration 7712) ===
Q mean: -77.950806
Q std: 30.385258
Actor loss: 77.954796
Action reg: 0.003993
  l1.weight: grad_norm = 0.163557
  l1.bias: grad_norm = 0.001768
  l2.weight: grad_norm = 0.289499
Total gradient norm: 0.429704
=== Actor Training Debug (Iteration 7713) ===
Q mean: -79.160393
Q std: 30.727375
Actor loss: 79.164383
Action reg: 0.003987
  l1.weight: grad_norm = 0.097834
  l1.bias: grad_norm = 0.004935
  l2.weight: grad_norm = 0.226603
Total gradient norm: 0.411060
=== Actor Training Debug (Iteration 7714) ===
Q mean: -80.368195
Q std: 29.905857
Actor loss: 80.372185
Action reg: 0.003993
  l1.weight: grad_norm = 0.228009
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.397677
Total gradient norm: 0.656606
=== Actor Training Debug (Iteration 7715) ===
Q mean: -77.515198
Q std: 29.524773
Actor loss: 77.519188
Action reg: 0.003992
  l1.weight: grad_norm = 0.094849
  l1.bias: grad_norm = 0.002588
  l2.weight: grad_norm = 0.170829
Total gradient norm: 0.337280
=== Actor Training Debug (Iteration 7716) ===
Q mean: -79.058167
Q std: 30.165688
Actor loss: 79.062149
Action reg: 0.003979
  l1.weight: grad_norm = 0.366688
  l1.bias: grad_norm = 0.003855
  l2.weight: grad_norm = 0.969487
Total gradient norm: 2.400449
=== Actor Training Debug (Iteration 7717) ===
Q mean: -76.587723
Q std: 29.589771
Actor loss: 76.591713
Action reg: 0.003993
  l1.weight: grad_norm = 0.098461
  l1.bias: grad_norm = 0.002093
  l2.weight: grad_norm = 0.183798
Total gradient norm: 0.295533
=== Actor Training Debug (Iteration 7718) ===
Q mean: -73.697235
Q std: 30.668600
Actor loss: 73.701225
Action reg: 0.003990
  l1.weight: grad_norm = 0.280171
  l1.bias: grad_norm = 0.003292
  l2.weight: grad_norm = 0.491946
Total gradient norm: 0.885288
=== Actor Training Debug (Iteration 7719) ===
Q mean: -78.123894
Q std: 29.556446
Actor loss: 78.127884
Action reg: 0.003994
  l1.weight: grad_norm = 0.248346
  l1.bias: grad_norm = 0.000507
  l2.weight: grad_norm = 0.486336
Total gradient norm: 1.065035
=== Actor Training Debug (Iteration 7720) ===
Q mean: -74.413773
Q std: 28.659412
Actor loss: 74.417763
Action reg: 0.003988
  l1.weight: grad_norm = 0.186649
  l1.bias: grad_norm = 0.001781
  l2.weight: grad_norm = 0.343721
Total gradient norm: 0.587085
=== Actor Training Debug (Iteration 7721) ===
Q mean: -76.328415
Q std: 29.942356
Actor loss: 76.332405
Action reg: 0.003991
  l1.weight: grad_norm = 0.022100
  l1.bias: grad_norm = 0.003515
  l2.weight: grad_norm = 0.042318
Total gradient norm: 0.102579
=== Actor Training Debug (Iteration 7722) ===
Q mean: -81.865616
Q std: 29.821754
Actor loss: 81.869606
Action reg: 0.003989
  l1.weight: grad_norm = 0.154118
  l1.bias: grad_norm = 0.002014
  l2.weight: grad_norm = 0.274012
Total gradient norm: 0.562087
=== Actor Training Debug (Iteration 7723) ===
Q mean: -81.573853
Q std: 30.349936
Actor loss: 81.577835
Action reg: 0.003981
  l1.weight: grad_norm = 0.403312
  l1.bias: grad_norm = 0.007106
  l2.weight: grad_norm = 0.756415
Total gradient norm: 1.589121
=== Actor Training Debug (Iteration 7724) ===
Q mean: -80.585556
Q std: 28.789841
Actor loss: 80.589546
Action reg: 0.003990
  l1.weight: grad_norm = 0.123785
  l1.bias: grad_norm = 0.003411
  l2.weight: grad_norm = 0.215941
Total gradient norm: 0.411945
=== Actor Training Debug (Iteration 7725) ===
Q mean: -74.035622
Q std: 29.142136
Actor loss: 74.039612
Action reg: 0.003990
  l1.weight: grad_norm = 0.306257
  l1.bias: grad_norm = 0.001766
  l2.weight: grad_norm = 0.467533
Total gradient norm: 0.866585
=== Actor Training Debug (Iteration 7726) ===
Q mean: -75.942108
Q std: 29.711229
Actor loss: 75.946106
Action reg: 0.003996
  l1.weight: grad_norm = 0.030334
  l1.bias: grad_norm = 0.001205
  l2.weight: grad_norm = 0.066265
Total gradient norm: 0.115870
=== Actor Training Debug (Iteration 7727) ===
Q mean: -78.538445
Q std: 29.205168
Actor loss: 78.542435
Action reg: 0.003992
  l1.weight: grad_norm = 0.098472
  l1.bias: grad_norm = 0.003964
  l2.weight: grad_norm = 0.164925
Total gradient norm: 0.312697
=== Actor Training Debug (Iteration 7728) ===
Q mean: -75.550049
Q std: 31.060118
Actor loss: 75.554039
Action reg: 0.003989
  l1.weight: grad_norm = 0.139249
  l1.bias: grad_norm = 0.004742
  l2.weight: grad_norm = 0.261372
Total gradient norm: 0.484344
=== Actor Training Debug (Iteration 7729) ===
Q mean: -77.780624
Q std: 29.241604
Actor loss: 77.784615
Action reg: 0.003994
  l1.weight: grad_norm = 0.199638
  l1.bias: grad_norm = 0.000761
  l2.weight: grad_norm = 0.511458
Total gradient norm: 1.093831
=== Actor Training Debug (Iteration 7730) ===
Q mean: -79.237267
Q std: 29.258696
Actor loss: 79.241257
Action reg: 0.003988
  l1.weight: grad_norm = 0.344911
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.717369
Total gradient norm: 1.335405
=== Actor Training Debug (Iteration 7731) ===
Q mean: -75.698982
Q std: 29.595913
Actor loss: 75.702965
Action reg: 0.003984
  l1.weight: grad_norm = 0.361521
  l1.bias: grad_norm = 0.002847
  l2.weight: grad_norm = 0.894904
Total gradient norm: 1.750385
=== Actor Training Debug (Iteration 7732) ===
Q mean: -80.031662
Q std: 29.787430
Actor loss: 80.035652
Action reg: 0.003990
  l1.weight: grad_norm = 0.274215
  l1.bias: grad_norm = 0.003737
  l2.weight: grad_norm = 0.627855
Total gradient norm: 1.356302
=== Actor Training Debug (Iteration 7733) ===
Q mean: -82.924149
Q std: 30.494022
Actor loss: 82.928146
Action reg: 0.003994
  l1.weight: grad_norm = 0.231713
  l1.bias: grad_norm = 0.000982
  l2.weight: grad_norm = 0.476501
Total gradient norm: 0.818760
=== Actor Training Debug (Iteration 7734) ===
Q mean: -80.478897
Q std: 29.817060
Actor loss: 80.482895
Action reg: 0.003997
  l1.weight: grad_norm = 0.030111
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.052812
Total gradient norm: 0.083624
=== Actor Training Debug (Iteration 7735) ===
Q mean: -78.316010
Q std: 30.700304
Actor loss: 78.320007
Action reg: 0.003995
  l1.weight: grad_norm = 0.031542
  l1.bias: grad_norm = 0.002234
  l2.weight: grad_norm = 0.057647
Total gradient norm: 0.122995
=== Actor Training Debug (Iteration 7736) ===
Q mean: -71.702698
Q std: 29.644838
Actor loss: 71.706688
Action reg: 0.003991
  l1.weight: grad_norm = 0.162031
  l1.bias: grad_norm = 0.000787
  l2.weight: grad_norm = 0.257613
Total gradient norm: 0.376991
=== Actor Training Debug (Iteration 7737) ===
Q mean: -78.578918
Q std: 28.664162
Actor loss: 78.582916
Action reg: 0.003999
  l1.weight: grad_norm = 0.028048
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.055072
Total gradient norm: 0.109959
=== Actor Training Debug (Iteration 7738) ===
Q mean: -80.013077
Q std: 30.269150
Actor loss: 80.017067
Action reg: 0.003993
  l1.weight: grad_norm = 0.120051
  l1.bias: grad_norm = 0.003288
  l2.weight: grad_norm = 0.251217
Total gradient norm: 0.507926
=== Actor Training Debug (Iteration 7739) ===
Q mean: -81.882187
Q std: 30.523067
Actor loss: 81.886169
Action reg: 0.003979
  l1.weight: grad_norm = 0.538726
  l1.bias: grad_norm = 0.006963
  l2.weight: grad_norm = 0.843714
Total gradient norm: 1.572705
=== Actor Training Debug (Iteration 7740) ===
Q mean: -79.433678
Q std: 29.154902
Actor loss: 79.437668
Action reg: 0.003989
  l1.weight: grad_norm = 0.328089
  l1.bias: grad_norm = 0.001310
  l2.weight: grad_norm = 0.598347
Total gradient norm: 1.117059
=== Actor Training Debug (Iteration 7741) ===
Q mean: -75.549614
Q std: 28.636900
Actor loss: 75.553604
Action reg: 0.003988
  l1.weight: grad_norm = 0.512914
  l1.bias: grad_norm = 0.001445
  l2.weight: grad_norm = 1.198000
Total gradient norm: 2.217534
=== Actor Training Debug (Iteration 7742) ===
Q mean: -77.034561
Q std: 30.391220
Actor loss: 77.038544
Action reg: 0.003984
  l1.weight: grad_norm = 0.263072
  l1.bias: grad_norm = 0.005083
  l2.weight: grad_norm = 0.462047
Total gradient norm: 0.750211
=== Actor Training Debug (Iteration 7743) ===
Q mean: -81.411537
Q std: 29.179232
Actor loss: 81.415527
Action reg: 0.003992
  l1.weight: grad_norm = 0.201800
  l1.bias: grad_norm = 0.002068
  l2.weight: grad_norm = 0.364497
Total gradient norm: 0.613256
=== Actor Training Debug (Iteration 7744) ===
Q mean: -79.637321
Q std: 31.408627
Actor loss: 79.641319
Action reg: 0.003995
  l1.weight: grad_norm = 0.216944
  l1.bias: grad_norm = 0.002298
  l2.weight: grad_norm = 0.438135
Total gradient norm: 0.821895
=== Actor Training Debug (Iteration 7745) ===
Q mean: -76.437485
Q std: 30.885693
Actor loss: 76.441475
Action reg: 0.003992
  l1.weight: grad_norm = 0.088756
  l1.bias: grad_norm = 0.001297
  l2.weight: grad_norm = 0.213140
Total gradient norm: 0.418343
=== Actor Training Debug (Iteration 7746) ===
Q mean: -75.073593
Q std: 28.293158
Actor loss: 75.077583
Action reg: 0.003989
  l1.weight: grad_norm = 0.224010
  l1.bias: grad_norm = 0.001730
  l2.weight: grad_norm = 0.420376
Total gradient norm: 0.753197
=== Actor Training Debug (Iteration 7747) ===
Q mean: -79.030121
Q std: 29.977583
Actor loss: 79.034103
Action reg: 0.003982
  l1.weight: grad_norm = 0.666545
  l1.bias: grad_norm = 0.004717
  l2.weight: grad_norm = 1.462151
Total gradient norm: 3.261665
=== Actor Training Debug (Iteration 7748) ===
Q mean: -79.367111
Q std: 29.336386
Actor loss: 79.371109
Action reg: 0.003997
  l1.weight: grad_norm = 0.055870
  l1.bias: grad_norm = 0.001682
  l2.weight: grad_norm = 0.101040
Total gradient norm: 0.188797
=== Actor Training Debug (Iteration 7749) ===
Q mean: -74.734222
Q std: 29.583529
Actor loss: 74.738213
Action reg: 0.003991
  l1.weight: grad_norm = 0.172967
  l1.bias: grad_norm = 0.002348
  l2.weight: grad_norm = 0.345493
Total gradient norm: 0.618675
=== Actor Training Debug (Iteration 7750) ===
Q mean: -80.377441
Q std: 30.421072
Actor loss: 80.381424
Action reg: 0.003985
  l1.weight: grad_norm = 0.107911
  l1.bias: grad_norm = 0.005241
  l2.weight: grad_norm = 0.213170
Total gradient norm: 0.461308
=== Actor Training Debug (Iteration 7751) ===
Q mean: -82.553146
Q std: 28.190903
Actor loss: 82.557137
Action reg: 0.003992
  l1.weight: grad_norm = 0.288082
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.565871
Total gradient norm: 1.088603
=== Actor Training Debug (Iteration 7752) ===
Q mean: -77.354294
Q std: 28.047850
Actor loss: 77.358284
Action reg: 0.003993
  l1.weight: grad_norm = 0.319056
  l1.bias: grad_norm = 0.001306
  l2.weight: grad_norm = 0.754473
Total gradient norm: 1.896202
=== Actor Training Debug (Iteration 7753) ===
Q mean: -75.503624
Q std: 31.136250
Actor loss: 75.507614
Action reg: 0.003992
  l1.weight: grad_norm = 0.029529
  l1.bias: grad_norm = 0.003254
  l2.weight: grad_norm = 0.058537
Total gradient norm: 0.124901
=== Actor Training Debug (Iteration 7754) ===
Q mean: -74.538857
Q std: 29.621878
Actor loss: 74.542847
Action reg: 0.003987
  l1.weight: grad_norm = 0.187365
  l1.bias: grad_norm = 0.001198
  l2.weight: grad_norm = 0.358053
Total gradient norm: 0.669288
=== Actor Training Debug (Iteration 7755) ===
Q mean: -79.747276
Q std: 29.074755
Actor loss: 79.751266
Action reg: 0.003990
  l1.weight: grad_norm = 0.135755
  l1.bias: grad_norm = 0.003395
  l2.weight: grad_norm = 0.270330
Total gradient norm: 0.492429
=== Actor Training Debug (Iteration 7756) ===
Q mean: -79.638718
Q std: 29.741585
Actor loss: 79.642708
Action reg: 0.003992
  l1.weight: grad_norm = 0.193797
  l1.weight: grad_norm = 0.090183on 6486) ===
  l1.bias: grad_norm = 0.004058
  l2.weight: grad_norm = 0.163060
Total gradient norm: 0.340032
=== Actor Training Debug (Iteration 7767) ===
Q mean: -77.056122
Q std: 29.718958
Actor loss: 77.060112
Action reg: 0.003987
  l1.weight: grad_norm = 0.504911
  l1.bias: grad_norm = 0.003805
  l2.weight: grad_norm = 0.965429
Total gradient norm: 2.117398
=== Actor Training Debug (Iteration 7768) ===
Q mean: -73.574936
Q std: 28.508547
Actor loss: 73.578918
Action reg: 0.003986
  l1.weight: grad_norm = 0.313391
  l1.bias: grad_norm = 0.004520
  l2.weight: grad_norm = 0.640245
Total gradient norm: 1.261850
=== Actor Training Debug (Iteration 7769) ===
Q mean: -76.784073
Q std: 31.122143
Actor loss: 76.788071
Action reg: 0.003995
  l1.weight: grad_norm = 0.012565
  l1.bias: grad_norm = 0.001761
  l2.weight: grad_norm = 0.026721
Total gradient norm: 0.056238
=== Actor Training Debug (Iteration 7770) ===
Q mean: -79.708412
Q std: 32.297287
Actor loss: 79.712402
Action reg: 0.003990
  l1.weight: grad_norm = 0.120397
  l1.bias: grad_norm = 0.004270
  l2.weight: grad_norm = 0.282878
Total gradient norm: 0.603164
=== Actor Training Debug (Iteration 7771) ===
Q mean: -82.611916
Q std: 31.102623
Actor loss: 82.615913
Action reg: 0.003994
  l1.weight: grad_norm = 0.065363
  l1.bias: grad_norm = 0.002465
  l2.weight: grad_norm = 0.120637
Total gradient norm: 0.234623
=== Actor Training Debug (Iteration 7772) ===
Q mean: -79.824875
Q std: 30.456205
Actor loss: 79.828865
Action reg: 0.003993
  l1.weight: grad_norm = 0.102699
  l1.bias: grad_norm = 0.002342
  l2.weight: grad_norm = 0.174972
Total gradient norm: 0.319584
=== Actor Training Debug (Iteration 7773) ===
Q mean: -79.683006
Q std: 30.553493
Actor loss: 79.687004
Action reg: 0.003995
  l1.weight: grad_norm = 0.013335
  l1.bias: grad_norm = 0.002807
  l2.weight: grad_norm = 0.025256
Total gradient norm: 0.049458
=== Actor Training Debug (Iteration 7774) ===
Q mean: -74.444664
Q std: 27.960436
Actor loss: 74.448662
Action reg: 0.003995
  l1.weight: grad_norm = 0.134164
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.269843
Total gradient norm: 0.642617
=== Actor Training Debug (Iteration 7775) ===
Q mean: -79.767273
Q std: 28.774485
Actor loss: 79.771263
Action reg: 0.003986
  l1.weight: grad_norm = 0.317977
  l1.bias: grad_norm = 0.002045
  l2.weight: grad_norm = 0.637713
Total gradient norm: 1.158752
=== Actor Training Debug (Iteration 7776) ===
Q mean: -75.061485
Q std: 28.432798
Actor loss: 75.065483
Action reg: 0.003996
  l1.weight: grad_norm = 0.176878
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.370958
Total gradient norm: 0.760269
=== Actor Training Debug (Iteration 7777) ===
Q mean: -77.745644
Q std: 30.051899
Actor loss: 77.749634
Action reg: 0.003991
  l1.weight: grad_norm = 0.181136
  l1.bias: grad_norm = 0.000642
  l2.weight: grad_norm = 0.349032
Total gradient norm: 0.649892
=== Actor Training Debug (Iteration 7778) ===
Q mean: -78.896820
Q std: 30.851423
Actor loss: 78.900818
Action reg: 0.003998
  l1.weight: grad_norm = 0.174805
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.291437
Total gradient norm: 0.562807
=== Actor Training Debug (Iteration 7779) ===
Q mean: -77.080605
Q std: 29.549551
Actor loss: 77.084595
Action reg: 0.003993
  l1.weight: grad_norm = 0.078277
  l1.bias: grad_norm = 0.001280
  l2.weight: grad_norm = 0.181781
Total gradient norm: 0.390947
=== Actor Training Debug (Iteration 7780) ===
Q mean: -79.614670
Q std: 30.353653
Actor loss: 79.618660
Action reg: 0.003989
  l1.weight: grad_norm = 0.359440
  l1.bias: grad_norm = 0.002212
  l2.weight: grad_norm = 0.682884
Total gradient norm: 1.449329
=== Actor Training Debug (Iteration 7781) ===
Q mean: -78.642166
Q std: 28.104242
Actor loss: 78.646156
Action reg: 0.003992
  l1.weight: grad_norm = 0.494678
  l1.bias: grad_norm = 0.001769
  l2.weight: grad_norm = 0.984095
Total gradient norm: 1.906663
=== Actor Training Debug (Iteration 7782) ===
Q mean: -79.039612
Q std: 31.062462
Actor loss: 79.043602
Action reg: 0.003990
  l1.weight: grad_norm = 0.062679
  l1.bias: grad_norm = 0.003515
  l2.weight: grad_norm = 0.137240
Total gradient norm: 0.276059
=== Actor Training Debug (Iteration 7783) ===
Q mean: -77.081879
Q std: 29.491066
Actor loss: 77.085869
Action reg: 0.003990
  l1.weight: grad_norm = 0.461996
  l1.bias: grad_norm = 0.001634
  l2.weight: grad_norm = 0.911221
Total gradient norm: 1.994534
=== Actor Training Debug (Iteration 7784) ===
Q mean: -74.253410
Q std: 27.847452
Actor loss: 74.257401
Action reg: 0.003990
  l1.weight: grad_norm = 0.204294
  l1.bias: grad_norm = 0.001908
  l2.weight: grad_norm = 0.375458
Total gradient norm: 0.809364
=== Actor Training Debug (Iteration 7785) ===
Q mean: -76.016571
Q std: 29.547943
Actor loss: 76.020569
Action reg: 0.003996
  l1.weight: grad_norm = 0.274470
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.526253
Total gradient norm: 0.853709
=== Actor Training Debug (Iteration 7786) ===
Q mean: -80.413132
Q std: 29.094028
Actor loss: 80.417122
Action reg: 0.003989
  l1.weight: grad_norm = 0.800460
  l1.bias: grad_norm = 0.000789
  l2.weight: grad_norm = 1.702664
Total gradient norm: 3.251525
=== Actor Training Debug (Iteration 7787) ===
Q mean: -79.066132
Q std: 29.276850
Actor loss: 79.070122
Action reg: 0.003993
  l1.weight: grad_norm = 0.111090
  l1.bias: grad_norm = 0.002223
  l2.weight: grad_norm = 0.209655
Total gradient norm: 0.347944
=== Actor Training Debug (Iteration 7788) ===
Q mean: -75.717819
Q std: 29.876272
Actor loss: 75.721809
Action reg: 0.003992
  l1.weight: grad_norm = 0.151245
  l1.bias: grad_norm = 0.002531
  l2.weight: grad_norm = 0.291581
Total gradient norm: 0.596937
=== Actor Training Debug (Iteration 7789) ===
Q mean: -77.166367
Q std: 30.718409
Actor loss: 77.170357
Action reg: 0.003993
  l1.weight: grad_norm = 0.161217
  l1.bias: grad_norm = 0.002090
  l2.weight: grad_norm = 0.351326
Total gradient norm: 0.908873
=== Actor Training Debug (Iteration 7790) ===
Q mean: -74.852592
Q std: 29.552959
Actor loss: 74.856583
Action reg: 0.003992
  l1.weight: grad_norm = 0.166161
  l1.bias: grad_norm = 0.002294
  l2.weight: grad_norm = 0.303930
Total gradient norm: 0.577373
=== Actor Training Debug (Iteration 7791) ===
Q mean: -78.062317
Q std: 29.712915
Actor loss: 78.066307
Action reg: 0.003989
  l1.weight: grad_norm = 0.126849
  l1.bias: grad_norm = 0.001252
  l2.weight: grad_norm = 0.319649
Total gradient norm: 0.683707
=== Actor Training Debug (Iteration 7792) ===
Q mean: -78.961922
Q std: 30.304514
Actor loss: 78.965912
Action reg: 0.003986
  l1.weight: grad_norm = 0.076306
  l1.bias: grad_norm = 0.002702
  l2.weight: grad_norm = 0.145496
Total gradient norm: 0.310091
=== Actor Training Debug (Iteration 7793) ===
Q mean: -75.500061
Q std: 29.676264
Actor loss: 75.504051
Action reg: 0.003987
  l1.weight: grad_norm = 0.270145
  l1.bias: grad_norm = 0.001344
  l2.weight: grad_norm = 0.545953
Total gradient norm: 1.051579
=== Actor Training Debug (Iteration 7794) ===
Q mean: -78.583008
Q std: 29.232580
Actor loss: 78.587006
Action reg: 0.003996
  l1.weight: grad_norm = 0.030102
  l1.bias: grad_norm = 0.001060
  l2.weight: grad_norm = 0.071670
Total gradient norm: 0.130192
=== Actor Training Debug (Iteration 7795) ===
Q mean: -78.609596
Q std: 28.785107
Actor loss: 78.613594
Action reg: 0.003997
  l1.weight: grad_norm = 0.156521
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.240625
Total gradient norm: 0.449297
=== Actor Training Debug (Iteration 7796) ===
Q mean: -78.601288
Q std: 29.944780
Actor loss: 78.605278
Action reg: 0.003989
  l1.weight: grad_norm = 0.156387
  l1.bias: grad_norm = 0.001246
  l2.weight: grad_norm = 0.303889
Total gradient norm: 0.520320
=== Actor Training Debug (Iteration 7797) ===
Q mean: -79.035751
Q std: 28.724916
Actor loss: 79.039742
Action reg: 0.003991
  l1.weight: grad_norm = 0.225628
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.563022
Total gradient norm: 1.058316
=== Actor Training Debug (Iteration 7798) ===
Q mean: -80.423904
Q std: 29.802927
Actor loss: 80.427895
Action reg: 0.003989
  l1.weight: grad_norm = 0.382095
  l1.bias: grad_norm = 0.003030
  l2.weight: grad_norm = 0.786806
Total gradient norm: 1.530843
=== Actor Training Debug (Iteration 7799) ===
Q mean: -77.895721
Q std: 30.291449
Actor loss: 77.899712
Action reg: 0.003994
  l1.weight: grad_norm = 0.082974
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.150560
Total gradient norm: 0.265152
=== Actor Training Debug (Iteration 7800) ===
Q mean: -79.247223
Q std: 29.372795
Actor loss: 79.251213
Action reg: 0.003992
  l1.weight: grad_norm = 0.160834
  l1.bias: grad_norm = 0.000378
  l2.weight: grad_norm = 0.348878
Total gradient norm: 0.663955
=== Actor Training Debug (Iteration 7801) ===
Q mean: -76.421577
Q std: 30.874662
Actor loss: 76.425560
Action reg: 0.003986
  l1.weight: grad_norm = 0.228515
  l1.bias: grad_norm = 0.003755
  l2.weight: grad_norm = 0.434442
Total gradient norm: 0.819840
=== Actor Training Debug (Iteration 7802) ===
Q mean: -76.801468
Q std: 30.801235
Actor loss: 76.805458
Action reg: 0.003989
  l1.weight: grad_norm = 0.224109
  l1.bias: grad_norm = 0.002497
  l2.weight: grad_norm = 0.427345
Total gradient norm: 0.778099
=== Actor Training Debug (Iteration 7803) ===
Q mean: -79.196083
Q std: 30.516401
Actor loss: 79.200066
Action reg: 0.003985
  l1.weight: grad_norm = 0.158471
  l1.bias: grad_norm = 0.001498
  l2.weight: grad_norm = 0.340753
Total gradient norm: 0.589256
=== Actor Training Debug (Iteration 7804) ===
Q mean: -78.950745
Q std: 28.942080
Actor loss: 78.954742
Action reg: 0.003996
  l1.weight: grad_norm = 0.160573
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.322594
Total gradient norm: 0.531323
=== Actor Training Debug (Iteration 7805) ===
Q mean: -77.539139
Q std: 30.245949
Actor loss: 77.543121
Action reg: 0.003979
  l1.weight: grad_norm = 0.377312
  l1.bias: grad_norm = 0.004410
  l2.weight: grad_norm = 0.785868
Total gradient norm: 1.584100
=== Actor Training Debug (Iteration 7806) ===
Q mean: -79.071365
Q std: 30.624937
Actor loss: 79.075356
Action reg: 0.003987
  l1.weight: grad_norm = 0.152177
  l1.bias: grad_norm = 0.002166
  l2.weight: grad_norm = 0.290832
Total gradient norm: 0.589931
=== Actor Training Debug (Iteration 7807) ===
Q mean: -76.582504
Q std: 31.455881
Actor loss: 76.586487
Action reg: 0.003985
  l1.weight: grad_norm = 0.186800
  l1.bias: grad_norm = 0.004325
  l2.weight: grad_norm = 0.355622
Total gradient norm: 0.654829
=== Actor Training Debug (Iteration 7808) ===
Q mean: -75.180649
Q std: 29.212818
Actor loss: 75.184639
Action reg: 0.003987
  l1.weight: grad_norm = 1.043821
  l1.bias: grad_norm = 0.002349
  l2.weight: grad_norm = 2.018697
Total gradient norm: 3.939513
=== Actor Training Debug (Iteration 7809) ===
Q mean: -75.395363
Q std: 28.882208
Actor loss: 75.399345
Action reg: 0.003984
  l1.weight: grad_norm = 0.507130
  l1.bias: grad_norm = 0.001782
  l2.weight: grad_norm = 1.025775
Total gradient norm: 2.037620
=== Actor Training Debug (Iteration 7810) ===
Q mean: -77.463196
Q std: 29.746010
Actor loss: 77.467186
Action reg: 0.003991
  l1.weight: grad_norm = 0.059359
  l1.bias: grad_norm = 0.001785
  l2.weight: grad_norm = 0.116067
Total gradient norm: 0.174657
=== Actor Training Debug (Iteration 7811) ===
Q mean: -81.324020
Q std: 31.067833
Actor loss: 81.328018
Action reg: 0.003995
  l1.weight: grad_norm = 0.075590
  l1.bias: grad_norm = 0.000773
  l2.weight: grad_norm = 0.169903
Total gradient norm: 0.295783
=== Actor Training Debug (Iteration 7812) ===
Q mean: -78.800438
Q std: 30.158133
Actor loss: 78.804428
Action reg: 0.003994
  l1.weight: grad_norm = 0.258528
  l1.bias: grad_norm = 0.000638
  l2.weight: grad_norm = 0.583503
Total gradient norm: 1.331889
=== Actor Training Debug (Iteration 7813) ===
Q mean: -81.722595
Q std: 29.652222
Actor loss: 81.726578
Action reg: 0.003986
  l1.weight: grad_norm = 0.821412
  l1.bias: grad_norm = 0.001112
  l2.weight: grad_norm = 1.852940
Total gradient norm: 4.153572
=== Actor Training Debug (Iteration 7814) ===
Q mean: -79.862663
Q std: 30.357599
Actor loss: 79.866661
Action reg: 0.003996
  l1.weight: grad_norm = 0.050113
  l1.bias: grad_norm = 0.001421
  l2.weight: grad_norm = 0.101901
Total gradient norm: 0.244536
=== Actor Training Debug (Iteration 7815) ===
Q mean: -80.587494
Q std: 29.035330
Actor loss: 80.591484
Action reg: 0.003992
  l1.weight: grad_norm = 0.026031
  l1.bias: grad_norm = 0.001358
  l2.weight: grad_norm = 0.051585
Total gradient norm: 0.103972
=== Actor Training Debug (Iteration 7816) ===
Q mean: -77.492340
Q std: 30.311462
Actor loss: 77.496330
Action reg: 0.003990
  l1.weight: grad_norm = 0.161387
  l1.bias: grad_norm = 0.001781
  l2.weight: grad_norm = 0.308351
Total gradient norm: 0.525165
=== Actor Training Debug (Iteration 7817) ===
Q mean: -79.332321
Q std: 30.043745
Actor loss: 79.336319
Action reg: 0.003997
  l1.weight: grad_norm = 0.292641
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.574518
Total gradient norm: 1.104114
=== Actor Training Debug (Iteration 7818) ===
Q mean: -79.576088
Q std: 29.683626
Actor loss: 79.580078
Action reg: 0.003992
  l1.weight: grad_norm = 0.162715
  l1.bias: grad_norm = 0.001768
  l2.weight: grad_norm = 0.356200
Total gradient norm: 0.710274
=== Actor Training Debug (Iteration 7819) ===
Q mean: -76.847183
Q std: 30.091753
Actor loss: 76.851173
Action reg: 0.003991
  l1.weight: grad_norm = 0.400589
  l1.bias: grad_norm = 0.002143
  l2.weight: grad_norm = 0.728222
Total gradient norm: 1.365562
=== Actor Training Debug (Iteration 7820) ===
Q mean: -81.440125
Q std: 30.590590
Actor loss: 81.444115
Action reg: 0.003990
  l1.weight: grad_norm = 0.476429
  l1.bias: grad_norm = 0.001563
  l2.weight: grad_norm = 1.072902
Total gradient norm: 2.284077
=== Actor Training Debug (Iteration 7821) ===
Q mean: -77.563370
Q std: 28.930113
Actor loss: 77.567360
Action reg: 0.003992
  l1.weight: grad_norm = 0.256529
  l1.bias: grad_norm = 0.000999
  l2.weight: grad_norm = 0.588971
Total gradient norm: 1.204512
=== Actor Training Debug (Iteration 7822) ===
Q mean: -75.607246
Q std: 29.793432
Actor loss: 75.611237
Action reg: 0.003991
  l1.weight: grad_norm = 0.086492
  l1.bias: grad_norm = 0.002224
  l2.weight: grad_norm = 0.200378
Total gradient norm: 0.409898
=== Actor Training Debug (Iteration 7823) ===
Q mean: -77.212494
Q std: 29.907053
Actor loss: 77.216484
Action reg: 0.003990
  l1.weight: grad_norm = 0.148518
  l1.bias: grad_norm = 0.004814
  l2.weight: grad_norm = 0.337664
Total gradient norm: 0.694481
=== Actor Training Debug (Iteration 7824) ===
Q mean: -77.251892
Q std: 29.698465
Actor loss: 77.255890
Action reg: 0.003996
  l1.weight: grad_norm = 0.904846
  l1.bias: grad_norm = 0.000902
  l2.weight: grad_norm = 1.977653
Total gradient norm: 4.195785
=== Actor Training Debug (Iteration 7825) ===
Q mean: -76.730255
Q std: 30.972061
Actor loss: 76.734245
Action reg: 0.003992
  l1.weight: grad_norm = 0.019624
  l1.bias: grad_norm = 0.003049
  l2.weight: grad_norm = 0.052587
Total gradient norm: 0.128484
=== Actor Training Debug (Iteration 7826) ===
Q mean: -79.364594
Q std: 30.023586
Actor loss: 79.368584
Action reg: 0.003994
  l1.weight: grad_norm = 0.166478
  l1.bias: grad_norm = 0.001465
  l2.weight: grad_norm = 0.389456
Total gradient norm: 0.712163
=== Actor Training Debug (Iteration 7827) ===
Q mean: -80.976395
Q std: 30.367498
Actor loss: 80.980377
Action reg: 0.003983
  l1.weight: grad_norm = 0.125403
  l1.bias: grad_norm = 0.006188
  l2.weight: grad_norm = 0.283286
Total gradient norm: 0.659026
=== Actor Training Debug (Iteration 7828) ===
Q mean: -78.692169
Q std: 29.624283
Actor loss: 78.696159
Action reg: 0.003993
  l1.weight: grad_norm = 0.181591
  l1.bias: grad_norm = 0.001230
  l2.weight: grad_norm = 0.333509
Total gradient norm: 0.515066
=== Actor Training Debug (Iteration 7829) ===
Q mean: -77.141548
Q std: 30.489420
Actor loss: 77.145538
Action reg: 0.003991
  l1.weight: grad_norm = 0.039085
  l1.bias: grad_norm = 0.002968
  l2.weight: grad_norm = 0.076906
Total gradient norm: 0.144658
=== Actor Training Debug (Iteration 7830) ===
Q mean: -80.410004
Q std: 29.720396
Actor loss: 80.414001
Action reg: 0.003994
  l1.weight: grad_norm = 0.174192
  l1.bias: grad_norm = 0.000667
  l2.weight: grad_norm = 0.392792
Total gradient norm: 0.781341
=== Actor Training Debug (Iteration 7831) ===
Q mean: -77.639336
Q std: 31.478058
Actor loss: 77.643326
Action reg: 0.003988
  l1.weight: grad_norm = 0.177308
  l1.bias: grad_norm = 0.000624
  l2.weight: grad_norm = 0.409600
Total gradient norm: 0.725015
=== Actor Training Debug (Iteration 7832) ===
Q mean: -78.527664
Q std: 29.898911
Actor loss: 78.531654
Action reg: 0.003992
  l1.weight: grad_norm = 0.296588
  l1.bias: grad_norm = 0.003133
  l2.weight: grad_norm = 0.725729
Total gradient norm: 1.241674
=== Actor Training Debug (Iteration 7833) ===
Q mean: -80.048935
Q std: 30.245892
Actor loss: 80.052917
Action reg: 0.003986
  l1.weight: grad_norm = 0.286278
  l1.bias: grad_norm = 0.003331
  l2.weight: grad_norm = 0.495874
Total gradient norm: 1.073133
=== Actor Training Debug (Iteration 7834) ===
Q mean: -77.166687
Q std: 31.586742
Actor loss: 77.170670
Action reg: 0.003985
  l1.weight: grad_norm = 0.750471
  l1.bias: grad_norm = 0.003164
  l2.weight: grad_norm = 1.408271
Total gradient norm: 2.635540
=== Actor Training Debug (Iteration 7835) ===
Q mean: -76.821724
Q std: 28.773558
Actor loss: 76.825722
Action reg: 0.003996
  l1.weight: grad_norm = 0.067697
  l1.bias: grad_norm = 0.001026
  l2.weight: grad_norm = 0.122514
Total gradient norm: 0.247438
=== Actor Training Debug (Iteration 7836) ===
Q mean: -76.571823
Q std: 29.370754
Actor loss: 76.575821
Action reg: 0.003998
  l1.weight: grad_norm = 0.237655
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.439600
Total gradient norm: 0.762020
=== Actor Training Debug (Iteration 7837) ===
Q mean: -77.772591
Q std: 30.017958
Actor loss: 77.776581
Action reg: 0.003992
  l1.weight: grad_norm = 0.261151
  l1.bias: grad_norm = 0.001704
  l2.weight: grad_norm = 0.490775
Total gradient norm: 0.859606
=== Actor Training Debug (Iteration 7838) ===
Q mean: -83.029282
Q std: 31.555435
Actor loss: 83.033272
Action reg: 0.003993
  l1.weight: grad_norm = 0.171525
  l1.bias: grad_norm = 0.001104
  l2.weight: grad_norm = 0.398302
Total gradient norm: 0.765168
=== Actor Training Debug (Iteration 7839) ===
Q mean: -79.802528
Q std: 31.605560
Actor loss: 79.806519
Action reg: 0.003992
  l1.weight: grad_norm = 0.079239
  l1.bias: grad_norm = 0.003082
  l2.weight: grad_norm = 0.126986
Total gradient norm: 0.288201
=== Actor Training Debug (Iteration 7840) ===
Q mean: -78.945618
Q std: 28.993807
Actor loss: 78.949615
Action reg: 0.003996
  l1.weight: grad_norm = 0.359304
  l1.bias: grad_norm = 0.000619
  l2.weight: grad_norm = 0.787172
Total gradient norm: 1.586431
=== Actor Training Debug (Iteration 7841) ===
Q mean: -78.498970
Q std: 30.445377
Actor loss: 78.502953
Action reg: 0.003986
  l1.weight: grad_norm = 0.117442
  l1.bias: grad_norm = 0.008445
  l2.weight: grad_norm = 0.201438
Total gradient norm: 0.343214
=== Actor Training Debug (Iteration 7842) ===
Q mean: -80.397293
Q std: 30.057673
Actor loss: 80.401276
Action reg: 0.003984
  l1.weight: grad_norm = 0.526678
  l1.bias: grad_norm = 0.003165
  l2.weight: grad_norm = 0.963746
Total gradient norm: 1.697698
=== Actor Training Debug (Iteration 7843) ===
Q mean: -80.902077
Q std: 29.633507
Actor loss: 80.906059
Action reg: 0.003986
  l1.weight: grad_norm = 0.534957
  l1.bias: grad_norm = 0.001437
  l2.weight: grad_norm = 1.388848
Total gradient norm: 2.905589
=== Actor Training Debug (Iteration 7844) ===
Q mean: -76.099487
Q std: 28.947165
Actor loss: 76.103477
Action reg: 0.003992
  l1.weight: grad_norm = 0.089732
  l1.bias: grad_norm = 0.001300
  l2.weight: grad_norm = 0.169168
Total gradient norm: 0.278981
=== Actor Training Debug (Iteration 7845) ===
Q mean: -81.008247
Q std: 32.254791
Actor loss: 81.012230
Action reg: 0.003985
  l1.weight: grad_norm = 0.267417
  l1.bias: grad_norm = 0.004630
  l2.weight: grad_norm = 0.453227
Total gradient norm: 0.661514
=== Actor Training Debug (Iteration 7846) ===
Q mean: -77.279434
Q std: 30.855881
Actor loss: 77.283417
Action reg: 0.003985
  l1.weight: grad_norm = 0.346516
  l1.bias: grad_norm = 0.004881
  l2.weight: grad_norm = 0.628007
Total gradient norm: 1.045328
=== Actor Training Debug (Iteration 7847) ===
Q mean: -82.999191
Q std: 30.137609
Actor loss: 83.003181
Action reg: 0.003992
  l1.weight: grad_norm = 0.093831
  l1.bias: grad_norm = 0.001262
  l2.weight: grad_norm = 0.226460
Total gradient norm: 0.491181
=== Actor Training Debug (Iteration 7848) ===
Q mean: -79.520439
Q std: 30.140244
Actor loss: 79.524429
Action reg: 0.003993
  l1.weight: grad_norm = 0.065772
  l1.bias: grad_norm = 0.002621
  l2.weight: grad_norm = 0.172472
Total gradient norm: 0.349489
=== Actor Training Debug (Iteration 7849) ===
Q mean: -78.621246
Q std: 29.557844
Actor loss: 78.625244
Action reg: 0.003995
  l1.weight: grad_norm = 0.101753
  l1.bias: grad_norm = 0.003129
  l2.weight: grad_norm = 0.173078
Total gradient norm: 0.381371
=== Actor Training Debug (Iteration 7850) ===
Q mean: -78.676987
Q std: 28.678186
Actor loss: 78.680977
Action reg: 0.003994
  l1.weight: grad_norm = 0.192945
  l1.bias: grad_norm = 0.000615
  l2.weight: grad_norm = 0.483753
Total gradient norm: 1.016565
=== Actor Training Debug (Iteration 7851) ===
Q mean: -81.819458
Q std: 29.695005
Actor loss: 81.823448
Action reg: 0.003987
  l1.weight: grad_norm = 0.727274
  l1.bias: grad_norm = 0.002559
  l2.weight: grad_norm = 1.710808
Total gradient norm: 3.262453
=== Actor Training Debug (Iteration 7852) ===
Q mean: -78.169579
Q std: 29.670132
Actor loss: 78.173576
Action reg: 0.003995
  l1.weight: grad_norm = 0.080007
  l1.bias: grad_norm = 0.000793
  l2.weight: grad_norm = 0.155311
Total gradient norm: 0.352645
=== Actor Training Debug (Iteration 7853) ===
Q mean: -76.589355
Q std: 29.105438
Actor loss: 76.593346
Action reg: 0.003991
  l1.weight: grad_norm = 0.276804
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.546842
Total gradient norm: 1.181700
=== Actor Training Debug (Iteration 7854) ===
Q mean: -79.574074
Q std: 29.540855
Actor loss: 79.578072
Action reg: 0.003996
  l1.weight: grad_norm = 0.010757
  l1.bias: grad_norm = 0.001967
  l2.weight: grad_norm = 0.025947
Total gradient norm: 0.062578
=== Actor Training Debug (Iteration 7855) ===
Q mean: -75.655991
Q std: 30.202946
Actor loss: 75.659981
Action reg: 0.003989
  l1.weight: grad_norm = 0.529017
  l1.bias: grad_norm = 0.000562
  l2.weight: grad_norm = 1.157863
Total gradient norm: 2.725825
=== Actor Training Debug (Iteration 7856) ===
Q mean: -76.847130
Q std: 29.785227
Actor loss: 76.851120
Action reg: 0.003991
  l1.weight: grad_norm = 0.292601
  l1.bias: grad_norm = 0.003317
  l2.weight: grad_norm = 0.627673
Total gradient norm: 1.133788
=== Actor Training Debug (Iteration 7857) ===
Q mean: -78.344559
Q std: 29.443869
Actor loss: 78.348549
Action reg: 0.003990
  l1.weight: grad_norm = 0.445055
  l1.bias: grad_norm = 0.002127
  l2.weight: grad_norm = 1.003370
Total gradient norm: 1.715595
=== Actor Training Debug (Iteration 7858) ===
Q mean: -76.825768
Q std: 30.652094
Actor loss: 76.829750
Action reg: 0.003985
  l1.weight: grad_norm = 0.402354
  l1.bias: grad_norm = 0.003557
  l2.weight: grad_norm = 0.742286
Total gradient norm: 1.479113
=== Actor Training Debug (Iteration 7859) ===
Q mean: -76.956985
Q std: 30.271049
Actor loss: 76.960983
Action reg: 0.003995
  l1.weight: grad_norm = 0.043529
  l1.bias: grad_norm = 0.001941
  l2.weight: grad_norm = 0.099210
Total gradient norm: 0.188648
=== Actor Training Debug (Iteration 7860) ===
Q mean: -82.749954
Q std: 29.672922
Actor loss: 82.753937
Action reg: 0.003985
  l1.weight: grad_norm = 0.303667
  l1.bias: grad_norm = 0.001694
  l2.weight: grad_norm = 0.623790
Total gradient norm: 1.283970
=== Actor Training Debug (Iteration 7861) ===
Q mean: -77.103569
Q std: 28.699923
Actor loss: 77.107559
Action reg: 0.003986
  l1.weight: grad_norm = 0.137529
  l1.bias: grad_norm = 0.005175
  l2.weight: grad_norm = 0.280612
Total gradient norm: 0.503486
=== Actor Training Debug (Iteration 7862) ===
Q mean: -77.923203
Q std: 27.936405
Actor loss: 77.927193
Action reg: 0.003993
  l1.weight: grad_norm = 0.151222
  l1.bias: grad_norm = 0.001864
  l2.weight: grad_norm = 0.374898
Total gradient norm: 0.735634
=== Actor Training Debug (Iteration 7863) ===
Q mean: -79.419029
Q std: 29.590815
Actor loss: 79.423019
Action reg: 0.003990
  l1.weight: grad_norm = 0.686283
  l1.bias: grad_norm = 0.001483
  l2.weight: grad_norm = 1.214317
Total gradient norm: 1.850952
=== Actor Training Debug (Iteration 7864) ===
Q mean: -77.818893
Q std: 30.428267
Actor loss: 77.822876
Action reg: 0.003986
  l1.weight: grad_norm = 0.141314
  l1.bias: grad_norm = 0.003685
  l2.weight: grad_norm = 0.328701
Total gradient norm: 0.586143
=== Actor Training Debug (Iteration 7865) ===
Q mean: -79.344620
Q std: 28.931009
Actor loss: 79.348610
Action reg: 0.003988
  l1.weight: grad_norm = 0.359206
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.780485
Total gradient norm: 1.458714
=== Actor Training Debug (Iteration 7866) ===
Q mean: -80.984406
Q std: 29.695604
Actor loss: 80.988396
Action reg: 0.003989
  l1.weight: grad_norm = 0.421420
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.911788
Total gradient norm: 1.763150
=== Actor Training Debug (Iteration 7867) ===
Q mean: -77.521820
Q std: 30.860426
Actor loss: 77.525810
Action reg: 0.003991
  l1.weight: grad_norm = 0.041078
  l1.bias: grad_norm = 0.004616
  l2.weight: grad_norm = 0.087636
Total gradient norm: 0.146805
=== Actor Training Debug (Iteration 7868) ===
Q mean: -79.535599
Q std: 29.913761
Actor loss: 79.539589
Action reg: 0.003993
  l1.weight: grad_norm = 0.460776
  l1.bias: grad_norm = 0.003643
  l2.weight: grad_norm = 0.656894
Total gradient norm: 1.292354
=== Actor Training Debug (Iteration 7869) ===
Q mean: -78.804489
Q std: 31.257963
Actor loss: 78.808479
Action reg: 0.003988
  l1.weight: grad_norm = 0.383525
  l1.bias: grad_norm = 0.001620
  l2.weight: grad_norm = 0.614877
Total gradient norm: 1.131386
=== Actor Training Debug (Iteration 7870) ===
Q mean: -79.837021
Q std: 28.876907
Actor loss: 79.841019
Action reg: 0.003994
  l1.weight: grad_norm = 0.128569
  l1.bias: grad_norm = 0.000756
  l2.weight: grad_norm = 0.229034
Total gradient norm: 0.390316
=== Actor Training Debug (Iteration 7871) ===
Q mean: -76.861160
Q std: 30.249149
Actor loss: 76.865150
Action reg: 0.003987
  l1.weight: grad_norm = 0.244467
  l1.bias: grad_norm = 0.004450
  l2.weight: grad_norm = 0.516856
Total gradient norm: 1.070274
=== Actor Training Debug (Iteration 7872) ===
Q mean: -75.332306
Q std: 30.500557
Actor loss: 75.336288
Action reg: 0.003986
  l1.weight: grad_norm = 0.102623
  l1.bias: grad_norm = 0.005045
  l2.weight: grad_norm = 0.248279
Total gradient norm: 0.627158
=== Actor Training Debug (Iteration 7873) ===
Q mean: -76.076920
Q std: 31.025398
Actor loss: 76.080910
Action reg: 0.003992
  l1.weight: grad_norm = 0.236259
  l1.bias: grad_norm = 0.001559
  l2.weight: grad_norm = 0.499018
Total gradient norm: 0.959144
=== Actor Training Debug (Iteration 7874) ===
Q mean: -77.348595
Q std: 29.918234
Actor loss: 77.352585
Action reg: 0.003990
  l1.weight: grad_norm = 0.533593
  l1.bias: grad_norm = 0.001661
  l2.weight: grad_norm = 1.026694
Total gradient norm: 2.135312
=== Actor Training Debug (Iteration 7875) ===
Q mean: -78.223045
Q std: 30.720793
Actor loss: 78.227036
Action reg: 0.003991
  l1.weight: grad_norm = 0.228495
  l1.bias: grad_norm = 0.003737
  l2.weight: grad_norm = 0.475157
Total gradient norm: 0.953278
=== Actor Training Debug (Iteration 7876) ===
Q mean: -78.092621
Q std: 29.174227
Actor loss: 78.096619
Action reg: 0.003994
  l1.weight: grad_norm = 0.207769
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.498313
Total gradient norm: 1.036013
=== Actor Training Debug (Iteration 7877) ===
Q mean: -75.909637
Q std: 29.500570
Actor loss: 75.913628
Action reg: 0.003994
  l1.weight: grad_norm = 0.106163
  l1.bias: grad_norm = 0.001848
  l2.weight: grad_norm = 0.194837
Total gradient norm: 0.379051
=== Actor Training Debug (Iteration 7878) ===
Q mean: -77.470207
Q std: 28.811132
Actor loss: 77.474205
Action reg: 0.003995
  l1.weight: grad_norm = 0.110064
  l1.bias: grad_norm = 0.002053
  l2.weight: grad_norm = 0.224309
Total gradient norm: 0.399463
=== Actor Training Debug (Iteration 7879) ===
Q mean: -80.725220
Q std: 28.920828
Actor loss: 80.729218
Action reg: 0.003994
  l1.weight: grad_norm = 0.149967
  l1.bias: grad_norm = 0.002250
  l2.weight: grad_norm = 0.390256
Total gradient norm: 0.763090
=== Actor Training Debug (Iteration 7880) ===
Q mean: -76.041397
Q std: 29.860664
Actor loss: 76.045395
Action reg: 0.003995
  l1.weight: grad_norm = 0.286022
  l1.bias: grad_norm = 0.000979
  l2.weight: grad_norm = 0.666999
Total gradient norm: 1.194338
=== Actor Training Debug (Iteration 7881) ===
Q mean: -76.084656
Q std: 29.185064
Actor loss: 76.088646
Action reg: 0.003990
  l1.weight: grad_norm = 0.443250
  l1.bias: grad_norm = 0.000649
  l2.weight: grad_norm = 0.965423
Total gradient norm: 1.730544
=== Actor Training Debug (Iteration 7882) ===
Q mean: -80.536186
Q std: 29.785444
Actor loss: 80.540176
Action reg: 0.003990
  l1.weight: grad_norm = 0.857391
  l1.bias: grad_norm = 0.003110
  l2.weight: grad_norm = 1.807529
Total gradient norm: 3.101733
=== Actor Training Debug (Iteration 7883) ===
Q mean: -80.273430
Q std: 30.975138
Actor loss: 80.277420
Action reg: 0.003987
  l1.weight: grad_norm = 0.419084
  l1.bias: grad_norm = 0.002249
  l2.weight: grad_norm = 0.845579
Total gradient norm: 1.859937
=== Actor Training Debug (Iteration 7884) ===
Q mean: -78.722931
Q std: 28.901779
Actor loss: 78.726929
Action reg: 0.003995
  l1.weight: grad_norm = 0.836917
  l1.bias: grad_norm = 0.000378
  l2.weight: grad_norm = 1.797571
Total gradient norm: 4.339183
=== Actor Training Debug (Iteration 7885) ===
Q mean: -77.982544
Q std: 28.851288
Actor loss: 77.986534
Action reg: 0.003991
  l1.weight: grad_norm = 0.156121
  l1.bias: grad_norm = 0.001994
  l2.weight: grad_norm = 0.395271
Total gradient norm: 0.829456
=== Actor Training Debug (Iteration 7886) ===
Q mean: -78.265381
Q std: 29.774345
Actor loss: 78.269363
Action reg: 0.003985
  l1.weight: grad_norm = 0.257367
  l1.bias: grad_norm = 0.001770
  l2.weight: grad_norm = 0.516902
Total gradient norm: 0.899664
=== Actor Training Debug (Iteration 7887) ===
Q mean: -78.332993
Q std: 31.061262
Actor loss: 78.336975
Action reg: 0.003985
  l1.weight: grad_norm = 0.406950
  l1.bias: grad_norm = 0.003230
  l2.weight: grad_norm = 0.961539
Total gradient norm: 1.911509
Total gradient norm: 0.1822310183on 6486) ===
=== Actor Training Debug (Iteration 7898) ===
Q mean: -81.716476
Q std: 30.949190
Actor loss: 81.720467
Action reg: 0.003989
  l1.weight: grad_norm = 0.254543
  l1.bias: grad_norm = 0.002064
  l2.weight: grad_norm = 0.535161
Total gradient norm: 1.153712
=== Actor Training Debug (Iteration 7899) ===
Q mean: -78.167061
Q std: 29.893816
Actor loss: 78.171051
Action reg: 0.003990
  l1.weight: grad_norm = 0.163090
  l1.bias: grad_norm = 0.001336
  l2.weight: grad_norm = 0.366211
Total gradient norm: 0.750410
=== Actor Training Debug (Iteration 7900) ===
Q mean: -76.944382
Q std: 29.394169
Actor loss: 76.948372
Action reg: 0.003994
  l1.weight: grad_norm = 0.208601
  l1.bias: grad_norm = 0.000914
  l2.weight: grad_norm = 0.456703
Total gradient norm: 0.869396
=== Actor Training Debug (Iteration 7901) ===
Q mean: -79.717377
Q std: 29.509672
Actor loss: 79.721359
Action reg: 0.003985
  l1.weight: grad_norm = 0.498514
  l1.bias: grad_norm = 0.001097
  l2.weight: grad_norm = 1.196915
Total gradient norm: 2.212852
=== Actor Training Debug (Iteration 7902) ===
Q mean: -79.218994
Q std: 29.688143
Actor loss: 79.222984
Action reg: 0.003991
  l1.weight: grad_norm = 0.253725
  l1.bias: grad_norm = 0.000830
  l2.weight: grad_norm = 0.502162
Total gradient norm: 0.932324
=== Actor Training Debug (Iteration 7903) ===
Q mean: -77.317429
Q std: 29.968744
Actor loss: 77.321411
Action reg: 0.003985
  l1.weight: grad_norm = 0.275939
  l1.bias: grad_norm = 0.003094
  l2.weight: grad_norm = 0.729150
Total gradient norm: 1.254051
=== Actor Training Debug (Iteration 7904) ===
Q mean: -77.122520
Q std: 31.000849
Actor loss: 77.126511
Action reg: 0.003991
  l1.weight: grad_norm = 0.254645
  l1.bias: grad_norm = 0.001748
  l2.weight: grad_norm = 0.514750
Total gradient norm: 1.064343
=== Actor Training Debug (Iteration 7905) ===
Q mean: -79.942764
Q std: 31.034039
Actor loss: 79.946754
Action reg: 0.003990
  l1.weight: grad_norm = 0.115166
  l1.bias: grad_norm = 0.003604
  l2.weight: grad_norm = 0.264828
Total gradient norm: 0.608977
=== Actor Training Debug (Iteration 7906) ===
Q mean: -75.502113
Q std: 30.315819
Actor loss: 75.506104
Action reg: 0.003989
  l1.weight: grad_norm = 0.049132
  l1.bias: grad_norm = 0.004628
  l2.weight: grad_norm = 0.108297
Total gradient norm: 0.237390
=== Actor Training Debug (Iteration 7907) ===
Q mean: -81.557755
Q std: 30.574690
Actor loss: 81.561745
Action reg: 0.003991
  l1.weight: grad_norm = 0.311973
  l1.bias: grad_norm = 0.001493
  l2.weight: grad_norm = 0.765277
Total gradient norm: 1.396589
=== Actor Training Debug (Iteration 7908) ===
Q mean: -79.631027
Q std: 30.207930
Actor loss: 79.635017
Action reg: 0.003992
  l1.weight: grad_norm = 0.306410
  l1.bias: grad_norm = 0.000644
  l2.weight: grad_norm = 0.492310
Total gradient norm: 0.937347
=== Actor Training Debug (Iteration 7909) ===
Q mean: -78.595024
Q std: 32.512348
Actor loss: 78.599014
Action reg: 0.003989
  l1.weight: grad_norm = 0.459668
  l1.bias: grad_norm = 0.001674
  l2.weight: grad_norm = 1.054651
Total gradient norm: 2.251360
=== Actor Training Debug (Iteration 7910) ===
Q mean: -81.327209
Q std: 32.121819
Actor loss: 81.331200
Action reg: 0.003991
  l1.weight: grad_norm = 0.268497
  l1.bias: grad_norm = 0.000773
  l2.weight: grad_norm = 0.454829
Total gradient norm: 0.894255
=== Actor Training Debug (Iteration 7911) ===
Q mean: -78.680855
Q std: 30.362305
Actor loss: 78.684853
Action reg: 0.003999
  l1.weight: grad_norm = 0.073301
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.146481
Total gradient norm: 0.295749
=== Actor Training Debug (Iteration 7912) ===
Q mean: -79.457619
Q std: 28.665359
Actor loss: 79.461617
Action reg: 0.003995
  l1.weight: grad_norm = 0.190622
  l1.bias: grad_norm = 0.000912
  l2.weight: grad_norm = 0.320891
Total gradient norm: 0.649634
=== Actor Training Debug (Iteration 7913) ===
Q mean: -78.441406
Q std: 30.062696
Actor loss: 78.445396
Action reg: 0.003989
  l1.weight: grad_norm = 0.246349
  l1.bias: grad_norm = 0.001045
  l2.weight: grad_norm = 0.489325
Total gradient norm: 0.920631
=== Actor Training Debug (Iteration 7914) ===
Q mean: -76.372780
Q std: 29.813942
Actor loss: 76.376770
Action reg: 0.003989
  l1.weight: grad_norm = 0.283541
  l1.bias: grad_norm = 0.000844
  l2.weight: grad_norm = 0.609471
Total gradient norm: 1.196408
=== Actor Training Debug (Iteration 7915) ===
Q mean: -79.730682
Q std: 30.717823
Actor loss: 79.734680
Action reg: 0.003997
  l1.weight: grad_norm = 0.107860
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.244043
Total gradient norm: 0.497397
=== Actor Training Debug (Iteration 7916) ===
Q mean: -78.391914
Q std: 29.586319
Actor loss: 78.395905
Action reg: 0.003989
  l1.weight: grad_norm = 0.320126
  l1.bias: grad_norm = 0.003156
  l2.weight: grad_norm = 0.591229
Total gradient norm: 1.205094
=== Actor Training Debug (Iteration 7917) ===
Q mean: -78.758286
Q std: 30.984554
Actor loss: 78.762268
Action reg: 0.003979
  l1.weight: grad_norm = 0.325315
  l1.bias: grad_norm = 0.005454
  l2.weight: grad_norm = 0.542230
Total gradient norm: 1.052174
=== Actor Training Debug (Iteration 7918) ===
Q mean: -75.395294
Q std: 30.913773
Actor loss: 75.399284
Action reg: 0.003991
  l1.weight: grad_norm = 0.200419
  l1.bias: grad_norm = 0.001944
  l2.weight: grad_norm = 0.370578
Total gradient norm: 0.800656
=== Actor Training Debug (Iteration 7919) ===
Q mean: -79.460899
Q std: 28.568756
Actor loss: 79.464890
Action reg: 0.003988
  l1.weight: grad_norm = 0.204986
  l1.bias: grad_norm = 0.001434
  l2.weight: grad_norm = 0.440336
Total gradient norm: 0.827282
=== Actor Training Debug (Iteration 7920) ===
Q mean: -82.422676
Q std: 30.379618
Actor loss: 82.426666
Action reg: 0.003991
  l1.weight: grad_norm = 0.211831
  l1.bias: grad_norm = 0.002102
  l2.weight: grad_norm = 0.394335
Total gradient norm: 0.690665
=== Actor Training Debug (Iteration 7921) ===
Q mean: -79.018082
Q std: 29.955309
Actor loss: 79.022064
Action reg: 0.003986
  l1.weight: grad_norm = 0.237700
  l1.bias: grad_norm = 0.001275
  l2.weight: grad_norm = 0.486826
Total gradient norm: 1.083568
=== Actor Training Debug (Iteration 7922) ===
Q mean: -77.334869
Q std: 29.678066
Actor loss: 77.338860
Action reg: 0.003990
  l1.weight: grad_norm = 0.259639
  l1.bias: grad_norm = 0.000682
  l2.weight: grad_norm = 0.677883
Total gradient norm: 1.094999
=== Actor Training Debug (Iteration 7923) ===
Q mean: -81.742912
Q std: 28.703779
Actor loss: 81.746902
Action reg: 0.003993
  l1.weight: grad_norm = 0.052603
  l1.bias: grad_norm = 0.002240
  l2.weight: grad_norm = 0.107614
Total gradient norm: 0.214107
=== Actor Training Debug (Iteration 7924) ===
Q mean: -80.453583
Q std: 29.073599
Actor loss: 80.457581
Action reg: 0.003996
  l1.weight: grad_norm = 0.404577
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.811521
Total gradient norm: 1.831643
=== Actor Training Debug (Iteration 7925) ===
Q mean: -76.055511
Q std: 30.508070
Actor loss: 76.059494
Action reg: 0.003983
  l1.weight: grad_norm = 0.429323
  l1.bias: grad_norm = 0.003255
  l2.weight: grad_norm = 0.832863
Total gradient norm: 1.350117
=== Actor Training Debug (Iteration 7926) ===
Q mean: -78.754707
Q std: 30.456375
Actor loss: 78.758698
Action reg: 0.003992
  l1.weight: grad_norm = 0.469091
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.945060
Total gradient norm: 1.877061
=== Actor Training Debug (Iteration 7927) ===
Q mean: -82.707664
Q std: 29.703028
Actor loss: 82.711655
Action reg: 0.003993
  l1.weight: grad_norm = 0.643048
  l1.bias: grad_norm = 0.000772
  l2.weight: grad_norm = 1.083946
Total gradient norm: 2.112090
=== Actor Training Debug (Iteration 7928) ===
Q mean: -76.771698
Q std: 30.896633
Actor loss: 76.775688
Action reg: 0.003994
  l1.weight: grad_norm = 0.090091
  l1.bias: grad_norm = 0.002440
  l2.weight: grad_norm = 0.212116
Total gradient norm: 0.402488
=== Actor Training Debug (Iteration 7929) ===
Q mean: -81.686928
Q std: 30.537600
Actor loss: 81.690918
Action reg: 0.003992
  l1.weight: grad_norm = 0.225354
  l1.bias: grad_norm = 0.001450
  l2.weight: grad_norm = 0.395205
Total gradient norm: 0.780279
=== Actor Training Debug (Iteration 7930) ===
Q mean: -84.780029
Q std: 29.255920
Actor loss: 84.784019
Action reg: 0.003992
  l1.weight: grad_norm = 0.253301
  l1.bias: grad_norm = 0.001639
  l2.weight: grad_norm = 0.481611
Total gradient norm: 0.856921
=== Actor Training Debug (Iteration 7931) ===
Q mean: -80.542664
Q std: 29.745264
Actor loss: 80.546661
Action reg: 0.003995
  l1.weight: grad_norm = 0.115348
  l1.bias: grad_norm = 0.001250
  l2.weight: grad_norm = 0.221202
Total gradient norm: 0.405952
=== Actor Training Debug (Iteration 7932) ===
Q mean: -79.017700
Q std: 29.845350
Actor loss: 79.021690
Action reg: 0.003991
  l1.weight: grad_norm = 0.136063
  l1.bias: grad_norm = 0.002077
  l2.weight: grad_norm = 0.284319
Total gradient norm: 0.517665
=== Actor Training Debug (Iteration 7933) ===
Q mean: -78.476776
Q std: 29.668953
Actor loss: 78.480766
Action reg: 0.003991
  l1.weight: grad_norm = 0.487304
  l1.bias: grad_norm = 0.000900
  l2.weight: grad_norm = 1.033911
Total gradient norm: 2.037230
=== Actor Training Debug (Iteration 7934) ===
Q mean: -77.787125
Q std: 29.483435
Actor loss: 77.791115
Action reg: 0.003992
  l1.weight: grad_norm = 0.525425
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 1.135159
Total gradient norm: 2.240176
=== Actor Training Debug (Iteration 7935) ===
Q mean: -77.385948
Q std: 30.827526
Actor loss: 77.389938
Action reg: 0.003989
  l1.weight: grad_norm = 0.149908
  l1.bias: grad_norm = 0.004126
  l2.weight: grad_norm = 0.265343
Total gradient norm: 0.481334
=== Actor Training Debug (Iteration 7936) ===
Q mean: -76.865753
Q std: 30.933062
Actor loss: 76.869743
Action reg: 0.003991
  l1.weight: grad_norm = 0.233845
  l1.bias: grad_norm = 0.000875
  l2.weight: grad_norm = 0.437780
Total gradient norm: 0.800544
=== Actor Training Debug (Iteration 7937) ===
Q mean: -80.891792
Q std: 31.450298
Actor loss: 80.895782
Action reg: 0.003994
  l1.weight: grad_norm = 0.236565
  l1.bias: grad_norm = 0.002199
  l2.weight: grad_norm = 0.426679
Total gradient norm: 0.643300
=== Actor Training Debug (Iteration 7938) ===
Q mean: -81.702148
Q std: 30.809217
Actor loss: 81.706139
Action reg: 0.003991
  l1.weight: grad_norm = 0.150746
  l1.bias: grad_norm = 0.001522
  l2.weight: grad_norm = 0.304766
Total gradient norm: 0.551299
=== Actor Training Debug (Iteration 7939) ===
Q mean: -82.986862
Q std: 29.979988
Actor loss: 82.990852
Action reg: 0.003991
  l1.weight: grad_norm = 0.114473
  l1.bias: grad_norm = 0.002238
  l2.weight: grad_norm = 0.263515
Total gradient norm: 0.553457
=== Actor Training Debug (Iteration 7940) ===
Q mean: -75.808517
Q std: 29.365702
Actor loss: 75.812508
Action reg: 0.003988
  l1.weight: grad_norm = 0.756422
  l1.bias: grad_norm = 0.001453
  l2.weight: grad_norm = 1.627816
Total gradient norm: 3.000544
=== Actor Training Debug (Iteration 7941) ===
Q mean: -78.368393
Q std: 30.888988
Actor loss: 78.372375
Action reg: 0.003985
  l1.weight: grad_norm = 0.214836
  l1.bias: grad_norm = 0.002241
  l2.weight: grad_norm = 0.444540
Total gradient norm: 0.776898
=== Actor Training Debug (Iteration 7942) ===
Q mean: -77.767456
Q std: 29.534925
Actor loss: 77.771454
Action reg: 0.003997
  l1.weight: grad_norm = 0.075092
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.137441
Total gradient norm: 0.238111
=== Actor Training Debug (Iteration 7943) ===
Q mean: -77.709854
Q std: 30.935944
Actor loss: 77.713844
Action reg: 0.003989
  l1.weight: grad_norm = 0.200662
  l1.bias: grad_norm = 0.004003
  l2.weight: grad_norm = 0.389534
Total gradient norm: 0.770853
=== Actor Training Debug (Iteration 7944) ===
Q mean: -78.249054
Q std: 29.753927
Actor loss: 78.253044
Action reg: 0.003993
  l1.weight: grad_norm = 0.103282
  l1.bias: grad_norm = 0.002376
  l2.weight: grad_norm = 0.187297
Total gradient norm: 0.335687
=== Actor Training Debug (Iteration 7945) ===
Q mean: -76.595627
Q std: 30.412313
Actor loss: 76.599617
Action reg: 0.003989
  l1.weight: grad_norm = 0.112405
  l1.bias: grad_norm = 0.002750
  l2.weight: grad_norm = 0.262521
Total gradient norm: 0.534943
=== Actor Training Debug (Iteration 7946) ===
Q mean: -77.851616
Q std: 28.999357
Actor loss: 77.855606
Action reg: 0.003987
  l1.weight: grad_norm = 0.274746
  l1.bias: grad_norm = 0.000986
  l2.weight: grad_norm = 0.581994
Total gradient norm: 1.102718
=== Actor Training Debug (Iteration 7947) ===
Q mean: -78.190125
Q std: 31.140940
Actor loss: 78.194115
Action reg: 0.003992
  l1.weight: grad_norm = 0.135542
  l1.bias: grad_norm = 0.001314
  l2.weight: grad_norm = 0.219603
Total gradient norm: 0.423542
=== Actor Training Debug (Iteration 7948) ===
Q mean: -77.649338
Q std: 29.644424
Actor loss: 77.653328
Action reg: 0.003989
  l1.weight: grad_norm = 0.145752
  l1.bias: grad_norm = 0.003168
  l2.weight: grad_norm = 0.291741
Total gradient norm: 0.595115
=== Actor Training Debug (Iteration 7949) ===
Q mean: -77.629379
Q std: 29.618700
Actor loss: 77.633369
Action reg: 0.003987
  l1.weight: grad_norm = 0.345855
  l1.bias: grad_norm = 0.001285
  l2.weight: grad_norm = 0.655881
Total gradient norm: 1.182171
=== Actor Training Debug (Iteration 7950) ===
Q mean: -80.564835
Q std: 29.104074
Actor loss: 80.568825
Action reg: 0.003991
  l1.weight: grad_norm = 0.701123
  l1.bias: grad_norm = 0.001176
  l2.weight: grad_norm = 1.206460
Total gradient norm: 2.386226
=== Actor Training Debug (Iteration 7951) ===
Q mean: -75.997665
Q std: 29.892086
Actor loss: 76.001648
Action reg: 0.003985
  l1.weight: grad_norm = 0.271979
  l1.bias: grad_norm = 0.003311
  l2.weight: grad_norm = 0.596346
Total gradient norm: 1.225773
=== Actor Training Debug (Iteration 7952) ===
Q mean: -78.957428
Q std: 31.988216
Actor loss: 78.961418
Action reg: 0.003988
  l1.weight: grad_norm = 0.307697
  l1.bias: grad_norm = 0.002797
  l2.weight: grad_norm = 0.617486
Total gradient norm: 1.111128
=== Actor Training Debug (Iteration 7953) ===
Q mean: -83.978958
Q std: 29.689901
Actor loss: 83.982956
Action reg: 0.003995
  l1.weight: grad_norm = 0.172767
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.354544
Total gradient norm: 0.727610
=== Actor Training Debug (Iteration 7954) ===
Q mean: -81.015686
Q std: 31.185728
Actor loss: 81.019676
Action reg: 0.003989
  l1.weight: grad_norm = 0.185850
  l1.bias: grad_norm = 0.004671
  l2.weight: grad_norm = 0.415068
Total gradient norm: 0.755986
=== Actor Training Debug (Iteration 7955) ===
Q mean: -80.913918
Q std: 31.272234
Actor loss: 80.917908
Action reg: 0.003990
  l1.weight: grad_norm = 0.233433
  l1.bias: grad_norm = 0.002471
  l2.weight: grad_norm = 0.431800
Total gradient norm: 0.804476
=== Actor Training Debug (Iteration 7956) ===
Q mean: -77.538979
Q std: 30.258171
Actor loss: 77.542969
Action reg: 0.003991
  l1.weight: grad_norm = 0.169978
  l1.bias: grad_norm = 0.002121
  l2.weight: grad_norm = 0.299696
Total gradient norm: 0.542348
=== Actor Training Debug (Iteration 7957) ===
Q mean: -76.590149
Q std: 30.530642
Actor loss: 76.594139
Action reg: 0.003989
  l1.weight: grad_norm = 0.104035
  l1.bias: grad_norm = 0.003584
  l2.weight: grad_norm = 0.230894
Total gradient norm: 0.475801
=== Actor Training Debug (Iteration 7958) ===
Q mean: -79.093941
Q std: 31.092108
Actor loss: 79.097939
Action reg: 0.003997
  l1.weight: grad_norm = 0.171355
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.291022
Total gradient norm: 0.544593
=== Actor Training Debug (Iteration 7959) ===
Q mean: -77.076019
Q std: 29.502426
Actor loss: 77.080009
Action reg: 0.003992
  l1.weight: grad_norm = 0.244896
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.433275
Total gradient norm: 0.719174
=== Actor Training Debug (Iteration 7960) ===
Q mean: -78.441689
Q std: 31.028782
Actor loss: 78.445679
Action reg: 0.003990
  l1.weight: grad_norm = 0.596903
  l1.bias: grad_norm = 0.000773
  l2.weight: grad_norm = 0.882769
Total gradient norm: 1.668862
=== Actor Training Debug (Iteration 7961) ===
Q mean: -77.941452
Q std: 31.343199
Actor loss: 77.945435
Action reg: 0.003981
  l1.weight: grad_norm = 0.298381
  l1.bias: grad_norm = 0.004204
  l2.weight: grad_norm = 0.593882
Total gradient norm: 1.152062
=== Actor Training Debug (Iteration 7962) ===
Q mean: -78.625259
Q std: 30.833044
Actor loss: 78.629242
Action reg: 0.003985
  l1.weight: grad_norm = 0.424135
  l1.bias: grad_norm = 0.001981
  l2.weight: grad_norm = 0.707336
Total gradient norm: 1.345191
=== Actor Training Debug (Iteration 7963) ===
Q mean: -79.508072
Q std: 32.110565
Actor loss: 79.512062
Action reg: 0.003989
  l1.weight: grad_norm = 0.295438
  l1.bias: grad_norm = 0.001520
  l2.weight: grad_norm = 0.552386
Total gradient norm: 1.080270
=== Actor Training Debug (Iteration 7964) ===
Q mean: -75.756325
Q std: 29.116434
Actor loss: 75.760323
Action reg: 0.003996
  l1.weight: grad_norm = 0.029182
  l1.bias: grad_norm = 0.001064
  l2.weight: grad_norm = 0.068723
Total gradient norm: 0.130039
=== Actor Training Debug (Iteration 7965) ===
Q mean: -79.326981
Q std: 29.645309
Actor loss: 79.330978
Action reg: 0.003996
  l1.weight: grad_norm = 0.069358
  l1.bias: grad_norm = 0.000974
  l2.weight: grad_norm = 0.166869
Total gradient norm: 0.341690
=== Actor Training Debug (Iteration 7966) ===
Q mean: -81.661705
Q std: 29.241053
Actor loss: 81.665695
Action reg: 0.003990
  l1.weight: grad_norm = 0.254523
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.491746
Total gradient norm: 0.818771
=== Actor Training Debug (Iteration 7967) ===
Q mean: -79.817139
Q std: 31.814058
Actor loss: 79.821129
Action reg: 0.003987
  l1.weight: grad_norm = 0.167966
  l1.bias: grad_norm = 0.003728
  l2.weight: grad_norm = 0.325025
Total gradient norm: 0.595352
=== Actor Training Debug (Iteration 7968) ===
Q mean: -76.082123
Q std: 30.870180
Actor loss: 76.086113
Action reg: 0.003989
  l1.weight: grad_norm = 0.361501
  l1.bias: grad_norm = 0.003010
  l2.weight: grad_norm = 0.614775
Total gradient norm: 1.232453
=== Actor Training Debug (Iteration 7969) ===
Q mean: -75.924789
Q std: 29.952856
Actor loss: 75.928780
Action reg: 0.003994
  l1.weight: grad_norm = 0.146268
  l1.bias: grad_norm = 0.000546
  l2.weight: grad_norm = 0.304034
Total gradient norm: 0.628264
=== Actor Training Debug (Iteration 7970) ===
Q mean: -81.200127
Q std: 29.040115
Actor loss: 81.204117
Action reg: 0.003987
  l1.weight: grad_norm = 0.216751
  l1.bias: grad_norm = 0.002403
  l2.weight: grad_norm = 0.462931
Total gradient norm: 0.925306
=== Actor Training Debug (Iteration 7971) ===
Q mean: -77.613083
Q std: 29.928007
Actor loss: 77.617073
Action reg: 0.003992
  l1.weight: grad_norm = 0.194501
  l1.bias: grad_norm = 0.002414
  l2.weight: grad_norm = 0.333937
Total gradient norm: 0.659480
=== Actor Training Debug (Iteration 7972) ===
Q mean: -77.871162
Q std: 30.850868
Actor loss: 77.875153
Action reg: 0.003989
  l1.weight: grad_norm = 0.129952
  l1.bias: grad_norm = 0.001746
  l2.weight: grad_norm = 0.285708
Total gradient norm: 0.610514
=== Actor Training Debug (Iteration 7973) ===
Q mean: -80.418457
Q std: 30.996668
Actor loss: 80.422447
Action reg: 0.003991
  l1.weight: grad_norm = 0.192975
  l1.bias: grad_norm = 0.001220
  l2.weight: grad_norm = 0.432001
Total gradient norm: 0.785309
=== Actor Training Debug (Iteration 7974) ===
Q mean: -79.223816
Q std: 29.072737
Actor loss: 79.227814
Action reg: 0.003996
  l1.weight: grad_norm = 0.093656
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.187165
Total gradient norm: 0.342491
=== Actor Training Debug (Iteration 7975) ===
Q mean: -77.877342
Q std: 29.880329
Actor loss: 77.881332
Action reg: 0.003989
  l1.weight: grad_norm = 0.179253
  l1.bias: grad_norm = 0.001259
  l2.weight: grad_norm = 0.387673
Total gradient norm: 0.840365
=== Actor Training Debug (Iteration 7976) ===
Q mean: -78.828293
Q std: 31.299768
Actor loss: 78.832283
Action reg: 0.003991
  l1.weight: grad_norm = 0.337559
  l1.bias: grad_norm = 0.000640
  l2.weight: grad_norm = 0.623449
Total gradient norm: 1.153717
=== Actor Training Debug (Iteration 7977) ===
Q mean: -78.013351
Q std: 33.228748
Actor loss: 78.017342
Action reg: 0.003993
  l1.weight: grad_norm = 0.041524
  l1.bias: grad_norm = 0.001747
  l2.weight: grad_norm = 0.092898
Total gradient norm: 0.194659
=== Actor Training Debug (Iteration 7978) ===
Q mean: -81.782158
Q std: 29.527388
Actor loss: 81.786140
Action reg: 0.003984
  l1.weight: grad_norm = 0.121317
  l1.bias: grad_norm = 0.001406
  l2.weight: grad_norm = 0.253098
Total gradient norm: 0.492259
=== Actor Training Debug (Iteration 7979) ===
Q mean: -79.910156
Q std: 29.823883
Actor loss: 79.914154
Action reg: 0.003999
  l1.weight: grad_norm = 0.036842
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.087202
Total gradient norm: 0.164884
=== Actor Training Debug (Iteration 7980) ===
Q mean: -78.907959
Q std: 29.485703
Actor loss: 78.911957
Action reg: 0.003996
  l1.weight: grad_norm = 0.609507
  l1.bias: grad_norm = 0.000766
  l2.weight: grad_norm = 1.082889
Total gradient norm: 2.048384
=== Actor Training Debug (Iteration 7981) ===
Q mean: -76.768265
Q std: 30.785549
Actor loss: 76.772255
Action reg: 0.003990
  l1.weight: grad_norm = 0.308127
  l1.bias: grad_norm = 0.001329
  l2.weight: grad_norm = 0.645241
Total gradient norm: 1.195951
=== Actor Training Debug (Iteration 7982) ===
Q mean: -82.656082
Q std: 30.975967
Actor loss: 82.660072
Action reg: 0.003990
  l1.weight: grad_norm = 0.214455
  l1.bias: grad_norm = 0.002544
  l2.weight: grad_norm = 0.421799
Total gradient norm: 0.692931
=== Actor Training Debug (Iteration 7983) ===
Q mean: -80.035095
Q std: 30.685863
Actor loss: 80.039085
Action reg: 0.003990
  l1.weight: grad_norm = 0.135137
  l1.bias: grad_norm = 0.001767
  l2.weight: grad_norm = 0.266775
Total gradient norm: 0.479614
=== Actor Training Debug (Iteration 7984) ===
Q mean: -77.680893
Q std: 30.458744
Actor loss: 77.684875
Action reg: 0.003986
  l1.weight: grad_norm = 0.072501
  l1.bias: grad_norm = 0.003353
  l2.weight: grad_norm = 0.126681
Total gradient norm: 0.218691
=== Actor Training Debug (Iteration 7985) ===
Q mean: -79.398300
Q std: 32.160294
Actor loss: 79.402290
Action reg: 0.003992
  l1.weight: grad_norm = 0.118023
  l1.bias: grad_norm = 0.001811
  l2.weight: grad_norm = 0.248027
Total gradient norm: 0.442740
=== Actor Training Debug (Iteration 7986) ===
Q mean: -80.179810
Q std: 30.202448
Actor loss: 80.183792
Action reg: 0.003985
  l1.weight: grad_norm = 0.577869
  l1.bias: grad_norm = 0.000975
  l2.weight: grad_norm = 1.222540
Total gradient norm: 2.452689
=== Actor Training Debug (Iteration 7987) ===
Q mean: -79.376701
Q std: 29.483915
Actor loss: 79.380692
Action reg: 0.003990
  l1.weight: grad_norm = 0.227835
  l1.bias: grad_norm = 0.002457
  l2.weight: grad_norm = 0.367926
Total gradient norm: 0.808841
=== Actor Training Debug (Iteration 7988) ===
Q mean: -80.236000
Q std: 30.196703
Actor loss: 80.239990
Action reg: 0.003992
  l1.weight: grad_norm = 0.225201
  l1.bias: grad_norm = 0.000613
  l2.weight: grad_norm = 0.373351
Total gradient norm: 0.750603
=== Actor Training Debug (Iteration 7989) ===
Q mean: -76.396317
Q std: 30.780537
Actor loss: 76.400307
Action reg: 0.003988
  l1.weight: grad_norm = 0.549445
  l1.bias: grad_norm = 0.002289
  l2.weight: grad_norm = 1.051892
Total gradient norm: 1.827601
=== Actor Training Debug (Iteration 7990) ===
Q mean: -78.953812
Q std: 30.957445
Actor loss: 78.957802
Action reg: 0.003992
  l1.weight: grad_norm = 0.300981
  l1.bias: grad_norm = 0.001427
  l2.weight: grad_norm = 0.583120
Total gradient norm: 1.034841
Total gradient norm: 1.0524630183on 6486) ===
Step 13000: Critic Loss: 4.8391, Actor Loss: 76.0575, Q Value: -76.0535
  Average reward: -336.476 | Average length: 100.0
Evaluation at episode 130: -336.476
=== Actor Training Debug (Iteration 8001) ===
Q mean: -81.319695
Q std: 29.334538
Actor loss: 81.323692
Action reg: 0.003995
  l1.weight: grad_norm = 0.055764
  l1.bias: grad_norm = 0.000962
  l2.weight: grad_norm = 0.120095
Total gradient norm: 0.236505
=== Actor Training Debug (Iteration 8002) ===
Q mean: -78.397202
Q std: 30.743519
Actor loss: 78.401192
Action reg: 0.003989
  l1.weight: grad_norm = 0.511718
  l1.bias: grad_norm = 0.001945
  l2.weight: grad_norm = 1.274899
Total gradient norm: 2.376425
=== Actor Training Debug (Iteration 8003) ===
Q mean: -79.116135
Q std: 29.139830
Actor loss: 79.120117
Action reg: 0.003986
  l1.weight: grad_norm = 0.403394
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.783581
Total gradient norm: 1.304212
=== Actor Training Debug (Iteration 8004) ===
Q mean: -76.943787
Q std: 29.823496
Actor loss: 76.947777
Action reg: 0.003993
  l1.weight: grad_norm = 0.051640
  l1.bias: grad_norm = 0.002157
  l2.weight: grad_norm = 0.094515
Total gradient norm: 0.175544
=== Actor Training Debug (Iteration 8005) ===
Q mean: -78.822075
Q std: 30.471674
Actor loss: 78.826065
Action reg: 0.003991
  l1.weight: grad_norm = 0.154679
  l1.bias: grad_norm = 0.002901
  l2.weight: grad_norm = 0.323266
Total gradient norm: 0.711533
=== Actor Training Debug (Iteration 8006) ===
Q mean: -78.807907
Q std: 30.526955
Actor loss: 78.811905
Action reg: 0.003994
  l1.weight: grad_norm = 0.242080
  l1.bias: grad_norm = 0.001019
  l2.weight: grad_norm = 0.496658
Total gradient norm: 1.155300
=== Actor Training Debug (Iteration 8007) ===
Q mean: -79.280739
Q std: 30.856613
Actor loss: 79.284737
Action reg: 0.003997
  l1.weight: grad_norm = 0.007652
  l1.bias: grad_norm = 0.001011
  l2.weight: grad_norm = 0.015756
Total gradient norm: 0.036646
=== Actor Training Debug (Iteration 8008) ===
Q mean: -81.323654
Q std: 29.799820
Actor loss: 81.327652
Action reg: 0.003998
  l1.weight: grad_norm = 0.223772
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.476880
Total gradient norm: 0.890742
=== Actor Training Debug (Iteration 8009) ===
Q mean: -81.818268
Q std: 30.185944
Actor loss: 81.822258
Action reg: 0.003993
  l1.weight: grad_norm = 0.180499
  l1.bias: grad_norm = 0.001328
  l2.weight: grad_norm = 0.346046
Total gradient norm: 0.839410
=== Actor Training Debug (Iteration 8010) ===
Q mean: -79.024002
Q std: 29.653980
Actor loss: 79.028000
Action reg: 0.003997
  l1.weight: grad_norm = 0.049222
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.104779
Total gradient norm: 0.208132
=== Actor Training Debug (Iteration 8011) ===
Q mean: -78.556305
Q std: 30.081804
Actor loss: 78.560303
Action reg: 0.003997
  l1.weight: grad_norm = 0.057319
  l1.bias: grad_norm = 0.001197
  l2.weight: grad_norm = 0.136440
Total gradient norm: 0.302462
=== Actor Training Debug (Iteration 8012) ===
Q mean: -75.650543
Q std: 31.000149
Actor loss: 75.654541
Action reg: 0.003995
  l1.weight: grad_norm = 0.241689
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.523911
Total gradient norm: 1.138365
=== Actor Training Debug (Iteration 8013) ===
Q mean: -80.690201
Q std: 30.693933
Actor loss: 80.694191
Action reg: 0.003988
  l1.weight: grad_norm = 0.511758
  l1.bias: grad_norm = 0.001148
  l2.weight: grad_norm = 1.187649
Total gradient norm: 2.450685
=== Actor Training Debug (Iteration 8014) ===
Q mean: -80.858307
Q std: 30.062191
Actor loss: 80.862297
Action reg: 0.003991
  l1.weight: grad_norm = 0.167419
  l1.bias: grad_norm = 0.001121
  l2.weight: grad_norm = 0.320311
Total gradient norm: 0.577105
=== Actor Training Debug (Iteration 8015) ===
Q mean: -78.044518
Q std: 28.564465
Actor loss: 78.048515
Action reg: 0.003994
  l1.weight: grad_norm = 0.008274
  l1.bias: grad_norm = 0.001688
  l2.weight: grad_norm = 0.020166
Total gradient norm: 0.048946
=== Actor Training Debug (Iteration 8016) ===
Q mean: -81.029953
Q std: 31.082230
Actor loss: 81.033943
Action reg: 0.003993
  l1.weight: grad_norm = 0.118529
  l1.bias: grad_norm = 0.000596
  l2.weight: grad_norm = 0.221930
Total gradient norm: 0.482274
=== Actor Training Debug (Iteration 8017) ===
Q mean: -77.525002
Q std: 28.068346
Actor loss: 77.528999
Action reg: 0.003994
  l1.weight: grad_norm = 0.299150
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.702382
Total gradient norm: 1.200891
=== Actor Training Debug (Iteration 8018) ===
Q mean: -81.302917
Q std: 31.183384
Actor loss: 81.306908
Action reg: 0.003992
  l1.weight: grad_norm = 0.233869
  l1.bias: grad_norm = 0.001007
  l2.weight: grad_norm = 0.515524
Total gradient norm: 1.117904
=== Actor Training Debug (Iteration 8019) ===
Q mean: -84.157295
Q std: 29.694040
Actor loss: 84.161285
Action reg: 0.003993
  l1.weight: grad_norm = 0.151952
  l1.bias: grad_norm = 0.002170
  l2.weight: grad_norm = 0.302160
Total gradient norm: 0.499458
=== Actor Training Debug (Iteration 8020) ===
Q mean: -80.212906
Q std: 31.004862
Actor loss: 80.216896
Action reg: 0.003989
  l1.weight: grad_norm = 0.319476
  l1.bias: grad_norm = 0.002329
  l2.weight: grad_norm = 0.687889
Total gradient norm: 1.248852
=== Actor Training Debug (Iteration 8021) ===
Q mean: -78.500610
Q std: 28.273499
Actor loss: 78.504601
Action reg: 0.003987
  l1.weight: grad_norm = 0.345556
  l1.bias: grad_norm = 0.001484
  l2.weight: grad_norm = 0.721705
Total gradient norm: 1.412124
=== Actor Training Debug (Iteration 8022) ===
Q mean: -76.314812
Q std: 30.122860
Actor loss: 76.318802
Action reg: 0.003988
  l1.weight: grad_norm = 0.294831
  l1.bias: grad_norm = 0.001935
  l2.weight: grad_norm = 0.592528
Total gradient norm: 1.186854
=== Actor Training Debug (Iteration 8023) ===
Q mean: -78.810028
Q std: 33.313568
Actor loss: 78.814011
Action reg: 0.003984
  l1.weight: grad_norm = 0.261349
  l1.bias: grad_norm = 0.002170
  l2.weight: grad_norm = 0.508204
Total gradient norm: 1.038776
=== Actor Training Debug (Iteration 8024) ===
Q mean: -77.149002
Q std: 30.509172
Actor loss: 77.152992
Action reg: 0.003990
  l1.weight: grad_norm = 0.207501
  l1.bias: grad_norm = 0.001802
  l2.weight: grad_norm = 0.479188
Total gradient norm: 0.846014
=== Actor Training Debug (Iteration 8025) ===
Q mean: -78.916000
Q std: 31.873360
Actor loss: 78.919991
Action reg: 0.003992
  l1.weight: grad_norm = 0.147442
  l1.bias: grad_norm = 0.001838
  l2.weight: grad_norm = 0.290843
Total gradient norm: 0.540950
=== Actor Training Debug (Iteration 8026) ===
Q mean: -79.296356
Q std: 32.126602
Actor loss: 79.300339
Action reg: 0.003986
  l1.weight: grad_norm = 0.287971
  l1.bias: grad_norm = 0.001810
  l2.weight: grad_norm = 0.606425
Total gradient norm: 1.207864
=== Actor Training Debug (Iteration 8027) ===
Q mean: -75.502151
Q std: 31.042645
Actor loss: 75.506142
Action reg: 0.003991
  l1.weight: grad_norm = 0.142157
  l1.bias: grad_norm = 0.002758
  l2.weight: grad_norm = 0.324609
Total gradient norm: 0.635883
=== Actor Training Debug (Iteration 8028) ===
Q mean: -80.311203
Q std: 31.033945
Actor loss: 80.315193
Action reg: 0.003994
  l1.weight: grad_norm = 0.289174
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.603637
Total gradient norm: 1.143513
=== Actor Training Debug (Iteration 8029) ===
Q mean: -76.176369
Q std: 28.840364
Actor loss: 76.180359
Action reg: 0.003992
  l1.weight: grad_norm = 0.316745
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.597022
Total gradient norm: 1.071729
=== Actor Training Debug (Iteration 8030) ===
Q mean: -79.697449
Q std: 30.077145
Actor loss: 79.701439
Action reg: 0.003992
  l1.weight: grad_norm = 0.225406
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.430342
Total gradient norm: 0.814811
=== Actor Training Debug (Iteration 8031) ===
Q mean: -79.261497
Q std: 30.003904
Actor loss: 79.265488
Action reg: 0.003989
  l1.weight: grad_norm = 0.174418
  l1.bias: grad_norm = 0.000792
  l2.weight: grad_norm = 0.386279
Total gradient norm: 0.692249
=== Actor Training Debug (Iteration 8032) ===
Q mean: -76.798218
Q std: 30.268703
Actor loss: 76.802208
Action reg: 0.003992
  l1.weight: grad_norm = 0.067810
  l1.bias: grad_norm = 0.001103
  l2.weight: grad_norm = 0.128265
Total gradient norm: 0.251776
=== Actor Training Debug (Iteration 8033) ===
Q mean: -80.799080
Q std: 30.557728
Actor loss: 80.803070
Action reg: 0.003993
  l1.weight: grad_norm = 0.111467
  l1.bias: grad_norm = 0.001054
  l2.weight: grad_norm = 0.215243
Total gradient norm: 0.382381
=== Actor Training Debug (Iteration 8034) ===
Q mean: -77.090317
Q std: 30.455675
Actor loss: 77.094299
Action reg: 0.003985
  l1.weight: grad_norm = 0.285292
  l1.bias: grad_norm = 0.000717
  l2.weight: grad_norm = 0.611502
Total gradient norm: 1.182804
=== Actor Training Debug (Iteration 8035) ===
Q mean: -80.126205
Q std: 31.115942
Actor loss: 80.130196
Action reg: 0.003988
  l1.weight: grad_norm = 0.366200
  l1.bias: grad_norm = 0.002097
  l2.weight: grad_norm = 0.769270
Total gradient norm: 1.318900
=== Actor Training Debug (Iteration 8036) ===
Q mean: -76.457855
Q std: 29.477682
Actor loss: 76.461853
Action reg: 0.003994
  l1.weight: grad_norm = 0.174116
  l1.bias: grad_norm = 0.000760
  l2.weight: grad_norm = 0.303585
Total gradient norm: 0.642909
=== Actor Training Debug (Iteration 8037) ===
Q mean: -78.245483
Q std: 30.377142
Actor loss: 78.249481
Action reg: 0.003996
  l1.weight: grad_norm = 0.206289
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.357006
Total gradient norm: 0.663389
=== Actor Training Debug (Iteration 8038) ===
Q mean: -78.474854
Q std: 30.869600
Actor loss: 78.478844
Action reg: 0.003990
  l1.weight: grad_norm = 0.316683
  l1.bias: grad_norm = 0.001628
  l2.weight: grad_norm = 0.595634
Total gradient norm: 1.206814
=== Actor Training Debug (Iteration 8039) ===
Q mean: -78.821777
Q std: 30.746302
Actor loss: 78.825760
Action reg: 0.003986
  l1.weight: grad_norm = 0.597753
  l1.bias: grad_norm = 0.002508
  l2.weight: grad_norm = 1.256794
Total gradient norm: 2.464424
=== Actor Training Debug (Iteration 8040) ===
Q mean: -82.630157
Q std: 28.594725
Actor loss: 82.634155
Action reg: 0.003998
  l1.weight: grad_norm = 0.105437
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.231616
Total gradient norm: 0.472340
=== Actor Training Debug (Iteration 8041) ===
Q mean: -80.493538
Q std: 29.056116
Actor loss: 80.497536
Action reg: 0.003998
  l1.weight: grad_norm = 0.041178
  l1.bias: grad_norm = 0.000552
  l2.weight: grad_norm = 0.080579
Total gradient norm: 0.159902
=== Actor Training Debug (Iteration 8042) ===
Q mean: -77.874077
Q std: 30.895443
Actor loss: 77.878067
Action reg: 0.003991
  l1.weight: grad_norm = 0.106129
  l1.bias: grad_norm = 0.002983
  l2.weight: grad_norm = 0.210527
Total gradient norm: 0.429867
=== Actor Training Debug (Iteration 8043) ===
Q mean: -77.353973
Q std: 30.428593
Actor loss: 77.357971
Action reg: 0.003997
  l1.weight: grad_norm = 0.254600
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.451464
Total gradient norm: 0.969538
=== Actor Training Debug (Iteration 8044) ===
Q mean: -80.284637
Q std: 30.320539
Actor loss: 80.288628
Action reg: 0.003993
  l1.weight: grad_norm = 0.354520
  l1.bias: grad_norm = 0.000925
  l2.weight: grad_norm = 0.610025
Total gradient norm: 1.105407
=== Actor Training Debug (Iteration 8045) ===
Q mean: -78.550514
Q std: 29.548756
Actor loss: 78.554504
Action reg: 0.003992
  l1.weight: grad_norm = 0.240553
  l1.bias: grad_norm = 0.002038
  l2.weight: grad_norm = 0.515197
Total gradient norm: 1.034763
=== Actor Training Debug (Iteration 8046) ===
Q mean: -75.991898
Q std: 29.890961
Actor loss: 75.995888
Action reg: 0.003992
  l1.weight: grad_norm = 0.225715
  l1.bias: grad_norm = 0.001672
  l2.weight: grad_norm = 0.386043
Total gradient norm: 0.713784
=== Actor Training Debug (Iteration 8047) ===
Q mean: -79.323814
Q std: 30.273361
Actor loss: 79.327805
Action reg: 0.003988
  l1.weight: grad_norm = 0.431288
  l1.bias: grad_norm = 0.001581
  l2.weight: grad_norm = 0.914263
Total gradient norm: 2.090556
=== Actor Training Debug (Iteration 8048) ===
Q mean: -80.028648
Q std: 30.708492
Actor loss: 80.032639
Action reg: 0.003991
  l1.weight: grad_norm = 0.162670
  l1.bias: grad_norm = 0.002662
  l2.weight: grad_norm = 0.300583
Total gradient norm: 0.519621
=== Actor Training Debug (Iteration 8049) ===
Q mean: -75.084473
Q std: 28.310146
Actor loss: 75.088463
Action reg: 0.003992
  l1.weight: grad_norm = 0.189171
  l1.bias: grad_norm = 0.001680
  l2.weight: grad_norm = 0.419415
Total gradient norm: 0.825819
=== Actor Training Debug (Iteration 8050) ===
Q mean: -78.823669
Q std: 30.958803
Actor loss: 78.827660
Action reg: 0.003988
  l1.weight: grad_norm = 0.524586
  l1.bias: grad_norm = 0.000971
  l2.weight: grad_norm = 1.011438
Total gradient norm: 2.114551
=== Actor Training Debug (Iteration 8051) ===
Q mean: -80.576706
Q std: 31.168444
Actor loss: 80.580688
Action reg: 0.003986
  l1.weight: grad_norm = 0.589762
  l1.bias: grad_norm = 0.001659
  l2.weight: grad_norm = 0.829817
Total gradient norm: 1.589146
=== Actor Training Debug (Iteration 8052) ===
Q mean: -81.877441
Q std: 31.971352
Actor loss: 81.881439
Action reg: 0.003995
  l1.weight: grad_norm = 0.187477
  l1.bias: grad_norm = 0.000680
  l2.weight: grad_norm = 0.383744
Total gradient norm: 0.705373
=== Actor Training Debug (Iteration 8053) ===
Q mean: -78.346245
Q std: 31.000204
Actor loss: 78.350235
Action reg: 0.003989
  l1.weight: grad_norm = 0.216205
  l1.bias: grad_norm = 0.003325
  l2.weight: grad_norm = 0.490225
Total gradient norm: 0.932628
=== Actor Training Debug (Iteration 8054) ===
Q mean: -78.368904
Q std: 31.537182
Actor loss: 78.372894
Action reg: 0.003991
  l1.weight: grad_norm = 0.077928
  l1.bias: grad_norm = 0.001665
  l2.weight: grad_norm = 0.141191
Total gradient norm: 0.298817
=== Actor Training Debug (Iteration 8055) ===
Q mean: -77.997162
Q std: 29.758919
Actor loss: 78.001152
Action reg: 0.003990
  l1.weight: grad_norm = 0.110680
  l1.bias: grad_norm = 0.001059
  l2.weight: grad_norm = 0.196829
Total gradient norm: 0.386837
=== Actor Training Debug (Iteration 8056) ===
Q mean: -82.972931
Q std: 28.276525
Actor loss: 82.976921
Action reg: 0.003993
  l1.weight: grad_norm = 1.230206
  l1.bias: grad_norm = 0.002482
  l2.weight: grad_norm = 2.300693
Total gradient norm: 4.812492
=== Actor Training Debug (Iteration 8057) ===
Q mean: -80.206802
Q std: 29.298468
Actor loss: 80.210793
Action reg: 0.003989
  l1.weight: grad_norm = 0.230228
  l1.bias: grad_norm = 0.001781
  l2.weight: grad_norm = 0.472171
Total gradient norm: 0.871985
=== Actor Training Debug (Iteration 8058) ===
Q mean: -77.491600
Q std: 29.784185
Actor loss: 77.495598
Action reg: 0.003996
  l1.weight: grad_norm = 0.053759
  l1.bias: grad_norm = 0.001000
  l2.weight: grad_norm = 0.119400
Total gradient norm: 0.225754
=== Actor Training Debug (Iteration 8059) ===
Q mean: -79.430634
Q std: 30.352108
Actor loss: 79.434631
Action reg: 0.003994
  l1.weight: grad_norm = 0.420095
  l1.bias: grad_norm = 0.000819
  l2.weight: grad_norm = 0.974794
Total gradient norm: 1.875798
=== Actor Training Debug (Iteration 8060) ===
Q mean: -81.504936
Q std: 32.735294
Actor loss: 81.508926
Action reg: 0.003988
  l1.weight: grad_norm = 0.092199
  l1.bias: grad_norm = 0.003603
  l2.weight: grad_norm = 0.197721
Total gradient norm: 0.359796
=== Actor Training Debug (Iteration 8061) ===
Q mean: -80.481430
Q std: 30.651333
Actor loss: 80.485420
Action reg: 0.003992
  l1.weight: grad_norm = 0.258037
  l1.bias: grad_norm = 0.000640
  l2.weight: grad_norm = 0.488040
Total gradient norm: 0.996748
=== Actor Training Debug (Iteration 8062) ===
Q mean: -79.120415
Q std: 29.330050
Actor loss: 79.124405
Action reg: 0.003992
  l1.weight: grad_norm = 0.380115
  l1.bias: grad_norm = 0.001649
  l2.weight: grad_norm = 0.824863
Total gradient norm: 1.664152
=== Actor Training Debug (Iteration 8063) ===
Q mean: -76.244850
Q std: 30.610840
Actor loss: 76.248840
Action reg: 0.003990
  l1.weight: grad_norm = 0.248932
  l1.bias: grad_norm = 0.002651
  l2.weight: grad_norm = 0.446636
Total gradient norm: 0.838030
=== Actor Training Debug (Iteration 8064) ===
Q mean: -77.005547
Q std: 31.509798
Actor loss: 77.009537
Action reg: 0.003988
  l1.weight: grad_norm = 0.294217
  l1.bias: grad_norm = 0.001938
  l2.weight: grad_norm = 0.501598
Total gradient norm: 0.994379
=== Actor Training Debug (Iteration 8065) ===
Q mean: -76.717003
Q std: 31.048912
Actor loss: 76.720993
Action reg: 0.003990
  l1.weight: grad_norm = 0.122583
  l1.bias: grad_norm = 0.002224
  l2.weight: grad_norm = 0.248428
Total gradient norm: 0.489520
=== Actor Training Debug (Iteration 8066) ===
Q mean: -81.165573
Q std: 31.479031
Actor loss: 81.169571
Action reg: 0.003995
  l1.weight: grad_norm = 0.352016
  l1.bias: grad_norm = 0.000723
  l2.weight: grad_norm = 0.553349
Total gradient norm: 1.062862
=== Actor Training Debug (Iteration 8067) ===
Q mean: -79.087250
Q std: 29.664047
Actor loss: 79.091248
Action reg: 0.003996
  l1.weight: grad_norm = 0.121795
  l1.bias: grad_norm = 0.001071
  l2.weight: grad_norm = 0.204801
Total gradient norm: 0.407233
=== Actor Training Debug (Iteration 8068) ===
Q mean: -82.463234
Q std: 30.236528
Actor loss: 82.467224
Action reg: 0.003994
  l1.weight: grad_norm = 0.156195
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.352395
Total gradient norm: 0.646330
=== Actor Training Debug (Iteration 8069) ===
Q mean: -79.373146
Q std: 29.274021
Actor loss: 79.377144
Action reg: 0.003996
  l1.weight: grad_norm = 0.313376
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.661824
Total gradient norm: 1.296352
=== Actor Training Debug (Iteration 8070) ===
Q mean: -79.588364
Q std: 31.810007
Actor loss: 79.592354
Action reg: 0.003990
  l1.weight: grad_norm = 0.121251
  l1.bias: grad_norm = 0.002770
  l2.weight: grad_norm = 0.246563
Total gradient norm: 0.536381
=== Actor Training Debug (Iteration 8071) ===
Q mean: -77.352089
Q std: 32.152390
Actor loss: 77.356071
Action reg: 0.003986
  l1.weight: grad_norm = 0.123374
  l1.bias: grad_norm = 0.003928
  l2.weight: grad_norm = 0.198382
Total gradient norm: 0.382442
=== Actor Training Debug (Iteration 8072) ===
Q mean: -77.729454
Q std: 30.364061
Actor loss: 77.733444
Action reg: 0.003989
  l1.weight: grad_norm = 0.401176
  l1.bias: grad_norm = 0.001067
  l2.weight: grad_norm = 0.930869
Total gradient norm: 1.661669
=== Actor Training Debug (Iteration 8073) ===
Q mean: -76.156273
Q std: 29.808371
Actor loss: 76.160263
Action reg: 0.003992
  l1.weight: grad_norm = 0.351921
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.655487
Total gradient norm: 1.301857
=== Actor Training Debug (Iteration 8074) ===
Q mean: -84.014236
Q std: 30.582006
Actor loss: 84.018227
Action reg: 0.003993
  l1.weight: grad_norm = 0.062615
  l1.bias: grad_norm = 0.001536
  l2.weight: grad_norm = 0.120877
Total gradient norm: 0.214754
=== Actor Training Debug (Iteration 8075) ===
Q mean: -81.751328
Q std: 28.868509
Actor loss: 81.755325
Action reg: 0.003994
  l1.weight: grad_norm = 0.265538
  l1.bias: grad_norm = 0.000747
  l2.weight: grad_norm = 0.518216
Total gradient norm: 1.032302
=== Actor Training Debug (Iteration 8076) ===
Q mean: -78.615967
Q std: 31.041895
Actor loss: 78.619957
Action reg: 0.003991
  l1.weight: grad_norm = 0.071877
  l1.bias: grad_norm = 0.002923
  l2.weight: grad_norm = 0.149683
Total gradient norm: 0.257071
=== Actor Training Debug (Iteration 8077) ===
Q mean: -81.235977
Q std: 31.782942
Actor loss: 81.239967
Action reg: 0.003991
  l1.weight: grad_norm = 0.160728
  l1.bias: grad_norm = 0.002156
  l2.weight: grad_norm = 0.347076
Total gradient norm: 0.770558
=== Actor Training Debug (Iteration 8078) ===
Q mean: -79.743973
Q std: 30.375870
Actor loss: 79.747963
Action reg: 0.003993
  l1.weight: grad_norm = 0.146202
  l1.bias: grad_norm = 0.001705
  l2.weight: grad_norm = 0.300031
Total gradient norm: 0.616452
=== Actor Training Debug (Iteration 8079) ===
Q mean: -76.303131
Q std: 29.759184
Actor loss: 76.307121
Action reg: 0.003990
  l1.weight: grad_norm = 0.050165
  l1.bias: grad_norm = 0.002059
  l2.weight: grad_norm = 0.102252
Total gradient norm: 0.191031
=== Actor Training Debug (Iteration 8080) ===
Q mean: -81.537483
Q std: 30.855825
Actor loss: 81.541473
Action reg: 0.003991
  l1.weight: grad_norm = 0.188397
  l1.bias: grad_norm = 0.001746
  l2.weight: grad_norm = 0.381575
Total gradient norm: 0.679377
=== Actor Training Debug (Iteration 8081) ===
Q mean: -81.240356
Q std: 31.202272
Actor loss: 81.244347
Action reg: 0.003993
  l1.weight: grad_norm = 0.097585
  l1.bias: grad_norm = 0.001294
  l2.weight: grad_norm = 0.202099
Total gradient norm: 0.383495
=== Actor Training Debug (Iteration 8082) ===
Q mean: -79.563522
Q std: 31.614702
Actor loss: 79.567513
Action reg: 0.003989
  l1.weight: grad_norm = 0.154366
  l1.bias: grad_norm = 0.002091
  l2.weight: grad_norm = 0.311385
Total gradient norm: 0.482740
=== Actor Training Debug (Iteration 8083) ===
Q mean: -80.812744
Q std: 30.804993
Actor loss: 80.816734
Action reg: 0.003990
  l1.weight: grad_norm = 0.689472
  l1.bias: grad_norm = 0.000984
  l2.weight: grad_norm = 1.504078
Total gradient norm: 3.236310
=== Actor Training Debug (Iteration 8084) ===
Q mean: -79.870705
Q std: 29.751526
Actor loss: 79.874695
Action reg: 0.003991
  l1.weight: grad_norm = 0.220367
  l1.bias: grad_norm = 0.000600
  l2.weight: grad_norm = 0.377240
Total gradient norm: 0.753241
=== Actor Training Debug (Iteration 8085) ===
Q mean: -81.350288
Q std: 29.767372
Actor loss: 81.354286
Action reg: 0.003995
  l1.weight: grad_norm = 0.026936
  l1.bias: grad_norm = 0.001518
  l2.weight: grad_norm = 0.046817
Total gradient norm: 0.104272
=== Actor Training Debug (Iteration 8086) ===
Q mean: -80.794594
Q std: 30.786966
Actor loss: 80.798576
Action reg: 0.003986
  l1.weight: grad_norm = 0.202261
  l1.bias: grad_norm = 0.002269
  l2.weight: grad_norm = 0.441217
Total gradient norm: 0.809579
=== Actor Training Debug (Iteration 8087) ===
Q mean: -78.166855
Q std: 29.900084
Actor loss: 78.170845
Action reg: 0.003993
  l1.weight: grad_norm = 0.044388
  l1.bias: grad_norm = 0.002012
  l2.weight: grad_norm = 0.092564
Total gradient norm: 0.165906
=== Actor Training Debug (Iteration 8088) ===
Q mean: -75.815392
Q std: 28.878960
Actor loss: 75.819382
Action reg: 0.003989
  l1.weight: grad_norm = 0.243502
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.437503
Total gradient norm: 0.853463
=== Actor Training Debug (Iteration 8089) ===
Q mean: -79.299347
Q std: 31.525480
Actor loss: 79.303329
Action reg: 0.003985
  l1.weight: grad_norm = 0.381289
  l1.bias: grad_norm = 0.002761
  l2.weight: grad_norm = 0.810509
Total gradient norm: 1.600377
=== Actor Training Debug (Iteration 8090) ===
Q mean: -81.501396
Q std: 29.971298
Actor loss: 81.505394
Action reg: 0.003995
  l1.weight: grad_norm = 0.180854
  l1.bias: grad_norm = 0.000666
  l2.weight: grad_norm = 0.351312
Total gradient norm: 0.689493
=== Actor Training Debug (Iteration 8091) ===
Q mean: -76.966553
Q std: 29.866608
Actor loss: 76.970551
Action reg: 0.003995
  l1.weight: grad_norm = 0.208340
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.466070
Total gradient norm: 0.862033
=== Actor Training Debug (Iteration 8092) ===
Q mean: -79.718079
Q std: 29.159378
Actor loss: 79.722069
Action reg: 0.003993
  l1.weight: grad_norm = 0.555954
  l1.bias: grad_norm = 0.000712
  l2.weight: grad_norm = 1.102309
Total gradient norm: 2.082217
=== Actor Training Debug (Iteration 8093) ===
Q mean: -81.784058
Q std: 29.982975
Actor loss: 81.788055
Action reg: 0.003996
  l1.weight: grad_norm = 0.092765
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.198639
Total gradient norm: 0.404473
=== Actor Training Debug (Iteration 8094) ===
Q mean: -84.705139
Q std: 30.796648
Actor loss: 84.709129
Action reg: 0.003990
  l1.weight: grad_norm = 0.198997
  l1.bias: grad_norm = 0.001640
  l2.weight: grad_norm = 0.379225
Total gradient norm: 0.761621
=== Actor Training Debug (Iteration 8095) ===
Q mean: -75.759674
Q std: 30.815086
Actor loss: 75.763664
Action reg: 0.003992
  l1.weight: grad_norm = 0.181407
  l1.bias: grad_norm = 0.000899
  l2.weight: grad_norm = 0.392150
Total gradient norm: 0.727877
=== Actor Training Debug (Iteration 8096) ===
Q mean: -83.143921
Q std: 29.672628
Actor loss: 83.147911
Action reg: 0.003992
  l1.weight: grad_norm = 0.121562
  l1.bias: grad_norm = 0.001482
  l2.weight: grad_norm = 0.256197
Total gradient norm: 0.475858
=== Actor Training Debug (Iteration 8097) ===
Q mean: -80.461563
Q std: 31.257381
Actor loss: 80.465561
Action reg: 0.003995
  l1.weight: grad_norm = 0.057149
  l1.bias: grad_norm = 0.001245
  l2.weight: grad_norm = 0.106110
Total gradient norm: 0.199036
=== Actor Training Debug (Iteration 8098) ===
Q mean: -78.821167
Q std: 31.267511
Actor loss: 78.825157
Action reg: 0.003993
  l1.weight: grad_norm = 0.200064
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.393880
Total gradient norm: 0.793497
=== Actor Training Debug (Iteration 8099) ===
Q mean: -79.430923
Q std: 30.489182
Actor loss: 79.434914
Action reg: 0.003994
  l1.weight: grad_norm = 0.014162
  l1.bias: grad_norm = 0.002659
  l2.weight: grad_norm = 0.035311
Total gradient norm: 0.081065
=== Actor Training Debug (Iteration 8100) ===
Q mean: -79.662750
Q std: 30.030731
Actor loss: 79.666748
Action reg: 0.003996
  l1.weight: grad_norm = 0.259241
  l1.bias: grad_norm = 0.000822
  l2.weight: grad_norm = 0.552682
Total gradient norm: 1.096281
Episode 131: Steps=100, Reward=-278.211, Buffer_size=13100
=== Actor Training Debug (Iteration 8101) ===
Q mean: -83.012222
Q std: 30.895472
Actor loss: 83.016205
Action reg: 0.003985
  l1.weight: grad_norm = 0.423061
  l1.bias: grad_norm = 0.001445
  l2.weight: grad_norm = 0.783235
Total gradient norm: 1.642340
=== Actor Training Debug (Iteration 8102) ===
Q mean: -82.208618
Q std: 30.113136
Actor loss: 82.212616
Action reg: 0.003995
  l1.weight: grad_norm = 0.102512
  l1.bias: grad_norm = 0.001510
  l2.weight: grad_norm = 0.227975
Total gradient norm: 0.393283
=== Actor Training Debug (Iteration 8103) ===
Q mean: -79.538765
Q std: 30.509008
Actor loss: 79.542755
Action reg: 0.003989
  l1.weight: grad_norm = 0.624219
  l1.bias: grad_norm = 0.001582
  l2.weight: grad_norm = 1.123476
Total gradient norm: 2.062164
=== Actor Training Debug (Iteration 8104) ===
Q mean: -79.449936
Q std: 30.347136
Actor loss: 79.453926
Action reg: 0.003989
  l1.weight: grad_norm = 0.447225
  l1.bias: grad_norm = 0.001555
  l2.weight: grad_norm = 0.964759
Total gradient norm: 1.663299
=== Actor Training Debug (Iteration 8105) ===
Q mean: -77.881546
Q std: 30.809057
Actor loss: 77.885536
Action reg: 0.003989
  l1.weight: grad_norm = 0.282735
  l1.bias: grad_norm = 0.001945
  l2.weight: grad_norm = 0.476192
Total gradient norm: 0.815128
=== Actor Training Debug (Iteration 8106) ===
Q mean: -79.872116
Q std: 30.896900
Actor loss: 79.876114
Action reg: 0.003996
  l1.weight: grad_norm = 0.003724
  l1.bias: grad_norm = 0.001581
  l2.weight: grad_norm = 0.012634
Total gradient norm: 0.040440
=== Actor Training Debug (Iteration 8107) ===
Q mean: -78.800423
Q std: 31.411198
Actor loss: 78.804413
Action reg: 0.003992
  l1.weight: grad_norm = 0.047914
  l1.bias: grad_norm = 0.002194
  l2.weight: grad_norm = 0.111953
Total gradient norm: 0.226294
=== Actor Training Debug (Iteration 8108) ===
Q mean: -78.414398
Q std: 31.294382
Actor loss: 78.418388
Action reg: 0.003994
  l1.weight: grad_norm = 0.051598
  l1.bias: grad_norm = 0.001403
  l2.weight: grad_norm = 0.096403
Total gradient norm: 0.199018
=== Actor Training Debug (Iteration 8109) ===
