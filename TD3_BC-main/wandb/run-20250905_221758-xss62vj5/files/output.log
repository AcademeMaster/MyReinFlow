开始纯在线训练...
Episode 1: Steps=100, Reward=-153.349, Buffer_size=100
Episode 2: Steps=100, Reward=-158.897, Buffer_size=200
Episode 3: Steps=100, Reward=-144.282, Buffer_size=300
Episode 4: Steps=100, Reward=-163.434, Buffer_size=400
Episode 5: Steps=100, Reward=-134.975, Buffer_size=500
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 5: -240.623
Episode 6: Steps=100, Reward=-148.276, Buffer_size=600
Episode 7: Steps=100, Reward=-143.990, Buffer_size=700
Episode 8: Steps=100, Reward=-156.337, Buffer_size=800
Episode 9: Steps=100, Reward=-149.612, Buffer_size=900
Episode 10: Steps=100, Reward=-143.900, Buffer_size=1000
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 10: -240.623
Episode 11: Steps=100, Reward=-153.140, Buffer_size=1100
Episode 12: Steps=100, Reward=-151.259, Buffer_size=1200
Episode 13: Steps=100, Reward=-161.398, Buffer_size=1300
Episode 14: Steps=100, Reward=-149.452, Buffer_size=1400
Episode 15: Steps=100, Reward=-156.107, Buffer_size=1500
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 15: -240.623
Episode 16: Steps=100, Reward=-159.695, Buffer_size=1600
Episode 17: Steps=100, Reward=-133.450, Buffer_size=1700
Episode 18: Steps=100, Reward=-147.584, Buffer_size=1800
Episode 19: Steps=100, Reward=-156.010, Buffer_size=1900
Episode 20: Steps=100, Reward=-155.286, Buffer_size=2000
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 20: -240.623
Episode 21: Steps=100, Reward=-141.957, Buffer_size=2100
Episode 22: Steps=100, Reward=-152.232, Buffer_size=2200
Episode 23: Steps=100, Reward=-163.158, Buffer_size=2300
Episode 24: Steps=100, Reward=-145.617, Buffer_size=2400
Episode 25: Steps=100, Reward=-148.027, Buffer_size=2500
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 25: -240.623
Episode 26: Steps=100, Reward=-147.196, Buffer_size=2600
Episode 27: Steps=100, Reward=-146.466, Buffer_size=2700
Episode 28: Steps=100, Reward=-136.507, Buffer_size=2800
Episode 29: Steps=100, Reward=-138.833, Buffer_size=2900
Episode 30: Steps=100, Reward=-144.373, Buffer_size=3000
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 30: -240.623
Episode 31: Steps=100, Reward=-153.301, Buffer_size=3100
Episode 32: Steps=100, Reward=-143.157, Buffer_size=3200
Episode 33: Steps=100, Reward=-138.648, Buffer_size=3300
Episode 34: Steps=100, Reward=-152.431, Buffer_size=3400
Episode 35: Steps=100, Reward=-146.192, Buffer_size=3500
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 35: -240.623
Episode 36: Steps=100, Reward=-155.974, Buffer_size=3600
Episode 37: Steps=100, Reward=-152.182, Buffer_size=3700
Episode 38: Steps=100, Reward=-146.618, Buffer_size=3800
Episode 39: Steps=100, Reward=-157.666, Buffer_size=3900
Episode 40: Steps=100, Reward=-145.000, Buffer_size=4000
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 40: -240.623
Episode 41: Steps=100, Reward=-135.649, Buffer_size=4100
Episode 42: Steps=100, Reward=-156.706, Buffer_size=4200
Episode 43: Steps=100, Reward=-135.041, Buffer_size=4300
Episode 44: Steps=100, Reward=-148.518, Buffer_size=4400
Episode 45: Steps=100, Reward=-160.098, Buffer_size=4500
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 45: -240.623
Episode 46: Steps=100, Reward=-155.470, Buffer_size=4600
Episode 47: Steps=100, Reward=-150.141, Buffer_size=4700
Episode 48: Steps=100, Reward=-160.902, Buffer_size=4800
Episode 49: Steps=100, Reward=-149.143, Buffer_size=4900
Episode 50: Steps=100, Reward=-142.800, Buffer_size=5000
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 50: -240.623
=== Actor Training Debug (Iteration 1) ===
Q mean: -1.594522
Q std: 2.402499
Actor loss: 1.597395
Action reg: 0.002873
  l1.weight: grad_norm = 0.014233
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.029467
Total gradient norm: 1.445985
=== Actor Training Debug (Iteration 2) ===
Q mean: -6.762556
Q std: 2.500477
Actor loss: 6.765853
Action reg: 0.003297
  l1.weight: grad_norm = 0.008170
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.013035
Total gradient norm: 0.554834
=== Actor Training Debug (Iteration 3) ===
Q mean: -7.753860
Q std: 2.686479
Actor loss: 7.757536
Action reg: 0.003676
  l1.weight: grad_norm = 0.003889
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.006150
Total gradient norm: 0.249092
=== Actor Training Debug (Iteration 4) ===
Q mean: -6.805856
Q std: 2.038288
Actor loss: 6.809703
Action reg: 0.003848
  l1.weight: grad_norm = 0.001915
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002539
Total gradient norm: 0.080581
=== Actor Training Debug (Iteration 5) ===
Q mean: -4.349092
Q std: 1.817201
Actor loss: 4.352966
Action reg: 0.003874
  l1.weight: grad_norm = 0.001010
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001198
Total gradient norm: 0.033720
=== Actor Training Debug (Iteration 6) ===
Q mean: -2.003397
Q std: 1.330774
Actor loss: 2.007299
Action reg: 0.003903
  l1.weight: grad_norm = 0.000531
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000687
Total gradient norm: 0.022311
=== Actor Training Debug (Iteration 7) ===
Q mean: -0.562349
Q std: 1.269677
Actor loss: 0.566273
Action reg: 0.003925
  l1.weight: grad_norm = 0.001515
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002414
Total gradient norm: 0.094933
=== Actor Training Debug (Iteration 8) ===
Q mean: -0.617305
Q std: 1.225684
Actor loss: 0.621208
Action reg: 0.003903
  l1.weight: grad_norm = 0.000652
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000825
Total gradient norm: 0.021706
=== Actor Training Debug (Iteration 9) ===
Q mean: -1.454864
Q std: 1.315570
Actor loss: 1.458772
Action reg: 0.003908
  l1.weight: grad_norm = 0.000825
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001005
Total gradient norm: 0.030299
=== Actor Training Debug (Iteration 10) ===
Q mean: -2.890938
Q std: 1.466411
Actor loss: 2.894862
Action reg: 0.003923
  l1.weight: grad_norm = 0.000975
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001206
Total gradient norm: 0.031038
=== Actor Training Debug (Iteration 11) ===
Q mean: -4.175345
Q std: 1.811608
Actor loss: 4.179290
Action reg: 0.003945
  l1.weight: grad_norm = 0.000851
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001013
Total gradient norm: 0.025585
=== Actor Training Debug (Iteration 12) ===
Q mean: -5.174214
Q std: 2.110961
Actor loss: 5.178102
Action reg: 0.003888
  l1.weight: grad_norm = 0.001037
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001215
Total gradient norm: 0.028413
=== Actor Training Debug (Iteration 13) ===
Q mean: -4.777711
Q std: 2.160833
Actor loss: 4.781637
Action reg: 0.003926
  l1.weight: grad_norm = 0.000918
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001048
Total gradient norm: 0.026426
=== Actor Training Debug (Iteration 14) ===
Q mean: -4.199913
Q std: 2.170062
Actor loss: 4.203838
Action reg: 0.003925
  l1.weight: grad_norm = 0.000681
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000860
Total gradient norm: 0.026818
=== Actor Training Debug (Iteration 15) ===
Q mean: -2.745722
Q std: 2.014704
Actor loss: 2.749630
Action reg: 0.003908
  l1.weight: grad_norm = 0.000973
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001443
Total gradient norm: 0.047016
=== Actor Training Debug (Iteration 16) ===
Q mean: -1.836438
Q std: 1.722354
Actor loss: 1.840354
Action reg: 0.003916
  l1.weight: grad_norm = 0.000808
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000903
Total gradient norm: 0.021843
=== Actor Training Debug (Iteration 17) ===
Q mean: -1.432926
Q std: 1.836048
Actor loss: 1.436870
Action reg: 0.003944
  l1.weight: grad_norm = 0.000884
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001308
Total gradient norm: 0.041146
=== Actor Training Debug (Iteration 18) ===
Q mean: -1.975224
Q std: 1.924200
Actor loss: 1.979154
Action reg: 0.003930
  l1.weight: grad_norm = 0.000629
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000894
Total gradient norm: 0.027634
=== Actor Training Debug (Iteration 19) ===
Q mean: -3.082715
Q std: 1.967961
Actor loss: 3.086621
Action reg: 0.003905
  l1.weight: grad_norm = 0.001096
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001482
Total gradient norm: 0.040963
=== Actor Training Debug (Iteration 20) ===
Q mean: -3.933608
Q std: 2.089061
Actor loss: 3.937538
Action reg: 0.003930
  l1.weight: grad_norm = 0.000808
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000975
Total gradient norm: 0.028739
=== Actor Training Debug (Iteration 21) ===
Q mean: -3.936768
Q std: 2.044078
Actor loss: 3.940683
Action reg: 0.003916
  l1.weight: grad_norm = 0.001969
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002908
Total gradient norm: 0.105813
=== Actor Training Debug (Iteration 22) ===
Q mean: -3.475803
Q std: 1.919859
Actor loss: 3.479703
Action reg: 0.003900
  l1.weight: grad_norm = 0.001189
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001510
Total gradient norm: 0.047038
=== Actor Training Debug (Iteration 23) ===
Q mean: -2.680325
Q std: 1.638908
Actor loss: 2.684260
Action reg: 0.003936
  l1.weight: grad_norm = 0.001638
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002325
Total gradient norm: 0.078595
=== Actor Training Debug (Iteration 24) ===
Q mean: -2.476354
Q std: 1.590499
Actor loss: 2.480276
Action reg: 0.003922
  l1.weight: grad_norm = 0.001097
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001232
Total gradient norm: 0.029114
=== Actor Training Debug (Iteration 25) ===
Q mean: -2.708183
Q std: 1.595767
Actor loss: 2.712096
Action reg: 0.003913
  l1.weight: grad_norm = 0.001099
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001134
Total gradient norm: 0.024184
=== Actor Training Debug (Iteration 26) ===
Q mean: -3.264279
Q std: 1.761512
Actor loss: 3.268185
Action reg: 0.003905
  l1.weight: grad_norm = 0.000974
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001005
Total gradient norm: 0.023388
=== Actor Training Debug (Iteration 27) ===
Q mean: -3.485173
Q std: 1.684680
Actor loss: 3.489090
Action reg: 0.003917
  l1.weight: grad_norm = 0.001277
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001680
Total gradient norm: 0.043501
=== Actor Training Debug (Iteration 28) ===
Q mean: -2.900484
Q std: 1.682130
Actor loss: 2.904369
Action reg: 0.003885
  l1.weight: grad_norm = 0.001477
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001653
Total gradient norm: 0.040434
=== Actor Training Debug (Iteration 29) ===
Q mean: -2.867213
Q std: 1.597400
Actor loss: 2.871148
Action reg: 0.003935
  l1.weight: grad_norm = 0.001079
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001194
Total gradient norm: 0.033996
=== Actor Training Debug (Iteration 30) ===
Q mean: -2.898512
Q std: 1.793283
Actor loss: 2.902417
Action reg: 0.003906
  l1.weight: grad_norm = 0.001392
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001737
Total gradient norm: 0.051084
=== Actor Training Debug (Iteration 31) ===
Q mean: -3.449562
Q std: 1.983654
Actor loss: 3.453460
Action reg: 0.003898
  l1.weight: grad_norm = 0.001168
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001398
Total gradient norm: 0.035022
=== Actor Training Debug (Iteration 32) ===
Q mean: -3.344641
Q std: 1.783549
Actor loss: 3.348531
Action reg: 0.003890
  l1.weight: grad_norm = 0.001567
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001986
Total gradient norm: 0.054915
=== Actor Training Debug (Iteration 33) ===
Q mean: -3.161663
Q std: 2.004135
Actor loss: 3.165460
Action reg: 0.003797
  l1.weight: grad_norm = 0.001553
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001783
Total gradient norm: 0.049105
=== Actor Training Debug (Iteration 34) ===
Q mean: -2.319614
Q std: 1.803056
Actor loss: 2.323474
Action reg: 0.003860
  l1.weight: grad_norm = 0.001471
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001754
Total gradient norm: 0.040848
=== Actor Training Debug (Iteration 35) ===
Q mean: -2.413811
Q std: 1.790726
Actor loss: 2.417649
Action reg: 0.003838
  l1.weight: grad_norm = 0.001572
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001674
Total gradient norm: 0.037071
=== Actor Training Debug (Iteration 36) ===
Q mean: -2.778173
Q std: 1.724898
Actor loss: 2.782030
Action reg: 0.003856
  l1.weight: grad_norm = 0.002167
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002593
Total gradient norm: 0.059292
=== Actor Training Debug (Iteration 37) ===
Q mean: -3.428760
Q std: 1.826442
Actor loss: 3.432655
Action reg: 0.003895
  l1.weight: grad_norm = 0.001187
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001330
Total gradient norm: 0.028682
=== Actor Training Debug (Iteration 38) ===
Q mean: -3.755114
Q std: 1.952831
Actor loss: 3.758989
Action reg: 0.003875
  l1.weight: grad_norm = 0.001716
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002280
Total gradient norm: 0.061466
=== Actor Training Debug (Iteration 39) ===
Q mean: -3.423653
Q std: 1.922923
Actor loss: 3.427482
Action reg: 0.003829
  l1.weight: grad_norm = 0.002205
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002732
Total gradient norm: 0.075368
=== Actor Training Debug (Iteration 40) ===
Q mean: -2.523350
Q std: 1.725522
Actor loss: 2.527253
Action reg: 0.003904
  l1.weight: grad_norm = 0.001880
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002180
Total gradient norm: 0.053198
=== Actor Training Debug (Iteration 41) ===
Q mean: -2.125852
Q std: 1.687632
Actor loss: 2.129762
Action reg: 0.003910
  l1.weight: grad_norm = 0.001749
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001771
Total gradient norm: 0.035640
=== Actor Training Debug (Iteration 42) ===
Q mean: -2.513012
Q std: 1.778249
Actor loss: 2.516938
Action reg: 0.003926
  l1.weight: grad_norm = 0.001047
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001084
Total gradient norm: 0.022482
=== Actor Training Debug (Iteration 43) ===
Q mean: -3.368497
Q std: 1.879253
Actor loss: 3.372384
Action reg: 0.003887
  l1.weight: grad_norm = 0.001185
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001303
Total gradient norm: 0.028379
=== Actor Training Debug (Iteration 44) ===
Q mean: -3.667421
Q std: 2.033859
Actor loss: 3.671326
Action reg: 0.003905
  l1.weight: grad_norm = 0.001481
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001592
Total gradient norm: 0.030249
=== Actor Training Debug (Iteration 45) ===
Q mean: -3.385600
Q std: 1.897235
Actor loss: 3.389414
Action reg: 0.003814
  l1.weight: grad_norm = 0.001296
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001514
Total gradient norm: 0.031954
=== Actor Training Debug (Iteration 46) ===
Q mean: -2.790002
Q std: 1.853229
Actor loss: 2.793893
Action reg: 0.003891
  l1.weight: grad_norm = 0.001746
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002054
Total gradient norm: 0.051473
=== Actor Training Debug (Iteration 47) ===
Q mean: -2.663533
Q std: 1.884462
Actor loss: 2.667395
Action reg: 0.003863
  l1.weight: grad_norm = 0.002329
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003188
Total gradient norm: 0.096694
=== Actor Training Debug (Iteration 48) ===
Q mean: -2.893482
Q std: 1.672872
Actor loss: 2.897333
Action reg: 0.003851
  l1.weight: grad_norm = 0.001692
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002062
Total gradient norm: 0.046378
=== Actor Training Debug (Iteration 49) ===
Q mean: -2.845673
Q std: 1.793428
Actor loss: 2.849570
Action reg: 0.003896
  l1.weight: grad_norm = 0.001854
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002647
Total gradient norm: 0.076072
=== Actor Training Debug (Iteration 50) ===
Q mean: -3.272947
Q std: 1.813739
Actor loss: 3.276853
Action reg: 0.003906
  l1.weight: grad_norm = 0.001555
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001892
Total gradient norm: 0.035593
=== Actor Training Debug (Iteration 51) ===
Q mean: -3.004666
Q std: 1.719488
Actor loss: 3.008544
Action reg: 0.003878
  l1.weight: grad_norm = 0.001297
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001452
Total gradient norm: 0.033323
=== Actor Training Debug (Iteration 52) ===
Q mean: -2.680894
Q std: 1.601511
Actor loss: 2.684800
Action reg: 0.003906
  l1.weight: grad_norm = 0.001414
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001648
Total gradient norm: 0.036917
=== Actor Training Debug (Iteration 53) ===
Q mean: -2.671691
Q std: 1.699392
Actor loss: 2.675556
Action reg: 0.003865
  l1.weight: grad_norm = 0.001209
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001441
Total gradient norm: 0.032660
=== Actor Training Debug (Iteration 54) ===
Q mean: -2.736033
Q std: 1.766037
Actor loss: 2.739912
Action reg: 0.003879
  l1.weight: grad_norm = 0.001228
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001267
Total gradient norm: 0.021830
=== Actor Training Debug (Iteration 55) ===
Q mean: -3.331527
Q std: 2.012128
Actor loss: 3.335423
Action reg: 0.003896
  l1.weight: grad_norm = 0.001618
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001670
Total gradient norm: 0.029821
=== Actor Training Debug (Iteration 56) ===
Q mean: -3.341552
Q std: 2.031291
Actor loss: 3.345439
Action reg: 0.003887
  l1.weight: grad_norm = 0.001811
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002073
Total gradient norm: 0.042712
=== Actor Training Debug (Iteration 57) ===
Q mean: -3.035566
Q std: 1.794425
Actor loss: 3.039438
Action reg: 0.003872
  l1.weight: grad_norm = 0.001942
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002588
Total gradient norm: 0.069364
=== Actor Training Debug (Iteration 58) ===
Q mean: -2.382827
Q std: 1.717929
Actor loss: 2.386703
Action reg: 0.003876
  l1.weight: grad_norm = 0.002032
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002305
Total gradient norm: 0.049575
=== Actor Training Debug (Iteration 59) ===
Q mean: -2.522376
Q std: 1.798737
Actor loss: 2.526272
Action reg: 0.003897
  l1.weight: grad_norm = 0.002048
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002538
Total gradient norm: 0.061892
=== Actor Training Debug (Iteration 60) ===
Q mean: -2.929374
Q std: 1.872166
Actor loss: 2.933256
Action reg: 0.003883
  l1.weight: grad_norm = 0.001357
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001441
Total gradient norm: 0.024118
=== Actor Training Debug (Iteration 61) ===
Q mean: -3.367337
Q std: 1.741847
Actor loss: 3.371208
Action reg: 0.003871
  l1.weight: grad_norm = 0.001022
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001350
Total gradient norm: 0.034268
=== Actor Training Debug (Iteration 62) ===
Q mean: -2.944287
Q std: 1.630571
Actor loss: 2.948177
Action reg: 0.003890
  l1.weight: grad_norm = 0.001539
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001677
Total gradient norm: 0.033868
=== Actor Training Debug (Iteration 63) ===
Q mean: -2.632363
Q std: 1.618979
Actor loss: 2.636284
Action reg: 0.003921
  l1.weight: grad_norm = 0.001372
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001510
Total gradient norm: 0.033185
=== Actor Training Debug (Iteration 64) ===
Q mean: -2.693135
Q std: 1.771532
Actor loss: 2.697040
Action reg: 0.003905
  l1.weight: grad_norm = 0.002175
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002540
Total gradient norm: 0.055215
=== Actor Training Debug (Iteration 65) ===
Q mean: -3.065718
Q std: 1.879539
Actor loss: 3.069652
Action reg: 0.003933
  l1.weight: grad_norm = 0.001106
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001355
Total gradient norm: 0.027353
=== Actor Training Debug (Iteration 66) ===
Q mean: -3.705367
Q std: 1.991623
Actor loss: 3.709291
Action reg: 0.003923
  l1.weight: grad_norm = 0.001968
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002375
Total gradient norm: 0.057426
=== Actor Training Debug (Iteration 67) ===
Q mean: -3.716909
Q std: 1.891353
Actor loss: 3.720822
Action reg: 0.003912
  l1.weight: grad_norm = 0.001588
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002043
Total gradient norm: 0.043698
=== Actor Training Debug (Iteration 68) ===
Q mean: -3.262879
Q std: 1.981993
Actor loss: 3.266784
Action reg: 0.003906
  l1.weight: grad_norm = 0.001611
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001642
Total gradient norm: 0.029032
=== Actor Training Debug (Iteration 69) ===
Q mean: -2.586235
Q std: 1.647732
Actor loss: 2.590137
Action reg: 0.003902
  l1.weight: grad_norm = 0.001969
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002169
Total gradient norm: 0.040330
=== Actor Training Debug (Iteration 70) ===
Q mean: -2.093936
Q std: 1.747046
Actor loss: 2.097851
Action reg: 0.003915
  l1.weight: grad_norm = 0.001087
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001308
Total gradient norm: 0.027115
=== Actor Training Debug (Iteration 71) ===
Q mean: -2.221482
Q std: 1.774173
Actor loss: 2.225382
Action reg: 0.003900
  l1.weight: grad_norm = 0.001344
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001597
Total gradient norm: 0.034641
=== Actor Training Debug (Iteration 72) ===
Q mean: -2.657053
Q std: 1.717358
Actor loss: 2.660900
Action reg: 0.003847
  l1.weight: grad_norm = 0.001260
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001337
Total gradient norm: 0.022649
=== Actor Training Debug (Iteration 73) ===
Q mean: -3.581848
Q std: 1.931357
Actor loss: 3.585745
Action reg: 0.003896
  l1.weight: grad_norm = 0.001037
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001167
Total gradient norm: 0.019929
=== Actor Training Debug (Iteration 74) ===
Q mean: -3.760910
Q std: 1.794584
Actor loss: 3.764796
Action reg: 0.003886
  l1.weight: grad_norm = 0.002433
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003155
Total gradient norm: 0.077977
=== Actor Training Debug (Iteration 75) ===
Q mean: -3.732560
Q std: 1.814001
Actor loss: 3.736445
Action reg: 0.003885
  l1.weight: grad_norm = 0.002070
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002640
Total gradient norm: 0.061883
=== Actor Training Debug (Iteration 76) ===
Q mean: -2.972777
Q std: 1.631427
Actor loss: 2.976687
Action reg: 0.003910
  l1.weight: grad_norm = 0.001416
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001535
Total gradient norm: 0.029023
=== Actor Training Debug (Iteration 77) ===
Q mean: -2.041803
Q std: 1.457469
Actor loss: 2.045665
Action reg: 0.003862
  l1.weight: grad_norm = 0.001378
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001481
Total gradient norm: 0.028012
=== Actor Training Debug (Iteration 78) ===
Q mean: -1.902251
Q std: 1.462831
Actor loss: 1.906179
Action reg: 0.003929
  l1.weight: grad_norm = 0.001363
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001692
Total gradient norm: 0.034729
=== Actor Training Debug (Iteration 79) ===
Q mean: -2.063992
Q std: 1.619260
Actor loss: 2.067894
Action reg: 0.003903
  l1.weight: grad_norm = 0.001367
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001753
Total gradient norm: 0.039047
=== Actor Training Debug (Iteration 80) ===
Q mean: -2.579020
Q std: 1.712075
Actor loss: 2.582939
Action reg: 0.003919
  l1.weight: grad_norm = 0.001317
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001366
Total gradient norm: 0.026533
=== Actor Training Debug (Iteration 81) ===
Q mean: -3.573449
Q std: 2.046474
Actor loss: 3.577325
Action reg: 0.003876
  l1.weight: grad_norm = 0.002086
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002867
Total gradient norm: 0.066712
=== Actor Training Debug (Iteration 82) ===
Q mean: -4.231405
Q std: 1.997638
Actor loss: 4.235321
Action reg: 0.003915
  l1.weight: grad_norm = 0.000994
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000993
Total gradient norm: 0.017723
=== Actor Training Debug (Iteration 83) ===
Q mean: -4.048677
Q std: 2.126145
Actor loss: 4.052568
Action reg: 0.003890
  l1.weight: grad_norm = 0.001197
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001285
Total gradient norm: 0.022757
=== Actor Training Debug (Iteration 84) ===
Q mean: -3.690256
Q std: 2.262444
Actor loss: 3.694118
Action reg: 0.003863
  l1.weight: grad_norm = 0.001142
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001358
Total gradient norm: 0.031419
=== Actor Training Debug (Iteration 85) ===
Q mean: -2.863297
Q std: 1.782412
Actor loss: 2.867201
Action reg: 0.003904
  l1.weight: grad_norm = 0.001436
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001478
Total gradient norm: 0.024803
=== Actor Training Debug (Iteration 86) ===
Q mean: -2.385989
Q std: 1.764727
Actor loss: 2.389921
Action reg: 0.003931
  l1.weight: grad_norm = 0.000946
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001051
Total gradient norm: 0.017741
=== Actor Training Debug (Iteration 87) ===
Q mean: -2.138464
Q std: 1.677586
Actor loss: 2.142359
Action reg: 0.003894
  l1.weight: grad_norm = 0.001502
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001712
Total gradient norm: 0.033917
=== Actor Training Debug (Iteration 88) ===
Q mean: -2.644170
Q std: 1.776255
Actor loss: 2.647999
Action reg: 0.003829
  l1.weight: grad_norm = 0.001292
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001402
Total gradient norm: 0.026778
=== Actor Training Debug (Iteration 89) ===
Q mean: -3.287393
Q std: 1.781211
Actor loss: 3.291278
Action reg: 0.003885
  l1.weight: grad_norm = 0.001825
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002471
Total gradient norm: 0.058142
=== Actor Training Debug (Iteration 90) ===
Q mean: -3.799557
Q std: 1.958756
Actor loss: 3.803492
Action reg: 0.003935
  l1.weight: grad_norm = 0.001386
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001608
Total gradient norm: 0.032557
=== Actor Training Debug (Iteration 91) ===
Q mean: -3.637392
Q std: 1.686652
Actor loss: 3.641307
Action reg: 0.003915
  l1.weight: grad_norm = 0.002277
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002722
Total gradient norm: 0.060817
=== Actor Training Debug (Iteration 92) ===
Q mean: -3.092606
Q std: 1.763994
Actor loss: 3.096540
Action reg: 0.003934
  l1.weight: grad_norm = 0.001474
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001869
Total gradient norm: 0.045421
=== Actor Training Debug (Iteration 93) ===
Q mean: -2.309441
Q std: 1.563631
Actor loss: 2.313336
Action reg: 0.003895
  l1.weight: grad_norm = 0.001469
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001999
Total gradient norm: 0.047248
=== Actor Training Debug (Iteration 94) ===
Q mean: -2.053240
Q std: 1.533711
Actor loss: 2.057080
Action reg: 0.003839
  l1.weight: grad_norm = 0.001533
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001877
Total gradient norm: 0.047860
=== Actor Training Debug (Iteration 95) ===
Q mean: -2.272421
Q std: 1.661297
Actor loss: 2.276329
Action reg: 0.003908
  l1.weight: grad_norm = 0.001264
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001488
Total gradient norm: 0.027906
=== Actor Training Debug (Iteration 96) ===
Q mean: -2.988626
Q std: 1.802674
Actor loss: 2.992506
Action reg: 0.003880
  l1.weight: grad_norm = 0.001542
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002062
Total gradient norm: 0.044808
=== Actor Training Debug (Iteration 97) ===
Q mean: -3.717079
Q std: 1.909364
Actor loss: 3.720968
Action reg: 0.003889
  l1.weight: grad_norm = 0.001930
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002273
Total gradient norm: 0.047109
=== Actor Training Debug (Iteration 98) ===
Q mean: -3.868880
Q std: 1.985293
Actor loss: 3.872746
Action reg: 0.003867
  l1.weight: grad_norm = 0.001065
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001173
Total gradient norm: 0.024697
=== Actor Training Debug (Iteration 99) ===
Q mean: -3.541078
Q std: 1.965690
Actor loss: 3.544954
Action reg: 0.003876
  l1.weight: grad_norm = 0.001709
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002137
Total gradient norm: 0.051830
=== Actor Training Debug (Iteration 100) ===
Q mean: -3.145099
Q std: 1.785617
Actor loss: 3.149017
Action reg: 0.003918
  l1.weight: grad_norm = 0.000877
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001053
Total gradient norm: 0.020886
Episode 51: Steps=100, Reward=-270.738, Buffer_size=5100
=== Actor Training Debug (Iteration 101) ===
Q mean: -2.601338
Q std: 1.767022
Actor loss: 2.605236
Action reg: 0.003898
  l1.weight: grad_norm = 0.001323
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001444
Total gradient norm: 0.029633
=== Actor Training Debug (Iteration 102) ===
Q mean: -2.310652
Q std: 1.834455
Actor loss: 2.314515
Action reg: 0.003863
  l1.weight: grad_norm = 0.001502
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001802
Total gradient norm: 0.039711
=== Actor Training Debug (Iteration 103) ===
Q mean: -2.187388
Q std: 1.757270
Actor loss: 2.191229
Action reg: 0.003841
  l1.weight: grad_norm = 0.001008
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001120
Total gradient norm: 0.018795
=== Actor Training Debug (Iteration 104) ===
Q mean: -2.562235
Q std: 1.833012
Actor loss: 2.566123
Action reg: 0.003888
  l1.weight: grad_norm = 0.001938
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002453
Total gradient norm: 0.059750
=== Actor Training Debug (Iteration 105) ===
Q mean: -3.155935
Q std: 2.109156
Actor loss: 3.159740
Action reg: 0.003805
  l1.weight: grad_norm = 0.001874
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002138
Total gradient norm: 0.047723
=== Actor Training Debug (Iteration 106) ===
Q mean: -3.583035
Q std: 1.907113
Actor loss: 3.586889
Action reg: 0.003854
  l1.weight: grad_norm = 0.002114
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002936
Total gradient norm: 0.066778
=== Actor Training Debug (Iteration 107) ===
Q mean: -3.523905
Q std: 1.816141
Actor loss: 3.527823
Action reg: 0.003918
  l1.weight: grad_norm = 0.001740
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002084
Total gradient norm: 0.045698
=== Actor Training Debug (Iteration 108) ===
Q mean: -3.225312
Q std: 1.911456
Actor loss: 3.229190
Action reg: 0.003878
  l1.weight: grad_norm = 0.001860
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002032
Total gradient norm: 0.037103
=== Actor Training Debug (Iteration 109) ===
Q mean: -2.609495
Q std: 1.756348
Actor loss: 2.613378
Action reg: 0.003883
  l1.weight: grad_norm = 0.001554
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001734
Total gradient norm: 0.027258
=== Actor Training Debug (Iteration 110) ===
Q mean: -2.446500
Q std: 1.717341
Actor loss: 2.450377
Action reg: 0.003876
  l1.weight: grad_norm = 0.001261
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001322
Total gradient norm: 0.024463
=== Actor Training Debug (Iteration 111) ===
Q mean: -2.538126
Q std: 1.698711
Actor loss: 2.541964
Action reg: 0.003839
  l1.weight: grad_norm = 0.001749
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002095
Total gradient norm: 0.043128
=== Actor Training Debug (Iteration 112) ===
Q mean: -3.214162
Q std: 1.728084
Actor loss: 3.218006
Action reg: 0.003844
  l1.weight: grad_norm = 0.002076
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002423
Total gradient norm: 0.051217
=== Actor Training Debug (Iteration 113) ===
Q mean: -3.606478
Q std: 1.797167
Actor loss: 3.610399
Action reg: 0.003921
  l1.weight: grad_norm = 0.002503
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003148
Total gradient norm: 0.066682
=== Actor Training Debug (Iteration 114) ===
Q mean: -3.463278
Q std: 1.858763
Actor loss: 3.467186
Action reg: 0.003908
  l1.weight: grad_norm = 0.001616
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002046
Total gradient norm: 0.048887
=== Actor Training Debug (Iteration 115) ===
Q mean: -2.949125
Q std: 1.795464
Actor loss: 2.952983
Action reg: 0.003858
  l1.weight: grad_norm = 0.000959
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001082
Total gradient norm: 0.020938
=== Actor Training Debug (Iteration 116) ===
Q mean: -2.649201
Q std: 1.698715
Actor loss: 2.653089
Action reg: 0.003888
  l1.weight: grad_norm = 0.001556
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001837
Total gradient norm: 0.041221
=== Actor Training Debug (Iteration 117) ===
Q mean: -2.686044
Q std: 1.682944
Actor loss: 2.689929
Action reg: 0.003884
  l1.weight: grad_norm = 0.001380
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001714
Total gradient norm: 0.035279
=== Actor Training Debug (Iteration 118) ===
Q mean: -2.894455
Q std: 1.800750
Actor loss: 2.898338
Action reg: 0.003883
  l1.weight: grad_norm = 0.000989
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001215
Total gradient norm: 0.028838
=== Actor Training Debug (Iteration 119) ===
Q mean: -3.189324
Q std: 1.752271
Actor loss: 3.193200
Action reg: 0.003875
  l1.weight: grad_norm = 0.001275
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001462
Total gradient norm: 0.025926
=== Actor Training Debug (Iteration 120) ===
Q mean: -3.070510
Q std: 1.852904
Actor loss: 3.074377
Action reg: 0.003867
  l1.weight: grad_norm = 0.001639
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002093
Total gradient norm: 0.051247
=== Actor Training Debug (Iteration 121) ===
Q mean: -2.885572
Q std: 1.730717
Actor loss: 2.889483
Action reg: 0.003911
  l1.weight: grad_norm = 0.001527
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001766
Total gradient norm: 0.035494
=== Actor Training Debug (Iteration 122) ===
Q mean: -2.810377
Q std: 1.742749
Actor loss: 2.814232
Action reg: 0.003854
  l1.weight: grad_norm = 0.001580
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001930
Total gradient norm: 0.040469
=== Actor Training Debug (Iteration 123) ===
Q mean: -2.875786
Q std: 1.916122
Actor loss: 2.879665
Action reg: 0.003879
  l1.weight: grad_norm = 0.001176
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001335
Total gradient norm: 0.021627
=== Actor Training Debug (Iteration 124) ===
Q mean: -3.063221
Q std: 1.858468
Actor loss: 3.067117
Action reg: 0.003896
  l1.weight: grad_norm = 0.001379
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001573
Total gradient norm: 0.026118
=== Actor Training Debug (Iteration 125) ===
Q mean: -3.112997
Q std: 1.699204
Actor loss: 3.116904
Action reg: 0.003907
  l1.weight: grad_norm = 0.001225
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001455
Total gradient norm: 0.030633
=== Actor Training Debug (Iteration 126) ===
Q mean: -2.800943
Q std: 1.924546
Actor loss: 2.804806
Action reg: 0.003863
  l1.weight: grad_norm = 0.001281
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001862
Total gradient norm: 0.048850
=== Actor Training Debug (Iteration 127) ===
Q mean: -2.849823
Q std: 1.698971
Actor loss: 2.853734
Action reg: 0.003911
  l1.weight: grad_norm = 0.001001
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001066
Total gradient norm: 0.018141
=== Actor Training Debug (Iteration 128) ===
Q mean: -2.940887
Q std: 1.724631
Actor loss: 2.944796
Action reg: 0.003908
  l1.weight: grad_norm = 0.000930
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001015
Total gradient norm: 0.017269
=== Actor Training Debug (Iteration 129) ===
Q mean: -2.868998
Q std: 1.766605
Actor loss: 2.872907
Action reg: 0.003910
  l1.weight: grad_norm = 0.001328
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001449
Total gradient norm: 0.028082
=== Actor Training Debug (Iteration 130) ===
Q mean: -2.845446
Q std: 1.816769
Actor loss: 2.849366
Action reg: 0.003920
  l1.weight: grad_norm = 0.001302
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001434
Total gradient norm: 0.024888
=== Actor Training Debug (Iteration 131) ===
Q mean: -3.072191
Q std: 1.873631
Actor loss: 3.076072
Action reg: 0.003881
  l1.weight: grad_norm = 0.001647
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002098
Total gradient norm: 0.048254
=== Actor Training Debug (Iteration 132) ===
Q mean: -3.111848
Q std: 1.853081
Actor loss: 3.115769
Action reg: 0.003921
  l1.weight: grad_norm = 0.000828
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000863
Total gradient norm: 0.012445
=== Actor Training Debug (Iteration 133) ===
Q mean: -3.294842
Q std: 1.850611
Actor loss: 3.298764
Action reg: 0.003922
  l1.weight: grad_norm = 0.002162
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002484
Total gradient norm: 0.049486
=== Actor Training Debug (Iteration 134) ===
Q mean: -2.781630
Q std: 1.892745
Actor loss: 2.785454
Action reg: 0.003823
  l1.weight: grad_norm = 0.001744
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001960
Total gradient norm: 0.037799
=== Actor Training Debug (Iteration 135) ===
Q mean: -3.099270
Q std: 1.743078
Actor loss: 3.103169
Action reg: 0.003900
  l1.weight: grad_norm = 0.000964
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001020
Total gradient norm: 0.013316
=== Actor Training Debug (Iteration 136) ===
Q mean: -2.959952
Q std: 1.904066
Actor loss: 2.963795
Action reg: 0.003844
  l1.weight: grad_norm = 0.001378
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001730
Total gradient norm: 0.031938
=== Actor Training Debug (Iteration 137) ===
Q mean: -2.919048
Q std: 1.862700
Actor loss: 2.922960
Action reg: 0.003911
  l1.weight: grad_norm = 0.001469
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001695
Total gradient norm: 0.035023
=== Actor Training Debug (Iteration 138) ===
Q mean: -2.872437
Q std: 1.857441
Actor loss: 2.876326
Action reg: 0.003889
  l1.weight: grad_norm = 0.001293
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001815
Total gradient norm: 0.047905
=== Actor Training Debug (Iteration 139) ===
Q mean: -2.911080
Q std: 1.841266
Actor loss: 2.914960
Action reg: 0.003880
  l1.weight: grad_norm = 0.001433
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001623
Total gradient norm: 0.031238
=== Actor Training Debug (Iteration 140) ===
Q mean: -3.176350
Q std: 1.832104
Actor loss: 3.180245
Action reg: 0.003896
  l1.weight: grad_norm = 0.001356
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001624
Total gradient norm: 0.034775
=== Actor Training Debug (Iteration 141) ===
Q mean: -2.655492
Q std: 1.743916
Actor loss: 2.659356
Action reg: 0.003864
  l1.weight: grad_norm = 0.001550
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001626
Total gradient norm: 0.025181
=== Actor Training Debug (Iteration 142) ===
Q mean: -2.817014
Q std: 1.788259
Actor loss: 2.820885
Action reg: 0.003871
  l1.weight: grad_norm = 0.001733
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002354
Total gradient norm: 0.057613
=== Actor Training Debug (Iteration 143) ===
Q mean: -2.985843
Q std: 1.675689
Actor loss: 2.989709
Action reg: 0.003866
  l1.weight: grad_norm = 0.001875
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001797
Total gradient norm: 0.026651
=== Actor Training Debug (Iteration 144) ===
Q mean: -3.217822
Q std: 1.866648
Actor loss: 3.221740
Action reg: 0.003918
  l1.weight: grad_norm = 0.001343
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001857
Total gradient norm: 0.055360
=== Actor Training Debug (Iteration 145) ===
Q mean: -3.144325
Q std: 1.837127
Actor loss: 3.148143
Action reg: 0.003818
  l1.weight: grad_norm = 0.000993
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.001110
Total gradient norm: 0.018725
=== Actor Training Debug (Iteration 146) ===
Q mean: -2.562315
Q std: 1.835708
Actor loss: 2.566218
Action reg: 0.003903
  l1.weight: grad_norm = 0.001134
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001337
Total gradient norm: 0.031140
=== Actor Training Debug (Iteration 147) ===
Q mean: -2.266793
Q std: 1.636171
Actor loss: 2.270708
Action reg: 0.003915
  l1.weight: grad_norm = 0.001451
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001907
Total gradient norm: 0.041043
=== Actor Training Debug (Iteration 148) ===
Q mean: -2.713786
Q std: 1.714216
Actor loss: 2.717698
Action reg: 0.003913
  l1.weight: grad_norm = 0.001546
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002593
Total gradient norm: 0.077490
=== Actor Training Debug (Iteration 149) ===
Q mean: -3.220981
Q std: 1.736317
Actor loss: 3.224891
Action reg: 0.003910
  l1.weight: grad_norm = 0.001269
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001745
Total gradient norm: 0.039763
=== Actor Training Debug (Iteration 150) ===
Q mean: -3.438595
Q std: 1.807542
Actor loss: 3.442527
Action reg: 0.003932
  l1.weight: grad_norm = 0.001157
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001722
Total gradient norm: 0.045700
=== Actor Training Debug (Iteration 151) ===
Q mean: -3.283640
Q std: 1.779017
Actor loss: 3.287507
Action reg: 0.003866
  l1.weight: grad_norm = 0.001242
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001322
Total gradient norm: 0.023314
=== Actor Training Debug (Iteration 152) ===
Q mean: -2.874374
Q std: 1.724880
Actor loss: 2.878230
Action reg: 0.003856
  l1.weight: grad_norm = 0.001377
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001971
Total gradient norm: 0.052851
=== Actor Training Debug (Iteration 153) ===
Q mean: -2.523971
Q std: 1.667579
Actor loss: 2.527816
Action reg: 0.003845
  l1.weight: grad_norm = 0.001436
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002023
Total gradient norm: 0.053376
=== Actor Training Debug (Iteration 154) ===
Q mean: -2.572284
Q std: 1.862399
Actor loss: 2.576149
Action reg: 0.003865
  l1.weight: grad_norm = 0.001323
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001762
Total gradient norm: 0.042747
=== Actor Training Debug (Iteration 155) ===
Q mean: -2.901951
Q std: 1.942079
Actor loss: 2.905811
Action reg: 0.003860
  l1.weight: grad_norm = 0.001597
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002033
Total gradient norm: 0.047851
=== Actor Training Debug (Iteration 156) ===
Q mean: -2.935685
Q std: 1.807032
Actor loss: 2.939575
Action reg: 0.003890
  l1.weight: grad_norm = 0.001382
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001644
Total gradient norm: 0.029386
=== Actor Training Debug (Iteration 157) ===
Q mean: -3.369396
Q std: 2.045426
Actor loss: 3.373273
Action reg: 0.003877
  l1.weight: grad_norm = 0.001370
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001942
Total gradient norm: 0.050789
=== Actor Training Debug (Iteration 158) ===
Q mean: -3.407085
Q std: 1.958428
Actor loss: 3.410977
Action reg: 0.003892
  l1.weight: grad_norm = 0.001346
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001534
Total gradient norm: 0.025673
=== Actor Training Debug (Iteration 159) ===
Q mean: -3.162680
Q std: 1.978176
Actor loss: 3.166579
Action reg: 0.003900
  l1.weight: grad_norm = 0.000901
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001029
Total gradient norm: 0.018078
=== Actor Training Debug (Iteration 160) ===
Q mean: -2.852949
Q std: 1.825895
Actor loss: 2.856835
Action reg: 0.003886
  l1.weight: grad_norm = 0.001461
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001836
Total gradient norm: 0.036778
=== Actor Training Debug (Iteration 161) ===
Q mean: -2.359815
Q std: 1.733789
Actor loss: 2.363726
Action reg: 0.003910
  l1.weight: grad_norm = 0.001498
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001983
Total gradient norm: 0.043818
=== Actor Training Debug (Iteration 162) ===
Q mean: -2.651458
Q std: 1.843449
Actor loss: 2.655341
Action reg: 0.003882
  l1.weight: grad_norm = 0.001313
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001631
Total gradient norm: 0.029447
=== Actor Training Debug (Iteration 163) ===
Q mean: -2.912192
Q std: 1.783597
Actor loss: 2.916036
Action reg: 0.003844
  l1.weight: grad_norm = 0.001168
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001414
Total gradient norm: 0.023782
=== Actor Training Debug (Iteration 164) ===
Q mean: -3.245125
Q std: 1.876935
Actor loss: 3.249015
Action reg: 0.003890
  l1.weight: grad_norm = 0.001355
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001599
Total gradient norm: 0.026829
=== Actor Training Debug (Iteration 165) ===
Q mean: -2.956926
Q std: 1.910928
Actor loss: 2.960757
Action reg: 0.003830
  l1.weight: grad_norm = 0.001659
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002863
Total gradient norm: 0.083841
=== Actor Training Debug (Iteration 166) ===
Q mean: -2.826404
Q std: 1.836700
Actor loss: 2.830272
Action reg: 0.003868
  l1.weight: grad_norm = 0.001587
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001970
Total gradient norm: 0.037051
=== Actor Training Debug (Iteration 167) ===
Q mean: -2.564053
Q std: 1.744517
Actor loss: 2.567950
Action reg: 0.003897
  l1.weight: grad_norm = 0.001199
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001511
Total gradient norm: 0.027428
=== Actor Training Debug (Iteration 168) ===
Q mean: -2.691565
Q std: 1.713020
Actor loss: 2.695450
Action reg: 0.003885
  l1.weight: grad_norm = 0.001187
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001579
Total gradient norm: 0.034733
=== Actor Training Debug (Iteration 169) ===
Q mean: -2.978618
Q std: 1.725960
Actor loss: 2.982557
Action reg: 0.003939
  l1.weight: grad_norm = 0.001119
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001399
Total gradient norm: 0.029065
=== Actor Training Debug (Iteration 170) ===
Q mean: -3.111929
Q std: 1.808235
Actor loss: 3.115803
Action reg: 0.003874
  l1.weight: grad_norm = 0.000901
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001148
Total gradient norm: 0.021430
=== Actor Training Debug (Iteration 171) ===
Q mean: -3.327231
Q std: 1.811093
Actor loss: 3.331089
Action reg: 0.003857
  l1.weight: grad_norm = 0.001084
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001307
Total gradient norm: 0.029244
=== Actor Training Debug (Iteration 172) ===
Q mean: -3.014686
Q std: 1.744752
Actor loss: 3.018595
Action reg: 0.003909
  l1.weight: grad_norm = 0.002055
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002947
Total gradient norm: 0.060071
=== Actor Training Debug (Iteration 173) ===
Q mean: -2.863949
Q std: 1.767528
Actor loss: 2.867840
Action reg: 0.003891
  l1.weight: grad_norm = 0.001211
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001530
Total gradient norm: 0.027780
=== Actor Training Debug (Iteration 174) ===
Q mean: -2.621867
Q std: 1.839582
Actor loss: 2.625713
Action reg: 0.003846
  l1.weight: grad_norm = 0.000839
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000955
Total gradient norm: 0.015101
=== Actor Training Debug (Iteration 175) ===
Q mean: -2.838102
Q std: 1.855103
Actor loss: 2.842032
Action reg: 0.003931
  l1.weight: grad_norm = 0.001404
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001779
Total gradient norm: 0.045330
=== Actor Training Debug (Iteration 176) ===
Q mean: -2.802095
Q std: 1.687907
Actor loss: 2.805985
Action reg: 0.003889
  l1.weight: grad_norm = 0.001404
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001747
Total gradient norm: 0.033200
=== Actor Training Debug (Iteration 177) ===
Q mean: -3.142657
Q std: 1.928787
Actor loss: 3.146542
Action reg: 0.003885
  l1.weight: grad_norm = 0.000887
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001122
Total gradient norm: 0.026600
=== Actor Training Debug (Iteration 178) ===
Q mean: -3.042263
Q std: 1.866613
Actor loss: 3.046127
Action reg: 0.003864
  l1.weight: grad_norm = 0.001944
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002741
Total gradient norm: 0.055790
=== Actor Training Debug (Iteration 179) ===
Q mean: -2.916536
Q std: 1.741073
Actor loss: 2.920417
Action reg: 0.003881
  l1.weight: grad_norm = 0.001561
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001940
Total gradient norm: 0.034205
=== Actor Training Debug (Iteration 180) ===
Q mean: -2.583452
Q std: 1.845466
Actor loss: 2.587328
Action reg: 0.003876
  l1.weight: grad_norm = 0.001203
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001461
Total gradient norm: 0.030389
=== Actor Training Debug (Iteration 181) ===
Q mean: -2.804541
Q std: 1.933583
Actor loss: 2.808449
Action reg: 0.003907
  l1.weight: grad_norm = 0.000943
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001124
Total gradient norm: 0.020984
=== Actor Training Debug (Iteration 182) ===
Q mean: -3.066369
Q std: 1.861654
Actor loss: 3.070266
Action reg: 0.003898
  l1.weight: grad_norm = 0.002287
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003649
Total gradient norm: 0.099887
=== Actor Training Debug (Iteration 183) ===
Q mean: -3.354449
Q std: 1.953814
Actor loss: 3.358369
Action reg: 0.003920
  l1.weight: grad_norm = 0.001556
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002144
Total gradient norm: 0.055031
=== Actor Training Debug (Iteration 184) ===
Q mean: -3.119553
Q std: 1.768723
Actor loss: 3.123478
Action reg: 0.003925
  l1.weight: grad_norm = 0.001444
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001955
Total gradient norm: 0.052668
=== Actor Training Debug (Iteration 185) ===
Q mean: -2.701595
Q std: 1.786240
Actor loss: 2.705487
Action reg: 0.003893
  l1.weight: grad_norm = 0.001648
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002506
Total gradient norm: 0.075247
=== Actor Training Debug (Iteration 186) ===
Q mean: -2.926838
Q std: 1.640898
Actor loss: 2.930763
Action reg: 0.003925
  l1.weight: grad_norm = 0.002430
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003872
Total gradient norm: 0.118600
=== Actor Training Debug (Iteration 187) ===
Q mean: -3.299558
Q std: 1.808447
Actor loss: 3.303439
Action reg: 0.003882
  l1.weight: grad_norm = 0.001597
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002230
Total gradient norm: 0.062428
=== Actor Training Debug (Iteration 188) ===
Q mean: -3.272217
Q std: 2.049161
Actor loss: 3.276100
Action reg: 0.003883
  l1.weight: grad_norm = 0.001110
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001242
Total gradient norm: 0.020219
=== Actor Training Debug (Iteration 189) ===
Q mean: -2.821749
Q std: 1.742134
Actor loss: 2.825670
Action reg: 0.003922
  l1.weight: grad_norm = 0.001848
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003011
Total gradient norm: 0.089180
=== Actor Training Debug (Iteration 190) ===
Q mean: -2.688532
Q std: 1.895764
Actor loss: 2.692439
Action reg: 0.003906
  l1.weight: grad_norm = 0.001094
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001272
Total gradient norm: 0.029633
=== Actor Training Debug (Iteration 191) ===
Q mean: -2.881767
Q std: 1.782859
Actor loss: 2.885624
Action reg: 0.003857
  l1.weight: grad_norm = 0.001462
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001824
Total gradient norm: 0.039770
=== Actor Training Debug (Iteration 192) ===
Q mean: -3.325988
Q std: 1.814786
Actor loss: 3.329892
Action reg: 0.003904
  l1.weight: grad_norm = 0.001312
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001663
Total gradient norm: 0.039641
=== Actor Training Debug (Iteration 193) ===
Q mean: -3.081081
Q std: 1.782422
Actor loss: 3.084939
Action reg: 0.003858
  l1.weight: grad_norm = 0.002580
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.003757
Total gradient norm: 0.094706
=== Actor Training Debug (Iteration 194) ===
Q mean: -2.982063
Q std: 1.863138
Actor loss: 2.985929
Action reg: 0.003867
  l1.weight: grad_norm = 0.001615
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001809
Total gradient norm: 0.034589
=== Actor Training Debug (Iteration 195) ===
Q mean: -2.801927
Q std: 1.791535
Actor loss: 2.805770
Action reg: 0.003843
  l1.weight: grad_norm = 0.001436
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001784
Total gradient norm: 0.032257
=== Actor Training Debug (Iteration 196) ===
Q mean: -2.743225
Q std: 1.966274
Actor loss: 2.747125
Action reg: 0.003899
  l1.weight: grad_norm = 0.001047
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001142
Total gradient norm: 0.018125
=== Actor Training Debug (Iteration 197) ===
Q mean: -2.856404
Q std: 1.859689
Actor loss: 2.860250
Action reg: 0.003846
  l1.weight: grad_norm = 0.001601
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.002276
Total gradient norm: 0.056505
=== Actor Training Debug (Iteration 198) ===
Q mean: -2.840481
Q std: 1.960954
Actor loss: 2.844348
Action reg: 0.003867
  l1.weight: grad_norm = 0.000995
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001253
Total gradient norm: 0.018468
=== Actor Training Debug (Iteration 199) ===
Q mean: -3.000846
Q std: 1.890141
Actor loss: 3.004716
Action reg: 0.003870
  l1.weight: grad_norm = 0.001066
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001216
Total gradient norm: 0.017285
=== Actor Training Debug (Iteration 200) ===
Q mean: -2.824271
Q std: 1.902526
Actor loss: 2.828176
Action reg: 0.003905
  l1.weight: grad_norm = 0.001217
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001491
Total gradient norm: 0.027064
=== Actor Training Debug (Iteration 201) ===
Q mean: -2.686242
Q std: 2.011279
Actor loss: 2.690106
Action reg: 0.003864
  l1.weight: grad_norm = 0.001170
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001743
Total gradient norm: 0.048040
=== Actor Training Debug (Iteration 202) ===
Q mean: -3.067410
Q std: 1.985580
Actor loss: 3.071294
Action reg: 0.003885
  l1.weight: grad_norm = 0.001546
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002353
Total gradient norm: 0.056538
=== Actor Training Debug (Iteration 203) ===
Q mean: -2.860859
Q std: 1.900956
Actor loss: 2.864715
Action reg: 0.003857
  l1.weight: grad_norm = 0.001918
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002432
Total gradient norm: 0.043501
=== Actor Training Debug (Iteration 204) ===
Q mean: -2.941540
Q std: 1.944396
Actor loss: 2.945412
Action reg: 0.003872
  l1.weight: grad_norm = 0.002141
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003500
Total gradient norm: 0.091898
=== Actor Training Debug (Iteration 205) ===
Q mean: -3.099577
Q std: 1.900799
Actor loss: 3.103472
Action reg: 0.003895
  l1.weight: grad_norm = 0.001258
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001500
Total gradient norm: 0.024893
=== Actor Training Debug (Iteration 206) ===
Q mean: -3.225711
Q std: 1.802983
Actor loss: 3.229620
Action reg: 0.003909
  l1.weight: grad_norm = 0.001159
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001514
Total gradient norm: 0.028208
=== Actor Training Debug (Iteration 207) ===
Q mean: -2.973802
Q std: 2.061296
Actor loss: 2.977683
Action reg: 0.003880
  l1.weight: grad_norm = 0.001090
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001330
Total gradient norm: 0.019803
=== Actor Training Debug (Iteration 208) ===
Q mean: -3.220997
Q std: 1.969139
Actor loss: 3.224843
Action reg: 0.003845
  l1.weight: grad_norm = 0.001516
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.001799
Total gradient norm: 0.034541
=== Actor Training Debug (Iteration 209) ===
Q mean: -3.021435
Q std: 1.938200
Actor loss: 3.025332
Action reg: 0.003897
  l1.weight: grad_norm = 0.001526
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001817
Total gradient norm: 0.029890
=== Actor Training Debug (Iteration 210) ===
Q mean: -2.650939
Q std: 1.795765
Actor loss: 2.654834
Action reg: 0.003895
  l1.weight: grad_norm = 0.001849
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002808
Total gradient norm: 0.075628
=== Actor Training Debug (Iteration 211) ===
Q mean: -2.840487
Q std: 1.987688
Actor loss: 2.844343
Action reg: 0.003856
  l1.weight: grad_norm = 0.001695
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002149
Total gradient norm: 0.034548
=== Actor Training Debug (Iteration 212) ===
Q mean: -3.004541
Q std: 2.008260
Actor loss: 3.008443
Action reg: 0.003902
  l1.weight: grad_norm = 0.001682
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002603
Total gradient norm: 0.074561
=== Actor Training Debug (Iteration 213) ===
Q mean: -3.397191
Q std: 2.006994
Actor loss: 3.401030
Action reg: 0.003839
  l1.weight: grad_norm = 0.003175
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.005404
Total gradient norm: 0.160205
=== Actor Training Debug (Iteration 214) ===
Q mean: -3.429524
Q std: 1.954684
Actor loss: 3.433404
Action reg: 0.003880
  l1.weight: grad_norm = 0.001054
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001300
Total gradient norm: 0.021307
=== Actor Training Debug (Iteration 215) ===
Q mean: -2.822426
Q std: 1.873584
Actor loss: 2.826333
Action reg: 0.003907
  l1.weight: grad_norm = 0.001488
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002420
Total gradient norm: 0.070735
=== Actor Training Debug (Iteration 216) ===
Q mean: -2.655233
Q std: 1.877921
Actor loss: 2.659156
Action reg: 0.003923
  l1.weight: grad_norm = 0.001656
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002222
Total gradient norm: 0.056517
=== Actor Training Debug (Iteration 217) ===
Q mean: -2.892463
Q std: 1.962820
Actor loss: 2.896361
Action reg: 0.003898
  l1.weight: grad_norm = 0.002139
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003416
Total gradient norm: 0.100761
=== Actor Training Debug (Iteration 218) ===
Q mean: -3.233465
Q std: 1.886050
Actor loss: 3.237345
Action reg: 0.003880
  l1.weight: grad_norm = 0.001427
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002113
Total gradient norm: 0.054153
=== Actor Training Debug (Iteration 219) ===
Q mean: -3.293477
Q std: 1.904026
Actor loss: 3.297331
Action reg: 0.003854
  l1.weight: grad_norm = 0.001445
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001888
Total gradient norm: 0.040592
=== Actor Training Debug (Iteration 220) ===
Q mean: -3.003030
Q std: 1.727905
Actor loss: 3.006897
Action reg: 0.003866
  l1.weight: grad_norm = 0.001998
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003332
Total gradient norm: 0.093841
=== Actor Training Debug (Iteration 221) ===
Q mean: -2.858766
Q std: 1.837254
Actor loss: 2.862654
Action reg: 0.003888
  l1.weight: grad_norm = 0.001256
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002064
Total gradient norm: 0.054563
=== Actor Training Debug (Iteration 222) ===
Q mean: -2.905969
Q std: 1.812781
Actor loss: 2.909854
Action reg: 0.003885
  l1.weight: grad_norm = 0.000793
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001000
Total gradient norm: 0.019275
=== Actor Training Debug (Iteration 223) ===
Q mean: -2.747708
Q std: 1.746924
Actor loss: 2.751600
Action reg: 0.003892
  l1.weight: grad_norm = 0.001071
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001333
Total gradient norm: 0.021743
=== Actor Training Debug (Iteration 224) ===
Q mean: -3.032582
Q std: 1.958971
Actor loss: 3.036489
Action reg: 0.003907
  l1.weight: grad_norm = 0.001316
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001634
Total gradient norm: 0.027611
=== Actor Training Debug (Iteration 225) ===
Q mean: -2.976404
Q std: 1.892917
Actor loss: 2.980308
Action reg: 0.003904
  l1.weight: grad_norm = 0.000862
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001235
Total gradient norm: 0.027342
=== Actor Training Debug (Iteration 226) ===
Q mean: -3.099253
Q std: 1.894605
Actor loss: 3.103151
Action reg: 0.003899
  l1.weight: grad_norm = 0.001413
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002000
Total gradient norm: 0.054635
=== Actor Training Debug (Iteration 227) ===
Q mean: -2.972710
Q std: 1.686781
Actor loss: 2.976600
Action reg: 0.003891
  l1.weight: grad_norm = 0.001097
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001319
Total gradient norm: 0.021200
=== Actor Training Debug (Iteration 228) ===
Q mean: -2.966238
Q std: 1.749785
Actor loss: 2.970157
Action reg: 0.003918
  l1.weight: grad_norm = 0.001476
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001797
Total gradient norm: 0.029961
=== Actor Training Debug (Iteration 229) ===
Q mean: -3.108072
Q std: 1.865474
Actor loss: 3.111961
Action reg: 0.003889
  l1.weight: grad_norm = 0.001450
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001811
Total gradient norm: 0.036119
=== Actor Training Debug (Iteration 230) ===
Q mean: -3.194108
Q std: 1.940206
Actor loss: 3.198021
Action reg: 0.003914
  l1.weight: grad_norm = 0.001575
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001878
Total gradient norm: 0.038379
=== Actor Training Debug (Iteration 231) ===
Q mean: -2.929992
Q std: 1.935257
Actor loss: 2.933890
Action reg: 0.003897
  l1.weight: grad_norm = 0.001175
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001474
Total gradient norm: 0.026157
=== Actor Training Debug (Iteration 232) ===
Q mean: -2.952897
Q std: 2.006475
Actor loss: 2.956822
Action reg: 0.003925
  l1.weight: grad_norm = 0.001128
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001381
Total gradient norm: 0.024445
=== Actor Training Debug (Iteration 233) ===
Q mean: -2.878181
Q std: 2.006904
Actor loss: 2.882042
Action reg: 0.003861
  l1.weight: grad_norm = 0.001558
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.002537
Total gradient norm: 0.066473
=== Actor Training Debug (Iteration 234) ===
Q mean: -3.136290
Q std: 1.976025
Actor loss: 3.140210
Action reg: 0.003921
  l1.weight: grad_norm = 0.001053
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001288
Total gradient norm: 0.025810
=== Actor Training Debug (Iteration 235) ===
Q mean: -3.251560
Q std: 1.840874
Actor loss: 3.255442
Action reg: 0.003882
  l1.weight: grad_norm = 0.001835
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002624
Total gradient norm: 0.062813
=== Actor Training Debug (Iteration 236) ===
Q mean: -2.932869
Q std: 1.791522
Actor loss: 2.936756
Action reg: 0.003887
  l1.weight: grad_norm = 0.001072
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001298
Total gradient norm: 0.024239
=== Actor Training Debug (Iteration 237) ===
Q mean: -2.863936
Q std: 1.662190
Actor loss: 2.867800
Action reg: 0.003864
  l1.weight: grad_norm = 0.000839
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001054
Total gradient norm: 0.015944
=== Actor Training Debug (Iteration 238) ===
Q mean: -2.879010
Q std: 1.862868
Actor loss: 2.882865
Action reg: 0.003855
  l1.weight: grad_norm = 0.001336
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001748
Total gradient norm: 0.036310
=== Actor Training Debug (Iteration 239) ===
Q mean: -2.917009
Q std: 1.872280
Actor loss: 2.920903
Action reg: 0.003894
  l1.weight: grad_norm = 0.001116
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.001201
Total gradient norm: 0.021890
=== Actor Training Debug (Iteration 240) ===
Q mean: -3.048211
Q std: 1.882210
Actor loss: 3.052091
Action reg: 0.003880
  l1.weight: grad_norm = 0.000903
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001046
Total gradient norm: 0.017001
=== Actor Training Debug (Iteration 241) ===
Q mean: -3.029861
Q std: 2.071277
Actor loss: 3.033805
Action reg: 0.003944
  l1.weight: grad_norm = 0.000986
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001204
Total gradient norm: 0.020551
=== Actor Training Debug (Iteration 242) ===
Q mean: -3.094605
Q std: 2.076223
Actor loss: 3.098486
Action reg: 0.003881
  l1.weight: grad_norm = 0.000976
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001381
Total gradient norm: 0.035049
=== Actor Training Debug (Iteration 243) ===
Q mean: -2.837096
Q std: 1.885196
Actor loss: 2.841049
Action reg: 0.003953
  l1.weight: grad_norm = 0.000653
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000877
Total gradient norm: 0.019160
=== Actor Training Debug (Iteration 244) ===
Q mean: -2.840515
Q std: 1.960423
Actor loss: 2.844448
Action reg: 0.003933
  l1.weight: grad_norm = 0.001071
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001477
Total gradient norm: 0.033644
=== Actor Training Debug (Iteration 245) ===
Q mean: -2.981539
Q std: 1.897152
Actor loss: 2.985435
Action reg: 0.003896
  l1.weight: grad_norm = 0.001096
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001355
Total gradient norm: 0.027975
=== Actor Training Debug (Iteration 246) ===
Q mean: -3.451721
Q std: 1.971476
Actor loss: 3.455612
Action reg: 0.003891
  l1.weight: grad_norm = 0.001458
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001752
Total gradient norm: 0.029100
=== Actor Training Debug (Iteration 247) ===
Q mean: -3.336930
Q std: 1.976591
Actor loss: 3.340745
Action reg: 0.003815
  l1.weight: grad_norm = 0.002006
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002746
Total gradient norm: 0.057783
=== Actor Training Debug (Iteration 248) ===
Q mean: -2.972721
Q std: 1.793014
Actor loss: 2.976596
Action reg: 0.003875
  l1.weight: grad_norm = 0.002362
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.003729
Total gradient norm: 0.098012
=== Actor Training Debug (Iteration 249) ===
Q mean: -2.772147
Q std: 1.954608
Actor loss: 2.776030
Action reg: 0.003882
  l1.weight: grad_norm = 0.001179
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001333
Total gradient norm: 0.018728
=== Actor Training Debug (Iteration 250) ===
Q mean: -3.149682
Q std: 1.918531
Actor loss: 3.153618
Action reg: 0.003936
  l1.weight: grad_norm = 0.001397
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001583
Total gradient norm: 0.023804
=== Actor Training Debug (Iteration 251) ===
Q mean: -3.023085
Q std: 1.862776
Actor loss: 3.027007
Action reg: 0.003922
  l1.weight: grad_norm = 0.000832
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000961
Total gradient norm: 0.014047
=== Actor Training Debug (Iteration 252) ===
Q mean: -3.082004
Q std: 2.112159
Actor loss: 3.085906
Action reg: 0.003902
  l1.weight: grad_norm = 0.001141
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001714
Total gradient norm: 0.046303
=== Actor Training Debug (Iteration 253) ===
Q mean: -2.999717
Q std: 1.902329
Actor loss: 3.003669
Action reg: 0.003952
  l1.weight: grad_norm = 0.000867
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001026
Total gradient norm: 0.016234
=== Actor Training Debug (Iteration 254) ===
Q mean: -3.037819
Q std: 1.973636
Actor loss: 3.041754
Action reg: 0.003935
  l1.weight: grad_norm = 0.001240
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001877
Total gradient norm: 0.047010
=== Actor Training Debug (Iteration 255) ===
Q mean: -3.083059
Q std: 2.013756
Actor loss: 3.086984
Action reg: 0.003925
  l1.weight: grad_norm = 0.001126
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001792
Total gradient norm: 0.049047
=== Actor Training Debug (Iteration 256) ===
Q mean: -2.766855
Q std: 2.040297
Actor loss: 2.770746
Action reg: 0.003891
  l1.weight: grad_norm = 0.001347
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001894
Total gradient norm: 0.045406
=== Actor Training Debug (Iteration 257) ===
Q mean: -2.827167
Q std: 1.910879
Actor loss: 2.831035
Action reg: 0.003868
  l1.weight: grad_norm = 0.001081
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001317
Total gradient norm: 0.018484
=== Actor Training Debug (Iteration 258) ===
Q mean: -3.122557
Q std: 1.989778
Actor loss: 3.126458
Action reg: 0.003901
  l1.weight: grad_norm = 0.002159
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.003152
Total gradient norm: 0.074217
=== Actor Training Debug (Iteration 259) ===
Q mean: -3.333860
Q std: 2.001516
Actor loss: 3.337765
Action reg: 0.003905
  l1.weight: grad_norm = 0.001666
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002383
Total gradient norm: 0.061358
=== Actor Training Debug (Iteration 260) ===
Q mean: -3.364839
Q std: 1.942495
Actor loss: 3.368721
Action reg: 0.003882
  l1.weight: grad_norm = 0.001782
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002736
Total gradient norm: 0.062626
=== Actor Training Debug (Iteration 261) ===
Q mean: -3.008756
Q std: 1.865661
Actor loss: 3.012681
Action reg: 0.003925
  l1.weight: grad_norm = 0.001296
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001581
Total gradient norm: 0.021964
=== Actor Training Debug (Iteration 262) ===
Q mean: -2.926932
Q std: 1.811689
Actor loss: 2.930871
Action reg: 0.003938
  l1.weight: grad_norm = 0.001144
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001573
Total gradient norm: 0.036493
=== Actor Training Debug (Iteration 263) ===
Q mean: -3.151366
Q std: 1.957038
Actor loss: 3.155297
Action reg: 0.003931
  l1.weight: grad_norm = 0.001528
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001952
Total gradient norm: 0.038085
=== Actor Training Debug (Iteration 264) ===
Q mean: -3.075147
Q std: 1.964952
Actor loss: 3.079067
Action reg: 0.003919
  l1.weight: grad_norm = 0.001347
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001815
Total gradient norm: 0.029000
=== Actor Training Debug (Iteration 265) ===
Q mean: -3.112254
Q std: 1.977427
Actor loss: 3.116151
Action reg: 0.003897
  l1.weight: grad_norm = 0.000969
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.001274
Total gradient norm: 0.023112
=== Actor Training Debug (Iteration 266) ===
Q mean: -3.104456
Q std: 1.902375
Actor loss: 3.108381
Action reg: 0.003925
  l1.weight: grad_norm = 0.001344
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001780
Total gradient norm: 0.041534
=== Actor Training Debug (Iteration 267) ===
Q mean: -3.095296
Q std: 1.996488
Actor loss: 3.099209
Action reg: 0.003912
  l1.weight: grad_norm = 0.001781
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002145
Total gradient norm: 0.032512
=== Actor Training Debug (Iteration 268) ===
Q mean: -2.758850
Q std: 1.916597
Actor loss: 2.762788
Action reg: 0.003938
  l1.weight: grad_norm = 0.001350
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001640
Total gradient norm: 0.031438
=== Actor Training Debug (Iteration 269) ===
Q mean: -2.913167
Q std: 2.057289
Actor loss: 2.917084
Action reg: 0.003917
  l1.weight: grad_norm = 0.000946
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.001114
Total gradient norm: 0.017411
=== Actor Training Debug (Iteration 270) ===
Q mean: -3.158147
Q std: 1.940265
Actor loss: 3.162070
Action reg: 0.003923
  l1.weight: grad_norm = 0.000764
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000912
Total gradient norm: 0.014041
=== Actor Training Debug (Iteration 271) ===
Q mean: -3.129692
Q std: 2.009577
Actor loss: 3.133639
Action reg: 0.003947
  l1.weight: grad_norm = 0.001471
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001752
Total gradient norm: 0.032340
=== Actor Training Debug (Iteration 272) ===
Q mean: -3.064518
Q std: 1.991772
Actor loss: 3.068405
Action reg: 0.003887
  l1.weight: grad_norm = 0.001092
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.001319
Total gradient norm: 0.018576
=== Actor Training Debug (Iteration 273) ===
Q mean: -3.131426
Q std: 2.062197
Actor loss: 3.135371
Action reg: 0.003945
  l1.weight: grad_norm = 0.000981
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001148
Total gradient norm: 0.018719
=== Actor Training Debug (Iteration 274) ===
Q mean: -3.112437
Q std: 2.081427
Actor loss: 3.116372
Action reg: 0.003935
  l1.weight: grad_norm = 0.000770
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000914
Total gradient norm: 0.015662
=== Actor Training Debug (Iteration 275) ===
Q mean: -3.076259
Q std: 2.064935
Actor loss: 3.080196
Action reg: 0.003937
  l1.weight: grad_norm = 0.001697
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002140
Total gradient norm: 0.031774
=== Actor Training Debug (Iteration 276) ===
Q mean: -2.790066
Q std: 1.986289
Actor loss: 2.793953
Action reg: 0.003887
  l1.weight: grad_norm = 0.001137
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001533
Total gradient norm: 0.035096
=== Actor Training Debug (Iteration 277) ===
Q mean: -3.078393
Q std: 2.068737
Actor loss: 3.082336
Action reg: 0.003943
  l1.weight: grad_norm = 0.002048
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002857
Total gradient norm: 0.063473
=== Actor Training Debug (Iteration 278) ===
Q mean: -3.141935
Q std: 1.971744
Actor loss: 3.145888
Action reg: 0.003952
  l1.weight: grad_norm = 0.000560
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000629
Total gradient norm: 0.008682
=== Actor Training Debug (Iteration 279) ===
Q mean: -3.029787
Q std: 1.861048
Actor loss: 3.033712
Action reg: 0.003926
  l1.weight: grad_norm = 0.002132
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002819
Total gradient norm: 0.067038
=== Actor Training Debug (Iteration 280) ===
Q mean: -2.900197
Q std: 2.052115
Actor loss: 2.904065
Action reg: 0.003868
  l1.weight: grad_norm = 0.001565
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.001888
Total gradient norm: 0.027303
=== Actor Training Debug (Iteration 281) ===
Q mean: -2.762580
Q std: 1.985145
Actor loss: 2.766473
Action reg: 0.003893
  l1.weight: grad_norm = 0.001368
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.002066
Total gradient norm: 0.046587
=== Actor Training Debug (Iteration 282) ===
Q mean: -3.146831
Q std: 2.046383
Actor loss: 3.150787
Action reg: 0.003956
  l1.weight: grad_norm = 0.000873
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001205
Total gradient norm: 0.021728
=== Actor Training Debug (Iteration 283) ===
Q mean: -2.946370
Q std: 1.863749
Actor loss: 2.950276
Action reg: 0.003906
  l1.weight: grad_norm = 0.001362
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001746
Total gradient norm: 0.028293
=== Actor Training Debug (Iteration 284) ===
Q mean: -2.873702
Q std: 1.795547
Actor loss: 2.877612
Action reg: 0.003910
  l1.weight: grad_norm = 0.000920
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001052
Total gradient norm: 0.015757
=== Actor Training Debug (Iteration 285) ===
Q mean: -2.811555
Q std: 2.099730
Actor loss: 2.815458
Action reg: 0.003902
  l1.weight: grad_norm = 0.000529
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000679
Total gradient norm: 0.013060
=== Actor Training Debug (Iteration 286) ===
Q mean: -3.099758
Q std: 2.143413
Actor loss: 3.103722
Action reg: 0.003963
  l1.weight: grad_norm = 0.001319
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001832
Total gradient norm: 0.040829
=== Actor Training Debug (Iteration 287) ===
Q mean: -3.334829
Q std: 2.009579
Actor loss: 3.338768
Action reg: 0.003939
  l1.weight: grad_norm = 0.000517
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000645
Total gradient norm: 0.010570
=== Actor Training Debug (Iteration 288) ===
Q mean: -3.077078
Q std: 1.812348
Actor loss: 3.080996
Action reg: 0.003918
  l1.weight: grad_norm = 0.001013
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001508
Total gradient norm: 0.038644
=== Actor Training Debug (Iteration 289) ===
Q mean: -2.857273
Q std: 1.937869
Actor loss: 2.861212
Action reg: 0.003939
  l1.weight: grad_norm = 0.000679
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001048
Total gradient norm: 0.025655
=== Actor Training Debug (Iteration 290) ===
Q mean: -2.730933
Q std: 2.196083
Actor loss: 2.734828
Action reg: 0.003895
  l1.weight: grad_norm = 0.000633
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000731
Total gradient norm: 0.010789
=== Actor Training Debug (Iteration 291) ===
Q mean: -3.057173
Q std: 1.982027
Actor loss: 3.061078
Action reg: 0.003905
  l1.weight: grad_norm = 0.001157
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.001452
Total gradient norm: 0.024101
=== Actor Training Debug (Iteration 292) ===
Q mean: -3.090879
Q std: 1.965903
Actor loss: 3.094801
Action reg: 0.003923
  l1.weight: grad_norm = 0.001819
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002118
Total gradient norm: 0.026435
=== Actor Training Debug (Iteration 293) ===
Q mean: -3.605557
Q std: 2.167838
Actor loss: 3.609491
Action reg: 0.003934
  l1.weight: grad_norm = 0.001087
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001347
Total gradient norm: 0.023066
=== Actor Training Debug (Iteration 294) ===
Q mean: -3.330823
Q std: 2.118158
Actor loss: 3.334715
Action reg: 0.003892
  l1.weight: grad_norm = 0.001442
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001911
Total gradient norm: 0.041203
=== Actor Training Debug (Iteration 295) ===
Q mean: -2.890280
Q std: 2.155006
Actor loss: 2.894183
Action reg: 0.003903
  l1.weight: grad_norm = 0.001520
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001816
Total gradient norm: 0.027589
=== Actor Training Debug (Iteration 296) ===
Q mean: -3.092381
Q std: 2.347190
Actor loss: 3.096328
Action reg: 0.003948
  l1.weight: grad_norm = 0.000971
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001092
Total gradient norm: 0.013751
=== Actor Training Debug (Iteration 297) ===
Q mean: -2.837898
Q std: 2.213492
Actor loss: 2.841878
Action reg: 0.003979
  l1.weight: grad_norm = 0.000772
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000869
Total gradient norm: 0.012388
=== Actor Training Debug (Iteration 298) ===
Q mean: -2.948857
Q std: 2.042523
Actor loss: 2.952779
Action reg: 0.003921
  l1.weight: grad_norm = 0.000776
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000919
Total gradient norm: 0.010518
=== Actor Training Debug (Iteration 299) ===
Q mean: -3.224036
Q std: 2.071199
Actor loss: 3.227983
Action reg: 0.003947
  l1.weight: grad_norm = 0.001481
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002120
Total gradient norm: 0.045784
=== Actor Training Debug (Iteration 300) ===
Q mean: -3.245093
Q std: 2.135823
Actor loss: 3.249032
Action reg: 0.003940
  l1.weight: grad_norm = 0.000671
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000837
Total gradient norm: 0.011494
=== Actor Training Debug (Iteration 301) ===
Q mean: -3.116683
Q std: 2.174684
Actor loss: 3.120597
Action reg: 0.003913
  l1.weight: grad_norm = 0.000822
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000984
Total gradient norm: 0.014997
=== Actor Training Debug (Iteration 302) ===
Q mean: -2.857112
Q std: 2.224853
Actor loss: 2.861042
Action reg: 0.003929
  l1.weight: grad_norm = 0.000755
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000961
Total gradient norm: 0.015342
=== Actor Training Debug (Iteration 303) ===
Q mean: -2.962802
Q std: 2.000782
Actor loss: 2.966754
Action reg: 0.003951
  l1.weight: grad_norm = 0.001443
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001878
Total gradient norm: 0.039949
=== Actor Training Debug (Iteration 304) ===
Q mean: -3.281570
Q std: 2.256394
Actor loss: 3.285535
Action reg: 0.003965
  l1.weight: grad_norm = 0.001317
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001702
Total gradient norm: 0.032688
=== Actor Training Debug (Iteration 305) ===
Q mean: -3.061746
Q std: 1.991039
Actor loss: 3.065709
Action reg: 0.003964
  l1.weight: grad_norm = 0.001034
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001273
Total gradient norm: 0.019364
=== Actor Training Debug (Iteration 306) ===
Q mean: -2.671821
Q std: 2.085638
Actor loss: 2.675786
Action reg: 0.003965
  l1.weight: grad_norm = 0.001172
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001437
Total gradient norm: 0.022492
=== Actor Training Debug (Iteration 307) ===
Q mean: -2.898561
Q std: 2.020913
Actor loss: 2.902498
Action reg: 0.003937
  l1.weight: grad_norm = 0.000481
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.000639
Total gradient norm: 0.012867
=== Actor Training Debug (Iteration 308) ===
Q mean: -3.228488
Q std: 2.162653
Actor loss: 3.232422
Action reg: 0.003934
  l1.weight: grad_norm = 0.001281
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.001952
Total gradient norm: 0.044793
=== Actor Training Debug (Iteration 309) ===
Q mean: -3.413408
Q std: 2.083978
Actor loss: 3.417377
Action reg: 0.003969
  l1.weight: grad_norm = 0.000589
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000738
Total gradient norm: 0.013205
=== Actor Training Debug (Iteration 310) ===
Q mean: -3.325966
Q std: 2.174398
Actor loss: 3.329899
Action reg: 0.003933
  l1.weight: grad_norm = 0.001229
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001600
Total gradient norm: 0.035903
=== Actor Training Debug (Iteration 311) ===
Q mean: -2.700650
Q std: 2.141884
Actor loss: 2.704593
Action reg: 0.003943
  l1.weight: grad_norm = 0.001488
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001717
Total gradient norm: 0.024695
=== Actor Training Debug (Iteration 312) ===
Q mean: -2.866495
Q std: 2.150214
Actor loss: 2.870461
Action reg: 0.003966
  l1.weight: grad_norm = 0.000866
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001072
Total gradient norm: 0.019283
=== Actor Training Debug (Iteration 313) ===
Q mean: -3.547608
Q std: 2.284149
Actor loss: 3.551577
Action reg: 0.003968
  l1.weight: grad_norm = 0.000987
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001095
Total gradient norm: 0.015176
=== Actor Training Debug (Iteration 314) ===
Q mean: -3.337266
Q std: 2.189320
Actor loss: 3.341216
Action reg: 0.003950
  l1.weight: grad_norm = 0.000860
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001333
Total gradient norm: 0.034551
=== Actor Training Debug (Iteration 315) ===
Q mean: -2.770082
Q std: 2.005812
Actor loss: 2.774033
Action reg: 0.003952
  l1.weight: grad_norm = 0.001320
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001768
Total gradient norm: 0.032090
=== Actor Training Debug (Iteration 316) ===
Q mean: -2.711245
Q std: 2.251720
Actor loss: 2.715115
Action reg: 0.003870
  l1.weight: grad_norm = 0.000411
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.000476
Total gradient norm: 0.008715
=== Actor Training Debug (Iteration 317) ===
Q mean: -3.240525
Q std: 2.264307
Actor loss: 3.244447
Action reg: 0.003923
  l1.weight: grad_norm = 0.001086
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001698
Total gradient norm: 0.041434
=== Actor Training Debug (Iteration 318) ===
Q mean: -3.265156
Q std: 1.948284
Actor loss: 3.269139
Action reg: 0.003983
  l1.weight: grad_norm = 0.000915
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001123
Total gradient norm: 0.016729
=== Actor Training Debug (Iteration 319) ===
Q mean: -2.993645
Q std: 2.131358
Actor loss: 2.997585
Action reg: 0.003940
  l1.weight: grad_norm = 0.000835
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001050
Total gradient norm: 0.016418
=== Actor Training Debug (Iteration 320) ===
Q mean: -2.594328
Q std: 2.108689
Actor loss: 2.598264
Action reg: 0.003936
  l1.weight: grad_norm = 0.000797
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.001088
Total gradient norm: 0.022641
=== Actor Training Debug (Iteration 321) ===
Q mean: -3.001021
Q std: 2.068735
Actor loss: 3.004943
Action reg: 0.003922
  l1.weight: grad_norm = 0.001273
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.001792
Total gradient norm: 0.031619
=== Actor Training Debug (Iteration 322) ===
Q mean: -3.320492
Q std: 2.274469
Actor loss: 3.324416
Action reg: 0.003925
  l1.weight: grad_norm = 0.000664
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.000918
Total gradient norm: 0.017622
=== Actor Training Debug (Iteration 323) ===
Q mean: -3.134277
Q std: 1.991154
Actor loss: 3.138251
Action reg: 0.003974
  l1.weight: grad_norm = 0.000536
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000664
Total gradient norm: 0.012105
=== Actor Training Debug (Iteration 324) ===
Q mean: -2.950615
Q std: 2.010797
Actor loss: 2.954588
Action reg: 0.003973
  l1.weight: grad_norm = 0.000747
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000861
Total gradient norm: 0.016918
=== Actor Training Debug (Iteration 325) ===
Q mean: -3.224817
Q std: 2.047259
Actor loss: 3.228805
Action reg: 0.003987
  l1.weight: grad_norm = 0.001460
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002162
Total gradient norm: 0.048048
=== Actor Training Debug (Iteration 326) ===
Q mean: -3.497643
Q std: 2.042172
Actor loss: 3.501552
Action reg: 0.003909
  l1.weight: grad_norm = 0.001320
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001579
Total gradient norm: 0.023692
=== Actor Training Debug (Iteration 327) ===
Q mean: -3.304940
Q std: 2.105217
Actor loss: 3.308848
Action reg: 0.003907
  l1.weight: grad_norm = 0.000737
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000923
Total gradient norm: 0.011483
=== Actor Training Debug (Iteration 328) ===
Q mean: -2.767323
Q std: 2.092968
Actor loss: 2.771222
Action reg: 0.003899
  l1.weight: grad_norm = 0.001356
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.001967
Total gradient norm: 0.029457
=== Actor Training Debug (Iteration 329) ===
Q mean: -2.710987
Q std: 2.101497
Actor loss: 2.714921
Action reg: 0.003934
  l1.weight: grad_norm = 0.001468
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002058
Total gradient norm: 0.047414
=== Actor Training Debug (Iteration 330) ===
Q mean: -2.818900
Q std: 2.206638
Actor loss: 2.822815
Action reg: 0.003916
  l1.weight: grad_norm = 0.000979
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.001258
Total gradient norm: 0.024339
=== Actor Training Debug (Iteration 331) ===
Q mean: -3.538209
Q std: 2.462183
Actor loss: 3.542160
Action reg: 0.003951
  l1.weight: grad_norm = 0.001055
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001574
Total gradient norm: 0.028055
=== Actor Training Debug (Iteration 332) ===
Q mean: -3.666945
Q std: 2.394331
Actor loss: 3.670893
Action reg: 0.003949
  l1.weight: grad_norm = 0.001061
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001172
Total gradient norm: 0.015601
=== Actor Training Debug (Iteration 333) ===
Q mean: -3.267622
Q std: 2.338797
Actor loss: 3.271550
Action reg: 0.003927
  l1.weight: grad_norm = 0.001110
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.001336
Total gradient norm: 0.018673
=== Actor Training Debug (Iteration 334) ===
Q mean: -2.805644
Q std: 2.073545
Actor loss: 2.809630
Action reg: 0.003986
  l1.weight: grad_norm = 0.000601
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000786
Total gradient norm: 0.014482
=== Actor Training Debug (Iteration 335) ===
Q mean: -2.792192
Q std: 2.179963
Actor loss: 2.796113
Action reg: 0.003922
  l1.weight: grad_norm = 0.000638
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.000920
Total gradient norm: 0.019634
=== Actor Training Debug (Iteration 336) ===
Q mean: -2.983757
Q std: 2.112373
Actor loss: 2.987703
Action reg: 0.003946
  l1.weight: grad_norm = 0.000225
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000271
Total gradient norm: 0.003722
=== Actor Training Debug (Iteration 337) ===
Q mean: -3.608805
Q std: 2.122280
Actor loss: 3.612786
Action reg: 0.003981
  l1.weight: grad_norm = 0.000197
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000230
Total gradient norm: 0.002980
=== Actor Training Debug (Iteration 338) ===
Q mean: -3.395110
Q std: 2.326173
Actor loss: 3.399075
Action reg: 0.003964
  l1.weight: grad_norm = 0.000416
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000625
Total gradient norm: 0.012852
=== Actor Training Debug (Iteration 339) ===
Q mean: -2.741735
Q std: 2.186291
Actor loss: 2.745696
Action reg: 0.003961
  l1.weight: grad_norm = 0.000566
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000788
Total gradient norm: 0.015701
=== Actor Training Debug (Iteration 340) ===
Q mean: -2.838868
Q std: 2.313680
Actor loss: 2.842805
Action reg: 0.003938
  l1.weight: grad_norm = 0.001310
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.001608
Total gradient norm: 0.028482
=== Actor Training Debug (Iteration 341) ===
Q mean: -3.082848
Q std: 2.202020
Actor loss: 3.086825
Action reg: 0.003978
  l1.weight: grad_norm = 0.000535
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000645
Total gradient norm: 0.009262
=== Actor Training Debug (Iteration 342) ===
Q mean: -3.638034
Q std: 2.209520
Actor loss: 3.641953
Action reg: 0.003918
  l1.weight: grad_norm = 0.000182
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.000223
Total gradient norm: 0.002758
=== Actor Training Debug (Iteration 343) ===
Q mean: -3.598641
Q std: 2.232588
Actor loss: 3.602595
Action reg: 0.003953
  l1.weight: grad_norm = 0.001230
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001380
Total gradient norm: 0.017173
=== Actor Training Debug (Iteration 344) ===
Q mean: -3.550257
Q std: 2.185277
Actor loss: 3.554200
Action reg: 0.003944
  l1.weight: grad_norm = 0.001372
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.001702
Total gradient norm: 0.032728
=== Actor Training Debug (Iteration 345) ===
Q mean: -3.139251
Q std: 2.295636
Actor loss: 3.143166
Action reg: 0.003915
  l1.weight: grad_norm = 0.000540
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000709
Total gradient norm: 0.012582
=== Actor Training Debug (Iteration 346) ===
Q mean: -2.447134
Q std: 2.063971
Actor loss: 2.451092
Action reg: 0.003958
  l1.weight: grad_norm = 0.000868
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001254
Total gradient norm: 0.026630
=== Actor Training Debug (Iteration 347) ===
Q mean: -2.494844
Q std: 1.953142
Actor loss: 2.498823
Action reg: 0.003979
  l1.weight: grad_norm = 0.000545
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000702
Total gradient norm: 0.011373
=== Actor Training Debug (Iteration 348) ===
Q mean: -2.982409
Q std: 2.118352
Actor loss: 2.986367
Action reg: 0.003958
  l1.weight: grad_norm = 0.000863
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001296
Total gradient norm: 0.027663
=== Actor Training Debug (Iteration 349) ===
Q mean: -3.304992
Q std: 2.323482
Actor loss: 3.308949
Action reg: 0.003957
  l1.weight: grad_norm = 0.000376
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000545
Total gradient norm: 0.010229
=== Actor Training Debug (Iteration 350) ===
Q mean: -3.446380
Q std: 2.355590
Actor loss: 3.450353
Action reg: 0.003973
  l1.weight: grad_norm = 0.001281
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001671
Total gradient norm: 0.028136
=== Actor Training Debug (Iteration 351) ===
Q mean: -3.022650
Q std: 2.188342
Actor loss: 3.026629
Action reg: 0.003980
  l1.weight: grad_norm = 0.000967
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001118
Total gradient norm: 0.014111
=== Actor Training Debug (Iteration 352) ===
Q mean: -2.971141
Q std: 2.487186
Actor loss: 2.975120
Action reg: 0.003979
  l1.weight: grad_norm = 0.000423
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000561
Total gradient norm: 0.009968
=== Actor Training Debug (Iteration 353) ===
Q mean: -2.907462
Q std: 2.181354
Actor loss: 2.911438
Action reg: 0.003977
  l1.weight: grad_norm = 0.000187
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000268
Total gradient norm: 0.005404
=== Actor Training Debug (Iteration 354) ===
Q mean: -2.934131
Q std: 2.286592
Actor loss: 2.938084
Action reg: 0.003953
  l1.weight: grad_norm = 0.001275
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001841
Total gradient norm: 0.036708
=== Actor Training Debug (Iteration 355) ===
Q mean: -3.064058
Q std: 2.271800
Actor loss: 3.067942
Action reg: 0.003884
  l1.weight: grad_norm = 0.000378
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.000470
Total gradient norm: 0.006705
=== Actor Training Debug (Iteration 356) ===
Q mean: -3.200267
Q std: 2.410261
Actor loss: 3.204214
Action reg: 0.003947
  l1.weight: grad_norm = 0.000486
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.000612
Total gradient norm: 0.009534
=== Actor Training Debug (Iteration 357) ===
Q mean: -3.029059
Q std: 2.265437
Actor loss: 3.032969
Action reg: 0.003910
  l1.weight: grad_norm = 0.000440
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.000461
Total gradient norm: 0.004950
=== Actor Training Debug (Iteration 358) ===
Q mean: -2.742767
Q std: 2.063400
Actor loss: 2.746668
Action reg: 0.003901
  l1.weight: grad_norm = 0.000305
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.000364
Total gradient norm: 0.005123
=== Actor Training Debug (Iteration 359) ===
Q mean: -2.806821
Q std: 1.988256
Actor loss: 2.810791
Action reg: 0.003970
  l1.weight: grad_norm = 0.000782
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000989
Total gradient norm: 0.016653
=== Actor Training Debug (Iteration 360) ===
Q mean: -3.029477
Q std: 2.387614
Actor loss: 3.033393
Action reg: 0.003916
  l1.weight: grad_norm = 0.000418
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.000537
Total gradient norm: 0.010037
=== Actor Training Debug (Iteration 361) ===
Q mean: -3.161200
Q std: 2.356078
Actor loss: 3.165131
Action reg: 0.003931
  l1.weight: grad_norm = 0.000800
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000986
Total gradient norm: 0.011881
=== Actor Training Debug (Iteration 362) ===
Q mean: -3.409988
Q std: 2.268187
Actor loss: 3.413920
Action reg: 0.003932
  l1.weight: grad_norm = 0.000274
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000373
Total gradient norm: 0.006092
=== Actor Training Debug (Iteration 363) ===
Q mean: -3.516297
Q std: 2.384039
Actor loss: 3.520259
Action reg: 0.003962
  l1.weight: grad_norm = 0.000875
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001112
Total gradient norm: 0.021279
=== Actor Training Debug (Iteration 364) ===
Q mean: -3.004995
Q std: 2.202972
Actor loss: 3.008972
Action reg: 0.003976
  l1.weight: grad_norm = 0.000289
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000399
Total gradient norm: 0.007657
=== Actor Training Debug (Iteration 365) ===
Q mean: -2.671400
Q std: 2.395965
Actor loss: 2.675331
Action reg: 0.003931
  l1.weight: grad_norm = 0.000494
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.000648
Total gradient norm: 0.012723
=== Actor Training Debug (Iteration 366) ===
Q mean: -2.860245
Q std: 2.209686
Actor loss: 2.864186
Action reg: 0.003941
  l1.weight: grad_norm = 0.000839
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001097
Total gradient norm: 0.021016
=== Actor Training Debug (Iteration 367) ===
Q mean: -3.122709
Q std: 2.148487
Actor loss: 3.126640
Action reg: 0.003931
  l1.weight: grad_norm = 0.000369
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000422
Total gradient norm: 0.004639
=== Actor Training Debug (Iteration 368) ===
Q mean: -3.506827
Q std: 2.445764
Actor loss: 3.510807
Action reg: 0.003980
  l1.weight: grad_norm = 0.000334
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000435
Total gradient norm: 0.007521
=== Actor Training Debug (Iteration 369) ===
Q mean: -3.237735
Q std: 2.370524
Actor loss: 3.241696
Action reg: 0.003961
  l1.weight: grad_norm = 0.000325
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000411
Total gradient norm: 0.005976
=== Actor Training Debug (Iteration 370) ===
Q mean: -3.166048
Q std: 2.358663
Actor loss: 3.170036
Action reg: 0.003988
  l1.weight: grad_norm = 0.000775
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000921
Total gradient norm: 0.012723
=== Actor Training Debug (Iteration 371) ===
Q mean: -3.081468
Q std: 2.241221
Actor loss: 3.085413
Action reg: 0.003944
  l1.weight: grad_norm = 0.000468
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000714
Total gradient norm: 0.015190
=== Actor Training Debug (Iteration 372) ===
Q mean: -2.941291
Q std: 2.050586
Actor loss: 2.945197
Action reg: 0.003905
  l1.weight: grad_norm = 0.000302
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.000424
Total gradient norm: 0.008070
=== Actor Training Debug (Iteration 373) ===
Q mean: -3.299460
Q std: 2.344550
Actor loss: 3.303426
Action reg: 0.003966
  l1.weight: grad_norm = 0.000249
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000364
Total gradient norm: 0.007913
=== Actor Training Debug (Iteration 374) ===
Q mean: -2.828486
Q std: 2.090687
Actor loss: 2.832484
Action reg: 0.003998
  l1.weight: grad_norm = 0.000078
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000102
Total gradient norm: 0.001838
=== Actor Training Debug (Iteration 375) ===
Q mean: -2.953054
Q std: 2.381690
Actor loss: 2.957006
Action reg: 0.003951
  l1.weight: grad_norm = 0.001085
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.001346
Total gradient norm: 0.022722
=== Actor Training Debug (Iteration 376) ===
Q mean: -2.907911
Q std: 2.236720
Actor loss: 2.911830
Action reg: 0.003919
  l1.weight: grad_norm = 0.000990
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001400
Total gradient norm: 0.024297
=== Actor Training Debug (Iteration 377) ===
Q mean: -3.220490
Q std: 2.198706
Actor loss: 3.224452
Action reg: 0.003962
  l1.weight: grad_norm = 0.000459
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.000505
Total gradient norm: 0.007880
=== Actor Training Debug (Iteration 378) ===
Q mean: -3.171790
Q std: 2.003580
Actor loss: 3.175755
Action reg: 0.003964
  l1.weight: grad_norm = 0.000338
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000445
Total gradient norm: 0.006964
=== Actor Training Debug (Iteration 379) ===
Q mean: -3.350888
Q std: 2.074132
Actor loss: 3.354882
Action reg: 0.003994
  l1.weight: grad_norm = 0.000093
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000123
Total gradient norm: 0.002615
=== Actor Training Debug (Iteration 380) ===
Q mean: -3.125144
Q std: 2.385355
Actor loss: 3.129076
Action reg: 0.003932
  l1.weight: grad_norm = 0.000429
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000503
Total gradient norm: 0.006951
=== Actor Training Debug (Iteration 381) ===
Q mean: -2.888547
Q std: 2.169492
Actor loss: 2.892543
Action reg: 0.003995
  l1.weight: grad_norm = 0.000559
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000670
Total gradient norm: 0.008302
=== Actor Training Debug (Iteration 382) ===
Q mean: -3.366251
Q std: 2.504899
Actor loss: 3.370226
Action reg: 0.003975
  l1.weight: grad_norm = 0.001475
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001878
Total gradient norm: 0.025955
=== Actor Training Debug (Iteration 383) ===
Q mean: -3.394580
Q std: 2.500824
Actor loss: 3.398554
Action reg: 0.003973
  l1.weight: grad_norm = 0.000629
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000963
Total gradient norm: 0.020153
=== Actor Training Debug (Iteration 384) ===
Q mean: -3.350474
Q std: 2.718910
Actor loss: 3.354461
Action reg: 0.003987
  l1.weight: grad_norm = 0.000757
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000956
Total gradient norm: 0.016439
=== Actor Training Debug (Iteration 385) ===
Q mean: -2.924952
Q std: 2.584061
Actor loss: 2.928908
Action reg: 0.003956
  l1.weight: grad_norm = 0.000657
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000720
Total gradient norm: 0.008032
=== Actor Training Debug (Iteration 386) ===
Q mean: -2.926549
Q std: 2.357698
Actor loss: 2.930528
Action reg: 0.003979
  l1.weight: grad_norm = 0.001026
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001207
Total gradient norm: 0.016570
=== Actor Training Debug (Iteration 387) ===
Q mean: -2.938095
Q std: 2.376223
Actor loss: 2.942034
Action reg: 0.003939
  l1.weight: grad_norm = 0.000405
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000470
Total gradient norm: 0.005986
=== Actor Training Debug (Iteration 388) ===
Q mean: -3.233442
Q std: 2.302472
Actor loss: 3.237429
Action reg: 0.003987
  l1.weight: grad_norm = 0.001729
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002418
Total gradient norm: 0.049645
=== Actor Training Debug (Iteration 389) ===
Q mean: -3.077812
Q std: 2.342887
Actor loss: 3.081757
Action reg: 0.003944
  l1.weight: grad_norm = 0.000251
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.000291
Total gradient norm: 0.003647
=== Actor Training Debug (Iteration 390) ===
Q mean: -3.109658
Q std: 2.168198
Actor loss: 3.113611
Action reg: 0.003953
  l1.weight: grad_norm = 0.000735
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000871
Total gradient norm: 0.012100
=== Actor Training Debug (Iteration 391) ===
Q mean: -3.349666
Q std: 2.408736
Actor loss: 3.353631
Action reg: 0.003965
  l1.weight: grad_norm = 0.000102
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000127
Total gradient norm: 0.001977
=== Actor Training Debug (Iteration 392) ===
Q mean: -3.205732
Q std: 2.311900
Actor loss: 3.209648
Action reg: 0.003916
  l1.weight: grad_norm = 0.000567
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.000650
Total gradient norm: 0.008689
=== Actor Training Debug (Iteration 393) ===
Q mean: -3.189943
Q std: 2.605435
Actor loss: 3.193861
Action reg: 0.003918
  l1.weight: grad_norm = 0.000415
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.000535
Total gradient norm: 0.006613
=== Actor Training Debug (Iteration 394) ===
Q mean: -3.120689
Q std: 2.369279
Actor loss: 3.124650
Action reg: 0.003962
  l1.weight: grad_norm = 0.000493
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000643
Total gradient norm: 0.011027
=== Actor Training Debug (Iteration 395) ===
Q mean: -3.125325
Q std: 2.422879
Actor loss: 3.129230
Action reg: 0.003905
  l1.weight: grad_norm = 0.000594
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.000747
Total gradient norm: 0.012533
=== Actor Training Debug (Iteration 396) ===
Q mean: -3.195336
Q std: 2.430443
Actor loss: 3.199317
Action reg: 0.003981
  l1.weight: grad_norm = 0.000104
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000139
Total gradient norm: 0.002263
=== Actor Training Debug (Iteration 397) ===
Q mean: -3.639720
Q std: 2.584138
Actor loss: 3.643702
Action reg: 0.003982
  l1.weight: grad_norm = 0.000086
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000118
Total gradient norm: 0.002165
=== Actor Training Debug (Iteration 398) ===
Q mean: -3.506174
Q std: 2.382611
Actor loss: 3.510144
Action reg: 0.003970
  l1.weight: grad_norm = 0.000037
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000053
Total gradient norm: 0.000978
=== Actor Training Debug (Iteration 399) ===
Q mean: -3.130044
Q std: 2.172376
Actor loss: 3.134026
Action reg: 0.003982
  l1.weight: grad_norm = 0.000149
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000207
Total gradient norm: 0.003737
=== Actor Training Debug (Iteration 400) ===
Q mean: -3.105648
Q std: 2.222237
Actor loss: 3.109631
Action reg: 0.003983
  l1.weight: grad_norm = 0.000183
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000212
Total gradient norm: 0.002826
=== Actor Training Debug (Iteration 401) ===
Q mean: -3.089638
Q std: 2.242489
Actor loss: 3.093603
Action reg: 0.003965
  l1.weight: grad_norm = 0.000664
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000771
Total gradient norm: 0.011965
=== Actor Training Debug (Iteration 402) ===
Q mean: -3.165447
Q std: 2.538533
Actor loss: 3.169371
Action reg: 0.003924
  l1.weight: grad_norm = 0.000026
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.000053
Total gradient norm: 0.000794
=== Actor Training Debug (Iteration 403) ===
Q mean: -3.143921
Q std: 2.243078
Actor loss: 3.147904
Action reg: 0.003983
  l1.weight: grad_norm = 0.000055
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000075
Total gradient norm: 0.001613
=== Actor Training Debug (Iteration 404) ===
Q mean: -3.677171
Q std: 2.652722
Actor loss: 3.681121
Action reg: 0.003951
  l1.weight: grad_norm = 0.000343
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.000400
Total gradient norm: 0.006553
=== Actor Training Debug (Iteration 405) ===
Q mean: -2.994315
Q std: 2.365639
Actor loss: 2.998309
Action reg: 0.003994
  l1.weight: grad_norm = 0.000345
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000403
Total gradient norm: 0.006947
=== Actor Training Debug (Iteration 406) ===
Q mean: -2.693233
Q std: 2.302320
Actor loss: 2.697198
Action reg: 0.003965
  l1.weight: grad_norm = 0.000877
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001018
Total gradient norm: 0.012320
=== Actor Training Debug (Iteration 407) ===
Q mean: -2.883840
Q std: 2.334049
Actor loss: 2.887835
Action reg: 0.003996
  l1.weight: grad_norm = 0.000358
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000475
Total gradient norm: 0.008362
=== Actor Training Debug (Iteration 408) ===
Q mean: -3.103580
Q std: 2.245967
Actor loss: 3.107547
Action reg: 0.003967
  l1.weight: grad_norm = 0.000088
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000120
Total gradient norm: 0.002226
=== Actor Training Debug (Iteration 409) ===
Q mean: -3.293605
Q std: 2.615810
Actor loss: 3.297539
Action reg: 0.003934
  l1.weight: grad_norm = 0.000699
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000917
Total gradient norm: 0.014169
=== Actor Training Debug (Iteration 410) ===
Q mean: -3.635095
Q std: 2.467617
Actor loss: 3.639025
Action reg: 0.003931
  l1.weight: grad_norm = 0.000488
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.000665
Total gradient norm: 0.010568
=== Actor Training Debug (Iteration 411) ===
Q mean: -3.447693
Q std: 2.504404
Actor loss: 3.451674
Action reg: 0.003981
  l1.weight: grad_norm = 0.000267
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000321
Total gradient norm: 0.003824
=== Actor Training Debug (Iteration 412) ===
Q mean: -2.826229
Q std: 2.327947
Actor loss: 2.830204
Action reg: 0.003976
  l1.weight: grad_norm = 0.000570
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000616
Total gradient norm: 0.007648
=== Actor Training Debug (Iteration 413) ===
Q mean: -2.808391
Q std: 2.501511
Actor loss: 2.812368
Action reg: 0.003977
  l1.weight: grad_norm = 0.000711
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000944
Total gradient norm: 0.017740
=== Actor Training Debug (Iteration 414) ===
Q mean: -3.388436
Q std: 2.492776
Actor loss: 3.392389
Action reg: 0.003953
  l1.weight: grad_norm = 0.000037
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000049
Total gradient norm: 0.000681
=== Actor Training Debug (Iteration 415) ===
Q mean: -3.747437
Q std: 2.473994
Actor loss: 3.751405
Action reg: 0.003968
  l1.weight: grad_norm = 0.000089
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000122
Total gradient norm: 0.002360
=== Actor Training Debug (Iteration 416) ===
Q mean: -4.226678
Q std: 2.717126
Actor loss: 4.230597
Action reg: 0.003919
  l1.weight: grad_norm = 0.001149
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.001332
Total gradient norm: 0.020247
=== Actor Training Debug (Iteration 417) ===
Q mean: -3.925592
Q std: 2.541415
Actor loss: 3.929556
Action reg: 0.003964
  l1.weight: grad_norm = 0.000280
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000359
Total gradient norm: 0.006206
=== Actor Training Debug (Iteration 418) ===
Q mean: -3.342259
Q std: 2.367619
Actor loss: 3.346226
Action reg: 0.003967
  l1.weight: grad_norm = 0.000244
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000317
Total gradient norm: 0.005009
=== Actor Training Debug (Iteration 419) ===
Q mean: -2.655103
Q std: 2.340164
Actor loss: 2.659054
Action reg: 0.003951
  l1.weight: grad_norm = 0.000350
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000479
Total gradient norm: 0.008838
=== Actor Training Debug (Iteration 420) ===
Q mean: -2.515934
Q std: 2.496058
Actor loss: 2.519884
Action reg: 0.003950
  l1.weight: grad_norm = 0.000215
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000231
Total gradient norm: 0.002737
=== Actor Training Debug (Iteration 421) ===
Q mean: -2.972690
Q std: 2.530006
Actor loss: 2.976655
Action reg: 0.003965
  l1.weight: grad_norm = 0.000440
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000559
Total gradient norm: 0.006821
=== Actor Training Debug (Iteration 422) ===
Q mean: -3.594333
Q std: 2.633750
Actor loss: 3.598271
Action reg: 0.003938
  l1.weight: grad_norm = 0.000188
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.000232
Total gradient norm: 0.003254
=== Actor Training Debug (Iteration 423) ===
Q mean: -3.579734
Q std: 2.608227
Actor loss: 3.583667
Action reg: 0.003933
  l1.weight: grad_norm = 0.000561
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.000747
Total gradient norm: 0.012232
=== Actor Training Debug (Iteration 424) ===
Q mean: -3.460294
Q std: 2.595661
Actor loss: 3.464260
Action reg: 0.003966
  l1.weight: grad_norm = 0.000272
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.000355
Total gradient norm: 0.005715
=== Actor Training Debug (Iteration 425) ===
Q mean: -2.847174
Q std: 2.246326
Actor loss: 2.851140
Action reg: 0.003966
  l1.weight: grad_norm = 0.000488
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000577
Total gradient norm: 0.008236
=== Actor Training Debug (Iteration 426) ===
Q mean: -3.031393
Q std: 2.530942
Actor loss: 3.035346
Action reg: 0.003953
  l1.weight: grad_norm = 0.000315
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.000418
Total gradient norm: 0.007745
=== Actor Training Debug (Iteration 427) ===
Q mean: -3.084109
Q std: 2.486134
Actor loss: 3.088061
Action reg: 0.003952
  l1.weight: grad_norm = 0.000152
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.000210
Total gradient norm: 0.003740
=== Actor Training Debug (Iteration 428) ===
Q mean: -3.786269
Q std: 2.499985
Actor loss: 3.790249
Action reg: 0.003980
  l1.weight: grad_norm = 0.000692
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000897
Total gradient norm: 0.016037
=== Actor Training Debug (Iteration 429) ===
Q mean: -4.219798
Q std: 2.710025
Actor loss: 4.223781
Action reg: 0.003983
  l1.weight: grad_norm = 0.000581
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000739
Total gradient norm: 0.013549
=== Actor Training Debug (Iteration 430) ===
Q mean: -3.713753
Q std: 2.653301
Actor loss: 3.717689
Action reg: 0.003937
  l1.weight: grad_norm = 0.000703
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.000972
Total gradient norm: 0.016712
=== Actor Training Debug (Iteration 431) ===
Q mean: -2.863356
Q std: 2.504851
Actor loss: 2.867278
Action reg: 0.003922
  l1.weight: grad_norm = 0.000143
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.000175
Total gradient norm: 0.002320
=== Actor Training Debug (Iteration 432) ===
Q mean: -2.480433
Q std: 2.182386
Actor loss: 2.484400
Action reg: 0.003967
  l1.weight: grad_norm = 0.000263
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.000345
Total gradient norm: 0.004564
=== Actor Training Debug (Iteration 433) ===
Q mean: -3.115233
Q std: 2.440055
Actor loss: 3.119217
Action reg: 0.003984
  l1.weight: grad_norm = 0.000036
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000051
Total gradient norm: 0.000917
=== Actor Training Debug (Iteration 434) ===
Q mean: -3.585693
Q std: 2.616812
Actor loss: 3.589677
Action reg: 0.003985
  l1.weight: grad_norm = 0.000013
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000017
Total gradient norm: 0.000233
=== Actor Training Debug (Iteration 435) ===
Q mean: -3.958347
Q std: 2.671169
Actor loss: 3.962286
Action reg: 0.003939
  l1.weight: grad_norm = 0.000133
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.000185
Total gradient norm: 0.002382
=== Actor Training Debug (Iteration 436) ===
Q mean: -3.546554
Q std: 2.413609
Actor loss: 3.550524
Action reg: 0.003971
  l1.weight: grad_norm = 0.000014
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000027
Total gradient norm: 0.000316
=== Actor Training Debug (Iteration 437) ===
Q mean: -2.734382
Q std: 2.279775
Actor loss: 2.738366
Action reg: 0.003984
  l1.weight: grad_norm = 0.000123
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000162
Total gradient norm: 0.002081
=== Actor Training Debug (Iteration 438) ===
Q mean: -2.997134
Q std: 2.397167
Actor loss: 3.001102
Action reg: 0.003969
  l1.weight: grad_norm = 0.000202
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000273
Total gradient norm: 0.003589
=== Actor Training Debug (Iteration 439) ===
Q mean: -3.038867
Q std: 2.396139
Actor loss: 3.042837
Action reg: 0.003970
  l1.weight: grad_norm = 0.000032
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000046
Total gradient norm: 0.000670
=== Actor Training Debug (Iteration 440) ===
Q mean: -3.788491
Q std: 2.592424
Actor loss: 3.792475
Action reg: 0.003985
  l1.weight: grad_norm = 0.000180
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000223
Total gradient norm: 0.003212
=== Actor Training Debug (Iteration 441) ===
Q mean: -3.775828
Q std: 2.705970
Actor loss: 3.779813
Action reg: 0.003985
  l1.weight: grad_norm = 0.000026
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000037
Total gradient norm: 0.000447
=== Actor Training Debug (Iteration 442) ===
Q mean: -3.385766
Q std: 2.356952
Actor loss: 3.389719
Action reg: 0.003953
  l1.weight: grad_norm = 0.000256
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.000338
Total gradient norm: 0.004799
=== Actor Training Debug (Iteration 443) ===
Q mean: -3.159119
Q std: 2.310726
Actor loss: 3.163074
Action reg: 0.003955
  l1.weight: grad_norm = 0.000035
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.000046
Total gradient norm: 0.000687
=== Actor Training Debug (Iteration 444) ===
Q mean: -3.283467
Q std: 2.523181
Actor loss: 3.287451
Action reg: 0.003984
  l1.weight: grad_norm = 0.000119
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000160
Total gradient norm: 0.001885
=== Actor Training Debug (Iteration 445) ===
Q mean: -3.271533
Q std: 2.703729
Actor loss: 3.275473
Action reg: 0.003940
  l1.weight: grad_norm = 0.000033
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.000062
Total gradient norm: 0.000703
=== Actor Training Debug (Iteration 446) ===
Q mean: -3.368567
Q std: 2.591971
Actor loss: 3.372552
Action reg: 0.003985
  l1.weight: grad_norm = 0.000017
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000027
Total gradient norm: 0.000206
=== Actor Training Debug (Iteration 447) ===
Q mean: -3.215165
Q std: 2.305211
Actor loss: 3.219150
Action reg: 0.003985
  l1.weight: grad_norm = 0.000015
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000024
Total gradient norm: 0.000237
=== Actor Training Debug (Iteration 448) ===
Q mean: -3.324618
Q std: 2.467433
Actor loss: 3.328573
Action reg: 0.003955
  l1.weight: grad_norm = 0.000599
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.000829
Total gradient norm: 0.010516
=== Actor Training Debug (Iteration 449) ===
Q mean: -3.293409
Q std: 2.401847
Actor loss: 3.297363
Action reg: 0.003954
  l1.weight: grad_norm = 0.001340
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.001763
Total gradient norm: 0.019911
=== Actor Training Debug (Iteration 450) ===
Q mean: -3.311238
Q std: 2.634760
Actor loss: 3.315205
Action reg: 0.003967
  l1.weight: grad_norm = 0.001530
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002337
Total gradient norm: 0.023815
=== Actor Training Debug (Iteration 451) ===
Q mean: -3.594938
Q std: 2.831833
Actor loss: 3.598902
Action reg: 0.003964
  l1.weight: grad_norm = 0.000841
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001183
Total gradient norm: 0.013030
=== Actor Training Debug (Iteration 452) ===
Q mean: -3.364447
Q std: 2.605669
Actor loss: 3.368414
Action reg: 0.003967
  l1.weight: grad_norm = 0.000105
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000129
Total gradient norm: 0.001807
=== Actor Training Debug (Iteration 453) ===
Q mean: -3.086418
Q std: 2.484241
Actor loss: 3.090366
Action reg: 0.003948
  l1.weight: grad_norm = 0.001026
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.001710
Total gradient norm: 0.019591
=== Actor Training Debug (Iteration 454) ===
Q mean: -3.395398
Q std: 2.551433
Actor loss: 3.399328
Action reg: 0.003930
  l1.weight: grad_norm = 0.003654
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.006523
Total gradient norm: 0.068585
=== Actor Training Debug (Iteration 455) ===
Q mean: -3.992298
Q std: 2.705611
Actor loss: 3.996216
Action reg: 0.003918
  l1.weight: grad_norm = 0.006866
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.008733
Total gradient norm: 0.077925
=== Actor Training Debug (Iteration 456) ===
Q mean: -3.262639
Q std: 2.391102
Actor loss: 3.266587
Action reg: 0.003948
  l1.weight: grad_norm = 0.005434
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.010219
Total gradient norm: 0.092575
=== Actor Training Debug (Iteration 457) ===
Q mean: -3.094276
Q std: 2.490490
Actor loss: 3.098216
Action reg: 0.003940
  l1.weight: grad_norm = 0.010967
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.017389
Total gradient norm: 0.178987
=== Actor Training Debug (Iteration 458) ===
Q mean: -3.619435
Q std: 2.794983
Actor loss: 3.623328
Action reg: 0.003893
  l1.weight: grad_norm = 0.009094
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.010881
Total gradient norm: 0.121152
=== Actor Training Debug (Iteration 459) ===
Q mean: -3.703951
Q std: 2.917222
Actor loss: 3.707878
Action reg: 0.003927
  l1.weight: grad_norm = 0.010280
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.014431
Total gradient norm: 0.162104
=== Actor Training Debug (Iteration 460) ===
Q mean: -3.134922
Q std: 2.653163
Actor loss: 3.138812
Action reg: 0.003890
  l1.weight: grad_norm = 0.014210
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.022692
Total gradient norm: 0.234729
=== Actor Training Debug (Iteration 461) ===
Q mean: -3.042791
Q std: 2.537782
Actor loss: 3.046705
Action reg: 0.003914
  l1.weight: grad_norm = 0.008601
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.013207
Total gradient norm: 0.149594
=== Actor Training Debug (Iteration 462) ===
Q mean: -3.124954
Q std: 2.790648
Actor loss: 3.128905
Action reg: 0.003951
  l1.weight: grad_norm = 0.007327
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.008902
Total gradient norm: 0.069454
=== Actor Training Debug (Iteration 463) ===
Q mean: -3.627533
Q std: 2.632471
Actor loss: 3.631468
Action reg: 0.003935
  l1.weight: grad_norm = 0.005640
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.006539
Total gradient norm: 0.073738
=== Actor Training Debug (Iteration 464) ===
Q mean: -3.486977
Q std: 2.471099
Actor loss: 3.490911
Action reg: 0.003934
  l1.weight: grad_norm = 0.012361
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.019295
Total gradient norm: 0.150816
=== Actor Training Debug (Iteration 465) ===
Q mean: -3.044999
Q std: 2.427639
Actor loss: 3.048890
Action reg: 0.003892
  l1.weight: grad_norm = 0.012748
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.018874
Total gradient norm: 0.128126
=== Actor Training Debug (Iteration 466) ===
Q mean: -3.515819
Q std: 2.857584
Actor loss: 3.519759
Action reg: 0.003941
  l1.weight: grad_norm = 0.005852
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.008533
Total gradient norm: 0.076788
=== Actor Training Debug (Iteration 467) ===
Q mean: -3.451192
Q std: 2.658184
Actor loss: 3.455126
Action reg: 0.003933
  l1.weight: grad_norm = 0.005257
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.005708
Total gradient norm: 0.048111
=== Actor Training Debug (Iteration 468) ===
Q mean: -3.590010
Q std: 3.042162
Actor loss: 3.593915
Action reg: 0.003906
  l1.weight: grad_norm = 0.004059
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004941
Total gradient norm: 0.039172
=== Actor Training Debug (Iteration 469) ===
Q mean: -3.143473
Q std: 2.794643
Actor loss: 3.147362
Action reg: 0.003889
  l1.weight: grad_norm = 0.003781
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004292
Total gradient norm: 0.055815
=== Actor Training Debug (Iteration 470) ===
Q mean: -3.154850
Q std: 2.505069
Actor loss: 3.158790
Action reg: 0.003940
  l1.weight: grad_norm = 0.004360
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004375
Total gradient norm: 0.032441
=== Actor Training Debug (Iteration 471) ===
Q mean: -3.496591
Q std: 2.943300
Actor loss: 3.500502
Action reg: 0.003911
  l1.weight: grad_norm = 0.004472
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.005453
Total gradient norm: 0.043510
=== Actor Training Debug (Iteration 472) ===
Q mean: -3.721996
Q std: 3.019084
Actor loss: 3.725942
Action reg: 0.003946
  l1.weight: grad_norm = 0.006094
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.007601
Total gradient norm: 0.052471
=== Actor Training Debug (Iteration 473) ===
Q mean: -3.194745
Q std: 2.601753
Actor loss: 3.198692
Action reg: 0.003947
  l1.weight: grad_norm = 0.003030
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.003443
Total gradient norm: 0.027838
=== Actor Training Debug (Iteration 474) ===
Q mean: -2.849494
Q std: 2.842001
Actor loss: 2.853414
Action reg: 0.003920
  l1.weight: grad_norm = 0.002609
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.003886
Total gradient norm: 0.043803
=== Actor Training Debug (Iteration 475) ===
Q mean: -3.159541
Q std: 2.895566
Actor loss: 3.163467
Action reg: 0.003925
  l1.weight: grad_norm = 0.004755
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.005130
Total gradient norm: 0.048407
=== Actor Training Debug (Iteration 476) ===
Q mean: -3.583340
Q std: 2.680439
Actor loss: 3.587283
Action reg: 0.003943
  l1.weight: grad_norm = 0.005174
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005358
Total gradient norm: 0.036088
=== Actor Training Debug (Iteration 477) ===
Q mean: -3.832093
Q std: 2.790429
Actor loss: 3.836014
Action reg: 0.003921
  l1.weight: grad_norm = 0.004375
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.004259
Total gradient norm: 0.038376
=== Actor Training Debug (Iteration 478) ===
Q mean: -3.470894
Q std: 2.708211
Actor loss: 3.474849
Action reg: 0.003955
  l1.weight: grad_norm = 0.006136
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.005968
Total gradient norm: 0.058800
=== Actor Training Debug (Iteration 479) ===
Q mean: -3.172675
Q std: 2.524182
Actor loss: 3.176607
Action reg: 0.003932
  l1.weight: grad_norm = 0.004257
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.004402
Total gradient norm: 0.035987
=== Actor Training Debug (Iteration 480) ===
Q mean: -2.804543
Q std: 2.289107
Actor loss: 2.808509
Action reg: 0.003965
  l1.weight: grad_norm = 0.003744
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003757
Total gradient norm: 0.034413
=== Actor Training Debug (Iteration 481) ===
Q mean: -3.323963
Q std: 2.536305
Actor loss: 3.327923
Action reg: 0.003960
  l1.weight: grad_norm = 0.002520
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002488
Total gradient norm: 0.026809
=== Actor Training Debug (Iteration 482) ===
Q mean: -3.747566
Q std: 2.612159
Actor loss: 3.751479
Action reg: 0.003914
  l1.weight: grad_norm = 0.005761
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.005870
Total gradient norm: 0.041049
=== Actor Training Debug (Iteration 483) ===
Q mean: -4.052423
Q std: 3.089948
Actor loss: 4.056382
Action reg: 0.003959
  l1.weight: grad_norm = 0.002765
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.002819
Total gradient norm: 0.023716
=== Actor Training Debug (Iteration 484) ===
Q mean: -3.207287
Q std: 2.723007
Actor loss: 3.211236
Action reg: 0.003949
  l1.weight: grad_norm = 0.003778
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.003607
Total gradient norm: 0.024426
=== Actor Training Debug (Iteration 485) ===
Q mean: -3.285210
Q std: 3.011950
Actor loss: 3.289173
Action reg: 0.003963
  l1.weight: grad_norm = 0.003564
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003650
Total gradient norm: 0.030974
=== Actor Training Debug (Iteration 486) ===
Q mean: -3.487002
Q std: 2.972536
Actor loss: 3.490937
Action reg: 0.003936
  l1.weight: grad_norm = 0.002764
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.002556
Total gradient norm: 0.021703
=== Actor Training Debug (Iteration 487) ===
Q mean: -3.347842
Q std: 2.634565
Actor loss: 3.351775
Action reg: 0.003933
  l1.weight: grad_norm = 0.003321
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003277
Total gradient norm: 0.025183
=== Actor Training Debug (Iteration 488) ===
Q mean: -3.347230
Q std: 2.430189
Actor loss: 3.351134
Action reg: 0.003904
  l1.weight: grad_norm = 0.002671
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.003009
Total gradient norm: 0.024771
=== Actor Training Debug (Iteration 489) ===
Q mean: -3.040923
Q std: 2.542402
Actor loss: 3.044815
Action reg: 0.003892
  l1.weight: grad_norm = 0.004996
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.005395
Total gradient norm: 0.062578
=== Actor Training Debug (Iteration 490) ===
Q mean: -2.903669
Q std: 2.520450
Actor loss: 2.907617
Action reg: 0.003948
  l1.weight: grad_norm = 0.004220
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.004276
Total gradient norm: 0.023272
=== Actor Training Debug (Iteration 491) ===
Q mean: -3.635350
Q std: 2.865497
Actor loss: 3.639302
Action reg: 0.003952
  l1.weight: grad_norm = 0.003157
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.002946
Total gradient norm: 0.024911
=== Actor Training Debug (Iteration 492) ===
Q mean: -3.611390
Q std: 3.056446
Actor loss: 3.615305
Action reg: 0.003915
  l1.weight: grad_norm = 0.002608
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.003103
Total gradient norm: 0.031663
=== Actor Training Debug (Iteration 493) ===
Q mean: -3.405805
Q std: 2.398425
Actor loss: 3.409740
Action reg: 0.003935
  l1.weight: grad_norm = 0.004891
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.004343
Total gradient norm: 0.034936
=== Actor Training Debug (Iteration 494) ===
Q mean: -3.416331
Q std: 2.965372
Actor loss: 3.420228
Action reg: 0.003897
  l1.weight: grad_norm = 0.002794
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.002797
Total gradient norm: 0.019172
=== Actor Training Debug (Iteration 495) ===
Q mean: -3.223602
Q std: 2.656220
Actor loss: 3.227533
Action reg: 0.003931
  l1.weight: grad_norm = 0.003432
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.003202
Total gradient norm: 0.030217
=== Actor Training Debug (Iteration 496) ===
Q mean: -3.148026
Q std: 2.707952
Actor loss: 3.152003
Action reg: 0.003977
  l1.weight: grad_norm = 0.004800
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.004371
Total gradient norm: 0.033058
=== Actor Training Debug (Iteration 497) ===
Q mean: -3.827944
Q std: 2.772027
Actor loss: 3.831869
Action reg: 0.003925
  l1.weight: grad_norm = 0.004743
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.005025
Total gradient norm: 0.039829
=== Actor Training Debug (Iteration 498) ===
Q mean: -3.707530
Q std: 2.807727
Actor loss: 3.711449
Action reg: 0.003918
  l1.weight: grad_norm = 0.004329
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.004324
Total gradient norm: 0.032141
=== Actor Training Debug (Iteration 499) ===
Q mean: -3.673532
Q std: 2.983970
Actor loss: 3.677485
Action reg: 0.003953
  l1.weight: grad_norm = 0.004166
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.003701
Total gradient norm: 0.027614
=== Actor Training Debug (Iteration 500) ===
Q mean: -3.315457
Q std: 2.860585
Actor loss: 3.319426
Action reg: 0.003969
  l1.weight: grad_norm = 0.005749
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005190
Total gradient norm: 0.037515
  Average reward: -359.979 | Average length: 100.0
Evaluation at episode 55: -359.979
=== Actor Training Debug (Iteration 501) ===
Q mean: -3.393512
Q std: 2.813250
Actor loss: 3.397457
Action reg: 0.003945
  l1.weight: grad_norm = 0.003789
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003847
Total gradient norm: 0.031874
=== Actor Training Debug (Iteration 502) ===
Q mean: -3.478477
Q std: 2.803166
Actor loss: 3.482426
Action reg: 0.003949
  l1.weight: grad_norm = 0.004542
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.005682
Total gradient norm: 0.063144
=== Actor Training Debug (Iteration 503) ===
Q mean: -3.322115
Q std: 2.970320
Actor loss: 3.326068
Action reg: 0.003952
  l1.weight: grad_norm = 0.001902
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002352
Total gradient norm: 0.021816
=== Actor Training Debug (Iteration 504) ===
Q mean: -3.363304
Q std: 2.711590
Actor loss: 3.367250
Action reg: 0.003946
  l1.weight: grad_norm = 0.002489
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.002904
Total gradient norm: 0.025784
=== Actor Training Debug (Iteration 505) ===
Q mean: -3.916872
Q std: 3.063250
Actor loss: 3.920805
Action reg: 0.003934
  l1.weight: grad_norm = 0.005536
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.005759
Total gradient norm: 0.048330
=== Actor Training Debug (Iteration 506) ===
Q mean: -3.459587
Q std: 2.924951
Actor loss: 3.463531
Action reg: 0.003945
  l1.weight: grad_norm = 0.002782
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.003553
Total gradient norm: 0.032040
=== Actor Training Debug (Iteration 507) ===
Q mean: -3.490784
Q std: 2.819488
Actor loss: 3.494745
Action reg: 0.003961
  l1.weight: grad_norm = 0.006553
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005698
Total gradient norm: 0.039743
=== Actor Training Debug (Iteration 508) ===
Q mean: -3.137942
Q std: 2.819658
Actor loss: 3.141895
Action reg: 0.003952
  l1.weight: grad_norm = 0.001801
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001616
Total gradient norm: 0.014811
=== Actor Training Debug (Iteration 509) ===
Q mean: -3.470496
Q std: 2.711218
Actor loss: 3.474432
Action reg: 0.003936
  l1.weight: grad_norm = 0.003968
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.004464
Total gradient norm: 0.030974
=== Actor Training Debug (Iteration 510) ===
Q mean: -3.544859
Q std: 2.873293
Actor loss: 3.548806
Action reg: 0.003947
  l1.weight: grad_norm = 0.003941
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.003260
Total gradient norm: 0.022604
=== Actor Training Debug (Iteration 511) ===
Q mean: -3.293020
Q std: 2.788875
Actor loss: 3.296940
Action reg: 0.003920
  l1.weight: grad_norm = 0.003320
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.003066
Total gradient norm: 0.023772
=== Actor Training Debug (Iteration 512) ===
Q mean: -3.604044
Q std: 2.951295
Actor loss: 3.607973
Action reg: 0.003929
  l1.weight: grad_norm = 0.005720
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.005603
Total gradient norm: 0.042871
=== Actor Training Debug (Iteration 513) ===
Q mean: -3.476968
Q std: 2.762683
Actor loss: 3.480888
Action reg: 0.003920
  l1.weight: grad_norm = 0.005323
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.005974
Total gradient norm: 0.043740
=== Actor Training Debug (Iteration 514) ===
Q mean: -3.809577
Q std: 2.879332
Actor loss: 3.813541
Action reg: 0.003964
  l1.weight: grad_norm = 0.004558
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005238
Total gradient norm: 0.031016
=== Actor Training Debug (Iteration 515) ===
Q mean: -3.434646
Q std: 2.893923
Actor loss: 3.438567
Action reg: 0.003922
  l1.weight: grad_norm = 0.006866
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.006927
Total gradient norm: 0.047824
=== Actor Training Debug (Iteration 516) ===
Q mean: -3.590150
Q std: 2.947389
Actor loss: 3.594120
Action reg: 0.003971
  l1.weight: grad_norm = 0.004628
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004523
Total gradient norm: 0.034295
=== Actor Training Debug (Iteration 517) ===
Q mean: -3.674294
Q std: 2.999482
Actor loss: 3.678254
Action reg: 0.003960
  l1.weight: grad_norm = 0.002672
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002328
Total gradient norm: 0.015735
=== Actor Training Debug (Iteration 518) ===
Q mean: -3.918354
Q std: 3.179283
Actor loss: 3.922328
Action reg: 0.003974
  l1.weight: grad_norm = 0.001978
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.002123
Total gradient norm: 0.020156
=== Actor Training Debug (Iteration 519) ===
Q mean: -3.761117
Q std: 3.162056
Actor loss: 3.765078
Action reg: 0.003961
  l1.weight: grad_norm = 0.001891
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001842
Total gradient norm: 0.011850
=== Actor Training Debug (Iteration 520) ===
Q mean: -3.382108
Q std: 2.887741
Actor loss: 3.386072
Action reg: 0.003964
  l1.weight: grad_norm = 0.005611
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005331
Total gradient norm: 0.034503
=== Actor Training Debug (Iteration 521) ===
Q mean: -3.264162
Q std: 2.744729
Actor loss: 3.268096
Action reg: 0.003934
  l1.weight: grad_norm = 0.004233
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.003970
Total gradient norm: 0.037704
=== Actor Training Debug (Iteration 522) ===
Q mean: -3.522943
Q std: 2.773862
Actor loss: 3.526895
Action reg: 0.003952
  l1.weight: grad_norm = 0.001979
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001793
Total gradient norm: 0.011662
=== Actor Training Debug (Iteration 523) ===
Q mean: -3.271425
Q std: 2.782877
Actor loss: 3.275378
Action reg: 0.003953
  l1.weight: grad_norm = 0.003962
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.003716
Total gradient norm: 0.034125
=== Actor Training Debug (Iteration 524) ===
Q mean: -3.554036
Q std: 2.744332
Actor loss: 3.557932
Action reg: 0.003897
  l1.weight: grad_norm = 0.008745
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.010042
Total gradient norm: 0.068437
=== Actor Training Debug (Iteration 525) ===
Q mean: -3.528463
Q std: 3.105865
Actor loss: 3.532387
Action reg: 0.003924
  l1.weight: grad_norm = 0.005362
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.004765
Total gradient norm: 0.039946
=== Actor Training Debug (Iteration 526) ===
Q mean: -3.601915
Q std: 3.073236
Actor loss: 3.605837
Action reg: 0.003923
  l1.weight: grad_norm = 0.004278
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.004305
Total gradient norm: 0.039216
=== Actor Training Debug (Iteration 527) ===
Q mean: -3.303595
Q std: 3.004690
Actor loss: 3.307520
Action reg: 0.003925
  l1.weight: grad_norm = 0.002480
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.002381
Total gradient norm: 0.026998
=== Actor Training Debug (Iteration 528) ===
Q mean: -3.422603
Q std: 2.857345
Actor loss: 3.426540
Action reg: 0.003937
  l1.weight: grad_norm = 0.003620
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003299
Total gradient norm: 0.025127
=== Actor Training Debug (Iteration 529) ===
Q mean: -3.306659
Q std: 2.778084
Actor loss: 3.310596
Action reg: 0.003937
  l1.weight: grad_norm = 0.006660
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.006618
Total gradient norm: 0.041864
=== Actor Training Debug (Iteration 530) ===
Q mean: -3.521730
Q std: 2.618747
Actor loss: 3.525692
Action reg: 0.003961
  l1.weight: grad_norm = 0.005928
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.005293
Total gradient norm: 0.046752
=== Actor Training Debug (Iteration 531) ===
Q mean: -3.665829
Q std: 2.771815
Actor loss: 3.669790
Action reg: 0.003961
  l1.weight: grad_norm = 0.004125
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.003879
Total gradient norm: 0.032687
=== Actor Training Debug (Iteration 532) ===
Q mean: -3.324541
Q std: 2.554872
Actor loss: 3.328465
Action reg: 0.003924
  l1.weight: grad_norm = 0.005055
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.004853
Total gradient norm: 0.048025
=== Actor Training Debug (Iteration 533) ===
Q mean: -3.667117
Q std: 3.189565
Actor loss: 3.671055
Action reg: 0.003938
  l1.weight: grad_norm = 0.002871
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.002594
Total gradient norm: 0.017975
=== Actor Training Debug (Iteration 534) ===
Q mean: -3.263304
Q std: 3.036768
Actor loss: 3.267246
Action reg: 0.003942
  l1.weight: grad_norm = 0.003235
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002887
Total gradient norm: 0.026157
=== Actor Training Debug (Iteration 535) ===
Q mean: -3.665569
Q std: 3.085813
Actor loss: 3.669505
Action reg: 0.003936
  l1.weight: grad_norm = 0.003920
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.004128
Total gradient norm: 0.037690
=== Actor Training Debug (Iteration 536) ===
Q mean: -3.764055
Q std: 3.126438
Actor loss: 3.767998
Action reg: 0.003944
  l1.weight: grad_norm = 0.005252
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004699
Total gradient norm: 0.053171
=== Actor Training Debug (Iteration 537) ===
Q mean: -3.537172
Q std: 2.853500
Actor loss: 3.541124
Action reg: 0.003952
  l1.weight: grad_norm = 0.003997
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.004168
Total gradient norm: 0.045890
=== Actor Training Debug (Iteration 538) ===
Q mean: -3.237048
Q std: 2.992215
Actor loss: 3.240970
Action reg: 0.003922
  l1.weight: grad_norm = 0.007213
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.007836
Total gradient norm: 0.058051
=== Actor Training Debug (Iteration 539) ===
Q mean: -3.201190
Q std: 2.756300
Actor loss: 3.205164
Action reg: 0.003974
  l1.weight: grad_norm = 0.004716
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004825
Total gradient norm: 0.047093
=== Actor Training Debug (Iteration 540) ===
Q mean: -3.647846
Q std: 2.915503
Actor loss: 3.651802
Action reg: 0.003956
  l1.weight: grad_norm = 0.003220
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.003066
Total gradient norm: 0.025294
=== Actor Training Debug (Iteration 541) ===
Q mean: -3.520778
Q std: 2.855887
Actor loss: 3.524749
Action reg: 0.003971
  l1.weight: grad_norm = 0.002885
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002324
Total gradient norm: 0.016434
=== Actor Training Debug (Iteration 542) ===
Q mean: -3.679608
Q std: 3.092708
Actor loss: 3.683562
Action reg: 0.003954
  l1.weight: grad_norm = 0.003373
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003395
Total gradient norm: 0.025727
=== Actor Training Debug (Iteration 543) ===
Q mean: -3.636556
Q std: 3.035140
Actor loss: 3.640493
Action reg: 0.003937
  l1.weight: grad_norm = 0.003430
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.002978
Total gradient norm: 0.021922
=== Actor Training Debug (Iteration 544) ===
Q mean: -3.387754
Q std: 2.917156
Actor loss: 3.391723
Action reg: 0.003968
  l1.weight: grad_norm = 0.005967
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.005539
Total gradient norm: 0.036881
=== Actor Training Debug (Iteration 545) ===
Q mean: -3.542115
Q std: 3.113288
Actor loss: 3.546036
Action reg: 0.003921
  l1.weight: grad_norm = 0.002259
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.002044
Total gradient norm: 0.017491
=== Actor Training Debug (Iteration 546) ===
Q mean: -3.820684
Q std: 3.008223
Actor loss: 3.824637
Action reg: 0.003953
  l1.weight: grad_norm = 0.005995
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.005005
Total gradient norm: 0.030281
=== Actor Training Debug (Iteration 547) ===
Q mean: -3.655674
Q std: 2.980693
Actor loss: 3.659649
Action reg: 0.003975
  l1.weight: grad_norm = 0.002736
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002279
Total gradient norm: 0.013448
=== Actor Training Debug (Iteration 548) ===
Q mean: -3.393902
Q std: 2.667370
Actor loss: 3.397866
Action reg: 0.003964
  l1.weight: grad_norm = 0.004181
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.004262
Total gradient norm: 0.036668
=== Actor Training Debug (Iteration 549) ===
Q mean: -3.589985
Q std: 3.017283
Actor loss: 3.593925
Action reg: 0.003940
  l1.weight: grad_norm = 0.003627
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003314
Total gradient norm: 0.034465
=== Actor Training Debug (Iteration 550) ===
Q mean: -3.799132
Q std: 3.473816
Actor loss: 3.803064
Action reg: 0.003932
  l1.weight: grad_norm = 0.008589
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.007722
Total gradient norm: 0.052780
=== Actor Training Debug (Iteration 551) ===
Q mean: -4.119601
Q std: 3.098345
Actor loss: 4.123572
Action reg: 0.003971
  l1.weight: grad_norm = 0.001653
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001479
Total gradient norm: 0.011809
=== Actor Training Debug (Iteration 552) ===
Q mean: -3.634526
Q std: 3.233044
Actor loss: 3.638403
Action reg: 0.003877
  l1.weight: grad_norm = 0.003690
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.003944
Total gradient norm: 0.032409
=== Actor Training Debug (Iteration 553) ===
Q mean: -3.404250
Q std: 3.111859
Actor loss: 3.408220
Action reg: 0.003971
  l1.weight: grad_norm = 0.002798
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002626
Total gradient norm: 0.027072
=== Actor Training Debug (Iteration 554) ===
Q mean: -2.973654
Q std: 2.681169
Actor loss: 2.977567
Action reg: 0.003913
  l1.weight: grad_norm = 0.003747
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.003322
Total gradient norm: 0.025387
=== Actor Training Debug (Iteration 555) ===
Q mean: -3.792108
Q std: 3.161404
Actor loss: 3.796089
Action reg: 0.003981
  l1.weight: grad_norm = 0.005494
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005170
Total gradient norm: 0.039744
=== Actor Training Debug (Iteration 556) ===
Q mean: -3.747587
Q std: 3.073119
Actor loss: 3.751545
Action reg: 0.003959
  l1.weight: grad_norm = 0.005352
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004745
Total gradient norm: 0.027925
=== Actor Training Debug (Iteration 557) ===
Q mean: -3.545035
Q std: 2.932033
Actor loss: 3.549005
Action reg: 0.003970
  l1.weight: grad_norm = 0.002441
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.002193
Total gradient norm: 0.020675
=== Actor Training Debug (Iteration 558) ===
Q mean: -3.344603
Q std: 3.075168
Actor loss: 3.348590
Action reg: 0.003988
  l1.weight: grad_norm = 0.005093
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005556
Total gradient norm: 0.030257
=== Actor Training Debug (Iteration 559) ===
Q mean: -3.743394
Q std: 3.277599
Actor loss: 3.747352
Action reg: 0.003958
  l1.weight: grad_norm = 0.002034
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002169
Total gradient norm: 0.021472
=== Actor Training Debug (Iteration 560) ===
Q mean: -3.440330
Q std: 3.187794
Actor loss: 3.444290
Action reg: 0.003961
  l1.weight: grad_norm = 0.004530
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.004497
Total gradient norm: 0.028946
=== Actor Training Debug (Iteration 561) ===
Q mean: -3.799167
Q std: 2.992043
Actor loss: 3.803110
Action reg: 0.003943
  l1.weight: grad_norm = 0.004581
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.004594
Total gradient norm: 0.041007
=== Actor Training Debug (Iteration 562) ===
Q mean: -3.687709
Q std: 3.313191
Actor loss: 3.691676
Action reg: 0.003967
  l1.weight: grad_norm = 0.004545
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.003948
Total gradient norm: 0.033038
=== Actor Training Debug (Iteration 563) ===
Q mean: -3.331741
Q std: 2.943828
Actor loss: 3.335673
Action reg: 0.003932
  l1.weight: grad_norm = 0.001937
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.001843
Total gradient norm: 0.016257
=== Actor Training Debug (Iteration 564) ===
Q mean: -3.832829
Q std: 3.404851
Actor loss: 3.836791
Action reg: 0.003962
  l1.weight: grad_norm = 0.006128
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.005523
Total gradient norm: 0.045724
=== Actor Training Debug (Iteration 565) ===
Q mean: -3.938172
Q std: 3.277879
Actor loss: 3.942129
Action reg: 0.003957
  l1.weight: grad_norm = 0.001969
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.002009
Total gradient norm: 0.015942
=== Actor Training Debug (Iteration 566) ===
Q mean: -3.264597
Q std: 3.241978
Actor loss: 3.268540
Action reg: 0.003944
  l1.weight: grad_norm = 0.002285
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.001919
Total gradient norm: 0.012089
=== Actor Training Debug (Iteration 567) ===
Q mean: -3.495322
Q std: 3.055789
Actor loss: 3.499270
Action reg: 0.003948
  l1.weight: grad_norm = 0.006107
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.005828
Total gradient norm: 0.037793
=== Actor Training Debug (Iteration 568) ===
Q mean: -4.108117
Q std: 3.403880
Actor loss: 4.112099
Action reg: 0.003982
  l1.weight: grad_norm = 0.003230
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003270
Total gradient norm: 0.026014
=== Actor Training Debug (Iteration 569) ===
Q mean: -4.036288
Q std: 3.295812
Actor loss: 4.040226
Action reg: 0.003937
  l1.weight: grad_norm = 0.003930
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.003512
Total gradient norm: 0.027109
=== Actor Training Debug (Iteration 570) ===
Q mean: -3.413715
Q std: 2.864479
Actor loss: 3.417665
Action reg: 0.003949
  l1.weight: grad_norm = 0.007126
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.006120
Total gradient norm: 0.058466
=== Actor Training Debug (Iteration 571) ===
Q mean: -3.404920
Q std: 2.968223
Actor loss: 3.408861
Action reg: 0.003941
  l1.weight: grad_norm = 0.007811
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.007251
Total gradient norm: 0.055270
=== Actor Training Debug (Iteration 572) ===
Q mean: -3.812609
Q std: 3.310640
Actor loss: 3.816548
Action reg: 0.003939
  l1.weight: grad_norm = 0.004809
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.004418
Total gradient norm: 0.040043
=== Actor Training Debug (Iteration 573) ===
Q mean: -3.907394
Q std: 3.327457
Actor loss: 3.911320
Action reg: 0.003926
  l1.weight: grad_norm = 0.004050
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.003817
Total gradient norm: 0.028846
=== Actor Training Debug (Iteration 574) ===
Q mean: -3.280778
Q std: 3.058256
Actor loss: 3.284721
Action reg: 0.003943
  l1.weight: grad_norm = 0.004093
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.003669
Total gradient norm: 0.028310
=== Actor Training Debug (Iteration 575) ===
Q mean: -3.923832
Q std: 3.543177
Actor loss: 3.927803
Action reg: 0.003970
  l1.weight: grad_norm = 0.003457
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003169
Total gradient norm: 0.023105
=== Actor Training Debug (Iteration 576) ===
Q mean: -3.396285
Q std: 3.012174
Actor loss: 3.400217
Action reg: 0.003932
  l1.weight: grad_norm = 0.002349
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.002001
Total gradient norm: 0.012430
=== Actor Training Debug (Iteration 577) ===
Q mean: -3.748530
Q std: 3.090238
Actor loss: 3.752479
Action reg: 0.003949
  l1.weight: grad_norm = 0.004697
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.004139
Total gradient norm: 0.032053
=== Actor Training Debug (Iteration 578) ===
Q mean: -3.728951
Q std: 3.488677
Actor loss: 3.732876
Action reg: 0.003924
  l1.weight: grad_norm = 0.003512
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.003504
Total gradient norm: 0.020551
=== Actor Training Debug (Iteration 579) ===
Q mean: -3.888791
Q std: 3.276433
Actor loss: 3.892724
Action reg: 0.003933
  l1.weight: grad_norm = 0.001239
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.001041
Total gradient norm: 0.007156
=== Actor Training Debug (Iteration 580) ===
Q mean: -3.528831
Q std: 2.970444
Actor loss: 3.532784
Action reg: 0.003954
  l1.weight: grad_norm = 0.011100
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.010517
Total gradient norm: 0.079264
=== Actor Training Debug (Iteration 581) ===
Q mean: -3.750883
Q std: 2.744209
Actor loss: 3.754852
Action reg: 0.003969
  l1.weight: grad_norm = 0.004371
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.004309
Total gradient norm: 0.030825
=== Actor Training Debug (Iteration 582) ===
Q mean: -3.853450
Q std: 3.310526
Actor loss: 3.857397
Action reg: 0.003948
  l1.weight: grad_norm = 0.005580
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004863
Total gradient norm: 0.033535
=== Actor Training Debug (Iteration 583) ===
Q mean: -3.751869
Q std: 3.090460
Actor loss: 3.755801
Action reg: 0.003932
  l1.weight: grad_norm = 0.006069
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.005311
Total gradient norm: 0.046723
=== Actor Training Debug (Iteration 584) ===
Q mean: -3.714121
Q std: 3.539412
Actor loss: 3.718033
Action reg: 0.003911
  l1.weight: grad_norm = 0.004057
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.004687
Total gradient norm: 0.041920
=== Actor Training Debug (Iteration 585) ===
Q mean: -3.532159
Q std: 3.872188
Actor loss: 3.536101
Action reg: 0.003942
  l1.weight: grad_norm = 0.005083
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.004464
Total gradient norm: 0.029640
=== Actor Training Debug (Iteration 586) ===
Q mean: -3.934250
Q std: 3.675000
Actor loss: 3.938224
Action reg: 0.003974
  l1.weight: grad_norm = 0.006159
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006035
Total gradient norm: 0.036223
=== Actor Training Debug (Iteration 587) ===
Q mean: -3.898659
Q std: 3.223377
Actor loss: 3.902598
Action reg: 0.003940
  l1.weight: grad_norm = 0.004274
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004148
Total gradient norm: 0.033541
=== Actor Training Debug (Iteration 588) ===
Q mean: -4.126405
Q std: 3.306134
Actor loss: 4.130347
Action reg: 0.003942
  l1.weight: grad_norm = 0.005381
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.005226
Total gradient norm: 0.031829
=== Actor Training Debug (Iteration 589) ===
Q mean: -3.604963
Q std: 2.885132
Actor loss: 3.608918
Action reg: 0.003955
  l1.weight: grad_norm = 0.003958
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.004130
Total gradient norm: 0.042884
=== Actor Training Debug (Iteration 590) ===
Q mean: -3.882744
Q std: 3.445015
Actor loss: 3.886708
Action reg: 0.003963
  l1.weight: grad_norm = 0.003827
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.003897
Total gradient norm: 0.022633
=== Actor Training Debug (Iteration 591) ===
Q mean: -3.645633
Q std: 3.241757
Actor loss: 3.649595
Action reg: 0.003962
  l1.weight: grad_norm = 0.007066
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.006254
Total gradient norm: 0.033574
=== Actor Training Debug (Iteration 592) ===
Q mean: -4.102651
Q std: 3.130259
Actor loss: 4.106611
Action reg: 0.003960
  l1.weight: grad_norm = 0.003116
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003200
Total gradient norm: 0.022114
=== Actor Training Debug (Iteration 593) ===
Q mean: -3.655266
Q std: 3.063901
Actor loss: 3.659214
Action reg: 0.003948
  l1.weight: grad_norm = 0.004506
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.004556
Total gradient norm: 0.041093
=== Actor Training Debug (Iteration 594) ===
Q mean: -3.528638
Q std: 3.406595
Actor loss: 3.532608
Action reg: 0.003970
  l1.weight: grad_norm = 0.003631
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.003094
Total gradient norm: 0.020441
=== Actor Training Debug (Iteration 595) ===
Q mean: -3.341081
Q std: 3.553838
Actor loss: 3.345051
Action reg: 0.003970
  l1.weight: grad_norm = 0.003646
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.003108
Total gradient norm: 0.021518
=== Actor Training Debug (Iteration 596) ===
Q mean: -4.062312
Q std: 3.635763
Actor loss: 4.066257
Action reg: 0.003945
  l1.weight: grad_norm = 0.007627
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.007680
Total gradient norm: 0.047312
=== Actor Training Debug (Iteration 597) ===
Q mean: -3.715044
Q std: 3.385391
Actor loss: 3.719007
Action reg: 0.003963
  l1.weight: grad_norm = 0.002868
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.002963
Total gradient norm: 0.023341
=== Actor Training Debug (Iteration 598) ===
Q mean: -3.902351
Q std: 3.690477
Actor loss: 3.906302
Action reg: 0.003951
  l1.weight: grad_norm = 0.004352
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.003963
Total gradient norm: 0.027993
=== Actor Training Debug (Iteration 599) ===
Q mean: -3.887677
Q std: 3.360807
Actor loss: 3.891661
Action reg: 0.003984
  l1.weight: grad_norm = 0.002200
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001908
Total gradient norm: 0.011778
=== Actor Training Debug (Iteration 600) ===
Q mean: -3.575011
Q std: 3.310188
Actor loss: 3.578985
Action reg: 0.003974
  l1.weight: grad_norm = 0.002924
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.002875
Total gradient norm: 0.022414
=== Actor Training Debug (Iteration 601) ===
Q mean: -3.669089
Q std: 3.109923
Actor loss: 3.673049
Action reg: 0.003959
  l1.weight: grad_norm = 0.003367
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.003343
Total gradient norm: 0.023283
=== Actor Training Debug (Iteration 602) ===
Q mean: -4.183585
Q std: 3.713604
Actor loss: 4.187518
Action reg: 0.003932
  l1.weight: grad_norm = 0.006231
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.007560
Total gradient norm: 0.047121
=== Actor Training Debug (Iteration 603) ===
Q mean: -3.681529
Q std: 3.100966
Actor loss: 3.685481
Action reg: 0.003952
  l1.weight: grad_norm = 0.004707
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004232
Total gradient norm: 0.036897
=== Actor Training Debug (Iteration 604) ===
Q mean: -3.386427
Q std: 3.168543
Actor loss: 3.390385
Action reg: 0.003958
  l1.weight: grad_norm = 0.003373
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002951
Total gradient norm: 0.023155
=== Actor Training Debug (Iteration 605) ===
Q mean: -3.537342
Q std: 3.512656
Actor loss: 3.541291
Action reg: 0.003949
  l1.weight: grad_norm = 0.005241
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.004981
Total gradient norm: 0.048549
=== Actor Training Debug (Iteration 606) ===
Q mean: -3.802185
Q std: 3.652961
Actor loss: 3.806148
Action reg: 0.003963
  l1.weight: grad_norm = 0.007588
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.006774
Total gradient norm: 0.045365
=== Actor Training Debug (Iteration 607) ===
Q mean: -3.747474
Q std: 3.173795
Actor loss: 3.751445
Action reg: 0.003970
  l1.weight: grad_norm = 0.005623
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004445
Total gradient norm: 0.022575
=== Actor Training Debug (Iteration 608) ===
Q mean: -3.962633
Q std: 3.397599
Actor loss: 3.966575
Action reg: 0.003942
  l1.weight: grad_norm = 0.005714
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.004756
Total gradient norm: 0.035290
=== Actor Training Debug (Iteration 609) ===
Q mean: -3.759723
Q std: 3.539248
Actor loss: 3.763644
Action reg: 0.003921
  l1.weight: grad_norm = 0.004553
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.003773
Total gradient norm: 0.026778
=== Actor Training Debug (Iteration 610) ===
Q mean: -3.272554
Q std: 3.590140
Actor loss: 3.276490
Action reg: 0.003936
  l1.weight: grad_norm = 0.008279
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.007310
Total gradient norm: 0.044347
=== Actor Training Debug (Iteration 611) ===
Q mean: -3.536182
Q std: 3.510517
Actor loss: 3.540137
Action reg: 0.003955
  l1.weight: grad_norm = 0.005140
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.004848
Total gradient norm: 0.030540
=== Actor Training Debug (Iteration 612) ===
Q mean: -3.483895
Q std: 3.090650
Actor loss: 3.487832
Action reg: 0.003937
  l1.weight: grad_norm = 0.003952
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.003842
Total gradient norm: 0.028514
=== Actor Training Debug (Iteration 613) ===
Q mean: -4.050091
Q std: 3.290408
Actor loss: 4.054070
Action reg: 0.003979
  l1.weight: grad_norm = 0.006273
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005915
Total gradient norm: 0.052220
=== Actor Training Debug (Iteration 614) ===
Q mean: -3.716592
Q std: 3.386293
Actor loss: 3.720536
Action reg: 0.003944
  l1.weight: grad_norm = 0.007557
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.006492
Total gradient norm: 0.039979
=== Actor Training Debug (Iteration 615) ===
Q mean: -3.463596
Q std: 3.268750
Actor loss: 3.467549
Action reg: 0.003952
  l1.weight: grad_norm = 0.005456
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.005036
Total gradient norm: 0.045939
=== Actor Training Debug (Iteration 616) ===
Q mean: -3.665936
Q std: 3.168650
Actor loss: 3.669870
Action reg: 0.003933
  l1.weight: grad_norm = 0.002965
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002700
Total gradient norm: 0.020443
=== Actor Training Debug (Iteration 617) ===
Q mean: -3.909292
Q std: 3.433386
Actor loss: 3.913274
Action reg: 0.003982
  l1.weight: grad_norm = 0.005669
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004724
Total gradient norm: 0.025743
=== Actor Training Debug (Iteration 618) ===
Q mean: -3.663509
Q std: 3.459598
Actor loss: 3.667461
Action reg: 0.003952
  l1.weight: grad_norm = 0.006453
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.005473
Total gradient norm: 0.032949
=== Actor Training Debug (Iteration 619) ===
Q mean: -3.634851
Q std: 3.525115
Actor loss: 3.638766
Action reg: 0.003915
  l1.weight: grad_norm = 0.006252
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.006026
Total gradient norm: 0.047600
=== Actor Training Debug (Iteration 620) ===
Q mean: -3.669867
Q std: 3.336017
Actor loss: 3.673827
Action reg: 0.003960
  l1.weight: grad_norm = 0.005833
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.006038
Total gradient norm: 0.049473
=== Actor Training Debug (Iteration 621) ===
Q mean: -3.507665
Q std: 3.353301
Actor loss: 3.511645
Action reg: 0.003979
  l1.weight: grad_norm = 0.005087
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004868
Total gradient norm: 0.040589
=== Actor Training Debug (Iteration 622) ===
Q mean: -3.527079
Q std: 2.736425
Actor loss: 3.531039
Action reg: 0.003961
  l1.weight: grad_norm = 0.004189
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.003885
Total gradient norm: 0.034089
=== Actor Training Debug (Iteration 623) ===
Q mean: -3.762778
Q std: 3.348435
Actor loss: 3.766745
Action reg: 0.003967
  l1.weight: grad_norm = 0.002008
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.001958
Total gradient norm: 0.012856
=== Actor Training Debug (Iteration 624) ===
Q mean: -3.943821
Q std: 3.667250
Actor loss: 3.947767
Action reg: 0.003945
  l1.weight: grad_norm = 0.002954
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.002511
Total gradient norm: 0.018283
=== Actor Training Debug (Iteration 625) ===
Q mean: -3.600890
Q std: 3.466778
Actor loss: 3.604839
Action reg: 0.003949
  l1.weight: grad_norm = 0.006675
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.006912
Total gradient norm: 0.044529
=== Actor Training Debug (Iteration 626) ===
Q mean: -4.052894
Q std: 3.361368
Actor loss: 4.056847
Action reg: 0.003953
  l1.weight: grad_norm = 0.002184
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.001915
Total gradient norm: 0.015771
=== Actor Training Debug (Iteration 627) ===
Q mean: -4.388291
Q std: 3.499134
Actor loss: 4.392246
Action reg: 0.003955
  l1.weight: grad_norm = 0.005163
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.005328
Total gradient norm: 0.033366
=== Actor Training Debug (Iteration 628) ===
Q mean: -4.315231
Q std: 3.663842
Actor loss: 4.319209
Action reg: 0.003978
  l1.weight: grad_norm = 0.006234
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005840
Total gradient norm: 0.049888
=== Actor Training Debug (Iteration 629) ===
Q mean: -3.251613
Q std: 3.546461
Actor loss: 3.255533
Action reg: 0.003920
  l1.weight: grad_norm = 0.003877
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.002945
Total gradient norm: 0.016669
=== Actor Training Debug (Iteration 630) ===
Q mean: -3.296117
Q std: 3.371510
Actor loss: 3.300077
Action reg: 0.003961
  l1.weight: grad_norm = 0.002607
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002629
Total gradient norm: 0.024562
=== Actor Training Debug (Iteration 631) ===
Q mean: -3.880985
Q std: 3.324632
Actor loss: 3.884926
Action reg: 0.003941
  l1.weight: grad_norm = 0.007448
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.006564
Total gradient norm: 0.058997
=== Actor Training Debug (Iteration 632) ===
Q mean: -4.256029
Q std: 3.760005
Actor loss: 4.259985
Action reg: 0.003956
  l1.weight: grad_norm = 0.004221
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.003574
Total gradient norm: 0.027790
=== Actor Training Debug (Iteration 633) ===
Q mean: -3.421725
Q std: 3.340488
Actor loss: 3.425687
Action reg: 0.003962
  l1.weight: grad_norm = 0.004076
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.003519
Total gradient norm: 0.028882
=== Actor Training Debug (Iteration 634) ===
Q mean: -3.715425
Q std: 3.353881
Actor loss: 3.719371
Action reg: 0.003946
  l1.weight: grad_norm = 0.002697
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.002437
Total gradient norm: 0.018948
=== Actor Training Debug (Iteration 635) ===
Q mean: -3.617664
Q std: 3.071925
Actor loss: 3.621619
Action reg: 0.003955
  l1.weight: grad_norm = 0.007502
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.007802
Total gradient norm: 0.068656
=== Actor Training Debug (Iteration 636) ===
Q mean: -4.071452
Q std: 4.058436
Actor loss: 4.075390
Action reg: 0.003938
  l1.weight: grad_norm = 0.005860
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.005042
Total gradient norm: 0.029148
=== Actor Training Debug (Iteration 637) ===
Q mean: -3.605111
Q std: 3.464954
Actor loss: 3.609056
Action reg: 0.003945
  l1.weight: grad_norm = 0.007665
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.007979
Total gradient norm: 0.046203
=== Actor Training Debug (Iteration 638) ===
Q mean: -3.217655
Q std: 3.403422
Actor loss: 3.221598
Action reg: 0.003942
  l1.weight: grad_norm = 0.004346
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.003587
Total gradient norm: 0.023364
=== Actor Training Debug (Iteration 639) ===
Q mean: -3.544190
Q std: 3.425624
Actor loss: 3.548150
Action reg: 0.003960
  l1.weight: grad_norm = 0.004894
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.004032
Total gradient norm: 0.022391
=== Actor Training Debug (Iteration 640) ===
Q mean: -3.981153
Q std: 3.467365
Actor loss: 3.985074
Action reg: 0.003921
  l1.weight: grad_norm = 0.009027
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.008339
Total gradient norm: 0.058913
=== Actor Training Debug (Iteration 641) ===
Q mean: -4.227772
Q std: 3.851476
Actor loss: 4.231740
Action reg: 0.003968
  l1.weight: grad_norm = 0.005281
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.004581
Total gradient norm: 0.038349
=== Actor Training Debug (Iteration 642) ===
Q mean: -3.600905
Q std: 3.645258
Actor loss: 3.604852
Action reg: 0.003947
  l1.weight: grad_norm = 0.005323
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.004489
Total gradient norm: 0.030830
=== Actor Training Debug (Iteration 643) ===
Q mean: -3.151707
Q std: 3.230653
Actor loss: 3.155665
Action reg: 0.003958
  l1.weight: grad_norm = 0.004339
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.004014
Total gradient norm: 0.025546
=== Actor Training Debug (Iteration 644) ===
Q mean: -3.556015
Q std: 3.364209
Actor loss: 3.559982
Action reg: 0.003968
  l1.weight: grad_norm = 0.006269
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.006421
Total gradient norm: 0.039258
=== Actor Training Debug (Iteration 645) ===
Q mean: -4.278541
Q std: 3.685874
Actor loss: 4.282482
Action reg: 0.003941
  l1.weight: grad_norm = 0.006398
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.006492
Total gradient norm: 0.042110
=== Actor Training Debug (Iteration 646) ===
Q mean: -4.612873
Q std: 3.882394
Actor loss: 4.616825
Action reg: 0.003952
  l1.weight: grad_norm = 0.004362
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.004745
Total gradient norm: 0.035578
=== Actor Training Debug (Iteration 647) ===
Q mean: -4.371958
Q std: 3.959821
Actor loss: 4.375930
Action reg: 0.003972
  l1.weight: grad_norm = 0.004245
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003577
Total gradient norm: 0.022404
=== Actor Training Debug (Iteration 648) ===
Q mean: -3.416985
Q std: 3.642235
Actor loss: 3.420908
Action reg: 0.003923
  l1.weight: grad_norm = 0.005743
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.004888
Total gradient norm: 0.036905
=== Actor Training Debug (Iteration 649) ===
Q mean: -4.138126
Q std: 3.939144
Actor loss: 4.142097
Action reg: 0.003971
  l1.weight: grad_norm = 0.006396
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005233
Total gradient norm: 0.039647
=== Actor Training Debug (Iteration 650) ===
Q mean: -3.882863
Q std: 3.812228
Actor loss: 3.886813
Action reg: 0.003950
  l1.weight: grad_norm = 0.005745
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.005049
Total gradient norm: 0.036795
=== Actor Training Debug (Iteration 651) ===
Q mean: -3.813292
Q std: 3.521133
Actor loss: 3.817228
Action reg: 0.003936
  l1.weight: grad_norm = 0.006015
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.004677
Total gradient norm: 0.026311
=== Actor Training Debug (Iteration 652) ===
Q mean: -4.382117
Q std: 3.851721
Actor loss: 4.386073
Action reg: 0.003956
  l1.weight: grad_norm = 0.003643
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.003284
Total gradient norm: 0.025047
=== Actor Training Debug (Iteration 653) ===
Q mean: -3.919230
Q std: 3.838541
Actor loss: 3.923155
Action reg: 0.003925
  l1.weight: grad_norm = 0.004316
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.003862
Total gradient norm: 0.031187
=== Actor Training Debug (Iteration 654) ===
Q mean: -4.180229
Q std: 3.749938
Actor loss: 4.184206
Action reg: 0.003977
  l1.weight: grad_norm = 0.009883
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.009425
Total gradient norm: 0.069201
=== Actor Training Debug (Iteration 655) ===
Q mean: -4.058118
Q std: 4.229266
Actor loss: 4.062047
Action reg: 0.003930
  l1.weight: grad_norm = 0.004750
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.004580
Total gradient norm: 0.031306
=== Actor Training Debug (Iteration 656) ===
Q mean: -3.700302
Q std: 3.764300
Actor loss: 3.704238
Action reg: 0.003937
  l1.weight: grad_norm = 0.009651
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.008404
Total gradient norm: 0.063692
=== Actor Training Debug (Iteration 657) ===
Q mean: -4.261481
Q std: 4.186944
Actor loss: 4.265434
Action reg: 0.003954
  l1.weight: grad_norm = 0.009286
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.008288
Total gradient norm: 0.050304
=== Actor Training Debug (Iteration 658) ===
Q mean: -4.115920
Q std: 4.233309
Actor loss: 4.119878
Action reg: 0.003958
  l1.weight: grad_norm = 0.007995
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.007134
Total gradient norm: 0.037853
=== Actor Training Debug (Iteration 659) ===
Q mean: -3.771909
Q std: 3.914573
Actor loss: 3.775883
Action reg: 0.003974
  l1.weight: grad_norm = 0.005330
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005304
Total gradient norm: 0.045679
=== Actor Training Debug (Iteration 660) ===
Q mean: -3.567213
Q std: 3.587958
Actor loss: 3.571187
Action reg: 0.003974
  l1.weight: grad_norm = 0.006926
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006183
Total gradient norm: 0.038467
=== Actor Training Debug (Iteration 661) ===
Q mean: -4.515484
Q std: 3.952733
Actor loss: 4.519425
Action reg: 0.003941
  l1.weight: grad_norm = 0.007260
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.007635
Total gradient norm: 0.043593
=== Actor Training Debug (Iteration 662) ===
Q mean: -3.678166
Q std: 3.554216
Actor loss: 3.682133
Action reg: 0.003967
  l1.weight: grad_norm = 0.002898
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002494
Total gradient norm: 0.017432
=== Actor Training Debug (Iteration 663) ===
Q mean: -3.750183
Q std: 3.727463
Actor loss: 3.754147
Action reg: 0.003964
  l1.weight: grad_norm = 0.003706
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.003028
Total gradient norm: 0.018550
=== Actor Training Debug (Iteration 664) ===
Q mean: -3.373306
Q std: 3.676067
Actor loss: 3.377264
Action reg: 0.003958
  l1.weight: grad_norm = 0.008544
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.006656
Total gradient norm: 0.034359
=== Actor Training Debug (Iteration 665) ===
Q mean: -4.032001
Q std: 3.722625
Actor loss: 4.035974
Action reg: 0.003972
  l1.weight: grad_norm = 0.004254
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.003810
Total gradient norm: 0.019069
=== Actor Training Debug (Iteration 666) ===
Q mean: -4.650109
Q std: 4.022340
Actor loss: 4.654062
Action reg: 0.003953
  l1.weight: grad_norm = 0.012811
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.011518
Total gradient norm: 0.062624
=== Actor Training Debug (Iteration 667) ===
Q mean: -3.896667
Q std: 3.686057
Actor loss: 3.900623
Action reg: 0.003956
  l1.weight: grad_norm = 0.006593
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.005825
Total gradient norm: 0.033331
=== Actor Training Debug (Iteration 668) ===
Q mean: -4.209332
Q std: 3.941812
Actor loss: 4.213302
Action reg: 0.003970
  l1.weight: grad_norm = 0.003491
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003083
Total gradient norm: 0.025366
=== Actor Training Debug (Iteration 669) ===
Q mean: -4.241378
Q std: 4.063307
Actor loss: 4.245288
Action reg: 0.003910
  l1.weight: grad_norm = 0.004807
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.004204
Total gradient norm: 0.026800
=== Actor Training Debug (Iteration 670) ===
Q mean: -4.240149
Q std: 3.849755
Actor loss: 4.244090
Action reg: 0.003940
  l1.weight: grad_norm = 0.003807
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.003608
Total gradient norm: 0.034582
=== Actor Training Debug (Iteration 671) ===
Q mean: -4.077428
Q std: 4.093536
Actor loss: 4.081373
Action reg: 0.003945
  l1.weight: grad_norm = 0.010180
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.009855
Total gradient norm: 0.062711
=== Actor Training Debug (Iteration 672) ===
Q mean: -3.770120
Q std: 3.889490
Actor loss: 3.774052
Action reg: 0.003932
  l1.weight: grad_norm = 0.004000
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.003436
Total gradient norm: 0.020580
=== Actor Training Debug (Iteration 673) ===
Q mean: -3.507077
Q std: 3.600970
Actor loss: 3.511063
Action reg: 0.003986
  l1.weight: grad_norm = 0.007410
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.007813
Total gradient norm: 0.045510
=== Actor Training Debug (Iteration 674) ===
Q mean: -4.336243
Q std: 3.918121
Actor loss: 4.340187
Action reg: 0.003944
  l1.weight: grad_norm = 0.002218
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.002387
Total gradient norm: 0.020258
=== Actor Training Debug (Iteration 675) ===
Q mean: -4.380632
Q std: 4.094801
Actor loss: 4.384582
Action reg: 0.003950
  l1.weight: grad_norm = 0.004227
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.003855
Total gradient norm: 0.030843
=== Actor Training Debug (Iteration 676) ===
Q mean: -4.352407
Q std: 4.079228
Actor loss: 4.356390
Action reg: 0.003983
  l1.weight: grad_norm = 0.007608
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.006241
Total gradient norm: 0.036972
=== Actor Training Debug (Iteration 677) ===
Q mean: -3.986899
Q std: 4.034967
Actor loss: 3.990853
Action reg: 0.003953
  l1.weight: grad_norm = 0.004068
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.004406
Total gradient norm: 0.029140
=== Actor Training Debug (Iteration 678) ===
Q mean: -3.635769
Q std: 3.863868
Actor loss: 3.639729
Action reg: 0.003960
  l1.weight: grad_norm = 0.013867
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.011329
Total gradient norm: 0.066663
=== Actor Training Debug (Iteration 679) ===
Q mean: -3.524161
Q std: 3.617151
Actor loss: 3.528116
Action reg: 0.003954
  l1.weight: grad_norm = 0.008079
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.007020
Total gradient norm: 0.052886
=== Actor Training Debug (Iteration 680) ===
Q mean: -4.178998
Q std: 3.877438
Actor loss: 4.182979
Action reg: 0.003981
  l1.weight: grad_norm = 0.003270
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002773
Total gradient norm: 0.016994
=== Actor Training Debug (Iteration 681) ===
Q mean: -4.546578
Q std: 3.690141
Actor loss: 4.550529
Action reg: 0.003951
  l1.weight: grad_norm = 0.007567
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.006559
Total gradient norm: 0.040586
=== Actor Training Debug (Iteration 682) ===
Q mean: -4.541043
Q std: 4.098325
Actor loss: 4.544981
Action reg: 0.003938
  l1.weight: grad_norm = 0.006376
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.005323
Total gradient norm: 0.037167
=== Actor Training Debug (Iteration 683) ===
Q mean: -4.263515
Q std: 4.361680
Actor loss: 4.267453
Action reg: 0.003938
  l1.weight: grad_norm = 0.008057
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.007100
Total gradient norm: 0.049380
=== Actor Training Debug (Iteration 684) ===
Q mean: -4.149303
Q std: 4.272747
Actor loss: 4.153257
Action reg: 0.003953
  l1.weight: grad_norm = 0.001933
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.001703
Total gradient norm: 0.010946
=== Actor Training Debug (Iteration 685) ===
Q mean: -3.422940
Q std: 3.537609
Actor loss: 3.426870
Action reg: 0.003930
  l1.weight: grad_norm = 0.008030
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.006885
Total gradient norm: 0.038441
=== Actor Training Debug (Iteration 686) ===
Q mean: -3.600019
Q std: 3.798689
Actor loss: 3.603997
Action reg: 0.003977
  l1.weight: grad_norm = 0.004335
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.004032
Total gradient norm: 0.023379
=== Actor Training Debug (Iteration 687) ===
Q mean: -4.336030
Q std: 3.847023
Actor loss: 4.339978
Action reg: 0.003948
  l1.weight: grad_norm = 0.008015
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.007084
Total gradient norm: 0.059978
=== Actor Training Debug (Iteration 688) ===
Q mean: -5.251642
Q std: 4.285869
Actor loss: 5.255591
Action reg: 0.003949
  l1.weight: grad_norm = 0.006381
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.005566
Total gradient norm: 0.037124
=== Actor Training Debug (Iteration 689) ===
Q mean: -4.398288
Q std: 4.078907
Actor loss: 4.402227
Action reg: 0.003939
  l1.weight: grad_norm = 0.004063
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.003727
Total gradient norm: 0.028314
=== Actor Training Debug (Iteration 690) ===
Q mean: -3.907891
Q std: 3.785523
Actor loss: 3.911862
Action reg: 0.003971
  l1.weight: grad_norm = 0.005646
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.005733
Total gradient norm: 0.039293
=== Actor Training Debug (Iteration 691) ===
Q mean: -3.594652
Q std: 3.617302
Actor loss: 3.598626
Action reg: 0.003974
  l1.weight: grad_norm = 0.003521
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003347
Total gradient norm: 0.018353
=== Actor Training Debug (Iteration 692) ===
Q mean: -3.593729
Q std: 3.724035
Actor loss: 3.597662
Action reg: 0.003933
  l1.weight: grad_norm = 0.007644
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.005676
Total gradient norm: 0.039097
=== Actor Training Debug (Iteration 693) ===
Q mean: -3.717412
Q std: 3.527824
Actor loss: 3.721366
Action reg: 0.003954
  l1.weight: grad_norm = 0.004894
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.003785
Total gradient norm: 0.019227
=== Actor Training Debug (Iteration 694) ===
Q mean: -4.104756
Q std: 4.052904
Actor loss: 4.108701
Action reg: 0.003945
  l1.weight: grad_norm = 0.005744
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.006530
Total gradient norm: 0.054074
=== Actor Training Debug (Iteration 695) ===
Q mean: -4.074463
Q std: 4.152360
Actor loss: 4.078405
Action reg: 0.003942
  l1.weight: grad_norm = 0.005796
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.004948
Total gradient norm: 0.037027
=== Actor Training Debug (Iteration 696) ===
Q mean: -4.027601
Q std: 4.158845
Actor loss: 4.031540
Action reg: 0.003939
  l1.weight: grad_norm = 0.006035
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.005931
Total gradient norm: 0.047459
=== Actor Training Debug (Iteration 697) ===
Q mean: -3.754864
Q std: 4.157439
Actor loss: 3.758797
Action reg: 0.003933
  l1.weight: grad_norm = 0.007168
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.005445
Total gradient norm: 0.029769
=== Actor Training Debug (Iteration 698) ===
Q mean: -4.206078
Q std: 4.481122
Actor loss: 4.210054
Action reg: 0.003977
  l1.weight: grad_norm = 0.009999
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.009120
Total gradient norm: 0.052494
=== Actor Training Debug (Iteration 699) ===
Q mean: -4.747324
Q std: 4.097356
Actor loss: 4.751299
Action reg: 0.003975
  l1.weight: grad_norm = 0.008399
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.008019
Total gradient norm: 0.061836
=== Actor Training Debug (Iteration 700) ===
Q mean: -4.202208
Q std: 3.651109
Actor loss: 4.206184
Action reg: 0.003976
  l1.weight: grad_norm = 0.006388
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005068
Total gradient norm: 0.038597
=== Actor Training Debug (Iteration 701) ===
Q mean: -3.866473
Q std: 4.015707
Actor loss: 3.870456
Action reg: 0.003983
  l1.weight: grad_norm = 0.006108
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005591
Total gradient norm: 0.028480
=== Actor Training Debug (Iteration 702) ===
Q mean: -3.842435
Q std: 4.095719
Actor loss: 3.846393
Action reg: 0.003958
  l1.weight: grad_norm = 0.003427
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.003106
Total gradient norm: 0.021583
=== Actor Training Debug (Iteration 703) ===
Q mean: -4.099932
Q std: 4.298282
Actor loss: 4.103896
Action reg: 0.003964
  l1.weight: grad_norm = 0.003431
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002758
Total gradient norm: 0.017382
=== Actor Training Debug (Iteration 704) ===
Q mean: -4.335498
Q std: 4.082485
Actor loss: 4.339446
Action reg: 0.003948
  l1.weight: grad_norm = 0.006074
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.004707
Total gradient norm: 0.030349
=== Actor Training Debug (Iteration 705) ===
Q mean: -4.553802
Q std: 4.058961
Actor loss: 4.557772
Action reg: 0.003970
  l1.weight: grad_norm = 0.006235
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005540
Total gradient norm: 0.036168
=== Actor Training Debug (Iteration 706) ===
Q mean: -3.935948
Q std: 3.718094
Actor loss: 3.939916
Action reg: 0.003968
  l1.weight: grad_norm = 0.005634
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.005070
Total gradient norm: 0.037785
=== Actor Training Debug (Iteration 707) ===
Q mean: -3.989181
Q std: 4.178013
Actor loss: 3.993107
Action reg: 0.003926
  l1.weight: grad_norm = 0.007372
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.007755
Total gradient norm: 0.046437
=== Actor Training Debug (Iteration 708) ===
Q mean: -3.409866
Q std: 3.472338
Actor loss: 3.413826
Action reg: 0.003960
  l1.weight: grad_norm = 0.004759
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004573
Total gradient norm: 0.028366
=== Actor Training Debug (Iteration 709) ===
Q mean: -4.344834
Q std: 4.173059
Actor loss: 4.348797
Action reg: 0.003962
  l1.weight: grad_norm = 0.001253
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.001206
Total gradient norm: 0.008516
=== Actor Training Debug (Iteration 710) ===
Q mean: -4.403849
Q std: 4.342173
Actor loss: 4.407810
Action reg: 0.003961
  l1.weight: grad_norm = 0.002917
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.002267
Total gradient norm: 0.018214
=== Actor Training Debug (Iteration 711) ===
Q mean: -4.572196
Q std: 4.464820
Actor loss: 4.576138
Action reg: 0.003942
  l1.weight: grad_norm = 0.007530
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.007128
Total gradient norm: 0.040978
=== Actor Training Debug (Iteration 712) ===
Q mean: -4.002655
Q std: 4.030828
Actor loss: 4.006626
Action reg: 0.003971
  l1.weight: grad_norm = 0.005708
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004603
Total gradient norm: 0.025440
=== Actor Training Debug (Iteration 713) ===
Q mean: -4.195101
Q std: 4.317069
Actor loss: 4.199066
Action reg: 0.003965
  l1.weight: grad_norm = 0.001651
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.001483
Total gradient norm: 0.012165
=== Actor Training Debug (Iteration 714) ===
Q mean: -4.324042
Q std: 4.091124
Actor loss: 4.328000
Action reg: 0.003957
  l1.weight: grad_norm = 0.003108
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.002669
Total gradient norm: 0.015027
=== Actor Training Debug (Iteration 715) ===
Q mean: -4.294798
Q std: 4.203451
Actor loss: 4.298748
Action reg: 0.003951
  l1.weight: grad_norm = 0.003790
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.003264
Total gradient norm: 0.021207
=== Actor Training Debug (Iteration 716) ===
Q mean: -3.666977
Q std: 3.624276
Actor loss: 3.670916
Action reg: 0.003939
  l1.weight: grad_norm = 0.004642
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.003792
Total gradient norm: 0.025857
=== Actor Training Debug (Iteration 717) ===
Q mean: -4.042627
Q std: 3.683502
Actor loss: 4.046582
Action reg: 0.003955
  l1.weight: grad_norm = 0.003285
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.003004
Total gradient norm: 0.017706
=== Actor Training Debug (Iteration 718) ===
Q mean: -4.291141
Q std: 4.103923
Actor loss: 4.295117
Action reg: 0.003976
  l1.weight: grad_norm = 0.009573
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.007703
Total gradient norm: 0.035847
=== Actor Training Debug (Iteration 719) ===
Q mean: -4.309069
Q std: 4.136800
Actor loss: 4.313028
Action reg: 0.003959
  l1.weight: grad_norm = 0.005703
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.005049
Total gradient norm: 0.042171
=== Actor Training Debug (Iteration 720) ===
Q mean: -4.007291
Q std: 4.151836
Actor loss: 4.011278
Action reg: 0.003987
  l1.weight: grad_norm = 0.004804
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004554
Total gradient norm: 0.034472
=== Actor Training Debug (Iteration 721) ===
Q mean: -4.137701
Q std: 4.490294
Actor loss: 4.141655
Action reg: 0.003954
  l1.weight: grad_norm = 0.003358
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.002839
Total gradient norm: 0.017034
=== Actor Training Debug (Iteration 722) ===
Q mean: -3.892153
Q std: 4.240361
Actor loss: 3.896131
Action reg: 0.003978
  l1.weight: grad_norm = 0.013258
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.011722
Total gradient norm: 0.070102
=== Actor Training Debug (Iteration 723) ===
Q mean: -3.825583
Q std: 4.077295
Actor loss: 3.829560
Action reg: 0.003978
  l1.weight: grad_norm = 0.005447
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004889
Total gradient norm: 0.034718
=== Actor Training Debug (Iteration 724) ===
Q mean: -4.469732
Q std: 4.576122
Actor loss: 4.473698
Action reg: 0.003966
  l1.weight: grad_norm = 0.007752
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.007511
Total gradient norm: 0.044079
=== Actor Training Debug (Iteration 725) ===
Q mean: -4.572053
Q std: 4.127726
Actor loss: 4.576026
Action reg: 0.003973
  l1.weight: grad_norm = 0.004519
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.004518
Total gradient norm: 0.030588
=== Actor Training Debug (Iteration 726) ===
Q mean: -4.460875
Q std: 4.124722
Actor loss: 4.464837
Action reg: 0.003961
  l1.weight: grad_norm = 0.007023
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.006782
Total gradient norm: 0.054651
=== Actor Training Debug (Iteration 727) ===
Q mean: -4.318903
Q std: 4.172534
Actor loss: 4.322887
Action reg: 0.003984
  l1.weight: grad_norm = 0.005132
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004884
Total gradient norm: 0.026702
=== Actor Training Debug (Iteration 728) ===
Q mean: -3.861488
Q std: 4.510489
Actor loss: 3.865441
Action reg: 0.003953
  l1.weight: grad_norm = 0.003865
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.003458
Total gradient norm: 0.025618
=== Actor Training Debug (Iteration 729) ===
Q mean: -4.007802
Q std: 3.956853
Actor loss: 4.011751
Action reg: 0.003949
  l1.weight: grad_norm = 0.006726
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.006229
Total gradient norm: 0.045188
=== Actor Training Debug (Iteration 730) ===
Q mean: -4.701138
Q std: 4.439660
Actor loss: 4.705101
Action reg: 0.003962
  l1.weight: grad_norm = 0.003469
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.003122
Total gradient norm: 0.018280
=== Actor Training Debug (Iteration 731) ===
Q mean: -4.814534
Q std: 4.096838
Actor loss: 4.818482
Action reg: 0.003949
  l1.weight: grad_norm = 0.010052
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.010581
Total gradient norm: 0.066675
=== Actor Training Debug (Iteration 732) ===
Q mean: -4.657491
Q std: 4.401333
Actor loss: 4.661438
Action reg: 0.003947
  l1.weight: grad_norm = 0.006571
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.006273
Total gradient norm: 0.038701
=== Actor Training Debug (Iteration 733) ===
Q mean: -4.066922
Q std: 4.238663
Actor loss: 4.070876
Action reg: 0.003954
  l1.weight: grad_norm = 0.003108
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.002909
Total gradient norm: 0.020983
=== Actor Training Debug (Iteration 734) ===
Q mean: -3.226390
Q std: 3.645272
Actor loss: 3.230323
Action reg: 0.003932
  l1.weight: grad_norm = 0.001909
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.001944
Total gradient norm: 0.014199
=== Actor Training Debug (Iteration 735) ===
Q mean: -4.185596
Q std: 4.307509
Actor loss: 4.189566
Action reg: 0.003969
  l1.weight: grad_norm = 0.012137
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.011538
Total gradient norm: 0.099573
=== Actor Training Debug (Iteration 736) ===
Q mean: -4.712510
Q std: 4.323036
Actor loss: 4.716465
Action reg: 0.003955
  l1.weight: grad_norm = 0.008547
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.009685
Total gradient norm: 0.082158
=== Actor Training Debug (Iteration 737) ===
Q mean: -5.005049
Q std: 4.324994
Actor loss: 5.008999
Action reg: 0.003950
  l1.weight: grad_norm = 0.003434
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.002901
Total gradient norm: 0.022557
=== Actor Training Debug (Iteration 738) ===
Q mean: -4.832559
Q std: 4.729910
Actor loss: 4.836504
Action reg: 0.003945
  l1.weight: grad_norm = 0.003151
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.002870
Total gradient norm: 0.016620
=== Actor Training Debug (Iteration 739) ===
Q mean: -3.783536
Q std: 4.176381
Actor loss: 3.787511
Action reg: 0.003975
  l1.weight: grad_norm = 0.006664
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.006307
Total gradient norm: 0.045650
=== Actor Training Debug (Iteration 740) ===
Q mean: -3.626050
Q std: 4.098344
Actor loss: 3.630017
Action reg: 0.003967
  l1.weight: grad_norm = 0.010239
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.008957
Total gradient norm: 0.044195
=== Actor Training Debug (Iteration 741) ===
Q mean: -4.325142
Q std: 4.319639
Actor loss: 4.329097
Action reg: 0.003955
  l1.weight: grad_norm = 0.003198
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.002601
Total gradient norm: 0.017946
=== Actor Training Debug (Iteration 742) ===
Q mean: -4.218774
Q std: 4.183523
Actor loss: 4.222710
Action reg: 0.003936
  l1.weight: grad_norm = 0.008658
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.007824
Total gradient norm: 0.039643
=== Actor Training Debug (Iteration 743) ===
Q mean: -4.978190
Q std: 4.510488
Actor loss: 4.982170
Action reg: 0.003979
  l1.weight: grad_norm = 0.003543
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002884
Total gradient norm: 0.018764
=== Actor Training Debug (Iteration 744) ===
Q mean: -4.257561
Q std: 4.330794
Actor loss: 4.261549
Action reg: 0.003988
  l1.weight: grad_norm = 0.005730
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004939
Total gradient norm: 0.030254
=== Actor Training Debug (Iteration 745) ===
Q mean: -3.715621
Q std: 3.830334
Actor loss: 3.719594
Action reg: 0.003973
  l1.weight: grad_norm = 0.006090
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.004834
Total gradient norm: 0.044137
=== Actor Training Debug (Iteration 746) ===
Q mean: -3.498794
Q std: 3.834736
Actor loss: 3.502735
Action reg: 0.003941
  l1.weight: grad_norm = 0.008302
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.007465
Total gradient norm: 0.041054
=== Actor Training Debug (Iteration 747) ===
Q mean: -4.373071
Q std: 4.567896
Actor loss: 4.377027
Action reg: 0.003956
  l1.weight: grad_norm = 0.011575
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.009314
Total gradient norm: 0.062614
=== Actor Training Debug (Iteration 748) ===
Q mean: -4.672103
Q std: 4.598257
Actor loss: 4.676050
Action reg: 0.003946
  l1.weight: grad_norm = 0.014066
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.012056
Total gradient norm: 0.062529
=== Actor Training Debug (Iteration 749) ===
Q mean: -4.794571
Q std: 4.497612
Actor loss: 4.798539
Action reg: 0.003967
  l1.weight: grad_norm = 0.011421
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.009614
Total gradient norm: 0.061714
=== Actor Training Debug (Iteration 750) ===
Q mean: -3.728177
Q std: 3.853599
Actor loss: 3.732149
Action reg: 0.003972
  l1.weight: grad_norm = 0.009305
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.008042
Total gradient norm: 0.063429
=== Actor Training Debug (Iteration 751) ===
Q mean: -4.031484
Q std: 4.336703
Actor loss: 4.035447
Action reg: 0.003963
  l1.weight: grad_norm = 0.005025
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.004077
Total gradient norm: 0.026960
=== Actor Training Debug (Iteration 752) ===
Q mean: -4.410715
Q std: 4.767067
Actor loss: 4.414676
Action reg: 0.003961
  l1.weight: grad_norm = 0.006178
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.004575
Total gradient norm: 0.019410
=== Actor Training Debug (Iteration 753) ===
Q mean: -4.655267
Q std: 4.539857
Actor loss: 4.659246
Action reg: 0.003980
  l1.weight: grad_norm = 0.004445
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003968
Total gradient norm: 0.025789
=== Actor Training Debug (Iteration 754) ===
Q mean: -4.754159
Q std: 4.410816
Actor loss: 4.758144
Action reg: 0.003985
  l1.weight: grad_norm = 0.008684
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.008147
Total gradient norm: 0.060313
=== Actor Training Debug (Iteration 755) ===
Q mean: -4.237357
Q std: 4.507054
Actor loss: 4.241323
Action reg: 0.003966
  l1.weight: grad_norm = 0.003298
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.003118
Total gradient norm: 0.023454
=== Actor Training Debug (Iteration 756) ===
Q mean: -3.920308
Q std: 4.165454
Actor loss: 3.924261
Action reg: 0.003954
  l1.weight: grad_norm = 0.004463
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.003495
Total gradient norm: 0.022545
=== Actor Training Debug (Iteration 757) ===
Q mean: -4.173333
Q std: 4.468808
Actor loss: 4.177314
Action reg: 0.003981
  l1.weight: grad_norm = 0.003925
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003191
Total gradient norm: 0.020361
=== Actor Training Debug (Iteration 758) ===
Q mean: -4.417980
Q std: 4.395065
Actor loss: 4.421944
Action reg: 0.003964
  l1.weight: grad_norm = 0.007550
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.006037
Total gradient norm: 0.031940
=== Actor Training Debug (Iteration 759) ===
Q mean: -4.853255
Q std: 4.224961
Actor loss: 4.857239
Action reg: 0.003984
  l1.weight: grad_norm = 0.004052
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003256
Total gradient norm: 0.020123
=== Actor Training Debug (Iteration 760) ===
Q mean: -3.838886
Q std: 3.994233
Actor loss: 3.842864
Action reg: 0.003978
  l1.weight: grad_norm = 0.009532
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.009774
Total gradient norm: 0.072065
=== Actor Training Debug (Iteration 761) ===
Q mean: -3.979641
Q std: 4.486549
Actor loss: 3.983602
Action reg: 0.003961
  l1.weight: grad_norm = 0.005074
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.004635
Total gradient norm: 0.032804
=== Actor Training Debug (Iteration 762) ===
Q mean: -4.996821
Q std: 4.802124
Actor loss: 5.000811
Action reg: 0.003989
  l1.weight: grad_norm = 0.003769
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004325
Total gradient norm: 0.035304
=== Actor Training Debug (Iteration 763) ===
Q mean: -5.013426
Q std: 4.848525
Actor loss: 5.017397
Action reg: 0.003971
  l1.weight: grad_norm = 0.006896
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005858
Total gradient norm: 0.033186
=== Actor Training Debug (Iteration 764) ===
Q mean: -4.576491
Q std: 4.260182
Actor loss: 4.580464
Action reg: 0.003973
  l1.weight: grad_norm = 0.006908
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.005515
Total gradient norm: 0.026868
=== Actor Training Debug (Iteration 765) ===
Q mean: -4.373399
Q std: 4.551068
Actor loss: 4.377327
Action reg: 0.003928
  l1.weight: grad_norm = 0.006406
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.005714
Total gradient norm: 0.038272
=== Actor Training Debug (Iteration 766) ===
Q mean: -4.063827
Q std: 4.464785
Actor loss: 4.067765
Action reg: 0.003938
  l1.weight: grad_norm = 0.004885
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.004809
Total gradient norm: 0.039956
=== Actor Training Debug (Iteration 767) ===
Q mean: -4.072714
Q std: 4.140749
Actor loss: 4.076672
Action reg: 0.003958
  l1.weight: grad_norm = 0.005156
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.003844
Total gradient norm: 0.021614
=== Actor Training Debug (Iteration 768) ===
Q mean: -4.506140
Q std: 4.443655
Actor loss: 4.510115
Action reg: 0.003975
  l1.weight: grad_norm = 0.013608
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.011057
Total gradient norm: 0.072126
=== Actor Training Debug (Iteration 769) ===
Q mean: -4.648029
Q std: 4.608105
Actor loss: 4.651999
Action reg: 0.003970
  l1.weight: grad_norm = 0.004152
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.004442
Total gradient norm: 0.040012
=== Actor Training Debug (Iteration 770) ===
Q mean: -4.306779
Q std: 4.802797
Actor loss: 4.310743
Action reg: 0.003963
  l1.weight: grad_norm = 0.001674
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.001403
Total gradient norm: 0.008190
=== Actor Training Debug (Iteration 771) ===
Q mean: -4.682391
Q std: 4.934694
Actor loss: 4.686367
Action reg: 0.003976
  l1.weight: grad_norm = 0.009706
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.008305
Total gradient norm: 0.044902
=== Actor Training Debug (Iteration 772) ===
Q mean: -3.949119
Q std: 4.549716
Actor loss: 3.953072
Action reg: 0.003952
  l1.weight: grad_norm = 0.009468
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.008430
Total gradient norm: 0.043074
=== Actor Training Debug (Iteration 773) ===
Q mean: -4.831468
Q std: 4.649758
Actor loss: 4.835429
Action reg: 0.003961
  l1.weight: grad_norm = 0.005973
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.005532
Total gradient norm: 0.032648
=== Actor Training Debug (Iteration 774) ===
Q mean: -4.149322
Q std: 4.094650
Actor loss: 4.153275
Action reg: 0.003954
  l1.weight: grad_norm = 0.005234
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.005156
Total gradient norm: 0.033410
=== Actor Training Debug (Iteration 775) ===
Q mean: -4.033628
Q std: 4.151643
Actor loss: 4.037613
Action reg: 0.003986
  l1.weight: grad_norm = 0.002671
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.002259
Total gradient norm: 0.013101
=== Actor Training Debug (Iteration 776) ===
Q mean: -4.531405
Q std: 4.633068
Actor loss: 4.535366
Action reg: 0.003961
  l1.weight: grad_norm = 0.005360
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.004488
Total gradient norm: 0.029278
=== Actor Training Debug (Iteration 777) ===
Q mean: -3.844695
Q std: 4.102312
Actor loss: 3.848676
Action reg: 0.003981
  l1.weight: grad_norm = 0.007070
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.006688
Total gradient norm: 0.045061
=== Actor Training Debug (Iteration 778) ===
Q mean: -4.417641
Q std: 4.590464
Actor loss: 4.421616
Action reg: 0.003975
  l1.weight: grad_norm = 0.008803
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.008892
Total gradient norm: 0.060051
=== Actor Training Debug (Iteration 779) ===
Q mean: -4.680175
Q std: 4.754033
Actor loss: 4.684144
Action reg: 0.003969
  l1.weight: grad_norm = 0.005087
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004430
Total gradient norm: 0.029221
=== Actor Training Debug (Iteration 780) ===
Q mean: -4.201493
Q std: 4.552248
Actor loss: 4.205459
Action reg: 0.003966
  l1.weight: grad_norm = 0.002954
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.002686
Total gradient norm: 0.012885
=== Actor Training Debug (Iteration 781) ===
Q mean: -4.332440
Q std: 4.758979
Actor loss: 4.336416
Action reg: 0.003976
  l1.weight: grad_norm = 0.005184
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.004356
Total gradient norm: 0.022183
=== Actor Training Debug (Iteration 782) ===
Q mean: -4.143829
Q std: 4.147758
Actor loss: 4.147807
Action reg: 0.003978
  l1.weight: grad_norm = 0.004207
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.003087
Total gradient norm: 0.013098
=== Actor Training Debug (Iteration 783) ===
Q mean: -4.048268
Q std: 4.289132
Actor loss: 4.052227
Action reg: 0.003958
  l1.weight: grad_norm = 0.008905
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.007600
Total gradient norm: 0.035421
=== Actor Training Debug (Iteration 784) ===
Q mean: -4.329505
Q std: 4.256865
Actor loss: 4.333469
Action reg: 0.003964
  l1.weight: grad_norm = 0.004763
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.003588
Total gradient norm: 0.027220
=== Actor Training Debug (Iteration 785) ===
Q mean: -4.473299
Q std: 4.941881
Actor loss: 4.477270
Action reg: 0.003971
  l1.weight: grad_norm = 0.011201
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.009849
Total gradient norm: 0.053864
=== Actor Training Debug (Iteration 786) ===
Q mean: -4.003020
Q std: 4.364630
Actor loss: 4.006989
Action reg: 0.003969
  l1.weight: grad_norm = 0.007226
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.005943
Total gradient norm: 0.029150
=== Actor Training Debug (Iteration 787) ===
Q mean: -4.343651
Q std: 4.733238
Actor loss: 4.347614
Action reg: 0.003963
  l1.weight: grad_norm = 0.003717
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.003051
Total gradient norm: 0.014783
=== Actor Training Debug (Iteration 788) ===
Q mean: -4.605324
Q std: 4.421233
Actor loss: 4.609306
Action reg: 0.003983
  l1.weight: grad_norm = 0.003030
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002387
Total gradient norm: 0.013614
=== Actor Training Debug (Iteration 789) ===
Q mean: -4.729776
Q std: 4.572138
Actor loss: 4.733738
Action reg: 0.003962
  l1.weight: grad_norm = 0.007300
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.006608
Total gradient norm: 0.045459
=== Actor Training Debug (Iteration 790) ===
Q mean: -4.400416
Q std: 4.461006
Actor loss: 4.404392
Action reg: 0.003976
  l1.weight: grad_norm = 0.009391
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.007141
Total gradient norm: 0.043906
=== Actor Training Debug (Iteration 791) ===
Q mean: -4.240306
Q std: 4.643265
Actor loss: 4.244268
Action reg: 0.003963
  l1.weight: grad_norm = 0.005986
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.005154
Total gradient norm: 0.030883
=== Actor Training Debug (Iteration 792) ===
Q mean: -3.830466
Q std: 4.090396
Actor loss: 3.834437
Action reg: 0.003971
  l1.weight: grad_norm = 0.009693
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.008134
Total gradient norm: 0.044313
=== Actor Training Debug (Iteration 793) ===
Q mean: -4.206275
Q std: 4.449426
Actor loss: 4.210216
Action reg: 0.003940
  l1.weight: grad_norm = 0.004547
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.005155
Total gradient norm: 0.036610
=== Actor Training Debug (Iteration 794) ===
Q mean: -5.037385
Q std: 5.055690
Actor loss: 5.041358
Action reg: 0.003973
  l1.weight: grad_norm = 0.003555
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.003327
Total gradient norm: 0.018350
=== Actor Training Debug (Iteration 795) ===
Q mean: -4.309675
Q std: 4.728999
Actor loss: 4.313650
Action reg: 0.003974
  l1.weight: grad_norm = 0.004460
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.003296
Total gradient norm: 0.016311
=== Actor Training Debug (Iteration 796) ===
Q mean: -4.189201
Q std: 4.653715
Actor loss: 4.193176
Action reg: 0.003975
  l1.weight: grad_norm = 0.003032
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.002563
Total gradient norm: 0.018674
=== Actor Training Debug (Iteration 797) ===
Q mean: -4.171453
Q std: 4.637280
Actor loss: 4.175428
Action reg: 0.003975
  l1.weight: grad_norm = 0.002710
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.001915
Total gradient norm: 0.012179
=== Actor Training Debug (Iteration 798) ===
Q mean: -4.770071
Q std: 4.820566
Actor loss: 4.774021
Action reg: 0.003950
  l1.weight: grad_norm = 0.005401
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.004491
Total gradient norm: 0.039947
=== Actor Training Debug (Iteration 799) ===
Q mean: -4.284619
Q std: 4.737859
Actor loss: 4.288581
Action reg: 0.003962
  l1.weight: grad_norm = 0.001051
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.000944
Total gradient norm: 0.006016
=== Actor Training Debug (Iteration 800) ===
Q mean: -4.805869
Q std: 4.544259
Actor loss: 4.809854
Action reg: 0.003985
  l1.weight: grad_norm = 0.007039
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.005772
Total gradient norm: 0.034435
=== Actor Training Debug (Iteration 801) ===
Q mean: -4.054586
Q std: 4.357607
Actor loss: 4.058567
Action reg: 0.003981
  l1.weight: grad_norm = 0.007847
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006132
Total gradient norm: 0.029364
=== Actor Training Debug (Iteration 802) ===
Q mean: -4.519429
Q std: 4.553554
Actor loss: 4.523409
Action reg: 0.003980
  l1.weight: grad_norm = 0.009610
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.009479
Total gradient norm: 0.071887
=== Actor Training Debug (Iteration 803) ===
Q mean: -4.255839
Q std: 4.801702
Actor loss: 4.259830
Action reg: 0.003991
  l1.weight: grad_norm = 0.005542
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005335
Total gradient norm: 0.033175
=== Actor Training Debug (Iteration 804) ===
Q mean: -4.510768
Q std: 4.859885
Actor loss: 4.514736
Action reg: 0.003968
  l1.weight: grad_norm = 0.004858
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.003676
Total gradient norm: 0.021508
=== Actor Training Debug (Iteration 805) ===
Q mean: -5.409312
Q std: 5.177846
Actor loss: 5.413286
Action reg: 0.003974
  l1.weight: grad_norm = 0.011957
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.010492
Total gradient norm: 0.067261
=== Actor Training Debug (Iteration 806) ===
Q mean: -4.648822
Q std: 4.802444
Actor loss: 4.652775
Action reg: 0.003953
  l1.weight: grad_norm = 0.008058
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.008297
Total gradient norm: 0.051042
=== Actor Training Debug (Iteration 807) ===
Q mean: -4.788531
Q std: 4.848196
Actor loss: 4.792500
Action reg: 0.003968
  l1.weight: grad_norm = 0.004436
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.003911
Total gradient norm: 0.024940
=== Actor Training Debug (Iteration 808) ===
Q mean: -3.760520
Q std: 4.537638
Actor loss: 3.764498
Action reg: 0.003978
  l1.weight: grad_norm = 0.007307
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.006221
Total gradient norm: 0.037602
=== Actor Training Debug (Iteration 809) ===
Q mean: -4.330802
Q std: 4.981555
Actor loss: 4.334780
Action reg: 0.003978
  l1.weight: grad_norm = 0.008529
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.007413
Total gradient norm: 0.044592
=== Actor Training Debug (Iteration 810) ===
Q mean: -4.203343
Q std: 4.346147
Actor loss: 4.207320
Action reg: 0.003977
  l1.weight: grad_norm = 0.001740
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.001441
Total gradient norm: 0.008255
=== Actor Training Debug (Iteration 811) ===
Q mean: -5.388899
Q std: 4.941902
Actor loss: 5.392894
Action reg: 0.003995
  l1.weight: grad_norm = 0.003131
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002301
Total gradient norm: 0.014306
=== Actor Training Debug (Iteration 812) ===
Q mean: -5.042824
Q std: 5.033926
Actor loss: 5.046782
Action reg: 0.003958
  l1.weight: grad_norm = 0.002043
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.001761
Total gradient norm: 0.010153
=== Actor Training Debug (Iteration 813) ===
Q mean: -4.662646
Q std: 4.452882
Actor loss: 4.666578
Action reg: 0.003932
  l1.weight: grad_norm = 0.006075
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.006015
Total gradient norm: 0.043573
=== Actor Training Debug (Iteration 814) ===
Q mean: -4.385386
Q std: 4.628908
Actor loss: 4.389355
Action reg: 0.003969
  l1.weight: grad_norm = 0.014245
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.012384
Total gradient norm: 0.070949
=== Actor Training Debug (Iteration 815) ===
Q mean: -3.869870
Q std: 4.695106
Actor loss: 3.873828
Action reg: 0.003958
  l1.weight: grad_norm = 0.007507
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.005468
Total gradient norm: 0.022577
=== Actor Training Debug (Iteration 816) ===
Q mean: -3.860958
Q std: 4.891581
Actor loss: 3.864929
Action reg: 0.003971
  l1.weight: grad_norm = 0.013053
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.011785
Total gradient norm: 0.085534
=== Actor Training Debug (Iteration 817) ===
Q mean: -4.232320
Q std: 4.588679
Actor loss: 4.236274
Action reg: 0.003954
  l1.weight: grad_norm = 0.005954
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.004749
Total gradient norm: 0.020460
=== Actor Training Debug (Iteration 818) ===
Q mean: -4.496910
Q std: 4.864936
Actor loss: 4.500863
Action reg: 0.003954
  l1.weight: grad_norm = 0.007687
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.006049
Total gradient norm: 0.034121
=== Actor Training Debug (Iteration 819) ===
Q mean: -5.003551
Q std: 4.852009
Actor loss: 5.007517
Action reg: 0.003966
  l1.weight: grad_norm = 0.003561
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.002968
Total gradient norm: 0.016846
=== Actor Training Debug (Iteration 820) ===
Q mean: -4.965058
Q std: 4.583930
Actor loss: 4.969028
Action reg: 0.003970
  l1.weight: grad_norm = 0.007252
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.006542
Total gradient norm: 0.034778
=== Actor Training Debug (Iteration 821) ===
Q mean: -4.949875
Q std: 5.219049
Actor loss: 4.953834
Action reg: 0.003959
  l1.weight: grad_norm = 0.005335
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.004672
Total gradient norm: 0.029096
=== Actor Training Debug (Iteration 822) ===
Q mean: -3.777909
Q std: 4.386720
Actor loss: 3.781882
Action reg: 0.003973
  l1.weight: grad_norm = 0.006977
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.006096
Total gradient norm: 0.031595
=== Actor Training Debug (Iteration 823) ===
Q mean: -3.981017
Q std: 4.701838
Actor loss: 3.984983
Action reg: 0.003966
  l1.weight: grad_norm = 0.008850
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.009067
Total gradient norm: 0.047998
=== Actor Training Debug (Iteration 824) ===
Q mean: -4.064457
Q std: 4.190210
Actor loss: 4.068426
Action reg: 0.003969
  l1.weight: grad_norm = 0.007264
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.006029
Total gradient norm: 0.029520
=== Actor Training Debug (Iteration 825) ===
Q mean: -5.255173
Q std: 4.915652
Actor loss: 5.259150
Action reg: 0.003977
  l1.weight: grad_norm = 0.008579
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.008050
Total gradient norm: 0.063466
=== Actor Training Debug (Iteration 826) ===
Q mean: -5.068627
Q std: 5.163160
Actor loss: 5.072596
Action reg: 0.003968
  l1.weight: grad_norm = 0.004512
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.003484
Total gradient norm: 0.015625
=== Actor Training Debug (Iteration 827) ===
Q mean: -4.605727
Q std: 4.818188
Actor loss: 4.609692
Action reg: 0.003965
  l1.weight: grad_norm = 0.006254
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.005191
Total gradient norm: 0.028734
=== Actor Training Debug (Iteration 828) ===
Q mean: -3.765549
Q std: 4.723236
Actor loss: 3.769528
Action reg: 0.003979
  l1.weight: grad_norm = 0.008671
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.006843
Total gradient norm: 0.032633
=== Actor Training Debug (Iteration 829) ===
Q mean: -4.028873
Q std: 5.159961
Actor loss: 4.032856
Action reg: 0.003983
  l1.weight: grad_norm = 0.004396
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003513
Total gradient norm: 0.016182
=== Actor Training Debug (Iteration 830) ===
Q mean: -3.926293
Q std: 4.975852
Actor loss: 3.930245
Action reg: 0.003951
  l1.weight: grad_norm = 0.003476
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.003204
Total gradient norm: 0.016724
=== Actor Training Debug (Iteration 831) ===
Q mean: -4.864805
Q std: 5.179832
Actor loss: 4.868751
Action reg: 0.003946
  l1.weight: grad_norm = 0.004042
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.004155
Total gradient norm: 0.022869
=== Actor Training Debug (Iteration 832) ===
Q mean: -5.223053
Q std: 4.835778
Actor loss: 5.227021
Action reg: 0.003968
  l1.weight: grad_norm = 0.003674
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.003703
Total gradient norm: 0.028197
=== Actor Training Debug (Iteration 833) ===
Q mean: -4.989100
Q std: 4.373086
Actor loss: 4.993080
Action reg: 0.003981
  l1.weight: grad_norm = 0.005117
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004459
Total gradient norm: 0.033886
=== Actor Training Debug (Iteration 834) ===
Q mean: -5.304228
Q std: 5.159102
Actor loss: 5.308210
Action reg: 0.003982
  l1.weight: grad_norm = 0.006233
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.006061
Total gradient norm: 0.032417
=== Actor Training Debug (Iteration 835) ===
Q mean: -3.755783
Q std: 4.376653
Actor loss: 3.759742
Action reg: 0.003959
  l1.weight: grad_norm = 0.005796
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.005206
Total gradient norm: 0.042751
=== Actor Training Debug (Iteration 836) ===
Q mean: -3.787479
Q std: 4.391854
Actor loss: 3.791431
Action reg: 0.003952
  l1.weight: grad_norm = 0.009232
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.008386
Total gradient norm: 0.048445
=== Actor Training Debug (Iteration 837) ===
Q mean: -5.052386
Q std: 5.458545
Actor loss: 5.056365
Action reg: 0.003979
  l1.weight: grad_norm = 0.004603
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003718
Total gradient norm: 0.018662
=== Actor Training Debug (Iteration 838) ===
Q mean: -5.160886
Q std: 5.091445
Actor loss: 5.164854
Action reg: 0.003968
  l1.weight: grad_norm = 0.010076
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.008439
Total gradient norm: 0.054428
=== Actor Training Debug (Iteration 839) ===
Q mean: -6.126977
Q std: 5.721455
Actor loss: 6.130960
Action reg: 0.003982
  l1.weight: grad_norm = 0.006843
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.005293
Total gradient norm: 0.029138
=== Actor Training Debug (Iteration 840) ===
Q mean: -5.116959
Q std: 4.906672
Actor loss: 5.120931
Action reg: 0.003973
  l1.weight: grad_norm = 0.006750
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.006382
Total gradient norm: 0.041464
=== Actor Training Debug (Iteration 841) ===
Q mean: -4.089944
Q std: 4.587347
Actor loss: 4.093922
Action reg: 0.003977
  l1.weight: grad_norm = 0.007660
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.008149
Total gradient norm: 0.055543
=== Actor Training Debug (Iteration 842) ===
Q mean: -4.313461
Q std: 4.498384
Actor loss: 4.317423
Action reg: 0.003962
  l1.weight: grad_norm = 0.005806
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.004791
Total gradient norm: 0.024625
=== Actor Training Debug (Iteration 843) ===
Q mean: -4.956477
Q std: 5.115445
Actor loss: 4.960454
Action reg: 0.003977
  l1.weight: grad_norm = 0.005776
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.004213
Total gradient norm: 0.020860
=== Actor Training Debug (Iteration 844) ===
Q mean: -4.947109
Q std: 5.270437
Actor loss: 4.951066
Action reg: 0.003957
  l1.weight: grad_norm = 0.007256
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.006359
Total gradient norm: 0.050950
=== Actor Training Debug (Iteration 845) ===
Q mean: -5.011024
Q std: 4.890867
Actor loss: 5.014999
Action reg: 0.003974
  l1.weight: grad_norm = 0.005277
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.004344
Total gradient norm: 0.027991
=== Actor Training Debug (Iteration 846) ===
Q mean: -4.573869
Q std: 4.897723
Actor loss: 4.577848
Action reg: 0.003979
  l1.weight: grad_norm = 0.003819
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.003295
Total gradient norm: 0.017531
=== Actor Training Debug (Iteration 847) ===
Q mean: -4.351171
Q std: 5.030240
Actor loss: 4.355126
Action reg: 0.003956
  l1.weight: grad_norm = 0.011011
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.008783
Total gradient norm: 0.042935
=== Actor Training Debug (Iteration 848) ===
Q mean: -4.869483
Q std: 5.419271
Actor loss: 4.873456
Action reg: 0.003974
  l1.weight: grad_norm = 0.006353
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.004870
Total gradient norm: 0.023760
=== Actor Training Debug (Iteration 849) ===
Q mean: -4.667558
Q std: 4.855532
Actor loss: 4.671542
Action reg: 0.003984
  l1.weight: grad_norm = 0.008910
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.008277
Total gradient norm: 0.035544
=== Actor Training Debug (Iteration 850) ===
Q mean: -4.937583
Q std: 5.257704
Actor loss: 4.941552
Action reg: 0.003969
  l1.weight: grad_norm = 0.004702
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.004910
Total gradient norm: 0.030003
=== Actor Training Debug (Iteration 851) ===
Q mean: -5.605010
Q std: 5.264520
Actor loss: 5.608980
Action reg: 0.003970
  l1.weight: grad_norm = 0.012774
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.009988
Total gradient norm: 0.057179
=== Actor Training Debug (Iteration 852) ===
Q mean: -4.927916
Q std: 4.994988
Actor loss: 4.931890
Action reg: 0.003974
  l1.weight: grad_norm = 0.012317
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.010395
Total gradient norm: 0.052246
=== Actor Training Debug (Iteration 853) ===
Q mean: -4.293702
Q std: 4.451334
Actor loss: 4.297667
Action reg: 0.003965
  l1.weight: grad_norm = 0.006964
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.005742
Total gradient norm: 0.028416
=== Actor Training Debug (Iteration 854) ===
Q mean: -4.471717
Q std: 4.731750
Actor loss: 4.475699
Action reg: 0.003982
  l1.weight: grad_norm = 0.008168
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.006660
Total gradient norm: 0.038431
=== Actor Training Debug (Iteration 855) ===
Q mean: -5.004414
Q std: 5.358844
Actor loss: 5.008370
Action reg: 0.003957
  l1.weight: grad_norm = 0.007519
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.005509
Total gradient norm: 0.031043
=== Actor Training Debug (Iteration 856) ===
Q mean: -4.688499
Q std: 4.895999
Actor loss: 4.692455
Action reg: 0.003956
  l1.weight: grad_norm = 0.012780
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.009920
Total gradient norm: 0.056079
=== Actor Training Debug (Iteration 857) ===
Q mean: -4.675624
Q std: 5.227729
Actor loss: 4.679577
Action reg: 0.003952
  l1.weight: grad_norm = 0.005670
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.004837
Total gradient norm: 0.024677
=== Actor Training Debug (Iteration 858) ===
Q mean: -4.625593
Q std: 5.391645
Actor loss: 4.629557
Action reg: 0.003964
  l1.weight: grad_norm = 0.009771
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.008204
Total gradient norm: 0.046162
=== Actor Training Debug (Iteration 859) ===
Q mean: -5.073666
Q std: 5.405146
Actor loss: 5.077647
Action reg: 0.003982
  l1.weight: grad_norm = 0.004195
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.003587
Total gradient norm: 0.018653
=== Actor Training Debug (Iteration 860) ===
Q mean: -4.712660
Q std: 4.693893
Actor loss: 4.716633
Action reg: 0.003973
  l1.weight: grad_norm = 0.008966
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.008158
Total gradient norm: 0.040260
=== Actor Training Debug (Iteration 861) ===
Q mean: -4.983760
Q std: 4.967454
Actor loss: 4.987746
Action reg: 0.003986
  l1.weight: grad_norm = 0.009180
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.007764
Total gradient norm: 0.040942
=== Actor Training Debug (Iteration 862) ===
Q mean: -5.247561
Q std: 5.348815
Actor loss: 5.251534
Action reg: 0.003973
  l1.weight: grad_norm = 0.005230
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.004057
Total gradient norm: 0.020427
=== Actor Training Debug (Iteration 863) ===
Q mean: -4.348650
Q std: 4.930343
Actor loss: 4.352628
Action reg: 0.003979
  l1.weight: grad_norm = 0.004134
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003471
Total gradient norm: 0.023952
=== Actor Training Debug (Iteration 864) ===
Q mean: -4.339912
Q std: 4.640215
Actor loss: 4.343879
Action reg: 0.003967
  l1.weight: grad_norm = 0.011687
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.009864
Total gradient norm: 0.038081
=== Actor Training Debug (Iteration 865) ===
Q mean: -4.691016
Q std: 4.897799
Actor loss: 4.694966
Action reg: 0.003951
  l1.weight: grad_norm = 0.005863
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.004935
Total gradient norm: 0.035107
=== Actor Training Debug (Iteration 866) ===
Q mean: -5.167886
Q std: 5.092003
Actor loss: 5.171848
Action reg: 0.003963
  l1.weight: grad_norm = 0.010916
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.009759
Total gradient norm: 0.063033
=== Actor Training Debug (Iteration 867) ===
Q mean: -5.087136
Q std: 5.165555
Actor loss: 5.091108
Action reg: 0.003971
  l1.weight: grad_norm = 0.012495
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.009586
Total gradient norm: 0.058099
=== Actor Training Debug (Iteration 868) ===
Q mean: -4.659983
Q std: 5.305482
Actor loss: 4.663960
Action reg: 0.003977
  l1.weight: grad_norm = 0.007845
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.005697
Total gradient norm: 0.027591
=== Actor Training Debug (Iteration 869) ===
Q mean: -4.719253
Q std: 5.290779
Actor loss: 4.723234
Action reg: 0.003981
  l1.weight: grad_norm = 0.007833
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.006482
Total gradient norm: 0.029582
=== Actor Training Debug (Iteration 870) ===
Q mean: -4.863753
Q std: 5.078273
Actor loss: 4.867744
Action reg: 0.003991
  l1.weight: grad_norm = 0.012724
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010058
Total gradient norm: 0.047844
=== Actor Training Debug (Iteration 871) ===
Q mean: -5.571651
Q std: 5.576973
Actor loss: 5.575625
Action reg: 0.003974
  l1.weight: grad_norm = 0.010223
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.009180
Total gradient norm: 0.047166
=== Actor Training Debug (Iteration 872) ===
Q mean: -4.560039
Q std: 5.080863
Actor loss: 4.564010
Action reg: 0.003971
  l1.weight: grad_norm = 0.005111
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.004346
Total gradient norm: 0.026872
=== Actor Training Debug (Iteration 873) ===
Q mean: -4.603770
Q std: 5.306548
Actor loss: 4.607747
Action reg: 0.003976
  l1.weight: grad_norm = 0.005095
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.004469
Total gradient norm: 0.022014
=== Actor Training Debug (Iteration 874) ===
Q mean: -5.169905
Q std: 5.720260
Actor loss: 5.173892
Action reg: 0.003988
  l1.weight: grad_norm = 0.004117
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003251
Total gradient norm: 0.020134
=== Actor Training Debug (Iteration 875) ===
Q mean: -5.028579
Q std: 5.166264
Actor loss: 5.032559
Action reg: 0.003980
  l1.weight: grad_norm = 0.010042
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.007420
Total gradient norm: 0.045244
=== Actor Training Debug (Iteration 876) ===
Q mean: -4.772636
Q std: 5.134946
Actor loss: 4.776612
Action reg: 0.003976
  l1.weight: grad_norm = 0.014455
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.010266
Total gradient norm: 0.052372
=== Actor Training Debug (Iteration 877) ===
Q mean: -5.342256
Q std: 5.324494
Actor loss: 5.346236
Action reg: 0.003980
  l1.weight: grad_norm = 0.007308
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.006165
Total gradient norm: 0.033155
=== Actor Training Debug (Iteration 878) ===
Q mean: -5.040415
Q std: 5.443593
Actor loss: 5.044393
Action reg: 0.003978
  l1.weight: grad_norm = 0.005996
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.005178
Total gradient norm: 0.026747
=== Actor Training Debug (Iteration 879) ===
Q mean: -4.396265
Q std: 4.896706
Actor loss: 4.400238
Action reg: 0.003973
  l1.weight: grad_norm = 0.006710
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.007047
Total gradient norm: 0.045602
=== Actor Training Debug (Iteration 880) ===
Q mean: -4.630964
Q std: 4.982701
Actor loss: 4.634937
Action reg: 0.003973
  l1.weight: grad_norm = 0.002910
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.002464
Total gradient norm: 0.013914
=== Actor Training Debug (Iteration 881) ===
Q mean: -5.381729
Q std: 5.473448
Actor loss: 5.385711
Action reg: 0.003983
  l1.weight: grad_norm = 0.004893
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.004445
Total gradient norm: 0.023070
=== Actor Training Debug (Iteration 882) ===
Q mean: -5.183042
Q std: 5.158682
Actor loss: 5.186999
Action reg: 0.003957
  l1.weight: grad_norm = 0.006105
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.004932
Total gradient norm: 0.032420
=== Actor Training Debug (Iteration 883) ===
Q mean: -4.914206
Q std: 5.207954
Actor loss: 4.918189
Action reg: 0.003983
  l1.weight: grad_norm = 0.002185
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001895
Total gradient norm: 0.011440
=== Actor Training Debug (Iteration 884) ===
Q mean: -4.614754
Q std: 5.645998
Actor loss: 4.618735
Action reg: 0.003982
  l1.weight: grad_norm = 0.003573
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.003421
Total gradient norm: 0.025406
=== Actor Training Debug (Iteration 885) ===
Q mean: -4.494449
Q std: 5.035105
Actor loss: 4.498438
Action reg: 0.003989
  l1.weight: grad_norm = 0.011465
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010136
Total gradient norm: 0.077035
=== Actor Training Debug (Iteration 886) ===
Q mean: -5.055016
Q std: 5.089195
Actor loss: 5.058978
Action reg: 0.003962
  l1.weight: grad_norm = 0.006643
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.005043
Total gradient norm: 0.030049
=== Actor Training Debug (Iteration 887) ===
Q mean: -5.922929
Q std: 5.961702
Actor loss: 5.926909
Action reg: 0.003980
  l1.weight: grad_norm = 0.004571
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.004361
Total gradient norm: 0.026198
=== Actor Training Debug (Iteration 888) ===
Q mean: -4.946442
Q std: 5.097054
Actor loss: 4.950414
Action reg: 0.003972
  l1.weight: grad_norm = 0.007830
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.007413
Total gradient norm: 0.039854
=== Actor Training Debug (Iteration 889) ===
Q mean: -4.821483
Q std: 5.155370
Actor loss: 4.825463
Action reg: 0.003980
  l1.weight: grad_norm = 0.005751
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.004663
Total gradient norm: 0.026224
=== Actor Training Debug (Iteration 890) ===
Q mean: -4.985876
Q std: 5.488666
Actor loss: 4.989847
Action reg: 0.003971
  l1.weight: grad_norm = 0.007801
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.005622
Total gradient norm: 0.026463
=== Actor Training Debug (Iteration 891) ===
Q mean: -4.159348
Q std: 4.701972
Actor loss: 4.163330
Action reg: 0.003982
  l1.weight: grad_norm = 0.010689
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.009564
Total gradient norm: 0.050491
=== Actor Training Debug (Iteration 892) ===
Q mean: -4.214459
Q std: 5.024148
Actor loss: 4.218436
Action reg: 0.003977
  l1.weight: grad_norm = 0.012189
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.011375
Total gradient norm: 0.058705
=== Actor Training Debug (Iteration 893) ===
Q mean: -4.477466
Q std: 5.101903
Actor loss: 4.481454
Action reg: 0.003988
  l1.weight: grad_norm = 0.000888
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.000719
Total gradient norm: 0.004292
=== Actor Training Debug (Iteration 894) ===
Q mean: -4.968274
Q std: 5.655657
Actor loss: 4.972249
Action reg: 0.003975
  l1.weight: grad_norm = 0.010239
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.008103
Total gradient norm: 0.035769
=== Actor Training Debug (Iteration 895) ===
Q mean: -5.215300
Q std: 5.254119
Actor loss: 5.219292
Action reg: 0.003992
  l1.weight: grad_norm = 0.001925
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001675
Total gradient norm: 0.010621
=== Actor Training Debug (Iteration 896) ===
Q mean: -5.138759
Q std: 4.997885
Actor loss: 5.142731
Action reg: 0.003972
  l1.weight: grad_norm = 0.010924
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.010211
Total gradient norm: 0.047072
=== Actor Training Debug (Iteration 897) ===
Q mean: -4.779041
Q std: 5.070334
Actor loss: 4.783017
Action reg: 0.003975
  l1.weight: grad_norm = 0.003620
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.002852
Total gradient norm: 0.014481
=== Actor Training Debug (Iteration 898) ===
Q mean: -4.526676
Q std: 4.962767
Actor loss: 4.530663
Action reg: 0.003988
  l1.weight: grad_norm = 0.004297
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.003398
Total gradient norm: 0.021601
=== Actor Training Debug (Iteration 899) ===
Q mean: -4.790344
Q std: 5.515859
Actor loss: 4.794322
Action reg: 0.003978
  l1.weight: grad_norm = 0.005810
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.005057
Total gradient norm: 0.032697
=== Actor Training Debug (Iteration 900) ===
Q mean: -5.097923
Q std: 5.623525
Actor loss: 5.101885
Action reg: 0.003962
  l1.weight: grad_norm = 0.008454
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.007055
Total gradient norm: 0.040198
=== Actor Training Debug (Iteration 901) ===
Q mean: -5.576356
Q std: 5.498330
Actor loss: 5.580329
Action reg: 0.003973
  l1.weight: grad_norm = 0.003308
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.002444
Total gradient norm: 0.011101
=== Actor Training Debug (Iteration 902) ===
Q mean: -5.270284
Q std: 5.621209
Actor loss: 5.274265
Action reg: 0.003981
  l1.weight: grad_norm = 0.006475
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004545
Total gradient norm: 0.019316
=== Actor Training Debug (Iteration 903) ===
Q mean: -5.260980
Q std: 5.811224
Actor loss: 5.264956
Action reg: 0.003976
  l1.weight: grad_norm = 0.007245
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005757
Total gradient norm: 0.024033
=== Actor Training Debug (Iteration 904) ===
Q mean: -4.840080
Q std: 5.470400
Actor loss: 4.844055
Action reg: 0.003975
  l1.weight: grad_norm = 0.006613
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.005982
Total gradient norm: 0.027201
=== Actor Training Debug (Iteration 905) ===
Q mean: -4.784074
Q std: 5.504702
Actor loss: 4.788064
Action reg: 0.003990
  l1.weight: grad_norm = 0.004108
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003813
Total gradient norm: 0.019039
=== Actor Training Debug (Iteration 906) ===
Q mean: -4.815141
Q std: 4.693798
Actor loss: 4.819125
Action reg: 0.003984
  l1.weight: grad_norm = 0.012287
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.010047
Total gradient norm: 0.068445
=== Actor Training Debug (Iteration 907) ===
Q mean: -5.529911
Q std: 5.814067
Actor loss: 5.533874
Action reg: 0.003963
  l1.weight: grad_norm = 0.007737
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.005990
Total gradient norm: 0.030761
=== Actor Training Debug (Iteration 908) ===
Q mean: -4.527634
Q std: 4.893802
Actor loss: 4.531613
Action reg: 0.003979
  l1.weight: grad_norm = 0.006986
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.006743
Total gradient norm: 0.043603
=== Actor Training Debug (Iteration 909) ===
Q mean: -5.068233
Q std: 5.397569
Actor loss: 5.072213
Action reg: 0.003981
  l1.weight: grad_norm = 0.004275
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.003676
Total gradient norm: 0.018321
=== Actor Training Debug (Iteration 910) ===
Q mean: -4.979843
Q std: 5.613301
Actor loss: 4.983803
Action reg: 0.003960
  l1.weight: grad_norm = 0.006415
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.005363
Total gradient norm: 0.025208
=== Actor Training Debug (Iteration 911) ===
Q mean: -4.887424
Q std: 5.448030
Actor loss: 4.891396
Action reg: 0.003973
  l1.weight: grad_norm = 0.005961
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.004959
Total gradient norm: 0.024471
=== Actor Training Debug (Iteration 912) ===
Q mean: -4.945861
Q std: 5.369123
Actor loss: 4.949802
Action reg: 0.003941
  l1.weight: grad_norm = 0.008632
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.007656
Total gradient norm: 0.044133
=== Actor Training Debug (Iteration 913) ===
Q mean: -5.007108
Q std: 5.417525
Actor loss: 5.011080
Action reg: 0.003972
  l1.weight: grad_norm = 0.003382
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.003444
Total gradient norm: 0.021887
=== Actor Training Debug (Iteration 914) ===
Q mean: -4.466781
Q std: 5.284702
Actor loss: 4.470769
Action reg: 0.003988
  l1.weight: grad_norm = 0.006157
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004992
Total gradient norm: 0.026921
=== Actor Training Debug (Iteration 915) ===
Q mean: -5.282963
Q std: 5.837440
Actor loss: 5.286926
Action reg: 0.003963
  l1.weight: grad_norm = 0.004950
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.004543
Total gradient norm: 0.023760
=== Actor Training Debug (Iteration 916) ===
Q mean: -4.446885
Q std: 5.361256
Actor loss: 4.450844
Action reg: 0.003959
  l1.weight: grad_norm = 0.005230
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.004556
Total gradient norm: 0.030244
=== Actor Training Debug (Iteration 917) ===
Q mean: -5.535978
Q std: 5.686912
Actor loss: 5.539936
Action reg: 0.003958
  l1.weight: grad_norm = 0.003863
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.003484
Total gradient norm: 0.025730
=== Actor Training Debug (Iteration 918) ===
Q mean: -5.096083
Q std: 5.682044
Actor loss: 5.100059
Action reg: 0.003976
  l1.weight: grad_norm = 0.007922
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.006718
Total gradient norm: 0.036169
=== Actor Training Debug (Iteration 919) ===
Q mean: -4.984197
Q std: 5.321431
Actor loss: 4.988174
Action reg: 0.003978
  l1.weight: grad_norm = 0.006999
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.006516
Total gradient norm: 0.034110
=== Actor Training Debug (Iteration 920) ===
Q mean: -4.620447
Q std: 5.314271
Actor loss: 4.624423
Action reg: 0.003975
  l1.weight: grad_norm = 0.003360
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002948
Total gradient norm: 0.016854
=== Actor Training Debug (Iteration 921) ===
Q mean: -5.207319
Q std: 5.407797
Actor loss: 5.211293
Action reg: 0.003974
  l1.weight: grad_norm = 0.007103
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.005466
Total gradient norm: 0.033915
=== Actor Training Debug (Iteration 922) ===
Q mean: -4.932684
Q std: 5.466163
Actor loss: 4.936663
Action reg: 0.003979
  l1.weight: grad_norm = 0.004584
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.004175
Total gradient norm: 0.026570
=== Actor Training Debug (Iteration 923) ===
Q mean: -5.755874
Q std: 5.977711
Actor loss: 5.759851
Action reg: 0.003977
  l1.weight: grad_norm = 0.006384
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005018
Total gradient norm: 0.021643
=== Actor Training Debug (Iteration 924) ===
Q mean: -5.458528
Q std: 5.972296
Actor loss: 5.462512
Action reg: 0.003985
  l1.weight: grad_norm = 0.008115
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.005916
Total gradient norm: 0.026125
=== Actor Training Debug (Iteration 925) ===
Q mean: -4.447030
Q std: 4.900558
Actor loss: 4.451001
Action reg: 0.003972
  l1.weight: grad_norm = 0.008810
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.007672
Total gradient norm: 0.034933
=== Actor Training Debug (Iteration 926) ===
Q mean: -4.653303
Q std: 5.621902
Actor loss: 4.657277
Action reg: 0.003974
  l1.weight: grad_norm = 0.007599
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.007311
Total gradient norm: 0.031557
=== Actor Training Debug (Iteration 927) ===
Q mean: -5.469457
Q std: 6.031161
Actor loss: 5.473406
Action reg: 0.003950
  l1.weight: grad_norm = 0.003768
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.003634
Total gradient norm: 0.021373
=== Actor Training Debug (Iteration 928) ===
Q mean: -4.690381
Q std: 5.313256
Actor loss: 4.694365
Action reg: 0.003984
  l1.weight: grad_norm = 0.006334
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.005013
Total gradient norm: 0.031064
=== Actor Training Debug (Iteration 929) ===
Q mean: -4.939070
Q std: 5.575214
Actor loss: 4.943040
Action reg: 0.003970
  l1.weight: grad_norm = 0.010619
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.009541
Total gradient norm: 0.046614
=== Actor Training Debug (Iteration 930) ===
Q mean: -4.911738
Q std: 5.903795
Actor loss: 4.915723
Action reg: 0.003985
  l1.weight: grad_norm = 0.005216
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.004728
Total gradient norm: 0.028620
=== Actor Training Debug (Iteration 931) ===
Q mean: -4.733243
Q std: 5.507196
Actor loss: 4.737218
Action reg: 0.003974
  l1.weight: grad_norm = 0.010587
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.009845
Total gradient norm: 0.050598
=== Actor Training Debug (Iteration 932) ===
Q mean: -4.214977
Q std: 4.909227
Actor loss: 4.218945
Action reg: 0.003967
  l1.weight: grad_norm = 0.005997
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.005292
Total gradient norm: 0.026147
=== Actor Training Debug (Iteration 933) ===
Q mean: -5.804897
Q std: 6.086777
Actor loss: 5.808874
Action reg: 0.003976
  l1.weight: grad_norm = 0.005873
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.005564
Total gradient norm: 0.028241
=== Actor Training Debug (Iteration 934) ===
Q mean: -4.724326
Q std: 5.500898
Actor loss: 4.728304
Action reg: 0.003978
  l1.weight: grad_norm = 0.003686
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.003482
Total gradient norm: 0.019325
=== Actor Training Debug (Iteration 935) ===
Q mean: -5.676064
Q std: 5.997885
Actor loss: 5.680052
Action reg: 0.003988
  l1.weight: grad_norm = 0.003680
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002686
Total gradient norm: 0.014353
=== Actor Training Debug (Iteration 936) ===
Q mean: -4.389066
Q std: 5.305353
Actor loss: 4.393047
Action reg: 0.003981
  l1.weight: grad_norm = 0.001175
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.000858
Total gradient norm: 0.005604
=== Actor Training Debug (Iteration 937) ===
Q mean: -5.701590
Q std: 6.082161
Actor loss: 5.705575
Action reg: 0.003985
  l1.weight: grad_norm = 0.005884
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005062
Total gradient norm: 0.026558
=== Actor Training Debug (Iteration 938) ===
Q mean: -5.891016
Q std: 6.001141
Actor loss: 5.894991
Action reg: 0.003975
  l1.weight: grad_norm = 0.004852
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.003775
Total gradient norm: 0.018106
=== Actor Training Debug (Iteration 939) ===
Q mean: -4.616560
Q std: 5.311004
Actor loss: 4.620533
Action reg: 0.003973
  l1.weight: grad_norm = 0.006611
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.005234
Total gradient norm: 0.027459
=== Actor Training Debug (Iteration 940) ===
Q mean: -5.103264
Q std: 5.526016
Actor loss: 5.107248
Action reg: 0.003984
  l1.weight: grad_norm = 0.007229
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.006873
Total gradient norm: 0.046789
=== Actor Training Debug (Iteration 941) ===
Q mean: -5.369258
Q std: 5.581099
Actor loss: 5.373229
Action reg: 0.003971
  l1.weight: grad_norm = 0.006358
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.005056
Total gradient norm: 0.026109
=== Actor Training Debug (Iteration 942) ===
Q mean: -5.623965
Q std: 6.026363
Actor loss: 5.627946
Action reg: 0.003981
  l1.weight: grad_norm = 0.014985
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.011284
Total gradient norm: 0.049286
=== Actor Training Debug (Iteration 943) ===
Q mean: -5.228831
Q std: 5.734446
Actor loss: 5.232796
Action reg: 0.003965
  l1.weight: grad_norm = 0.011485
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.009194
Total gradient norm: 0.057947
=== Actor Training Debug (Iteration 944) ===
Q mean: -5.555896
Q std: 6.091637
Actor loss: 5.559890
Action reg: 0.003993
  l1.weight: grad_norm = 0.011946
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.010149
Total gradient norm: 0.047072
=== Actor Training Debug (Iteration 945) ===
Q mean: -5.150192
Q std: 5.143586
Actor loss: 5.154163
Action reg: 0.003972
  l1.weight: grad_norm = 0.006867
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.006368
Total gradient norm: 0.037110
=== Actor Training Debug (Iteration 946) ===
Q mean: -5.294985
Q std: 5.687231
Actor loss: 5.298967
Action reg: 0.003982
  l1.weight: grad_norm = 0.006799
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005973
Total gradient norm: 0.031066
=== Actor Training Debug (Iteration 947) ===
Q mean: -5.003119
Q std: 5.590183
Actor loss: 5.007098
Action reg: 0.003979
  l1.weight: grad_norm = 0.001859
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.001387
Total gradient norm: 0.006982
=== Actor Training Debug (Iteration 948) ===
Q mean: -5.094089
Q std: 5.809017
Actor loss: 5.098064
Action reg: 0.003976
  l1.weight: grad_norm = 0.009021
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.008068
Total gradient norm: 0.053290
=== Actor Training Debug (Iteration 949) ===
Q mean: -4.546459
Q std: 5.278325
Actor loss: 4.550439
Action reg: 0.003980
  l1.weight: grad_norm = 0.008842
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.006981
Total gradient norm: 0.037271
=== Actor Training Debug (Iteration 950) ===
Q mean: -4.803481
Q std: 5.283092
Actor loss: 4.807436
Action reg: 0.003955
  l1.weight: grad_norm = 0.011014
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.009910
Total gradient norm: 0.067731
=== Actor Training Debug (Iteration 951) ===
Q mean: -4.762061
Q std: 5.268236
Actor loss: 4.766020
Action reg: 0.003959
  l1.weight: grad_norm = 0.006410
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.005354
Total gradient norm: 0.027022
=== Actor Training Debug (Iteration 952) ===
Q mean: -5.225438
Q std: 6.179080
Actor loss: 5.229401
Action reg: 0.003963
  l1.weight: grad_norm = 0.007534
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.006136
Total gradient norm: 0.025516
=== Actor Training Debug (Iteration 953) ===
Q mean: -5.272442
Q std: 5.826663
Actor loss: 5.276418
Action reg: 0.003976
  l1.weight: grad_norm = 0.008289
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.007756
Total gradient norm: 0.042629
=== Actor Training Debug (Iteration 954) ===
Q mean: -5.931852
Q std: 6.272825
Actor loss: 5.935822
Action reg: 0.003970
  l1.weight: grad_norm = 0.008569
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.007697
Total gradient norm: 0.045011
=== Actor Training Debug (Iteration 955) ===
Q mean: -5.234305
Q std: 6.020494
Actor loss: 5.238290
Action reg: 0.003985
  l1.weight: grad_norm = 0.007673
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006946
Total gradient norm: 0.036904
=== Actor Training Debug (Iteration 956) ===
Q mean: -5.554062
Q std: 6.165955
Actor loss: 5.558019
Action reg: 0.003957
  l1.weight: grad_norm = 0.004730
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.004099
Total gradient norm: 0.019734
=== Actor Training Debug (Iteration 957) ===
Q mean: -5.462163
Q std: 5.929235
Actor loss: 5.466139
Action reg: 0.003976
  l1.weight: grad_norm = 0.002971
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.002409
Total gradient norm: 0.011888
=== Actor Training Debug (Iteration 958) ===
Q mean: -4.759773
Q std: 5.040206
Actor loss: 4.763755
Action reg: 0.003982
  l1.weight: grad_norm = 0.001888
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.001548
Total gradient norm: 0.007571
=== Actor Training Debug (Iteration 959) ===
Q mean: -5.026841
Q std: 5.759387
Actor loss: 5.030815
Action reg: 0.003973
  l1.weight: grad_norm = 0.008458
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.007200
Total gradient norm: 0.034381
=== Actor Training Debug (Iteration 960) ===
Q mean: -4.836315
Q std: 5.590877
Actor loss: 4.840302
Action reg: 0.003987
  l1.weight: grad_norm = 0.009232
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.007497
Total gradient norm: 0.048418
=== Actor Training Debug (Iteration 961) ===
Q mean: -5.103424
Q std: 6.151141
Actor loss: 5.107409
Action reg: 0.003985
  l1.weight: grad_norm = 0.014135
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.012349
Total gradient norm: 0.080120
=== Actor Training Debug (Iteration 962) ===
Q mean: -5.440267
Q std: 6.002486
Actor loss: 5.444253
Action reg: 0.003986
  l1.weight: grad_norm = 0.003857
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.003303
Total gradient norm: 0.015866
=== Actor Training Debug (Iteration 963) ===
Q mean: -5.126918
Q std: 5.389566
Actor loss: 5.130902
Action reg: 0.003984
  l1.weight: grad_norm = 0.005304
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003770
Total gradient norm: 0.018282
=== Actor Training Debug (Iteration 964) ===
Q mean: -5.805356
Q std: 5.715819
Actor loss: 5.809346
Action reg: 0.003990
  l1.weight: grad_norm = 0.007828
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005608
Total gradient norm: 0.025667
=== Actor Training Debug (Iteration 965) ===
Q mean: -5.937037
Q std: 6.312378
Actor loss: 5.941024
Action reg: 0.003988
  l1.weight: grad_norm = 0.004574
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.003820
Total gradient norm: 0.022656
=== Actor Training Debug (Iteration 966) ===
Q mean: -5.014566
Q std: 5.926829
Actor loss: 5.018548
Action reg: 0.003981
  l1.weight: grad_norm = 0.006218
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.005047
Total gradient norm: 0.028825
=== Actor Training Debug (Iteration 967) ===
Q mean: -5.529276
Q std: 6.224966
Actor loss: 5.533247
Action reg: 0.003971
  l1.weight: grad_norm = 0.007764
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.006562
Total gradient norm: 0.043318
=== Actor Training Debug (Iteration 968) ===
Q mean: -5.802800
Q std: 6.278441
Actor loss: 5.806767
Action reg: 0.003967
  l1.weight: grad_norm = 0.011287
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.010509
Total gradient norm: 0.048287
=== Actor Training Debug (Iteration 969) ===
Q mean: -4.915268
Q std: 5.574510
Actor loss: 4.919239
Action reg: 0.003971
  l1.weight: grad_norm = 0.013446
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.010426
Total gradient norm: 0.060005
=== Actor Training Debug (Iteration 970) ===
Q mean: -5.165504
Q std: 5.718339
Actor loss: 5.169497
Action reg: 0.003994
  l1.weight: grad_norm = 0.006977
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005971
Total gradient norm: 0.026665
=== Actor Training Debug (Iteration 971) ===
Q mean: -5.599481
Q std: 5.916091
Actor loss: 5.603440
Action reg: 0.003959
  l1.weight: grad_norm = 0.007429
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.005961
Total gradient norm: 0.029400
=== Actor Training Debug (Iteration 972) ===
Q mean: -5.594103
Q std: 6.274309
Actor loss: 5.598083
Action reg: 0.003979
  l1.weight: grad_norm = 0.015347
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.013547
Total gradient norm: 0.083212
=== Actor Training Debug (Iteration 973) ===
Q mean: -5.270774
Q std: 6.093587
Actor loss: 5.274742
Action reg: 0.003968
  l1.weight: grad_norm = 0.004661
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.003700
Total gradient norm: 0.017091
=== Actor Training Debug (Iteration 974) ===
Q mean: -4.692861
Q std: 5.619339
Actor loss: 4.696825
Action reg: 0.003964
  l1.weight: grad_norm = 0.007122
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.006024
Total gradient norm: 0.045122
=== Actor Training Debug (Iteration 975) ===
Q mean: -5.215960
Q std: 6.107913
Actor loss: 5.219933
Action reg: 0.003973
  l1.weight: grad_norm = 0.003361
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.003201
Total gradient norm: 0.024678
=== Actor Training Debug (Iteration 976) ===
Q mean: -5.633535
Q std: 6.421156
Actor loss: 5.637500
Action reg: 0.003965
  l1.weight: grad_norm = 0.008831
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.008105
Total gradient norm: 0.056340
=== Actor Training Debug (Iteration 977) ===
Q mean: -4.650352
Q std: 5.302005
Actor loss: 4.654325
Action reg: 0.003974
  l1.weight: grad_norm = 0.004033
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.003513
Total gradient norm: 0.021763
=== Actor Training Debug (Iteration 978) ===
Q mean: -5.331214
Q std: 5.503810
Actor loss: 5.335192
Action reg: 0.003978
  l1.weight: grad_norm = 0.006190
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.006201
Total gradient norm: 0.035280
=== Actor Training Debug (Iteration 979) ===
Q mean: -5.331487
Q std: 5.740494
Actor loss: 5.335452
Action reg: 0.003965
  l1.weight: grad_norm = 0.014775
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.012912
Total gradient norm: 0.071510
=== Actor Training Debug (Iteration 980) ===
Q mean: -5.492181
Q std: 5.860135
Actor loss: 5.496162
Action reg: 0.003980
  l1.weight: grad_norm = 0.004372
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.003526
Total gradient norm: 0.015728
=== Actor Training Debug (Iteration 981) ===
Q mean: -5.134580
Q std: 6.063370
Actor loss: 5.138540
Action reg: 0.003960
  l1.weight: grad_norm = 0.017467
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.013278
Total gradient norm: 0.083477
=== Actor Training Debug (Iteration 982) ===
Q mean: -5.309267
Q std: 5.726408
Actor loss: 5.313251
Action reg: 0.003984
  l1.weight: grad_norm = 0.003947
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002969
Total gradient norm: 0.012610
=== Actor Training Debug (Iteration 983) ===
Q mean: -5.498422
Q std: 6.033416
Actor loss: 5.502386
Action reg: 0.003965
  l1.weight: grad_norm = 0.006439
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.005667
Total gradient norm: 0.039124
=== Actor Training Debug (Iteration 984) ===
Q mean: -5.514979
Q std: 5.653697
Actor loss: 5.518965
Action reg: 0.003986
  l1.weight: grad_norm = 0.002223
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.002044
Total gradient norm: 0.012694
=== Actor Training Debug (Iteration 985) ===
Q mean: -4.781515
Q std: 5.687552
Actor loss: 4.785468
Action reg: 0.003953
  l1.weight: grad_norm = 0.002933
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.002294
Total gradient norm: 0.011691
=== Actor Training Debug (Iteration 986) ===
Q mean: -5.622902
Q std: 6.275814
Actor loss: 5.626890
Action reg: 0.003988
  l1.weight: grad_norm = 0.006507
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005397
Total gradient norm: 0.025092
=== Actor Training Debug (Iteration 987) ===
Q mean: -5.492381
Q std: 6.438863
Actor loss: 5.496374
Action reg: 0.003994
  l1.weight: grad_norm = 0.006428
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005401
Total gradient norm: 0.030676
=== Actor Training Debug (Iteration 988) ===
Q mean: -5.102209
Q std: 5.903044
Actor loss: 5.106195
Action reg: 0.003986
  l1.weight: grad_norm = 0.001571
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.001342
Total gradient norm: 0.006094
=== Actor Training Debug (Iteration 989) ===
Q mean: -5.311813
Q std: 5.881386
Actor loss: 5.315798
Action reg: 0.003984
  l1.weight: grad_norm = 0.007946
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.006419
Total gradient norm: 0.043558
=== Actor Training Debug (Iteration 990) ===
Q mean: -5.283397
Q std: 5.889182
Actor loss: 5.287369
Action reg: 0.003972
  l1.weight: grad_norm = 0.014222
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.011415
Total gradient norm: 0.045985
=== Actor Training Debug (Iteration 991) ===
Q mean: -4.728679
Q std: 5.684237
Actor loss: 4.732643
Action reg: 0.003964
  l1.weight: grad_norm = 0.002532
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.002024
Total gradient norm: 0.008422
=== Actor Training Debug (Iteration 992) ===
Q mean: -5.989861
Q std: 6.592625
Actor loss: 5.993839
Action reg: 0.003977
  l1.weight: grad_norm = 0.003439
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.002604
Total gradient norm: 0.011471
=== Actor Training Debug (Iteration 993) ===
Q mean: -5.288790
Q std: 5.936035
Actor loss: 5.292767
Action reg: 0.003977
  l1.weight: grad_norm = 0.007086
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.006202
Total gradient norm: 0.025393
=== Actor Training Debug (Iteration 994) ===
Q mean: -5.769884
Q std: 6.199792
Actor loss: 5.773867
Action reg: 0.003983
  l1.weight: grad_norm = 0.005229
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004317
Total gradient norm: 0.019739
=== Actor Training Debug (Iteration 995) ===
Q mean: -5.452778
Q std: 6.101281
Actor loss: 5.456766
Action reg: 0.003988
  l1.weight: grad_norm = 0.002461
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.002125
Total gradient norm: 0.014254
=== Actor Training Debug (Iteration 996) ===
Q mean: -5.197382
Q std: 6.047938
Actor loss: 5.201348
Action reg: 0.003966
  l1.weight: grad_norm = 0.007498
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.006669
Total gradient norm: 0.027544
=== Actor Training Debug (Iteration 997) ===
Q mean: -4.790040
Q std: 5.570691
Actor loss: 4.794021
Action reg: 0.003981
  l1.weight: grad_norm = 0.001797
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.001497
Total gradient norm: 0.006768
=== Actor Training Debug (Iteration 998) ===
Q mean: -5.288137
Q std: 6.057500
Actor loss: 5.292120
Action reg: 0.003983
  l1.weight: grad_norm = 0.006942
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.007145
Total gradient norm: 0.047008
=== Actor Training Debug (Iteration 999) ===
Q mean: -5.643142
Q std: 6.329535
Actor loss: 5.647127
Action reg: 0.003986
  l1.weight: grad_norm = 0.003968
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.003573
Total gradient norm: 0.013897
=== Actor Training Debug (Iteration 1000) ===
Q mean: -5.387955
Q std: 6.291292
Actor loss: 5.391941
Action reg: 0.003986
  l1.weight: grad_norm = 0.001283
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.001115
Total gradient norm: 0.007967
Step 6000: Critic Loss: 0.7727, Actor Loss: 5.3919, Q Value: -5.3880
  Average reward: -361.945 | Average length: 100.0
Evaluation at episode 60: -361.945
=== Actor Training Debug (Iteration 1001) ===
Q mean: -5.228143
Q std: 5.715537
Actor loss: 5.232137
Action reg: 0.003994
  l1.weight: grad_norm = 0.001771
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.001332
Total gradient norm: 0.007171
=== Actor Training Debug (Iteration 1002) ===
Q mean: -5.213095
Q std: 5.590762
Actor loss: 5.217063
Action reg: 0.003968
  l1.weight: grad_norm = 0.008592
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.006847
Total gradient norm: 0.038729
=== Actor Training Debug (Iteration 1003) ===
Q mean: -5.663956
Q std: 6.367122
Actor loss: 5.667932
Action reg: 0.003976
  l1.weight: grad_norm = 0.026109
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.022939
Total gradient norm: 0.121292
=== Actor Training Debug (Iteration 1004) ===
Q mean: -5.307500
Q std: 6.195815
Actor loss: 5.311479
Action reg: 0.003979
  l1.weight: grad_norm = 0.003952
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002873
Total gradient norm: 0.012009
=== Actor Training Debug (Iteration 1005) ===
Q mean: -5.332770
Q std: 6.225440
Actor loss: 5.336754
Action reg: 0.003984
  l1.weight: grad_norm = 0.004720
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.004192
Total gradient norm: 0.019362
=== Actor Training Debug (Iteration 1006) ===
Q mean: -5.793225
Q std: 6.059567
Actor loss: 5.797195
Action reg: 0.003970
  l1.weight: grad_norm = 0.009187
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.007526
Total gradient norm: 0.038854
=== Actor Training Debug (Iteration 1007) ===
Q mean: -5.935590
Q std: 6.203328
Actor loss: 5.939576
Action reg: 0.003986
  l1.weight: grad_norm = 0.006364
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004970
Total gradient norm: 0.028121
=== Actor Training Debug (Iteration 1008) ===
Q mean: -5.869897
Q std: 6.504477
Actor loss: 5.873874
Action reg: 0.003977
  l1.weight: grad_norm = 0.016586
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.014438
Total gradient norm: 0.064439
=== Actor Training Debug (Iteration 1009) ===
Q mean: -4.143392
Q std: 5.542211
Actor loss: 4.147373
Action reg: 0.003981
  l1.weight: grad_norm = 0.002749
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.002382
Total gradient norm: 0.012761
=== Actor Training Debug (Iteration 1010) ===
Q mean: -5.107980
Q std: 6.365467
Actor loss: 5.111949
Action reg: 0.003969
  l1.weight: grad_norm = 0.009637
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.008201
Total gradient norm: 0.050781
=== Actor Training Debug (Iteration 1011) ===
Q mean: -5.311337
Q std: 6.449095
Actor loss: 5.315321
Action reg: 0.003984
  l1.weight: grad_norm = 0.007542
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.006648
Total gradient norm: 0.031447
=== Actor Training Debug (Iteration 1012) ===
Q mean: -5.768593
Q std: 6.597483
Actor loss: 5.772574
Action reg: 0.003982
  l1.weight: grad_norm = 0.009300
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.007930
Total gradient norm: 0.034584
=== Actor Training Debug (Iteration 1013) ===
Q mean: -4.892950
Q std: 5.624567
Actor loss: 4.896937
Action reg: 0.003987
  l1.weight: grad_norm = 0.008202
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.006742
Total gradient norm: 0.039580
=== Actor Training Debug (Iteration 1014) ===
Q mean: -5.775171
Q std: 6.613586
Actor loss: 5.779133
Action reg: 0.003961
  l1.weight: grad_norm = 0.014424
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.013582
Total gradient norm: 0.096974
=== Actor Training Debug (Iteration 1015) ===
Q mean: -5.463245
Q std: 6.572709
Actor loss: 5.467223
Action reg: 0.003978
  l1.weight: grad_norm = 0.005369
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.004139
Total gradient norm: 0.020709
=== Actor Training Debug (Iteration 1016) ===
Q mean: -5.223965
Q std: 6.385757
Actor loss: 5.227942
Action reg: 0.003978
  l1.weight: grad_norm = 0.008753
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.006855
Total gradient norm: 0.038810
=== Actor Training Debug (Iteration 1017) ===
Q mean: -5.735860
Q std: 6.334920
Actor loss: 5.739853
Action reg: 0.003993
  l1.weight: grad_norm = 0.008093
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.007068
Total gradient norm: 0.034440
=== Actor Training Debug (Iteration 1018) ===
Q mean: -5.697031
Q std: 6.098405
Actor loss: 5.700997
Action reg: 0.003966
  l1.weight: grad_norm = 0.012496
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.009489
Total gradient norm: 0.042019
=== Actor Training Debug (Iteration 1019) ===
Q mean: -5.652993
Q std: 6.198891
Actor loss: 5.656981
Action reg: 0.003989
  l1.weight: grad_norm = 0.002197
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.001857
Total gradient norm: 0.009546
=== Actor Training Debug (Iteration 1020) ===
Q mean: -5.155173
Q std: 6.113217
Actor loss: 5.159158
Action reg: 0.003984
  l1.weight: grad_norm = 0.002915
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002701
Total gradient norm: 0.011592
=== Actor Training Debug (Iteration 1021) ===
Q mean: -5.701916
Q std: 6.504766
Actor loss: 5.705907
Action reg: 0.003991
  l1.weight: grad_norm = 0.002941
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.002222
Total gradient norm: 0.012754
=== Actor Training Debug (Iteration 1022) ===
Q mean: -6.055578
Q std: 6.518988
Actor loss: 6.059541
Action reg: 0.003963
  l1.weight: grad_norm = 0.005310
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.004130
Total gradient norm: 0.021911
=== Actor Training Debug (Iteration 1023) ===
Q mean: -5.870853
Q std: 6.538691
Actor loss: 5.874843
Action reg: 0.003989
  l1.weight: grad_norm = 0.004741
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003561
Total gradient norm: 0.020563
=== Actor Training Debug (Iteration 1024) ===
Q mean: -5.390521
Q std: 6.312389
Actor loss: 5.394485
Action reg: 0.003964
  l1.weight: grad_norm = 0.005441
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.004533
Total gradient norm: 0.018562
=== Actor Training Debug (Iteration 1025) ===
Q mean: -4.594300
Q std: 5.910978
Actor loss: 4.598271
Action reg: 0.003971
  l1.weight: grad_norm = 0.008254
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.007327
Total gradient norm: 0.032996
=== Actor Training Debug (Iteration 1026) ===
Q mean: -5.643271
Q std: 6.244480
Actor loss: 5.647240
Action reg: 0.003968
  l1.weight: grad_norm = 0.009650
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.008431
Total gradient norm: 0.042806
=== Actor Training Debug (Iteration 1027) ===
Q mean: -6.079366
Q std: 6.175594
Actor loss: 6.083347
Action reg: 0.003981
  l1.weight: grad_norm = 0.006283
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005163
Total gradient norm: 0.022998
=== Actor Training Debug (Iteration 1028) ===
Q mean: -6.123906
Q std: 6.517873
Actor loss: 6.127881
Action reg: 0.003975
  l1.weight: grad_norm = 0.006344
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004706
Total gradient norm: 0.026870
=== Actor Training Debug (Iteration 1029) ===
Q mean: -5.410231
Q std: 6.334442
Actor loss: 5.414207
Action reg: 0.003976
  l1.weight: grad_norm = 0.005696
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.005399
Total gradient norm: 0.024990
=== Actor Training Debug (Iteration 1030) ===
Q mean: -4.992917
Q std: 6.016685
Actor loss: 4.996908
Action reg: 0.003991
  l1.weight: grad_norm = 0.006727
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.005656
Total gradient norm: 0.032114
=== Actor Training Debug (Iteration 1031) ===
Q mean: -5.324854
Q std: 6.595446
Actor loss: 5.328826
Action reg: 0.003973
  l1.weight: grad_norm = 0.006453
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.004326
Total gradient norm: 0.018719
=== Actor Training Debug (Iteration 1032) ===
Q mean: -4.876434
Q std: 5.943173
Actor loss: 4.880427
Action reg: 0.003993
  l1.weight: grad_norm = 0.002104
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001743
Total gradient norm: 0.010266
=== Actor Training Debug (Iteration 1033) ===
Q mean: -5.447195
Q std: 6.290336
Actor loss: 5.451184
Action reg: 0.003990
  l1.weight: grad_norm = 0.003413
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.002972
Total gradient norm: 0.015906
=== Actor Training Debug (Iteration 1034) ===
Q mean: -6.051083
Q std: 6.602612
Actor loss: 6.055063
Action reg: 0.003980
  l1.weight: grad_norm = 0.007695
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.007144
Total gradient norm: 0.035416
=== Actor Training Debug (Iteration 1035) ===
Q mean: -5.228207
Q std: 6.280603
Actor loss: 5.232193
Action reg: 0.003987
  l1.weight: grad_norm = 0.013379
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.010350
Total gradient norm: 0.065919
=== Actor Training Debug (Iteration 1036) ===
Q mean: -4.710247
Q std: 6.292782
Actor loss: 4.714226
Action reg: 0.003979
  l1.weight: grad_norm = 0.006286
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.005543
Total gradient norm: 0.028759
=== Actor Training Debug (Iteration 1037) ===
Q mean: -5.181189
Q std: 6.118005
Actor loss: 5.185168
Action reg: 0.003979
  l1.weight: grad_norm = 0.005100
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004364
Total gradient norm: 0.017633
=== Actor Training Debug (Iteration 1038) ===
Q mean: -5.543497
Q std: 6.414518
Actor loss: 5.547470
Action reg: 0.003973
  l1.weight: grad_norm = 0.013061
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.013124
Total gradient norm: 0.088491
=== Actor Training Debug (Iteration 1039) ===
Q mean: -5.673153
Q std: 5.834456
Actor loss: 5.677116
Action reg: 0.003963
  l1.weight: grad_norm = 0.025486
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.032474
Total gradient norm: 0.223956
=== Actor Training Debug (Iteration 1040) ===
Q mean: -6.236278
Q std: 6.656083
Actor loss: 6.240265
Action reg: 0.003987
  l1.weight: grad_norm = 0.012688
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.016101
Total gradient norm: 0.147889
=== Actor Training Debug (Iteration 1041) ===
Q mean: -5.234802
Q std: 6.290969
Actor loss: 5.238753
Action reg: 0.003952
  l1.weight: grad_norm = 0.029027
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.051628
Total gradient norm: 0.584044
=== Actor Training Debug (Iteration 1042) ===
Q mean: -4.827019
Q std: 6.182188
Actor loss: 4.830989
Action reg: 0.003971
  l1.weight: grad_norm = 0.030556
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.050218
Total gradient norm: 0.401891
=== Actor Training Debug (Iteration 1043) ===
Q mean: -4.837871
Q std: 6.041349
Actor loss: 4.841839
Action reg: 0.003968
  l1.weight: grad_norm = 0.012336
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.011287
Total gradient norm: 0.063581
=== Actor Training Debug (Iteration 1044) ===
Q mean: -4.581238
Q std: 5.428198
Actor loss: 4.585191
Action reg: 0.003953
  l1.weight: grad_norm = 0.012304
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.012272
Total gradient norm: 0.039894
=== Actor Training Debug (Iteration 1045) ===
Q mean: -5.708388
Q std: 5.934663
Actor loss: 5.712373
Action reg: 0.003985
  l1.weight: grad_norm = 0.002469
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.002637
Total gradient norm: 0.014740
=== Actor Training Debug (Iteration 1046) ===
Q mean: -6.464829
Q std: 6.667060
Actor loss: 6.468808
Action reg: 0.003978
  l1.weight: grad_norm = 0.003113
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.002614
Total gradient norm: 0.013655
=== Actor Training Debug (Iteration 1047) ===
Q mean: -5.918092
Q std: 6.831172
Actor loss: 5.922073
Action reg: 0.003982
  l1.weight: grad_norm = 0.011957
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.012068
Total gradient norm: 0.044661
=== Actor Training Debug (Iteration 1048) ===
Q mean: -5.303208
Q std: 6.210866
Actor loss: 5.307199
Action reg: 0.003990
  l1.weight: grad_norm = 0.003560
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.003160
Total gradient norm: 0.017521
=== Actor Training Debug (Iteration 1049) ===
Q mean: -5.639578
Q std: 6.532037
Actor loss: 5.643537
Action reg: 0.003958
  l1.weight: grad_norm = 0.006924
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.006953
Total gradient norm: 0.033639
=== Actor Training Debug (Iteration 1050) ===
Q mean: -5.635934
Q std: 6.376068
Actor loss: 5.639919
Action reg: 0.003985
  l1.weight: grad_norm = 0.005296
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.004263
Total gradient norm: 0.019492
=== Actor Training Debug (Iteration 1051) ===
Q mean: -6.012214
Q std: 6.590761
Actor loss: 6.016203
Action reg: 0.003990
  l1.weight: grad_norm = 0.016975
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.020806
Total gradient norm: 0.064469
=== Actor Training Debug (Iteration 1052) ===
Q mean: -4.839987
Q std: 6.101434
Actor loss: 4.843957
Action reg: 0.003970
  l1.weight: grad_norm = 0.007812
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.006240
Total gradient norm: 0.032156
=== Actor Training Debug (Iteration 1053) ===
Q mean: -5.414564
Q std: 6.368039
Actor loss: 5.418554
Action reg: 0.003990
  l1.weight: grad_norm = 0.003204
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.002798
Total gradient norm: 0.017883
=== Actor Training Debug (Iteration 1054) ===
Q mean: -5.244009
Q std: 6.461048
Actor loss: 5.247993
Action reg: 0.003984
  l1.weight: grad_norm = 0.004915
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.003579
Total gradient norm: 0.018760
=== Actor Training Debug (Iteration 1055) ===
Q mean: -5.728919
Q std: 6.434491
Actor loss: 5.732902
Action reg: 0.003983
  l1.weight: grad_norm = 0.004714
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.004136
Total gradient norm: 0.020843
=== Actor Training Debug (Iteration 1056) ===
Q mean: -5.517519
Q std: 6.678337
Actor loss: 5.521499
Action reg: 0.003980
  l1.weight: grad_norm = 0.008545
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.009231
Total gradient norm: 0.030429
=== Actor Training Debug (Iteration 1057) ===
Q mean: -5.342066
Q std: 6.069324
Actor loss: 5.346040
Action reg: 0.003974
  l1.weight: grad_norm = 0.006595
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.005144
Total gradient norm: 0.022566
=== Actor Training Debug (Iteration 1058) ===
Q mean: -5.022104
Q std: 5.703257
Actor loss: 5.026089
Action reg: 0.003985
  l1.weight: grad_norm = 0.014899
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.013668
Total gradient norm: 0.078059
=== Actor Training Debug (Iteration 1059) ===
Q mean: -5.422625
Q std: 6.195597
Actor loss: 5.426594
Action reg: 0.003970
  l1.weight: grad_norm = 0.004441
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.003993
Total gradient norm: 0.020947
=== Actor Training Debug (Iteration 1060) ===
Q mean: -5.340039
Q std: 6.215542
Actor loss: 5.344010
Action reg: 0.003971
  l1.weight: grad_norm = 0.005737
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.004120
Total gradient norm: 0.020402
=== Actor Training Debug (Iteration 1061) ===
Q mean: -5.760859
Q std: 6.483531
Actor loss: 5.764843
Action reg: 0.003983
  l1.weight: grad_norm = 0.006057
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004799
Total gradient norm: 0.022460
=== Actor Training Debug (Iteration 1062) ===
Q mean: -5.446027
Q std: 6.671829
Actor loss: 5.450010
Action reg: 0.003983
  l1.weight: grad_norm = 0.006676
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.005351
Total gradient norm: 0.018612
=== Actor Training Debug (Iteration 1063) ===
Q mean: -5.637746
Q std: 6.718486
Actor loss: 5.641718
Action reg: 0.003972
  l1.weight: grad_norm = 0.004422
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.002936
Total gradient norm: 0.012142
=== Actor Training Debug (Iteration 1064) ===
Q mean: -5.275914
Q std: 6.190273
Actor loss: 5.279902
Action reg: 0.003988
  l1.weight: grad_norm = 0.005716
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.004854
Total gradient norm: 0.028975
=== Actor Training Debug (Iteration 1065) ===
Q mean: -5.677621
Q std: 6.096677
Actor loss: 5.681608
Action reg: 0.003987
  l1.weight: grad_norm = 0.005122
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.004729
Total gradient norm: 0.014948
=== Actor Training Debug (Iteration 1066) ===
Q mean: -5.421790
Q std: 5.866488
Actor loss: 5.425766
Action reg: 0.003976
  l1.weight: grad_norm = 0.009365
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.007820
Total gradient norm: 0.034311
=== Actor Training Debug (Iteration 1067) ===
Q mean: -5.642210
Q std: 6.238691
Actor loss: 5.646187
Action reg: 0.003977
  l1.weight: grad_norm = 0.009286
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.008386
Total gradient norm: 0.045864
=== Actor Training Debug (Iteration 1068) ===
Q mean: -5.599264
Q std: 6.551123
Actor loss: 5.603249
Action reg: 0.003984
  l1.weight: grad_norm = 0.008285
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.009664
Total gradient norm: 0.028759
=== Actor Training Debug (Iteration 1069) ===
Q mean: -5.273480
Q std: 6.616140
Actor loss: 5.277452
Action reg: 0.003973
  l1.weight: grad_norm = 0.021233
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.016066
Total gradient norm: 0.062525
=== Actor Training Debug (Iteration 1070) ===
Q mean: -5.258611
Q std: 6.299716
Actor loss: 5.262597
Action reg: 0.003986
  l1.weight: grad_norm = 0.020010
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.019565
Total gradient norm: 0.075865
=== Actor Training Debug (Iteration 1071) ===
Q mean: -5.241789
Q std: 6.101162
Actor loss: 5.245776
Action reg: 0.003986
  l1.weight: grad_norm = 0.016737
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.016514
Total gradient norm: 0.106119
=== Actor Training Debug (Iteration 1072) ===
Q mean: -5.404811
Q std: 5.860064
Actor loss: 5.408792
Action reg: 0.003981
  l1.weight: grad_norm = 0.010150
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.011663
Total gradient norm: 0.038986
=== Actor Training Debug (Iteration 1073) ===
Q mean: -5.541780
Q std: 5.969724
Actor loss: 5.545758
Action reg: 0.003978
  l1.weight: grad_norm = 0.008334
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.008491
Total gradient norm: 0.030592
=== Actor Training Debug (Iteration 1074) ===
Q mean: -6.041073
Q std: 6.454897
Actor loss: 6.045055
Action reg: 0.003982
  l1.weight: grad_norm = 0.009370
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.010591
Total gradient norm: 0.032123
=== Actor Training Debug (Iteration 1075) ===
Q mean: -5.977485
Q std: 6.660914
Actor loss: 5.981468
Action reg: 0.003983
  l1.weight: grad_norm = 0.005047
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.004348
Total gradient norm: 0.015610
=== Actor Training Debug (Iteration 1076) ===
Q mean: -4.714063
Q std: 5.864770
Actor loss: 4.718047
Action reg: 0.003985
  l1.weight: grad_norm = 0.015041
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.015711
Total gradient norm: 0.062560
=== Actor Training Debug (Iteration 1077) ===
Q mean: -5.478343
Q std: 6.916100
Actor loss: 5.482311
Action reg: 0.003968
  l1.weight: grad_norm = 0.010249
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.008432
Total gradient norm: 0.031306
=== Actor Training Debug (Iteration 1078) ===
Q mean: -5.422286
Q std: 6.660352
Actor loss: 5.426262
Action reg: 0.003976
  l1.weight: grad_norm = 0.008656
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.007915
Total gradient norm: 0.027380
=== Actor Training Debug (Iteration 1079) ===
Q mean: -5.700734
Q std: 6.712445
Actor loss: 5.704714
Action reg: 0.003980
  l1.weight: grad_norm = 0.012465
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.013409
Total gradient norm: 0.046034
=== Actor Training Debug (Iteration 1080) ===
Q mean: -6.300618
Q std: 7.067767
Actor loss: 6.304604
Action reg: 0.003986
  l1.weight: grad_norm = 0.008059
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.007429
Total gradient norm: 0.031648
=== Actor Training Debug (Iteration 1081) ===
Q mean: -5.633093
Q std: 6.031870
Actor loss: 5.637076
Action reg: 0.003983
  l1.weight: grad_norm = 0.004069
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.003201
Total gradient norm: 0.011410
=== Actor Training Debug (Iteration 1082) ===
Q mean: -6.147813
Q std: 6.518273
Actor loss: 6.151796
Action reg: 0.003984
  l1.weight: grad_norm = 0.010247
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.008703
Total gradient norm: 0.045032
=== Actor Training Debug (Iteration 1083) ===
Q mean: -5.813718
Q std: 6.122793
Actor loss: 5.817701
Action reg: 0.003983
  l1.weight: grad_norm = 0.018275
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.014437
Total gradient norm: 0.044432
=== Actor Training Debug (Iteration 1084) ===
Q mean: -5.195640
Q std: 6.202305
Actor loss: 5.199617
Action reg: 0.003978
  l1.weight: grad_norm = 0.008553
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.009101
Total gradient norm: 0.030781
=== Actor Training Debug (Iteration 1085) ===
Q mean: -5.446255
Q std: 6.221846
Actor loss: 5.450239
Action reg: 0.003984
  l1.weight: grad_norm = 0.015444
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.015196
Total gradient norm: 0.047857
=== Actor Training Debug (Iteration 1086) ===
Q mean: -6.190986
Q std: 6.978511
Actor loss: 6.194952
Action reg: 0.003966
  l1.weight: grad_norm = 0.014429
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.013424
Total gradient norm: 0.062495
=== Actor Training Debug (Iteration 1087) ===
Q mean: -5.663779
Q std: 6.584022
Actor loss: 5.667764
Action reg: 0.003985
  l1.weight: grad_norm = 0.027758
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.029986
Total gradient norm: 0.084895
=== Actor Training Debug (Iteration 1088) ===
Q mean: -5.530403
Q std: 6.642153
Actor loss: 5.534385
Action reg: 0.003982
  l1.weight: grad_norm = 0.007960
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.007248
Total gradient norm: 0.031301
=== Actor Training Debug (Iteration 1089) ===
Q mean: -5.079785
Q std: 6.297973
Actor loss: 5.083757
Action reg: 0.003972
  l1.weight: grad_norm = 0.015443
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.015608
Total gradient norm: 0.052133
=== Actor Training Debug (Iteration 1090) ===
Q mean: -5.711035
Q std: 6.332534
Actor loss: 5.715000
Action reg: 0.003966
  l1.weight: grad_norm = 0.026218
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.028251
Total gradient norm: 0.094764
=== Actor Training Debug (Iteration 1091) ===
Q mean: -5.968591
Q std: 6.480396
Actor loss: 5.972565
Action reg: 0.003974
  l1.weight: grad_norm = 0.011771
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.010328
Total gradient norm: 0.044198
=== Actor Training Debug (Iteration 1092) ===
Q mean: -6.155140
Q std: 6.605740
Actor loss: 6.159118
Action reg: 0.003978
  l1.weight: grad_norm = 0.009259
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.008837
Total gradient norm: 0.033661
=== Actor Training Debug (Iteration 1093) ===
Q mean: -6.180624
Q std: 6.949082
Actor loss: 6.184607
Action reg: 0.003983
  l1.weight: grad_norm = 0.011597
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.011798
Total gradient norm: 0.039688
=== Actor Training Debug (Iteration 1094) ===
Q mean: -6.152616
Q std: 7.261975
Actor loss: 6.156605
Action reg: 0.003989
  l1.weight: grad_norm = 0.007752
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.006659
Total gradient norm: 0.026829
=== Actor Training Debug (Iteration 1095) ===
Q mean: -5.916010
Q std: 6.716892
Actor loss: 5.919996
Action reg: 0.003986
  l1.weight: grad_norm = 0.000903
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.000816
Total gradient norm: 0.002737
=== Actor Training Debug (Iteration 1096) ===
Q mean: -5.871490
Q std: 6.938682
Actor loss: 5.875464
Action reg: 0.003974
  l1.weight: grad_norm = 0.015917
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.017416
Total gradient norm: 0.053661
=== Actor Training Debug (Iteration 1097) ===
Q mean: -6.070886
Q std: 6.517035
Actor loss: 6.074862
Action reg: 0.003977
  l1.weight: grad_norm = 0.016293
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.013887
Total gradient norm: 0.077980
=== Actor Training Debug (Iteration 1098) ===
Q mean: -5.308537
Q std: 6.100464
Actor loss: 5.312528
Action reg: 0.003991
  l1.weight: grad_norm = 0.006625
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.005114
Total gradient norm: 0.018507
=== Actor Training Debug (Iteration 1099) ===
Q mean: -5.056248
Q std: 6.091285
Actor loss: 5.060228
Action reg: 0.003980
  l1.weight: grad_norm = 0.008279
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.007119
Total gradient norm: 0.038706
=== Actor Training Debug (Iteration 1100) ===
Q mean: -5.750558
Q std: 7.178298
Actor loss: 5.754537
Action reg: 0.003979
  l1.weight: grad_norm = 0.008763
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.008000
Total gradient norm: 0.030798
Episode 61: Steps=100, Reward=-306.833, Buffer_size=6100
=== Actor Training Debug (Iteration 1101) ===
Q mean: -5.768557
Q std: 6.936832
Actor loss: 5.772538
Action reg: 0.003981
  l1.weight: grad_norm = 0.009549
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.008034
Total gradient norm: 0.044854
=== Actor Training Debug (Iteration 1102) ===
Q mean: -5.701473
Q std: 6.788920
Actor loss: 5.705451
Action reg: 0.003979
  l1.weight: grad_norm = 0.006000
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.005406
Total gradient norm: 0.019919
=== Actor Training Debug (Iteration 1103) ===
Q mean: -6.523942
Q std: 6.857228
Actor loss: 6.527920
Action reg: 0.003978
  l1.weight: grad_norm = 0.012886
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.012396
Total gradient norm: 0.048724
=== Actor Training Debug (Iteration 1104) ===
Q mean: -5.388107
Q std: 6.586576
Actor loss: 5.392086
Action reg: 0.003978
  l1.weight: grad_norm = 0.016467
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.015988
Total gradient norm: 0.103656
=== Actor Training Debug (Iteration 1105) ===
Q mean: -5.524901
Q std: 6.747363
Actor loss: 5.528887
Action reg: 0.003986
  l1.weight: grad_norm = 0.005891
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004707
Total gradient norm: 0.020394
=== Actor Training Debug (Iteration 1106) ===
Q mean: -6.351929
Q std: 7.054583
Actor loss: 6.355908
Action reg: 0.003979
  l1.weight: grad_norm = 0.017028
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.016077
Total gradient norm: 0.068673
=== Actor Training Debug (Iteration 1107) ===
Q mean: -5.622928
Q std: 6.331821
Actor loss: 5.626912
Action reg: 0.003984
  l1.weight: grad_norm = 0.014584
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.012397
Total gradient norm: 0.073710
=== Actor Training Debug (Iteration 1108) ===
Q mean: -6.209894
Q std: 6.843693
Actor loss: 6.213876
Action reg: 0.003982
  l1.weight: grad_norm = 0.007502
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.006773
Total gradient norm: 0.029842
=== Actor Training Debug (Iteration 1109) ===
Q mean: -6.260057
Q std: 7.183260
Actor loss: 6.264036
Action reg: 0.003979
  l1.weight: grad_norm = 0.035124
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.036197
Total gradient norm: 0.122777
=== Actor Training Debug (Iteration 1110) ===
Q mean: -5.401150
Q std: 6.225944
Actor loss: 5.405129
Action reg: 0.003979
  l1.weight: grad_norm = 0.009690
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.009983
Total gradient norm: 0.040258
=== Actor Training Debug (Iteration 1111) ===
Q mean: -5.610765
Q std: 6.368331
Actor loss: 5.614734
Action reg: 0.003969
  l1.weight: grad_norm = 0.010697
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.009924
Total gradient norm: 0.048791
=== Actor Training Debug (Iteration 1112) ===
Q mean: -6.080170
Q std: 7.108457
Actor loss: 6.084160
Action reg: 0.003990
  l1.weight: grad_norm = 0.008619
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.008237
Total gradient norm: 0.027074
=== Actor Training Debug (Iteration 1113) ===
Q mean: -5.989457
Q std: 6.714786
Actor loss: 5.993437
Action reg: 0.003979
  l1.weight: grad_norm = 0.006011
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.005053
Total gradient norm: 0.019083
=== Actor Training Debug (Iteration 1114) ===
Q mean: -5.568805
Q std: 6.668015
Actor loss: 5.572767
Action reg: 0.003962
  l1.weight: grad_norm = 0.012221
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.011517
Total gradient norm: 0.041346
=== Actor Training Debug (Iteration 1115) ===
Q mean: -6.185684
Q std: 7.032764
Actor loss: 6.189668
Action reg: 0.003984
  l1.weight: grad_norm = 0.011561
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.010339
Total gradient norm: 0.052952
=== Actor Training Debug (Iteration 1116) ===
Q mean: -5.242120
Q std: 6.246684
Actor loss: 5.246103
Action reg: 0.003983
  l1.weight: grad_norm = 0.002227
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.001904
Total gradient norm: 0.011907
=== Actor Training Debug (Iteration 1117) ===
Q mean: -5.193122
Q std: 6.225570
Actor loss: 5.197096
Action reg: 0.003974
  l1.weight: grad_norm = 0.013131
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.010343
Total gradient norm: 0.049462
=== Actor Training Debug (Iteration 1118) ===
Q mean: -6.561275
Q std: 7.007087
Actor loss: 6.565258
Action reg: 0.003983
  l1.weight: grad_norm = 0.017673
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.014300
Total gradient norm: 0.072171
=== Actor Training Debug (Iteration 1119) ===
Q mean: -6.743307
Q std: 7.391181
Actor loss: 6.747279
Action reg: 0.003973
  l1.weight: grad_norm = 0.009749
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.008402
Total gradient norm: 0.046207
=== Actor Training Debug (Iteration 1120) ===
Q mean: -6.353850
Q std: 6.738053
Actor loss: 6.357836
Action reg: 0.003986
  l1.weight: grad_norm = 0.015327
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.014713
Total gradient norm: 0.046726
=== Actor Training Debug (Iteration 1121) ===
Q mean: -5.263707
Q std: 6.443488
Actor loss: 5.267688
Action reg: 0.003981
  l1.weight: grad_norm = 0.008731
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.006541
Total gradient norm: 0.032946
=== Actor Training Debug (Iteration 1122) ===
Q mean: -5.341601
Q std: 6.474934
Actor loss: 5.345592
Action reg: 0.003992
  l1.weight: grad_norm = 0.006025
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004307
Total gradient norm: 0.016348
=== Actor Training Debug (Iteration 1123) ===
Q mean: -5.732105
Q std: 6.362959
Actor loss: 5.736083
Action reg: 0.003977
  l1.weight: grad_norm = 0.013077
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.012126
Total gradient norm: 0.041697
=== Actor Training Debug (Iteration 1124) ===
Q mean: -6.095597
Q std: 6.577211
Actor loss: 6.099585
Action reg: 0.003988
  l1.weight: grad_norm = 0.020070
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.020248
Total gradient norm: 0.083777
=== Actor Training Debug (Iteration 1125) ===
Q mean: -5.839350
Q std: 6.515854
Actor loss: 5.843333
Action reg: 0.003983
  l1.weight: grad_norm = 0.008667
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.008665
Total gradient norm: 0.023497
=== Actor Training Debug (Iteration 1126) ===
Q mean: -6.478398
Q std: 7.071806
Actor loss: 6.482381
Action reg: 0.003983
  l1.weight: grad_norm = 0.011458
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.010960
Total gradient norm: 0.047711
=== Actor Training Debug (Iteration 1127) ===
Q mean: -5.686481
Q std: 6.425932
Actor loss: 5.690456
Action reg: 0.003975
  l1.weight: grad_norm = 0.006490
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.006802
Total gradient norm: 0.036478
=== Actor Training Debug (Iteration 1128) ===
Q mean: -5.487573
Q std: 6.490928
Actor loss: 5.491560
Action reg: 0.003987
  l1.weight: grad_norm = 0.014644
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.013382
Total gradient norm: 0.062056
=== Actor Training Debug (Iteration 1129) ===
Q mean: -5.443992
Q std: 6.393828
Actor loss: 5.447985
Action reg: 0.003993
  l1.weight: grad_norm = 0.004210
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003477
Total gradient norm: 0.019690
=== Actor Training Debug (Iteration 1130) ===
Q mean: -5.164022
Q std: 6.173529
Actor loss: 5.167993
Action reg: 0.003970
  l1.weight: grad_norm = 0.009249
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.008213
Total gradient norm: 0.038491
=== Actor Training Debug (Iteration 1131) ===
Q mean: -6.025732
Q std: 7.112441
Actor loss: 6.029709
Action reg: 0.003978
  l1.weight: grad_norm = 0.009698
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.008011
Total gradient norm: 0.050378
=== Actor Training Debug (Iteration 1132) ===
Q mean: -5.693686
Q std: 6.671368
Actor loss: 5.697672
Action reg: 0.003986
  l1.weight: grad_norm = 0.002013
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.001714
Total gradient norm: 0.006345
=== Actor Training Debug (Iteration 1133) ===
Q mean: -4.859643
Q std: 6.194018
Actor loss: 4.863632
Action reg: 0.003989
  l1.weight: grad_norm = 0.003003
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.002815
Total gradient norm: 0.009132
=== Actor Training Debug (Iteration 1134) ===
Q mean: -6.701224
Q std: 7.393939
Actor loss: 6.705213
Action reg: 0.003988
  l1.weight: grad_norm = 0.009633
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.008740
Total gradient norm: 0.039311
=== Actor Training Debug (Iteration 1135) ===
Q mean: -5.720689
Q std: 7.079808
Actor loss: 5.724658
Action reg: 0.003969
  l1.weight: grad_norm = 0.007271
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.005383
Total gradient norm: 0.022176
=== Actor Training Debug (Iteration 1136) ===
Q mean: -6.273576
Q std: 6.883046
Actor loss: 6.277562
Action reg: 0.003986
  l1.weight: grad_norm = 0.019062
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.019468
Total gradient norm: 0.068399
=== Actor Training Debug (Iteration 1137) ===
Q mean: -5.667947
Q std: 6.529738
Actor loss: 5.671933
Action reg: 0.003987
  l1.weight: grad_norm = 0.011718
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.011324
Total gradient norm: 0.036023
=== Actor Training Debug (Iteration 1138) ===
Q mean: -5.602212
Q std: 6.317492
Actor loss: 5.606183
Action reg: 0.003970
  l1.weight: grad_norm = 0.019526
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.015507
Total gradient norm: 0.059742
=== Actor Training Debug (Iteration 1139) ===
Q mean: -5.749271
Q std: 6.842857
Actor loss: 5.753255
Action reg: 0.003984
  l1.weight: grad_norm = 0.010840
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.009461
Total gradient norm: 0.060423
=== Actor Training Debug (Iteration 1140) ===
Q mean: -5.694831
Q std: 7.000150
Actor loss: 5.698811
Action reg: 0.003980
  l1.weight: grad_norm = 0.006950
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.005499
Total gradient norm: 0.027992
=== Actor Training Debug (Iteration 1141) ===
Q mean: -5.424732
Q std: 6.534303
Actor loss: 5.428709
Action reg: 0.003976
  l1.weight: grad_norm = 0.009901
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.008217
Total gradient norm: 0.040219
=== Actor Training Debug (Iteration 1142) ===
Q mean: -6.089602
Q std: 7.029944
Actor loss: 6.093585
Action reg: 0.003983
  l1.weight: grad_norm = 0.013949
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.015350
Total gradient norm: 0.048648
=== Actor Training Debug (Iteration 1143) ===
Q mean: -5.653643
Q std: 6.592080
Actor loss: 5.657625
Action reg: 0.003982
  l1.weight: grad_norm = 0.015664
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.014943
Total gradient norm: 0.049267
=== Actor Training Debug (Iteration 1144) ===
Q mean: -6.021135
Q std: 6.858264
Actor loss: 6.025115
Action reg: 0.003980
  l1.weight: grad_norm = 0.013206
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.012137
Total gradient norm: 0.045699
=== Actor Training Debug (Iteration 1145) ===
Q mean: -6.251179
Q std: 7.067328
Actor loss: 6.255166
Action reg: 0.003987
  l1.weight: grad_norm = 0.024020
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.025078
Total gradient norm: 0.079840
=== Actor Training Debug (Iteration 1146) ===
Q mean: -6.494353
Q std: 7.247262
Actor loss: 6.498336
Action reg: 0.003983
  l1.weight: grad_norm = 0.024423
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.023074
Total gradient norm: 0.096520
=== Actor Training Debug (Iteration 1147) ===
Q mean: -6.750093
Q std: 7.398409
Actor loss: 6.754076
Action reg: 0.003984
  l1.weight: grad_norm = 0.007465
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.007955
Total gradient norm: 0.034506
=== Actor Training Debug (Iteration 1148) ===
Q mean: -6.244723
Q std: 7.319401
Actor loss: 6.248697
Action reg: 0.003973
  l1.weight: grad_norm = 0.014225
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.012734
Total gradient norm: 0.062026
=== Actor Training Debug (Iteration 1149) ===
Q mean: -5.258594
Q std: 6.395622
Actor loss: 5.262558
Action reg: 0.003965
  l1.weight: grad_norm = 0.063012
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.066608
Total gradient norm: 0.202746
=== Actor Training Debug (Iteration 1150) ===
Q mean: -5.979115
Q std: 7.027160
Actor loss: 5.983090
Action reg: 0.003976
  l1.weight: grad_norm = 0.012194
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.012693
Total gradient norm: 0.047833
=== Actor Training Debug (Iteration 1151) ===
Q mean: -5.532478
Q std: 6.793384
Actor loss: 5.536460
Action reg: 0.003982
  l1.weight: grad_norm = 0.014007
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.010636
Total gradient norm: 0.053953
=== Actor Training Debug (Iteration 1152) ===
Q mean: -5.835616
Q std: 6.616868
Actor loss: 5.839603
Action reg: 0.003987
  l1.weight: grad_norm = 0.010908
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010223
Total gradient norm: 0.039677
=== Actor Training Debug (Iteration 1153) ===
Q mean: -5.298464
Q std: 6.724664
Actor loss: 5.302441
Action reg: 0.003977
  l1.weight: grad_norm = 0.039938
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.041817
Total gradient norm: 0.149265
=== Actor Training Debug (Iteration 1154) ===
Q mean: -5.741780
Q std: 6.950757
Actor loss: 5.745766
Action reg: 0.003985
  l1.weight: grad_norm = 0.010902
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.009230
Total gradient norm: 0.041583
=== Actor Training Debug (Iteration 1155) ===
Q mean: -5.511759
Q std: 6.419507
Actor loss: 5.515739
Action reg: 0.003981
  l1.weight: grad_norm = 0.018097
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.013462
Total gradient norm: 0.081221
=== Actor Training Debug (Iteration 1156) ===
Q mean: -6.247342
Q std: 6.591901
Actor loss: 6.251326
Action reg: 0.003984
  l1.weight: grad_norm = 0.012189
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.010635
Total gradient norm: 0.041374
=== Actor Training Debug (Iteration 1157) ===
Q mean: -5.903087
Q std: 6.860232
Actor loss: 5.907067
Action reg: 0.003980
  l1.weight: grad_norm = 0.017061
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.012022
Total gradient norm: 0.060924
=== Actor Training Debug (Iteration 1158) ===
Q mean: -6.357144
Q std: 7.495814
Actor loss: 6.361120
Action reg: 0.003975
  l1.weight: grad_norm = 0.034061
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.032866
Total gradient norm: 0.120861
=== Actor Training Debug (Iteration 1159) ===
Q mean: -5.987722
Q std: 6.757067
Actor loss: 5.991708
Action reg: 0.003986
  l1.weight: grad_norm = 0.012150
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010812
Total gradient norm: 0.045008
=== Actor Training Debug (Iteration 1160) ===
Q mean: -5.607239
Q std: 6.339046
Actor loss: 5.611228
Action reg: 0.003989
  l1.weight: grad_norm = 0.004613
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.004867
Total gradient norm: 0.018741
=== Actor Training Debug (Iteration 1161) ===
Q mean: -5.712576
Q std: 6.895287
Actor loss: 5.716561
Action reg: 0.003984
  l1.weight: grad_norm = 0.015618
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.014303
Total gradient norm: 0.046813
=== Actor Training Debug (Iteration 1162) ===
Q mean: -6.136223
Q std: 7.305125
Actor loss: 6.140197
Action reg: 0.003974
  l1.weight: grad_norm = 0.008486
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.006933
Total gradient norm: 0.029095
=== Actor Training Debug (Iteration 1163) ===
Q mean: -6.537475
Q std: 7.380990
Actor loss: 6.541456
Action reg: 0.003981
  l1.weight: grad_norm = 0.016134
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.015522
Total gradient norm: 0.067129
=== Actor Training Debug (Iteration 1164) ===
Q mean: -5.902724
Q std: 6.919957
Actor loss: 5.906697
Action reg: 0.003973
  l1.weight: grad_norm = 0.015659
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.017801
Total gradient norm: 0.057576
=== Actor Training Debug (Iteration 1165) ===
Q mean: -6.540128
Q std: 7.023493
Actor loss: 6.544109
Action reg: 0.003982
  l1.weight: grad_norm = 0.011015
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.012134
Total gradient norm: 0.044786
=== Actor Training Debug (Iteration 1166) ===
Q mean: -5.891069
Q std: 6.805943
Actor loss: 5.895051
Action reg: 0.003981
  l1.weight: grad_norm = 0.005859
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.005904
Total gradient norm: 0.020689
=== Actor Training Debug (Iteration 1167) ===
Q mean: -5.927589
Q std: 7.168799
Actor loss: 5.931571
Action reg: 0.003982
  l1.weight: grad_norm = 0.006472
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004604
Total gradient norm: 0.022140
=== Actor Training Debug (Iteration 1168) ===
Q mean: -6.114990
Q std: 7.326324
Actor loss: 6.118976
Action reg: 0.003985
  l1.weight: grad_norm = 0.016194
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.015804
Total gradient norm: 0.043447
=== Actor Training Debug (Iteration 1169) ===
Q mean: -5.039688
Q std: 6.072268
Actor loss: 5.043677
Action reg: 0.003989
  l1.weight: grad_norm = 0.010059
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.008573
Total gradient norm: 0.051314
=== Actor Training Debug (Iteration 1170) ===
Q mean: -5.791432
Q std: 6.762712
Actor loss: 5.795415
Action reg: 0.003983
  l1.weight: grad_norm = 0.009989
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.006772
Total gradient norm: 0.037451
=== Actor Training Debug (Iteration 1171) ===
Q mean: -5.470037
Q std: 6.276299
Actor loss: 5.474028
Action reg: 0.003991
  l1.weight: grad_norm = 0.006523
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004657
Total gradient norm: 0.025112
=== Actor Training Debug (Iteration 1172) ===
Q mean: -6.141174
Q std: 6.844202
Actor loss: 6.145161
Action reg: 0.003987
  l1.weight: grad_norm = 0.005089
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.005509
Total gradient norm: 0.020425
=== Actor Training Debug (Iteration 1173) ===
Q mean: -6.850892
Q std: 6.899960
Actor loss: 6.854881
Action reg: 0.003989
  l1.weight: grad_norm = 0.012510
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.010863
Total gradient norm: 0.052205
=== Actor Training Debug (Iteration 1174) ===
Q mean: -5.504761
Q std: 6.794475
Actor loss: 5.508734
Action reg: 0.003973
  l1.weight: grad_norm = 0.024990
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.021711
Total gradient norm: 0.072789
=== Actor Training Debug (Iteration 1175) ===
Q mean: -5.788871
Q std: 7.117951
Actor loss: 5.792856
Action reg: 0.003985
  l1.weight: grad_norm = 0.012619
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.009796
Total gradient norm: 0.038430
=== Actor Training Debug (Iteration 1176) ===
Q mean: -5.995362
Q std: 7.054049
Actor loss: 5.999346
Action reg: 0.003984
  l1.weight: grad_norm = 0.012238
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.009934
Total gradient norm: 0.041123
=== Actor Training Debug (Iteration 1177) ===
Q mean: -6.626215
Q std: 7.206639
Actor loss: 6.630199
Action reg: 0.003985
  l1.weight: grad_norm = 0.013240
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.012599
Total gradient norm: 0.039829
=== Actor Training Debug (Iteration 1178) ===
Q mean: -6.533812
Q std: 7.618849
Actor loss: 6.537807
Action reg: 0.003994
  l1.weight: grad_norm = 0.004757
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.005001
Total gradient norm: 0.019565
=== Actor Training Debug (Iteration 1179) ===
Q mean: -5.730837
Q std: 6.841069
Actor loss: 5.734810
Action reg: 0.003973
  l1.weight: grad_norm = 0.013253
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.014026
Total gradient norm: 0.059678
=== Actor Training Debug (Iteration 1180) ===
Q mean: -5.991284
Q std: 7.030459
Actor loss: 5.995281
Action reg: 0.003997
  l1.weight: grad_norm = 0.001508
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001341
Total gradient norm: 0.006216
=== Actor Training Debug (Iteration 1181) ===
Q mean: -5.512092
Q std: 6.616730
Actor loss: 5.516075
Action reg: 0.003984
  l1.weight: grad_norm = 0.017006
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.014520
Total gradient norm: 0.055627
=== Actor Training Debug (Iteration 1182) ===
Q mean: -6.088009
Q std: 7.305750
Actor loss: 6.091997
Action reg: 0.003988
  l1.weight: grad_norm = 0.018500
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.019091
Total gradient norm: 0.054452
=== Actor Training Debug (Iteration 1183) ===
Q mean: -6.243105
Q std: 7.128847
Actor loss: 6.247098
Action reg: 0.003993
  l1.weight: grad_norm = 0.006641
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.006451
Total gradient norm: 0.017377
=== Actor Training Debug (Iteration 1184) ===
Q mean: -5.461346
Q std: 6.738353
Actor loss: 5.465331
Action reg: 0.003985
  l1.weight: grad_norm = 0.007852
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.005847
Total gradient norm: 0.030795
=== Actor Training Debug (Iteration 1185) ===
Q mean: -5.976125
Q std: 6.815168
Actor loss: 5.980113
Action reg: 0.003988
  l1.weight: grad_norm = 0.005153
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003731
Total gradient norm: 0.014277
=== Actor Training Debug (Iteration 1186) ===
Q mean: -5.708987
Q std: 6.328934
Actor loss: 5.712967
Action reg: 0.003980
  l1.weight: grad_norm = 0.018629
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.016064
Total gradient norm: 0.072987
=== Actor Training Debug (Iteration 1187) ===
Q mean: -7.092817
Q std: 7.602571
Actor loss: 7.096804
Action reg: 0.003987
  l1.weight: grad_norm = 0.008234
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.006471
Total gradient norm: 0.026215
=== Actor Training Debug (Iteration 1188) ===
Q mean: -5.628595
Q std: 6.873095
Actor loss: 5.632574
Action reg: 0.003978
  l1.weight: grad_norm = 0.011312
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.011043
Total gradient norm: 0.030941
=== Actor Training Debug (Iteration 1189) ===
Q mean: -6.090121
Q std: 7.436330
Actor loss: 6.094106
Action reg: 0.003985
  l1.weight: grad_norm = 0.006185
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.005951
Total gradient norm: 0.033852
=== Actor Training Debug (Iteration 1190) ===
Q mean: -6.025466
Q std: 7.124946
Actor loss: 6.029448
Action reg: 0.003981
  l1.weight: grad_norm = 0.008407
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.006173
Total gradient norm: 0.027495
=== Actor Training Debug (Iteration 1191) ===
Q mean: -6.585253
Q std: 7.245721
Actor loss: 6.589248
Action reg: 0.003995
  l1.weight: grad_norm = 0.001814
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001539
Total gradient norm: 0.007789
=== Actor Training Debug (Iteration 1192) ===
Q mean: -6.333277
Q std: 7.185173
Actor loss: 6.337256
Action reg: 0.003979
  l1.weight: grad_norm = 0.034325
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.032178
Total gradient norm: 0.110308
=== Actor Training Debug (Iteration 1193) ===
Q mean: -6.145166
Q std: 7.316042
Actor loss: 6.149148
Action reg: 0.003982
  l1.weight: grad_norm = 0.019903
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.016582
Total gradient norm: 0.059274
=== Actor Training Debug (Iteration 1194) ===
Q mean: -5.597507
Q std: 6.590833
Actor loss: 5.601492
Action reg: 0.003984
  l1.weight: grad_norm = 0.006832
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.007080
Total gradient norm: 0.033915
=== Actor Training Debug (Iteration 1195) ===
Q mean: -5.790359
Q std: 6.877319
Actor loss: 5.794350
Action reg: 0.003990
  l1.weight: grad_norm = 0.012709
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.009772
Total gradient norm: 0.044384
=== Actor Training Debug (Iteration 1196) ===
Q mean: -6.426412
Q std: 6.904160
Actor loss: 6.430402
Action reg: 0.003991
  l1.weight: grad_norm = 0.017426
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.014026
Total gradient norm: 0.054553
=== Actor Training Debug (Iteration 1197) ===
Q mean: -6.652062
Q std: 7.818470
Actor loss: 6.656051
Action reg: 0.003989
  l1.weight: grad_norm = 0.003878
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.003454
Total gradient norm: 0.013289
=== Actor Training Debug (Iteration 1198) ===
Q mean: -6.189316
Q std: 7.286447
Actor loss: 6.193296
Action reg: 0.003980
  l1.weight: grad_norm = 0.007895
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.006101
Total gradient norm: 0.031774
=== Actor Training Debug (Iteration 1199) ===
Q mean: -5.286854
Q std: 6.452643
Actor loss: 5.290843
Action reg: 0.003989
  l1.weight: grad_norm = 0.011071
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.008158
Total gradient norm: 0.038140
=== Actor Training Debug (Iteration 1200) ===
Q mean: -6.639753
Q std: 7.525387
Actor loss: 6.643734
Action reg: 0.003981
  l1.weight: grad_norm = 0.011782
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.011975
Total gradient norm: 0.048125
=== Actor Training Debug (Iteration 1201) ===
Q mean: -7.027935
Q std: 7.642723
Actor loss: 7.031930
Action reg: 0.003996
  l1.weight: grad_norm = 0.010034
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.008339
Total gradient norm: 0.036065
=== Actor Training Debug (Iteration 1202) ===
Q mean: -5.589563
Q std: 6.799160
Actor loss: 5.593550
Action reg: 0.003987
  l1.weight: grad_norm = 0.002946
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.002172
Total gradient norm: 0.008800
=== Actor Training Debug (Iteration 1203) ===
Q mean: -5.380256
Q std: 6.666567
Actor loss: 5.384244
Action reg: 0.003988
  l1.weight: grad_norm = 0.009154
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.005833
Total gradient norm: 0.023581
=== Actor Training Debug (Iteration 1204) ===
Q mean: -6.834588
Q std: 7.553536
Actor loss: 6.838573
Action reg: 0.003985
  l1.weight: grad_norm = 0.012478
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.011705
Total gradient norm: 0.057754
=== Actor Training Debug (Iteration 1205) ===
Q mean: -6.762071
Q std: 7.503155
Actor loss: 6.766055
Action reg: 0.003984
  l1.weight: grad_norm = 0.009824
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.008523
Total gradient norm: 0.027421
=== Actor Training Debug (Iteration 1206) ===
Q mean: -6.694705
Q std: 7.610126
Actor loss: 6.698677
Action reg: 0.003972
  l1.weight: grad_norm = 0.013437
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.009460
Total gradient norm: 0.060903
=== Actor Training Debug (Iteration 1207) ===
Q mean: -5.522206
Q std: 6.812788
Actor loss: 5.526190
Action reg: 0.003984
  l1.weight: grad_norm = 0.001938
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.001449
Total gradient norm: 0.006401
=== Actor Training Debug (Iteration 1208) ===
Q mean: -6.361639
Q std: 7.896716
Actor loss: 6.365627
Action reg: 0.003988
  l1.weight: grad_norm = 0.012288
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.010500
Total gradient norm: 0.040637
=== Actor Training Debug (Iteration 1209) ===
Q mean: -6.804021
Q std: 7.764015
Actor loss: 6.808007
Action reg: 0.003986
  l1.weight: grad_norm = 0.016046
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.015855
Total gradient norm: 0.071587
=== Actor Training Debug (Iteration 1210) ===
Q mean: -6.652326
Q std: 7.179636
Actor loss: 6.656304
Action reg: 0.003979
  l1.weight: grad_norm = 0.007419
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.008197
Total gradient norm: 0.027544
=== Actor Training Debug (Iteration 1211) ===
Q mean: -6.050824
Q std: 7.203607
Actor loss: 6.054809
Action reg: 0.003984
  l1.weight: grad_norm = 0.010781
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.009712
Total gradient norm: 0.041421
=== Actor Training Debug (Iteration 1212) ===
Q mean: -6.021655
Q std: 7.099730
Actor loss: 6.025646
Action reg: 0.003991
  l1.weight: grad_norm = 0.005398
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004574
Total gradient norm: 0.015718
=== Actor Training Debug (Iteration 1213) ===
Q mean: -6.458740
Q std: 7.041906
Actor loss: 6.462727
Action reg: 0.003986
  l1.weight: grad_norm = 0.013283
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.014507
Total gradient norm: 0.052389
=== Actor Training Debug (Iteration 1214) ===
Q mean: -6.835683
Q std: 7.882083
Actor loss: 6.839659
Action reg: 0.003975
  l1.weight: grad_norm = 0.009873
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.008304
Total gradient norm: 0.048083
=== Actor Training Debug (Iteration 1215) ===
Q mean: -6.184181
Q std: 7.550635
Actor loss: 6.188159
Action reg: 0.003978
  l1.weight: grad_norm = 0.021999
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.022535
Total gradient norm: 0.095200
=== Actor Training Debug (Iteration 1216) ===
Q mean: -5.782673
Q std: 6.666731
Actor loss: 5.786663
Action reg: 0.003990
  l1.weight: grad_norm = 0.009078
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.008229
Total gradient norm: 0.029267
=== Actor Training Debug (Iteration 1217) ===
Q mean: -7.204422
Q std: 7.785228
Actor loss: 7.208412
Action reg: 0.003990
  l1.weight: grad_norm = 0.016827
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.017079
Total gradient norm: 0.056374
=== Actor Training Debug (Iteration 1218) ===
Q mean: -6.210070
Q std: 7.252038
Actor loss: 6.214053
Action reg: 0.003984
  l1.weight: grad_norm = 0.006876
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.005877
Total gradient norm: 0.026854
=== Actor Training Debug (Iteration 1219) ===
Q mean: -5.071860
Q std: 6.485529
Actor loss: 5.075839
Action reg: 0.003978
  l1.weight: grad_norm = 0.018460
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.015880
Total gradient norm: 0.045328
=== Actor Training Debug (Iteration 1220) ===
Q mean: -5.892156
Q std: 6.695943
Actor loss: 5.896144
Action reg: 0.003988
  l1.weight: grad_norm = 0.013966
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.014157
Total gradient norm: 0.037548
=== Actor Training Debug (Iteration 1221) ===
Q mean: -6.645671
Q std: 7.377276
Actor loss: 6.649650
Action reg: 0.003979
  l1.weight: grad_norm = 0.008877
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.008211
Total gradient norm: 0.055517
=== Actor Training Debug (Iteration 1222) ===
Q mean: -5.683723
Q std: 6.576329
Actor loss: 5.687710
Action reg: 0.003987
  l1.weight: grad_norm = 0.019511
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.017961
Total gradient norm: 0.092638
=== Actor Training Debug (Iteration 1223) ===
Q mean: -6.110883
Q std: 6.994462
Actor loss: 6.114858
Action reg: 0.003975
  l1.weight: grad_norm = 0.016875
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.017841
Total gradient norm: 0.057449
=== Actor Training Debug (Iteration 1224) ===
Q mean: -5.886434
Q std: 6.740129
Actor loss: 5.890419
Action reg: 0.003986
  l1.weight: grad_norm = 0.007540
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.006852
Total gradient norm: 0.025220
=== Actor Training Debug (Iteration 1225) ===
Q mean: -5.548326
Q std: 6.628605
Actor loss: 5.552310
Action reg: 0.003983
  l1.weight: grad_norm = 0.014940
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.011857
Total gradient norm: 0.056231
=== Actor Training Debug (Iteration 1226) ===
Q mean: -5.833523
Q std: 7.239861
Actor loss: 5.837502
Action reg: 0.003979
  l1.weight: grad_norm = 0.006555
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.005682
Total gradient norm: 0.025483
=== Actor Training Debug (Iteration 1227) ===
Q mean: -7.048140
Q std: 8.250193
Actor loss: 7.052121
Action reg: 0.003981
  l1.weight: grad_norm = 0.023798
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.022553
Total gradient norm: 0.081674
=== Actor Training Debug (Iteration 1228) ===
Q mean: -7.026489
Q std: 7.751666
Actor loss: 7.030474
Action reg: 0.003985
  l1.weight: grad_norm = 0.010761
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.008462
Total gradient norm: 0.046344
=== Actor Training Debug (Iteration 1229) ===
Q mean: -5.787005
Q std: 7.099781
Actor loss: 5.790995
Action reg: 0.003990
  l1.weight: grad_norm = 0.006816
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.005438
Total gradient norm: 0.024897
=== Actor Training Debug (Iteration 1230) ===
Q mean: -5.369139
Q std: 7.013683
Actor loss: 5.373115
Action reg: 0.003976
  l1.weight: grad_norm = 0.013312
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.011221
Total gradient norm: 0.056540
=== Actor Training Debug (Iteration 1231) ===
Q mean: -6.344713
Q std: 7.100279
Actor loss: 6.348697
Action reg: 0.003984
  l1.weight: grad_norm = 0.007483
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.006395
Total gradient norm: 0.027534
=== Actor Training Debug (Iteration 1232) ===
Q mean: -7.083765
Q std: 7.777541
Actor loss: 7.087746
Action reg: 0.003981
  l1.weight: grad_norm = 0.010251
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.008106
Total gradient norm: 0.028956
=== Actor Training Debug (Iteration 1233) ===
Q mean: -6.181435
Q std: 7.622296
Actor loss: 6.185423
Action reg: 0.003989
  l1.weight: grad_norm = 0.009225
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.007897
Total gradient norm: 0.044734
=== Actor Training Debug (Iteration 1234) ===
Q mean: -6.003507
Q std: 7.062142
Actor loss: 6.007502
Action reg: 0.003995
  l1.weight: grad_norm = 0.007144
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004831
Total gradient norm: 0.023690
=== Actor Training Debug (Iteration 1235) ===
Q mean: -5.443124
Q std: 7.559249
Actor loss: 5.447106
Action reg: 0.003982
  l1.weight: grad_norm = 0.014737
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.012176
Total gradient norm: 0.072620
=== Actor Training Debug (Iteration 1236) ===
Q mean: -6.538946
Q std: 7.587286
Actor loss: 6.542937
Action reg: 0.003991
  l1.weight: grad_norm = 0.004515
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.004084
Total gradient norm: 0.022234
=== Actor Training Debug (Iteration 1237) ===
Q mean: -6.609737
Q std: 7.194335
Actor loss: 6.613726
Action reg: 0.003989
  l1.weight: grad_norm = 0.005678
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.004585
Total gradient norm: 0.019023
=== Actor Training Debug (Iteration 1238) ===
Q mean: -6.037938
Q std: 6.643635
Actor loss: 6.041919
Action reg: 0.003980
  l1.weight: grad_norm = 0.015680
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.012867
Total gradient norm: 0.064315
=== Actor Training Debug (Iteration 1239) ===
Q mean: -7.096606
Q std: 7.813135
Actor loss: 7.100597
Action reg: 0.003991
  l1.weight: grad_norm = 0.006131
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005212
Total gradient norm: 0.033820
=== Actor Training Debug (Iteration 1240) ===
Q mean: -5.376832
Q std: 6.950683
Actor loss: 5.380817
Action reg: 0.003986
  l1.weight: grad_norm = 0.009622
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.008887
Total gradient norm: 0.041648
=== Actor Training Debug (Iteration 1241) ===
Q mean: -6.736758
Q std: 7.836402
Actor loss: 6.740749
Action reg: 0.003990
  l1.weight: grad_norm = 0.036040
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.023645
Total gradient norm: 0.101726
=== Actor Training Debug (Iteration 1242) ===
Q mean: -7.234497
Q std: 7.757177
Actor loss: 7.238481
Action reg: 0.003984
  l1.weight: grad_norm = 0.015317
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.011385
Total gradient norm: 0.048689
=== Actor Training Debug (Iteration 1243) ===
Q mean: -6.823751
Q std: 7.392941
Actor loss: 6.827740
Action reg: 0.003989
  l1.weight: grad_norm = 0.009422
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.008178
Total gradient norm: 0.038213
=== Actor Training Debug (Iteration 1244) ===
Q mean: -5.455915
Q std: 6.421587
Actor loss: 5.459891
Action reg: 0.003976
  l1.weight: grad_norm = 0.014077
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.009804
Total gradient norm: 0.042709
=== Actor Training Debug (Iteration 1245) ===
Q mean: -5.717854
Q std: 6.944236
Actor loss: 5.721840
Action reg: 0.003985
  l1.weight: grad_norm = 0.015094
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.014770
Total gradient norm: 0.051846
=== Actor Training Debug (Iteration 1246) ===
Q mean: -5.551577
Q std: 6.779818
Actor loss: 5.555567
Action reg: 0.003990
  l1.weight: grad_norm = 0.005596
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.004896
Total gradient norm: 0.020718
=== Actor Training Debug (Iteration 1247) ===
Q mean: -6.419404
Q std: 7.541021
Actor loss: 6.423383
Action reg: 0.003979
  l1.weight: grad_norm = 0.023636
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.019342
Total gradient norm: 0.090638
=== Actor Training Debug (Iteration 1248) ===
Q mean: -7.075006
Q std: 7.248814
Actor loss: 7.078981
Action reg: 0.003975
  l1.weight: grad_norm = 0.019902
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.018620
Total gradient norm: 0.059457
=== Actor Training Debug (Iteration 1249) ===
Q mean: -6.246432
Q std: 7.701408
Actor loss: 6.250416
Action reg: 0.003984
  l1.weight: grad_norm = 0.012603
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.010977
Total gradient norm: 0.038885
=== Actor Training Debug (Iteration 1250) ===
Q mean: -6.722874
Q std: 7.988651
Actor loss: 6.726857
Action reg: 0.003983
  l1.weight: grad_norm = 0.019024
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.017820
Total gradient norm: 0.062373
=== Actor Training Debug (Iteration 1251) ===
Q mean: -5.710205
Q std: 6.985781
Actor loss: 5.714191
Action reg: 0.003987
  l1.weight: grad_norm = 0.018390
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.018058
Total gradient norm: 0.055527
=== Actor Training Debug (Iteration 1252) ===
Q mean: -6.051106
Q std: 6.538182
Actor loss: 6.055087
Action reg: 0.003981
  l1.weight: grad_norm = 0.004520
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.003858
Total gradient norm: 0.020482
=== Actor Training Debug (Iteration 1253) ===
Q mean: -6.237769
Q std: 6.670548
Actor loss: 6.241757
Action reg: 0.003988
  l1.weight: grad_norm = 0.006196
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.005054
Total gradient norm: 0.019847
=== Actor Training Debug (Iteration 1254) ===
Q mean: -5.466554
Q std: 6.415962
Actor loss: 5.470535
Action reg: 0.003981
  l1.weight: grad_norm = 0.015328
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.015157
Total gradient norm: 0.054729
=== Actor Training Debug (Iteration 1255) ===
Q mean: -5.740774
Q std: 7.130890
Actor loss: 5.744768
Action reg: 0.003994
  l1.weight: grad_norm = 0.006489
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005125
Total gradient norm: 0.021183
=== Actor Training Debug (Iteration 1256) ===
Q mean: -5.759724
Q std: 8.000346
Actor loss: 5.763717
Action reg: 0.003993
  l1.weight: grad_norm = 0.000239
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.000221
Total gradient norm: 0.001376
=== Actor Training Debug (Iteration 1257) ===
Q mean: -7.082770
Q std: 7.978460
Actor loss: 7.086752
Action reg: 0.003982
  l1.weight: grad_norm = 0.017313
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.016247
Total gradient norm: 0.054518
=== Actor Training Debug (Iteration 1258) ===
Q mean: -6.610740
Q std: 7.949237
Actor loss: 6.614726
Action reg: 0.003986
  l1.weight: grad_norm = 0.009851
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.008563
Total gradient norm: 0.036784
=== Actor Training Debug (Iteration 1259) ===
Q mean: -6.306689
Q std: 7.201426
Actor loss: 6.310671
Action reg: 0.003982
  l1.weight: grad_norm = 0.007336
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.006513
Total gradient norm: 0.032449
=== Actor Training Debug (Iteration 1260) ===
Q mean: -6.552147
Q std: 7.400692
Actor loss: 6.556135
Action reg: 0.003988
  l1.weight: grad_norm = 0.013232
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.009278
Total gradient norm: 0.037127
=== Actor Training Debug (Iteration 1261) ===
Q mean: -5.927097
Q std: 6.641526
Actor loss: 5.931088
Action reg: 0.003991
  l1.weight: grad_norm = 0.012162
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.011049
Total gradient norm: 0.032922
=== Actor Training Debug (Iteration 1262) ===
Q mean: -6.563145
Q std: 7.473113
Actor loss: 6.567136
Action reg: 0.003991
  l1.weight: grad_norm = 0.005231
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003949
Total gradient norm: 0.017172
=== Actor Training Debug (Iteration 1263) ===
Q mean: -6.949650
Q std: 8.100330
Actor loss: 6.953646
Action reg: 0.003996
  l1.weight: grad_norm = 0.004664
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003265
Total gradient norm: 0.016220
=== Actor Training Debug (Iteration 1264) ===
Q mean: -5.870754
Q std: 7.361304
Actor loss: 5.874744
Action reg: 0.003990
  l1.weight: grad_norm = 0.007522
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.007649
Total gradient norm: 0.027056
=== Actor Training Debug (Iteration 1265) ===
Q mean: -6.727942
Q std: 8.384354
Actor loss: 6.731931
Action reg: 0.003989
  l1.weight: grad_norm = 0.013518
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.010806
Total gradient norm: 0.064363
=== Actor Training Debug (Iteration 1266) ===
Q mean: -6.006973
Q std: 7.360749
Actor loss: 6.010956
Action reg: 0.003983
  l1.weight: grad_norm = 0.021875
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.019241
Total gradient norm: 0.096484
=== Actor Training Debug (Iteration 1267) ===
Q mean: -5.959544
Q std: 6.725822
Actor loss: 5.963530
Action reg: 0.003986
  l1.weight: grad_norm = 0.017869
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.015372
Total gradient norm: 0.044668
=== Actor Training Debug (Iteration 1268) ===
Q mean: -6.857870
Q std: 7.242763
Actor loss: 6.861859
Action reg: 0.003989
  l1.weight: grad_norm = 0.012931
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.011695
Total gradient norm: 0.050351
=== Actor Training Debug (Iteration 1269) ===
Q mean: -7.115510
Q std: 7.603426
Actor loss: 7.119499
Action reg: 0.003988
  l1.weight: grad_norm = 0.014357
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.011500
Total gradient norm: 0.071990
=== Actor Training Debug (Iteration 1270) ===
Q mean: -5.537440
Q std: 7.399953
Actor loss: 5.541426
Action reg: 0.003985
  l1.weight: grad_norm = 0.008906
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.007640
Total gradient norm: 0.030505
=== Actor Training Debug (Iteration 1271) ===
Q mean: -5.837891
Q std: 7.767151
Actor loss: 5.841880
Action reg: 0.003989
  l1.weight: grad_norm = 0.016870
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.012416
Total gradient norm: 0.058661
=== Actor Training Debug (Iteration 1272) ===
Q mean: -6.622931
Q std: 8.056651
Actor loss: 6.626921
Action reg: 0.003990
  l1.weight: grad_norm = 0.009435
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.008219
Total gradient norm: 0.032051
=== Actor Training Debug (Iteration 1273) ===
Q mean: -5.563377
Q std: 6.902410
Actor loss: 5.567361
Action reg: 0.003983
  l1.weight: grad_norm = 0.010459
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.009691
Total gradient norm: 0.046665
=== Actor Training Debug (Iteration 1274) ===
Q mean: -6.942969
Q std: 7.352664
Actor loss: 6.946957
Action reg: 0.003987
  l1.weight: grad_norm = 0.008260
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.007670
Total gradient norm: 0.027058
=== Actor Training Debug (Iteration 1275) ===
Q mean: -7.242708
Q std: 7.701404
Actor loss: 7.246694
Action reg: 0.003985
  l1.weight: grad_norm = 0.013291
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.010152
Total gradient norm: 0.043796
=== Actor Training Debug (Iteration 1276) ===
Q mean: -5.596801
Q std: 6.784554
Actor loss: 5.600779
Action reg: 0.003978
  l1.weight: grad_norm = 0.010086
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.009440
Total gradient norm: 0.041231
=== Actor Training Debug (Iteration 1277) ===
Q mean: -6.379836
Q std: 7.771832
Actor loss: 6.383815
Action reg: 0.003980
  l1.weight: grad_norm = 0.028587
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.029944
Total gradient norm: 0.080483
=== Actor Training Debug (Iteration 1278) ===
Q mean: -6.817977
Q std: 7.787846
Actor loss: 6.821957
Action reg: 0.003980
  l1.weight: grad_norm = 0.005295
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.004614
Total gradient norm: 0.018498
=== Actor Training Debug (Iteration 1279) ===
Q mean: -6.836938
Q std: 7.622938
Actor loss: 6.840925
Action reg: 0.003987
  l1.weight: grad_norm = 0.016163
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.013351
Total gradient norm: 0.065358
=== Actor Training Debug (Iteration 1280) ===
Q mean: -6.699290
Q std: 7.866448
Actor loss: 6.703281
Action reg: 0.003991
  l1.weight: grad_norm = 0.010751
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.009398
Total gradient norm: 0.060445
=== Actor Training Debug (Iteration 1281) ===
Q mean: -5.737705
Q std: 7.270270
Actor loss: 5.741696
Action reg: 0.003991
  l1.weight: grad_norm = 0.007130
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.006350
Total gradient norm: 0.034335
=== Actor Training Debug (Iteration 1282) ===
Q mean: -5.655538
Q std: 7.346587
Actor loss: 5.659519
Action reg: 0.003980
  l1.weight: grad_norm = 0.012751
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.009778
Total gradient norm: 0.041900
=== Actor Training Debug (Iteration 1283) ===
Q mean: -5.908731
Q std: 7.105527
Actor loss: 5.912723
Action reg: 0.003991
  l1.weight: grad_norm = 0.022136
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.016771
Total gradient norm: 0.080140
=== Actor Training Debug (Iteration 1284) ===
Q mean: -7.044416
Q std: 7.851735
Actor loss: 7.048407
Action reg: 0.003990
  l1.weight: grad_norm = 0.006064
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.005732
Total gradient norm: 0.024026
=== Actor Training Debug (Iteration 1285) ===
Q mean: -6.580672
Q std: 7.844244
Actor loss: 6.584649
Action reg: 0.003977
  l1.weight: grad_norm = 0.015202
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.012723
Total gradient norm: 0.072684
=== Actor Training Debug (Iteration 1286) ===
Q mean: -6.746267
Q std: 8.198957
Actor loss: 6.750256
Action reg: 0.003989
  l1.weight: grad_norm = 0.012983
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.010790
Total gradient norm: 0.063895
=== Actor Training Debug (Iteration 1287) ===
Q mean: -5.571762
Q std: 7.302896
Actor loss: 5.575748
Action reg: 0.003986
  l1.weight: grad_norm = 0.018225
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.014757
Total gradient norm: 0.054015
=== Actor Training Debug (Iteration 1288) ===
Q mean: -6.280650
Q std: 7.363659
Actor loss: 6.284637
Action reg: 0.003987
  l1.weight: grad_norm = 0.004505
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004103
Total gradient norm: 0.015737
=== Actor Training Debug (Iteration 1289) ===
Q mean: -7.420769
Q std: 8.051557
Actor loss: 7.424743
Action reg: 0.003975
  l1.weight: grad_norm = 0.009459
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.008548
Total gradient norm: 0.026317
=== Actor Training Debug (Iteration 1290) ===
Q mean: -6.761763
Q std: 7.825927
Actor loss: 6.765738
Action reg: 0.003975
  l1.weight: grad_norm = 0.015916
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.014499
Total gradient norm: 0.051354
=== Actor Training Debug (Iteration 1291) ===
Q mean: -5.418681
Q std: 7.211078
Actor loss: 5.422667
Action reg: 0.003986
  l1.weight: grad_norm = 0.010364
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.008404
Total gradient norm: 0.037448
=== Actor Training Debug (Iteration 1292) ===
Q mean: -5.652986
Q std: 7.691060
Actor loss: 5.656979
Action reg: 0.003993
  l1.weight: grad_norm = 0.006563
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005504
Total gradient norm: 0.023376
=== Actor Training Debug (Iteration 1293) ===
Q mean: -6.229817
Q std: 7.478915
Actor loss: 6.233808
Action reg: 0.003990
  l1.weight: grad_norm = 0.016633
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.014889
Total gradient norm: 0.049323
=== Actor Training Debug (Iteration 1294) ===
Q mean: -6.762149
Q std: 7.745026
Actor loss: 6.766136
Action reg: 0.003987
  l1.weight: grad_norm = 0.008972
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.008189
Total gradient norm: 0.029506
=== Actor Training Debug (Iteration 1295) ===
Q mean: -7.405959
Q std: 7.717004
Actor loss: 7.409947
Action reg: 0.003989
  l1.weight: grad_norm = 0.005293
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004643
Total gradient norm: 0.019057
=== Actor Training Debug (Iteration 1296) ===
Q mean: -5.664479
Q std: 6.503852
Actor loss: 5.668471
Action reg: 0.003992
  l1.weight: grad_norm = 0.002451
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002031
Total gradient norm: 0.011167
=== Actor Training Debug (Iteration 1297) ===
Q mean: -6.260350
Q std: 7.681992
Actor loss: 6.264343
Action reg: 0.003993
  l1.weight: grad_norm = 0.007300
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.006153
Total gradient norm: 0.028057
=== Actor Training Debug (Iteration 1298) ===
Q mean: -5.294792
Q std: 6.694005
Actor loss: 5.298780
Action reg: 0.003988
  l1.weight: grad_norm = 0.009626
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.008128
Total gradient norm: 0.048592
=== Actor Training Debug (Iteration 1299) ===
Q mean: -5.307980
Q std: 6.959306
Actor loss: 5.311970
Action reg: 0.003990
  l1.weight: grad_norm = 0.006501
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.004841
Total gradient norm: 0.020201
=== Actor Training Debug (Iteration 1300) ===
Q mean: -7.200974
Q std: 7.751131
Actor loss: 7.204961
Action reg: 0.003987
  l1.weight: grad_norm = 0.006643
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.006314
Total gradient norm: 0.021877
=== Actor Training Debug (Iteration 1301) ===
Q mean: -6.903250
Q std: 7.359449
Actor loss: 6.907238
Action reg: 0.003988
  l1.weight: grad_norm = 0.016829
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.013579
Total gradient norm: 0.087477
=== Actor Training Debug (Iteration 1302) ===
Q mean: -7.407467
Q std: 7.904351
Actor loss: 7.411458
Action reg: 0.003991
  l1.weight: grad_norm = 0.005319
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.005250
Total gradient norm: 0.022152
=== Actor Training Debug (Iteration 1303) ===
Q mean: -5.789436
Q std: 7.114001
Actor loss: 5.793416
Action reg: 0.003980
  l1.weight: grad_norm = 0.012488
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.010960
Total gradient norm: 0.044292
=== Actor Training Debug (Iteration 1304) ===
Q mean: -5.553776
Q std: 7.288484
Actor loss: 5.557760
Action reg: 0.003984
  l1.weight: grad_norm = 0.016958
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.017016
Total gradient norm: 0.077420
=== Actor Training Debug (Iteration 1305) ===
Q mean: -6.700965
Q std: 8.216003
Actor loss: 6.704956
Action reg: 0.003991
  l1.weight: grad_norm = 0.010669
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.010129
Total gradient norm: 0.045536
=== Actor Training Debug (Iteration 1306) ===
Q mean: -6.223603
Q std: 7.639690
Actor loss: 6.227591
Action reg: 0.003988
  l1.weight: grad_norm = 0.013708
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.013349
Total gradient norm: 0.051168
=== Actor Training Debug (Iteration 1307) ===
Q mean: -6.200041
Q std: 7.133098
Actor loss: 6.204028
Action reg: 0.003987
  l1.weight: grad_norm = 0.004211
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.003240
Total gradient norm: 0.012258
=== Actor Training Debug (Iteration 1308) ===
Q mean: -7.028934
Q std: 8.038205
Actor loss: 7.032925
Action reg: 0.003991
  l1.weight: grad_norm = 0.021129
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.021273
Total gradient norm: 0.057535
=== Actor Training Debug (Iteration 1309) ===
Q mean: -6.573810
Q std: 7.295759
Actor loss: 6.577795
Action reg: 0.003985
  l1.weight: grad_norm = 0.013250
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.013997
Total gradient norm: 0.041306
=== Actor Training Debug (Iteration 1310) ===
Q mean: -5.801574
Q std: 7.528461
Actor loss: 5.805558
Action reg: 0.003984
  l1.weight: grad_norm = 0.008900
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.006780
Total gradient norm: 0.033733
=== Actor Training Debug (Iteration 1311) ===
Q mean: -6.027039
Q std: 7.390501
Actor loss: 6.031019
Action reg: 0.003980
  l1.weight: grad_norm = 0.010952
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.011142
Total gradient norm: 0.033793
=== Actor Training Debug (Iteration 1312) ===
Q mean: -6.766149
Q std: 7.549364
Actor loss: 6.770142
Action reg: 0.003993
  l1.weight: grad_norm = 0.015619
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.014354
Total gradient norm: 0.044965
=== Actor Training Debug (Iteration 1313) ===
Q mean: -7.210995
Q std: 8.147676
Actor loss: 7.214989
Action reg: 0.003994
  l1.weight: grad_norm = 0.012121
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.008429
Total gradient norm: 0.043389
=== Actor Training Debug (Iteration 1314) ===
Q mean: -6.836064
Q std: 8.327972
Actor loss: 6.840050
Action reg: 0.003987
  l1.weight: grad_norm = 0.011540
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.008724
Total gradient norm: 0.043965
=== Actor Training Debug (Iteration 1315) ===
Q mean: -5.673192
Q std: 7.020653
Actor loss: 5.677182
Action reg: 0.003990
  l1.weight: grad_norm = 0.010547
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.007749
Total gradient norm: 0.033105
=== Actor Training Debug (Iteration 1316) ===
Q mean: -6.853155
Q std: 7.884193
Actor loss: 6.857136
Action reg: 0.003981
  l1.weight: grad_norm = 0.011477
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.008674
Total gradient norm: 0.044970
=== Actor Training Debug (Iteration 1317) ===
Q mean: -6.703872
Q std: 7.783287
Actor loss: 6.707859
Action reg: 0.003987
  l1.weight: grad_norm = 0.007966
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.006876
Total gradient norm: 0.029409
=== Actor Training Debug (Iteration 1318) ===
Q mean: -6.187852
Q std: 7.485867
Actor loss: 6.191838
Action reg: 0.003986
  l1.weight: grad_norm = 0.012524
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.010512
Total gradient norm: 0.057975
=== Actor Training Debug (Iteration 1319) ===
Q mean: -5.984801
Q std: 7.462052
Actor loss: 5.988786
Action reg: 0.003984
  l1.weight: grad_norm = 0.014598
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.012650
Total gradient norm: 0.077386
=== Actor Training Debug (Iteration 1320) ===
Q mean: -6.482515
Q std: 7.696997
Actor loss: 6.486508
Action reg: 0.003993
  l1.weight: grad_norm = 0.003178
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002769
Total gradient norm: 0.012048
=== Actor Training Debug (Iteration 1321) ===
Q mean: -6.263748
Q std: 7.644786
Actor loss: 6.267727
Action reg: 0.003979
  l1.weight: grad_norm = 0.007046
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.006338
Total gradient norm: 0.030432
=== Actor Training Debug (Iteration 1322) ===
Q mean: -6.757677
Q std: 7.830454
Actor loss: 6.761666
Action reg: 0.003989
  l1.weight: grad_norm = 0.005044
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.004873
Total gradient norm: 0.017225
=== Actor Training Debug (Iteration 1323) ===
Q mean: -6.527041
Q std: 7.888847
Actor loss: 6.531027
Action reg: 0.003986
  l1.weight: grad_norm = 0.016393
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.016217
Total gradient norm: 0.083294
=== Actor Training Debug (Iteration 1324) ===
Q mean: -5.580774
Q std: 7.332573
Actor loss: 5.584764
Action reg: 0.003990
  l1.weight: grad_norm = 0.021236
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.017302
Total gradient norm: 0.100154
=== Actor Training Debug (Iteration 1325) ===
Q mean: -6.226964
Q std: 7.389752
Actor loss: 6.230951
Action reg: 0.003988
  l1.weight: grad_norm = 0.008461
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.006654
Total gradient norm: 0.026802
=== Actor Training Debug (Iteration 1326) ===
Q mean: -6.383730
Q std: 7.682709
Actor loss: 6.387719
Action reg: 0.003989
  l1.weight: grad_norm = 0.006603
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.006849
Total gradient norm: 0.039206
=== Actor Training Debug (Iteration 1327) ===
Q mean: -6.113316
Q std: 7.133162
Actor loss: 6.117306
Action reg: 0.003990
  l1.weight: grad_norm = 0.005091
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.003903
Total gradient norm: 0.015316
=== Actor Training Debug (Iteration 1328) ===
Q mean: -6.977276
Q std: 7.640781
Actor loss: 6.981265
Action reg: 0.003989
  l1.weight: grad_norm = 0.005787
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004641
Total gradient norm: 0.016813
=== Actor Training Debug (Iteration 1329) ===
Q mean: -6.124197
Q std: 7.854673
Actor loss: 6.128188
Action reg: 0.003991
  l1.weight: grad_norm = 0.006717
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.005931
Total gradient norm: 0.020542
=== Actor Training Debug (Iteration 1330) ===
Q mean: -5.999562
Q std: 7.500042
Actor loss: 6.003552
Action reg: 0.003990
  l1.weight: grad_norm = 0.013837
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.012171
Total gradient norm: 0.075518
=== Actor Training Debug (Iteration 1331) ===
Q mean: -5.943374
Q std: 7.735806
Actor loss: 5.947364
Action reg: 0.003990
  l1.weight: grad_norm = 0.008293
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.006983
Total gradient norm: 0.042200
=== Actor Training Debug (Iteration 1332) ===
Q mean: -7.230094
Q std: 7.955014
Actor loss: 7.234076
Action reg: 0.003982
  l1.weight: grad_norm = 0.023090
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.022284
Total gradient norm: 0.068015
=== Actor Training Debug (Iteration 1333) ===
Q mean: -6.302489
Q std: 7.225006
Actor loss: 6.306482
Action reg: 0.003992
  l1.weight: grad_norm = 0.002307
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.001854
Total gradient norm: 0.008530
=== Actor Training Debug (Iteration 1334) ===
Q mean: -6.625343
Q std: 7.596811
Actor loss: 6.629326
Action reg: 0.003983
  l1.weight: grad_norm = 0.011800
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.008695
Total gradient norm: 0.036848
=== Actor Training Debug (Iteration 1335) ===
Q mean: -6.798253
Q std: 8.080703
Actor loss: 6.802243
Action reg: 0.003990
  l1.weight: grad_norm = 0.016631
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.012494
Total gradient norm: 0.058140
=== Actor Training Debug (Iteration 1336) ===
Q mean: -6.103126
Q std: 7.875093
Actor loss: 6.107115
Action reg: 0.003989
  l1.weight: grad_norm = 0.001327
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.001326
Total gradient norm: 0.007693
=== Actor Training Debug (Iteration 1337) ===
Q mean: -5.118721
Q std: 7.183862
Actor loss: 5.122712
Action reg: 0.003992
  l1.weight: grad_norm = 0.011636
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.009760
Total gradient norm: 0.044340
=== Actor Training Debug (Iteration 1338) ===
Q mean: -5.846560
Q std: 7.447747
Actor loss: 5.850539
Action reg: 0.003979
  l1.weight: grad_norm = 0.011227
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.010224
Total gradient norm: 0.038973
=== Actor Training Debug (Iteration 1339) ===
Q mean: -6.499139
Q std: 7.574635
Actor loss: 6.503129
Action reg: 0.003991
  l1.weight: grad_norm = 0.013074
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.011609
Total gradient norm: 0.050344
=== Actor Training Debug (Iteration 1340) ===
Q mean: -6.836120
Q std: 7.501058
Actor loss: 6.840111
Action reg: 0.003991
  l1.weight: grad_norm = 0.010376
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.008148
Total gradient norm: 0.035632
=== Actor Training Debug (Iteration 1341) ===
Q mean: -7.189717
Q std: 7.780035
Actor loss: 7.193710
Action reg: 0.003993
  l1.weight: grad_norm = 0.023012
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.022293
Total gradient norm: 0.081947
=== Actor Training Debug (Iteration 1342) ===
Q mean: -6.279732
Q std: 7.142422
Actor loss: 6.283717
Action reg: 0.003985
  l1.weight: grad_norm = 0.014926
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.011426
Total gradient norm: 0.049269
=== Actor Training Debug (Iteration 1343) ===
Q mean: -5.543699
Q std: 7.043931
Actor loss: 5.547687
Action reg: 0.003988
  l1.weight: grad_norm = 0.018143
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.016737
Total gradient norm: 0.084812
=== Actor Training Debug (Iteration 1344) ===
Q mean: -6.904587
Q std: 8.084240
Actor loss: 6.908575
Action reg: 0.003987
  l1.weight: grad_norm = 0.011470
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.009662
Total gradient norm: 0.051806
=== Actor Training Debug (Iteration 1345) ===
Q mean: -5.910310
Q std: 7.476786
Actor loss: 5.914287
Action reg: 0.003977
  l1.weight: grad_norm = 0.019103
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.012986
Total gradient norm: 0.051579
=== Actor Training Debug (Iteration 1346) ===
Q mean: -7.255093
Q std: 8.103629
Actor loss: 7.259086
Action reg: 0.003993
  l1.weight: grad_norm = 0.007329
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.005833
Total gradient norm: 0.034284
=== Actor Training Debug (Iteration 1347) ===
Q mean: -6.223207
Q std: 7.629329
Actor loss: 6.227194
Action reg: 0.003988
  l1.weight: grad_norm = 0.017122
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.014580
Total gradient norm: 0.076983
=== Actor Training Debug (Iteration 1348) ===
Q mean: -6.681412
Q std: 7.881924
Actor loss: 6.685394
Action reg: 0.003983
  l1.weight: grad_norm = 0.025036
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.019145
Total gradient norm: 0.084113
=== Actor Training Debug (Iteration 1349) ===
Q mean: -6.354438
Q std: 7.551824
Actor loss: 6.358418
Action reg: 0.003980
  l1.weight: grad_norm = 0.008095
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.006803
Total gradient norm: 0.023992
=== Actor Training Debug (Iteration 1350) ===
Q mean: -7.159836
Q std: 7.351727
Actor loss: 7.163822
Action reg: 0.003985
  l1.weight: grad_norm = 0.018535
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.016821
Total gradient norm: 0.091123
=== Actor Training Debug (Iteration 1351) ===
Q mean: -6.112377
Q std: 7.388621
Actor loss: 6.116370
Action reg: 0.003993
  l1.weight: grad_norm = 0.008801
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.009258
Total gradient norm: 0.022147
=== Actor Training Debug (Iteration 1352) ===
Q mean: -5.458963
Q std: 7.301014
Actor loss: 5.462953
Action reg: 0.003990
  l1.weight: grad_norm = 0.013036
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.010935
Total gradient norm: 0.037851
=== Actor Training Debug (Iteration 1353) ===
Q mean: -5.949869
Q std: 7.356483
Actor loss: 5.953848
Action reg: 0.003979
  l1.weight: grad_norm = 0.009456
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.009350
Total gradient norm: 0.057742
=== Actor Training Debug (Iteration 1354) ===
Q mean: -7.613596
Q std: 8.443227
Actor loss: 7.617587
Action reg: 0.003990
  l1.weight: grad_norm = 0.008533
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.008098
Total gradient norm: 0.032716
=== Actor Training Debug (Iteration 1355) ===
Q mean: -7.349157
Q std: 7.943262
Actor loss: 7.353144
Action reg: 0.003987
  l1.weight: grad_norm = 0.004189
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.002985
Total gradient norm: 0.011186
=== Actor Training Debug (Iteration 1356) ===
Q mean: -6.739323
Q std: 8.435163
Actor loss: 6.743315
Action reg: 0.003992
  l1.weight: grad_norm = 0.011715
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.011377
Total gradient norm: 0.056388
=== Actor Training Debug (Iteration 1357) ===
Q mean: -6.639160
Q std: 7.885297
Actor loss: 6.643152
Action reg: 0.003992
  l1.weight: grad_norm = 0.013936
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.012292
Total gradient norm: 0.035421
=== Actor Training Debug (Iteration 1358) ===
Q mean: -6.256644
Q std: 7.862001
Actor loss: 6.260638
Action reg: 0.003995
  l1.weight: grad_norm = 0.015009
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.013280
Total gradient norm: 0.044981
=== Actor Training Debug (Iteration 1359) ===
Q mean: -7.224401
Q std: 8.248512
Actor loss: 7.228389
Action reg: 0.003988
  l1.weight: grad_norm = 0.010780
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.009733
Total gradient norm: 0.055429
=== Actor Training Debug (Iteration 1360) ===
Q mean: -7.248392
Q std: 8.044928
Actor loss: 7.252385
Action reg: 0.003993
  l1.weight: grad_norm = 0.015755
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.015362
Total gradient norm: 0.049158
=== Actor Training Debug (Iteration 1361) ===
Q mean: -7.338892
Q std: 8.261737
Actor loss: 7.342879
Action reg: 0.003987
  l1.weight: grad_norm = 0.022639
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.016609
Total gradient norm: 0.082425
=== Actor Training Debug (Iteration 1362) ===
Q mean: -6.203951
Q std: 7.778226
Actor loss: 6.207944
Action reg: 0.003993
  l1.weight: grad_norm = 0.007777
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.006365
Total gradient norm: 0.034257
=== Actor Training Debug (Iteration 1363) ===
Q mean: -6.636018
Q std: 7.583018
Actor loss: 6.640012
Action reg: 0.003994
  l1.weight: grad_norm = 0.010801
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.007536
Total gradient norm: 0.039136
=== Actor Training Debug (Iteration 1364) ===
Q mean: -6.561655
Q std: 7.621356
Actor loss: 6.565643
Action reg: 0.003988
  l1.weight: grad_norm = 0.019566
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.017835
Total gradient norm: 0.072601
=== Actor Training Debug (Iteration 1365) ===
Q mean: -6.703590
Q std: 8.110023
Actor loss: 6.707575
Action reg: 0.003985
  l1.weight: grad_norm = 0.018273
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.014595
Total gradient norm: 0.062356
=== Actor Training Debug (Iteration 1366) ===
Q mean: -5.812613
Q std: 7.185536
Actor loss: 5.816602
Action reg: 0.003990
  l1.weight: grad_norm = 0.008887
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.009003
Total gradient norm: 0.026054
=== Actor Training Debug (Iteration 1367) ===
Q mean: -6.108237
Q std: 7.646467
Actor loss: 6.112226
Action reg: 0.003989
  l1.weight: grad_norm = 0.014502
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.013881
Total gradient norm: 0.041744
=== Actor Training Debug (Iteration 1368) ===
Q mean: -7.356834
Q std: 8.108641
Actor loss: 7.360813
Action reg: 0.003979
  l1.weight: grad_norm = 0.023911
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.021979
Total gradient norm: 0.130278
=== Actor Training Debug (Iteration 1369) ===
Q mean: -6.989981
Q std: 8.388760
Actor loss: 6.993972
Action reg: 0.003991
  l1.weight: grad_norm = 0.007936
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.007544
Total gradient norm: 0.044163
=== Actor Training Debug (Iteration 1370) ===
Q mean: -6.303607
Q std: 7.678492
Actor loss: 6.307595
Action reg: 0.003987
  l1.weight: grad_norm = 0.017740
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.015906
Total gradient norm: 0.054637
=== Actor Training Debug (Iteration 1371) ===
Q mean: -7.203466
Q std: 8.669356
Actor loss: 7.207453
Action reg: 0.003987
  l1.weight: grad_norm = 0.008463
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.007163
Total gradient norm: 0.039226
=== Actor Training Debug (Iteration 1372) ===
Q mean: -6.140584
Q std: 7.761661
Actor loss: 6.144571
Action reg: 0.003987
  l1.weight: grad_norm = 0.012116
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.010811
Total gradient norm: 0.039750
=== Actor Training Debug (Iteration 1373) ===
Q mean: -6.480507
Q std: 7.250636
Actor loss: 6.484494
Action reg: 0.003987
  l1.weight: grad_norm = 0.013406
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.010888
Total gradient norm: 0.042669
=== Actor Training Debug (Iteration 1374) ===
Q mean: -7.503742
Q std: 8.251481
Actor loss: 7.507730
Action reg: 0.003988
  l1.weight: grad_norm = 0.012134
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.009684
Total gradient norm: 0.060805
=== Actor Training Debug (Iteration 1375) ===
Q mean: -6.737034
Q std: 8.056814
Actor loss: 6.741028
Action reg: 0.003994
  l1.weight: grad_norm = 0.005571
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.004849
Total gradient norm: 0.016828
=== Actor Training Debug (Iteration 1376) ===
Q mean: -6.945086
Q std: 8.126434
Actor loss: 6.949079
Action reg: 0.003992
  l1.weight: grad_norm = 0.004790
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004853
Total gradient norm: 0.013783
=== Actor Training Debug (Iteration 1377) ===
Q mean: -7.282649
Q std: 8.227656
Actor loss: 7.286638
Action reg: 0.003989
  l1.weight: grad_norm = 0.023730
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.019329
Total gradient norm: 0.122665
=== Actor Training Debug (Iteration 1378) ===
Q mean: -6.577576
Q std: 7.805108
Actor loss: 6.581559
Action reg: 0.003982
  l1.weight: grad_norm = 0.022668
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.017852
Total gradient norm: 0.075652
=== Actor Training Debug (Iteration 1379) ===
Q mean: -6.213490
Q std: 7.865093
Actor loss: 6.217478
Action reg: 0.003987
  l1.weight: grad_norm = 0.006887
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.005286
Total gradient norm: 0.025447
=== Actor Training Debug (Iteration 1380) ===
Q mean: -6.486418
Q std: 8.133201
Actor loss: 6.490412
Action reg: 0.003995
  l1.weight: grad_norm = 0.004369
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.003456
Total gradient norm: 0.016601
=== Actor Training Debug (Iteration 1381) ===
Q mean: -6.395043
Q std: 7.745395
Actor loss: 6.399032
Action reg: 0.003988
  l1.weight: grad_norm = 0.015834
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.012008
Total gradient norm: 0.044607
=== Actor Training Debug (Iteration 1382) ===
Q mean: -6.718938
Q std: 7.789745
Actor loss: 6.722925
Action reg: 0.003987
  l1.weight: grad_norm = 0.006886
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.005508
Total gradient norm: 0.018303
=== Actor Training Debug (Iteration 1383) ===
Q mean: -6.328501
Q std: 8.398704
Actor loss: 6.332487
Action reg: 0.003987
  l1.weight: grad_norm = 0.010255
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.007723
Total gradient norm: 0.031969
=== Actor Training Debug (Iteration 1384) ===
Q mean: -5.941761
Q std: 7.985792
Actor loss: 5.945747
Action reg: 0.003986
  l1.weight: grad_norm = 0.010936
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.008418
Total gradient norm: 0.043670
=== Actor Training Debug (Iteration 1385) ===
Q mean: -6.842636
Q std: 7.808665
Actor loss: 6.846626
Action reg: 0.003990
  l1.weight: grad_norm = 0.007258
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.005177
Total gradient norm: 0.026278
=== Actor Training Debug (Iteration 1386) ===
Q mean: -6.938796
Q std: 8.352036
Actor loss: 6.942786
Action reg: 0.003990
  l1.weight: grad_norm = 0.005111
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.004854
Total gradient norm: 0.022870
=== Actor Training Debug (Iteration 1387) ===
Q mean: -6.568996
Q std: 7.867452
Actor loss: 6.572985
Action reg: 0.003988
  l1.weight: grad_norm = 0.010585
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.010958
Total gradient norm: 0.038306
=== Actor Training Debug (Iteration 1388) ===
Q mean: -6.367839
Q std: 7.498189
Actor loss: 6.371828
Action reg: 0.003988
  l1.weight: grad_norm = 0.010207
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.009803
Total gradient norm: 0.042083
=== Actor Training Debug (Iteration 1389) ===
Q mean: -5.856726
Q std: 7.505273
Actor loss: 5.860711
Action reg: 0.003986
  l1.weight: grad_norm = 0.012015
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.010381
Total gradient norm: 0.043031
=== Actor Training Debug (Iteration 1390) ===
Q mean: -6.930877
Q std: 8.423707
Actor loss: 6.934865
Action reg: 0.003989
  l1.weight: grad_norm = 0.025117
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.019921
Total gradient norm: 0.068494
=== Actor Training Debug (Iteration 1391) ===
Q mean: -6.549840
Q std: 7.609502
Actor loss: 6.553835
Action reg: 0.003995
  l1.weight: grad_norm = 0.007377
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.006826
Total gradient norm: 0.039682
=== Actor Training Debug (Iteration 1392) ===
Q mean: -6.440523
Q std: 7.687445
Actor loss: 6.444507
Action reg: 0.003983
  l1.weight: grad_norm = 0.021560
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.015275
Total gradient norm: 0.051736
=== Actor Training Debug (Iteration 1393) ===
Q mean: -7.787193
Q std: 8.583320
Actor loss: 7.791183
Action reg: 0.003990
  l1.weight: grad_norm = 0.009307
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.008452
Total gradient norm: 0.029585
=== Actor Training Debug (Iteration 1394) ===
Q mean: -7.029451
Q std: 8.083009
Actor loss: 7.033447
Action reg: 0.003995
  l1.weight: grad_norm = 0.013559
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010822
Total gradient norm: 0.043560
=== Actor Training Debug (Iteration 1395) ===
Q mean: -7.221274
Q std: 8.312551
Actor loss: 7.225267
Action reg: 0.003994
  l1.weight: grad_norm = 0.000922
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.000750
Total gradient norm: 0.002702
=== Actor Training Debug (Iteration 1396) ===
Q mean: -7.116128
Q std: 8.827861
Actor loss: 7.120121
Action reg: 0.003992
  l1.weight: grad_norm = 0.011020
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.008956
Total gradient norm: 0.039972
=== Actor Training Debug (Iteration 1397) ===
Q mean: -6.343963
Q std: 7.944706
Actor loss: 6.347955
Action reg: 0.003992
  l1.weight: grad_norm = 0.017359
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.012733
Total gradient norm: 0.054440
=== Actor Training Debug (Iteration 1398) ===
Q mean: -6.748480
Q std: 7.758072
Actor loss: 6.752466
Action reg: 0.003986
  l1.weight: grad_norm = 0.012289
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.012478
Total gradient norm: 0.064722
=== Actor Training Debug (Iteration 1399) ===
Q mean: -6.182737
Q std: 7.449348
Actor loss: 6.186727
Action reg: 0.003989
  l1.weight: grad_norm = 0.003525
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.003533
Total gradient norm: 0.016518
=== Actor Training Debug (Iteration 1400) ===
Q mean: -7.389742
Q std: 8.390540
Actor loss: 7.393737
Action reg: 0.003995
  l1.weight: grad_norm = 0.001317
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000982
Total gradient norm: 0.003658
=== Actor Training Debug (Iteration 1401) ===
Q mean: -6.099319
Q std: 8.173143
Actor loss: 6.103301
Action reg: 0.003982
  l1.weight: grad_norm = 0.021281
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.020174
Total gradient norm: 0.059347
=== Actor Training Debug (Iteration 1402) ===
Q mean: -5.264230
Q std: 7.452063
Actor loss: 5.268219
Action reg: 0.003989
  l1.weight: grad_norm = 0.007470
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.006709
Total gradient norm: 0.025268
=== Actor Training Debug (Iteration 1403) ===
Q mean: -7.057634
Q std: 8.647213
Actor loss: 7.061618
Action reg: 0.003983
  l1.weight: grad_norm = 0.029008
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.018290
Total gradient norm: 0.101059
=== Actor Training Debug (Iteration 1404) ===
Q mean: -7.491323
Q std: 8.739077
Actor loss: 7.495317
Action reg: 0.003994
  l1.weight: grad_norm = 0.003183
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002993
Total gradient norm: 0.011347
=== Actor Training Debug (Iteration 1405) ===
Q mean: -7.101075
Q std: 8.154716
Actor loss: 7.105065
Action reg: 0.003990
  l1.weight: grad_norm = 0.009649
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.006727
Total gradient norm: 0.025866
=== Actor Training Debug (Iteration 1406) ===
Q mean: -7.831239
Q std: 8.470654
Actor loss: 7.835233
Action reg: 0.003994
  l1.weight: grad_norm = 0.004880
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003788
Total gradient norm: 0.018667
=== Actor Training Debug (Iteration 1407) ===
Q mean: -6.538911
Q std: 7.708396
Actor loss: 6.542907
Action reg: 0.003996
  l1.weight: grad_norm = 0.005749
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005811
Total gradient norm: 0.016740
=== Actor Training Debug (Iteration 1408) ===
Q mean: -7.223327
Q std: 8.485617
Actor loss: 7.227314
Action reg: 0.003987
  l1.weight: grad_norm = 0.036133
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.035532
Total gradient norm: 0.148357
=== Actor Training Debug (Iteration 1409) ===
Q mean: -6.812095
Q std: 8.239169
Actor loss: 6.816079
Action reg: 0.003984
  l1.weight: grad_norm = 0.015230
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.014264
Total gradient norm: 0.060805
=== Actor Training Debug (Iteration 1410) ===
Q mean: -6.490084
Q std: 7.765025
Actor loss: 6.494066
Action reg: 0.003982
  l1.weight: grad_norm = 0.036056
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.034022
Total gradient norm: 0.126353
=== Actor Training Debug (Iteration 1411) ===
Q mean: -6.092113
Q std: 7.664553
Actor loss: 6.096105
Action reg: 0.003992
  l1.weight: grad_norm = 0.006341
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.004906
Total gradient norm: 0.017005
=== Actor Training Debug (Iteration 1412) ===
Q mean: -7.110505
Q std: 8.532335
Actor loss: 7.114501
Action reg: 0.003995
  l1.weight: grad_norm = 0.001076
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.000846
Total gradient norm: 0.003333
=== Actor Training Debug (Iteration 1413) ===
Q mean: -7.115800
Q std: 8.588148
Actor loss: 7.119790
Action reg: 0.003990
  l1.weight: grad_norm = 0.015730
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.011145
Total gradient norm: 0.049548
=== Actor Training Debug (Iteration 1414) ===
Q mean: -6.013112
Q std: 7.593525
Actor loss: 6.017103
Action reg: 0.003991
  l1.weight: grad_norm = 0.011297
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.010146
Total gradient norm: 0.052761
=== Actor Training Debug (Iteration 1415) ===
Q mean: -6.732702
Q std: 7.963315
Actor loss: 6.736690
Action reg: 0.003987
  l1.weight: grad_norm = 0.012554
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.012388
Total gradient norm: 0.044686
=== Actor Training Debug (Iteration 1416) ===
Q mean: -7.119895
Q std: 8.464346
Actor loss: 7.123880
Action reg: 0.003985
  l1.weight: grad_norm = 0.021057
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.018398
Total gradient norm: 0.093612
=== Actor Training Debug (Iteration 1417) ===
Q mean: -7.211903
Q std: 8.228496
Actor loss: 7.215893
Action reg: 0.003990
  l1.weight: grad_norm = 0.011016
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.010267
Total gradient norm: 0.042275
=== Actor Training Debug (Iteration 1418) ===
Q mean: -6.748862
Q std: 8.075535
Actor loss: 6.752850
Action reg: 0.003988
  l1.weight: grad_norm = 0.015067
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.013859
Total gradient norm: 0.042489
=== Actor Training Debug (Iteration 1419) ===
Q mean: -7.319946
Q std: 8.852327
Actor loss: 7.323937
Action reg: 0.003992
  l1.weight: grad_norm = 0.002552
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.002112
Total gradient norm: 0.010607
=== Actor Training Debug (Iteration 1420) ===
Q mean: -7.481870
Q std: 8.873133
Actor loss: 7.485856
Action reg: 0.003986
  l1.weight: grad_norm = 0.012160
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.008667
Total gradient norm: 0.031746
=== Actor Training Debug (Iteration 1421) ===
Q mean: -6.661756
Q std: 8.650427
Actor loss: 6.665741
Action reg: 0.003985
  l1.weight: grad_norm = 0.029043
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.032710
Total gradient norm: 0.092228
=== Actor Training Debug (Iteration 1422) ===
Q mean: -7.179330
Q std: 8.530918
Actor loss: 7.183316
Action reg: 0.003987
  l1.weight: grad_norm = 0.008231
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.006985
Total gradient norm: 0.026609
=== Actor Training Debug (Iteration 1423) ===
Q mean: -6.852515
Q std: 8.095756
Actor loss: 6.856503
Action reg: 0.003988
  l1.weight: grad_norm = 0.010047
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.009925
Total gradient norm: 0.040341
=== Actor Training Debug (Iteration 1424) ===
Q mean: -7.117086
Q std: 8.301162
Actor loss: 7.121074
Action reg: 0.003988
  l1.weight: grad_norm = 0.034098
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.036648
Total gradient norm: 0.116611
=== Actor Training Debug (Iteration 1425) ===
Q mean: -6.276595
Q std: 8.095294
Actor loss: 6.280574
Action reg: 0.003979
  l1.weight: grad_norm = 0.023957
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.024448
Total gradient norm: 0.085270
=== Actor Training Debug (Iteration 1426) ===
Q mean: -6.121183
Q std: 7.905769
Actor loss: 6.125171
Action reg: 0.003988
  l1.weight: grad_norm = 0.009064
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.009017
Total gradient norm: 0.039583
=== Actor Training Debug (Iteration 1427) ===
Q mean: -7.172693
Q std: 8.535023
Actor loss: 7.176686
Action reg: 0.003993
  l1.weight: grad_norm = 0.004551
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004090
Total gradient norm: 0.016614
=== Actor Training Debug (Iteration 1428) ===
Q mean: -6.872113
Q std: 8.201448
Actor loss: 6.876100
Action reg: 0.003987
  l1.weight: grad_norm = 0.010505
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.009571
Total gradient norm: 0.037502
=== Actor Training Debug (Iteration 1429) ===
Q mean: -7.409558
Q std: 8.214010
Actor loss: 7.413548
Action reg: 0.003989
  l1.weight: grad_norm = 0.003045
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002633
Total gradient norm: 0.010595
=== Actor Training Debug (Iteration 1430) ===
Q mean: -6.693747
Q std: 7.925988
Actor loss: 6.697742
Action reg: 0.003996
  l1.weight: grad_norm = 0.017305
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.018937
Total gradient norm: 0.059435
=== Actor Training Debug (Iteration 1431) ===
Q mean: -7.100920
Q std: 8.268385
Actor loss: 7.104914
Action reg: 0.003994
  l1.weight: grad_norm = 0.006920
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006198
Total gradient norm: 0.032727
=== Actor Training Debug (Iteration 1432) ===
Q mean: -6.293266
Q std: 7.979251
Actor loss: 6.297256
Action reg: 0.003990
  l1.weight: grad_norm = 0.016182
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.014745
Total gradient norm: 0.092705
=== Actor Training Debug (Iteration 1433) ===
Q mean: -7.759571
Q std: 8.900497
Actor loss: 7.763561
Action reg: 0.003990
  l1.weight: grad_norm = 0.010770
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.009361
Total gradient norm: 0.024925
=== Actor Training Debug (Iteration 1434) ===
Q mean: -7.228322
Q std: 8.514203
Actor loss: 7.232313
Action reg: 0.003991
  l1.weight: grad_norm = 0.008297
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.009074
Total gradient norm: 0.039242
=== Actor Training Debug (Iteration 1435) ===
Q mean: -6.911367
Q std: 8.183965
Actor loss: 6.915358
Action reg: 0.003990
  l1.weight: grad_norm = 0.033636
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.033918
Total gradient norm: 0.119389
=== Actor Training Debug (Iteration 1436) ===
Q mean: -5.903085
Q std: 7.312781
Actor loss: 5.907074
Action reg: 0.003989
  l1.weight: grad_norm = 0.016548
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.017655
Total gradient norm: 0.053182
=== Actor Training Debug (Iteration 1437) ===
Q mean: -6.836715
Q std: 7.941559
Actor loss: 6.840705
Action reg: 0.003990
  l1.weight: grad_norm = 0.008183
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.007237
Total gradient norm: 0.024728
=== Actor Training Debug (Iteration 1438) ===
Q mean: -7.552077
Q std: 8.333466
Actor loss: 7.556071
Action reg: 0.003993
  l1.weight: grad_norm = 0.008285
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.006505
Total gradient norm: 0.029794
=== Actor Training Debug (Iteration 1439) ===
Q mean: -7.019417
Q std: 8.369821
Actor loss: 7.023407
Action reg: 0.003990
  l1.weight: grad_norm = 0.007345
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.006570
Total gradient norm: 0.039320
=== Actor Training Debug (Iteration 1440) ===
Q mean: -6.457985
Q std: 8.178183
Actor loss: 6.461982
Action reg: 0.003997
  l1.weight: grad_norm = 0.002879
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002393
Total gradient norm: 0.011210
=== Actor Training Debug (Iteration 1441) ===
Q mean: -6.915656
Q std: 8.087865
Actor loss: 6.919649
Action reg: 0.003993
  l1.weight: grad_norm = 0.017982
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.018892
Total gradient norm: 0.051921
=== Actor Training Debug (Iteration 1442) ===
Q mean: -7.908439
Q std: 9.074388
Actor loss: 7.912426
Action reg: 0.003987
  l1.weight: grad_norm = 0.015589
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.014385
Total gradient norm: 0.090253
=== Actor Training Debug (Iteration 1443) ===
Q mean: -7.329760
Q std: 8.467514
Actor loss: 7.333753
Action reg: 0.003993
  l1.weight: grad_norm = 0.013811
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.010608
Total gradient norm: 0.060039
=== Actor Training Debug (Iteration 1444) ===
Q mean: -5.768178
Q std: 7.481024
Actor loss: 5.772171
Action reg: 0.003993
  l1.weight: grad_norm = 0.014512
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.014731
Total gradient norm: 0.034849
=== Actor Training Debug (Iteration 1445) ===
Q mean: -7.435780
Q std: 8.826696
Actor loss: 7.439773
Action reg: 0.003993
  l1.weight: grad_norm = 0.007439
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.006505
Total gradient norm: 0.030894
=== Actor Training Debug (Iteration 1446) ===
Q mean: -6.249554
Q std: 7.957495
Actor loss: 6.253545
Action reg: 0.003991
  l1.weight: grad_norm = 0.009122
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.007471
Total gradient norm: 0.040839
=== Actor Training Debug (Iteration 1447) ===
Q mean: -6.590150
Q std: 8.491803
Actor loss: 6.594146
Action reg: 0.003996
  l1.weight: grad_norm = 0.012451
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.012464
Total gradient norm: 0.037074
=== Actor Training Debug (Iteration 1448) ===
Q mean: -6.089601
Q std: 7.860874
Actor loss: 6.093585
Action reg: 0.003985
  l1.weight: grad_norm = 0.016165
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.014275
Total gradient norm: 0.088627
=== Actor Training Debug (Iteration 1449) ===
Q mean: -6.716088
Q std: 8.091135
Actor loss: 6.720081
Action reg: 0.003993
  l1.weight: grad_norm = 0.003488
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003040
Total gradient norm: 0.012608
=== Actor Training Debug (Iteration 1450) ===
Q mean: -7.802609
Q std: 8.859114
Actor loss: 7.806597
Action reg: 0.003988
  l1.weight: grad_norm = 0.010238
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.007683
Total gradient norm: 0.038488
=== Actor Training Debug (Iteration 1451) ===
Q mean: -6.474391
Q std: 7.787453
Actor loss: 6.478374
Action reg: 0.003984
  l1.weight: grad_norm = 0.023321
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.017413
Total gradient norm: 0.078089
=== Actor Training Debug (Iteration 1452) ===
Q mean: -6.776405
Q std: 7.747260
Actor loss: 6.780394
Action reg: 0.003989
  l1.weight: grad_norm = 0.005771
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.004691
Total gradient norm: 0.017299
=== Actor Training Debug (Iteration 1453) ===
Q mean: -7.151362
Q std: 8.563423
Actor loss: 7.155351
Action reg: 0.003989
  l1.weight: grad_norm = 0.009103
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.008959
Total gradient norm: 0.047537
=== Actor Training Debug (Iteration 1454) ===
Q mean: -6.397021
Q std: 7.914587
Actor loss: 6.401006
Action reg: 0.003985
  l1.weight: grad_norm = 0.023443
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.023227
Total gradient norm: 0.093816
=== Actor Training Debug (Iteration 1455) ===
Q mean: -7.406992
Q std: 8.538950
Actor loss: 7.410978
Action reg: 0.003987
  l1.weight: grad_norm = 0.015607
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.015026
Total gradient norm: 0.081563
=== Actor Training Debug (Iteration 1456) ===
Q mean: -8.563187
Q std: 9.529967
Actor loss: 8.567180
Action reg: 0.003993
  l1.weight: grad_norm = 0.005827
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.004443
Total gradient norm: 0.016780
=== Actor Training Debug (Iteration 1457) ===
Q mean: -7.608703
Q std: 8.796140
Actor loss: 7.612694
Action reg: 0.003991
  l1.weight: grad_norm = 0.007362
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005862
Total gradient norm: 0.028415
=== Actor Training Debug (Iteration 1458) ===
Q mean: -7.252390
Q std: 9.068255
Actor loss: 7.256381
Action reg: 0.003991
  l1.weight: grad_norm = 0.008556
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.006424
Total gradient norm: 0.029201
=== Actor Training Debug (Iteration 1459) ===
Q mean: -6.394948
Q std: 8.259760
Actor loss: 6.398942
Action reg: 0.003994
  l1.weight: grad_norm = 0.006843
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005376
Total gradient norm: 0.023033
=== Actor Training Debug (Iteration 1460) ===
Q mean: -7.071089
Q std: 8.330180
Actor loss: 7.075080
Action reg: 0.003992
  l1.weight: grad_norm = 0.006844
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.006641
Total gradient norm: 0.025274
=== Actor Training Debug (Iteration 1461) ===
Q mean: -8.506427
Q std: 8.858454
Actor loss: 8.510417
Action reg: 0.003990
  l1.weight: grad_norm = 0.012095
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.011941
Total gradient norm: 0.030305
=== Actor Training Debug (Iteration 1462) ===
Q mean: -6.984964
Q std: 8.346250
Actor loss: 6.988956
Action reg: 0.003992
  l1.weight: grad_norm = 0.003275
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002835
Total gradient norm: 0.013547
=== Actor Training Debug (Iteration 1463) ===
Q mean: -7.766369
Q std: 8.612786
Actor loss: 7.770362
Action reg: 0.003993
  l1.weight: grad_norm = 0.009376
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.007747
Total gradient norm: 0.047200
=== Actor Training Debug (Iteration 1464) ===
Q mean: -6.546931
Q std: 8.249817
Actor loss: 6.550923
Action reg: 0.003992
  l1.weight: grad_norm = 0.014757
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.014587
Total gradient norm: 0.038924
=== Actor Training Debug (Iteration 1465) ===
Q mean: -6.569619
Q std: 8.674698
Actor loss: 6.573610
Action reg: 0.003991
  l1.weight: grad_norm = 0.009412
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.008942
Total gradient norm: 0.034490
=== Actor Training Debug (Iteration 1466) ===
Q mean: -7.019388
Q std: 8.710111
Actor loss: 7.023383
Action reg: 0.003994
  l1.weight: grad_norm = 0.006515
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.006428
Total gradient norm: 0.025431
=== Actor Training Debug (Iteration 1467) ===
Q mean: -7.486059
Q std: 8.640752
Actor loss: 7.490043
Action reg: 0.003984
  l1.weight: grad_norm = 0.020534
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.021327
Total gradient norm: 0.062867
=== Actor Training Debug (Iteration 1468) ===
Q mean: -6.978309
Q std: 8.662932
Actor loss: 6.982295
Action reg: 0.003985
  l1.weight: grad_norm = 0.018325
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.014609
Total gradient norm: 0.069243
=== Actor Training Debug (Iteration 1469) ===
Q mean: -6.796553
Q std: 8.106750
Actor loss: 6.800542
Action reg: 0.003990
  l1.weight: grad_norm = 0.004511
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.004034
Total gradient norm: 0.013235
=== Actor Training Debug (Iteration 1470) ===
Q mean: -6.446071
Q std: 8.312244
Actor loss: 6.450061
Action reg: 0.003990
  l1.weight: grad_norm = 0.004215
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.003272
Total gradient norm: 0.016661
=== Actor Training Debug (Iteration 1471) ===
Q mean: -6.520250
Q std: 7.980707
Actor loss: 6.524239
Action reg: 0.003989
  l1.weight: grad_norm = 0.015161
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.013520
Total gradient norm: 0.049341
=== Actor Training Debug (Iteration 1472) ===
Q mean: -7.292983
Q std: 8.505085
Actor loss: 7.296975
Action reg: 0.003992
  l1.weight: grad_norm = 0.012734
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.012198
Total gradient norm: 0.041003
=== Actor Training Debug (Iteration 1473) ===
Q mean: -6.226146
Q std: 7.604560
Actor loss: 6.230134
Action reg: 0.003988
  l1.weight: grad_norm = 0.007206
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.006157
Total gradient norm: 0.030029
=== Actor Training Debug (Iteration 1474) ===
Q mean: -7.883503
Q std: 8.518740
Actor loss: 7.887494
Action reg: 0.003991
  l1.weight: grad_norm = 0.024614
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.018412
Total gradient norm: 0.081533
=== Actor Training Debug (Iteration 1475) ===
Q mean: -6.857106
Q std: 8.514524
Actor loss: 6.861094
Action reg: 0.003989
  l1.weight: grad_norm = 0.016451
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.014658
Total gradient norm: 0.080244
=== Actor Training Debug (Iteration 1476) ===
Q mean: -6.825383
Q std: 8.610366
Actor loss: 6.829377
Action reg: 0.003994
  l1.weight: grad_norm = 0.004659
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.003661
Total gradient norm: 0.013044
=== Actor Training Debug (Iteration 1477) ===
Q mean: -6.370624
Q std: 8.447787
Actor loss: 6.374611
Action reg: 0.003987
  l1.weight: grad_norm = 0.053990
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.053263
Total gradient norm: 0.185685
=== Actor Training Debug (Iteration 1478) ===
Q mean: -7.581223
Q std: 8.663278
Actor loss: 7.585210
Action reg: 0.003987
  l1.weight: grad_norm = 0.008373
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.007507
Total gradient norm: 0.034676
=== Actor Training Debug (Iteration 1479) ===
Q mean: -6.912936
Q std: 8.519527
Actor loss: 6.916925
Action reg: 0.003990
  l1.weight: grad_norm = 0.007081
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005389
Total gradient norm: 0.022479
=== Actor Training Debug (Iteration 1480) ===
Q mean: -6.823617
Q std: 7.894465
Actor loss: 6.827613
Action reg: 0.003995
  l1.weight: grad_norm = 0.003187
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002593
Total gradient norm: 0.015743
=== Actor Training Debug (Iteration 1481) ===
Q mean: -6.927507
Q std: 8.451333
Actor loss: 6.931497
Action reg: 0.003990
  l1.weight: grad_norm = 0.011865
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.009102
Total gradient norm: 0.040146
=== Actor Training Debug (Iteration 1482) ===
Q mean: -6.939689
Q std: 8.217172
Actor loss: 6.943684
Action reg: 0.003995
  l1.weight: grad_norm = 0.001836
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001463
Total gradient norm: 0.005081
=== Actor Training Debug (Iteration 1483) ===
Q mean: -6.722068
Q std: 8.213381
Actor loss: 6.726064
Action reg: 0.003996
  l1.weight: grad_norm = 0.002538
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.001903
Total gradient norm: 0.011066
=== Actor Training Debug (Iteration 1484) ===
Q mean: -6.593861
Q std: 8.435304
Actor loss: 6.597855
Action reg: 0.003994
  l1.weight: grad_norm = 0.000983
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.000799
Total gradient norm: 0.003478
=== Actor Training Debug (Iteration 1485) ===
Q mean: -6.567047
Q std: 8.225098
Actor loss: 6.571038
Action reg: 0.003991
  l1.weight: grad_norm = 0.011504
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.009679
Total gradient norm: 0.051797
=== Actor Training Debug (Iteration 1486) ===
Q mean: -7.075865
Q std: 8.267443
Actor loss: 7.079863
Action reg: 0.003998
  l1.weight: grad_norm = 0.000250
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000229
Total gradient norm: 0.000795
=== Actor Training Debug (Iteration 1487) ===
Q mean: -7.481174
Q std: 8.321688
Actor loss: 7.485172
Action reg: 0.003998
  l1.weight: grad_norm = 0.003662
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002842
Total gradient norm: 0.017516
=== Actor Training Debug (Iteration 1488) ===
Q mean: -7.095544
Q std: 8.376060
Actor loss: 7.099529
Action reg: 0.003985
  l1.weight: grad_norm = 0.033863
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.032422
Total gradient norm: 0.093391
=== Actor Training Debug (Iteration 1489) ===
Q mean: -6.426034
Q std: 8.152569
Actor loss: 6.430024
Action reg: 0.003990
  l1.weight: grad_norm = 0.014772
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.013879
Total gradient norm: 0.050386
=== Actor Training Debug (Iteration 1490) ===
Q mean: -7.149573
Q std: 8.603609
Actor loss: 7.153564
Action reg: 0.003991
  l1.weight: grad_norm = 0.007461
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.006094
Total gradient norm: 0.033575
=== Actor Training Debug (Iteration 1491) ===
Q mean: -6.493562
Q std: 8.283164
Actor loss: 6.497555
Action reg: 0.003993
  l1.weight: grad_norm = 0.020482
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.017919
Total gradient norm: 0.059622
=== Actor Training Debug (Iteration 1492) ===
Q mean: -6.795806
Q std: 8.346095
Actor loss: 6.799792
Action reg: 0.003986
  l1.weight: grad_norm = 0.009128
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.007417
Total gradient norm: 0.043246
=== Actor Training Debug (Iteration 1493) ===
Q mean: -7.070890
Q std: 8.478241
Actor loss: 7.074879
Action reg: 0.003988
  l1.weight: grad_norm = 0.015939
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.012004
Total gradient norm: 0.059439
=== Actor Training Debug (Iteration 1494) ===
Q mean: -6.964624
Q std: 8.304029
Actor loss: 6.968621
Action reg: 0.003997
  l1.weight: grad_norm = 0.005916
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004225
Total gradient norm: 0.024812
=== Actor Training Debug (Iteration 1495) ===
Q mean: -7.365274
Q std: 8.354927
Actor loss: 7.369266
Action reg: 0.003991
  l1.weight: grad_norm = 0.013129
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.010729
Total gradient norm: 0.042314
=== Actor Training Debug (Iteration 1496) ===
Q mean: -6.177570
Q std: 7.987072
Actor loss: 6.181561
Action reg: 0.003990
  l1.weight: grad_norm = 0.022981
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.019856
Total gradient norm: 0.051183
=== Actor Training Debug (Iteration 1497) ===
Q mean: -7.012779
Q std: 8.185867
Actor loss: 7.016774
Action reg: 0.003995
  l1.weight: grad_norm = 0.006229
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.005836
Total gradient norm: 0.018288
=== Actor Training Debug (Iteration 1498) ===
Q mean: -6.729854
Q std: 7.999788
Actor loss: 6.733849
Action reg: 0.003994
  l1.weight: grad_norm = 0.001441
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.001118
Total gradient norm: 0.004559
=== Actor Training Debug (Iteration 1499) ===
Q mean: -7.966395
Q std: 9.061577
Actor loss: 7.970390
Action reg: 0.003995
  l1.weight: grad_norm = 0.003164
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002667
Total gradient norm: 0.013293
=== Actor Training Debug (Iteration 1500) ===
Q mean: -8.390565
Q std: 9.804567
Actor loss: 8.394558
Action reg: 0.003993
  l1.weight: grad_norm = 0.004587
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.003703
Total gradient norm: 0.014755
  Average reward: -362.779 | Average length: 100.0
Evaluation at episode 65: -362.779
=== Actor Training Debug (Iteration 1501) ===
Q mean: -8.358517
Q std: 9.474540
Actor loss: 8.362510
Action reg: 0.003993
  l1.weight: grad_norm = 0.011134
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.010833
Total gradient norm: 0.045316
=== Actor Training Debug (Iteration 1502) ===
Q mean: -6.755149
Q std: 8.663952
Actor loss: 6.759142
Action reg: 0.003992
  l1.weight: grad_norm = 0.013759
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.013583
Total gradient norm: 0.036160
=== Actor Training Debug (Iteration 1503) ===
Q mean: -6.737971
Q std: 8.710111
Actor loss: 6.741963
Action reg: 0.003992
  l1.weight: grad_norm = 0.002249
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.001605
Total gradient norm: 0.005021
=== Actor Training Debug (Iteration 1504) ===
Q mean: -5.932273
Q std: 7.500765
Actor loss: 5.936264
Action reg: 0.003990
  l1.weight: grad_norm = 0.020256
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.018772
Total gradient norm: 0.074852
=== Actor Training Debug (Iteration 1505) ===
Q mean: -6.671982
Q std: 8.399361
Actor loss: 6.675975
Action reg: 0.003993
  l1.weight: grad_norm = 0.011389
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.008745
Total gradient norm: 0.039747
=== Actor Training Debug (Iteration 1506) ===
Q mean: -7.583360
Q std: 8.999791
Actor loss: 7.587357
Action reg: 0.003998
  l1.weight: grad_norm = 0.001019
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000694
Total gradient norm: 0.002743
=== Actor Training Debug (Iteration 1507) ===
Q mean: -7.727830
Q std: 9.130262
Actor loss: 7.731827
Action reg: 0.003997
  l1.weight: grad_norm = 0.006234
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.005891
Total gradient norm: 0.017624
=== Actor Training Debug (Iteration 1508) ===
Q mean: -6.898355
Q std: 8.571569
Actor loss: 6.902348
Action reg: 0.003994
  l1.weight: grad_norm = 0.006258
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004754
Total gradient norm: 0.021081
=== Actor Training Debug (Iteration 1509) ===
Q mean: -6.887536
Q std: 8.622011
Actor loss: 6.891527
Action reg: 0.003991
  l1.weight: grad_norm = 0.003727
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.003517
Total gradient norm: 0.011714
=== Actor Training Debug (Iteration 1510) ===
Q mean: -7.335857
Q std: 8.806159
Actor loss: 7.339846
Action reg: 0.003989
  l1.weight: grad_norm = 0.012207
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.009327
Total gradient norm: 0.035974
=== Actor Training Debug (Iteration 1511) ===
Q mean: -7.400387
Q std: 8.468870
Actor loss: 7.404375
Action reg: 0.003987
  l1.weight: grad_norm = 0.027191
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.021779
Total gradient norm: 0.096274
=== Actor Training Debug (Iteration 1512) ===
Q mean: -7.513958
Q std: 8.600983
Actor loss: 7.517948
Action reg: 0.003990
  l1.weight: grad_norm = 0.008830
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.006307
Total gradient norm: 0.026199
=== Actor Training Debug (Iteration 1513) ===
Q mean: -7.475227
Q std: 8.765352
Actor loss: 7.479220
Action reg: 0.003993
  l1.weight: grad_norm = 0.019199
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.017540
Total gradient norm: 0.097371
=== Actor Training Debug (Iteration 1514) ===
Q mean: -7.224697
Q std: 8.922409
Actor loss: 7.228691
Action reg: 0.003994
  l1.weight: grad_norm = 0.010228
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.008241
Total gradient norm: 0.040393
=== Actor Training Debug (Iteration 1515) ===
Q mean: -7.496338
Q std: 8.811626
Actor loss: 7.500332
Action reg: 0.003994
  l1.weight: grad_norm = 0.009363
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.007655
Total gradient norm: 0.034441
=== Actor Training Debug (Iteration 1516) ===
Q mean: -7.704630
Q std: 9.146444
Actor loss: 7.708621
Action reg: 0.003990
  l1.weight: grad_norm = 0.004484
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.003674
Total gradient norm: 0.015887
=== Actor Training Debug (Iteration 1517) ===
Q mean: -7.240814
Q std: 8.821822
Actor loss: 7.244805
Action reg: 0.003991
  l1.weight: grad_norm = 0.003662
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.003412
Total gradient norm: 0.018370
=== Actor Training Debug (Iteration 1518) ===
Q mean: -7.721410
Q std: 8.726839
Actor loss: 7.725396
Action reg: 0.003986
  l1.weight: grad_norm = 0.012288
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.011552
Total gradient norm: 0.070997
=== Actor Training Debug (Iteration 1519) ===
Q mean: -6.766206
Q std: 8.351120
Actor loss: 6.770201
Action reg: 0.003994
  l1.weight: grad_norm = 0.011983
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.008607
Total gradient norm: 0.041752
=== Actor Training Debug (Iteration 1520) ===
Q mean: -7.308128
Q std: 8.857906
Actor loss: 7.312119
Action reg: 0.003991
  l1.weight: grad_norm = 0.015612
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.014320
Total gradient norm: 0.041072
=== Actor Training Debug (Iteration 1521) ===
Q mean: -6.523228
Q std: 8.479953
Actor loss: 6.527218
Action reg: 0.003990
  l1.weight: grad_norm = 0.014206
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.010635
Total gradient norm: 0.033884
=== Actor Training Debug (Iteration 1522) ===
Q mean: -7.808640
Q std: 8.773660
Actor loss: 7.812638
Action reg: 0.003997
  l1.weight: grad_norm = 0.002462
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002200
Total gradient norm: 0.011258
=== Actor Training Debug (Iteration 1523) ===
Q mean: -7.044568
Q std: 8.593866
Actor loss: 7.048558
Action reg: 0.003990
  l1.weight: grad_norm = 0.018839
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.016865
Total gradient norm: 0.062713
=== Actor Training Debug (Iteration 1524) ===
Q mean: -7.711152
Q std: 8.903695
Actor loss: 7.715147
Action reg: 0.003996
  l1.weight: grad_norm = 0.005462
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004884
Total gradient norm: 0.017628
=== Actor Training Debug (Iteration 1525) ===
Q mean: -8.581955
Q std: 8.954997
Actor loss: 8.585950
Action reg: 0.003995
  l1.weight: grad_norm = 0.002970
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.002650
Total gradient norm: 0.010869
=== Actor Training Debug (Iteration 1526) ===
Q mean: -7.298613
Q std: 8.696969
Actor loss: 7.302609
Action reg: 0.003996
  l1.weight: grad_norm = 0.001647
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.001302
Total gradient norm: 0.006758
=== Actor Training Debug (Iteration 1527) ===
Q mean: -7.418566
Q std: 8.880117
Actor loss: 7.422561
Action reg: 0.003994
  l1.weight: grad_norm = 0.008567
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.007079
Total gradient norm: 0.025854
=== Actor Training Debug (Iteration 1528) ===
Q mean: -7.013996
Q std: 8.833210
Actor loss: 7.017994
Action reg: 0.003998
  l1.weight: grad_norm = 0.002113
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001664
Total gradient norm: 0.007758
=== Actor Training Debug (Iteration 1529) ===
Q mean: -7.720855
Q std: 8.871887
Actor loss: 7.724847
Action reg: 0.003992
  l1.weight: grad_norm = 0.015075
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.012437
Total gradient norm: 0.054405
=== Actor Training Debug (Iteration 1530) ===
Q mean: -7.071633
Q std: 8.835800
Actor loss: 7.075629
Action reg: 0.003996
  l1.weight: grad_norm = 0.004927
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004143
Total gradient norm: 0.021710
=== Actor Training Debug (Iteration 1531) ===
Q mean: -7.905755
Q std: 8.776846
Actor loss: 7.909749
Action reg: 0.003995
  l1.weight: grad_norm = 0.014045
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.012127
Total gradient norm: 0.036359
=== Actor Training Debug (Iteration 1532) ===
Q mean: -8.015429
Q std: 8.939891
Actor loss: 8.019423
Action reg: 0.003995
  l1.weight: grad_norm = 0.008852
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.006984
Total gradient norm: 0.031010
=== Actor Training Debug (Iteration 1533) ===
Q mean: -6.653736
Q std: 7.837605
Actor loss: 6.657729
Action reg: 0.003993
  l1.weight: grad_norm = 0.013368
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.010691
Total gradient norm: 0.051168
=== Actor Training Debug (Iteration 1534) ===
Q mean: -7.664742
Q std: 9.360859
Actor loss: 7.668735
Action reg: 0.003993
  l1.weight: grad_norm = 0.004779
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.005066
Total gradient norm: 0.029562
=== Actor Training Debug (Iteration 1535) ===
Q mean: -7.343646
Q std: 8.903583
Actor loss: 7.347640
Action reg: 0.003994
  l1.weight: grad_norm = 0.003945
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.003524
Total gradient norm: 0.021353
=== Actor Training Debug (Iteration 1536) ===
Q mean: -7.208400
Q std: 8.894857
Actor loss: 7.212394
Action reg: 0.003994
  l1.weight: grad_norm = 0.018782
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.014188
Total gradient norm: 0.060395
=== Actor Training Debug (Iteration 1537) ===
Q mean: -8.015856
Q std: 8.987478
Actor loss: 8.019851
Action reg: 0.003995
  l1.weight: grad_norm = 0.006846
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005493
Total gradient norm: 0.028396
=== Actor Training Debug (Iteration 1538) ===
Q mean: -8.025470
Q std: 9.159330
Actor loss: 8.029467
Action reg: 0.003997
  l1.weight: grad_norm = 0.005958
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.004520
Total gradient norm: 0.018675
=== Actor Training Debug (Iteration 1539) ===
Q mean: -7.538040
Q std: 9.103990
Actor loss: 7.542035
Action reg: 0.003995
  l1.weight: grad_norm = 0.009414
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.008810
Total gradient norm: 0.022243
=== Actor Training Debug (Iteration 1540) ===
Q mean: -6.537218
Q std: 8.522807
Actor loss: 6.541207
Action reg: 0.003989
  l1.weight: grad_norm = 0.019215
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.016776
Total gradient norm: 0.086897
=== Actor Training Debug (Iteration 1541) ===
Q mean: -7.577493
Q std: 9.318836
Actor loss: 7.581485
Action reg: 0.003992
  l1.weight: grad_norm = 0.005733
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005167
Total gradient norm: 0.016868
=== Actor Training Debug (Iteration 1542) ===
Q mean: -6.446769
Q std: 8.224693
Actor loss: 6.450761
Action reg: 0.003993
  l1.weight: grad_norm = 0.033549
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.028121
Total gradient norm: 0.092688
=== Actor Training Debug (Iteration 1543) ===
Q mean: -7.576047
Q std: 8.538721
Actor loss: 7.580042
Action reg: 0.003995
  l1.weight: grad_norm = 0.011702
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.008908
Total gradient norm: 0.053367
=== Actor Training Debug (Iteration 1544) ===
Q mean: -7.108045
Q std: 8.736158
Actor loss: 7.112032
Action reg: 0.003988
  l1.weight: grad_norm = 0.013828
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.013144
Total gradient norm: 0.044160
=== Actor Training Debug (Iteration 1545) ===
Q mean: -8.124081
Q std: 9.196828
Actor loss: 8.128074
Action reg: 0.003993
  l1.weight: grad_norm = 0.008526
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.007745
Total gradient norm: 0.025074
=== Actor Training Debug (Iteration 1546) ===
Q mean: -7.430696
Q std: 9.350013
Actor loss: 7.434689
Action reg: 0.003993
  l1.weight: grad_norm = 0.026382
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.020082
Total gradient norm: 0.093224
=== Actor Training Debug (Iteration 1547) ===
Q mean: -7.975940
Q std: 9.335632
Actor loss: 7.979936
Action reg: 0.003996
  l1.weight: grad_norm = 0.003513
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002944
Total gradient norm: 0.011202
=== Actor Training Debug (Iteration 1548) ===
Q mean: -8.566750
Q std: 9.709162
Actor loss: 8.570737
Action reg: 0.003988
  l1.weight: grad_norm = 0.009383
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.007050
Total gradient norm: 0.023471
=== Actor Training Debug (Iteration 1549) ===
Q mean: -6.426506
Q std: 8.435083
Actor loss: 6.430495
Action reg: 0.003989
  l1.weight: grad_norm = 0.015789
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.014350
Total gradient norm: 0.045057
=== Actor Training Debug (Iteration 1550) ===
Q mean: -7.852524
Q std: 9.237603
Actor loss: 7.856513
Action reg: 0.003989
  l1.weight: grad_norm = 0.024805
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.021841
Total gradient norm: 0.112566
=== Actor Training Debug (Iteration 1551) ===
Q mean: -7.333247
Q std: 8.585075
Actor loss: 7.337238
Action reg: 0.003991
  l1.weight: grad_norm = 0.011205
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.008389
Total gradient norm: 0.038376
=== Actor Training Debug (Iteration 1552) ===
Q mean: -6.872256
Q std: 8.658362
Actor loss: 6.876255
Action reg: 0.003999
  l1.weight: grad_norm = 0.010939
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.009694
Total gradient norm: 0.032592
=== Actor Training Debug (Iteration 1553) ===
Q mean: -7.121576
Q std: 8.946261
Actor loss: 7.125569
Action reg: 0.003993
  l1.weight: grad_norm = 0.008975
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.007643
Total gradient norm: 0.043431
=== Actor Training Debug (Iteration 1554) ===
Q mean: -6.999009
Q std: 8.527980
Actor loss: 7.002997
Action reg: 0.003988
  l1.weight: grad_norm = 0.012618
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.010543
Total gradient norm: 0.047460
=== Actor Training Debug (Iteration 1555) ===
Q mean: -7.899863
Q std: 8.743958
Actor loss: 7.903856
Action reg: 0.003994
  l1.weight: grad_norm = 0.008002
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.007236
Total gradient norm: 0.046486
=== Actor Training Debug (Iteration 1556) ===
Q mean: -6.985107
Q std: 8.044930
Actor loss: 6.989095
Action reg: 0.003987
  l1.weight: grad_norm = 0.029826
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.024739
Total gradient norm: 0.093605
=== Actor Training Debug (Iteration 1557) ===
Q mean: -7.915591
Q std: 9.231198
Actor loss: 7.919581
Action reg: 0.003990
  l1.weight: grad_norm = 0.001954
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.001523
Total gradient norm: 0.006639
=== Actor Training Debug (Iteration 1558) ===
Q mean: -7.575967
Q std: 8.729814
Actor loss: 7.579960
Action reg: 0.003993
  l1.weight: grad_norm = 0.006449
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.005274
Total gradient norm: 0.022745
=== Actor Training Debug (Iteration 1559) ===
Q mean: -6.812371
Q std: 8.562190
Actor loss: 6.816360
Action reg: 0.003989
  l1.weight: grad_norm = 0.002439
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001829
Total gradient norm: 0.011080
=== Actor Training Debug (Iteration 1560) ===
Q mean: -6.751260
Q std: 8.428393
Actor loss: 6.755250
Action reg: 0.003989
  l1.weight: grad_norm = 0.012296
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.009359
Total gradient norm: 0.049995
=== Actor Training Debug (Iteration 1561) ===
Q mean: -8.085732
Q std: 9.421902
Actor loss: 8.089722
Action reg: 0.003990
  l1.weight: grad_norm = 0.009129
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.007283
Total gradient norm: 0.041503
=== Actor Training Debug (Iteration 1562) ===
Q mean: -8.015464
Q std: 9.110070
Actor loss: 8.019459
Action reg: 0.003995
  l1.weight: grad_norm = 0.009190
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.006298
Total gradient norm: 0.021192
=== Actor Training Debug (Iteration 1563) ===
Q mean: -7.239017
Q std: 8.788289
Actor loss: 7.243008
Action reg: 0.003991
  l1.weight: grad_norm = 0.014821
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.012017
Total gradient norm: 0.048080
=== Actor Training Debug (Iteration 1564) ===
Q mean: -7.502179
Q std: 8.849840
Actor loss: 7.506170
Action reg: 0.003992
  l1.weight: grad_norm = 0.004545
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.003330
Total gradient norm: 0.012378
=== Actor Training Debug (Iteration 1565) ===
Q mean: -7.883536
Q std: 9.346735
Actor loss: 7.887527
Action reg: 0.003991
  l1.weight: grad_norm = 0.011807
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.011172
Total gradient norm: 0.030873
=== Actor Training Debug (Iteration 1566) ===
Q mean: -7.762731
Q std: 9.060117
Actor loss: 7.766727
Action reg: 0.003996
  l1.weight: grad_norm = 0.005480
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.004548
Total gradient norm: 0.023338
=== Actor Training Debug (Iteration 1567) ===
Q mean: -6.689516
Q std: 8.460989
Actor loss: 6.693511
Action reg: 0.003995
  l1.weight: grad_norm = 0.014203
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.011643
Total gradient norm: 0.039332
=== Actor Training Debug (Iteration 1568) ===
Q mean: -6.802783
Q std: 8.448267
Actor loss: 6.806775
Action reg: 0.003991
  l1.weight: grad_norm = 0.025820
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.023973
Total gradient norm: 0.079904
=== Actor Training Debug (Iteration 1569) ===
Q mean: -7.545065
Q std: 8.765529
Actor loss: 7.549061
Action reg: 0.003996
  l1.weight: grad_norm = 0.005633
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.003795
Total gradient norm: 0.018105
=== Actor Training Debug (Iteration 1570) ===
Q mean: -7.576336
Q std: 8.906897
Actor loss: 7.580332
Action reg: 0.003996
  l1.weight: grad_norm = 0.012983
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.009632
Total gradient norm: 0.057814
=== Actor Training Debug (Iteration 1571) ===
Q mean: -8.464233
Q std: 9.089561
Actor loss: 8.468226
Action reg: 0.003993
  l1.weight: grad_norm = 0.007741
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.006298
Total gradient norm: 0.035200
=== Actor Training Debug (Iteration 1572) ===
Q mean: -7.205299
Q std: 8.438656
Actor loss: 7.209286
Action reg: 0.003987
  l1.weight: grad_norm = 0.021481
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.017797
Total gradient norm: 0.069444
=== Actor Training Debug (Iteration 1573) ===
Q mean: -6.091194
Q std: 8.388510
Actor loss: 6.095186
Action reg: 0.003991
  l1.weight: grad_norm = 0.016876
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.014977
Total gradient norm: 0.060437
=== Actor Training Debug (Iteration 1574) ===
Q mean: -6.920168
Q std: 8.960388
Actor loss: 6.924164
Action reg: 0.003996
  l1.weight: grad_norm = 0.000545
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.000509
Total gradient norm: 0.001731
=== Actor Training Debug (Iteration 1575) ===
Q mean: -6.620720
Q std: 8.287077
Actor loss: 6.624713
Action reg: 0.003993
  l1.weight: grad_norm = 0.000882
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.000685
Total gradient norm: 0.002990
=== Actor Training Debug (Iteration 1576) ===
Q mean: -7.407865
Q std: 8.877414
Actor loss: 7.411856
Action reg: 0.003991
  l1.weight: grad_norm = 0.018645
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.016912
Total gradient norm: 0.070153
=== Actor Training Debug (Iteration 1577) ===
Q mean: -8.042520
Q std: 9.373421
Actor loss: 8.046508
Action reg: 0.003988
  l1.weight: grad_norm = 0.016532
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.014731
Total gradient norm: 0.061927
=== Actor Training Debug (Iteration 1578) ===
Q mean: -7.483251
Q std: 8.829887
Actor loss: 7.487245
Action reg: 0.003995
  l1.weight: grad_norm = 0.009578
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.007737
Total gradient norm: 0.030324
=== Actor Training Debug (Iteration 1579) ===
Q mean: -7.844626
Q std: 9.414150
Actor loss: 7.848619
Action reg: 0.003993
  l1.weight: grad_norm = 0.009812
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.009676
Total gradient norm: 0.026386
=== Actor Training Debug (Iteration 1580) ===
Q mean: -7.133626
Q std: 9.184267
Actor loss: 7.137618
Action reg: 0.003992
  l1.weight: grad_norm = 0.011308
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.008876
Total gradient norm: 0.031231
=== Actor Training Debug (Iteration 1581) ===
Q mean: -7.809198
Q std: 9.097847
Actor loss: 7.813190
Action reg: 0.003992
  l1.weight: grad_norm = 0.007596
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.006185
Total gradient norm: 0.021201
=== Actor Training Debug (Iteration 1582) ===
Q mean: -7.848368
Q std: 9.121349
Actor loss: 7.852360
Action reg: 0.003992
  l1.weight: grad_norm = 0.008289
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.006185
Total gradient norm: 0.026478
=== Actor Training Debug (Iteration 1583) ===
Q mean: -8.265069
Q std: 9.142978
Actor loss: 8.269064
Action reg: 0.003995
  l1.weight: grad_norm = 0.023240
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.020553
Total gradient norm: 0.058413
=== Actor Training Debug (Iteration 1584) ===
Q mean: -6.049182
Q std: 8.274647
Actor loss: 6.053179
Action reg: 0.003997
  l1.weight: grad_norm = 0.002591
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002205
Total gradient norm: 0.012622
=== Actor Training Debug (Iteration 1585) ===
Q mean: -7.244292
Q std: 8.704881
Actor loss: 7.248289
Action reg: 0.003997
  l1.weight: grad_norm = 0.001801
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001666
Total gradient norm: 0.007462
=== Actor Training Debug (Iteration 1586) ===
Q mean: -7.317628
Q std: 8.891608
Actor loss: 7.321622
Action reg: 0.003994
  l1.weight: grad_norm = 0.004791
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.004362
Total gradient norm: 0.018034
=== Actor Training Debug (Iteration 1587) ===
Q mean: -7.559855
Q std: 8.720354
Actor loss: 7.563849
Action reg: 0.003994
  l1.weight: grad_norm = 0.001344
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.000943
Total gradient norm: 0.003904
=== Actor Training Debug (Iteration 1588) ===
Q mean: -7.803740
Q std: 9.257074
Actor loss: 7.807724
Action reg: 0.003984
  l1.weight: grad_norm = 0.013733
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.012068
Total gradient norm: 0.055066
=== Actor Training Debug (Iteration 1589) ===
Q mean: -7.245961
Q std: 9.110756
Actor loss: 7.249956
Action reg: 0.003995
  l1.weight: grad_norm = 0.016755
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.015543
Total gradient norm: 0.049955
=== Actor Training Debug (Iteration 1590) ===
Q mean: -6.786747
Q std: 9.025104
Actor loss: 6.790739
Action reg: 0.003992
  l1.weight: grad_norm = 0.028464
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.027307
Total gradient norm: 0.154273
=== Actor Training Debug (Iteration 1591) ===
Q mean: -8.893821
Q std: 10.192638
Actor loss: 8.897819
Action reg: 0.003998
  l1.weight: grad_norm = 0.001667
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001434
Total gradient norm: 0.006879
=== Actor Training Debug (Iteration 1592) ===
Q mean: -9.589878
Q std: 10.360959
Actor loss: 9.593872
Action reg: 0.003994
  l1.weight: grad_norm = 0.022442
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.019015
Total gradient norm: 0.080728
=== Actor Training Debug (Iteration 1593) ===
Q mean: -7.188104
Q std: 8.904027
Actor loss: 7.192093
Action reg: 0.003990
  l1.weight: grad_norm = 0.016855
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.015880
Total gradient norm: 0.069453
=== Actor Training Debug (Iteration 1594) ===
Q mean: -7.360296
Q std: 9.109024
Actor loss: 7.364290
Action reg: 0.003994
  l1.weight: grad_norm = 0.003575
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003391
Total gradient norm: 0.011003
=== Actor Training Debug (Iteration 1595) ===
Q mean: -8.068886
Q std: 9.279913
Actor loss: 8.072879
Action reg: 0.003993
  l1.weight: grad_norm = 0.012152
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.008955
Total gradient norm: 0.044782
=== Actor Training Debug (Iteration 1596) ===
Q mean: -7.719300
Q std: 9.164884
Actor loss: 7.723290
Action reg: 0.003990
  l1.weight: grad_norm = 0.004223
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003319
Total gradient norm: 0.013468
=== Actor Training Debug (Iteration 1597) ===
Q mean: -6.702326
Q std: 8.794797
Actor loss: 6.706315
Action reg: 0.003989
  l1.weight: grad_norm = 0.012914
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.012249
Total gradient norm: 0.039479
=== Actor Training Debug (Iteration 1598) ===
Q mean: -7.031939
Q std: 9.383491
Actor loss: 7.035933
Action reg: 0.003994
  l1.weight: grad_norm = 0.002825
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.002836
Total gradient norm: 0.016728
=== Actor Training Debug (Iteration 1599) ===
Q mean: -7.072949
Q std: 9.033817
Actor loss: 7.076947
Action reg: 0.003998
  l1.weight: grad_norm = 0.006810
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005620
Total gradient norm: 0.025176
=== Actor Training Debug (Iteration 1600) ===
Q mean: -8.124020
Q std: 9.382976
Actor loss: 8.128015
Action reg: 0.003995
  l1.weight: grad_norm = 0.007457
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.006743
Total gradient norm: 0.027813
=== Actor Training Debug (Iteration 1601) ===
Q mean: -8.114479
Q std: 8.797953
Actor loss: 8.118471
Action reg: 0.003992
  l1.weight: grad_norm = 0.005473
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003817
Total gradient norm: 0.016378
=== Actor Training Debug (Iteration 1602) ===
Q mean: -6.705728
Q std: 8.346071
Actor loss: 6.709718
Action reg: 0.003990
  l1.weight: grad_norm = 0.012867
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.010268
Total gradient norm: 0.050380
=== Actor Training Debug (Iteration 1603) ===
Q mean: -8.014165
Q std: 9.350335
Actor loss: 8.018163
Action reg: 0.003998
  l1.weight: grad_norm = 0.000472
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000425
Total gradient norm: 0.001650
=== Actor Training Debug (Iteration 1604) ===
Q mean: -8.365532
Q std: 9.332074
Actor loss: 8.369528
Action reg: 0.003996
  l1.weight: grad_norm = 0.008543
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.007427
Total gradient norm: 0.030168
=== Actor Training Debug (Iteration 1605) ===
Q mean: -6.665791
Q std: 8.743150
Actor loss: 6.669779
Action reg: 0.003988
  l1.weight: grad_norm = 0.015299
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.011135
Total gradient norm: 0.056239
=== Actor Training Debug (Iteration 1606) ===
Q mean: -7.001493
Q std: 8.853483
Actor loss: 7.005490
Action reg: 0.003997
  l1.weight: grad_norm = 0.002803
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.002283
Total gradient norm: 0.010489
=== Actor Training Debug (Iteration 1607) ===
Q mean: -7.080705
Q std: 8.768435
Actor loss: 7.084696
Action reg: 0.003990
  l1.weight: grad_norm = 0.010078
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.007876
Total gradient norm: 0.032626
=== Actor Training Debug (Iteration 1608) ===
Q mean: -7.424850
Q std: 9.253036
Actor loss: 7.428843
Action reg: 0.003994
  l1.weight: grad_norm = 0.008076
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.006997
Total gradient norm: 0.041043
=== Actor Training Debug (Iteration 1609) ===
Q mean: -7.021942
Q std: 8.847075
Actor loss: 7.025937
Action reg: 0.003994
  l1.weight: grad_norm = 0.004842
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004089
Total gradient norm: 0.020485
=== Actor Training Debug (Iteration 1610) ===
Q mean: -7.527382
Q std: 9.469637
Actor loss: 7.531369
Action reg: 0.003987
  l1.weight: grad_norm = 0.020046
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.018421
Total gradient norm: 0.086621
=== Actor Training Debug (Iteration 1611) ===
Q mean: -7.307353
Q std: 8.807573
Actor loss: 7.311349
Action reg: 0.003996
  l1.weight: grad_norm = 0.003114
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002624
Total gradient norm: 0.009747
=== Actor Training Debug (Iteration 1612) ===
Q mean: -7.967394
Q std: 8.862131
Actor loss: 7.971388
Action reg: 0.003994
  l1.weight: grad_norm = 0.010810
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.009199
Total gradient norm: 0.039370
=== Actor Training Debug (Iteration 1613) ===
Q mean: -6.953117
Q std: 8.834832
Actor loss: 6.957101
Action reg: 0.003984
  l1.weight: grad_norm = 0.023712
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.021781
Total gradient norm: 0.074073
=== Actor Training Debug (Iteration 1614) ===
Q mean: -7.612862
Q std: 8.912048
Actor loss: 7.616851
Action reg: 0.003989
  l1.weight: grad_norm = 0.021807
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.018193
Total gradient norm: 0.088442
=== Actor Training Debug (Iteration 1615) ===
Q mean: -7.370596
Q std: 9.082921
Actor loss: 7.374586
Action reg: 0.003990
  l1.weight: grad_norm = 0.011753
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.009693
Total gradient norm: 0.034635
=== Actor Training Debug (Iteration 1616) ===
Q mean: -7.349806
Q std: 9.379486
Actor loss: 7.353795
Action reg: 0.003989
  l1.weight: grad_norm = 0.008935
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.007020
Total gradient norm: 0.044927
=== Actor Training Debug (Iteration 1617) ===
Q mean: -8.228362
Q std: 9.613987
Actor loss: 8.232356
Action reg: 0.003994
  l1.weight: grad_norm = 0.009993
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.007342
Total gradient norm: 0.034040
=== Actor Training Debug (Iteration 1618) ===
Q mean: -7.764616
Q std: 9.426402
Actor loss: 7.768610
Action reg: 0.003994
  l1.weight: grad_norm = 0.011646
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.008861
Total gradient norm: 0.052997
=== Actor Training Debug (Iteration 1619) ===
Q mean: -7.582190
Q std: 9.105971
Actor loss: 7.586185
Action reg: 0.003996
  l1.weight: grad_norm = 0.001825
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001730
Total gradient norm: 0.005060
=== Actor Training Debug (Iteration 1620) ===
Q mean: -6.859084
Q std: 8.931652
Actor loss: 6.863078
Action reg: 0.003994
  l1.weight: grad_norm = 0.013493
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.009225
Total gradient norm: 0.042087
=== Actor Training Debug (Iteration 1621) ===
Q mean: -6.777473
Q std: 8.481288
Actor loss: 6.781470
Action reg: 0.003997
  l1.weight: grad_norm = 0.004205
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.003800
Total gradient norm: 0.025961
=== Actor Training Debug (Iteration 1622) ===
Q mean: -6.929534
Q std: 8.942445
Actor loss: 6.933527
Action reg: 0.003992
  l1.weight: grad_norm = 0.004355
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.004013
Total gradient norm: 0.018785
=== Actor Training Debug (Iteration 1623) ===
Q mean: -6.944523
Q std: 9.325320
Actor loss: 6.948511
Action reg: 0.003988
  l1.weight: grad_norm = 0.014588
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.010342
Total gradient norm: 0.038603
=== Actor Training Debug (Iteration 1624) ===
Q mean: -8.137968
Q std: 9.322463
Actor loss: 8.141959
Action reg: 0.003991
  l1.weight: grad_norm = 0.014113
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.009320
Total gradient norm: 0.045809
=== Actor Training Debug (Iteration 1625) ===
Q mean: -7.976722
Q std: 9.311252
Actor loss: 7.980711
Action reg: 0.003988
  l1.weight: grad_norm = 0.004998
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004277
Total gradient norm: 0.018292
=== Actor Training Debug (Iteration 1626) ===
Q mean: -6.175483
Q std: 8.150957
Actor loss: 6.179478
Action reg: 0.003996
  l1.weight: grad_norm = 0.006511
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005561
Total gradient norm: 0.020037
=== Actor Training Debug (Iteration 1627) ===
Q mean: -7.702436
Q std: 9.509826
Actor loss: 7.706431
Action reg: 0.003995
  l1.weight: grad_norm = 0.004608
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003864
Total gradient norm: 0.016619
=== Actor Training Debug (Iteration 1628) ===
Q mean: -7.521502
Q std: 9.531260
Actor loss: 7.525496
Action reg: 0.003994
  l1.weight: grad_norm = 0.006767
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.005026
Total gradient norm: 0.019179
=== Actor Training Debug (Iteration 1629) ===
Q mean: -8.238744
Q std: 9.638472
Actor loss: 8.242737
Action reg: 0.003993
  l1.weight: grad_norm = 0.009772
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.008422
Total gradient norm: 0.028112
=== Actor Training Debug (Iteration 1630) ===
Q mean: -8.553670
Q std: 9.425714
Actor loss: 8.557667
Action reg: 0.003997
  l1.weight: grad_norm = 0.002898
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002170
Total gradient norm: 0.009974
=== Actor Training Debug (Iteration 1631) ===
Q mean: -8.072769
Q std: 9.299044
Actor loss: 8.076758
Action reg: 0.003989
  l1.weight: grad_norm = 0.011549
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.011183
Total gradient norm: 0.066500
=== Actor Training Debug (Iteration 1632) ===
Q mean: -8.430889
Q std: 9.630335
Actor loss: 8.434882
Action reg: 0.003993
  l1.weight: grad_norm = 0.009784
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.007638
Total gradient norm: 0.039157
=== Actor Training Debug (Iteration 1633) ===
Q mean: -7.728342
Q std: 9.282377
Actor loss: 7.732341
Action reg: 0.003999
  l1.weight: grad_norm = 0.000739
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000615
Total gradient norm: 0.003278
=== Actor Training Debug (Iteration 1634) ===
Q mean: -7.767739
Q std: 9.222581
Actor loss: 7.771736
Action reg: 0.003996
  l1.weight: grad_norm = 0.005523
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004847
Total gradient norm: 0.025424
=== Actor Training Debug (Iteration 1635) ===
Q mean: -7.542275
Q std: 9.382758
Actor loss: 7.546262
Action reg: 0.003987
  l1.weight: grad_norm = 0.029933
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.024149
Total gradient norm: 0.088130
=== Actor Training Debug (Iteration 1636) ===
Q mean: -6.707531
Q std: 8.600636
Actor loss: 6.711526
Action reg: 0.003996
  l1.weight: grad_norm = 0.002962
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002224
Total gradient norm: 0.009711
=== Actor Training Debug (Iteration 1637) ===
Q mean: -8.023014
Q std: 9.733032
Actor loss: 8.027006
Action reg: 0.003992
  l1.weight: grad_norm = 0.017509
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.017439
Total gradient norm: 0.059753
=== Actor Training Debug (Iteration 1638) ===
Q mean: -8.227924
Q std: 9.642377
Actor loss: 8.231913
Action reg: 0.003988
  l1.weight: grad_norm = 0.010566
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.009056
Total gradient norm: 0.045206
=== Actor Training Debug (Iteration 1639) ===
Q mean: -8.189563
Q std: 9.480976
Actor loss: 8.193558
Action reg: 0.003995
  l1.weight: grad_norm = 0.010960
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.007200
Total gradient norm: 0.028560
=== Actor Training Debug (Iteration 1640) ===
Q mean: -7.313756
Q std: 9.137207
Actor loss: 7.317750
Action reg: 0.003994
  l1.weight: grad_norm = 0.003890
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003183
Total gradient norm: 0.014789
=== Actor Training Debug (Iteration 1641) ===
Q mean: -8.210976
Q std: 9.260330
Actor loss: 8.214973
Action reg: 0.003998
  l1.weight: grad_norm = 0.002334
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001704
Total gradient norm: 0.007684
=== Actor Training Debug (Iteration 1642) ===
Q mean: -8.766990
Q std: 9.511907
Actor loss: 8.770987
Action reg: 0.003996
  l1.weight: grad_norm = 0.014730
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.014703
Total gradient norm: 0.052020
=== Actor Training Debug (Iteration 1643) ===
Q mean: -8.138703
Q std: 9.664358
Actor loss: 8.142691
Action reg: 0.003988
  l1.weight: grad_norm = 0.009251
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.006769
Total gradient norm: 0.021962
=== Actor Training Debug (Iteration 1644) ===
Q mean: -6.677218
Q std: 9.279613
Actor loss: 6.681211
Action reg: 0.003992
  l1.weight: grad_norm = 0.002112
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.001695
Total gradient norm: 0.006450
=== Actor Training Debug (Iteration 1645) ===
Q mean: -7.195818
Q std: 9.026231
Actor loss: 7.199816
Action reg: 0.003998
  l1.weight: grad_norm = 0.000586
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000472
Total gradient norm: 0.001680
=== Actor Training Debug (Iteration 1646) ===
Q mean: -9.093745
Q std: 10.319324
Actor loss: 9.097742
Action reg: 0.003996
  l1.weight: grad_norm = 0.004822
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.003345
Total gradient norm: 0.013822
=== Actor Training Debug (Iteration 1647) ===
Q mean: -8.819782
Q std: 9.630284
Actor loss: 8.823780
Action reg: 0.003998
  l1.weight: grad_norm = 0.000281
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.000242
Total gradient norm: 0.001085
=== Actor Training Debug (Iteration 1648) ===
Q mean: -7.243894
Q std: 8.784559
Actor loss: 7.247891
Action reg: 0.003997
  l1.weight: grad_norm = 0.003486
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002933
Total gradient norm: 0.016362
=== Actor Training Debug (Iteration 1649) ===
Q mean: -7.401653
Q std: 8.985130
Actor loss: 7.405647
Action reg: 0.003994
  l1.weight: grad_norm = 0.010707
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.007564
Total gradient norm: 0.022886
=== Actor Training Debug (Iteration 1650) ===
Q mean: -7.117517
Q std: 8.937422
Actor loss: 7.121508
Action reg: 0.003991
  l1.weight: grad_norm = 0.004868
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.004606
Total gradient norm: 0.016448
=== Actor Training Debug (Iteration 1651) ===
Q mean: -9.223633
Q std: 9.987974
Actor loss: 9.227623
Action reg: 0.003990
  l1.weight: grad_norm = 0.013562
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.011126
Total gradient norm: 0.045933
=== Actor Training Debug (Iteration 1652) ===
Q mean: -7.953469
Q std: 9.199429
Actor loss: 7.957457
Action reg: 0.003988
  l1.weight: grad_norm = 0.034926
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.031090
Total gradient norm: 0.128220
=== Actor Training Debug (Iteration 1653) ===
Q mean: -8.274837
Q std: 9.356769
Actor loss: 8.278834
Action reg: 0.003997
  l1.weight: grad_norm = 0.000944
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.000755
Total gradient norm: 0.003662
=== Actor Training Debug (Iteration 1654) ===
Q mean: -7.887541
Q std: 10.032709
Actor loss: 7.891538
Action reg: 0.003997
  l1.weight: grad_norm = 0.002856
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002511
Total gradient norm: 0.011244
=== Actor Training Debug (Iteration 1655) ===
Q mean: -8.135818
Q std: 10.290870
Actor loss: 8.139808
Action reg: 0.003989
  l1.weight: grad_norm = 0.008677
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.006948
Total gradient norm: 0.024309
=== Actor Training Debug (Iteration 1656) ===
Q mean: -8.375956
Q std: 10.065935
Actor loss: 8.379950
Action reg: 0.003994
  l1.weight: grad_norm = 0.008257
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.006554
Total gradient norm: 0.029456
=== Actor Training Debug (Iteration 1657) ===
Q mean: -7.715131
Q std: 9.215941
Actor loss: 7.719118
Action reg: 0.003987
  l1.weight: grad_norm = 0.012658
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.011832
Total gradient norm: 0.064859
=== Actor Training Debug (Iteration 1658) ===
Q mean: -7.378736
Q std: 9.386637
Actor loss: 7.382729
Action reg: 0.003993
  l1.weight: grad_norm = 0.018817
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.015387
Total gradient norm: 0.056578
=== Actor Training Debug (Iteration 1659) ===
Q mean: -7.020772
Q std: 8.944379
Actor loss: 7.024767
Action reg: 0.003995
  l1.weight: grad_norm = 0.011218
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.009588
Total gradient norm: 0.036401
=== Actor Training Debug (Iteration 1660) ===
Q mean: -8.259030
Q std: 9.002501
Actor loss: 8.263024
Action reg: 0.003994
  l1.weight: grad_norm = 0.013295
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.011784
Total gradient norm: 0.040418
=== Actor Training Debug (Iteration 1661) ===
Q mean: -8.312536
Q std: 9.773392
Actor loss: 8.316526
Action reg: 0.003990
  l1.weight: grad_norm = 0.004381
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003461
Total gradient norm: 0.011372
=== Actor Training Debug (Iteration 1662) ===
Q mean: -7.435961
Q std: 9.165152
Actor loss: 7.439953
Action reg: 0.003992
  l1.weight: grad_norm = 0.008179
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.007837
Total gradient norm: 0.035522
=== Actor Training Debug (Iteration 1663) ===
Q mean: -6.693809
Q std: 8.914765
Actor loss: 6.697798
Action reg: 0.003990
  l1.weight: grad_norm = 0.023501
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.020193
Total gradient norm: 0.117311
=== Actor Training Debug (Iteration 1664) ===
Q mean: -7.320587
Q std: 8.936001
Actor loss: 7.324584
Action reg: 0.003997
  l1.weight: grad_norm = 0.005614
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.004490
Total gradient norm: 0.025941
=== Actor Training Debug (Iteration 1665) ===
Q mean: -7.813803
Q std: 9.332610
Actor loss: 7.817789
Action reg: 0.003986
  l1.weight: grad_norm = 0.020495
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.018045
Total gradient norm: 0.057038
=== Actor Training Debug (Iteration 1666) ===
Q mean: -8.526230
Q std: 9.847887
Actor loss: 8.530227
Action reg: 0.003997
  l1.weight: grad_norm = 0.006114
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.005147
Total gradient norm: 0.019070
=== Actor Training Debug (Iteration 1667) ===
Q mean: -8.428741
Q std: 9.743466
Actor loss: 8.432731
Action reg: 0.003990
  l1.weight: grad_norm = 0.024220
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.021160
Total gradient norm: 0.131110
=== Actor Training Debug (Iteration 1668) ===
Q mean: -8.296583
Q std: 9.560717
Actor loss: 8.300580
Action reg: 0.003997
  l1.weight: grad_norm = 0.015722
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.011061
Total gradient norm: 0.044477
=== Actor Training Debug (Iteration 1669) ===
Q mean: -7.423352
Q std: 8.902267
Actor loss: 7.427351
Action reg: 0.003998
  l1.weight: grad_norm = 0.000476
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000421
Total gradient norm: 0.002158
=== Actor Training Debug (Iteration 1670) ===
Q mean: -8.062083
Q std: 9.974792
Actor loss: 8.066068
Action reg: 0.003985
  l1.weight: grad_norm = 0.017346
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.013094
Total gradient norm: 0.055804
=== Actor Training Debug (Iteration 1671) ===
Q mean: -7.275317
Q std: 8.920788
Actor loss: 7.279312
Action reg: 0.003994
  l1.weight: grad_norm = 0.029787
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.026461
Total gradient norm: 0.068608
=== Actor Training Debug (Iteration 1672) ===
Q mean: -7.837722
Q std: 9.528433
Actor loss: 7.841716
Action reg: 0.003994
  l1.weight: grad_norm = 0.010766
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.008257
Total gradient norm: 0.032526
=== Actor Training Debug (Iteration 1673) ===
Q mean: -7.203615
Q std: 8.841929
Actor loss: 7.207609
Action reg: 0.003994
  l1.weight: grad_norm = 0.009595
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.008338
Total gradient norm: 0.031813
=== Actor Training Debug (Iteration 1674) ===
Q mean: -7.000620
Q std: 9.111616
Actor loss: 7.004617
Action reg: 0.003997
  l1.weight: grad_norm = 0.004405
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.003096
Total gradient norm: 0.013604
=== Actor Training Debug (Iteration 1675) ===
Q mean: -7.909992
Q std: 9.495296
Actor loss: 7.913986
Action reg: 0.003995
  l1.weight: grad_norm = 0.001278
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.000934
Total gradient norm: 0.003752
=== Actor Training Debug (Iteration 1676) ===
Q mean: -8.691783
Q std: 10.081530
Actor loss: 8.695777
Action reg: 0.003994
  l1.weight: grad_norm = 0.017238
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.013809
Total gradient norm: 0.058602
=== Actor Training Debug (Iteration 1677) ===
Q mean: -8.464668
Q std: 9.014254
Actor loss: 8.468667
Action reg: 0.003999
  l1.weight: grad_norm = 0.002499
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002411
Total gradient norm: 0.009147
=== Actor Training Debug (Iteration 1678) ===
Q mean: -8.729627
Q std: 9.795672
Actor loss: 8.733621
Action reg: 0.003994
  l1.weight: grad_norm = 0.021647
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.017344
Total gradient norm: 0.057271
=== Actor Training Debug (Iteration 1679) ===
Q mean: -8.834711
Q std: 9.616668
Actor loss: 8.838706
Action reg: 0.003995
  l1.weight: grad_norm = 0.004822
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004620
Total gradient norm: 0.019573
=== Actor Training Debug (Iteration 1680) ===
Q mean: -7.400383
Q std: 8.761674
Actor loss: 7.404375
Action reg: 0.003993
  l1.weight: grad_norm = 0.012984
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.010602
Total gradient norm: 0.049436
=== Actor Training Debug (Iteration 1681) ===
Q mean: -8.934991
Q std: 10.327752
Actor loss: 8.938980
Action reg: 0.003989
  l1.weight: grad_norm = 0.030670
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.025497
Total gradient norm: 0.090889
=== Actor Training Debug (Iteration 1682) ===
Q mean: -7.509696
Q std: 9.099454
Actor loss: 7.513686
Action reg: 0.003990
  l1.weight: grad_norm = 0.013964
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.011549
Total gradient norm: 0.043659
=== Actor Training Debug (Iteration 1683) ===
Q mean: -7.889843
Q std: 9.089109
Actor loss: 7.893833
Action reg: 0.003990
  l1.weight: grad_norm = 0.004839
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.004308
Total gradient norm: 0.019832
=== Actor Training Debug (Iteration 1684) ===
Q mean: -7.748780
Q std: 8.611897
Actor loss: 7.752773
Action reg: 0.003993
  l1.weight: grad_norm = 0.003777
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.003617
Total gradient norm: 0.025109
=== Actor Training Debug (Iteration 1685) ===
Q mean: -6.945316
Q std: 8.803534
Actor loss: 6.949308
Action reg: 0.003992
  l1.weight: grad_norm = 0.006526
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.005338
Total gradient norm: 0.023627
=== Actor Training Debug (Iteration 1686) ===
Q mean: -7.906441
Q std: 9.280238
Actor loss: 7.910433
Action reg: 0.003992
  l1.weight: grad_norm = 0.014334
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.010626
Total gradient norm: 0.044843
=== Actor Training Debug (Iteration 1687) ===
Q mean: -7.223330
Q std: 9.829679
Actor loss: 7.227324
Action reg: 0.003993
  l1.weight: grad_norm = 0.006268
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.004539
Total gradient norm: 0.022900
=== Actor Training Debug (Iteration 1688) ===
Q mean: -8.409039
Q std: 10.265704
Actor loss: 8.413033
Action reg: 0.003994
  l1.weight: grad_norm = 0.009838
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.007750
Total gradient norm: 0.031464
=== Actor Training Debug (Iteration 1689) ===
Q mean: -8.479069
Q std: 9.391771
Actor loss: 8.483053
Action reg: 0.003985
  l1.weight: grad_norm = 0.030732
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.021371
Total gradient norm: 0.076727
=== Actor Training Debug (Iteration 1690) ===
Q mean: -8.322527
Q std: 9.525002
Actor loss: 8.326525
Action reg: 0.003998
  l1.weight: grad_norm = 0.000982
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000784
Total gradient norm: 0.003694
=== Actor Training Debug (Iteration 1691) ===
Q mean: -6.720492
Q std: 8.502746
Actor loss: 6.724480
Action reg: 0.003988
  l1.weight: grad_norm = 0.014896
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.011720
Total gradient norm: 0.063407
=== Actor Training Debug (Iteration 1692) ===
Q mean: -9.257895
Q std: 10.424606
Actor loss: 9.261889
Action reg: 0.003995
  l1.weight: grad_norm = 0.017060
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.013681
Total gradient norm: 0.069561
=== Actor Training Debug (Iteration 1693) ===
Q mean: -9.062395
Q std: 9.875339
Actor loss: 9.066387
Action reg: 0.003993
  l1.weight: grad_norm = 0.020360
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.016008
Total gradient norm: 0.053872
=== Actor Training Debug (Iteration 1694) ===
Q mean: -8.100854
Q std: 10.069440
Actor loss: 8.104846
Action reg: 0.003992
  l1.weight: grad_norm = 0.014445
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.010777
Total gradient norm: 0.037104
=== Actor Training Debug (Iteration 1695) ===
Q mean: -7.088268
Q std: 9.409022
Actor loss: 7.092262
Action reg: 0.003994
  l1.weight: grad_norm = 0.004651
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.003819
Total gradient norm: 0.016637
=== Actor Training Debug (Iteration 1696) ===
Q mean: -8.045151
Q std: 9.570099
Actor loss: 8.049147
Action reg: 0.003996
  l1.weight: grad_norm = 0.000768
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000791
Total gradient norm: 0.003955
=== Actor Training Debug (Iteration 1697) ===
Q mean: -7.391800
Q std: 9.153710
Actor loss: 7.395795
Action reg: 0.003995
  l1.weight: grad_norm = 0.005018
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004917
Total gradient norm: 0.011606
=== Actor Training Debug (Iteration 1698) ===
Q mean: -8.410396
Q std: 9.917542
Actor loss: 8.414386
Action reg: 0.003990
  l1.weight: grad_norm = 0.008859
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.006868
Total gradient norm: 0.024349
=== Actor Training Debug (Iteration 1699) ===
Q mean: -8.639793
Q std: 9.933237
Actor loss: 8.643787
Action reg: 0.003994
  l1.weight: grad_norm = 0.018276
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.012548
Total gradient norm: 0.072764
=== Actor Training Debug (Iteration 1700) ===
Q mean: -8.168968
Q std: 9.770312
Actor loss: 8.172966
Action reg: 0.003998
  l1.weight: grad_norm = 0.003432
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002435
Total gradient norm: 0.012124
=== Actor Training Debug (Iteration 1701) ===
Q mean: -8.084045
Q std: 9.960260
Actor loss: 8.088037
Action reg: 0.003992
  l1.weight: grad_norm = 0.005898
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.003908
Total gradient norm: 0.015658
=== Actor Training Debug (Iteration 1702) ===
Q mean: -8.015042
Q std: 9.055216
Actor loss: 8.019030
Action reg: 0.003987
  l1.weight: grad_norm = 0.036630
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.027248
Total gradient norm: 0.122069
=== Actor Training Debug (Iteration 1703) ===
Q mean: -6.920434
Q std: 8.668439
Actor loss: 6.924426
Action reg: 0.003992
  l1.weight: grad_norm = 0.013863
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.010198
Total gradient norm: 0.056023
=== Actor Training Debug (Iteration 1704) ===
Q mean: -7.898337
Q std: 10.205073
Actor loss: 7.902328
Action reg: 0.003991
  l1.weight: grad_norm = 0.008329
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.007661
Total gradient norm: 0.031635
=== Actor Training Debug (Iteration 1705) ===
Q mean: -7.875330
Q std: 9.552060
Actor loss: 7.879324
Action reg: 0.003994
  l1.weight: grad_norm = 0.008844
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.007021
Total gradient norm: 0.042024
=== Actor Training Debug (Iteration 1706) ===
Q mean: -8.344465
Q std: 9.986627
Actor loss: 8.348457
Action reg: 0.003992
  l1.weight: grad_norm = 0.006982
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005438
Total gradient norm: 0.019149
=== Actor Training Debug (Iteration 1707) ===
Q mean: -8.637987
Q std: 9.853017
Actor loss: 8.641978
Action reg: 0.003992
  l1.weight: grad_norm = 0.014938
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.013953
Total gradient norm: 0.042522
=== Actor Training Debug (Iteration 1708) ===
Q mean: -7.465991
Q std: 9.377839
Actor loss: 7.469984
Action reg: 0.003993
  l1.weight: grad_norm = 0.029596
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.024987
Total gradient norm: 0.158770
=== Actor Training Debug (Iteration 1709) ===
Q mean: -7.730280
Q std: 9.617290
Actor loss: 7.734273
Action reg: 0.003993
  l1.weight: grad_norm = 0.017748
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.012188
Total gradient norm: 0.049899
=== Actor Training Debug (Iteration 1710) ===
Q mean: -7.862483
Q std: 9.212465
Actor loss: 7.866475
Action reg: 0.003992
  l1.weight: grad_norm = 0.007113
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.005650
Total gradient norm: 0.021663
=== Actor Training Debug (Iteration 1711) ===
Q mean: -7.635815
Q std: 9.044836
Actor loss: 7.639810
Action reg: 0.003995
  l1.weight: grad_norm = 0.008748
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006666
Total gradient norm: 0.026954
=== Actor Training Debug (Iteration 1712) ===
Q mean: -8.522923
Q std: 10.366536
Actor loss: 8.526921
Action reg: 0.003999
  l1.weight: grad_norm = 0.000360
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000344
Total gradient norm: 0.002038
=== Actor Training Debug (Iteration 1713) ===
Q mean: -8.343704
Q std: 9.399574
Actor loss: 8.347697
Action reg: 0.003993
  l1.weight: grad_norm = 0.010876
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.008298
Total gradient norm: 0.047137
=== Actor Training Debug (Iteration 1714) ===
Q mean: -7.307273
Q std: 9.457142
Actor loss: 7.311265
Action reg: 0.003991
  l1.weight: grad_norm = 0.009254
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.007284
Total gradient norm: 0.032839
=== Actor Training Debug (Iteration 1715) ===
Q mean: -7.010050
Q std: 8.869209
Actor loss: 7.014037
Action reg: 0.003987
  l1.weight: grad_norm = 0.022706
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.019509
Total gradient norm: 0.102095
=== Actor Training Debug (Iteration 1716) ===
Q mean: -8.438848
Q std: 9.918679
Actor loss: 8.442842
Action reg: 0.003993
  l1.weight: grad_norm = 0.018602
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.014425
Total gradient norm: 0.055173
=== Actor Training Debug (Iteration 1717) ===
Q mean: -8.625382
Q std: 10.003913
Actor loss: 8.629371
Action reg: 0.003988
  l1.weight: grad_norm = 0.019909
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.017682
Total gradient norm: 0.081662
=== Actor Training Debug (Iteration 1718) ===
Q mean: -7.779696
Q std: 9.348898
Actor loss: 7.783692
Action reg: 0.003995
  l1.weight: grad_norm = 0.003787
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.002660
Total gradient norm: 0.011229
=== Actor Training Debug (Iteration 1719) ===
Q mean: -8.268951
Q std: 9.917257
Actor loss: 8.272944
Action reg: 0.003993
  l1.weight: grad_norm = 0.036578
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.032551
Total gradient norm: 0.082697
=== Actor Training Debug (Iteration 1720) ===
Q mean: -8.435289
Q std: 10.035269
Actor loss: 8.439285
Action reg: 0.003996
  l1.weight: grad_norm = 0.003828
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003849
Total gradient norm: 0.023352
=== Actor Training Debug (Iteration 1721) ===
Q mean: -7.073248
Q std: 9.044839
Actor loss: 7.077243
Action reg: 0.003996
  l1.weight: grad_norm = 0.008097
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.007084
Total gradient norm: 0.048291
=== Actor Training Debug (Iteration 1722) ===
Q mean: -7.261415
Q std: 9.262704
Actor loss: 7.265403
Action reg: 0.003988
  l1.weight: grad_norm = 0.010793
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.008812
Total gradient norm: 0.043399
=== Actor Training Debug (Iteration 1723) ===
Q mean: -7.618822
Q std: 9.856972
Actor loss: 7.622805
Action reg: 0.003983
  l1.weight: grad_norm = 0.011610
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.012808
Total gradient norm: 0.070043
=== Actor Training Debug (Iteration 1724) ===
Q mean: -8.445169
Q std: 10.169882
Actor loss: 8.449156
Action reg: 0.003986
  l1.weight: grad_norm = 0.023489
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.024472
Total gradient norm: 0.171505
=== Actor Training Debug (Iteration 1725) ===
Q mean: -6.574267
Q std: 9.124566
Actor loss: 6.578254
Action reg: 0.003987
  l1.weight: grad_norm = 0.031435
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.048911
Total gradient norm: 0.375523
=== Actor Training Debug (Iteration 1726) ===
Q mean: -7.811080
Q std: 9.875316
Actor loss: 7.815058
Action reg: 0.003978
  l1.weight: grad_norm = 0.077067
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.100354
Total gradient norm: 0.586854
=== Actor Training Debug (Iteration 1727) ===
Q mean: -8.162485
Q std: 9.689444
Actor loss: 8.166463
Action reg: 0.003977
  l1.weight: grad_norm = 0.099401
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.126721
Total gradient norm: 0.814031
=== Actor Training Debug (Iteration 1728) ===
Q mean: -7.631852
Q std: 9.129615
Actor loss: 7.635839
Action reg: 0.003987
  l1.weight: grad_norm = 0.077500
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.108487
Total gradient norm: 0.727845
=== Actor Training Debug (Iteration 1729) ===
Q mean: -8.747250
Q std: 10.014622
Actor loss: 8.751224
Action reg: 0.003974
  l1.weight: grad_norm = 0.058165
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.092548
Total gradient norm: 0.453557
=== Actor Training Debug (Iteration 1730) ===
Q mean: -8.079823
Q std: 9.494250
Actor loss: 8.083807
Action reg: 0.003985
  l1.weight: grad_norm = 0.012537
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.011064
Total gradient norm: 0.049774
=== Actor Training Debug (Iteration 1731) ===
Q mean: -7.568984
Q std: 9.978688
Actor loss: 7.572972
Action reg: 0.003988
  l1.weight: grad_norm = 0.010756
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.010413
Total gradient norm: 0.042366
=== Actor Training Debug (Iteration 1732) ===
Q mean: -7.032433
Q std: 9.053963
Actor loss: 7.036418
Action reg: 0.003985
  l1.weight: grad_norm = 0.014739
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.014502
Total gradient norm: 0.083685
=== Actor Training Debug (Iteration 1733) ===
Q mean: -7.190406
Q std: 8.914159
Actor loss: 7.194387
Action reg: 0.003981
  l1.weight: grad_norm = 0.020036
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.019673
Total gradient norm: 0.091971
=== Actor Training Debug (Iteration 1734) ===
Q mean: -8.177229
Q std: 9.903585
Actor loss: 8.181211
Action reg: 0.003982
  l1.weight: grad_norm = 0.007296
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.005731
Total gradient norm: 0.019346
=== Actor Training Debug (Iteration 1735) ===
Q mean: -8.016195
Q std: 9.551785
Actor loss: 8.020186
Action reg: 0.003991
  l1.weight: grad_norm = 0.018128
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.012080
Total gradient norm: 0.063813
=== Actor Training Debug (Iteration 1736) ===
Q mean: -8.633570
Q std: 10.020974
Actor loss: 8.637557
Action reg: 0.003987
  l1.weight: grad_norm = 0.026322
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.023818
Total gradient norm: 0.108167
=== Actor Training Debug (Iteration 1737) ===
Q mean: -8.375073
Q std: 9.812747
Actor loss: 8.379066
Action reg: 0.003993
  l1.weight: grad_norm = 0.009165
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.009687
Total gradient norm: 0.057237
=== Actor Training Debug (Iteration 1738) ===
Q mean: -8.331388
Q std: 10.177190
Actor loss: 8.335379
Action reg: 0.003990
  l1.weight: grad_norm = 0.018546
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.015993
Total gradient norm: 0.090928
=== Actor Training Debug (Iteration 1739) ===
Q mean: -8.246025
Q std: 10.012890
Actor loss: 8.250019
Action reg: 0.003994
  l1.weight: grad_norm = 0.004233
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.003667
Total gradient norm: 0.019943
=== Actor Training Debug (Iteration 1740) ===
Q mean: -7.896406
Q std: 9.312788
Actor loss: 7.900397
Action reg: 0.003991
  l1.weight: grad_norm = 0.036701
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.023832
Total gradient norm: 0.117042
=== Actor Training Debug (Iteration 1741) ===
Q mean: -8.177568
Q std: 9.306852
Actor loss: 8.181562
Action reg: 0.003994
  l1.weight: grad_norm = 0.002103
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002000
Total gradient norm: 0.005807
=== Actor Training Debug (Iteration 1742) ===
Q mean: -7.892085
Q std: 9.718650
Actor loss: 7.896079
Action reg: 0.003993
  l1.weight: grad_norm = 0.003327
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.002984
Total gradient norm: 0.021049
=== Actor Training Debug (Iteration 1743) ===
Q mean: -8.166994
Q std: 9.705013
Actor loss: 8.170984
Action reg: 0.003990
  l1.weight: grad_norm = 0.009681
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.008565
Total gradient norm: 0.041740
=== Actor Training Debug (Iteration 1744) ===
Q mean: -8.404416
Q std: 10.058327
Actor loss: 8.408406
Action reg: 0.003990
  l1.weight: grad_norm = 0.018783
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.015901
Total gradient norm: 0.121135
=== Actor Training Debug (Iteration 1745) ===
Q mean: -7.681377
Q std: 9.061038
Actor loss: 7.685369
Action reg: 0.003992
  l1.weight: grad_norm = 0.013776
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.012122
Total gradient norm: 0.068738
=== Actor Training Debug (Iteration 1746) ===
Q mean: -7.056565
Q std: 9.367393
Actor loss: 7.060551
Action reg: 0.003986
  l1.weight: grad_norm = 0.042440
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.038289
Total gradient norm: 0.184123
=== Actor Training Debug (Iteration 1747) ===
Q mean: -6.911962
Q std: 9.427117
Actor loss: 6.915957
Action reg: 0.003996
  l1.weight: grad_norm = 0.002146
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.001606
Total gradient norm: 0.007689
=== Actor Training Debug (Iteration 1748) ===
Q mean: -8.683601
Q std: 10.238989
Actor loss: 8.687583
Action reg: 0.003982
  l1.weight: grad_norm = 0.025719
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.024528
Total gradient norm: 0.120889
=== Actor Training Debug (Iteration 1749) ===
Q mean: -8.686540
Q std: 10.218202
Actor loss: 8.690535
Action reg: 0.003995
  l1.weight: grad_norm = 0.026416
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.026274
Total gradient norm: 0.081517
=== Actor Training Debug (Iteration 1750) ===
Q mean: -8.233985
Q std: 9.617037
Actor loss: 8.237977
Action reg: 0.003992
  l1.weight: grad_norm = 0.012616
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.012873
Total gradient norm: 0.076216
=== Actor Training Debug (Iteration 1751) ===
Q mean: -8.140377
Q std: 10.045316
Actor loss: 8.144360
Action reg: 0.003982
  l1.weight: grad_norm = 0.039071
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.035688
Total gradient norm: 0.124339
=== Actor Training Debug (Iteration 1752) ===
Q mean: -8.839838
Q std: 10.256897
Actor loss: 8.843832
Action reg: 0.003994
  l1.weight: grad_norm = 0.007427
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005940
Total gradient norm: 0.025715
=== Actor Training Debug (Iteration 1753) ===
Q mean: -8.590672
Q std: 9.896565
Actor loss: 8.594660
Action reg: 0.003987
  l1.weight: grad_norm = 0.018759
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.017921
Total gradient norm: 0.082923
=== Actor Training Debug (Iteration 1754) ===
Q mean: -8.583330
Q std: 10.186624
Actor loss: 8.587328
Action reg: 0.003997
  l1.weight: grad_norm = 0.017255
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.014503
Total gradient norm: 0.046918
=== Actor Training Debug (Iteration 1755) ===
Q mean: -9.081074
Q std: 9.890388
Actor loss: 9.085065
Action reg: 0.003991
  l1.weight: grad_norm = 0.022884
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.017829
Total gradient norm: 0.073824
=== Actor Training Debug (Iteration 1756) ===
Q mean: -6.948392
Q std: 8.279743
Actor loss: 6.952375
Action reg: 0.003983
  l1.weight: grad_norm = 0.008317
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.007459
Total gradient norm: 0.024620
=== Actor Training Debug (Iteration 1757) ===
Q mean: -7.178608
Q std: 9.368083
Actor loss: 7.182594
Action reg: 0.003987
  l1.weight: grad_norm = 0.024352
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.019383
Total gradient norm: 0.105762
=== Actor Training Debug (Iteration 1758) ===
Q mean: -6.780976
Q std: 9.000008
Actor loss: 6.784964
Action reg: 0.003987
  l1.weight: grad_norm = 0.015849
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.012620
Total gradient norm: 0.052817
=== Actor Training Debug (Iteration 1759) ===
Q mean: -7.901449
Q std: 10.392843
Actor loss: 7.905443
Action reg: 0.003994
  l1.weight: grad_norm = 0.002422
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002000
Total gradient norm: 0.009629
=== Actor Training Debug (Iteration 1760) ===
Q mean: -7.557405
Q std: 9.596298
Actor loss: 7.561400
Action reg: 0.003996
  l1.weight: grad_norm = 0.002722
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002548
Total gradient norm: 0.009919
=== Actor Training Debug (Iteration 1761) ===
Q mean: -8.564388
Q std: 10.479346
Actor loss: 8.568380
Action reg: 0.003992
  l1.weight: grad_norm = 0.006586
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004945
Total gradient norm: 0.021160
=== Actor Training Debug (Iteration 1762) ===
Q mean: -7.687954
Q std: 9.736938
Actor loss: 7.691939
Action reg: 0.003985
  l1.weight: grad_norm = 0.021443
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.018549
Total gradient norm: 0.084777
=== Actor Training Debug (Iteration 1763) ===
Q mean: -7.820089
Q std: 9.930430
Actor loss: 7.824077
Action reg: 0.003987
  l1.weight: grad_norm = 0.024742
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.021202
Total gradient norm: 0.113556
=== Actor Training Debug (Iteration 1764) ===
Q mean: -8.489934
Q std: 9.829217
Actor loss: 8.493925
Action reg: 0.003991
  l1.weight: grad_norm = 0.002150
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.002210
Total gradient norm: 0.012960
=== Actor Training Debug (Iteration 1765) ===
Q mean: -7.784337
Q std: 9.919902
Actor loss: 7.788327
Action reg: 0.003990
  l1.weight: grad_norm = 0.018940
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.017243
Total gradient norm: 0.048781
=== Actor Training Debug (Iteration 1766) ===
Q mean: -7.484461
Q std: 9.397777
Actor loss: 7.488451
Action reg: 0.003990
  l1.weight: grad_norm = 0.061842
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.051487
Total gradient norm: 0.165396
=== Actor Training Debug (Iteration 1767) ===
Q mean: -7.440006
Q std: 9.646537
Actor loss: 7.443993
Action reg: 0.003987
  l1.weight: grad_norm = 0.015463
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.012271
Total gradient norm: 0.043054
=== Actor Training Debug (Iteration 1768) ===
Q mean: -8.155312
Q std: 10.017203
Actor loss: 8.159308
Action reg: 0.003997
  l1.weight: grad_norm = 0.011757
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.011550
Total gradient norm: 0.032826
=== Actor Training Debug (Iteration 1769) ===
Q mean: -6.671910
Q std: 8.518670
Actor loss: 6.675896
Action reg: 0.003985
  l1.weight: grad_norm = 0.010951
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.009208
Total gradient norm: 0.045428
=== Actor Training Debug (Iteration 1770) ===
Q mean: -8.449108
Q std: 10.060837
Actor loss: 8.453101
Action reg: 0.003993
  l1.weight: grad_norm = 0.005755
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.005353
Total gradient norm: 0.027713
=== Actor Training Debug (Iteration 1771) ===
Q mean: -7.785474
Q std: 10.022476
Actor loss: 7.789461
Action reg: 0.003987
  l1.weight: grad_norm = 0.018666
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.016226
Total gradient norm: 0.050667
=== Actor Training Debug (Iteration 1772) ===
Q mean: -8.419242
Q std: 9.890373
Actor loss: 8.423239
Action reg: 0.003997
  l1.weight: grad_norm = 0.007929
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.006533
Total gradient norm: 0.037829
=== Actor Training Debug (Iteration 1773) ===
Q mean: -9.321783
Q std: 10.715456
Actor loss: 9.325776
Action reg: 0.003993
  l1.weight: grad_norm = 0.015148
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.012835
Total gradient norm: 0.039452
=== Actor Training Debug (Iteration 1774) ===
Q mean: -8.388039
Q std: 10.031658
Actor loss: 8.392032
Action reg: 0.003993
  l1.weight: grad_norm = 0.015654
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.013148
Total gradient norm: 0.079982
=== Actor Training Debug (Iteration 1775) ===
Q mean: -8.103628
Q std: 10.270912
Actor loss: 8.107627
Action reg: 0.003999
  l1.weight: grad_norm = 0.004301
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003537
Total gradient norm: 0.019966
=== Actor Training Debug (Iteration 1776) ===
Q mean: -9.329711
Q std: 10.601916
Actor loss: 9.333705
Action reg: 0.003994
  l1.weight: grad_norm = 0.005889
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.005101
Total gradient norm: 0.024396
=== Actor Training Debug (Iteration 1777) ===
Q mean: -8.493181
Q std: 10.615721
Actor loss: 8.497172
Action reg: 0.003991
  l1.weight: grad_norm = 0.009269
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.007249
Total gradient norm: 0.053241
=== Actor Training Debug (Iteration 1778) ===
Q mean: -8.914025
Q std: 10.377704
Actor loss: 8.918021
Action reg: 0.003996
  l1.weight: grad_norm = 0.005102
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004147
Total gradient norm: 0.013604
=== Actor Training Debug (Iteration 1779) ===
Q mean: -8.124789
Q std: 9.443092
Actor loss: 8.128784
Action reg: 0.003995
  l1.weight: grad_norm = 0.006388
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005634
Total gradient norm: 0.021886
=== Actor Training Debug (Iteration 1780) ===
Q mean: -8.611135
Q std: 9.993038
Actor loss: 8.615123
Action reg: 0.003987
  l1.weight: grad_norm = 0.014438
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.013531
Total gradient norm: 0.093874
=== Actor Training Debug (Iteration 1781) ===
Q mean: -8.798158
Q std: 10.631395
Actor loss: 8.802153
Action reg: 0.003995
  l1.weight: grad_norm = 0.007628
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.006290
Total gradient norm: 0.026103
=== Actor Training Debug (Iteration 1782) ===
Q mean: -8.691840
Q std: 10.066201
Actor loss: 8.695830
Action reg: 0.003990
  l1.weight: grad_norm = 0.011610
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.009584
Total gradient norm: 0.048784
=== Actor Training Debug (Iteration 1783) ===
Q mean: -7.701039
Q std: 9.398519
Actor loss: 7.705031
Action reg: 0.003992
  l1.weight: grad_norm = 0.008790
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.006386
Total gradient norm: 0.030908
=== Actor Training Debug (Iteration 1784) ===
Q mean: -7.831965
Q std: 9.377688
Actor loss: 7.835962
Action reg: 0.003997
  l1.weight: grad_norm = 0.005124
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004114
Total gradient norm: 0.018698
=== Actor Training Debug (Iteration 1785) ===
Q mean: -8.472082
Q std: 9.850197
Actor loss: 8.476078
Action reg: 0.003996
  l1.weight: grad_norm = 0.011272
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.010677
Total gradient norm: 0.064524
=== Actor Training Debug (Iteration 1786) ===
Q mean: -8.907288
Q std: 10.531985
Actor loss: 8.911278
Action reg: 0.003990
  l1.weight: grad_norm = 0.014948
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.013518
Total gradient norm: 0.056608
=== Actor Training Debug (Iteration 1787) ===
Q mean: -7.755461
Q std: 9.905102
Actor loss: 7.759454
Action reg: 0.003993
  l1.weight: grad_norm = 0.005190
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004387
Total gradient norm: 0.023817
=== Actor Training Debug (Iteration 1788) ===
Q mean: -7.917546
Q std: 9.955397
Actor loss: 7.921542
Action reg: 0.003996
  l1.weight: grad_norm = 0.022439
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.015644
Total gradient norm: 0.111657
=== Actor Training Debug (Iteration 1789) ===
Q mean: -7.953293
Q std: 9.950279
Actor loss: 7.957285
Action reg: 0.003992
  l1.weight: grad_norm = 0.017299
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.013434
Total gradient norm: 0.100202
=== Actor Training Debug (Iteration 1790) ===
Q mean: -8.965314
Q std: 10.244380
Actor loss: 8.969309
Action reg: 0.003995
  l1.weight: grad_norm = 0.013427
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.010964
Total gradient norm: 0.042104
=== Actor Training Debug (Iteration 1791) ===
Q mean: -6.683592
Q std: 9.178155
Actor loss: 6.687586
Action reg: 0.003994
  l1.weight: grad_norm = 0.004489
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003938
Total gradient norm: 0.020907
=== Actor Training Debug (Iteration 1792) ===
Q mean: -8.557088
Q std: 9.919995
Actor loss: 8.561081
Action reg: 0.003993
  l1.weight: grad_norm = 0.019957
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.018235
Total gradient norm: 0.112372
=== Actor Training Debug (Iteration 1793) ===
Q mean: -8.086636
Q std: 10.327973
Actor loss: 8.090628
Action reg: 0.003992
  l1.weight: grad_norm = 0.014742
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.012210
Total gradient norm: 0.072214
=== Actor Training Debug (Iteration 1794) ===
Q mean: -7.993246
Q std: 9.806055
Actor loss: 7.997238
Action reg: 0.003992
  l1.weight: grad_norm = 0.024513
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.022229
Total gradient norm: 0.084070
=== Actor Training Debug (Iteration 1795) ===
Q mean: -8.758042
Q std: 10.419114
Actor loss: 8.762034
Action reg: 0.003992
  l1.weight: grad_norm = 0.005258
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.004725
Total gradient norm: 0.032236
=== Actor Training Debug (Iteration 1796) ===
Q mean: -7.566476
Q std: 9.500342
Actor loss: 7.570469
Action reg: 0.003993
  l1.weight: grad_norm = 0.013774
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.012865
Total gradient norm: 0.067961
=== Actor Training Debug (Iteration 1797) ===
Q mean: -7.579831
Q std: 10.022882
Actor loss: 7.583822
Action reg: 0.003992
  l1.weight: grad_norm = 0.021716
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.017046
Total gradient norm: 0.092263
=== Actor Training Debug (Iteration 1798) ===
Q mean: -8.255636
Q std: 10.052925
Actor loss: 8.259631
Action reg: 0.003995
  l1.weight: grad_norm = 0.004214
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003748
Total gradient norm: 0.018343
=== Actor Training Debug (Iteration 1799) ===
Q mean: -7.861030
Q std: 9.826821
Actor loss: 7.865023
Action reg: 0.003994
  l1.weight: grad_norm = 0.008335
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.007445
Total gradient norm: 0.030527
=== Actor Training Debug (Iteration 1800) ===
Q mean: -7.937767
Q std: 9.881105
Actor loss: 7.941754
Action reg: 0.003987
  l1.weight: grad_norm = 0.022510
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.021524
Total gradient norm: 0.157906
=== Actor Training Debug (Iteration 1801) ===
Q mean: -7.743969
Q std: 9.498947
Actor loss: 7.747964
Action reg: 0.003996
  l1.weight: grad_norm = 0.002105
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002042
Total gradient norm: 0.008095
=== Actor Training Debug (Iteration 1802) ===
Q mean: -8.409864
Q std: 10.160456
Actor loss: 8.413857
Action reg: 0.003992
  l1.weight: grad_norm = 0.018474
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.017321
Total gradient norm: 0.074098
=== Actor Training Debug (Iteration 1803) ===
Q mean: -7.636224
Q std: 9.563588
Actor loss: 7.640219
Action reg: 0.003995
  l1.weight: grad_norm = 0.006197
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005695
Total gradient norm: 0.017177
=== Actor Training Debug (Iteration 1804) ===
Q mean: -8.290268
Q std: 10.113364
Actor loss: 8.294253
Action reg: 0.003985
  l1.weight: grad_norm = 0.024099
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.020607
Total gradient norm: 0.112240
=== Actor Training Debug (Iteration 1805) ===
Q mean: -8.333917
Q std: 10.072738
Actor loss: 8.337912
Action reg: 0.003995
  l1.weight: grad_norm = 0.018297
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.016732
Total gradient norm: 0.053022
=== Actor Training Debug (Iteration 1806) ===
Q mean: -8.954796
Q std: 10.028812
Actor loss: 8.958791
Action reg: 0.003995
  l1.weight: grad_norm = 0.001302
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001103
Total gradient norm: 0.003260
=== Actor Training Debug (Iteration 1807) ===
Q mean: -6.759914
Q std: 9.396625
Actor loss: 6.763904
Action reg: 0.003990
  l1.weight: grad_norm = 0.011221
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.010290
Total gradient norm: 0.037791
=== Actor Training Debug (Iteration 1808) ===
Q mean: -7.552442
Q std: 10.108320
Actor loss: 7.556434
Action reg: 0.003992
  l1.weight: grad_norm = 0.027937
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.022175
Total gradient norm: 0.073987
=== Actor Training Debug (Iteration 1809) ===
Q mean: -7.022381
Q std: 9.540206
Actor loss: 7.026374
Action reg: 0.003993
  l1.weight: grad_norm = 0.023847
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.022965
Total gradient norm: 0.094311
=== Actor Training Debug (Iteration 1810) ===
Q mean: -8.950536
Q std: 10.089654
Actor loss: 8.954532
Action reg: 0.003995
  l1.weight: grad_norm = 0.016980
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.014954
Total gradient norm: 0.053270
=== Actor Training Debug (Iteration 1811) ===
Q mean: -7.486495
Q std: 9.646245
Actor loss: 7.490485
Action reg: 0.003991
  l1.weight: grad_norm = 0.013152
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.010366
Total gradient norm: 0.051527
=== Actor Training Debug (Iteration 1812) ===
Q mean: -7.594350
Q std: 10.060150
Actor loss: 7.598344
Action reg: 0.003994
  l1.weight: grad_norm = 0.018628
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.019271
Total gradient norm: 0.060784
=== Actor Training Debug (Iteration 1813) ===
Q mean: -8.069436
Q std: 10.674301
Actor loss: 8.073429
Action reg: 0.003993
  l1.weight: grad_norm = 0.009457
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.007771
Total gradient norm: 0.044306
=== Actor Training Debug (Iteration 1814) ===
Q mean: -8.630592
Q std: 10.248422
Actor loss: 8.634584
Action reg: 0.003992
  l1.weight: grad_norm = 0.004748
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004677
Total gradient norm: 0.035577
=== Actor Training Debug (Iteration 1815) ===
Q mean: -9.197096
Q std: 11.009212
Actor loss: 9.201089
Action reg: 0.003993
  l1.weight: grad_norm = 0.016219
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.013675
Total gradient norm: 0.062964
=== Actor Training Debug (Iteration 1816) ===
Q mean: -9.702281
Q std: 10.346185
Actor loss: 9.706270
Action reg: 0.003989
  l1.weight: grad_norm = 0.016455
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.014572
Total gradient norm: 0.069540
=== Actor Training Debug (Iteration 1817) ===
Q mean: -8.664221
Q std: 10.469895
Actor loss: 8.668211
Action reg: 0.003991
  l1.weight: grad_norm = 0.010892
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.010401
Total gradient norm: 0.085426
=== Actor Training Debug (Iteration 1818) ===
Q mean: -7.903166
Q std: 10.487610
Actor loss: 7.907154
Action reg: 0.003987
  l1.weight: grad_norm = 0.019606
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.016002
Total gradient norm: 0.059555
=== Actor Training Debug (Iteration 1819) ===
Q mean: -7.222092
Q std: 9.413998
Actor loss: 7.226086
Action reg: 0.003994
  l1.weight: grad_norm = 0.005632
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.004343
Total gradient norm: 0.022839
=== Actor Training Debug (Iteration 1820) ===
Q mean: -8.640516
Q std: 10.281027
Actor loss: 8.644507
Action reg: 0.003991
  l1.weight: grad_norm = 0.006610
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005382
Total gradient norm: 0.018042
=== Actor Training Debug (Iteration 1821) ===
Q mean: -8.912464
Q std: 10.181696
Actor loss: 8.916456
Action reg: 0.003992
  l1.weight: grad_norm = 0.016939
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.013401
Total gradient norm: 0.050661
=== Actor Training Debug (Iteration 1822) ===
Q mean: -8.056394
Q std: 9.929721
Actor loss: 8.060384
Action reg: 0.003990
  l1.weight: grad_norm = 0.006889
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005010
Total gradient norm: 0.031126
=== Actor Training Debug (Iteration 1823) ===
Q mean: -8.172792
Q std: 10.251757
Actor loss: 8.176786
Action reg: 0.003994
  l1.weight: grad_norm = 0.004114
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.003052
Total gradient norm: 0.020601
=== Actor Training Debug (Iteration 1824) ===
Q mean: -7.122253
Q std: 9.808949
Actor loss: 7.126250
Action reg: 0.003996
  l1.weight: grad_norm = 0.014888
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.013474
Total gradient norm: 0.072194
=== Actor Training Debug (Iteration 1825) ===
Q mean: -7.755894
Q std: 10.250032
Actor loss: 7.759892
Action reg: 0.003999
  l1.weight: grad_norm = 0.000487
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000415
Total gradient norm: 0.001479
=== Actor Training Debug (Iteration 1826) ===
Q mean: -8.972655
Q std: 10.395128
Actor loss: 8.976648
Action reg: 0.003993
  l1.weight: grad_norm = 0.022190
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.015838
Total gradient norm: 0.066472
=== Actor Training Debug (Iteration 1827) ===
Q mean: -8.663458
Q std: 10.467550
Actor loss: 8.667454
Action reg: 0.003996
  l1.weight: grad_norm = 0.005498
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.005081
Total gradient norm: 0.025074
=== Actor Training Debug (Iteration 1828) ===
Q mean: -8.508237
Q std: 10.045828
Actor loss: 8.512231
Action reg: 0.003994
  l1.weight: grad_norm = 0.012971
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.011909
Total gradient norm: 0.067896
=== Actor Training Debug (Iteration 1829) ===
Q mean: -8.266003
Q std: 10.023603
Actor loss: 8.269999
Action reg: 0.003996
  l1.weight: grad_norm = 0.006403
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005085
Total gradient norm: 0.026097
=== Actor Training Debug (Iteration 1830) ===
Q mean: -8.197183
Q std: 10.126734
Actor loss: 8.201176
Action reg: 0.003993
  l1.weight: grad_norm = 0.007433
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.006193
Total gradient norm: 0.027127
=== Actor Training Debug (Iteration 1831) ===
Q mean: -8.263818
Q std: 10.410312
Actor loss: 8.267816
Action reg: 0.003998
  l1.weight: grad_norm = 0.000126
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000105
Total gradient norm: 0.000568
=== Actor Training Debug (Iteration 1832) ===
Q mean: -8.231229
Q std: 10.170121
Actor loss: 8.235217
Action reg: 0.003988
  l1.weight: grad_norm = 0.019966
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.017936
Total gradient norm: 0.137298
=== Actor Training Debug (Iteration 1833) ===
Q mean: -8.440064
Q std: 10.276111
Actor loss: 8.444053
Action reg: 0.003988
  l1.weight: grad_norm = 0.024060
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.020915
Total gradient norm: 0.181414
=== Actor Training Debug (Iteration 1834) ===
Q mean: -8.768950
Q std: 9.975306
Actor loss: 8.772947
Action reg: 0.003997
  l1.weight: grad_norm = 0.014870
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.012060
Total gradient norm: 0.080225
=== Actor Training Debug (Iteration 1835) ===
Q mean: -8.730310
Q std: 10.865028
Actor loss: 8.734307
Action reg: 0.003997
  l1.weight: grad_norm = 0.001890
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001747
Total gradient norm: 0.010215
=== Actor Training Debug (Iteration 1836) ===
Q mean: -8.841253
Q std: 10.825371
Actor loss: 8.845244
Action reg: 0.003991
  l1.weight: grad_norm = 0.028559
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.019551
Total gradient norm: 0.071250
=== Actor Training Debug (Iteration 1837) ===
Q mean: -8.605650
Q std: 10.201663
Actor loss: 8.609646
Action reg: 0.003996
  l1.weight: grad_norm = 0.004651
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.003700
Total gradient norm: 0.020985
=== Actor Training Debug (Iteration 1838) ===
Q mean: -7.656277
Q std: 9.663819
Actor loss: 7.660267
Action reg: 0.003990
  l1.weight: grad_norm = 0.028546
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.023165
Total gradient norm: 0.085717
=== Actor Training Debug (Iteration 1839) ===
Q mean: -8.537627
Q std: 10.335497
Actor loss: 8.541617
Action reg: 0.003990
  l1.weight: grad_norm = 0.022712
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.020476
Total gradient norm: 0.105591
=== Actor Training Debug (Iteration 1840) ===
Q mean: -7.818667
Q std: 9.803786
Actor loss: 7.822662
Action reg: 0.003994
  l1.weight: grad_norm = 0.002181
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001723
Total gradient norm: 0.006504
=== Actor Training Debug (Iteration 1841) ===
Q mean: -7.814163
Q std: 9.970298
Actor loss: 7.818158
Action reg: 0.003995
  l1.weight: grad_norm = 0.015305
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.012626
Total gradient norm: 0.035479
=== Actor Training Debug (Iteration 1842) ===
Q mean: -7.982409
Q std: 10.323073
Actor loss: 7.986400
Action reg: 0.003992
  l1.weight: grad_norm = 0.004525
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004098
Total gradient norm: 0.019750
=== Actor Training Debug (Iteration 1843) ===
Q mean: -8.432491
Q std: 10.098367
Actor loss: 8.436489
Action reg: 0.003998
  l1.weight: grad_norm = 0.001544
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001335
Total gradient norm: 0.009071
=== Actor Training Debug (Iteration 1844) ===
Q mean: -8.353468
Q std: 10.167698
Actor loss: 8.357461
Action reg: 0.003993
  l1.weight: grad_norm = 0.007577
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.005838
Total gradient norm: 0.027569
=== Actor Training Debug (Iteration 1845) ===
Q mean: -8.862455
Q std: 10.311478
Actor loss: 8.866447
Action reg: 0.003992
  l1.weight: grad_norm = 0.015862
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.012966
Total gradient norm: 0.090661
=== Actor Training Debug (Iteration 1846) ===
Q mean: -8.600712
Q std: 10.071905
Actor loss: 8.604700
Action reg: 0.003989
  l1.weight: grad_norm = 0.012637
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010986
Total gradient norm: 0.039836
=== Actor Training Debug (Iteration 1847) ===
Q mean: -7.425962
Q std: 9.625801
Actor loss: 7.429957
Action reg: 0.003995
  l1.weight: grad_norm = 0.012134
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010207
Total gradient norm: 0.031649
=== Actor Training Debug (Iteration 1848) ===
Q mean: -9.195951
Q std: 10.665223
Actor loss: 9.199944
Action reg: 0.003993
  l1.weight: grad_norm = 0.006404
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.004975
Total gradient norm: 0.029019
=== Actor Training Debug (Iteration 1849) ===
Q mean: -9.238589
Q std: 11.053053
Actor loss: 9.242575
Action reg: 0.003985
  l1.weight: grad_norm = 0.012581
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.011010
Total gradient norm: 0.093396
=== Actor Training Debug (Iteration 1850) ===
Q mean: -8.307673
Q std: 10.674995
Actor loss: 8.311665
Action reg: 0.003991
  l1.weight: grad_norm = 0.010383
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.009085
Total gradient norm: 0.038719
=== Actor Training Debug (Iteration 1851) ===
Q mean: -9.458060
Q std: 11.117966
Actor loss: 9.462050
Action reg: 0.003990
  l1.weight: grad_norm = 0.017676
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.014929
Total gradient norm: 0.066011
=== Actor Training Debug (Iteration 1852) ===
Q mean: -7.650784
Q std: 9.570216
Actor loss: 7.654775
Action reg: 0.003992
  l1.weight: grad_norm = 0.007121
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005210
Total gradient norm: 0.024087
=== Actor Training Debug (Iteration 1853) ===
Q mean: -8.484240
Q std: 9.777712
Actor loss: 8.488230
Action reg: 0.003990
  l1.weight: grad_norm = 0.044027
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.035948
Total gradient norm: 0.222769
=== Actor Training Debug (Iteration 1854) ===
Q mean: -8.891567
Q std: 10.775126
Actor loss: 8.895560
Action reg: 0.003993
  l1.weight: grad_norm = 0.009705
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.007896
Total gradient norm: 0.037110
=== Actor Training Debug (Iteration 1855) ===
Q mean: -8.126112
Q std: 10.165646
Actor loss: 8.130108
Action reg: 0.003996
  l1.weight: grad_norm = 0.009278
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.006375
Total gradient norm: 0.030264
=== Actor Training Debug (Iteration 1856) ===
Q mean: -7.932590
Q std: 10.357227
Actor loss: 7.936580
Action reg: 0.003989
  l1.weight: grad_norm = 0.023596
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.017638
Total gradient norm: 0.066087
=== Actor Training Debug (Iteration 1857) ===
Q mean: -8.825703
Q std: 10.528939
Actor loss: 8.829697
Action reg: 0.003994
  l1.weight: grad_norm = 0.005274
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004834
Total gradient norm: 0.015919
=== Actor Training Debug (Iteration 1858) ===
Q mean: -7.340821
Q std: 9.716665
Actor loss: 7.344818
Action reg: 0.003996
  l1.weight: grad_norm = 0.003032
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002284
Total gradient norm: 0.014195
=== Actor Training Debug (Iteration 1859) ===
Q mean: -9.037827
Q std: 10.184622
Actor loss: 9.041818
Action reg: 0.003991
  l1.weight: grad_norm = 0.009804
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.009242
Total gradient norm: 0.065060
=== Actor Training Debug (Iteration 1860) ===
Q mean: -10.006407
Q std: 11.042287
Actor loss: 10.010401
Action reg: 0.003994
  l1.weight: grad_norm = 0.017385
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.013587
Total gradient norm: 0.043250
=== Actor Training Debug (Iteration 1861) ===
Q mean: -8.623323
Q std: 10.855022
Actor loss: 8.627311
Action reg: 0.003987
  l1.weight: grad_norm = 0.027261
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.022680
Total gradient norm: 0.133111
=== Actor Training Debug (Iteration 1862) ===
Q mean: -8.986092
Q std: 10.586979
Actor loss: 8.990084
Action reg: 0.003992
  l1.weight: grad_norm = 0.007182
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.006034
Total gradient norm: 0.022198
=== Actor Training Debug (Iteration 1863) ===
Q mean: -8.623879
Q std: 10.393177
Actor loss: 8.627868
Action reg: 0.003988
  l1.weight: grad_norm = 0.028770
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.020712
Total gradient norm: 0.132175
=== Actor Training Debug (Iteration 1864) ===
Q mean: -9.425003
Q std: 10.549472
Actor loss: 9.428994
Action reg: 0.003992
  l1.weight: grad_norm = 0.011420
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.010147
Total gradient norm: 0.046333
=== Actor Training Debug (Iteration 1865) ===
Q mean: -8.523058
Q std: 10.682758
Actor loss: 8.527050
Action reg: 0.003992
  l1.weight: grad_norm = 0.006369
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.005503
Total gradient norm: 0.032542
=== Actor Training Debug (Iteration 1866) ===
Q mean: -8.597599
Q std: 10.535774
Actor loss: 8.601594
Action reg: 0.003995
  l1.weight: grad_norm = 0.011211
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.010702
Total gradient norm: 0.044190
=== Actor Training Debug (Iteration 1867) ===
Q mean: -9.202744
Q std: 10.664143
Actor loss: 9.206739
Action reg: 0.003996
  l1.weight: grad_norm = 0.013600
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.010758
Total gradient norm: 0.047402
=== Actor Training Debug (Iteration 1868) ===
Q mean: -8.768151
Q std: 10.915453
Actor loss: 8.772143
Action reg: 0.003992
  l1.weight: grad_norm = 0.010123
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.007765
Total gradient norm: 0.027579
=== Actor Training Debug (Iteration 1869) ===
Q mean: -8.692985
Q std: 10.285677
Actor loss: 8.696971
Action reg: 0.003986
  l1.weight: grad_norm = 0.038444
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.029247
Total gradient norm: 0.077752
=== Actor Training Debug (Iteration 1870) ===
Q mean: -8.127792
Q std: 9.890176
Actor loss: 8.131783
Action reg: 0.003991
  l1.weight: grad_norm = 0.016932
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.014317
Total gradient norm: 0.049109
=== Actor Training Debug (Iteration 1871) ===
Q mean: -7.854499
Q std: 9.717623
Actor loss: 7.858495
Action reg: 0.003996
  l1.weight: grad_norm = 0.007015
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005880
Total gradient norm: 0.031150
=== Actor Training Debug (Iteration 1872) ===
Q mean: -8.007079
Q std: 10.461987
Actor loss: 8.011069
Action reg: 0.003991
  l1.weight: grad_norm = 0.007518
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.005489
Total gradient norm: 0.026344
=== Actor Training Debug (Iteration 1873) ===
Q mean: -9.732551
Q std: 11.328083
Actor loss: 9.736542
Action reg: 0.003991
  l1.weight: grad_norm = 0.008925
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.006553
Total gradient norm: 0.029747
=== Actor Training Debug (Iteration 1874) ===
Q mean: -8.680858
Q std: 10.599086
Actor loss: 8.684850
Action reg: 0.003992
  l1.weight: grad_norm = 0.038111
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.027982
Total gradient norm: 0.165129
=== Actor Training Debug (Iteration 1875) ===
Q mean: -8.260727
Q std: 10.163054
Actor loss: 8.264715
Action reg: 0.003988
  l1.weight: grad_norm = 0.022372
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.019965
Total gradient norm: 0.062516
=== Actor Training Debug (Iteration 1876) ===
Q mean: -8.016731
Q std: 9.702082
Actor loss: 8.020726
Action reg: 0.003995
  l1.weight: grad_norm = 0.010077
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.007548
Total gradient norm: 0.030202
=== Actor Training Debug (Iteration 1877) ===
Q mean: -8.503441
Q std: 10.105468
Actor loss: 8.507433
Action reg: 0.003993
  l1.weight: grad_norm = 0.015603
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.013215
Total gradient norm: 0.048119
=== Actor Training Debug (Iteration 1878) ===
Q mean: -9.503815
Q std: 10.780146
Actor loss: 9.507808
Action reg: 0.003993
  l1.weight: grad_norm = 0.028532
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.025817
Total gradient norm: 0.177737
=== Actor Training Debug (Iteration 1879) ===
Q mean: -7.522993
Q std: 9.508188
Actor loss: 7.526983
Action reg: 0.003990
  l1.weight: grad_norm = 0.012757
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.010896
Total gradient norm: 0.073430
=== Actor Training Debug (Iteration 1880) ===
Q mean: -8.897728
Q std: 10.376125
Actor loss: 8.901723
Action reg: 0.003995
  l1.weight: grad_norm = 0.013115
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.011301
Total gradient norm: 0.065128
=== Actor Training Debug (Iteration 1881) ===
Q mean: -8.607582
Q std: 10.343499
Actor loss: 8.611576
Action reg: 0.003994
  l1.weight: grad_norm = 0.007120
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.006480
Total gradient norm: 0.037770
=== Actor Training Debug (Iteration 1882) ===
Q mean: -8.564604
Q std: 10.837456
Actor loss: 8.568599
Action reg: 0.003995
  l1.weight: grad_norm = 0.026681
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.018369
Total gradient norm: 0.088901
=== Actor Training Debug (Iteration 1883) ===
Q mean: -9.317801
Q std: 10.923672
Actor loss: 9.321795
Action reg: 0.003994
  l1.weight: grad_norm = 0.007355
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005616
Total gradient norm: 0.043561
=== Actor Training Debug (Iteration 1884) ===
Q mean: -8.327228
Q std: 10.432270
Actor loss: 8.331217
Action reg: 0.003989
  l1.weight: grad_norm = 0.017621
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.014409
Total gradient norm: 0.046862
=== Actor Training Debug (Iteration 1885) ===
Q mean: -8.859741
Q std: 10.601830
Actor loss: 8.863736
Action reg: 0.003995
  l1.weight: grad_norm = 0.025881
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.021133
Total gradient norm: 0.096410
=== Actor Training Debug (Iteration 1886) ===
Q mean: -8.108871
Q std: 10.359277
Actor loss: 8.112865
Action reg: 0.003994
  l1.weight: grad_norm = 0.014745
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.011145
Total gradient norm: 0.074998
=== Actor Training Debug (Iteration 1887) ===
Q mean: -10.878252
Q std: 11.301157
Actor loss: 10.882244
Action reg: 0.003992
  l1.weight: grad_norm = 0.015698
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.012650
Total gradient norm: 0.085551
=== Actor Training Debug (Iteration 1888) ===
Q mean: -8.651465
Q std: 10.432036
Actor loss: 8.655458
Action reg: 0.003993
  l1.weight: grad_norm = 0.012218
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.010578
Total gradient norm: 0.064999
=== Actor Training Debug (Iteration 1889) ===
Q mean: -9.254538
Q std: 10.764318
Actor loss: 9.258533
Action reg: 0.003996
  l1.weight: grad_norm = 0.001640
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001303
Total gradient norm: 0.007383
=== Actor Training Debug (Iteration 1890) ===
Q mean: -9.096277
Q std: 10.626630
Actor loss: 9.100266
Action reg: 0.003988
  l1.weight: grad_norm = 0.041201
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.030808
Total gradient norm: 0.163595
=== Actor Training Debug (Iteration 1891) ===
Q mean: -7.977561
Q std: 9.589908
Actor loss: 7.981555
Action reg: 0.003993
  l1.weight: grad_norm = 0.012845
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.010853
Total gradient norm: 0.049936
=== Actor Training Debug (Iteration 1892) ===
Q mean: -7.853720
Q std: 9.758494
Actor loss: 7.857711
Action reg: 0.003991
  l1.weight: grad_norm = 0.023189
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.016889
Total gradient norm: 0.104198
=== Actor Training Debug (Iteration 1893) ===
Q mean: -9.775133
Q std: 10.996668
Actor loss: 9.779127
Action reg: 0.003994
  l1.weight: grad_norm = 0.012217
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.011393
Total gradient norm: 0.069389
=== Actor Training Debug (Iteration 1894) ===
Q mean: -8.205173
Q std: 10.344125
Actor loss: 8.209168
Action reg: 0.003995
  l1.weight: grad_norm = 0.003464
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002629
Total gradient norm: 0.013591
=== Actor Training Debug (Iteration 1895) ===
Q mean: -7.770603
Q std: 10.086460
Actor loss: 7.774591
Action reg: 0.003988
  l1.weight: grad_norm = 0.019308
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.019340
Total gradient norm: 0.094724
=== Actor Training Debug (Iteration 1896) ===
Q mean: -8.990294
Q std: 10.671810
Actor loss: 8.994287
Action reg: 0.003994
  l1.weight: grad_norm = 0.021563
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.016379
Total gradient norm: 0.072078
=== Actor Training Debug (Iteration 1897) ===
Q mean: -8.119831
Q std: 9.947734
Actor loss: 8.123820
Action reg: 0.003989
  l1.weight: grad_norm = 0.042014
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.028598
Total gradient norm: 0.128162
=== Actor Training Debug (Iteration 1898) ===
Q mean: -8.493386
Q std: 10.202888
Actor loss: 8.497378
Action reg: 0.003993
  l1.weight: grad_norm = 0.008008
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.005278
Total gradient norm: 0.027803
=== Actor Training Debug (Iteration 1899) ===
Q mean: -8.336597
Q std: 9.885747
Actor loss: 8.340590
Action reg: 0.003992
  l1.weight: grad_norm = 0.020809
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.018866
Total gradient norm: 0.063549
=== Actor Training Debug (Iteration 1900) ===
Q mean: -8.469365
Q std: 10.746675
Actor loss: 8.473357
Action reg: 0.003992
  l1.weight: grad_norm = 0.015574
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.012291
Total gradient norm: 0.054147
=== Actor Training Debug (Iteration 1901) ===
Q mean: -7.651503
Q std: 9.785224
Actor loss: 7.655491
Action reg: 0.003989
  l1.weight: grad_norm = 0.021816
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.016632
Total gradient norm: 0.061982
=== Actor Training Debug (Iteration 1902) ===
Q mean: -9.251827
Q std: 10.588967
Actor loss: 9.255821
Action reg: 0.003994
  l1.weight: grad_norm = 0.029849
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.024969
Total gradient norm: 0.074203
=== Actor Training Debug (Iteration 1903) ===
Q mean: -8.887481
Q std: 10.599342
Actor loss: 8.891478
Action reg: 0.003997
  l1.weight: grad_norm = 0.003210
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002975
Total gradient norm: 0.014512
=== Actor Training Debug (Iteration 1904) ===
Q mean: -8.879994
Q std: 10.992402
Actor loss: 8.883986
Action reg: 0.003992
  l1.weight: grad_norm = 0.007899
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005451
Total gradient norm: 0.026000
=== Actor Training Debug (Iteration 1905) ===
Q mean: -9.338520
Q std: 11.398577
Actor loss: 9.342518
Action reg: 0.003998
  l1.weight: grad_norm = 0.001356
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001165
Total gradient norm: 0.006849
=== Actor Training Debug (Iteration 1906) ===
Q mean: -8.753115
Q std: 10.782174
Actor loss: 8.757109
Action reg: 0.003994
  l1.weight: grad_norm = 0.010702
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.008494
Total gradient norm: 0.050042
=== Actor Training Debug (Iteration 1907) ===
Q mean: -8.782309
Q std: 10.884493
Actor loss: 8.786294
Action reg: 0.003986
  l1.weight: grad_norm = 0.031566
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.029275
Total gradient norm: 0.154374
=== Actor Training Debug (Iteration 1908) ===
Q mean: -8.647098
Q std: 10.288004
Actor loss: 8.651088
Action reg: 0.003990
  l1.weight: grad_norm = 0.022804
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.019276
Total gradient norm: 0.097577
=== Actor Training Debug (Iteration 1909) ===
Q mean: -8.800624
Q std: 10.608843
Actor loss: 8.804610
Action reg: 0.003987
  l1.weight: grad_norm = 0.027941
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.025699
Total gradient norm: 0.215628
=== Actor Training Debug (Iteration 1910) ===
Q mean: -7.854371
Q std: 10.162370
Actor loss: 7.858365
Action reg: 0.003995
  l1.weight: grad_norm = 0.001916
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001848
Total gradient norm: 0.007432
=== Actor Training Debug (Iteration 1911) ===
Q mean: -9.068010
Q std: 11.062400
Actor loss: 9.072003
Action reg: 0.003993
  l1.weight: grad_norm = 0.003299
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002430
Total gradient norm: 0.011644
=== Actor Training Debug (Iteration 1912) ===
Q mean: -9.488615
Q std: 11.457214
Actor loss: 9.492600
Action reg: 0.003985
  l1.weight: grad_norm = 0.007838
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.006308
Total gradient norm: 0.024866
=== Actor Training Debug (Iteration 1913) ===
Q mean: -9.374517
Q std: 10.756759
Actor loss: 9.378510
Action reg: 0.003992
  l1.weight: grad_norm = 0.012566
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010820
Total gradient norm: 0.060523
=== Actor Training Debug (Iteration 1914) ===
Q mean: -7.669654
Q std: 9.830625
Actor loss: 7.673649
Action reg: 0.003995
  l1.weight: grad_norm = 0.024391
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.018098
Total gradient norm: 0.093065
=== Actor Training Debug (Iteration 1915) ===
Q mean: -8.754374
Q std: 10.095316
Actor loss: 8.758359
Action reg: 0.003986
  l1.weight: grad_norm = 0.025247
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.022416
Total gradient norm: 0.094132
=== Actor Training Debug (Iteration 1916) ===
Q mean: -8.687020
Q std: 9.927866
Actor loss: 8.691010
Action reg: 0.003990
  l1.weight: grad_norm = 0.016979
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.012822
Total gradient norm: 0.059833
=== Actor Training Debug (Iteration 1917) ===
Q mean: -9.249226
Q std: 11.113785
Actor loss: 9.253222
Action reg: 0.003996
  l1.weight: grad_norm = 0.013639
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.009790
Total gradient norm: 0.049104
=== Actor Training Debug (Iteration 1918) ===
Q mean: -8.577508
Q std: 10.763515
Actor loss: 8.581501
Action reg: 0.003993
  l1.weight: grad_norm = 0.011521
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.008760
Total gradient norm: 0.032383
=== Actor Training Debug (Iteration 1919) ===
Q mean: -9.759560
Q std: 11.282295
Actor loss: 9.763546
Action reg: 0.003986
  l1.weight: grad_norm = 0.023104
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.021179
Total gradient norm: 0.131894
=== Actor Training Debug (Iteration 1920) ===
Q mean: -9.319683
Q std: 10.984440
Actor loss: 9.323673
Action reg: 0.003990
  l1.weight: grad_norm = 0.020387
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.016205
Total gradient norm: 0.099425
=== Actor Training Debug (Iteration 1921) ===
Q mean: -7.699204
Q std: 9.774320
Actor loss: 7.703197
Action reg: 0.003993
  l1.weight: grad_norm = 0.010360
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.007210
Total gradient norm: 0.041169
=== Actor Training Debug (Iteration 1922) ===
Q mean: -9.891956
Q std: 11.037016
Actor loss: 9.895952
Action reg: 0.003996
  l1.weight: grad_norm = 0.002204
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001936
Total gradient norm: 0.009776
=== Actor Training Debug (Iteration 1923) ===
Q mean: -8.916671
Q std: 10.617803
Actor loss: 8.920669
Action reg: 0.003998
  l1.weight: grad_norm = 0.003063
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.003087
Total gradient norm: 0.008327
=== Actor Training Debug (Iteration 1924) ===
Q mean: -9.153446
Q std: 11.044220
Actor loss: 9.157434
Action reg: 0.003988
  l1.weight: grad_norm = 0.033190
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.027235
Total gradient norm: 0.169068
=== Actor Training Debug (Iteration 1925) ===
Q mean: -8.935014
Q std: 10.571661
Actor loss: 8.939009
Action reg: 0.003995
  l1.weight: grad_norm = 0.001771
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001379
Total gradient norm: 0.004663
=== Actor Training Debug (Iteration 1926) ===
Q mean: -9.681934
Q std: 10.632339
Actor loss: 9.685931
Action reg: 0.003997
  l1.weight: grad_norm = 0.010856
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.008558
Total gradient norm: 0.039854
=== Actor Training Debug (Iteration 1927) ===
Q mean: -8.512825
Q std: 10.702479
Actor loss: 8.516817
Action reg: 0.003992
  l1.weight: grad_norm = 0.027972
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.021501
Total gradient norm: 0.143572
=== Actor Training Debug (Iteration 1928) ===
Q mean: -7.084664
Q std: 10.577542
Actor loss: 7.088658
Action reg: 0.003994
  l1.weight: grad_norm = 0.038972
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.036678
Total gradient norm: 0.100709
=== Actor Training Debug (Iteration 1929) ===
Q mean: -8.865017
Q std: 11.249305
Actor loss: 8.869008
Action reg: 0.003991
  l1.weight: grad_norm = 0.011192
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.008307
Total gradient norm: 0.037996
=== Actor Training Debug (Iteration 1930) ===
Q mean: -8.593204
Q std: 11.675421
Actor loss: 8.597199
Action reg: 0.003995
  l1.weight: grad_norm = 0.007566
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005427
Total gradient norm: 0.023081
=== Actor Training Debug (Iteration 1931) ===
Q mean: -8.876688
Q std: 10.455322
Actor loss: 8.880683
Action reg: 0.003995
  l1.weight: grad_norm = 0.007536
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005913
Total gradient norm: 0.031154
=== Actor Training Debug (Iteration 1932) ===
Q mean: -9.263817
Q std: 10.580276
Actor loss: 9.267816
Action reg: 0.003998
  l1.weight: grad_norm = 0.000234
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000220
Total gradient norm: 0.001298
=== Actor Training Debug (Iteration 1933) ===
Q mean: -9.836219
Q std: 11.201338
Actor loss: 9.840215
Action reg: 0.003996
  l1.weight: grad_norm = 0.002946
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002322
Total gradient norm: 0.009475
=== Actor Training Debug (Iteration 1934) ===
Q mean: -9.125614
Q std: 10.571964
Actor loss: 9.129605
Action reg: 0.003991
  l1.weight: grad_norm = 0.008768
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.006655
Total gradient norm: 0.026973
=== Actor Training Debug (Iteration 1935) ===
Q mean: -9.284082
Q std: 10.842071
Actor loss: 9.288074
Action reg: 0.003992
  l1.weight: grad_norm = 0.015770
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.013518
Total gradient norm: 0.069942
=== Actor Training Debug (Iteration 1936) ===
Q mean: -8.532581
Q std: 10.579310
Actor loss: 8.536579
Action reg: 0.003998
  l1.weight: grad_norm = 0.001712
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001319
Total gradient norm: 0.008470
=== Actor Training Debug (Iteration 1937) ===
Q mean: -9.324083
Q std: 11.246523
Actor loss: 9.328076
Action reg: 0.003993
  l1.weight: grad_norm = 0.021504
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.016964
Total gradient norm: 0.122170
=== Actor Training Debug (Iteration 1938) ===
Q mean: -9.055455
Q std: 10.734518
Actor loss: 9.059448
Action reg: 0.003993
  l1.weight: grad_norm = 0.011075
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.008237
Total gradient norm: 0.049570
=== Actor Training Debug (Iteration 1939) ===
Q mean: -8.123517
Q std: 9.767585
Actor loss: 8.127513
Action reg: 0.003996
  l1.weight: grad_norm = 0.019518
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.015315
Total gradient norm: 0.095412
=== Actor Training Debug (Iteration 1940) ===
Q mean: -8.373458
Q std: 10.438020
Actor loss: 8.377451
Action reg: 0.003993
  l1.weight: grad_norm = 0.004495
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004070
Total gradient norm: 0.026720
=== Actor Training Debug (Iteration 1941) ===
Q mean: -9.260454
Q std: 10.689826
Actor loss: 9.264453
Action reg: 0.003999
  l1.weight: grad_norm = 0.000631
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000643
Total gradient norm: 0.003613
=== Actor Training Debug (Iteration 1942) ===
Q mean: -9.192198
Q std: 11.188285
Actor loss: 9.196195
Action reg: 0.003997
  l1.weight: grad_norm = 0.000507
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000417
Total gradient norm: 0.001702
=== Actor Training Debug (Iteration 1943) ===
Q mean: -9.830975
Q std: 11.160374
Actor loss: 9.834971
Action reg: 0.003997
  l1.weight: grad_norm = 0.003191
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.003140
Total gradient norm: 0.017593
=== Actor Training Debug (Iteration 1944) ===
Q mean: -8.902510
Q std: 10.636430
Actor loss: 8.906507
Action reg: 0.003997
  l1.weight: grad_norm = 0.000656
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000584
Total gradient norm: 0.002619
=== Actor Training Debug (Iteration 1945) ===
Q mean: -8.337801
Q std: 9.803103
Actor loss: 8.341793
Action reg: 0.003992
  l1.weight: grad_norm = 0.027850
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.026219
Total gradient norm: 0.120117
=== Actor Training Debug (Iteration 1946) ===
Q mean: -9.273310
Q std: 10.969831
Actor loss: 9.277306
Action reg: 0.003996
  l1.weight: grad_norm = 0.002722
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002130
Total gradient norm: 0.013662
=== Actor Training Debug (Iteration 1947) ===
Q mean: -9.242355
Q std: 10.967945
Actor loss: 9.246347
Action reg: 0.003992
  l1.weight: grad_norm = 0.015307
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.013079
Total gradient norm: 0.096492
=== Actor Training Debug (Iteration 1948) ===
Q mean: -8.068125
Q std: 10.724540
Actor loss: 8.072119
Action reg: 0.003994
  l1.weight: grad_norm = 0.013814
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.009758
Total gradient norm: 0.043758
=== Actor Training Debug (Iteration 1949) ===
Q mean: -10.392330
Q std: 11.621933
Actor loss: 10.396324
Action reg: 0.003994
  l1.weight: grad_norm = 0.015261
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.011197
Total gradient norm: 0.070858
=== Actor Training Debug (Iteration 1950) ===
Q mean: -8.905085
Q std: 11.062029
Actor loss: 8.909078
Action reg: 0.003993
  l1.weight: grad_norm = 0.017630
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.014224
Total gradient norm: 0.054824
=== Actor Training Debug (Iteration 1951) ===
Q mean: -8.878312
Q std: 10.629352
Actor loss: 8.882305
Action reg: 0.003993
  l1.weight: grad_norm = 0.014430
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.013951
Total gradient norm: 0.044635
=== Actor Training Debug (Iteration 1952) ===
Q mean: -8.470388
Q std: 10.103386
Actor loss: 8.474383
Action reg: 0.003995
  l1.weight: grad_norm = 0.003033
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002320
Total gradient norm: 0.010332
=== Actor Training Debug (Iteration 1953) ===
Q mean: -9.246237
Q std: 10.762726
Actor loss: 9.250229
Action reg: 0.003993
  l1.weight: grad_norm = 0.028129
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.021424
Total gradient norm: 0.116228
=== Actor Training Debug (Iteration 1954) ===
Q mean: -8.126079
Q std: 10.610827
Actor loss: 8.130075
Action reg: 0.003996
  l1.weight: grad_norm = 0.004324
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003406
Total gradient norm: 0.015381
=== Actor Training Debug (Iteration 1955) ===
Q mean: -8.722649
Q std: 11.490060
Actor loss: 8.726643
Action reg: 0.003994
  l1.weight: grad_norm = 0.013914
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.011328
Total gradient norm: 0.033119
=== Actor Training Debug (Iteration 1956) ===
Q mean: -8.806107
Q std: 10.916412
Actor loss: 8.810101
Action reg: 0.003994
  l1.weight: grad_norm = 0.012715
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.011191
Total gradient norm: 0.052162
=== Actor Training Debug (Iteration 1957) ===
Q mean: -8.338785
Q std: 10.593384
Actor loss: 8.342777
Action reg: 0.003992
  l1.weight: grad_norm = 0.017021
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.011521
Total gradient norm: 0.062952
=== Actor Training Debug (Iteration 1958) ===
Q mean: -9.028694
Q std: 10.967588
Actor loss: 9.032688
Action reg: 0.003994
  l1.weight: grad_norm = 0.027300
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.019428
Total gradient norm: 0.104531
=== Actor Training Debug (Iteration 1959) ===
Q mean: -9.766382
Q std: 10.763096
Actor loss: 9.770378
Action reg: 0.003996
  l1.weight: grad_norm = 0.006440
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005196
Total gradient norm: 0.022032
=== Actor Training Debug (Iteration 1960) ===
Q mean: -10.663427
Q std: 11.707059
Actor loss: 10.667420
Action reg: 0.003993
  l1.weight: grad_norm = 0.015974
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.015965
Total gradient norm: 0.086637
=== Actor Training Debug (Iteration 1961) ===
Q mean: -8.805227
Q std: 10.984491
Actor loss: 8.809224
Action reg: 0.003997
  l1.weight: grad_norm = 0.003490
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003105
Total gradient norm: 0.011869
=== Actor Training Debug (Iteration 1962) ===
Q mean: -7.591639
Q std: 10.486557
Actor loss: 7.595629
Action reg: 0.003990
  l1.weight: grad_norm = 0.018266
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.016679
Total gradient norm: 0.059173
=== Actor Training Debug (Iteration 1963) ===
Q mean: -8.877481
Q std: 10.648967
Actor loss: 8.881474
Action reg: 0.003993
  l1.weight: grad_norm = 0.016140
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.013800
Total gradient norm: 0.058272
=== Actor Training Debug (Iteration 1964) ===
Q mean: -8.294132
Q std: 10.589294
Actor loss: 8.298129
Action reg: 0.003997
  l1.weight: grad_norm = 0.000392
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000322
Total gradient norm: 0.001218
=== Actor Training Debug (Iteration 1965) ===
Q mean: -8.844197
Q std: 10.762173
Actor loss: 8.848187
Action reg: 0.003990
  l1.weight: grad_norm = 0.019024
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.017233
Total gradient norm: 0.061902
=== Actor Training Debug (Iteration 1966) ===
Q mean: -8.532100
Q std: 10.759636
Actor loss: 8.536094
Action reg: 0.003994
  l1.weight: grad_norm = 0.004897
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004348
Total gradient norm: 0.024342
=== Actor Training Debug (Iteration 1967) ===
Q mean: -8.061247
Q std: 10.566907
Actor loss: 8.065242
Action reg: 0.003995
  l1.weight: grad_norm = 0.005213
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.004492
Total gradient norm: 0.020214
=== Actor Training Debug (Iteration 1968) ===
Q mean: -8.690691
Q std: 10.816753
Actor loss: 8.694689
Action reg: 0.003997
  l1.weight: grad_norm = 0.005756
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.004240
Total gradient norm: 0.018409
=== Actor Training Debug (Iteration 1969) ===
Q mean: -8.829169
Q std: 11.035822
Actor loss: 8.833164
Action reg: 0.003995
  l1.weight: grad_norm = 0.016533
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.013336
Total gradient norm: 0.038494
=== Actor Training Debug (Iteration 1970) ===
Q mean: -8.553146
Q std: 10.436987
Actor loss: 8.557143
Action reg: 0.003996
  l1.weight: grad_norm = 0.001051
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000972
Total gradient norm: 0.004433
=== Actor Training Debug (Iteration 1971) ===
Q mean: -9.398560
Q std: 10.811280
Actor loss: 9.402553
Action reg: 0.003993
  l1.weight: grad_norm = 0.007372
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.007939
Total gradient norm: 0.045404
=== Actor Training Debug (Iteration 1972) ===
Q mean: -8.435917
Q std: 10.779220
Actor loss: 8.439912
Action reg: 0.003995
  l1.weight: grad_norm = 0.004453
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.004278
Total gradient norm: 0.014183
=== Actor Training Debug (Iteration 1973) ===
Q mean: -9.484526
Q std: 11.141942
Actor loss: 9.488518
Action reg: 0.003992
  l1.weight: grad_norm = 0.012048
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.010286
Total gradient norm: 0.067467
=== Actor Training Debug (Iteration 1974) ===
Q mean: -8.990156
Q std: 11.285007
Actor loss: 8.994145
Action reg: 0.003989
  l1.weight: grad_norm = 0.031100
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.028728
Total gradient norm: 0.139151
=== Actor Training Debug (Iteration 1975) ===
Q mean: -9.827894
Q std: 11.289445
Actor loss: 9.831883
Action reg: 0.003989
  l1.weight: grad_norm = 0.011540
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.011329
Total gradient norm: 0.062050
=== Actor Training Debug (Iteration 1976) ===
Q mean: -9.041230
Q std: 10.918930
Actor loss: 9.045225
Action reg: 0.003995
  l1.weight: grad_norm = 0.001735
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001591
Total gradient norm: 0.008311
=== Actor Training Debug (Iteration 1977) ===
Q mean: -8.585428
Q std: 10.535899
Actor loss: 8.589419
Action reg: 0.003991
  l1.weight: grad_norm = 0.012066
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.010843
Total gradient norm: 0.032488
=== Actor Training Debug (Iteration 1978) ===
Q mean: -9.417629
Q std: 11.635080
Actor loss: 9.421626
Action reg: 0.003997
  l1.weight: grad_norm = 0.026550
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.023512
Total gradient norm: 0.074031
=== Actor Training Debug (Iteration 1979) ===
Q mean: -9.587004
Q std: 10.884727
Actor loss: 9.591000
Action reg: 0.003996
  l1.weight: grad_norm = 0.009943
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010095
Total gradient norm: 0.057773
=== Actor Training Debug (Iteration 1980) ===
Q mean: -9.298328
Q std: 10.927761
Actor loss: 9.302324
Action reg: 0.003996
  l1.weight: grad_norm = 0.023233
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.019521
Total gradient norm: 0.063021
=== Actor Training Debug (Iteration 1981) ===
Q mean: -9.039124
Q std: 11.136731
Actor loss: 9.043118
Action reg: 0.003994
  l1.weight: grad_norm = 0.022201
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.018656
Total gradient norm: 0.106326
=== Actor Training Debug (Iteration 1982) ===
Q mean: -9.032261
Q std: 10.833094
Actor loss: 9.036259
Action reg: 0.003998
  l1.weight: grad_norm = 0.004405
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003812
Total gradient norm: 0.018014
=== Actor Training Debug (Iteration 1983) ===
Q mean: -7.943374
Q std: 10.231920
Actor loss: 7.947371
Action reg: 0.003997
  l1.weight: grad_norm = 0.000102
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.000082
Total gradient norm: 0.000466
=== Actor Training Debug (Iteration 1984) ===
Q mean: -8.207924
Q std: 10.176573
Actor loss: 8.211913
Action reg: 0.003990
  l1.weight: grad_norm = 0.025158
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.021171
Total gradient norm: 0.066832
=== Actor Training Debug (Iteration 1985) ===
Q mean: -8.961836
Q std: 10.486681
Actor loss: 8.965835
Action reg: 0.003998
  l1.weight: grad_norm = 0.003316
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002871
Total gradient norm: 0.008909
=== Actor Training Debug (Iteration 1986) ===
Q mean: -9.414791
Q std: 10.738452
Actor loss: 9.418786
Action reg: 0.003995
  l1.weight: grad_norm = 0.005924
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.005574
Total gradient norm: 0.019874
=== Actor Training Debug (Iteration 1987) ===
Q mean: -10.840919
Q std: 11.610920
Actor loss: 10.844910
Action reg: 0.003990
  l1.weight: grad_norm = 0.019693
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.017036
Total gradient norm: 0.111866
=== Actor Training Debug (Iteration 1988) ===
Q mean: -8.764206
Q std: 10.773731
Actor loss: 8.768203
Action reg: 0.003997
  l1.weight: grad_norm = 0.001374
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001336
Total gradient norm: 0.005046
=== Actor Training Debug (Iteration 1989) ===
Q mean: -8.362429
Q std: 10.629889
Actor loss: 8.366426
Action reg: 0.003998
  l1.weight: grad_norm = 0.022451
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.018174
Total gradient norm: 0.051094
=== Actor Training Debug (Iteration 1990) ===
Q mean: -9.018405
Q std: 10.925973
Actor loss: 9.022399
Action reg: 0.003994
  l1.weight: grad_norm = 0.011078
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.009424
Total gradient norm: 0.044588
=== Actor Training Debug (Iteration 1991) ===
Q mean: -9.307213
Q std: 11.307618
Actor loss: 9.311209
Action reg: 0.003996
  l1.weight: grad_norm = 0.009097
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.008599
Total gradient norm: 0.043099
=== Actor Training Debug (Iteration 1992) ===
Q mean: -9.784050
Q std: 11.422729
Actor loss: 9.788044
Action reg: 0.003994
  l1.weight: grad_norm = 0.018089
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.015631
Total gradient norm: 0.122700
=== Actor Training Debug (Iteration 1993) ===
Q mean: -9.941663
Q std: 11.190510
Actor loss: 9.945655
Action reg: 0.003992
  l1.weight: grad_norm = 0.013103
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010707
Total gradient norm: 0.034788
=== Actor Training Debug (Iteration 1994) ===
Q mean: -10.088409
Q std: 11.554649
Actor loss: 10.092402
Action reg: 0.003992
  l1.weight: grad_norm = 0.014384
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.012561
Total gradient norm: 0.071155
=== Actor Training Debug (Iteration 1995) ===
Q mean: -8.404134
Q std: 10.591425
Actor loss: 8.408131
Action reg: 0.003996
  l1.weight: grad_norm = 0.003678
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.003282
Total gradient norm: 0.021191
=== Actor Training Debug (Iteration 1996) ===
Q mean: -8.031204
Q std: 10.537495
Actor loss: 8.035195
Action reg: 0.003991
  l1.weight: grad_norm = 0.010144
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.008993
Total gradient norm: 0.035329
=== Actor Training Debug (Iteration 1997) ===
Q mean: -8.319962
Q std: 10.513308
Actor loss: 8.323952
Action reg: 0.003990
  l1.weight: grad_norm = 0.010537
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.007422
Total gradient norm: 0.030199
=== Actor Training Debug (Iteration 1998) ===
Q mean: -8.020681
Q std: 10.474195
Actor loss: 8.024672
Action reg: 0.003990
  l1.weight: grad_norm = 0.021293
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.017643
Total gradient norm: 0.068276
=== Actor Training Debug (Iteration 1999) ===
Q mean: -9.863195
Q std: 10.986860
Actor loss: 9.867186
Action reg: 0.003990
  l1.weight: grad_norm = 0.025808
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.022381
Total gradient norm: 0.112192
=== Actor Training Debug (Iteration 2000) ===
Q mean: -9.896216
Q std: 11.353988
Actor loss: 9.900210
Action reg: 0.003994
  l1.weight: grad_norm = 0.002355
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002368
Total gradient norm: 0.010559
Step 7000: Critic Loss: 1.2113, Actor Loss: 9.9002, Q Value: -9.8962
  Average reward: -358.250 | Average length: 100.0
Evaluation at episode 70: -358.250
=== Actor Training Debug (Iteration 2001) ===
Q mean: -9.242920
Q std: 10.936259
Actor loss: 9.246910
Action reg: 0.003990
  l1.weight: grad_norm = 0.053357
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.051063
Total gradient norm: 0.147707
=== Actor Training Debug (Iteration 2002) ===
Q mean: -8.072865
Q std: 10.664626
Actor loss: 8.076860
Action reg: 0.003995
  l1.weight: grad_norm = 0.017887
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.013261
Total gradient norm: 0.077919
=== Actor Training Debug (Iteration 2003) ===
Q mean: -6.967310
Q std: 9.745748
Actor loss: 6.971304
Action reg: 0.003994
  l1.weight: grad_norm = 0.026355
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.025609
Total gradient norm: 0.071456
=== Actor Training Debug (Iteration 2004) ===
Q mean: -8.978527
Q std: 10.432449
Actor loss: 8.982523
Action reg: 0.003996
  l1.weight: grad_norm = 0.017830
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.015852
Total gradient norm: 0.047042
=== Actor Training Debug (Iteration 2005) ===
Q mean: -10.160632
Q std: 11.642382
Actor loss: 10.164627
Action reg: 0.003995
  l1.weight: grad_norm = 0.002452
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001904
Total gradient norm: 0.008040
=== Actor Training Debug (Iteration 2006) ===
Q mean: -10.047689
Q std: 11.249680
Actor loss: 10.051682
Action reg: 0.003992
  l1.weight: grad_norm = 0.006004
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.004579
Total gradient norm: 0.026851
=== Actor Training Debug (Iteration 2007) ===
Q mean: -9.064487
Q std: 10.737404
Actor loss: 9.068482
Action reg: 0.003995
  l1.weight: grad_norm = 0.031664
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.021106
Total gradient norm: 0.113102
=== Actor Training Debug (Iteration 2008) ===
Q mean: -8.867454
Q std: 10.684216
Actor loss: 8.871446
Action reg: 0.003992
  l1.weight: grad_norm = 0.016814
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.015553
Total gradient norm: 0.062664
=== Actor Training Debug (Iteration 2009) ===
Q mean: -9.605316
Q std: 11.353749
Actor loss: 9.609311
Action reg: 0.003995
  l1.weight: grad_norm = 0.008487
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.008574
Total gradient norm: 0.026224
=== Actor Training Debug (Iteration 2010) ===
Q mean: -9.129894
Q std: 11.185888
Actor loss: 9.133889
Action reg: 0.003995
  l1.weight: grad_norm = 0.022907
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.019284
Total gradient norm: 0.081412
=== Actor Training Debug (Iteration 2011) ===
Q mean: -9.481634
Q std: 11.338217
Actor loss: 9.485627
Action reg: 0.003993
  l1.weight: grad_norm = 0.016666
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.012628
Total gradient norm: 0.064923
=== Actor Training Debug (Iteration 2012) ===
Q mean: -9.040761
Q std: 10.846014
Actor loss: 9.044752
Action reg: 0.003991
  l1.weight: grad_norm = 0.005402
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.004654
Total gradient norm: 0.016927
=== Actor Training Debug (Iteration 2013) ===
Q mean: -10.377628
Q std: 11.540359
Actor loss: 10.381621
Action reg: 0.003993
  l1.weight: grad_norm = 0.020179
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.017929
Total gradient norm: 0.060468
=== Actor Training Debug (Iteration 2014) ===
Q mean: -9.366395
Q std: 11.481164
Actor loss: 9.370390
Action reg: 0.003995
  l1.weight: grad_norm = 0.027261
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.022475
Total gradient norm: 0.091856
=== Actor Training Debug (Iteration 2015) ===
Q mean: -8.491381
Q std: 11.288817
Actor loss: 8.495373
Action reg: 0.003992
  l1.weight: grad_norm = 0.011043
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.008386
Total gradient norm: 0.038790
=== Actor Training Debug (Iteration 2016) ===
Q mean: -8.932123
Q std: 11.198434
Actor loss: 8.936113
Action reg: 0.003990
  l1.weight: grad_norm = 0.008161
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.007358
Total gradient norm: 0.041305
=== Actor Training Debug (Iteration 2017) ===
Q mean: -9.259609
Q std: 11.543017
Actor loss: 9.263600
Action reg: 0.003991
  l1.weight: grad_norm = 0.010084
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.008250
Total gradient norm: 0.050551
=== Actor Training Debug (Iteration 2018) ===
Q mean: -10.030503
Q std: 11.375363
Actor loss: 10.034498
Action reg: 0.003995
  l1.weight: grad_norm = 0.019533
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.016551
Total gradient norm: 0.079485
=== Actor Training Debug (Iteration 2019) ===
Q mean: -8.841775
Q std: 10.974907
Actor loss: 8.845766
Action reg: 0.003991
  l1.weight: grad_norm = 0.020191
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.017340
Total gradient norm: 0.079800
=== Actor Training Debug (Iteration 2020) ===
Q mean: -8.994495
Q std: 11.396590
Actor loss: 8.998488
Action reg: 0.003993
  l1.weight: grad_norm = 0.010298
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.008302
Total gradient norm: 0.028732
=== Actor Training Debug (Iteration 2021) ===
Q mean: -9.018906
Q std: 10.727168
Actor loss: 9.022893
Action reg: 0.003987
  l1.weight: grad_norm = 0.013529
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.012237
Total gradient norm: 0.081853
=== Actor Training Debug (Iteration 2022) ===
Q mean: -8.592564
Q std: 10.871846
Actor loss: 8.596556
Action reg: 0.003992
  l1.weight: grad_norm = 0.026762
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.018923
Total gradient norm: 0.069454
=== Actor Training Debug (Iteration 2023) ===
Q mean: -8.815315
Q std: 11.216884
Actor loss: 8.819310
Action reg: 0.003995
  l1.weight: grad_norm = 0.004045
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002978
Total gradient norm: 0.014786
=== Actor Training Debug (Iteration 2024) ===
Q mean: -7.649453
Q std: 10.289065
Actor loss: 7.653444
Action reg: 0.003991
  l1.weight: grad_norm = 0.020885
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.013813
Total gradient norm: 0.052843
=== Actor Training Debug (Iteration 2025) ===
Q mean: -10.091021
Q std: 11.029258
Actor loss: 10.095011
Action reg: 0.003990
  l1.weight: grad_norm = 0.039527
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.035399
Total gradient norm: 0.106305
=== Actor Training Debug (Iteration 2026) ===
Q mean: -9.840017
Q std: 11.179641
Actor loss: 9.844008
Action reg: 0.003991
  l1.weight: grad_norm = 0.015700
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.014058
Total gradient norm: 0.079419
=== Actor Training Debug (Iteration 2027) ===
Q mean: -9.112247
Q std: 10.726911
Actor loss: 9.116237
Action reg: 0.003989
  l1.weight: grad_norm = 0.018693
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.019894
Total gradient norm: 0.066829
=== Actor Training Debug (Iteration 2028) ===
Q mean: -9.231524
Q std: 10.903349
Actor loss: 9.235518
Action reg: 0.003994
  l1.weight: grad_norm = 0.008931
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.007238
Total gradient norm: 0.022116
=== Actor Training Debug (Iteration 2029) ===
Q mean: -8.494417
Q std: 11.654614
Actor loss: 8.498411
Action reg: 0.003994
  l1.weight: grad_norm = 0.019555
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.017436
Total gradient norm: 0.079151
=== Actor Training Debug (Iteration 2030) ===
Q mean: -7.631481
Q std: 10.211402
Actor loss: 7.635476
Action reg: 0.003995
  l1.weight: grad_norm = 0.001715
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.001469
Total gradient norm: 0.008340
=== Actor Training Debug (Iteration 2031) ===
Q mean: -8.566779
Q std: 11.158997
Actor loss: 8.570769
Action reg: 0.003990
  l1.weight: grad_norm = 0.018627
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.014556
Total gradient norm: 0.085262
=== Actor Training Debug (Iteration 2032) ===
Q mean: -9.466543
Q std: 11.565444
Actor loss: 9.470536
Action reg: 0.003993
  l1.weight: grad_norm = 0.026337
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.022038
Total gradient norm: 0.061893
=== Actor Training Debug (Iteration 2033) ===
Q mean: -8.499820
Q std: 10.776921
Actor loss: 8.503815
Action reg: 0.003995
  l1.weight: grad_norm = 0.007466
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.006856
Total gradient norm: 0.034640
=== Actor Training Debug (Iteration 2034) ===
Q mean: -8.477215
Q std: 10.703094
Actor loss: 8.481204
Action reg: 0.003989
  l1.weight: grad_norm = 0.011912
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.008486
Total gradient norm: 0.030651
=== Actor Training Debug (Iteration 2035) ===
Q mean: -9.107483
Q std: 10.650227
Actor loss: 9.111483
Action reg: 0.003999
  l1.weight: grad_norm = 0.001250
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001283
Total gradient norm: 0.003892
=== Actor Training Debug (Iteration 2036) ===
Q mean: -9.141256
Q std: 11.268557
Actor loss: 9.145250
Action reg: 0.003994
  l1.weight: grad_norm = 0.011073
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.008397
Total gradient norm: 0.036662
=== Actor Training Debug (Iteration 2037) ===
Q mean: -8.845354
Q std: 11.053649
Actor loss: 8.849351
Action reg: 0.003997
  l1.weight: grad_norm = 0.001567
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001441
Total gradient norm: 0.010606
=== Actor Training Debug (Iteration 2038) ===
Q mean: -9.335449
Q std: 11.598562
Actor loss: 9.339446
Action reg: 0.003996
  l1.weight: grad_norm = 0.001823
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.001443
Total gradient norm: 0.004813
=== Actor Training Debug (Iteration 2039) ===
Q mean: -8.354032
Q std: 10.760209
Actor loss: 8.358026
Action reg: 0.003994
  l1.weight: grad_norm = 0.002948
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.003368
Total gradient norm: 0.016902
=== Actor Training Debug (Iteration 2040) ===
Q mean: -10.233984
Q std: 11.842223
Actor loss: 10.237975
Action reg: 0.003992
  l1.weight: grad_norm = 0.021797
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.018596
Total gradient norm: 0.110344
=== Actor Training Debug (Iteration 2041) ===
Q mean: -10.086130
Q std: 11.606084
Actor loss: 10.090128
Action reg: 0.003998
  l1.weight: grad_norm = 0.001053
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000918
Total gradient norm: 0.004747
=== Actor Training Debug (Iteration 2042) ===
Q mean: -8.919939
Q std: 11.255882
Actor loss: 8.923934
Action reg: 0.003995
  l1.weight: grad_norm = 0.018762
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.016115
Total gradient norm: 0.057516
=== Actor Training Debug (Iteration 2043) ===
Q mean: -7.942088
Q std: 10.718103
Actor loss: 7.946081
Action reg: 0.003992
  l1.weight: grad_norm = 0.001674
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.001432
Total gradient norm: 0.004389
=== Actor Training Debug (Iteration 2044) ===
Q mean: -8.677803
Q std: 11.065121
Actor loss: 8.681798
Action reg: 0.003995
  l1.weight: grad_norm = 0.003890
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003458
Total gradient norm: 0.011103
=== Actor Training Debug (Iteration 2045) ===
Q mean: -8.417403
Q std: 10.875455
Actor loss: 8.421395
Action reg: 0.003992
  l1.weight: grad_norm = 0.014691
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.011460
Total gradient norm: 0.064773
=== Actor Training Debug (Iteration 2046) ===
Q mean: -9.124691
Q std: 11.125307
Actor loss: 9.128688
Action reg: 0.003997
  l1.weight: grad_norm = 0.003515
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.003327
Total gradient norm: 0.021287
=== Actor Training Debug (Iteration 2047) ===
Q mean: -10.470400
Q std: 11.585214
Actor loss: 10.474397
Action reg: 0.003997
  l1.weight: grad_norm = 0.007726
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006355
Total gradient norm: 0.031435
=== Actor Training Debug (Iteration 2048) ===
Q mean: -8.995358
Q std: 10.909919
Actor loss: 8.999350
Action reg: 0.003992
  l1.weight: grad_norm = 0.007265
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.006262
Total gradient norm: 0.025405
=== Actor Training Debug (Iteration 2049) ===
Q mean: -8.598526
Q std: 11.064996
Actor loss: 8.602524
Action reg: 0.003998
  l1.weight: grad_norm = 0.001825
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001852
Total gradient norm: 0.006615
=== Actor Training Debug (Iteration 2050) ===
Q mean: -7.880493
Q std: 10.362135
Actor loss: 7.884487
Action reg: 0.003994
  l1.weight: grad_norm = 0.017386
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.015506
Total gradient norm: 0.083283
=== Actor Training Debug (Iteration 2051) ===
Q mean: -9.502345
Q std: 11.313747
Actor loss: 9.506334
Action reg: 0.003989
  l1.weight: grad_norm = 0.031397
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.026012
Total gradient norm: 0.099144
=== Actor Training Debug (Iteration 2052) ===
Q mean: -10.008753
Q std: 11.461634
Actor loss: 10.012750
Action reg: 0.003997
  l1.weight: grad_norm = 0.011452
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.011801
Total gradient norm: 0.056773
=== Actor Training Debug (Iteration 2053) ===
Q mean: -11.019016
Q std: 11.191723
Actor loss: 11.023007
Action reg: 0.003991
  l1.weight: grad_norm = 0.025548
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.020189
Total gradient norm: 0.067414
=== Actor Training Debug (Iteration 2054) ===
Q mean: -9.839843
Q std: 11.180878
Actor loss: 9.843836
Action reg: 0.003993
  l1.weight: grad_norm = 0.009422
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.008000
Total gradient norm: 0.028861
=== Actor Training Debug (Iteration 2055) ===
Q mean: -9.517738
Q std: 11.800638
Actor loss: 9.521733
Action reg: 0.003995
  l1.weight: grad_norm = 0.001399
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001081
Total gradient norm: 0.005724
=== Actor Training Debug (Iteration 2056) ===
Q mean: -8.875923
Q std: 11.544963
Actor loss: 8.879914
Action reg: 0.003991
  l1.weight: grad_norm = 0.007087
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.005984
Total gradient norm: 0.020989
=== Actor Training Debug (Iteration 2057) ===
Q mean: -8.288162
Q std: 11.186988
Actor loss: 8.292155
Action reg: 0.003993
  l1.weight: grad_norm = 0.039523
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.031012
Total gradient norm: 0.148182
=== Actor Training Debug (Iteration 2058) ===
Q mean: -8.520657
Q std: 10.887731
Actor loss: 8.524649
Action reg: 0.003992
  l1.weight: grad_norm = 0.049596
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.045298
Total gradient norm: 0.146563
=== Actor Training Debug (Iteration 2059) ===
Q mean: -9.348553
Q std: 11.552377
Actor loss: 9.352551
Action reg: 0.003997
  l1.weight: grad_norm = 0.003263
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003103
Total gradient norm: 0.011086
=== Actor Training Debug (Iteration 2060) ===
Q mean: -9.476437
Q std: 11.244441
Actor loss: 9.480428
Action reg: 0.003991
  l1.weight: grad_norm = 0.024550
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.017502
Total gradient norm: 0.073568
=== Actor Training Debug (Iteration 2061) ===
Q mean: -7.847772
Q std: 10.239279
Actor loss: 7.851767
Action reg: 0.003995
  l1.weight: grad_norm = 0.018196
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.013396
Total gradient norm: 0.058874
=== Actor Training Debug (Iteration 2062) ===
Q mean: -9.506657
Q std: 11.539339
Actor loss: 9.510653
Action reg: 0.003996
  l1.weight: grad_norm = 0.012260
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.010031
Total gradient norm: 0.035615
=== Actor Training Debug (Iteration 2063) ===
Q mean: -8.538689
Q std: 10.795185
Actor loss: 8.542683
Action reg: 0.003994
  l1.weight: grad_norm = 0.025261
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.016877
Total gradient norm: 0.063024
=== Actor Training Debug (Iteration 2064) ===
Q mean: -9.093365
Q std: 11.554071
Actor loss: 9.097354
Action reg: 0.003990
  l1.weight: grad_norm = 0.022401
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.021002
Total gradient norm: 0.068212
=== Actor Training Debug (Iteration 2065) ===
Q mean: -9.268969
Q std: 11.525414
Actor loss: 9.272964
Action reg: 0.003996
  l1.weight: grad_norm = 0.000797
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000642
Total gradient norm: 0.003735
=== Actor Training Debug (Iteration 2066) ===
Q mean: -8.443970
Q std: 10.988726
Actor loss: 8.447968
Action reg: 0.003998
  l1.weight: grad_norm = 0.020775
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.021492
Total gradient norm: 0.058782
=== Actor Training Debug (Iteration 2067) ===
Q mean: -9.218580
Q std: 12.113039
Actor loss: 9.222571
Action reg: 0.003991
  l1.weight: grad_norm = 0.026931
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.021729
Total gradient norm: 0.113142
=== Actor Training Debug (Iteration 2068) ===
Q mean: -8.003205
Q std: 10.757351
Actor loss: 8.007195
Action reg: 0.003989
  l1.weight: grad_norm = 0.015748
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.013428
Total gradient norm: 0.064947
=== Actor Training Debug (Iteration 2069) ===
Q mean: -8.465117
Q std: 10.695815
Actor loss: 8.469110
Action reg: 0.003993
  l1.weight: grad_norm = 0.019546
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.013823
Total gradient norm: 0.058202
=== Actor Training Debug (Iteration 2070) ===
Q mean: -9.369373
Q std: 11.695034
Actor loss: 9.373365
Action reg: 0.003992
  l1.weight: grad_norm = 0.019317
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.016487
Total gradient norm: 0.071204
=== Actor Training Debug (Iteration 2071) ===
Q mean: -8.806581
Q std: 10.860026
Actor loss: 8.810574
Action reg: 0.003992
  l1.weight: grad_norm = 0.016525
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.013924
Total gradient norm: 0.072406
=== Actor Training Debug (Iteration 2072) ===
Q mean: -10.896477
Q std: 11.865797
Actor loss: 10.900474
Action reg: 0.003997
  l1.weight: grad_norm = 0.012815
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.010106
Total gradient norm: 0.035406
=== Actor Training Debug (Iteration 2073) ===
Q mean: -8.674955
Q std: 11.435162
Actor loss: 8.678944
Action reg: 0.003988
  l1.weight: grad_norm = 0.020303
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.014085
Total gradient norm: 0.067509
=== Actor Training Debug (Iteration 2074) ===
Q mean: -8.402236
Q std: 10.657816
Actor loss: 8.406228
Action reg: 0.003992
  l1.weight: grad_norm = 0.010727
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.008321
Total gradient norm: 0.027030
=== Actor Training Debug (Iteration 2075) ===
Q mean: -9.654537
Q std: 11.433623
Actor loss: 9.658529
Action reg: 0.003992
  l1.weight: grad_norm = 0.011489
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.009991
Total gradient norm: 0.058840
=== Actor Training Debug (Iteration 2076) ===
Q mean: -10.100023
Q std: 11.013079
Actor loss: 10.104020
Action reg: 0.003997
  l1.weight: grad_norm = 0.003663
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003664
Total gradient norm: 0.020806
=== Actor Training Debug (Iteration 2077) ===
Q mean: -9.060389
Q std: 10.823771
Actor loss: 9.064382
Action reg: 0.003993
  l1.weight: grad_norm = 0.058268
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.048103
Total gradient norm: 0.118124
=== Actor Training Debug (Iteration 2078) ===
Q mean: -9.891842
Q std: 11.435225
Actor loss: 9.895838
Action reg: 0.003996
  l1.weight: grad_norm = 0.001633
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001357
Total gradient norm: 0.005886
=== Actor Training Debug (Iteration 2079) ===
Q mean: -9.970881
Q std: 11.218349
Actor loss: 9.974873
Action reg: 0.003993
  l1.weight: grad_norm = 0.014878
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.012216
Total gradient norm: 0.056657
=== Actor Training Debug (Iteration 2080) ===
Q mean: -10.378755
Q std: 11.998531
Actor loss: 10.382746
Action reg: 0.003991
  l1.weight: grad_norm = 0.011879
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.008993
Total gradient norm: 0.038953
=== Actor Training Debug (Iteration 2081) ===
Q mean: -9.133505
Q std: 11.688637
Actor loss: 9.137492
Action reg: 0.003987
  l1.weight: grad_norm = 0.012294
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.009983
Total gradient norm: 0.040481
=== Actor Training Debug (Iteration 2082) ===
Q mean: -9.769217
Q std: 11.657301
Actor loss: 9.773209
Action reg: 0.003992
  l1.weight: grad_norm = 0.044754
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.040779
Total gradient norm: 0.146704
=== Actor Training Debug (Iteration 2083) ===
Q mean: -9.241303
Q std: 11.149847
Actor loss: 9.245298
Action reg: 0.003995
  l1.weight: grad_norm = 0.010371
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010065
Total gradient norm: 0.042295
=== Actor Training Debug (Iteration 2084) ===
Q mean: -8.883038
Q std: 11.064547
Actor loss: 8.887026
Action reg: 0.003988
  l1.weight: grad_norm = 0.016717
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.013650
Total gradient norm: 0.071504
=== Actor Training Debug (Iteration 2085) ===
Q mean: -8.980640
Q std: 11.228741
Actor loss: 8.984630
Action reg: 0.003989
  l1.weight: grad_norm = 0.014315
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.010930
Total gradient norm: 0.038950
=== Actor Training Debug (Iteration 2086) ===
Q mean: -8.865472
Q std: 11.211931
Actor loss: 8.869470
Action reg: 0.003998
  l1.weight: grad_norm = 0.001612
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001375
Total gradient norm: 0.008611
=== Actor Training Debug (Iteration 2087) ===
Q mean: -9.165833
Q std: 11.333199
Actor loss: 9.169821
Action reg: 0.003988
  l1.weight: grad_norm = 0.043083
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.028134
Total gradient norm: 0.091895
=== Actor Training Debug (Iteration 2088) ===
Q mean: -9.813154
Q std: 11.684148
Actor loss: 9.817143
Action reg: 0.003989
  l1.weight: grad_norm = 0.027636
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.023203
Total gradient norm: 0.100171
=== Actor Training Debug (Iteration 2089) ===
Q mean: -10.317632
Q std: 11.319135
Actor loss: 10.321627
Action reg: 0.003995
  l1.weight: grad_norm = 0.010808
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010201
Total gradient norm: 0.046264
=== Actor Training Debug (Iteration 2090) ===
Q mean: -8.826567
Q std: 11.331843
Actor loss: 8.830558
Action reg: 0.003991
  l1.weight: grad_norm = 0.010713
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.008539
Total gradient norm: 0.044514
=== Actor Training Debug (Iteration 2091) ===
Q mean: -9.621913
Q std: 10.950727
Actor loss: 9.625905
Action reg: 0.003992
  l1.weight: grad_norm = 0.036648
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.028523
Total gradient norm: 0.082466
=== Actor Training Debug (Iteration 2092) ===
Q mean: -8.329582
Q std: 10.459429
Actor loss: 8.333575
Action reg: 0.003993
  l1.weight: grad_norm = 0.006076
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.004637
Total gradient norm: 0.018941
=== Actor Training Debug (Iteration 2093) ===
Q mean: -9.981924
Q std: 11.530747
Actor loss: 9.985913
Action reg: 0.003989
  l1.weight: grad_norm = 0.024572
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.018042
Total gradient norm: 0.077128
=== Actor Training Debug (Iteration 2094) ===
Q mean: -8.313986
Q std: 11.048120
Actor loss: 8.317976
Action reg: 0.003990
  l1.weight: grad_norm = 0.050704
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.043499
Total gradient norm: 0.122310
=== Actor Training Debug (Iteration 2095) ===
Q mean: -7.589968
Q std: 10.527597
Actor loss: 7.593959
Action reg: 0.003991
  l1.weight: grad_norm = 0.024464
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.020372
Total gradient norm: 0.094417
=== Actor Training Debug (Iteration 2096) ===
Q mean: -8.403806
Q std: 10.837330
Actor loss: 8.407798
Action reg: 0.003992
  l1.weight: grad_norm = 0.008764
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.007434
Total gradient norm: 0.030546
=== Actor Training Debug (Iteration 2097) ===
Q mean: -10.374447
Q std: 11.572111
Actor loss: 10.378440
Action reg: 0.003993
  l1.weight: grad_norm = 0.010961
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.009294
Total gradient norm: 0.045023
=== Actor Training Debug (Iteration 2098) ===
Q mean: -9.500902
Q std: 11.408423
Actor loss: 9.504899
Action reg: 0.003997
  l1.weight: grad_norm = 0.007608
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.007479
Total gradient norm: 0.043226
=== Actor Training Debug (Iteration 2099) ===
Q mean: -8.471277
Q std: 11.488960
Actor loss: 8.475267
Action reg: 0.003990
  l1.weight: grad_norm = 0.017146
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.015752
Total gradient norm: 0.073730
=== Actor Training Debug (Iteration 2100) ===
Q mean: -8.763689
Q std: 10.898367
Actor loss: 8.767686
Action reg: 0.003997
  l1.weight: grad_norm = 0.035521
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.027921
Total gradient norm: 0.093268
Episode 71: Steps=100, Reward=-304.839, Buffer_size=7100
=== Actor Training Debug (Iteration 2101) ===
Q mean: -10.259892
Q std: 11.729154
Actor loss: 10.263890
Action reg: 0.003998
  l1.weight: grad_norm = 0.002333
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002104
Total gradient norm: 0.011763
=== Actor Training Debug (Iteration 2102) ===
Q mean: -10.329455
Q std: 12.524332
Actor loss: 10.333452
Action reg: 0.003997
  l1.weight: grad_norm = 0.002960
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002228
Total gradient norm: 0.009767
=== Actor Training Debug (Iteration 2103) ===
Q mean: -10.197211
Q std: 11.778655
Actor loss: 10.201200
Action reg: 0.003988
  l1.weight: grad_norm = 0.027419
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.021987
Total gradient norm: 0.096259
=== Actor Training Debug (Iteration 2104) ===
Q mean: -8.938837
Q std: 11.511133
Actor loss: 8.942828
Action reg: 0.003991
  l1.weight: grad_norm = 0.008050
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.006276
Total gradient norm: 0.024825
=== Actor Training Debug (Iteration 2105) ===
Q mean: -8.290040
Q std: 11.171418
Actor loss: 8.294032
Action reg: 0.003992
  l1.weight: grad_norm = 0.044791
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.041024
Total gradient norm: 0.114943
=== Actor Training Debug (Iteration 2106) ===
Q mean: -8.260844
Q std: 11.331069
Actor loss: 8.264835
Action reg: 0.003991
  l1.weight: grad_norm = 0.016556
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.014927
Total gradient norm: 0.088136
=== Actor Training Debug (Iteration 2107) ===
Q mean: -9.594960
Q std: 11.505282
Actor loss: 9.598951
Action reg: 0.003992
  l1.weight: grad_norm = 0.004480
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.003909
Total gradient norm: 0.021386
=== Actor Training Debug (Iteration 2108) ===
Q mean: -10.403534
Q std: 11.855781
Actor loss: 10.407521
Action reg: 0.003987
  l1.weight: grad_norm = 0.040496
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.034011
Total gradient norm: 0.157207
=== Actor Training Debug (Iteration 2109) ===
Q mean: -10.176939
Q std: 11.483713
Actor loss: 10.180930
Action reg: 0.003991
  l1.weight: grad_norm = 0.007542
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005922
Total gradient norm: 0.026575
=== Actor Training Debug (Iteration 2110) ===
Q mean: -9.187683
Q std: 10.863882
Actor loss: 9.191676
Action reg: 0.003993
  l1.weight: grad_norm = 0.016127
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.013746
Total gradient norm: 0.049273
=== Actor Training Debug (Iteration 2111) ===
Q mean: -8.522058
Q std: 10.622819
Actor loss: 8.526048
Action reg: 0.003990
  l1.weight: grad_norm = 0.011827
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.007176
Total gradient norm: 0.033380
=== Actor Training Debug (Iteration 2112) ===
Q mean: -8.722002
Q std: 10.680560
Actor loss: 8.725998
Action reg: 0.003996
  l1.weight: grad_norm = 0.005599
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003948
Total gradient norm: 0.019268
=== Actor Training Debug (Iteration 2113) ===
Q mean: -9.199926
Q std: 10.596592
Actor loss: 9.203923
Action reg: 0.003997
  l1.weight: grad_norm = 0.002565
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002437
Total gradient norm: 0.006994
=== Actor Training Debug (Iteration 2114) ===
Q mean: -9.699836
Q std: 11.613494
Actor loss: 9.703831
Action reg: 0.003995
  l1.weight: grad_norm = 0.006518
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.006057
Total gradient norm: 0.025869
=== Actor Training Debug (Iteration 2115) ===
Q mean: -9.637156
Q std: 11.598749
Actor loss: 9.641149
Action reg: 0.003993
  l1.weight: grad_norm = 0.014123
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.012521
Total gradient norm: 0.046515
=== Actor Training Debug (Iteration 2116) ===
Q mean: -8.989910
Q std: 11.594235
Actor loss: 8.993903
Action reg: 0.003993
  l1.weight: grad_norm = 0.019908
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.018845
Total gradient norm: 0.097467
=== Actor Training Debug (Iteration 2117) ===
Q mean: -8.028716
Q std: 10.198422
Actor loss: 8.032705
Action reg: 0.003989
  l1.weight: grad_norm = 0.011104
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.009009
Total gradient norm: 0.044669
=== Actor Training Debug (Iteration 2118) ===
Q mean: -10.358288
Q std: 11.936550
Actor loss: 10.362285
Action reg: 0.003997
  l1.weight: grad_norm = 0.007763
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.009158
Total gradient norm: 0.052174
=== Actor Training Debug (Iteration 2119) ===
Q mean: -9.732431
Q std: 11.573609
Actor loss: 9.736425
Action reg: 0.003994
  l1.weight: grad_norm = 0.027677
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.021161
Total gradient norm: 0.093714
=== Actor Training Debug (Iteration 2120) ===
Q mean: -9.001372
Q std: 11.469470
Actor loss: 9.005369
Action reg: 0.003996
  l1.weight: grad_norm = 0.002164
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001739
Total gradient norm: 0.007708
=== Actor Training Debug (Iteration 2121) ===
Q mean: -8.851040
Q std: 11.827233
Actor loss: 8.855033
Action reg: 0.003993
  l1.weight: grad_norm = 0.018927
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.016055
Total gradient norm: 0.080974
=== Actor Training Debug (Iteration 2122) ===
Q mean: -10.168427
Q std: 11.334275
Actor loss: 10.172422
Action reg: 0.003996
  l1.weight: grad_norm = 0.007591
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.006188
Total gradient norm: 0.022719
=== Actor Training Debug (Iteration 2123) ===
Q mean: -8.652205
Q std: 10.867985
Actor loss: 8.656194
Action reg: 0.003990
  l1.weight: grad_norm = 0.012560
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.009906
Total gradient norm: 0.043782
=== Actor Training Debug (Iteration 2124) ===
Q mean: -9.468246
Q std: 11.524379
Actor loss: 9.472239
Action reg: 0.003993
  l1.weight: grad_norm = 0.006944
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.006029
Total gradient norm: 0.022572
=== Actor Training Debug (Iteration 2125) ===
Q mean: -8.670301
Q std: 11.074309
Actor loss: 8.674295
Action reg: 0.003994
  l1.weight: grad_norm = 0.012741
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.013152
Total gradient norm: 0.065513
=== Actor Training Debug (Iteration 2126) ===
Q mean: -11.180641
Q std: 12.375018
Actor loss: 11.184637
Action reg: 0.003995
  l1.weight: grad_norm = 0.009375
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.006444
Total gradient norm: 0.028175
=== Actor Training Debug (Iteration 2127) ===
Q mean: -9.437862
Q std: 11.422853
Actor loss: 9.441856
Action reg: 0.003994
  l1.weight: grad_norm = 0.004836
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.003302
Total gradient norm: 0.011977
=== Actor Training Debug (Iteration 2128) ===
Q mean: -8.731144
Q std: 11.344128
Actor loss: 8.735139
Action reg: 0.003995
  l1.weight: grad_norm = 0.046080
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.035116
Total gradient norm: 0.108752
=== Actor Training Debug (Iteration 2129) ===
Q mean: -8.883583
Q std: 11.484529
Actor loss: 8.887577
Action reg: 0.003994
  l1.weight: grad_norm = 0.007877
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.007170
Total gradient norm: 0.046659
=== Actor Training Debug (Iteration 2130) ===
Q mean: -10.766867
Q std: 12.355604
Actor loss: 10.770858
Action reg: 0.003992
  l1.weight: grad_norm = 0.012850
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.011118
Total gradient norm: 0.037106
=== Actor Training Debug (Iteration 2131) ===
Q mean: -9.336488
Q std: 11.374374
Actor loss: 9.340486
Action reg: 0.003998
  l1.weight: grad_norm = 0.004590
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.003791
Total gradient norm: 0.013991
=== Actor Training Debug (Iteration 2132) ===
Q mean: -10.268002
Q std: 11.916327
Actor loss: 10.271996
Action reg: 0.003995
  l1.weight: grad_norm = 0.016665
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.013539
Total gradient norm: 0.074639
=== Actor Training Debug (Iteration 2133) ===
Q mean: -9.285690
Q std: 11.343719
Actor loss: 9.289685
Action reg: 0.003995
  l1.weight: grad_norm = 0.008078
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.007201
Total gradient norm: 0.024445
=== Actor Training Debug (Iteration 2134) ===
Q mean: -8.694178
Q std: 11.292603
Actor loss: 8.698174
Action reg: 0.003996
  l1.weight: grad_norm = 0.007436
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006031
Total gradient norm: 0.033569
=== Actor Training Debug (Iteration 2135) ===
Q mean: -9.038240
Q std: 11.428779
Actor loss: 9.042237
Action reg: 0.003997
  l1.weight: grad_norm = 0.027923
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.022645
Total gradient norm: 0.064801
=== Actor Training Debug (Iteration 2136) ===
Q mean: -8.858411
Q std: 11.070563
Actor loss: 8.862404
Action reg: 0.003993
  l1.weight: grad_norm = 0.017596
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.013935
Total gradient norm: 0.085739
=== Actor Training Debug (Iteration 2137) ===
Q mean: -9.592769
Q std: 11.173719
Actor loss: 9.596766
Action reg: 0.003997
  l1.weight: grad_norm = 0.004955
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004517
Total gradient norm: 0.017298
=== Actor Training Debug (Iteration 2138) ===
Q mean: -10.385605
Q std: 11.504487
Actor loss: 10.389599
Action reg: 0.003994
  l1.weight: grad_norm = 0.024811
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.020542
Total gradient norm: 0.118888
=== Actor Training Debug (Iteration 2139) ===
Q mean: -8.313811
Q std: 11.095044
Actor loss: 8.317805
Action reg: 0.003994
  l1.weight: grad_norm = 0.033321
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.031134
Total gradient norm: 0.179310
=== Actor Training Debug (Iteration 2140) ===
Q mean: -9.965231
Q std: 12.054800
Actor loss: 9.969224
Action reg: 0.003993
  l1.weight: grad_norm = 0.020999
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.015708
Total gradient norm: 0.064474
=== Actor Training Debug (Iteration 2141) ===
Q mean: -9.798807
Q std: 11.782763
Actor loss: 9.802793
Action reg: 0.003985
  l1.weight: grad_norm = 0.019522
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.014587
Total gradient norm: 0.065079
=== Actor Training Debug (Iteration 2142) ===
Q mean: -10.432959
Q std: 11.723358
Actor loss: 10.436949
Action reg: 0.003990
  l1.weight: grad_norm = 0.007757
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.005977
Total gradient norm: 0.017571
=== Actor Training Debug (Iteration 2143) ===
Q mean: -9.779681
Q std: 11.932286
Actor loss: 9.783668
Action reg: 0.003986
  l1.weight: grad_norm = 0.015136
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.011828
Total gradient norm: 0.043117
=== Actor Training Debug (Iteration 2144) ===
Q mean: -9.390984
Q std: 11.476943
Actor loss: 9.394978
Action reg: 0.003994
  l1.weight: grad_norm = 0.009184
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.008063
Total gradient norm: 0.057422
=== Actor Training Debug (Iteration 2145) ===
Q mean: -9.944811
Q std: 11.641711
Actor loss: 9.948803
Action reg: 0.003992
  l1.weight: grad_norm = 0.020826
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.016683
Total gradient norm: 0.092812
=== Actor Training Debug (Iteration 2146) ===
Q mean: -9.855783
Q std: 12.423171
Actor loss: 9.859772
Action reg: 0.003989
  l1.weight: grad_norm = 0.123769
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.086224
Total gradient norm: 0.475505
=== Actor Training Debug (Iteration 2147) ===
Q mean: -8.072821
Q std: 10.917001
Actor loss: 8.076818
Action reg: 0.003998
  l1.weight: grad_norm = 0.009580
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.008390
Total gradient norm: 0.031655
=== Actor Training Debug (Iteration 2148) ===
Q mean: -9.676040
Q std: 11.532212
Actor loss: 9.680029
Action reg: 0.003990
  l1.weight: grad_norm = 0.031488
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.025156
Total gradient norm: 0.088445
=== Actor Training Debug (Iteration 2149) ===
Q mean: -9.311658
Q std: 11.125897
Actor loss: 9.315639
Action reg: 0.003981
  l1.weight: grad_norm = 0.068466
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.067424
Total gradient norm: 0.251597
=== Actor Training Debug (Iteration 2150) ===
Q mean: -9.294865
Q std: 10.728667
Actor loss: 9.298862
Action reg: 0.003998
  l1.weight: grad_norm = 0.012964
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.012195
Total gradient norm: 0.052140
=== Actor Training Debug (Iteration 2151) ===
Q mean: -10.454201
Q std: 11.509952
Actor loss: 10.458192
Action reg: 0.003991
  l1.weight: grad_norm = 0.021596
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.018228
Total gradient norm: 0.095463
=== Actor Training Debug (Iteration 2152) ===
Q mean: -10.309412
Q std: 11.707316
Actor loss: 10.313401
Action reg: 0.003989
  l1.weight: grad_norm = 0.031568
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.026835
Total gradient norm: 0.092312
=== Actor Training Debug (Iteration 2153) ===
Q mean: -9.483727
Q std: 12.370092
Actor loss: 9.487717
Action reg: 0.003990
  l1.weight: grad_norm = 0.021605
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.018838
Total gradient norm: 0.099284
=== Actor Training Debug (Iteration 2154) ===
Q mean: -10.860754
Q std: 12.756689
Actor loss: 10.864746
Action reg: 0.003992
  l1.weight: grad_norm = 0.015033
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.012742
Total gradient norm: 0.068146
=== Actor Training Debug (Iteration 2155) ===
Q mean: -9.465624
Q std: 12.127170
Actor loss: 9.469619
Action reg: 0.003995
  l1.weight: grad_norm = 0.006023
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.005955
Total gradient norm: 0.022210
=== Actor Training Debug (Iteration 2156) ===
Q mean: -9.515493
Q std: 11.486711
Actor loss: 9.519479
Action reg: 0.003985
  l1.weight: grad_norm = 0.068211
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.066730
Total gradient norm: 0.222532
=== Actor Training Debug (Iteration 2157) ===
Q mean: -9.021143
Q std: 11.329690
Actor loss: 9.025130
Action reg: 0.003988
  l1.weight: grad_norm = 0.024308
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.019415
Total gradient norm: 0.110349
=== Actor Training Debug (Iteration 2158) ===
Q mean: -10.012675
Q std: 11.116528
Actor loss: 10.016668
Action reg: 0.003993
  l1.weight: grad_norm = 0.016183
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.012493
Total gradient norm: 0.062952
=== Actor Training Debug (Iteration 2159) ===
Q mean: -10.116129
Q std: 11.755878
Actor loss: 10.120122
Action reg: 0.003993
  l1.weight: grad_norm = 0.012584
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.009682
Total gradient norm: 0.053692
=== Actor Training Debug (Iteration 2160) ===
Q mean: -8.527074
Q std: 11.668405
Actor loss: 8.531067
Action reg: 0.003993
  l1.weight: grad_norm = 0.027749
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.022877
Total gradient norm: 0.141383
=== Actor Training Debug (Iteration 2161) ===
Q mean: -9.131723
Q std: 11.660250
Actor loss: 9.135718
Action reg: 0.003995
  l1.weight: grad_norm = 0.002566
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.001951
Total gradient norm: 0.008091
=== Actor Training Debug (Iteration 2162) ===
Q mean: -10.280112
Q std: 12.016386
Actor loss: 10.284105
Action reg: 0.003993
  l1.weight: grad_norm = 0.007106
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.006425
Total gradient norm: 0.043407
=== Actor Training Debug (Iteration 2163) ===
Q mean: -9.459198
Q std: 11.254359
Actor loss: 9.463185
Action reg: 0.003987
  l1.weight: grad_norm = 0.037747
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.030694
Total gradient norm: 0.207013
=== Actor Training Debug (Iteration 2164) ===
Q mean: -8.654491
Q std: 11.192997
Actor loss: 8.658484
Action reg: 0.003992
  l1.weight: grad_norm = 0.008991
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.007961
Total gradient norm: 0.026973
=== Actor Training Debug (Iteration 2165) ===
Q mean: -8.860583
Q std: 11.258457
Actor loss: 8.864573
Action reg: 0.003989
  l1.weight: grad_norm = 0.004010
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.003081
Total gradient norm: 0.012961
=== Actor Training Debug (Iteration 2166) ===
Q mean: -10.208347
Q std: 12.726683
Actor loss: 10.212339
Action reg: 0.003992
  l1.weight: grad_norm = 0.007208
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.005643
Total gradient norm: 0.021707
=== Actor Training Debug (Iteration 2167) ===
Q mean: -9.288952
Q std: 11.698822
Actor loss: 9.292945
Action reg: 0.003993
  l1.weight: grad_norm = 0.013429
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.010338
Total gradient norm: 0.074588
=== Actor Training Debug (Iteration 2168) ===
Q mean: -9.434092
Q std: 11.926303
Actor loss: 9.438087
Action reg: 0.003995
  l1.weight: grad_norm = 0.012392
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.010199
Total gradient norm: 0.055052
=== Actor Training Debug (Iteration 2169) ===
Q mean: -10.322505
Q std: 11.656232
Actor loss: 10.326493
Action reg: 0.003988
  l1.weight: grad_norm = 0.014663
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.012829
Total gradient norm: 0.061928
=== Actor Training Debug (Iteration 2170) ===
Q mean: -9.996778
Q std: 11.349133
Actor loss: 10.000771
Action reg: 0.003992
  l1.weight: grad_norm = 0.009282
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.007442
Total gradient norm: 0.025474
=== Actor Training Debug (Iteration 2171) ===
Q mean: -9.898787
Q std: 12.054997
Actor loss: 9.902774
Action reg: 0.003988
  l1.weight: grad_norm = 0.025904
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.025405
Total gradient norm: 0.158775
=== Actor Training Debug (Iteration 2172) ===
Q mean: -9.296433
Q std: 11.514478
Actor loss: 9.300430
Action reg: 0.003997
  l1.weight: grad_norm = 0.004306
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.003473
Total gradient norm: 0.017293
=== Actor Training Debug (Iteration 2173) ===
Q mean: -9.032906
Q std: 11.063021
Actor loss: 9.036898
Action reg: 0.003992
  l1.weight: grad_norm = 0.008688
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.007356
Total gradient norm: 0.036282
=== Actor Training Debug (Iteration 2174) ===
Q mean: -8.938852
Q std: 11.274439
Actor loss: 8.942847
Action reg: 0.003995
  l1.weight: grad_norm = 0.009206
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.009216
Total gradient norm: 0.049959
=== Actor Training Debug (Iteration 2175) ===
Q mean: -9.971968
Q std: 11.483188
Actor loss: 9.975962
Action reg: 0.003994
  l1.weight: grad_norm = 0.005242
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003598
Total gradient norm: 0.016325
=== Actor Training Debug (Iteration 2176) ===
Q mean: -9.968581
Q std: 11.881298
Actor loss: 9.972576
Action reg: 0.003995
  l1.weight: grad_norm = 0.020191
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.015497
Total gradient norm: 0.094402
=== Actor Training Debug (Iteration 2177) ===
Q mean: -9.235291
Q std: 11.727091
Actor loss: 9.239289
Action reg: 0.003999
  l1.weight: grad_norm = 0.004813
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003277
Total gradient norm: 0.016125
=== Actor Training Debug (Iteration 2178) ===
Q mean: -9.432590
Q std: 11.431970
Actor loss: 9.436583
Action reg: 0.003993
  l1.weight: grad_norm = 0.003608
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.002816
Total gradient norm: 0.018078
=== Actor Training Debug (Iteration 2179) ===
Q mean: -9.628048
Q std: 11.747840
Actor loss: 9.632043
Action reg: 0.003995
  l1.weight: grad_norm = 0.013900
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.010673
Total gradient norm: 0.047856
=== Actor Training Debug (Iteration 2180) ===
Q mean: -9.222269
Q std: 11.737287
Actor loss: 9.226259
Action reg: 0.003991
  l1.weight: grad_norm = 0.003438
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.003164
Total gradient norm: 0.021623
=== Actor Training Debug (Iteration 2181) ===
Q mean: -9.800659
Q std: 11.557987
Actor loss: 9.804649
Action reg: 0.003990
  l1.weight: grad_norm = 0.012725
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.009547
Total gradient norm: 0.048657
=== Actor Training Debug (Iteration 2182) ===
Q mean: -10.553339
Q std: 11.851193
Actor loss: 10.557327
Action reg: 0.003988
  l1.weight: grad_norm = 0.018945
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.014342
Total gradient norm: 0.048507
=== Actor Training Debug (Iteration 2183) ===
Q mean: -10.382446
Q std: 11.995606
Actor loss: 10.386435
Action reg: 0.003988
  l1.weight: grad_norm = 0.031664
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.023646
Total gradient norm: 0.128193
=== Actor Training Debug (Iteration 2184) ===
Q mean: -8.120131
Q std: 11.599092
Actor loss: 8.124124
Action reg: 0.003993
  l1.weight: grad_norm = 0.013502
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.011035
Total gradient norm: 0.064331
=== Actor Training Debug (Iteration 2185) ===
Q mean: -9.089705
Q std: 11.997540
Actor loss: 9.093699
Action reg: 0.003994
  l1.weight: grad_norm = 0.021715
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.016240
Total gradient norm: 0.059139
=== Actor Training Debug (Iteration 2186) ===
Q mean: -10.235494
Q std: 12.009205
Actor loss: 10.239481
Action reg: 0.003988
  l1.weight: grad_norm = 0.015127
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.013596
Total gradient norm: 0.043024
=== Actor Training Debug (Iteration 2187) ===
Q mean: -9.677174
Q std: 11.862421
Actor loss: 9.681165
Action reg: 0.003991
  l1.weight: grad_norm = 0.035817
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.028675
Total gradient norm: 0.113634
=== Actor Training Debug (Iteration 2188) ===
Q mean: -9.877047
Q std: 11.219703
Actor loss: 9.881037
Action reg: 0.003991
  l1.weight: grad_norm = 0.025015
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.018239
Total gradient norm: 0.074173
=== Actor Training Debug (Iteration 2189) ===
Q mean: -9.198622
Q std: 11.610784
Actor loss: 9.202612
Action reg: 0.003990
  l1.weight: grad_norm = 0.024468
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.020650
Total gradient norm: 0.066324
=== Actor Training Debug (Iteration 2190) ===
Q mean: -9.771564
Q std: 11.947289
Actor loss: 9.775558
Action reg: 0.003995
  l1.weight: grad_norm = 0.015038
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.010799
Total gradient norm: 0.048445
=== Actor Training Debug (Iteration 2191) ===
Q mean: -9.536385
Q std: 11.979722
Actor loss: 9.540380
Action reg: 0.003996
  l1.weight: grad_norm = 0.003318
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003185
Total gradient norm: 0.018335
=== Actor Training Debug (Iteration 2192) ===
Q mean: -10.421194
Q std: 12.388539
Actor loss: 10.425186
Action reg: 0.003992
  l1.weight: grad_norm = 0.002268
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.001616
Total gradient norm: 0.006148
=== Actor Training Debug (Iteration 2193) ===
Q mean: -9.222450
Q std: 11.595457
Actor loss: 9.226441
Action reg: 0.003991
  l1.weight: grad_norm = 0.015196
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.013022
Total gradient norm: 0.070496
=== Actor Training Debug (Iteration 2194) ===
Q mean: -10.537710
Q std: 12.246325
Actor loss: 10.541704
Action reg: 0.003994
  l1.weight: grad_norm = 0.006417
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.005439
Total gradient norm: 0.018148
=== Actor Training Debug (Iteration 2195) ===
Q mean: -10.556593
Q std: 12.536024
Actor loss: 10.560585
Action reg: 0.003992
  l1.weight: grad_norm = 0.041977
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.033407
Total gradient norm: 0.159337
=== Actor Training Debug (Iteration 2196) ===
Q mean: -10.057503
Q std: 12.286180
Actor loss: 10.061494
Action reg: 0.003991
  l1.weight: grad_norm = 0.016934
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.012634
Total gradient norm: 0.055326
=== Actor Training Debug (Iteration 2197) ===
Q mean: -9.526861
Q std: 12.040118
Actor loss: 9.530858
Action reg: 0.003997
  l1.weight: grad_norm = 0.013911
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.012518
Total gradient norm: 0.071269
=== Actor Training Debug (Iteration 2198) ===
Q mean: -8.957877
Q std: 11.616865
Actor loss: 8.961865
Action reg: 0.003988
  l1.weight: grad_norm = 0.025990
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.022134
Total gradient norm: 0.114135
=== Actor Training Debug (Iteration 2199) ===
Q mean: -9.856470
Q std: 11.490104
Actor loss: 9.860464
Action reg: 0.003994
  l1.weight: grad_norm = 0.014571
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.012238
Total gradient norm: 0.045207
=== Actor Training Debug (Iteration 2200) ===
Q mean: -8.508004
Q std: 11.002092
Actor loss: 8.511996
Action reg: 0.003992
  l1.weight: grad_norm = 0.018690
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.015945
Total gradient norm: 0.055159
=== Actor Training Debug (Iteration 2201) ===
Q mean: -10.681078
Q std: 12.564243
Actor loss: 10.685071
Action reg: 0.003993
  l1.weight: grad_norm = 0.010741
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.007578
Total gradient norm: 0.038481
=== Actor Training Debug (Iteration 2202) ===
Q mean: -10.146103
Q std: 12.283350
Actor loss: 10.150088
Action reg: 0.003985
  l1.weight: grad_norm = 0.032621
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.027305
Total gradient norm: 0.152511
=== Actor Training Debug (Iteration 2203) ===
Q mean: -8.826242
Q std: 10.375522
Actor loss: 8.830231
Action reg: 0.003988
  l1.weight: grad_norm = 0.033546
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.024765
Total gradient norm: 0.080095
=== Actor Training Debug (Iteration 2204) ===
Q mean: -9.328610
Q std: 11.058101
Actor loss: 9.332607
Action reg: 0.003997
  l1.weight: grad_norm = 0.004042
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003525
Total gradient norm: 0.017186
=== Actor Training Debug (Iteration 2205) ===
Q mean: -9.982684
Q std: 11.504173
Actor loss: 9.986678
Action reg: 0.003994
  l1.weight: grad_norm = 0.019612
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.012016
Total gradient norm: 0.062849
=== Actor Training Debug (Iteration 2206) ===
Q mean: -9.655479
Q std: 12.288279
Actor loss: 9.659475
Action reg: 0.003996
  l1.weight: grad_norm = 0.018066
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.013922
Total gradient norm: 0.074985
=== Actor Training Debug (Iteration 2207) ===
Q mean: -11.338326
Q std: 12.302313
Actor loss: 11.342323
Action reg: 0.003997
  l1.weight: grad_norm = 0.020627
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.016286
Total gradient norm: 0.072049
=== Actor Training Debug (Iteration 2208) ===
Q mean: -9.676905
Q std: 11.914065
Actor loss: 9.680892
Action reg: 0.003987
  l1.weight: grad_norm = 0.040120
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.040182
Total gradient norm: 0.114941
=== Actor Training Debug (Iteration 2209) ===
Q mean: -8.838104
Q std: 11.886503
Actor loss: 8.842093
Action reg: 0.003988
  l1.weight: grad_norm = 0.026525
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.024370
Total gradient norm: 0.178657
=== Actor Training Debug (Iteration 2210) ===
Q mean: -10.736977
Q std: 11.565954
Actor loss: 10.740973
Action reg: 0.003996
  l1.weight: grad_norm = 0.025065
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.018861
Total gradient norm: 0.122891
=== Actor Training Debug (Iteration 2211) ===
Q mean: -8.490816
Q std: 11.027160
Actor loss: 8.494815
Action reg: 0.003999
  l1.weight: grad_norm = 0.000535
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000530
Total gradient norm: 0.001702
=== Actor Training Debug (Iteration 2212) ===
Q mean: -10.489710
Q std: 11.926535
Actor loss: 10.493702
Action reg: 0.003992
  l1.weight: grad_norm = 0.006797
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.005323
Total gradient norm: 0.027483
=== Actor Training Debug (Iteration 2213) ===
Q mean: -9.967636
Q std: 11.954465
Actor loss: 9.971631
Action reg: 0.003995
  l1.weight: grad_norm = 0.013417
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.010779
Total gradient norm: 0.038149
=== Actor Training Debug (Iteration 2214) ===
Q mean: -10.904030
Q std: 11.974964
Actor loss: 10.908027
Action reg: 0.003997
  l1.weight: grad_norm = 0.003086
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002480
Total gradient norm: 0.013542
=== Actor Training Debug (Iteration 2215) ===
Q mean: -10.238820
Q std: 12.447248
Actor loss: 10.242811
Action reg: 0.003991
  l1.weight: grad_norm = 0.005795
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.005231
Total gradient norm: 0.035263
=== Actor Training Debug (Iteration 2216) ===
Q mean: -10.164783
Q std: 11.973739
Actor loss: 10.168774
Action reg: 0.003990
  l1.weight: grad_norm = 0.014957
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.010984
Total gradient norm: 0.045347
=== Actor Training Debug (Iteration 2217) ===
Q mean: -8.560736
Q std: 11.303757
Actor loss: 8.564732
Action reg: 0.003996
  l1.weight: grad_norm = 0.002669
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.001880
Total gradient norm: 0.007115
=== Actor Training Debug (Iteration 2218) ===
Q mean: -9.506170
Q std: 12.288850
Actor loss: 9.510162
Action reg: 0.003992
  l1.weight: grad_norm = 0.017601
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.014717
Total gradient norm: 0.089881
=== Actor Training Debug (Iteration 2219) ===
Q mean: -9.661375
Q std: 11.812000
Actor loss: 9.665365
Action reg: 0.003990
  l1.weight: grad_norm = 0.027876
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.021638
Total gradient norm: 0.098307
=== Actor Training Debug (Iteration 2220) ===
Q mean: -8.740468
Q std: 11.557946
Actor loss: 8.744457
Action reg: 0.003989
  l1.weight: grad_norm = 0.008026
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.007173
Total gradient norm: 0.040261
=== Actor Training Debug (Iteration 2221) ===
Q mean: -9.982719
Q std: 11.914915
Actor loss: 9.986715
Action reg: 0.003996
  l1.weight: grad_norm = 0.004095
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.003605
Total gradient norm: 0.017845
=== Actor Training Debug (Iteration 2222) ===
Q mean: -11.265894
Q std: 12.449080
Actor loss: 11.269886
Action reg: 0.003992
  l1.weight: grad_norm = 0.008801
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.007099
Total gradient norm: 0.051010
=== Actor Training Debug (Iteration 2223) ===
Q mean: -8.957739
Q std: 11.074686
Actor loss: 8.961735
Action reg: 0.003996
  l1.weight: grad_norm = 0.021410
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.019311
Total gradient norm: 0.111960
=== Actor Training Debug (Iteration 2224) ===
Q mean: -8.414240
Q std: 11.587980
Actor loss: 8.418235
Action reg: 0.003995
  l1.weight: grad_norm = 0.019702
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.012040
Total gradient norm: 0.061917
=== Actor Training Debug (Iteration 2225) ===
Q mean: -10.219291
Q std: 12.889039
Actor loss: 10.223282
Action reg: 0.003991
  l1.weight: grad_norm = 0.026117
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.021225
Total gradient norm: 0.112224
=== Actor Training Debug (Iteration 2226) ===
Q mean: -8.990428
Q std: 11.635652
Actor loss: 8.994418
Action reg: 0.003990
  l1.weight: grad_norm = 0.025642
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.020139
Total gradient norm: 0.116178
=== Actor Training Debug (Iteration 2227) ===
Q mean: -10.914452
Q std: 12.367772
Actor loss: 10.918445
Action reg: 0.003993
  l1.weight: grad_norm = 0.017076
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.014884
Total gradient norm: 0.074730
=== Actor Training Debug (Iteration 2228) ===
Q mean: -10.476619
Q std: 12.441000
Actor loss: 10.480611
Action reg: 0.003992
  l1.weight: grad_norm = 0.004559
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.003812
Total gradient norm: 0.016149
=== Actor Training Debug (Iteration 2229) ===
Q mean: -8.664843
Q std: 11.131236
Actor loss: 8.668835
Action reg: 0.003992
  l1.weight: grad_norm = 0.007408
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.006357
Total gradient norm: 0.035992
=== Actor Training Debug (Iteration 2230) ===
Q mean: -8.964097
Q std: 11.651724
Actor loss: 8.968094
Action reg: 0.003996
  l1.weight: grad_norm = 0.006431
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.004799
Total gradient norm: 0.022106
=== Actor Training Debug (Iteration 2231) ===
Q mean: -8.956684
Q std: 12.183308
Actor loss: 8.960679
Action reg: 0.003995
  l1.weight: grad_norm = 0.008566
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.005992
Total gradient norm: 0.028720
=== Actor Training Debug (Iteration 2232) ===
Q mean: -9.745213
Q std: 11.913021
Actor loss: 9.749210
Action reg: 0.003998
  l1.weight: grad_norm = 0.000815
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.000730
Total gradient norm: 0.002784
=== Actor Training Debug (Iteration 2233) ===
Q mean: -9.727033
Q std: 11.596518
Actor loss: 9.731027
Action reg: 0.003994
  l1.weight: grad_norm = 0.035355
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.027359
Total gradient norm: 0.099373
=== Actor Training Debug (Iteration 2234) ===
Q mean: -11.281254
Q std: 12.951466
Actor loss: 11.285247
Action reg: 0.003993
  l1.weight: grad_norm = 0.009606
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.006874
Total gradient norm: 0.037264
=== Actor Training Debug (Iteration 2235) ===
Q mean: -9.870665
Q std: 12.372241
Actor loss: 9.874657
Action reg: 0.003993
  l1.weight: grad_norm = 0.011411
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.008811
Total gradient norm: 0.054710
=== Actor Training Debug (Iteration 2236) ===
Q mean: -9.498757
Q std: 12.100757
Actor loss: 9.502752
Action reg: 0.003995
  l1.weight: grad_norm = 0.006750
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005675
Total gradient norm: 0.027908
=== Actor Training Debug (Iteration 2237) ===
Q mean: -9.530776
Q std: 12.418179
Actor loss: 9.534766
Action reg: 0.003990
  l1.weight: grad_norm = 0.036629
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.029836
Total gradient norm: 0.111625
=== Actor Training Debug (Iteration 2238) ===
Q mean: -10.287599
Q std: 12.306340
Actor loss: 10.291593
Action reg: 0.003994
  l1.weight: grad_norm = 0.011918
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.008915
Total gradient norm: 0.060430
=== Actor Training Debug (Iteration 2239) ===
Q mean: -10.635550
Q std: 13.311627
Actor loss: 10.639542
Action reg: 0.003991
  l1.weight: grad_norm = 0.020621
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.018894
Total gradient norm: 0.072536
=== Actor Training Debug (Iteration 2240) ===
Q mean: -9.437084
Q std: 12.272302
Actor loss: 9.441074
Action reg: 0.003990
  l1.weight: grad_norm = 0.010574
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.008616
Total gradient norm: 0.041485
=== Actor Training Debug (Iteration 2241) ===
Q mean: -9.630429
Q std: 12.095822
Actor loss: 9.634418
Action reg: 0.003990
  l1.weight: grad_norm = 0.009424
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.007278
Total gradient norm: 0.025713
=== Actor Training Debug (Iteration 2242) ===
Q mean: -11.940629
Q std: 12.783546
Actor loss: 11.944623
Action reg: 0.003994
  l1.weight: grad_norm = 0.009135
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.006849
Total gradient norm: 0.030360
=== Actor Training Debug (Iteration 2243) ===
Q mean: -9.454245
Q std: 11.680734
Actor loss: 9.458239
Action reg: 0.003994
  l1.weight: grad_norm = 0.007256
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.006848
Total gradient norm: 0.041548
=== Actor Training Debug (Iteration 2244) ===
Q mean: -10.286169
Q std: 12.596960
Actor loss: 10.290164
Action reg: 0.003995
  l1.weight: grad_norm = 0.013726
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.011277
Total gradient norm: 0.062752
=== Actor Training Debug (Iteration 2245) ===
Q mean: -8.846580
Q std: 11.841693
Actor loss: 8.850572
Action reg: 0.003992
  l1.weight: grad_norm = 0.007728
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.006655
Total gradient norm: 0.043022
=== Actor Training Debug (Iteration 2246) ===
Q mean: -10.170520
Q std: 12.133926
Actor loss: 10.174510
Action reg: 0.003990
  l1.weight: grad_norm = 0.019410
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.016836
Total gradient norm: 0.065970
=== Actor Training Debug (Iteration 2247) ===
Q mean: -10.172889
Q std: 11.867025
Actor loss: 10.176884
Action reg: 0.003995
  l1.weight: grad_norm = 0.006889
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.006533
Total gradient norm: 0.032204
=== Actor Training Debug (Iteration 2248) ===
Q mean: -10.008528
Q std: 12.016149
Actor loss: 10.012522
Action reg: 0.003994
  l1.weight: grad_norm = 0.006824
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.005498
Total gradient norm: 0.024639
=== Actor Training Debug (Iteration 2249) ===
Q mean: -9.873583
Q std: 12.374456
Actor loss: 9.877572
Action reg: 0.003989
  l1.weight: grad_norm = 0.002172
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.001853
Total gradient norm: 0.010765
=== Actor Training Debug (Iteration 2250) ===
Q mean: -9.954568
Q std: 12.222364
Actor loss: 9.958565
Action reg: 0.003997
  l1.weight: grad_norm = 0.025439
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.018179
Total gradient norm: 0.065673
=== Actor Training Debug (Iteration 2251) ===
Q mean: -9.869057
Q std: 11.874193
Actor loss: 9.873045
Action reg: 0.003988
  l1.weight: grad_norm = 0.021768
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.019243
Total gradient norm: 0.089671
=== Actor Training Debug (Iteration 2252) ===
Q mean: -9.246454
Q std: 12.040998
Actor loss: 9.250445
Action reg: 0.003991
  l1.weight: grad_norm = 0.011712
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.008452
Total gradient norm: 0.048815
=== Actor Training Debug (Iteration 2253) ===
Q mean: -9.862328
Q std: 11.776559
Actor loss: 9.866321
Action reg: 0.003993
  l1.weight: grad_norm = 0.002526
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.002763
Total gradient norm: 0.030392
=== Actor Training Debug (Iteration 2254) ===
Q mean: -10.556602
Q std: 12.548206
Actor loss: 10.560594
Action reg: 0.003991
  l1.weight: grad_norm = 0.011442
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.008144
Total gradient norm: 0.046192
=== Actor Training Debug (Iteration 2255) ===
Q mean: -9.847786
Q std: 11.460225
Actor loss: 9.851779
Action reg: 0.003993
  l1.weight: grad_norm = 0.014638
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.010577
Total gradient norm: 0.073356
=== Actor Training Debug (Iteration 2256) ===
Q mean: -9.747751
Q std: 11.779229
Actor loss: 9.751744
Action reg: 0.003993
  l1.weight: grad_norm = 0.022564
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.021246
Total gradient norm: 0.076566
=== Actor Training Debug (Iteration 2257) ===
Q mean: -9.988072
Q std: 12.693079
Actor loss: 9.992063
Action reg: 0.003990
  l1.weight: grad_norm = 0.022258
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.024665
Total gradient norm: 0.196752
=== Actor Training Debug (Iteration 2258) ===
Q mean: -9.562769
Q std: 12.185649
Actor loss: 9.566757
Action reg: 0.003988
  l1.weight: grad_norm = 0.016375
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.016169
Total gradient norm: 0.150480
=== Actor Training Debug (Iteration 2259) ===
Q mean: -10.065620
Q std: 12.301785
Actor loss: 10.069604
Action reg: 0.003983
  l1.weight: grad_norm = 0.034442
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.030968
Total gradient norm: 0.246592
=== Actor Training Debug (Iteration 2260) ===
Q mean: -10.344628
Q std: 12.450553
Actor loss: 10.348620
Action reg: 0.003992
  l1.weight: grad_norm = 0.010325
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.010413
Total gradient norm: 0.062356
=== Actor Training Debug (Iteration 2261) ===
Q mean: -9.228834
Q std: 11.994296
Actor loss: 9.232814
Action reg: 0.003980
  l1.weight: grad_norm = 0.015618
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.017458
Total gradient norm: 0.130950
=== Actor Training Debug (Iteration 2262) ===
Q mean: -9.212152
Q std: 11.771121
Actor loss: 9.216134
Action reg: 0.003982
  l1.weight: grad_norm = 0.017054
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.021749
Total gradient norm: 0.171916
=== Actor Training Debug (Iteration 2263) ===
Q mean: -9.957063
Q std: 12.863241
Actor loss: 9.961052
Action reg: 0.003989
  l1.weight: grad_norm = 0.026098
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.031443
Total gradient norm: 0.334543
=== Actor Training Debug (Iteration 2264) ===
Q mean: -8.020689
Q std: 10.713402
Actor loss: 8.024667
Action reg: 0.003977
  l1.weight: grad_norm = 0.043573
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.044840
Total gradient norm: 0.228670
=== Actor Training Debug (Iteration 2265) ===
Q mean: -9.705661
Q std: 12.095658
Actor loss: 9.709639
Action reg: 0.003978
  l1.weight: grad_norm = 0.099953
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.097547
Total gradient norm: 0.635490
=== Actor Training Debug (Iteration 2266) ===
Q mean: -10.881355
Q std: 12.366409
Actor loss: 10.885334
Action reg: 0.003979
  l1.weight: grad_norm = 0.064129
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.068541
Total gradient norm: 0.489731
=== Actor Training Debug (Iteration 2267) ===
Q mean: -10.714367
Q std: 13.625082
Actor loss: 10.718346
Action reg: 0.003979
  l1.weight: grad_norm = 0.021353
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.025990
Total gradient norm: 0.222043
=== Actor Training Debug (Iteration 2268) ===
Q mean: -9.937099
Q std: 12.311946
Actor loss: 9.941080
Action reg: 0.003981
  l1.weight: grad_norm = 0.050538
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.061664
Total gradient norm: 0.456838
=== Actor Training Debug (Iteration 2269) ===
Q mean: -9.874113
Q std: 11.907661
Actor loss: 9.878081
Action reg: 0.003969
  l1.weight: grad_norm = 0.041295
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.041804
Total gradient norm: 0.257213
=== Actor Training Debug (Iteration 2270) ===
Q mean: -10.079446
Q std: 12.072797
Actor loss: 10.083432
Action reg: 0.003987
  l1.weight: grad_norm = 0.016855
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.018182
Total gradient norm: 0.068347
=== Actor Training Debug (Iteration 2271) ===
Q mean: -11.137849
Q std: 12.801624
Actor loss: 11.141836
Action reg: 0.003988
  l1.weight: grad_norm = 0.022108
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.023319
Total gradient norm: 0.146926
=== Actor Training Debug (Iteration 2272) ===
Q mean: -10.890201
Q std: 12.691461
Actor loss: 10.894194
Action reg: 0.003994
  l1.weight: grad_norm = 0.037373
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.026009
Total gradient norm: 0.114696
=== Actor Training Debug (Iteration 2273) ===
Q mean: -11.065939
Q std: 12.607228
Actor loss: 11.069922
Action reg: 0.003984
  l1.weight: grad_norm = 0.030252
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.027151
Total gradient norm: 0.140476
=== Actor Training Debug (Iteration 2274) ===
Q mean: -9.273816
Q std: 12.476911
Actor loss: 9.277805
Action reg: 0.003990
  l1.weight: grad_norm = 0.011658
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.008312
Total gradient norm: 0.041195
=== Actor Training Debug (Iteration 2275) ===
Q mean: -9.840899
Q std: 11.858572
Actor loss: 9.844877
Action reg: 0.003979
  l1.weight: grad_norm = 0.028619
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.027752
Total gradient norm: 0.164988
=== Actor Training Debug (Iteration 2276) ===
Q mean: -10.102728
Q std: 12.214464
Actor loss: 10.106711
Action reg: 0.003983
  l1.weight: grad_norm = 0.010784
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.010631
Total gradient norm: 0.063665
=== Actor Training Debug (Iteration 2277) ===
Q mean: -9.696015
Q std: 11.570853
Actor loss: 9.699999
Action reg: 0.003983
  l1.weight: grad_norm = 0.026966
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.028550
Total gradient norm: 0.178843
=== Actor Training Debug (Iteration 2278) ===
Q mean: -9.743490
Q std: 12.166989
Actor loss: 9.747467
Action reg: 0.003977
  l1.weight: grad_norm = 0.050246
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.039935
Total gradient norm: 0.216417
=== Actor Training Debug (Iteration 2279) ===
Q mean: -10.423998
Q std: 11.956070
Actor loss: 10.427986
Action reg: 0.003988
  l1.weight: grad_norm = 0.036822
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.034225
Total gradient norm: 0.144510
=== Actor Training Debug (Iteration 2280) ===
Q mean: -9.652198
Q std: 12.420712
Actor loss: 9.656178
Action reg: 0.003980
  l1.weight: grad_norm = 0.012790
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.011429
Total gradient norm: 0.050946
=== Actor Training Debug (Iteration 2281) ===
Q mean: -9.507030
Q std: 12.546459
Actor loss: 9.511016
Action reg: 0.003987
  l1.weight: grad_norm = 0.011897
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.010555
Total gradient norm: 0.066006
=== Actor Training Debug (Iteration 2282) ===
Q mean: -10.370396
Q std: 11.611129
Actor loss: 10.374381
Action reg: 0.003986
  l1.weight: grad_norm = 0.007436
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.006866
Total gradient norm: 0.041372
=== Actor Training Debug (Iteration 2283) ===
Q mean: -11.828902
Q std: 12.493515
Actor loss: 11.832886
Action reg: 0.003983
  l1.weight: grad_norm = 0.035228
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.039155
Total gradient norm: 0.271783
=== Actor Training Debug (Iteration 2284) ===
Q mean: -9.739760
Q std: 12.425341
Actor loss: 9.743755
Action reg: 0.003995
  l1.weight: grad_norm = 0.024847
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.019335
Total gradient norm: 0.086152
=== Actor Training Debug (Iteration 2285) ===
Q mean: -8.435020
Q std: 11.470070
Actor loss: 8.439014
Action reg: 0.003994
  l1.weight: grad_norm = 0.010257
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.007813
Total gradient norm: 0.035354
=== Actor Training Debug (Iteration 2286) ===
Q mean: -9.934007
Q std: 12.443011
Actor loss: 9.938000
Action reg: 0.003993
  l1.weight: grad_norm = 0.013567
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.009243
Total gradient norm: 0.037745
=== Actor Training Debug (Iteration 2287) ===
Q mean: -10.966786
Q std: 12.310790
Actor loss: 10.970777
Action reg: 0.003990
  l1.weight: grad_norm = 0.006501
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.005982
Total gradient norm: 0.029281
=== Actor Training Debug (Iteration 2288) ===
Q mean: -10.015263
Q std: 12.380172
Actor loss: 10.019250
Action reg: 0.003988
  l1.weight: grad_norm = 0.041235
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.036806
Total gradient norm: 0.190789
=== Actor Training Debug (Iteration 2289) ===
Q mean: -10.444492
Q std: 12.268814
Actor loss: 10.448479
Action reg: 0.003986
  l1.weight: grad_norm = 0.012457
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.009960
Total gradient norm: 0.047201
=== Actor Training Debug (Iteration 2290) ===
Q mean: -11.274025
Q std: 12.926537
Actor loss: 11.278012
Action reg: 0.003987
  l1.weight: grad_norm = 0.009194
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.006836
Total gradient norm: 0.036237
=== Actor Training Debug (Iteration 2291) ===
Q mean: -8.248551
Q std: 11.582697
Actor loss: 8.252535
Action reg: 0.003983
  l1.weight: grad_norm = 0.021708
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.022343
Total gradient norm: 0.113299
=== Actor Training Debug (Iteration 2292) ===
Q mean: -9.407557
Q std: 11.571918
Actor loss: 9.411531
Action reg: 0.003975
  l1.weight: grad_norm = 0.041321
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.032000
Total gradient norm: 0.155629
=== Actor Training Debug (Iteration 2293) ===
Q mean: -10.111858
Q std: 12.212937
Actor loss: 10.115846
Action reg: 0.003987
  l1.weight: grad_norm = 0.042880
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.039521
Total gradient norm: 0.223390
=== Actor Training Debug (Iteration 2294) ===
Q mean: -8.776717
Q std: 11.158649
Actor loss: 8.780690
Action reg: 0.003973
  l1.weight: grad_norm = 0.073215
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.074152
Total gradient norm: 0.206653
=== Actor Training Debug (Iteration 2295) ===
Q mean: -10.138197
Q std: 12.457798
Actor loss: 10.142177
Action reg: 0.003980
  l1.weight: grad_norm = 0.041585
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.035488
Total gradient norm: 0.208292
=== Actor Training Debug (Iteration 2296) ===
Q mean: -10.037008
Q std: 12.942835
Actor loss: 10.040998
Action reg: 0.003991
  l1.weight: grad_norm = 0.026737
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.028182
Total gradient norm: 0.134942
=== Actor Training Debug (Iteration 2297) ===
Q mean: -9.828186
Q std: 11.711004
Actor loss: 9.832169
Action reg: 0.003983
  l1.weight: grad_norm = 0.061922
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.061085
Total gradient norm: 0.360107
=== Actor Training Debug (Iteration 2298) ===
Q mean: -8.180233
Q std: 11.179853
Actor loss: 8.184205
Action reg: 0.003972
  l1.weight: grad_norm = 0.046040
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.043348
Total gradient norm: 0.220083
=== Actor Training Debug (Iteration 2299) ===
Q mean: -9.867710
Q std: 12.450022
Actor loss: 9.871694
Action reg: 0.003983
  l1.weight: grad_norm = 0.001225
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.001555
Total gradient norm: 0.012043
=== Actor Training Debug (Iteration 2300) ===
Q mean: -9.980890
Q std: 12.203296
Actor loss: 9.984876
Action reg: 0.003986
  l1.weight: grad_norm = 0.030855
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.025763
Total gradient norm: 0.118929
=== Actor Training Debug (Iteration 2301) ===
Q mean: -8.978231
Q std: 12.033458
Actor loss: 8.982219
Action reg: 0.003988
  l1.weight: grad_norm = 0.022098
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.019011
Total gradient norm: 0.092768
=== Actor Training Debug (Iteration 2302) ===
Q mean: -9.977468
Q std: 12.311312
Actor loss: 9.981461
Action reg: 0.003992
  l1.weight: grad_norm = 0.009830
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.008352
Total gradient norm: 0.035195
=== Actor Training Debug (Iteration 2303) ===
Q mean: -8.369833
Q std: 11.220313
Actor loss: 8.373817
Action reg: 0.003984
  l1.weight: grad_norm = 0.022286
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.019133
Total gradient norm: 0.067050
=== Actor Training Debug (Iteration 2304) ===
Q mean: -10.981027
Q std: 12.591280
Actor loss: 10.985003
Action reg: 0.003976
  l1.weight: grad_norm = 0.015530
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.013677
Total gradient norm: 0.066167
=== Actor Training Debug (Iteration 2305) ===
Q mean: -10.441072
Q std: 12.425505
Actor loss: 10.445058
Action reg: 0.003985
  l1.weight: grad_norm = 0.009859
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.009319
Total gradient norm: 0.049652
=== Actor Training Debug (Iteration 2306) ===
Q mean: -10.494722
Q std: 12.806954
Actor loss: 10.498707
Action reg: 0.003984
  l1.weight: grad_norm = 0.020186
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.018685
Total gradient norm: 0.102898
=== Actor Training Debug (Iteration 2307) ===
Q mean: -9.782537
Q std: 12.308260
Actor loss: 9.786530
Action reg: 0.003992
  l1.weight: grad_norm = 0.006727
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.007068
Total gradient norm: 0.037831
=== Actor Training Debug (Iteration 2308) ===
Q mean: -9.287487
Q std: 11.548910
Actor loss: 9.291466
Action reg: 0.003979
  l1.weight: grad_norm = 0.020646
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.017083
Total gradient norm: 0.077456
=== Actor Training Debug (Iteration 2309) ===
Q mean: -10.246798
Q std: 12.083043
Actor loss: 10.250788
Action reg: 0.003990
  l1.weight: grad_norm = 0.017273
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.014800
Total gradient norm: 0.058810
=== Actor Training Debug (Iteration 2310) ===
Q mean: -9.678917
Q std: 12.081275
Actor loss: 9.682905
Action reg: 0.003989
  l1.weight: grad_norm = 0.024240
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.017873
Total gradient norm: 0.097073
=== Actor Training Debug (Iteration 2311) ===
Q mean: -10.038496
Q std: 12.623529
Actor loss: 10.042470
Action reg: 0.003974
  l1.weight: grad_norm = 0.026944
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.020995
Total gradient norm: 0.125085
=== Actor Training Debug (Iteration 2312) ===
Q mean: -9.389448
Q std: 12.402985
Actor loss: 9.393425
Action reg: 0.003977
  l1.weight: grad_norm = 0.007136
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.006224
Total gradient norm: 0.038002
=== Actor Training Debug (Iteration 2313) ===
Q mean: -11.336357
Q std: 12.916192
Actor loss: 11.340339
Action reg: 0.003982
  l1.weight: grad_norm = 0.023771
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.020943
Total gradient norm: 0.112692
=== Actor Training Debug (Iteration 2314) ===
Q mean: -9.839167
Q std: 12.019183
Actor loss: 9.843154
Action reg: 0.003988
  l1.weight: grad_norm = 0.031269
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.029160
Total gradient norm: 0.166767
=== Actor Training Debug (Iteration 2315) ===
Q mean: -10.587205
Q std: 12.648275
Actor loss: 10.591192
Action reg: 0.003988
  l1.weight: grad_norm = 0.008404
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.007798
Total gradient norm: 0.032959
=== Actor Training Debug (Iteration 2316) ===
Q mean: -9.066126
Q std: 11.794220
Actor loss: 9.070107
Action reg: 0.003981
  l1.weight: grad_norm = 0.024613
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.023564
Total gradient norm: 0.141159
=== Actor Training Debug (Iteration 2317) ===
Q mean: -10.283697
Q std: 11.807703
Actor loss: 10.287685
Action reg: 0.003988
  l1.weight: grad_norm = 0.033941
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.026892
Total gradient norm: 0.124880
=== Actor Training Debug (Iteration 2318) ===
Q mean: -9.555296
Q std: 12.084620
Actor loss: 9.559276
Action reg: 0.003980
  l1.weight: grad_norm = 0.021270
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.015644
Total gradient norm: 0.067440
=== Actor Training Debug (Iteration 2319) ===
Q mean: -9.991128
Q std: 12.314141
Actor loss: 9.995105
Action reg: 0.003977
  l1.weight: grad_norm = 0.030305
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.026818
Total gradient norm: 0.147144
=== Actor Training Debug (Iteration 2320) ===
Q mean: -9.626073
Q std: 11.983474
Actor loss: 9.630054
Action reg: 0.003982
  l1.weight: grad_norm = 0.011398
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.010646
Total gradient norm: 0.061903
=== Actor Training Debug (Iteration 2321) ===
Q mean: -9.103430
Q std: 12.121595
Actor loss: 9.107420
Action reg: 0.003990
  l1.weight: grad_norm = 0.010642
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.008349
Total gradient norm: 0.041643
=== Actor Training Debug (Iteration 2322) ===
Q mean: -9.777310
Q std: 11.943052
Actor loss: 9.781289
Action reg: 0.003979
  l1.weight: grad_norm = 0.016656
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.013744
Total gradient norm: 0.048318
=== Actor Training Debug (Iteration 2323) ===
Q mean: -8.926131
Q std: 11.429120
Actor loss: 8.930117
Action reg: 0.003985
  l1.weight: grad_norm = 0.013240
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.010193
Total gradient norm: 0.049609
=== Actor Training Debug (Iteration 2324) ===
Q mean: -9.790951
Q std: 11.888924
Actor loss: 9.794937
Action reg: 0.003986
  l1.weight: grad_norm = 0.033153
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.026008
Total gradient norm: 0.115188
=== Actor Training Debug (Iteration 2325) ===
Q mean: -10.491497
Q std: 12.225535
Actor loss: 10.495476
Action reg: 0.003979
  l1.weight: grad_norm = 0.030006
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.023608
Total gradient norm: 0.142350
=== Actor Training Debug (Iteration 2326) ===
Q mean: -9.287004
Q std: 12.159612
Actor loss: 9.290989
Action reg: 0.003985
  l1.weight: grad_norm = 0.005630
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.004384
Total gradient norm: 0.020427
=== Actor Training Debug (Iteration 2327) ===
Q mean: -9.392643
Q std: 12.442826
Actor loss: 9.396627
Action reg: 0.003984
  l1.weight: grad_norm = 0.069067
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.046025
Total gradient norm: 0.215574
=== Actor Training Debug (Iteration 2328) ===
Q mean: -8.853464
Q std: 12.329086
Actor loss: 8.857444
Action reg: 0.003980
  l1.weight: grad_norm = 0.017785
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.016305
Total gradient norm: 0.074480
=== Actor Training Debug (Iteration 2329) ===
Q mean: -10.128670
Q std: 12.186685
Actor loss: 10.132659
Action reg: 0.003989
  l1.weight: grad_norm = 0.029714
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.025926
Total gradient norm: 0.134981
=== Actor Training Debug (Iteration 2330) ===
Q mean: -10.106523
Q std: 12.202625
Actor loss: 10.110519
Action reg: 0.003997
  l1.weight: grad_norm = 0.014010
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.013327
Total gradient norm: 0.069177
=== Actor Training Debug (Iteration 2331) ===
Q mean: -9.222521
Q std: 11.347869
Actor loss: 9.226506
Action reg: 0.003986
  l1.weight: grad_norm = 0.026340
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.021120
Total gradient norm: 0.107536
=== Actor Training Debug (Iteration 2332) ===
Q mean: -9.781175
Q std: 12.600594
Actor loss: 9.785165
Action reg: 0.003990
  l1.weight: grad_norm = 0.009324
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.006144
Total gradient norm: 0.029955
=== Actor Training Debug (Iteration 2333) ===
Q mean: -9.216229
Q std: 12.270288
Actor loss: 9.220211
Action reg: 0.003982
  l1.weight: grad_norm = 0.036924
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.032314
Total gradient norm: 0.096070
=== Actor Training Debug (Iteration 2334) ===
Q mean: -10.760712
Q std: 12.832254
Actor loss: 10.764697
Action reg: 0.003986
  l1.weight: grad_norm = 0.002672
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.002696
Total gradient norm: 0.018119
=== Actor Training Debug (Iteration 2335) ===
Q mean: -12.110143
Q std: 13.328875
Actor loss: 12.114120
Action reg: 0.003977
  l1.weight: grad_norm = 0.020449
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.017215
Total gradient norm: 0.107806
=== Actor Training Debug (Iteration 2336) ===
Q mean: -9.694429
Q std: 12.051797
Actor loss: 9.698424
Action reg: 0.003995
  l1.weight: grad_norm = 0.028180
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.027044
Total gradient norm: 0.129725
=== Actor Training Debug (Iteration 2337) ===
Q mean: -9.023859
Q std: 11.745822
Actor loss: 9.027841
Action reg: 0.003981
  l1.weight: grad_norm = 0.019235
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.014233
Total gradient norm: 0.076237
=== Actor Training Debug (Iteration 2338) ===
Q mean: -9.725918
Q std: 12.256461
Actor loss: 9.729893
Action reg: 0.003975
  l1.weight: grad_norm = 0.011914
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.010571
Total gradient norm: 0.059695
=== Actor Training Debug (Iteration 2339) ===
Q mean: -9.396398
Q std: 12.038513
Actor loss: 9.400379
Action reg: 0.003982
  l1.weight: grad_norm = 0.004874
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.004275
Total gradient norm: 0.023692
=== Actor Training Debug (Iteration 2340) ===
Q mean: -10.654551
Q std: 12.534673
Actor loss: 10.658535
Action reg: 0.003985
  l1.weight: grad_norm = 0.052597
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.039100
Total gradient norm: 0.215752
=== Actor Training Debug (Iteration 2341) ===
Q mean: -8.913408
Q std: 12.014303
Actor loss: 8.917388
Action reg: 0.003979
  l1.weight: grad_norm = 0.021956
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.018142
Total gradient norm: 0.093043
=== Actor Training Debug (Iteration 2342) ===
Q mean: -10.328017
Q std: 12.839521
Actor loss: 10.332004
Action reg: 0.003986
  l1.weight: grad_norm = 0.031007
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.025588
Total gradient norm: 0.123079
=== Actor Training Debug (Iteration 2343) ===
Q mean: -9.573319
Q std: 11.979163
Actor loss: 9.577297
Action reg: 0.003977
  l1.weight: grad_norm = 0.045613
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.044816
Total gradient norm: 0.197675
=== Actor Training Debug (Iteration 2344) ===
Q mean: -9.312785
Q std: 11.953579
Actor loss: 9.316768
Action reg: 0.003983
  l1.weight: grad_norm = 0.036187
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.031690
Total gradient norm: 0.120052
=== Actor Training Debug (Iteration 2345) ===
Q mean: -9.166075
Q std: 11.595040
Actor loss: 9.170057
Action reg: 0.003982
  l1.weight: grad_norm = 0.022686
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.018080
Total gradient norm: 0.073236
=== Actor Training Debug (Iteration 2346) ===
Q mean: -9.360077
Q std: 12.161236
Actor loss: 9.364057
Action reg: 0.003980
  l1.weight: grad_norm = 0.027650
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.021894
Total gradient norm: 0.067540
=== Actor Training Debug (Iteration 2347) ===
Q mean: -9.317617
Q std: 12.167521
Actor loss: 9.321584
Action reg: 0.003966
  l1.weight: grad_norm = 0.026747
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.024056
Total gradient norm: 0.123201
=== Actor Training Debug (Iteration 2348) ===
Q mean: -10.324780
Q std: 12.326494
Actor loss: 10.328759
Action reg: 0.003979
  l1.weight: grad_norm = 0.031783
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.024551
Total gradient norm: 0.126623
=== Actor Training Debug (Iteration 2349) ===
Q mean: -9.653092
Q std: 12.678229
Actor loss: 9.657079
Action reg: 0.003986
  l1.weight: grad_norm = 0.045379
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.034904
Total gradient norm: 0.159219
=== Actor Training Debug (Iteration 2350) ===
Q mean: -9.866201
Q std: 11.729562
Actor loss: 9.870193
Action reg: 0.003992
  l1.weight: grad_norm = 0.006724
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.005417
Total gradient norm: 0.027127
=== Actor Training Debug (Iteration 2351) ===
Q mean: -10.232038
Q std: 11.921238
Actor loss: 10.236025
Action reg: 0.003986
  l1.weight: grad_norm = 0.034172
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.032049
Total gradient norm: 0.168728
=== Actor Training Debug (Iteration 2352) ===
Q mean: -10.085422
Q std: 12.605559
Actor loss: 10.089402
Action reg: 0.003981
  l1.weight: grad_norm = 0.033814
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.027147
Total gradient norm: 0.155208
=== Actor Training Debug (Iteration 2353) ===
Q mean: -9.639487
Q std: 12.517475
Actor loss: 9.643462
Action reg: 0.003975
  l1.weight: grad_norm = 0.014359
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.015403
Total gradient norm: 0.071392
=== Actor Training Debug (Iteration 2354) ===
Q mean: -11.328933
Q std: 12.704873
Actor loss: 11.332923
Action reg: 0.003990
  l1.weight: grad_norm = 0.033433
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.028073
Total gradient norm: 0.135996
=== Actor Training Debug (Iteration 2355) ===
Q mean: -11.630739
Q std: 13.284269
Actor loss: 11.634721
Action reg: 0.003982
  l1.weight: grad_norm = 0.029232
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.029510
Total gradient norm: 0.156383
=== Actor Training Debug (Iteration 2356) ===
Q mean: -9.545383
Q std: 12.467187
Actor loss: 9.549348
Action reg: 0.003965
  l1.weight: grad_norm = 0.034428
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.029719
Total gradient norm: 0.132641
=== Actor Training Debug (Iteration 2357) ===
Q mean: -10.403265
Q std: 12.051801
Actor loss: 10.407251
Action reg: 0.003987
  l1.weight: grad_norm = 0.012481
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.010722
Total gradient norm: 0.049405
=== Actor Training Debug (Iteration 2358) ===
Q mean: -9.608118
Q std: 11.802460
Actor loss: 9.612102
Action reg: 0.003983
  l1.weight: grad_norm = 0.019628
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.014905
Total gradient norm: 0.063628
=== Actor Training Debug (Iteration 2359) ===
Q mean: -9.331455
Q std: 12.074201
Actor loss: 9.335427
Action reg: 0.003972
  l1.weight: grad_norm = 0.031744
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.026655
Total gradient norm: 0.126901
=== Actor Training Debug (Iteration 2360) ===
Q mean: -10.717088
Q std: 12.882603
Actor loss: 10.721080
Action reg: 0.003992
  l1.weight: grad_norm = 0.034158
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.034871
Total gradient norm: 0.128459
=== Actor Training Debug (Iteration 2361) ===
Q mean: -9.615532
Q std: 12.518132
Actor loss: 9.619518
Action reg: 0.003986
  l1.weight: grad_norm = 0.042419
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.038722
Total gradient norm: 0.208826
=== Actor Training Debug (Iteration 2362) ===
Q mean: -10.032437
Q std: 13.495572
Actor loss: 10.036429
Action reg: 0.003992
  l1.weight: grad_norm = 0.033345
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.026406
Total gradient norm: 0.137280
=== Actor Training Debug (Iteration 2363) ===
Q mean: -10.155100
Q std: 12.523758
Actor loss: 10.159075
Action reg: 0.003975
  l1.weight: grad_norm = 0.017916
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.016121
Total gradient norm: 0.077044
=== Actor Training Debug (Iteration 2364) ===
Q mean: -12.323689
Q std: 13.757692
Actor loss: 12.327677
Action reg: 0.003989
  l1.weight: grad_norm = 0.014735
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.011952
Total gradient norm: 0.050186
=== Actor Training Debug (Iteration 2365) ===
Q mean: -9.614281
Q std: 12.132021
Actor loss: 9.618263
Action reg: 0.003982
  l1.weight: grad_norm = 0.056567
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.047766
Total gradient norm: 0.209542
=== Actor Training Debug (Iteration 2366) ===
Q mean: -9.005391
Q std: 12.274085
Actor loss: 9.009379
Action reg: 0.003988
  l1.weight: grad_norm = 0.007314
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.005801
Total gradient norm: 0.018741
=== Actor Training Debug (Iteration 2367) ===
Q mean: -10.553391
Q std: 12.659073
Actor loss: 10.557373
Action reg: 0.003982
  l1.weight: grad_norm = 0.005692
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.004198
Total gradient norm: 0.019006
=== Actor Training Debug (Iteration 2368) ===
Q mean: -9.497869
Q std: 12.280245
Actor loss: 9.501851
Action reg: 0.003981
  l1.weight: grad_norm = 0.026533
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.023231
Total gradient norm: 0.106671
=== Actor Training Debug (Iteration 2369) ===
Q mean: -10.442196
Q std: 13.028432
Actor loss: 10.446177
Action reg: 0.003981
  l1.weight: grad_norm = 0.018799
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.016359
Total gradient norm: 0.058943
=== Actor Training Debug (Iteration 2370) ===
Q mean: -11.369297
Q std: 13.328238
Actor loss: 11.373279
Action reg: 0.003982
  l1.weight: grad_norm = 0.032657
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.026360
Total gradient norm: 0.134034
=== Actor Training Debug (Iteration 2371) ===
Q mean: -11.054718
Q std: 12.645234
Actor loss: 11.058696
Action reg: 0.003978
  l1.weight: grad_norm = 0.022497
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.016926
Total gradient norm: 0.091040
=== Actor Training Debug (Iteration 2372) ===
Q mean: -9.844986
Q std: 12.425583
Actor loss: 9.848971
Action reg: 0.003986
  l1.weight: grad_norm = 0.021083
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.016406
Total gradient norm: 0.080002
=== Actor Training Debug (Iteration 2373) ===
Q mean: -8.790760
Q std: 11.749748
Actor loss: 8.794737
Action reg: 0.003977
  l1.weight: grad_norm = 0.015265
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.014125
Total gradient norm: 0.071254
=== Actor Training Debug (Iteration 2374) ===
Q mean: -10.877317
Q std: 12.801991
Actor loss: 10.881299
Action reg: 0.003981
  l1.weight: grad_norm = 0.016622
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.012760
Total gradient norm: 0.056459
=== Actor Training Debug (Iteration 2375) ===
Q mean: -10.608576
Q std: 12.639521
Actor loss: 10.612564
Action reg: 0.003988
  l1.weight: grad_norm = 0.023697
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.019631
Total gradient norm: 0.107941
=== Actor Training Debug (Iteration 2376) ===
Q mean: -10.594073
Q std: 13.006463
Actor loss: 10.598038
Action reg: 0.003965
  l1.weight: grad_norm = 0.032110
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.024614
Total gradient norm: 0.122214
=== Actor Training Debug (Iteration 2377) ===
Q mean: -10.489085
Q std: 12.548615
Actor loss: 10.493069
Action reg: 0.003983
  l1.weight: grad_norm = 0.035530
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.028601
Total gradient norm: 0.152867
=== Actor Training Debug (Iteration 2378) ===
Q mean: -10.776413
Q std: 12.899837
Actor loss: 10.780391
Action reg: 0.003977
  l1.weight: grad_norm = 0.039685
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.034735
Total gradient norm: 0.154916
=== Actor Training Debug (Iteration 2379) ===
Q mean: -10.597953
Q std: 12.641998
Actor loss: 10.601940
Action reg: 0.003988
  l1.weight: grad_norm = 0.027281
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.020046
Total gradient norm: 0.101535
=== Actor Training Debug (Iteration 2380) ===
Q mean: -9.807230
Q std: 12.723620
Actor loss: 9.811205
Action reg: 0.003975
  l1.weight: grad_norm = 0.041659
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.030858
Total gradient norm: 0.126937
=== Actor Training Debug (Iteration 2381) ===
Q mean: -10.784772
Q std: 13.092633
Actor loss: 10.788764
Action reg: 0.003993
  l1.weight: grad_norm = 0.022419
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.021705
Total gradient norm: 0.098614
=== Actor Training Debug (Iteration 2382) ===
Q mean: -9.920519
Q std: 12.775596
Actor loss: 9.924489
Action reg: 0.003970
  l1.weight: grad_norm = 0.045119
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.035630
Total gradient norm: 0.129832
=== Actor Training Debug (Iteration 2383) ===
Q mean: -12.477909
Q std: 13.922384
Actor loss: 12.481899
Action reg: 0.003990
  l1.weight: grad_norm = 0.047641
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.039892
Total gradient norm: 0.133775
=== Actor Training Debug (Iteration 2384) ===
Q mean: -9.618168
Q std: 12.039632
Actor loss: 9.622161
Action reg: 0.003993
  l1.weight: grad_norm = 0.002764
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.002337
Total gradient norm: 0.012168
=== Actor Training Debug (Iteration 2385) ===
Q mean: -10.591604
Q std: 12.715785
Actor loss: 10.595594
Action reg: 0.003991
  l1.weight: grad_norm = 0.009393
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.009278
Total gradient norm: 0.048114
=== Actor Training Debug (Iteration 2386) ===
Q mean: -11.340438
Q std: 12.953172
Actor loss: 11.344424
Action reg: 0.003987
  l1.weight: grad_norm = 0.020347
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.020274
Total gradient norm: 0.120149
=== Actor Training Debug (Iteration 2387) ===
Q mean: -10.976608
Q std: 12.894559
Actor loss: 10.980595
Action reg: 0.003986
  l1.weight: grad_norm = 0.022195
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.020109
Total gradient norm: 0.105056
=== Actor Training Debug (Iteration 2388) ===
Q mean: -10.095361
Q std: 12.616618
Actor loss: 10.099340
Action reg: 0.003980
  l1.weight: grad_norm = 0.020983
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.015790
Total gradient norm: 0.075996
=== Actor Training Debug (Iteration 2389) ===
Q mean: -10.466263
Q std: 12.753254
Actor loss: 10.470240
Action reg: 0.003977
  l1.weight: grad_norm = 0.033245
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.031482
Total gradient norm: 0.123792
=== Actor Training Debug (Iteration 2390) ===
Q mean: -9.610777
Q std: 13.178337
Actor loss: 9.614750
Action reg: 0.003973
  l1.weight: grad_norm = 0.037242
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.028135
Total gradient norm: 0.149481
=== Actor Training Debug (Iteration 2391) ===
Q mean: -9.187867
Q std: 12.248031
Actor loss: 9.191833
Action reg: 0.003967
  l1.weight: grad_norm = 0.022846
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.018535
Total gradient norm: 0.087238
=== Actor Training Debug (Iteration 2392) ===
Q mean: -11.978279
Q std: 12.830890
Actor loss: 11.982271
Action reg: 0.003992
  l1.weight: grad_norm = 0.010429
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.007364
Total gradient norm: 0.035957
=== Actor Training Debug (Iteration 2393) ===
Q mean: -10.722057
Q std: 13.296345
Actor loss: 10.726045
Action reg: 0.003987
  l1.weight: grad_norm = 0.028417
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.020782
Total gradient norm: 0.146368
=== Actor Training Debug (Iteration 2394) ===
Q mean: -9.142216
Q std: 12.413149
Actor loss: 9.146202
Action reg: 0.003987
  l1.weight: grad_norm = 0.023684
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.017053
Total gradient norm: 0.071134
=== Actor Training Debug (Iteration 2395) ===
Q mean: -11.085260
Q std: 13.308867
Actor loss: 11.089245
Action reg: 0.003984
  l1.weight: grad_norm = 0.017522
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.014408
Total gradient norm: 0.070582
=== Actor Training Debug (Iteration 2396) ===
Q mean: -10.175197
Q std: 12.474561
Actor loss: 10.179184
Action reg: 0.003987
  l1.weight: grad_norm = 0.022430
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.021935
Total gradient norm: 0.098067
=== Actor Training Debug (Iteration 2397) ===
Q mean: -10.232796
Q std: 12.589745
Actor loss: 10.236781
Action reg: 0.003986
  l1.weight: grad_norm = 0.041466
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.035567
Total gradient norm: 0.172129
=== Actor Training Debug (Iteration 2398) ===
Q mean: -9.624943
Q std: 11.944867
Actor loss: 9.628924
Action reg: 0.003981
  l1.weight: grad_norm = 0.021952
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.021128
Total gradient norm: 0.132051
=== Actor Training Debug (Iteration 2399) ===
Q mean: -9.782016
Q std: 12.358472
Actor loss: 9.785997
Action reg: 0.003982
  l1.weight: grad_norm = 0.105248
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.102561
Total gradient norm: 0.431392
=== Actor Training Debug (Iteration 2400) ===
Q mean: -10.825218
Q std: 13.175449
Actor loss: 10.829206
Action reg: 0.003987
  l1.weight: grad_norm = 0.011354
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.009656
Total gradient norm: 0.038170
=== Actor Training Debug (Iteration 2401) ===
Q mean: -10.665209
Q std: 12.870310
Actor loss: 10.669188
Action reg: 0.003979
  l1.weight: grad_norm = 0.020228
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.013849
Total gradient norm: 0.069027
=== Actor Training Debug (Iteration 2402) ===
Q mean: -8.769943
Q std: 11.998527
Actor loss: 8.773930
Action reg: 0.003986
  l1.weight: grad_norm = 0.043099
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.037250
Total gradient norm: 0.203182
=== Actor Training Debug (Iteration 2403) ===
Q mean: -10.689537
Q std: 12.784446
Actor loss: 10.693516
Action reg: 0.003979
  l1.weight: grad_norm = 0.007067
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.005558
Total gradient norm: 0.033133
=== Actor Training Debug (Iteration 2404) ===
Q mean: -9.772330
Q std: 11.950229
Actor loss: 9.776309
Action reg: 0.003979
  l1.weight: grad_norm = 0.020821
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.016551
Total gradient norm: 0.076277
=== Actor Training Debug (Iteration 2405) ===
Q mean: -9.705109
Q std: 12.232327
Actor loss: 9.709089
Action reg: 0.003981
  l1.weight: grad_norm = 0.055464
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.047399
Total gradient norm: 0.185008
=== Actor Training Debug (Iteration 2406) ===
Q mean: -8.677814
Q std: 11.964508
Actor loss: 8.681808
Action reg: 0.003994
  l1.weight: grad_norm = 0.030651
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.027939
Total gradient norm: 0.121073
=== Actor Training Debug (Iteration 2407) ===
Q mean: -10.871532
Q std: 12.801466
Actor loss: 10.875506
Action reg: 0.003974
  l1.weight: grad_norm = 0.023983
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.020303
Total gradient norm: 0.101172
=== Actor Training Debug (Iteration 2408) ===
Q mean: -9.111519
Q std: 11.988007
Actor loss: 9.115488
Action reg: 0.003969
  l1.weight: grad_norm = 0.026528
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.022197
Total gradient norm: 0.102954
=== Actor Training Debug (Iteration 2409) ===
Q mean: -10.822494
Q std: 12.738393
Actor loss: 10.826463
Action reg: 0.003969
  l1.weight: grad_norm = 0.015521
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.013269
Total gradient norm: 0.060453
=== Actor Training Debug (Iteration 2410) ===
Q mean: -9.044670
Q std: 11.779593
Actor loss: 9.048652
Action reg: 0.003982
  l1.weight: grad_norm = 0.062077
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.051306
Total gradient norm: 0.269774
=== Actor Training Debug (Iteration 2411) ===
Q mean: -9.776595
Q std: 12.891680
Actor loss: 9.780564
Action reg: 0.003970
  l1.weight: grad_norm = 0.063299
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.053264
Total gradient norm: 0.194875
=== Actor Training Debug (Iteration 2412) ===
Q mean: -10.118416
Q std: 13.109242
Actor loss: 10.122393
Action reg: 0.003977
  l1.weight: grad_norm = 0.004168
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.003788
Total gradient norm: 0.023464
=== Actor Training Debug (Iteration 2413) ===
Q mean: -10.526244
Q std: 12.559712
Actor loss: 10.530219
Action reg: 0.003974
  l1.weight: grad_norm = 0.038723
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.033219
Total gradient norm: 0.134210
=== Actor Training Debug (Iteration 2414) ===
Q mean: -10.079281
Q std: 12.632491
Actor loss: 10.083271
Action reg: 0.003990
  l1.weight: grad_norm = 0.016736
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.011734
Total gradient norm: 0.059714
=== Actor Training Debug (Iteration 2415) ===
Q mean: -9.957957
Q std: 12.386937
Actor loss: 9.961944
Action reg: 0.003986
  l1.weight: grad_norm = 0.010966
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.010111
Total gradient norm: 0.044229
=== Actor Training Debug (Iteration 2416) ===
Q mean: -10.323101
Q std: 12.528154
Actor loss: 10.327059
Action reg: 0.003957
  l1.weight: grad_norm = 0.011548
  l1.bias: grad_norm = 0.000481
  l2.weight: grad_norm = 0.010483
Total gradient norm: 0.056226
=== Actor Training Debug (Iteration 2417) ===
Q mean: -11.150543
Q std: 12.683601
Actor loss: 11.154520
Action reg: 0.003977
  l1.weight: grad_norm = 0.035128
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.025173
Total gradient norm: 0.089500
=== Actor Training Debug (Iteration 2418) ===
Q mean: -9.599123
Q std: 11.764355
Actor loss: 9.603106
Action reg: 0.003984
  l1.weight: grad_norm = 0.056852
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.050699
Total gradient norm: 0.240668
=== Actor Training Debug (Iteration 2419) ===
Q mean: -9.563679
Q std: 12.542058
Actor loss: 9.567645
Action reg: 0.003966
  l1.weight: grad_norm = 0.030999
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.025430
Total gradient norm: 0.109214
=== Actor Training Debug (Iteration 2420) ===
Q mean: -9.794750
Q std: 12.585586
Actor loss: 9.798724
Action reg: 0.003974
  l1.weight: grad_norm = 0.018152
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.014102
Total gradient norm: 0.060779
=== Actor Training Debug (Iteration 2421) ===
Q mean: -10.751719
Q std: 13.526785
Actor loss: 10.755690
Action reg: 0.003971
  l1.weight: grad_norm = 0.034240
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.030624
Total gradient norm: 0.139073
=== Actor Training Debug (Iteration 2422) ===
Q mean: -10.429075
Q std: 12.834829
Actor loss: 10.433059
Action reg: 0.003984
  l1.weight: grad_norm = 0.013728
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.010385
Total gradient norm: 0.047568
=== Actor Training Debug (Iteration 2423) ===
Q mean: -11.356110
Q std: 13.116735
Actor loss: 11.360104
Action reg: 0.003994
  l1.weight: grad_norm = 0.012170
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010114
Total gradient norm: 0.048058
=== Actor Training Debug (Iteration 2424) ===
Q mean: -9.221613
Q std: 11.840159
Actor loss: 9.225584
Action reg: 0.003971
  l1.weight: grad_norm = 0.004680
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.003833
Total gradient norm: 0.025859
=== Actor Training Debug (Iteration 2425) ===
Q mean: -10.677578
Q std: 13.409428
Actor loss: 10.681564
Action reg: 0.003986
  l1.weight: grad_norm = 0.012288
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.010959
Total gradient norm: 0.050136
=== Actor Training Debug (Iteration 2426) ===
Q mean: -9.455586
Q std: 12.544774
Actor loss: 9.459559
Action reg: 0.003973
  l1.weight: grad_norm = 0.041351
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.035167
Total gradient norm: 0.145130
=== Actor Training Debug (Iteration 2427) ===
Q mean: -11.242052
Q std: 12.705826
Actor loss: 11.246026
Action reg: 0.003974
  l1.weight: grad_norm = 0.032453
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.028322
Total gradient norm: 0.114704
=== Actor Training Debug (Iteration 2428) ===
Q mean: -11.027931
Q std: 12.785060
Actor loss: 11.031902
Action reg: 0.003971
  l1.weight: grad_norm = 0.056033
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.049506
Total gradient norm: 0.227265
=== Actor Training Debug (Iteration 2429) ===
Q mean: -10.467766
Q std: 12.954990
Actor loss: 10.471748
Action reg: 0.003983
  l1.weight: grad_norm = 0.031141
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.024358
Total gradient norm: 0.095560
=== Actor Training Debug (Iteration 2430) ===
Q mean: -10.733589
Q std: 12.771197
Actor loss: 10.737558
Action reg: 0.003969
  l1.weight: grad_norm = 0.040954
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.038072
Total gradient norm: 0.188783
=== Actor Training Debug (Iteration 2431) ===
Q mean: -10.227677
Q std: 13.192077
Actor loss: 10.231655
Action reg: 0.003978
  l1.weight: grad_norm = 0.076179
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.056254
Total gradient norm: 0.232534
=== Actor Training Debug (Iteration 2432) ===
Q mean: -11.017689
Q std: 12.910525
Actor loss: 11.021663
Action reg: 0.003974
  l1.weight: grad_norm = 0.050572
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.042327
Total gradient norm: 0.199413
=== Actor Training Debug (Iteration 2433) ===
Q mean: -10.224066
Q std: 12.895854
Actor loss: 10.228045
Action reg: 0.003979
  l1.weight: grad_norm = 0.021198
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.015661
Total gradient norm: 0.063774
=== Actor Training Debug (Iteration 2434) ===
Q mean: -10.085196
Q std: 12.325303
Actor loss: 10.089174
Action reg: 0.003978
  l1.weight: grad_norm = 0.043924
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.035435
Total gradient norm: 0.158260
=== Actor Training Debug (Iteration 2435) ===
Q mean: -12.185752
Q std: 13.083684
Actor loss: 12.189732
Action reg: 0.003980
  l1.weight: grad_norm = 0.032029
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.024607
Total gradient norm: 0.104205
=== Actor Training Debug (Iteration 2436) ===
Q mean: -10.911371
Q std: 12.903256
Actor loss: 10.915350
Action reg: 0.003979
  l1.weight: grad_norm = 0.052965
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.045619
Total gradient norm: 0.215902
=== Actor Training Debug (Iteration 2437) ===
Q mean: -8.944304
Q std: 12.476481
Actor loss: 8.948279
Action reg: 0.003976
  l1.weight: grad_norm = 0.019809
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.021320
Total gradient norm: 0.131366
=== Actor Training Debug (Iteration 2438) ===
Q mean: -11.062120
Q std: 13.353863
Actor loss: 11.066097
Action reg: 0.003977
  l1.weight: grad_norm = 0.031489
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.026354
Total gradient norm: 0.132773
=== Actor Training Debug (Iteration 2439) ===
Q mean: -10.630348
Q std: 13.261186
Actor loss: 10.634327
Action reg: 0.003979
  l1.weight: grad_norm = 0.012275
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.010774
Total gradient norm: 0.047816
=== Actor Training Debug (Iteration 2440) ===
Q mean: -10.967751
Q std: 13.064293
Actor loss: 10.971741
Action reg: 0.003990
  l1.weight: grad_norm = 0.025441
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.023113
Total gradient norm: 0.078717
=== Actor Training Debug (Iteration 2441) ===
Q mean: -10.033962
Q std: 12.721638
Actor loss: 10.037936
Action reg: 0.003974
  l1.weight: grad_norm = 0.025589
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.020324
Total gradient norm: 0.095251
=== Actor Training Debug (Iteration 2442) ===
Q mean: -10.223864
Q std: 13.108786
Actor loss: 10.227855
Action reg: 0.003992
  l1.weight: grad_norm = 0.017342
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.013426
Total gradient norm: 0.073066
=== Actor Training Debug (Iteration 2443) ===
Q mean: -9.482588
Q std: 12.074377
Actor loss: 9.486572
Action reg: 0.003984
  l1.weight: grad_norm = 0.163911
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.146981
Total gradient norm: 0.815505
=== Actor Training Debug (Iteration 2444) ===
Q mean: -10.251732
Q std: 12.784525
Actor loss: 10.255705
Action reg: 0.003973
  l1.weight: grad_norm = 0.043879
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.036946
Total gradient norm: 0.186555
=== Actor Training Debug (Iteration 2445) ===
Q mean: -10.246909
Q std: 12.857894
Actor loss: 10.250902
Action reg: 0.003993
  l1.weight: grad_norm = 0.035302
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.031658
Total gradient norm: 0.166035
=== Actor Training Debug (Iteration 2446) ===
Q mean: -10.309013
Q std: 13.287325
Actor loss: 10.312989
Action reg: 0.003976
  l1.weight: grad_norm = 0.049450
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.045807
Total gradient norm: 0.169384
=== Actor Training Debug (Iteration 2447) ===
Q mean: -8.894093
Q std: 11.576822
Actor loss: 8.898061
Action reg: 0.003968
  l1.weight: grad_norm = 0.033543
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.023999
Total gradient norm: 0.107647
=== Actor Training Debug (Iteration 2448) ===
Q mean: -11.221800
Q std: 12.924803
Actor loss: 11.225793
Action reg: 0.003993
  l1.weight: grad_norm = 0.044185
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.035647
Total gradient norm: 0.115195
=== Actor Training Debug (Iteration 2449) ===
Q mean: -9.423648
Q std: 12.830729
Actor loss: 9.427623
Action reg: 0.003975
  l1.weight: grad_norm = 0.034228
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.026438
Total gradient norm: 0.145522
=== Actor Training Debug (Iteration 2450) ===
Q mean: -9.301246
Q std: 12.316219
Actor loss: 9.305226
Action reg: 0.003980
  l1.weight: grad_norm = 0.022202
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.017235
Total gradient norm: 0.082544
=== Actor Training Debug (Iteration 2451) ===
Q mean: -10.648695
Q std: 13.170711
Actor loss: 10.652669
Action reg: 0.003974
  l1.weight: grad_norm = 0.052661
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.055341
Total gradient norm: 0.297593
=== Actor Training Debug (Iteration 2452) ===
Q mean: -10.306343
Q std: 12.447108
Actor loss: 10.310328
Action reg: 0.003984
  l1.weight: grad_norm = 0.034299
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.026948
Total gradient norm: 0.117692
=== Actor Training Debug (Iteration 2453) ===
Q mean: -10.305328
Q std: 12.916375
Actor loss: 10.309319
Action reg: 0.003990
  l1.weight: grad_norm = 0.013457
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.011954
Total gradient norm: 0.070190
=== Actor Training Debug (Iteration 2454) ===
Q mean: -8.957464
Q std: 12.535024
Actor loss: 8.961434
Action reg: 0.003970
  l1.weight: grad_norm = 0.062915
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.052318
Total gradient norm: 0.221067
=== Actor Training Debug (Iteration 2455) ===
Q mean: -10.746229
Q std: 12.627278
Actor loss: 10.750215
Action reg: 0.003985
  l1.weight: grad_norm = 0.054186
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.045959
Total gradient norm: 0.251929
=== Actor Training Debug (Iteration 2456) ===
Q mean: -10.749001
Q std: 13.180116
Actor loss: 10.752984
Action reg: 0.003983
  l1.weight: grad_norm = 0.053978
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.042110
Total gradient norm: 0.278779
=== Actor Training Debug (Iteration 2457) ===
Q mean: -10.080112
Q std: 13.122041
Actor loss: 10.084089
Action reg: 0.003977
  l1.weight: grad_norm = 0.062024
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.046531
Total gradient norm: 0.219730
=== Actor Training Debug (Iteration 2458) ===
Q mean: -9.958788
Q std: 12.684248
Actor loss: 9.962774
Action reg: 0.003986
  l1.weight: grad_norm = 0.055373
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.053784
Total gradient norm: 0.238457
=== Actor Training Debug (Iteration 2459) ===
Q mean: -10.731668
Q std: 13.083432
Actor loss: 10.735650
Action reg: 0.003982
  l1.weight: grad_norm = 0.010136
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.008490
Total gradient norm: 0.045118
=== Actor Training Debug (Iteration 2460) ===
Q mean: -9.513120
Q std: 12.568587
Actor loss: 9.517099
Action reg: 0.003980
  l1.weight: grad_norm = 0.031275
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.023813
Total gradient norm: 0.095150
=== Actor Training Debug (Iteration 2461) ===
Q mean: -9.172554
Q std: 12.345941
Actor loss: 9.176543
Action reg: 0.003989
  l1.weight: grad_norm = 0.048228
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.036172
Total gradient norm: 0.149751
=== Actor Training Debug (Iteration 2462) ===
Q mean: -10.298521
Q std: 12.423042
Actor loss: 10.302504
Action reg: 0.003982
  l1.weight: grad_norm = 0.028782
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.022588
Total gradient norm: 0.118708
=== Actor Training Debug (Iteration 2463) ===
Q mean: -10.388430
Q std: 13.028516
Actor loss: 10.392422
Action reg: 0.003992
  l1.weight: grad_norm = 0.015615
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.013204
Total gradient norm: 0.047536
=== Actor Training Debug (Iteration 2464) ===
Q mean: -8.884428
Q std: 12.375364
Actor loss: 8.888412
Action reg: 0.003984
  l1.weight: grad_norm = 0.033418
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.032075
Total gradient norm: 0.146098
=== Actor Training Debug (Iteration 2465) ===
Q mean: -10.302416
Q std: 12.671206
Actor loss: 10.306406
Action reg: 0.003990
  l1.weight: grad_norm = 0.034922
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.025573
Total gradient norm: 0.101527
=== Actor Training Debug (Iteration 2466) ===
Q mean: -10.963161
Q std: 13.101361
Actor loss: 10.967146
Action reg: 0.003985
  l1.weight: grad_norm = 0.016451
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.015257
Total gradient norm: 0.072595
=== Actor Training Debug (Iteration 2467) ===
Q mean: -10.867069
Q std: 12.570240
Actor loss: 10.871059
Action reg: 0.003990
  l1.weight: grad_norm = 0.044794
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.039730
Total gradient norm: 0.145012
=== Actor Training Debug (Iteration 2468) ===
Q mean: -9.499584
Q std: 12.949448
Actor loss: 9.503570
Action reg: 0.003985
  l1.weight: grad_norm = 0.027196
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.022178
Total gradient norm: 0.097285
=== Actor Training Debug (Iteration 2469) ===
Q mean: -8.387562
Q std: 12.328335
Actor loss: 8.391546
Action reg: 0.003984
  l1.weight: grad_norm = 0.007430
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.005773
Total gradient norm: 0.029147
=== Actor Training Debug (Iteration 2470) ===
Q mean: -10.466028
Q std: 12.682448
Actor loss: 10.470012
Action reg: 0.003984
  l1.weight: grad_norm = 0.021174
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.017331
Total gradient norm: 0.072151
=== Actor Training Debug (Iteration 2471) ===
Q mean: -9.315354
Q std: 12.090146
Actor loss: 9.319310
Action reg: 0.003956
  l1.weight: grad_norm = 0.080118
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.059214
Total gradient norm: 0.283626
=== Actor Training Debug (Iteration 2472) ===
Q mean: -10.245930
Q std: 13.203670
Actor loss: 10.249918
Action reg: 0.003988
  l1.weight: grad_norm = 0.017903
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.014886
Total gradient norm: 0.074654
=== Actor Training Debug (Iteration 2473) ===
Q mean: -12.211613
Q std: 14.164535
Actor loss: 12.215593
Action reg: 0.003980
  l1.weight: grad_norm = 0.015479
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.015888
Total gradient norm: 0.089376
=== Actor Training Debug (Iteration 2474) ===
Q mean: -9.268436
Q std: 12.358408
Actor loss: 9.272410
Action reg: 0.003974
  l1.weight: grad_norm = 0.025054
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.019672
Total gradient norm: 0.088164
=== Actor Training Debug (Iteration 2475) ===
Q mean: -11.215038
Q std: 13.661935
Actor loss: 11.219020
Action reg: 0.003982
  l1.weight: grad_norm = 0.031911
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.025309
Total gradient norm: 0.136950
=== Actor Training Debug (Iteration 2476) ===
Q mean: -11.849335
Q std: 13.273493
Actor loss: 11.853327
Action reg: 0.003992
  l1.weight: grad_norm = 0.025860
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.020218
Total gradient norm: 0.089166
=== Actor Training Debug (Iteration 2477) ===
Q mean: -10.224049
Q std: 12.449103
Actor loss: 10.228030
Action reg: 0.003982
  l1.weight: grad_norm = 0.033992
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.031349
Total gradient norm: 0.145508
=== Actor Training Debug (Iteration 2478) ===
Q mean: -11.296808
Q std: 13.012065
Actor loss: 11.300792
Action reg: 0.003984
  l1.weight: grad_norm = 0.018973
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.017156
Total gradient norm: 0.080044
=== Actor Training Debug (Iteration 2479) ===
Q mean: -9.333797
Q std: 12.356764
Actor loss: 9.337787
Action reg: 0.003990
  l1.weight: grad_norm = 0.006786
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.005573
Total gradient norm: 0.026521
=== Actor Training Debug (Iteration 2480) ===
Q mean: -10.587104
Q std: 13.181542
Actor loss: 10.591085
Action reg: 0.003982
  l1.weight: grad_norm = 0.042102
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.035907
Total gradient norm: 0.151587
=== Actor Training Debug (Iteration 2481) ===
Q mean: -10.156303
Q std: 13.144894
Actor loss: 10.160284
Action reg: 0.003981
  l1.weight: grad_norm = 0.056751
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.047861
Total gradient norm: 0.235460
=== Actor Training Debug (Iteration 2482) ===
Q mean: -8.786732
Q std: 12.201280
Actor loss: 8.790705
Action reg: 0.003973
  l1.weight: grad_norm = 0.035351
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.030523
Total gradient norm: 0.122460
=== Actor Training Debug (Iteration 2483) ===
Q mean: -9.397850
Q std: 13.342175
Actor loss: 9.401825
Action reg: 0.003975
  l1.weight: grad_norm = 0.025041
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.020888
Total gradient norm: 0.087353
=== Actor Training Debug (Iteration 2484) ===
Q mean: -10.257762
Q std: 12.884554
Actor loss: 10.261747
Action reg: 0.003986
  l1.weight: grad_norm = 0.009975
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.007589
Total gradient norm: 0.030095
=== Actor Training Debug (Iteration 2485) ===
Q mean: -10.957724
Q std: 13.397734
Actor loss: 10.961715
Action reg: 0.003991
  l1.weight: grad_norm = 0.052982
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.039996
Total gradient norm: 0.171771
=== Actor Training Debug (Iteration 2486) ===
Q mean: -10.624302
Q std: 12.854505
Actor loss: 10.628292
Action reg: 0.003990
  l1.weight: grad_norm = 0.008732
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.008607
Total gradient norm: 0.048175
=== Actor Training Debug (Iteration 2487) ===
Q mean: -10.186884
Q std: 12.275104
Actor loss: 10.190867
Action reg: 0.003984
  l1.weight: grad_norm = 0.023079
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.017797
Total gradient norm: 0.079457
=== Actor Training Debug (Iteration 2488) ===
Q mean: -8.895409
Q std: 12.206177
Actor loss: 8.899402
Action reg: 0.003993
  l1.weight: grad_norm = 0.014738
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.010852
Total gradient norm: 0.050060
=== Actor Training Debug (Iteration 2489) ===
Q mean: -9.505890
Q std: 11.834422
Actor loss: 9.509862
Action reg: 0.003972
  l1.weight: grad_norm = 0.024878
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.023228
Total gradient norm: 0.131568
=== Actor Training Debug (Iteration 2490) ===
Q mean: -9.833418
Q std: 12.549266
Actor loss: 9.837401
Action reg: 0.003983
  l1.weight: grad_norm = 0.060539
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.048744
Total gradient norm: 0.137379
=== Actor Training Debug (Iteration 2491) ===
Q mean: -10.810675
Q std: 12.576218
Actor loss: 10.814656
Action reg: 0.003981
  l1.weight: grad_norm = 0.061651
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.050012
Total gradient norm: 0.174222
=== Actor Training Debug (Iteration 2492) ===
Q mean: -11.472211
Q std: 12.517628
Actor loss: 11.476193
Action reg: 0.003982
  l1.weight: grad_norm = 0.025419
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.022334
Total gradient norm: 0.118394
=== Actor Training Debug (Iteration 2493) ===
Q mean: -8.644097
Q std: 12.427411
Actor loss: 8.648071
Action reg: 0.003973
  l1.weight: grad_norm = 0.042008
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.030068
Total gradient norm: 0.110159
=== Actor Training Debug (Iteration 2494) ===
Q mean: -9.834423
Q std: 12.140010
Actor loss: 9.838419
Action reg: 0.003996
  l1.weight: grad_norm = 0.006030
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.005293
Total gradient norm: 0.019125
=== Actor Training Debug (Iteration 2495) ===
Q mean: -10.783977
Q std: 13.054518
Actor loss: 10.787962
Action reg: 0.003985
  l1.weight: grad_norm = 0.034333
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.034175
Total gradient norm: 0.098076
=== Actor Training Debug (Iteration 2496) ===
Q mean: -10.519643
Q std: 12.892647
Actor loss: 10.523622
Action reg: 0.003979
  l1.weight: grad_norm = 0.063515
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.055091
Total gradient norm: 0.180495
=== Actor Training Debug (Iteration 2497) ===
Q mean: -10.420694
Q std: 12.983148
Actor loss: 10.424667
Action reg: 0.003973
  l1.weight: grad_norm = 0.038000
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.032425
Total gradient norm: 0.153611
=== Actor Training Debug (Iteration 2498) ===
Q mean: -10.115456
Q std: 12.997499
Actor loss: 10.119440
Action reg: 0.003985
  l1.weight: grad_norm = 0.063968
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.054181
Total gradient norm: 0.228446
=== Actor Training Debug (Iteration 2499) ===
Q mean: -9.781363
Q std: 12.455480
Actor loss: 9.785336
Action reg: 0.003973
  l1.weight: grad_norm = 0.030864
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.025751
Total gradient norm: 0.096398
=== Actor Training Debug (Iteration 2500) ===
Q mean: -10.325230
Q std: 13.192962
Actor loss: 10.329208
Action reg: 0.003979
  l1.weight: grad_norm = 0.037146
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.031637
Total gradient norm: 0.194808
  Average reward: -316.714 | Average length: 100.0
Evaluation at episode 75: -316.714
=== Actor Training Debug (Iteration 2501) ===
Q mean: -10.728771
Q std: 13.267524
Actor loss: 10.732754
Action reg: 0.003982
  l1.weight: grad_norm = 0.048255
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.038156
Total gradient norm: 0.122678
=== Actor Training Debug (Iteration 2502) ===
Q mean: -9.582819
Q std: 12.634420
Actor loss: 9.586803
Action reg: 0.003985
  l1.weight: grad_norm = 0.012489
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.010573
Total gradient norm: 0.039725
=== Actor Training Debug (Iteration 2503) ===
Q mean: -10.463823
Q std: 12.639030
Actor loss: 10.467810
Action reg: 0.003987
  l1.weight: grad_norm = 0.011805
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.008554
Total gradient norm: 0.034146
=== Actor Training Debug (Iteration 2504) ===
Q mean: -10.043462
Q std: 13.055383
Actor loss: 10.047453
Action reg: 0.003991
  l1.weight: grad_norm = 0.027691
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.022662
Total gradient norm: 0.105719
=== Actor Training Debug (Iteration 2505) ===
Q mean: -11.423173
Q std: 13.203055
Actor loss: 11.427144
Action reg: 0.003971
  l1.weight: grad_norm = 0.054349
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.046510
Total gradient norm: 0.222379
=== Actor Training Debug (Iteration 2506) ===
Q mean: -10.268562
Q std: 13.150954
Actor loss: 10.272546
Action reg: 0.003984
  l1.weight: grad_norm = 0.032559
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.025765
Total gradient norm: 0.125000
=== Actor Training Debug (Iteration 2507) ===
Q mean: -10.370097
Q std: 12.850071
Actor loss: 10.374084
Action reg: 0.003988
  l1.weight: grad_norm = 0.041575
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.032114
Total gradient norm: 0.110454
=== Actor Training Debug (Iteration 2508) ===
Q mean: -10.289644
Q std: 12.940546
Actor loss: 10.293614
Action reg: 0.003970
  l1.weight: grad_norm = 0.026801
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.024739
Total gradient norm: 0.127775
=== Actor Training Debug (Iteration 2509) ===
Q mean: -10.304554
Q std: 13.127065
Actor loss: 10.308538
Action reg: 0.003984
  l1.weight: grad_norm = 0.019950
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.017944
Total gradient norm: 0.074868
=== Actor Training Debug (Iteration 2510) ===
Q mean: -9.583941
Q std: 12.826694
Actor loss: 9.587932
Action reg: 0.003990
  l1.weight: grad_norm = 0.020686
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.014766
Total gradient norm: 0.072577
=== Actor Training Debug (Iteration 2511) ===
Q mean: -10.671373
Q std: 13.058979
Actor loss: 10.675354
Action reg: 0.003980
  l1.weight: grad_norm = 0.019535
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.016158
Total gradient norm: 0.082316
=== Actor Training Debug (Iteration 2512) ===
Q mean: -10.444635
Q std: 13.315208
Actor loss: 10.448622
Action reg: 0.003986
  l1.weight: grad_norm = 0.007996
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.005768
Total gradient norm: 0.026003
=== Actor Training Debug (Iteration 2513) ===
Q mean: -10.413041
Q std: 13.587114
Actor loss: 10.417011
Action reg: 0.003970
  l1.weight: grad_norm = 0.031045
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.026024
Total gradient norm: 0.120803
=== Actor Training Debug (Iteration 2514) ===
Q mean: -10.870489
Q std: 13.822488
Actor loss: 10.874476
Action reg: 0.003987
  l1.weight: grad_norm = 0.031668
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.028557
Total gradient norm: 0.151139
=== Actor Training Debug (Iteration 2515) ===
Q mean: -11.147853
Q std: 13.233619
Actor loss: 11.151824
Action reg: 0.003971
  l1.weight: grad_norm = 0.052449
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.043963
Total gradient norm: 0.259795
=== Actor Training Debug (Iteration 2516) ===
Q mean: -11.027212
Q std: 12.496654
Actor loss: 11.031199
Action reg: 0.003986
  l1.weight: grad_norm = 0.039982
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.030926
Total gradient norm: 0.130354
=== Actor Training Debug (Iteration 2517) ===
Q mean: -10.184877
Q std: 13.278975
Actor loss: 10.188854
Action reg: 0.003977
  l1.weight: grad_norm = 0.054100
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.044120
Total gradient norm: 0.165091
=== Actor Training Debug (Iteration 2518) ===
Q mean: -9.502507
Q std: 13.030025
Actor loss: 9.506489
Action reg: 0.003982
  l1.weight: grad_norm = 0.007793
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.006823
Total gradient norm: 0.033984
=== Actor Training Debug (Iteration 2519) ===
Q mean: -9.581957
Q std: 12.853364
Actor loss: 9.585945
Action reg: 0.003988
  l1.weight: grad_norm = 0.038472
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.036781
Total gradient norm: 0.173233
=== Actor Training Debug (Iteration 2520) ===
Q mean: -9.825127
Q std: 12.554455
Actor loss: 9.829116
Action reg: 0.003989
  l1.weight: grad_norm = 0.008755
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.006804
Total gradient norm: 0.032110
=== Actor Training Debug (Iteration 2521) ===
Q mean: -11.358585
Q std: 13.024268
Actor loss: 11.362574
Action reg: 0.003989
  l1.weight: grad_norm = 0.034304
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.026156
Total gradient norm: 0.077683
=== Actor Training Debug (Iteration 2522) ===
Q mean: -9.020020
Q std: 11.785871
Actor loss: 9.024011
Action reg: 0.003990
  l1.weight: grad_norm = 0.039290
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.032437
Total gradient norm: 0.205181
=== Actor Training Debug (Iteration 2523) ===
Q mean: -9.439919
Q std: 12.523847
Actor loss: 9.443896
Action reg: 0.003977
  l1.weight: grad_norm = 0.026938
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.023719
Total gradient norm: 0.112197
=== Actor Training Debug (Iteration 2524) ===
Q mean: -10.091635
Q std: 13.206151
Actor loss: 10.095613
Action reg: 0.003979
  l1.weight: grad_norm = 0.046376
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.034901
Total gradient norm: 0.225659
=== Actor Training Debug (Iteration 2525) ===
Q mean: -8.312267
Q std: 12.062046
Actor loss: 8.316253
Action reg: 0.003985
  l1.weight: grad_norm = 0.028712
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.020149
Total gradient norm: 0.113094
=== Actor Training Debug (Iteration 2526) ===
Q mean: -11.025906
Q std: 12.974763
Actor loss: 11.029887
Action reg: 0.003981
  l1.weight: grad_norm = 0.018446
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.015632
Total gradient norm: 0.055434
=== Actor Training Debug (Iteration 2527) ===
Q mean: -10.535996
Q std: 13.068923
Actor loss: 10.539989
Action reg: 0.003992
  l1.weight: grad_norm = 0.020604
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.017433
Total gradient norm: 0.077405
=== Actor Training Debug (Iteration 2528) ===
Q mean: -11.170221
Q std: 13.150861
Actor loss: 11.174198
Action reg: 0.003977
  l1.weight: grad_norm = 0.032268
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.026026
Total gradient norm: 0.191957
=== Actor Training Debug (Iteration 2529) ===
Q mean: -11.116472
Q std: 13.243091
Actor loss: 11.120465
Action reg: 0.003993
  l1.weight: grad_norm = 0.024669
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.021120
Total gradient norm: 0.099587
=== Actor Training Debug (Iteration 2530) ===
Q mean: -10.249533
Q std: 12.560917
Actor loss: 10.253519
Action reg: 0.003986
  l1.weight: grad_norm = 0.030104
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.026162
Total gradient norm: 0.125030
=== Actor Training Debug (Iteration 2531) ===
Q mean: -10.797003
Q std: 12.799296
Actor loss: 10.800985
Action reg: 0.003982
  l1.weight: grad_norm = 0.040710
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.028551
Total gradient norm: 0.145487
=== Actor Training Debug (Iteration 2532) ===
Q mean: -10.049089
Q std: 12.611753
Actor loss: 10.053075
Action reg: 0.003986
  l1.weight: grad_norm = 0.032297
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.027819
Total gradient norm: 0.131162
=== Actor Training Debug (Iteration 2533) ===
Q mean: -11.470813
Q std: 13.908843
Actor loss: 11.474804
Action reg: 0.003991
  l1.weight: grad_norm = 0.015045
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.012197
Total gradient norm: 0.059518
=== Actor Training Debug (Iteration 2534) ===
Q mean: -11.081407
Q std: 14.077427
Actor loss: 11.085394
Action reg: 0.003988
  l1.weight: grad_norm = 0.015087
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.012748
Total gradient norm: 0.067850
=== Actor Training Debug (Iteration 2535) ===
Q mean: -9.487848
Q std: 12.025906
Actor loss: 9.491833
Action reg: 0.003984
  l1.weight: grad_norm = 0.040763
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.034743
Total gradient norm: 0.147401
=== Actor Training Debug (Iteration 2536) ===
Q mean: -9.829065
Q std: 13.233053
Actor loss: 9.833053
Action reg: 0.003987
  l1.weight: grad_norm = 0.068037
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.056183
Total gradient norm: 0.297193
=== Actor Training Debug (Iteration 2537) ===
Q mean: -10.668274
Q std: 13.437493
Actor loss: 10.672260
Action reg: 0.003986
  l1.weight: grad_norm = 0.026981
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.017791
Total gradient norm: 0.083531
=== Actor Training Debug (Iteration 2538) ===
Q mean: -9.429638
Q std: 12.306531
Actor loss: 9.433620
Action reg: 0.003983
  l1.weight: grad_norm = 0.045838
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.034627
Total gradient norm: 0.130393
=== Actor Training Debug (Iteration 2539) ===
Q mean: -10.516669
Q std: 12.533044
Actor loss: 10.520636
Action reg: 0.003966
  l1.weight: grad_norm = 0.025384
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.019539
Total gradient norm: 0.120046
=== Actor Training Debug (Iteration 2540) ===
Q mean: -9.652641
Q std: 12.462351
Actor loss: 9.656611
Action reg: 0.003971
  l1.weight: grad_norm = 0.042235
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.033027
Total gradient norm: 0.128483
=== Actor Training Debug (Iteration 2541) ===
Q mean: -11.711506
Q std: 13.660784
Actor loss: 11.715499
Action reg: 0.003993
  l1.weight: grad_norm = 0.003609
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.003319
Total gradient norm: 0.017422
=== Actor Training Debug (Iteration 2542) ===
Q mean: -12.425149
Q std: 13.610718
Actor loss: 12.429137
Action reg: 0.003989
  l1.weight: grad_norm = 0.035672
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.028742
Total gradient norm: 0.131330
=== Actor Training Debug (Iteration 2543) ===
Q mean: -10.291744
Q std: 12.621652
Actor loss: 10.295725
Action reg: 0.003980
  l1.weight: grad_norm = 0.035218
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.029595
Total gradient norm: 0.108734
=== Actor Training Debug (Iteration 2544) ===
Q mean: -10.418821
Q std: 13.252582
Actor loss: 10.422808
Action reg: 0.003987
  l1.weight: grad_norm = 0.025776
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.019925
Total gradient norm: 0.097161
=== Actor Training Debug (Iteration 2545) ===
Q mean: -10.190171
Q std: 12.736972
Actor loss: 10.194158
Action reg: 0.003986
  l1.weight: grad_norm = 0.009656
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.007188
Total gradient norm: 0.028065
=== Actor Training Debug (Iteration 2546) ===
Q mean: -9.013477
Q std: 12.130632
Actor loss: 9.017455
Action reg: 0.003977
  l1.weight: grad_norm = 0.034148
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.023904
Total gradient norm: 0.087189
=== Actor Training Debug (Iteration 2547) ===
Q mean: -10.323225
Q std: 13.444890
Actor loss: 10.327220
Action reg: 0.003995
  l1.weight: grad_norm = 0.007630
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004823
Total gradient norm: 0.021988
=== Actor Training Debug (Iteration 2548) ===
Q mean: -9.770085
Q std: 12.601038
Actor loss: 9.774060
Action reg: 0.003975
  l1.weight: grad_norm = 0.039490
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.028998
Total gradient norm: 0.161032
=== Actor Training Debug (Iteration 2549) ===
Q mean: -10.700883
Q std: 13.204711
Actor loss: 10.704866
Action reg: 0.003983
  l1.weight: grad_norm = 0.020522
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.017087
Total gradient norm: 0.080774
=== Actor Training Debug (Iteration 2550) ===
Q mean: -9.267643
Q std: 12.468959
Actor loss: 9.271625
Action reg: 0.003982
  l1.weight: grad_norm = 0.033843
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.024749
Total gradient norm: 0.089699
=== Actor Training Debug (Iteration 2551) ===
Q mean: -11.065364
Q std: 13.880987
Actor loss: 11.069355
Action reg: 0.003991
  l1.weight: grad_norm = 0.075452
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.065320
Total gradient norm: 0.340619
=== Actor Training Debug (Iteration 2552) ===
Q mean: -10.719961
Q std: 13.247045
Actor loss: 10.723940
Action reg: 0.003979
  l1.weight: grad_norm = 0.012236
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.009886
Total gradient norm: 0.039434
=== Actor Training Debug (Iteration 2553) ===
Q mean: -10.223063
Q std: 13.799733
Actor loss: 10.227043
Action reg: 0.003980
  l1.weight: grad_norm = 0.021989
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.015521
Total gradient norm: 0.058470
=== Actor Training Debug (Iteration 2554) ===
Q mean: -10.665461
Q std: 13.491700
Actor loss: 10.669435
Action reg: 0.003974
  l1.weight: grad_norm = 0.057803
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.056950
Total gradient norm: 0.300947
=== Actor Training Debug (Iteration 2555) ===
Q mean: -11.110821
Q std: 14.027479
Actor loss: 11.114803
Action reg: 0.003983
  l1.weight: grad_norm = 0.031438
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.021966
Total gradient norm: 0.100378
=== Actor Training Debug (Iteration 2556) ===
Q mean: -11.528120
Q std: 13.915916
Actor loss: 11.532108
Action reg: 0.003988
  l1.weight: grad_norm = 0.034230
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.026981
Total gradient norm: 0.098963
=== Actor Training Debug (Iteration 2557) ===
Q mean: -11.027507
Q std: 13.060855
Actor loss: 11.031487
Action reg: 0.003980
  l1.weight: grad_norm = 0.032389
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.028052
Total gradient norm: 0.205085
=== Actor Training Debug (Iteration 2558) ===
Q mean: -11.272144
Q std: 13.031632
Actor loss: 11.276127
Action reg: 0.003982
  l1.weight: grad_norm = 0.029934
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.028063
Total gradient norm: 0.177443
=== Actor Training Debug (Iteration 2559) ===
Q mean: -9.751400
Q std: 13.044221
Actor loss: 9.755389
Action reg: 0.003990
  l1.weight: grad_norm = 0.041893
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.040122
Total gradient norm: 0.228530
=== Actor Training Debug (Iteration 2560) ===
Q mean: -11.563988
Q std: 13.001872
Actor loss: 11.567978
Action reg: 0.003990
  l1.weight: grad_norm = 0.003851
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.003460
Total gradient norm: 0.013738
=== Actor Training Debug (Iteration 2561) ===
Q mean: -11.744312
Q std: 13.104513
Actor loss: 11.748292
Action reg: 0.003980
  l1.weight: grad_norm = 0.056900
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.047123
Total gradient norm: 0.219060
=== Actor Training Debug (Iteration 2562) ===
Q mean: -10.917888
Q std: 13.734787
Actor loss: 10.921877
Action reg: 0.003989
  l1.weight: grad_norm = 0.016127
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.013935
Total gradient norm: 0.068362
=== Actor Training Debug (Iteration 2563) ===
Q mean: -9.740741
Q std: 13.263634
Actor loss: 9.744728
Action reg: 0.003987
  l1.weight: grad_norm = 0.065577
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.047447
Total gradient norm: 0.198977
=== Actor Training Debug (Iteration 2564) ===
Q mean: -10.196114
Q std: 12.817269
Actor loss: 10.200089
Action reg: 0.003976
  l1.weight: grad_norm = 0.022464
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.017187
Total gradient norm: 0.082132
=== Actor Training Debug (Iteration 2565) ===
Q mean: -11.263022
Q std: 13.156401
Actor loss: 11.266995
Action reg: 0.003973
  l1.weight: grad_norm = 0.036111
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.033431
Total gradient norm: 0.161424
=== Actor Training Debug (Iteration 2566) ===
Q mean: -11.132970
Q std: 13.475660
Actor loss: 11.136950
Action reg: 0.003980
  l1.weight: grad_norm = 0.053577
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.047006
Total gradient norm: 0.150977
=== Actor Training Debug (Iteration 2567) ===
Q mean: -11.274656
Q std: 13.027458
Actor loss: 11.278638
Action reg: 0.003982
  l1.weight: grad_norm = 0.029776
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.025695
Total gradient norm: 0.123314
=== Actor Training Debug (Iteration 2568) ===
Q mean: -9.814462
Q std: 12.431398
Actor loss: 9.818439
Action reg: 0.003976
  l1.weight: grad_norm = 0.032007
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.022810
Total gradient norm: 0.109603
=== Actor Training Debug (Iteration 2569) ===
Q mean: -10.275102
Q std: 13.553642
Actor loss: 10.279075
Action reg: 0.003973
  l1.weight: grad_norm = 0.043563
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.034803
Total gradient norm: 0.164596
=== Actor Training Debug (Iteration 2570) ===
Q mean: -11.271265
Q std: 13.690378
Actor loss: 11.275242
Action reg: 0.003977
  l1.weight: grad_norm = 0.112456
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.090966
Total gradient norm: 0.430731
=== Actor Training Debug (Iteration 2571) ===
Q mean: -10.740524
Q std: 13.055695
Actor loss: 10.744513
Action reg: 0.003988
  l1.weight: grad_norm = 0.006313
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.005826
Total gradient norm: 0.028458
=== Actor Training Debug (Iteration 2572) ===
Q mean: -11.809720
Q std: 14.506262
Actor loss: 11.813705
Action reg: 0.003985
  l1.weight: grad_norm = 0.016936
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.013862
Total gradient norm: 0.062153
=== Actor Training Debug (Iteration 2573) ===
Q mean: -12.109026
Q std: 13.327568
Actor loss: 12.113004
Action reg: 0.003977
  l1.weight: grad_norm = 0.032285
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.025857
Total gradient norm: 0.134100
=== Actor Training Debug (Iteration 2574) ===
Q mean: -10.456837
Q std: 12.466010
Actor loss: 10.460819
Action reg: 0.003982
  l1.weight: grad_norm = 0.021658
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.015506
Total gradient norm: 0.067177
=== Actor Training Debug (Iteration 2575) ===
Q mean: -11.544426
Q std: 13.218039
Actor loss: 11.548418
Action reg: 0.003992
  l1.weight: grad_norm = 0.044531
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.036692
Total gradient norm: 0.156816
=== Actor Training Debug (Iteration 2576) ===
Q mean: -11.058491
Q std: 14.421361
Actor loss: 11.062462
Action reg: 0.003971
  l1.weight: grad_norm = 0.034342
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.030403
Total gradient norm: 0.114365
=== Actor Training Debug (Iteration 2577) ===
Q mean: -10.334644
Q std: 13.190623
Actor loss: 10.338616
Action reg: 0.003972
  l1.weight: grad_norm = 0.032545
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.027618
Total gradient norm: 0.117840
=== Actor Training Debug (Iteration 2578) ===
Q mean: -10.993618
Q std: 13.201551
Actor loss: 10.997603
Action reg: 0.003985
  l1.weight: grad_norm = 0.021757
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.016365
Total gradient norm: 0.068989
=== Actor Training Debug (Iteration 2579) ===
Q mean: -11.866221
Q std: 14.122561
Actor loss: 11.870195
Action reg: 0.003974
  l1.weight: grad_norm = 0.047136
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.034527
Total gradient norm: 0.161356
=== Actor Training Debug (Iteration 2580) ===
Q mean: -10.775399
Q std: 13.097371
Actor loss: 10.779371
Action reg: 0.003972
  l1.weight: grad_norm = 0.076131
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.064819
Total gradient norm: 0.389181
=== Actor Training Debug (Iteration 2581) ===
Q mean: -10.944452
Q std: 13.840855
Actor loss: 10.948428
Action reg: 0.003976
  l1.weight: grad_norm = 0.022191
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.017503
Total gradient norm: 0.072522
=== Actor Training Debug (Iteration 2582) ===
Q mean: -10.080191
Q std: 13.447438
Actor loss: 10.084178
Action reg: 0.003987
  l1.weight: grad_norm = 0.059648
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.043216
Total gradient norm: 0.224633
=== Actor Training Debug (Iteration 2583) ===
Q mean: -9.742817
Q std: 12.834653
Actor loss: 9.746798
Action reg: 0.003981
  l1.weight: grad_norm = 0.021295
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.017650
Total gradient norm: 0.080927
=== Actor Training Debug (Iteration 2584) ===
Q mean: -12.051668
Q std: 13.117473
Actor loss: 12.055655
Action reg: 0.003987
  l1.weight: grad_norm = 0.054152
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.037359
Total gradient norm: 0.129800
=== Actor Training Debug (Iteration 2585) ===
Q mean: -11.371247
Q std: 13.607755
Actor loss: 11.375229
Action reg: 0.003982
  l1.weight: grad_norm = 0.044578
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.031214
Total gradient norm: 0.140354
=== Actor Training Debug (Iteration 2586) ===
Q mean: -10.383432
Q std: 13.068283
Actor loss: 10.387413
Action reg: 0.003981
  l1.weight: grad_norm = 0.039676
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.034875
Total gradient norm: 0.144804
=== Actor Training Debug (Iteration 2587) ===
Q mean: -10.456879
Q std: 13.060542
Actor loss: 10.460875
Action reg: 0.003996
  l1.weight: grad_norm = 0.011160
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.009046
Total gradient norm: 0.048488
=== Actor Training Debug (Iteration 2588) ===
Q mean: -10.879948
Q std: 13.710371
Actor loss: 10.883933
Action reg: 0.003986
  l1.weight: grad_norm = 0.010477
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.010669
Total gradient norm: 0.060490
=== Actor Training Debug (Iteration 2589) ===
Q mean: -9.724052
Q std: 12.657251
Actor loss: 9.728035
Action reg: 0.003983
  l1.weight: grad_norm = 0.024144
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.019147
Total gradient norm: 0.077312
=== Actor Training Debug (Iteration 2590) ===
Q mean: -9.860982
Q std: 12.607172
Actor loss: 9.864961
Action reg: 0.003979
  l1.weight: grad_norm = 0.035527
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.027818
Total gradient norm: 0.130051
=== Actor Training Debug (Iteration 2591) ===
Q mean: -11.211216
Q std: 13.501951
Actor loss: 11.215189
Action reg: 0.003973
  l1.weight: grad_norm = 0.022543
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.017173
Total gradient norm: 0.092590
=== Actor Training Debug (Iteration 2592) ===
Q mean: -9.135668
Q std: 11.993337
Actor loss: 9.139653
Action reg: 0.003985
  l1.weight: grad_norm = 0.073806
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.052172
Total gradient norm: 0.208215
=== Actor Training Debug (Iteration 2593) ===
Q mean: -12.491986
Q std: 14.177993
Actor loss: 12.495961
Action reg: 0.003975
  l1.weight: grad_norm = 0.019005
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.015255
Total gradient norm: 0.063589
=== Actor Training Debug (Iteration 2594) ===
Q mean: -11.327256
Q std: 13.848598
Actor loss: 11.331232
Action reg: 0.003975
  l1.weight: grad_norm = 0.032546
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.028534
Total gradient norm: 0.127983
=== Actor Training Debug (Iteration 2595) ===
Q mean: -11.220509
Q std: 13.072588
Actor loss: 11.224488
Action reg: 0.003979
  l1.weight: grad_norm = 0.029953
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.021851
Total gradient norm: 0.106783
=== Actor Training Debug (Iteration 2596) ===
Q mean: -10.257921
Q std: 13.820262
Actor loss: 10.261895
Action reg: 0.003974
  l1.weight: grad_norm = 0.031231
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.025548
Total gradient norm: 0.144010
=== Actor Training Debug (Iteration 2597) ===
Q mean: -11.222055
Q std: 13.352193
Actor loss: 11.226040
Action reg: 0.003985
  l1.weight: grad_norm = 0.055843
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.050026
Total gradient norm: 0.267431
=== Actor Training Debug (Iteration 2598) ===
Q mean: -9.362623
Q std: 13.282183
Actor loss: 9.366592
Action reg: 0.003969
  l1.weight: grad_norm = 0.052829
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.039711
Total gradient norm: 0.186215
=== Actor Training Debug (Iteration 2599) ===
Q mean: -12.851343
Q std: 14.158719
Actor loss: 12.855326
Action reg: 0.003983
  l1.weight: grad_norm = 0.035798
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.031168
Total gradient norm: 0.109385
=== Actor Training Debug (Iteration 2600) ===
Q mean: -12.114867
Q std: 13.651899
Actor loss: 12.118859
Action reg: 0.003992
  l1.weight: grad_norm = 0.026808
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.024263
Total gradient norm: 0.080486
=== Actor Training Debug (Iteration 2601) ===
Q mean: -10.711604
Q std: 13.700494
Actor loss: 10.715585
Action reg: 0.003980
  l1.weight: grad_norm = 0.018628
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.015832
Total gradient norm: 0.070575
=== Actor Training Debug (Iteration 2602) ===
Q mean: -9.941888
Q std: 13.551849
Actor loss: 9.945860
Action reg: 0.003972
  l1.weight: grad_norm = 0.037374
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.033412
Total gradient norm: 0.163275
=== Actor Training Debug (Iteration 2603) ===
Q mean: -11.985153
Q std: 14.406839
Actor loss: 11.989139
Action reg: 0.003986
  l1.weight: grad_norm = 0.040712
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.035667
Total gradient norm: 0.134034
=== Actor Training Debug (Iteration 2604) ===
Q mean: -11.164364
Q std: 13.503820
Actor loss: 11.168344
Action reg: 0.003980
  l1.weight: grad_norm = 0.044470
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.037930
Total gradient norm: 0.173217
=== Actor Training Debug (Iteration 2605) ===
Q mean: -11.842013
Q std: 14.250925
Actor loss: 11.845993
Action reg: 0.003980
  l1.weight: grad_norm = 0.045282
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.035798
Total gradient norm: 0.151711
=== Actor Training Debug (Iteration 2606) ===
Q mean: -9.746136
Q std: 13.267779
Actor loss: 9.750118
Action reg: 0.003983
  l1.weight: grad_norm = 0.124020
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.113084
Total gradient norm: 0.577347
=== Actor Training Debug (Iteration 2607) ===
Q mean: -10.106145
Q std: 12.673525
Actor loss: 10.110126
Action reg: 0.003982
  l1.weight: grad_norm = 0.012266
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.009188
Total gradient norm: 0.034095
=== Actor Training Debug (Iteration 2608) ===
Q mean: -10.284536
Q std: 13.239836
Actor loss: 10.288503
Action reg: 0.003967
  l1.weight: grad_norm = 0.092051
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.070138
Total gradient norm: 0.308266
=== Actor Training Debug (Iteration 2609) ===
Q mean: -9.699825
Q std: 12.548467
Actor loss: 9.703807
Action reg: 0.003982
  l1.weight: grad_norm = 0.026857
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.022141
Total gradient norm: 0.088100
=== Actor Training Debug (Iteration 2610) ===
Q mean: -9.645532
Q std: 12.708585
Actor loss: 9.649517
Action reg: 0.003986
  l1.weight: grad_norm = 0.047044
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.048803
Total gradient norm: 0.285990
=== Actor Training Debug (Iteration 2611) ===
Q mean: -10.385490
Q std: 12.955032
Actor loss: 10.389464
Action reg: 0.003974
  l1.weight: grad_norm = 0.028370
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.023809
Total gradient norm: 0.109961
=== Actor Training Debug (Iteration 2612) ===
Q mean: -10.360074
Q std: 13.035633
Actor loss: 10.364048
Action reg: 0.003974
  l1.weight: grad_norm = 0.052672
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.035937
Total gradient norm: 0.133508
=== Actor Training Debug (Iteration 2613) ===
Q mean: -10.858164
Q std: 13.044827
Actor loss: 10.862154
Action reg: 0.003991
  l1.weight: grad_norm = 0.016922
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.011853
Total gradient norm: 0.052472
=== Actor Training Debug (Iteration 2614) ===
Q mean: -10.718416
Q std: 12.921150
Actor loss: 10.722393
Action reg: 0.003976
  l1.weight: grad_norm = 0.051935
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.048308
Total gradient norm: 0.209095
=== Actor Training Debug (Iteration 2615) ===
Q mean: -11.114697
Q std: 13.555211
Actor loss: 11.118683
Action reg: 0.003986
  l1.weight: grad_norm = 0.058086
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.043279
Total gradient norm: 0.129769
=== Actor Training Debug (Iteration 2616) ===
Q mean: -12.498569
Q std: 14.526874
Actor loss: 12.502547
Action reg: 0.003978
  l1.weight: grad_norm = 0.041355
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.029746
Total gradient norm: 0.125331
=== Actor Training Debug (Iteration 2617) ===
Q mean: -10.705827
Q std: 13.002117
Actor loss: 10.709807
Action reg: 0.003981
  l1.weight: grad_norm = 0.011329
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.008965
Total gradient norm: 0.047897
=== Actor Training Debug (Iteration 2618) ===
Q mean: -10.843552
Q std: 13.037137
Actor loss: 10.847536
Action reg: 0.003985
  l1.weight: grad_norm = 0.036696
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.026369
Total gradient norm: 0.131812
=== Actor Training Debug (Iteration 2619) ===
Q mean: -10.005622
Q std: 13.185805
Actor loss: 10.009594
Action reg: 0.003972
  l1.weight: grad_norm = 0.018399
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.016070
Total gradient norm: 0.062215
=== Actor Training Debug (Iteration 2620) ===
Q mean: -11.650976
Q std: 13.518433
Actor loss: 11.654969
Action reg: 0.003993
  l1.weight: grad_norm = 0.058265
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.043725
Total gradient norm: 0.197219
=== Actor Training Debug (Iteration 2621) ===
Q mean: -9.682896
Q std: 12.583259
Actor loss: 9.686867
Action reg: 0.003971
  l1.weight: grad_norm = 0.079458
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.067815
Total gradient norm: 0.245799
=== Actor Training Debug (Iteration 2622) ===
Q mean: -10.562920
Q std: 13.173525
Actor loss: 10.566897
Action reg: 0.003978
  l1.weight: grad_norm = 0.034286
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.034990
Total gradient norm: 0.183404
=== Actor Training Debug (Iteration 2623) ===
Q mean: -10.839400
Q std: 12.940265
Actor loss: 10.843376
Action reg: 0.003976
  l1.weight: grad_norm = 0.030163
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.024777
Total gradient norm: 0.092442
=== Actor Training Debug (Iteration 2624) ===
Q mean: -10.921000
Q std: 13.328166
Actor loss: 10.924980
Action reg: 0.003979
  l1.weight: grad_norm = 0.040262
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.034717
Total gradient norm: 0.155913
=== Actor Training Debug (Iteration 2625) ===
Q mean: -10.390148
Q std: 13.557513
Actor loss: 10.394134
Action reg: 0.003985
  l1.weight: grad_norm = 0.037349
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.027501
Total gradient norm: 0.115744
=== Actor Training Debug (Iteration 2626) ===
Q mean: -11.085832
Q std: 13.421748
Actor loss: 11.089808
Action reg: 0.003976
  l1.weight: grad_norm = 0.042835
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.035261
Total gradient norm: 0.167578
=== Actor Training Debug (Iteration 2627) ===
Q mean: -10.836630
Q std: 13.432796
Actor loss: 10.840606
Action reg: 0.003976
  l1.weight: grad_norm = 0.144090
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.106371
Total gradient norm: 0.445865
=== Actor Training Debug (Iteration 2628) ===
Q mean: -10.631689
Q std: 12.914027
Actor loss: 10.635663
Action reg: 0.003974
  l1.weight: grad_norm = 0.019417
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.013337
Total gradient norm: 0.053019
=== Actor Training Debug (Iteration 2629) ===
Q mean: -11.643810
Q std: 14.069251
Actor loss: 11.647798
Action reg: 0.003987
  l1.weight: grad_norm = 0.041982
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.042303
Total gradient norm: 0.237476
=== Actor Training Debug (Iteration 2630) ===
Q mean: -11.466858
Q std: 13.967329
Actor loss: 11.470843
Action reg: 0.003985
  l1.weight: grad_norm = 0.038255
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.032143
Total gradient norm: 0.164574
=== Actor Training Debug (Iteration 2631) ===
Q mean: -9.221341
Q std: 12.542822
Actor loss: 9.225312
Action reg: 0.003971
  l1.weight: grad_norm = 0.009772
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.007933
Total gradient norm: 0.038156
=== Actor Training Debug (Iteration 2632) ===
Q mean: -9.733215
Q std: 12.878414
Actor loss: 9.737183
Action reg: 0.003967
  l1.weight: grad_norm = 0.038480
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.034193
Total gradient norm: 0.111466
=== Actor Training Debug (Iteration 2633) ===
Q mean: -12.181297
Q std: 13.775159
Actor loss: 12.185272
Action reg: 0.003975
  l1.weight: grad_norm = 0.072495
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.049921
Total gradient norm: 0.218789
=== Actor Training Debug (Iteration 2634) ===
Q mean: -11.367550
Q std: 13.063825
Actor loss: 11.371534
Action reg: 0.003984
  l1.weight: grad_norm = 0.030350
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.026745
Total gradient norm: 0.095163
=== Actor Training Debug (Iteration 2635) ===
Q mean: -9.153680
Q std: 12.750196
Actor loss: 9.157661
Action reg: 0.003982
  l1.weight: grad_norm = 0.038345
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.027426
Total gradient norm: 0.092761
=== Actor Training Debug (Iteration 2636) ===
Q mean: -11.331141
Q std: 13.091834
Actor loss: 11.335117
Action reg: 0.003977
  l1.weight: grad_norm = 0.025249
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.019677
Total gradient norm: 0.065349
=== Actor Training Debug (Iteration 2637) ===
Q mean: -9.977745
Q std: 12.712702
Actor loss: 9.981720
Action reg: 0.003975
  l1.weight: grad_norm = 0.077769
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.054756
Total gradient norm: 0.209858
=== Actor Training Debug (Iteration 2638) ===
Q mean: -11.225416
Q std: 13.572166
Actor loss: 11.229403
Action reg: 0.003987
  l1.weight: grad_norm = 0.044133
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.028558
Total gradient norm: 0.128092
=== Actor Training Debug (Iteration 2639) ===
Q mean: -10.976995
Q std: 13.724556
Actor loss: 10.980970
Action reg: 0.003976
  l1.weight: grad_norm = 0.043585
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.034246
Total gradient norm: 0.145850
=== Actor Training Debug (Iteration 2640) ===
Q mean: -10.479499
Q std: 13.488457
Actor loss: 10.483480
Action reg: 0.003982
  l1.weight: grad_norm = 0.063590
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.040403
Total gradient norm: 0.193079
=== Actor Training Debug (Iteration 2641) ===
Q mean: -9.658445
Q std: 12.799363
Actor loss: 9.662430
Action reg: 0.003984
  l1.weight: grad_norm = 0.053404
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.040634
Total gradient norm: 0.185555
=== Actor Training Debug (Iteration 2642) ===
Q mean: -11.126289
Q std: 13.602303
Actor loss: 11.130270
Action reg: 0.003981
  l1.weight: grad_norm = 0.014227
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.011437
Total gradient norm: 0.048925
=== Actor Training Debug (Iteration 2643) ===
Q mean: -12.295019
Q std: 14.207437
Actor loss: 12.299003
Action reg: 0.003983
  l1.weight: grad_norm = 0.040590
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.032216
Total gradient norm: 0.105704
=== Actor Training Debug (Iteration 2644) ===
Q mean: -10.034569
Q std: 13.552004
Actor loss: 10.038546
Action reg: 0.003976
  l1.weight: grad_norm = 0.059440
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.045636
Total gradient norm: 0.190817
=== Actor Training Debug (Iteration 2645) ===
Q mean: -10.268209
Q std: 12.912841
Actor loss: 10.272185
Action reg: 0.003976
  l1.weight: grad_norm = 0.043176
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.034759
Total gradient norm: 0.149271
=== Actor Training Debug (Iteration 2646) ===
Q mean: -11.439190
Q std: 13.644393
Actor loss: 11.443169
Action reg: 0.003978
  l1.weight: grad_norm = 0.115352
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.098494
Total gradient norm: 0.504498
=== Actor Training Debug (Iteration 2647) ===
Q mean: -11.004299
Q std: 13.614379
Actor loss: 11.008286
Action reg: 0.003987
  l1.weight: grad_norm = 0.094642
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.068547
Total gradient norm: 0.320151
=== Actor Training Debug (Iteration 2648) ===
Q mean: -11.348037
Q std: 13.890919
Actor loss: 11.352011
Action reg: 0.003974
  l1.weight: grad_norm = 0.041856
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.029402
Total gradient norm: 0.127721
=== Actor Training Debug (Iteration 2649) ===
Q mean: -11.129456
Q std: 13.109557
Actor loss: 11.133442
Action reg: 0.003986
  l1.weight: grad_norm = 0.062468
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.046574
Total gradient norm: 0.163114
=== Actor Training Debug (Iteration 2650) ===
Q mean: -11.451669
Q std: 14.018533
Actor loss: 11.455648
Action reg: 0.003980
  l1.weight: grad_norm = 0.066157
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.047831
Total gradient norm: 0.164280
=== Actor Training Debug (Iteration 2651) ===
Q mean: -9.480182
Q std: 12.944184
Actor loss: 9.484167
Action reg: 0.003985
  l1.weight: grad_norm = 0.003434
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.003097
Total gradient norm: 0.012396
=== Actor Training Debug (Iteration 2652) ===
Q mean: -9.288103
Q std: 12.323692
Actor loss: 9.292083
Action reg: 0.003979
  l1.weight: grad_norm = 0.015832
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.010673
Total gradient norm: 0.041323
=== Actor Training Debug (Iteration 2653) ===
Q mean: -12.340195
Q std: 13.887099
Actor loss: 12.344161
Action reg: 0.003966
  l1.weight: grad_norm = 0.074032
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.068823
Total gradient norm: 0.254936
=== Actor Training Debug (Iteration 2654) ===
Q mean: -10.234759
Q std: 13.110966
Actor loss: 10.238746
Action reg: 0.003987
  l1.weight: grad_norm = 0.014322
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.011307
Total gradient norm: 0.054563
=== Actor Training Debug (Iteration 2655) ===
Q mean: -10.280884
Q std: 13.413339
Actor loss: 10.284863
Action reg: 0.003978
  l1.weight: grad_norm = 0.055807
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.049374
Total gradient norm: 0.221734
=== Actor Training Debug (Iteration 2656) ===
Q mean: -10.577957
Q std: 12.682670
Actor loss: 10.581951
Action reg: 0.003994
  l1.weight: grad_norm = 0.015222
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.013311
Total gradient norm: 0.049617
=== Actor Training Debug (Iteration 2657) ===
Q mean: -11.225592
Q std: 13.669474
Actor loss: 11.229571
Action reg: 0.003980
  l1.weight: grad_norm = 0.053032
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.035676
Total gradient norm: 0.161949
=== Actor Training Debug (Iteration 2658) ===
Q mean: -11.031735
Q std: 13.551626
Actor loss: 11.035702
Action reg: 0.003966
  l1.weight: grad_norm = 0.015927
  l1.bias: grad_norm = 0.000509
  l2.weight: grad_norm = 0.011902
Total gradient norm: 0.059020
=== Actor Training Debug (Iteration 2659) ===
Q mean: -9.416614
Q std: 12.522355
Actor loss: 9.420602
Action reg: 0.003989
  l1.weight: grad_norm = 0.058357
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.058973
Total gradient norm: 0.220417
=== Actor Training Debug (Iteration 2660) ===
Q mean: -10.459509
Q std: 13.331711
Actor loss: 10.463498
Action reg: 0.003989
  l1.weight: grad_norm = 0.023597
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.018488
Total gradient norm: 0.073837
=== Actor Training Debug (Iteration 2661) ===
Q mean: -10.102631
Q std: 13.461967
Actor loss: 10.106611
Action reg: 0.003981
  l1.weight: grad_norm = 0.030456
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.026325
Total gradient norm: 0.101088
=== Actor Training Debug (Iteration 2662) ===
Q mean: -11.179226
Q std: 13.384919
Actor loss: 11.183223
Action reg: 0.003997
  l1.weight: grad_norm = 0.014248
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.012016
Total gradient norm: 0.054172
=== Actor Training Debug (Iteration 2663) ===
Q mean: -11.755095
Q std: 13.690357
Actor loss: 11.759074
Action reg: 0.003979
  l1.weight: grad_norm = 0.052942
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.047314
Total gradient norm: 0.183706
=== Actor Training Debug (Iteration 2664) ===
Q mean: -10.455091
Q std: 13.282539
Actor loss: 10.459062
Action reg: 0.003971
  l1.weight: grad_norm = 0.007503
  l1.bias: grad_norm = 0.000435
  l2.weight: grad_norm = 0.005623
Total gradient norm: 0.026298
=== Actor Training Debug (Iteration 2665) ===
Q mean: -10.342184
Q std: 12.773420
Actor loss: 10.346157
Action reg: 0.003973
  l1.weight: grad_norm = 0.060757
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.043767
Total gradient norm: 0.193227
=== Actor Training Debug (Iteration 2666) ===
Q mean: -10.938297
Q std: 13.369345
Actor loss: 10.942276
Action reg: 0.003979
  l1.weight: grad_norm = 0.064961
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.044489
Total gradient norm: 0.169640
=== Actor Training Debug (Iteration 2667) ===
Q mean: -10.060108
Q std: 12.768163
Actor loss: 10.064095
Action reg: 0.003987
  l1.weight: grad_norm = 0.100639
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.069825
Total gradient norm: 0.357407
=== Actor Training Debug (Iteration 2668) ===
Q mean: -12.017795
Q std: 14.275995
Actor loss: 12.021776
Action reg: 0.003982
  l1.weight: grad_norm = 0.008881
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.007478
Total gradient norm: 0.029552
=== Actor Training Debug (Iteration 2669) ===
Q mean: -11.188034
Q std: 13.614287
Actor loss: 11.192002
Action reg: 0.003969
  l1.weight: grad_norm = 0.045458
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.038731
Total gradient norm: 0.136346
=== Actor Training Debug (Iteration 2670) ===
Q mean: -10.874221
Q std: 13.722624
Actor loss: 10.878203
Action reg: 0.003983
  l1.weight: grad_norm = 0.027472
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.020411
Total gradient norm: 0.068418
=== Actor Training Debug (Iteration 2671) ===
Q mean: -10.308996
Q std: 12.619366
Actor loss: 10.312978
Action reg: 0.003981
  l1.weight: grad_norm = 0.034649
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.028945
Total gradient norm: 0.093768
=== Actor Training Debug (Iteration 2672) ===
Q mean: -10.990860
Q std: 14.005602
Actor loss: 10.994841
Action reg: 0.003981
  l1.weight: grad_norm = 0.035516
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.030145
Total gradient norm: 0.115560
=== Actor Training Debug (Iteration 2673) ===
Q mean: -11.352066
Q std: 13.160830
Actor loss: 11.356047
Action reg: 0.003980
  l1.weight: grad_norm = 0.018960
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.015882
Total gradient norm: 0.087531
=== Actor Training Debug (Iteration 2674) ===
Q mean: -11.972248
Q std: 14.342370
Actor loss: 11.976219
Action reg: 0.003971
  l1.weight: grad_norm = 0.077101
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.063664
Total gradient norm: 0.281170
=== Actor Training Debug (Iteration 2675) ===
Q mean: -11.406662
Q std: 14.424044
Actor loss: 11.410639
Action reg: 0.003977
  l1.weight: grad_norm = 0.079735
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.059169
Total gradient norm: 0.252206
=== Actor Training Debug (Iteration 2676) ===
Q mean: -9.685816
Q std: 13.623678
Actor loss: 9.689791
Action reg: 0.003975
  l1.weight: grad_norm = 0.030037
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.025555
Total gradient norm: 0.127655
=== Actor Training Debug (Iteration 2677) ===
Q mean: -9.526525
Q std: 13.111221
Actor loss: 9.530517
Action reg: 0.003991
  l1.weight: grad_norm = 0.016274
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.012010
Total gradient norm: 0.055480
=== Actor Training Debug (Iteration 2678) ===
Q mean: -10.686001
Q std: 13.232066
Actor loss: 10.689965
Action reg: 0.003964
  l1.weight: grad_norm = 0.033472
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.025182
Total gradient norm: 0.106996
=== Actor Training Debug (Iteration 2679) ===
Q mean: -12.723701
Q std: 14.277691
Actor loss: 12.727695
Action reg: 0.003993
  l1.weight: grad_norm = 0.012108
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.007847
Total gradient norm: 0.032621
=== Actor Training Debug (Iteration 2680) ===
Q mean: -10.146597
Q std: 12.906046
Actor loss: 10.150588
Action reg: 0.003991
  l1.weight: grad_norm = 0.025638
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.020865
Total gradient norm: 0.085952
=== Actor Training Debug (Iteration 2681) ===
Q mean: -11.121204
Q std: 13.667440
Actor loss: 11.125182
Action reg: 0.003978
  l1.weight: grad_norm = 0.091418
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.074619
Total gradient norm: 0.406597
=== Actor Training Debug (Iteration 2682) ===
Q mean: -10.244009
Q std: 12.993229
Actor loss: 10.247984
Action reg: 0.003975
  l1.weight: grad_norm = 0.028016
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.027683
Total gradient norm: 0.181103
=== Actor Training Debug (Iteration 2683) ===
Q mean: -11.564694
Q std: 14.211705
Actor loss: 11.568669
Action reg: 0.003975
  l1.weight: grad_norm = 0.059759
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.045748
Total gradient norm: 0.166452
=== Actor Training Debug (Iteration 2684) ===
Q mean: -11.367262
Q std: 13.725170
Actor loss: 11.371243
Action reg: 0.003982
  l1.weight: grad_norm = 0.044963
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.034706
Total gradient norm: 0.160022
=== Actor Training Debug (Iteration 2685) ===
Q mean: -11.789248
Q std: 13.718601
Actor loss: 11.793209
Action reg: 0.003961
  l1.weight: grad_norm = 0.026364
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.017708
Total gradient norm: 0.068319
=== Actor Training Debug (Iteration 2686) ===
Q mean: -11.476123
Q std: 13.451290
Actor loss: 11.480107
Action reg: 0.003984
  l1.weight: grad_norm = 0.029954
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.024442
Total gradient norm: 0.129138
=== Actor Training Debug (Iteration 2687) ===
Q mean: -11.039112
Q std: 13.636912
Actor loss: 11.043078
Action reg: 0.003966
  l1.weight: grad_norm = 0.046304
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.035973
Total gradient norm: 0.131397
=== Actor Training Debug (Iteration 2688) ===
Q mean: -10.295560
Q std: 13.207067
Actor loss: 10.299542
Action reg: 0.003982
  l1.weight: grad_norm = 0.030220
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.022844
Total gradient norm: 0.102645
=== Actor Training Debug (Iteration 2689) ===
Q mean: -9.607883
Q std: 12.958010
Actor loss: 9.611858
Action reg: 0.003975
  l1.weight: grad_norm = 0.047378
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.037209
Total gradient norm: 0.147437
=== Actor Training Debug (Iteration 2690) ===
Q mean: -10.446701
Q std: 13.346375
Actor loss: 10.450684
Action reg: 0.003982
  l1.weight: grad_norm = 0.069423
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.053350
Total gradient norm: 0.205864
=== Actor Training Debug (Iteration 2691) ===
Q mean: -10.414909
Q std: 13.204563
Actor loss: 10.418890
Action reg: 0.003980
  l1.weight: grad_norm = 0.027388
  l1.bias: grad_norm = 0.000796
  l2.weight: grad_norm = 0.021367
Total gradient norm: 0.090705
=== Actor Training Debug (Iteration 2692) ===
Q mean: -9.940640
Q std: 13.438608
Actor loss: 9.944635
Action reg: 0.003995
  l1.weight: grad_norm = 0.026357
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.019421
Total gradient norm: 0.083751
=== Actor Training Debug (Iteration 2693) ===
Q mean: -12.557762
Q std: 14.515852
Actor loss: 12.561740
Action reg: 0.003978
  l1.weight: grad_norm = 0.094034
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.068336
Total gradient norm: 0.315899
=== Actor Training Debug (Iteration 2694) ===
Q mean: -11.323347
Q std: 13.847432
Actor loss: 11.327337
Action reg: 0.003990
  l1.weight: grad_norm = 0.039770
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.032187
Total gradient norm: 0.116609
=== Actor Training Debug (Iteration 2695) ===
Q mean: -11.797598
Q std: 13.470329
Actor loss: 11.801579
Action reg: 0.003980
  l1.weight: grad_norm = 0.023753
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.018746
Total gradient norm: 0.077537
=== Actor Training Debug (Iteration 2696) ===
Q mean: -11.193630
Q std: 14.034479
Actor loss: 11.197608
Action reg: 0.003977
  l1.weight: grad_norm = 0.033608
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.027039
Total gradient norm: 0.108252
=== Actor Training Debug (Iteration 2697) ===
Q mean: -11.591167
Q std: 14.249880
Actor loss: 11.595139
Action reg: 0.003972
  l1.weight: grad_norm = 0.045131
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.035061
Total gradient norm: 0.140784
=== Actor Training Debug (Iteration 2698) ===
Q mean: -9.837992
Q std: 13.062155
Actor loss: 9.841968
Action reg: 0.003976
  l1.weight: grad_norm = 0.042427
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.028643
Total gradient norm: 0.109562
=== Actor Training Debug (Iteration 2699) ===
Q mean: -10.338209
Q std: 13.548409
Actor loss: 10.342189
Action reg: 0.003980
  l1.weight: grad_norm = 0.015486
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.010393
Total gradient norm: 0.043194
=== Actor Training Debug (Iteration 2700) ===
Q mean: -11.041251
Q std: 13.394098
Actor loss: 11.045239
Action reg: 0.003989
  l1.weight: grad_norm = 0.051721
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.046655
Total gradient norm: 0.273261
=== Actor Training Debug (Iteration 2701) ===
Q mean: -9.633342
Q std: 13.448159
Actor loss: 9.637325
Action reg: 0.003983
  l1.weight: grad_norm = 0.081881
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.070807
Total gradient norm: 0.233972
=== Actor Training Debug (Iteration 2702) ===
Q mean: -11.045613
Q std: 13.224704
Actor loss: 11.049603
Action reg: 0.003990
  l1.weight: grad_norm = 0.035655
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.023923
Total gradient norm: 0.092322
=== Actor Training Debug (Iteration 2703) ===
Q mean: -12.566617
Q std: 13.728348
Actor loss: 12.570601
Action reg: 0.003985
  l1.weight: grad_norm = 0.054870
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.040602
Total gradient norm: 0.194050
=== Actor Training Debug (Iteration 2704) ===
Q mean: -10.242370
Q std: 13.446622
Actor loss: 10.246344
Action reg: 0.003974
  l1.weight: grad_norm = 0.081579
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.055890
Total gradient norm: 0.257477
=== Actor Training Debug (Iteration 2705) ===
Q mean: -10.435465
Q std: 13.949265
Actor loss: 10.439445
Action reg: 0.003980
  l1.weight: grad_norm = 0.054228
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.046624
Total gradient norm: 0.145957
=== Actor Training Debug (Iteration 2706) ===
Q mean: -11.969691
Q std: 13.780079
Actor loss: 11.973667
Action reg: 0.003975
  l1.weight: grad_norm = 0.028265
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.020094
Total gradient norm: 0.097004
=== Actor Training Debug (Iteration 2707) ===
Q mean: -12.236666
Q std: 14.012691
Actor loss: 12.240650
Action reg: 0.003984
  l1.weight: grad_norm = 0.021594
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.016174
Total gradient norm: 0.055898
=== Actor Training Debug (Iteration 2708) ===
Q mean: -10.400238
Q std: 12.889260
Actor loss: 10.404227
Action reg: 0.003989
  l1.weight: grad_norm = 0.023322
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.016673
Total gradient norm: 0.081945
=== Actor Training Debug (Iteration 2709) ===
Q mean: -11.126465
Q std: 13.486929
Actor loss: 11.130450
Action reg: 0.003985
  l1.weight: grad_norm = 0.054636
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.040468
Total gradient norm: 0.232396
=== Actor Training Debug (Iteration 2710) ===
Q mean: -11.043758
Q std: 13.021734
Actor loss: 11.047723
Action reg: 0.003964
  l1.weight: grad_norm = 0.073864
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.058000
Total gradient norm: 0.300176
=== Actor Training Debug (Iteration 2711) ===
Q mean: -13.060235
Q std: 14.245140
Actor loss: 13.064207
Action reg: 0.003972
  l1.weight: grad_norm = 0.053814
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.037391
Total gradient norm: 0.193273
=== Actor Training Debug (Iteration 2712) ===
Q mean: -11.742678
Q std: 13.961084
Actor loss: 11.746653
Action reg: 0.003975
  l1.weight: grad_norm = 0.048792
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.041626
Total gradient norm: 0.234016
=== Actor Training Debug (Iteration 2713) ===
Q mean: -11.945404
Q std: 13.899278
Actor loss: 11.949390
Action reg: 0.003987
  l1.weight: grad_norm = 0.013359
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.011254
Total gradient norm: 0.058029
=== Actor Training Debug (Iteration 2714) ===
Q mean: -11.051720
Q std: 13.587443
Actor loss: 11.055712
Action reg: 0.003992
  l1.weight: grad_norm = 0.007601
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.005346
Total gradient norm: 0.029633
=== Actor Training Debug (Iteration 2715) ===
Q mean: -10.983374
Q std: 13.956587
Actor loss: 10.987348
Action reg: 0.003974
  l1.weight: grad_norm = 0.068322
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.055712
Total gradient norm: 0.240873
=== Actor Training Debug (Iteration 2716) ===
Q mean: -11.105865
Q std: 13.736628
Actor loss: 11.109854
Action reg: 0.003990
  l1.weight: grad_norm = 0.044345
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.040891
Total gradient norm: 0.176804
=== Actor Training Debug (Iteration 2717) ===
Q mean: -10.914729
Q std: 14.372632
Actor loss: 10.918698
Action reg: 0.003970
  l1.weight: grad_norm = 0.061740
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.049028
Total gradient norm: 0.174446
=== Actor Training Debug (Iteration 2718) ===
Q mean: -10.682811
Q std: 13.691075
Actor loss: 10.686790
Action reg: 0.003978
  l1.weight: grad_norm = 0.051171
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.041643
Total gradient norm: 0.185353
=== Actor Training Debug (Iteration 2719) ===
Q mean: -12.133908
Q std: 14.459841
Actor loss: 12.137889
Action reg: 0.003981
  l1.weight: grad_norm = 0.017400
  l1.bias: grad_norm = 0.000595
  l2.weight: grad_norm = 0.011502
Total gradient norm: 0.056984
=== Actor Training Debug (Iteration 2720) ===
Q mean: -12.187519
Q std: 13.871522
Actor loss: 12.191505
Action reg: 0.003986
  l1.weight: grad_norm = 0.031552
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.026150
Total gradient norm: 0.102996
=== Actor Training Debug (Iteration 2721) ===
Q mean: -11.433937
Q std: 13.486906
Actor loss: 11.437916
Action reg: 0.003979
  l1.weight: grad_norm = 0.016361
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.010574
Total gradient norm: 0.045468
=== Actor Training Debug (Iteration 2722) ===
Q mean: -9.734509
Q std: 13.353654
Actor loss: 9.738473
Action reg: 0.003963
  l1.weight: grad_norm = 0.059703
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.043959
Total gradient norm: 0.148906
=== Actor Training Debug (Iteration 2723) ===
Q mean: -10.642483
Q std: 13.117887
Actor loss: 10.646465
Action reg: 0.003983
  l1.weight: grad_norm = 0.016970
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.014752
Total gradient norm: 0.060880
=== Actor Training Debug (Iteration 2724) ===
Q mean: -12.081409
Q std: 14.127913
Actor loss: 12.085399
Action reg: 0.003989
  l1.weight: grad_norm = 0.034675
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.025012
Total gradient norm: 0.088415
=== Actor Training Debug (Iteration 2725) ===
Q mean: -10.307659
Q std: 12.495381
Actor loss: 10.311635
Action reg: 0.003976
  l1.weight: grad_norm = 0.030782
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.029604
Total gradient norm: 0.152671
=== Actor Training Debug (Iteration 2726) ===
Q mean: -9.702869
Q std: 13.151073
Actor loss: 9.706822
Action reg: 0.003953
  l1.weight: grad_norm = 0.060113
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.053244
Total gradient norm: 0.228004
=== Actor Training Debug (Iteration 2727) ===
Q mean: -12.003283
Q std: 13.709456
Actor loss: 12.007252
Action reg: 0.003969
  l1.weight: grad_norm = 0.069268
  l1.bias: grad_norm = 0.000414
  l2.weight: grad_norm = 0.050511
Total gradient norm: 0.205656
=== Actor Training Debug (Iteration 2728) ===
Q mean: -10.600128
Q std: 13.125578
Actor loss: 10.604096
Action reg: 0.003969
  l1.weight: grad_norm = 0.036697
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.027096
Total gradient norm: 0.113985
=== Actor Training Debug (Iteration 2729) ===
Q mean: -10.388142
Q std: 12.919127
Actor loss: 10.392122
Action reg: 0.003981
  l1.weight: grad_norm = 0.026465
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.022262
Total gradient norm: 0.087663
=== Actor Training Debug (Iteration 2730) ===
Q mean: -10.473806
Q std: 14.302181
Actor loss: 10.477793
Action reg: 0.003987
  l1.weight: grad_norm = 0.069380
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.051315
Total gradient norm: 0.234376
=== Actor Training Debug (Iteration 2731) ===
Q mean: -9.251961
Q std: 13.139376
Actor loss: 9.255933
Action reg: 0.003972
  l1.weight: grad_norm = 0.050638
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.044146
Total gradient norm: 0.197216
=== Actor Training Debug (Iteration 2732) ===
Q mean: -10.161346
Q std: 13.501616
Actor loss: 10.165308
Action reg: 0.003961
  l1.weight: grad_norm = 0.022106
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.018860
Total gradient norm: 0.106289
=== Actor Training Debug (Iteration 2733) ===
Q mean: -10.477882
Q std: 13.807863
Actor loss: 10.481860
Action reg: 0.003978
  l1.weight: grad_norm = 0.031465
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.028573
Total gradient norm: 0.129276
=== Actor Training Debug (Iteration 2734) ===
Q mean: -11.088379
Q std: 13.752562
Actor loss: 11.092346
Action reg: 0.003967
  l1.weight: grad_norm = 0.032406
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.031318
Total gradient norm: 0.165990
=== Actor Training Debug (Iteration 2735) ===
Q mean: -10.187719
Q std: 13.370549
Actor loss: 10.191690
Action reg: 0.003971
  l1.weight: grad_norm = 0.094228
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.075904
Total gradient norm: 0.328038
=== Actor Training Debug (Iteration 2736) ===
Q mean: -11.766055
Q std: 14.148500
Actor loss: 11.770033
Action reg: 0.003977
  l1.weight: grad_norm = 0.015224
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.010767
Total gradient norm: 0.046629
=== Actor Training Debug (Iteration 2737) ===
Q mean: -11.192789
Q std: 13.605572
Actor loss: 11.196779
Action reg: 0.003991
  l1.weight: grad_norm = 0.093242
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.071779
Total gradient norm: 0.268496
=== Actor Training Debug (Iteration 2738) ===
Q mean: -11.109987
Q std: 13.679204
Actor loss: 11.113977
Action reg: 0.003990
  l1.weight: grad_norm = 0.046265
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.036365
Total gradient norm: 0.139939
=== Actor Training Debug (Iteration 2739) ===
Q mean: -10.844999
Q std: 14.170941
Actor loss: 10.848982
Action reg: 0.003982
  l1.weight: grad_norm = 0.016679
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.014058
Total gradient norm: 0.078468
=== Actor Training Debug (Iteration 2740) ===
Q mean: -10.728228
Q std: 13.744651
Actor loss: 10.732208
Action reg: 0.003981
  l1.weight: grad_norm = 0.068507
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.053418
Total gradient norm: 0.287113
=== Actor Training Debug (Iteration 2741) ===
Q mean: -11.174179
Q std: 13.435198
Actor loss: 11.178165
Action reg: 0.003986
  l1.weight: grad_norm = 0.049431
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.037178
Total gradient norm: 0.160909
=== Actor Training Debug (Iteration 2742) ===
Q mean: -9.995659
Q std: 13.770130
Actor loss: 9.999639
Action reg: 0.003980
  l1.weight: grad_norm = 0.026385
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.019133
Total gradient norm: 0.097877
=== Actor Training Debug (Iteration 2743) ===
Q mean: -11.848316
Q std: 13.918472
Actor loss: 11.852307
Action reg: 0.003992
  l1.weight: grad_norm = 0.032161
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.025850
Total gradient norm: 0.121403
=== Actor Training Debug (Iteration 2744) ===
Q mean: -10.557108
Q std: 13.775542
Actor loss: 10.561097
Action reg: 0.003989
  l1.weight: grad_norm = 0.030742
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.026214
Total gradient norm: 0.121770
=== Actor Training Debug (Iteration 2745) ===
Q mean: -11.194688
Q std: 14.503428
Actor loss: 11.198666
Action reg: 0.003978
  l1.weight: grad_norm = 0.177992
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.154437
Total gradient norm: 0.572060
=== Actor Training Debug (Iteration 2746) ===
Q mean: -11.979576
Q std: 13.596765
Actor loss: 11.983559
Action reg: 0.003983
  l1.weight: grad_norm = 0.006039
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.004431
Total gradient norm: 0.018816
=== Actor Training Debug (Iteration 2747) ===
Q mean: -11.424009
Q std: 13.501650
Actor loss: 11.427992
Action reg: 0.003983
  l1.weight: grad_norm = 0.071903
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.060313
Total gradient norm: 0.189004
=== Actor Training Debug (Iteration 2748) ===
Q mean: -11.973722
Q std: 13.822603
Actor loss: 11.977699
Action reg: 0.003977
  l1.weight: grad_norm = 0.011364
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.010325
Total gradient norm: 0.052162
=== Actor Training Debug (Iteration 2749) ===
Q mean: -9.989796
Q std: 13.041451
Actor loss: 9.993765
Action reg: 0.003969
  l1.weight: grad_norm = 0.029895
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.025184
Total gradient norm: 0.109650
=== Actor Training Debug (Iteration 2750) ===
Q mean: -12.598951
Q std: 14.290087
Actor loss: 12.602923
Action reg: 0.003972
  l1.weight: grad_norm = 0.043209
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.036273
Total gradient norm: 0.174740
=== Actor Training Debug (Iteration 2751) ===
Q mean: -11.079067
Q std: 13.699480
Actor loss: 11.083023
Action reg: 0.003956
  l1.weight: grad_norm = 0.061569
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.042926
Total gradient norm: 0.163357
=== Actor Training Debug (Iteration 2752) ===
Q mean: -10.562339
Q std: 12.615961
Actor loss: 10.566299
Action reg: 0.003960
  l1.weight: grad_norm = 0.043949
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.037924
Total gradient norm: 0.159211
=== Actor Training Debug (Iteration 2753) ===
Q mean: -11.423039
Q std: 14.496938
Actor loss: 11.427023
Action reg: 0.003983
  l1.weight: grad_norm = 0.019536
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.018141
Total gradient norm: 0.061408
=== Actor Training Debug (Iteration 2754) ===
Q mean: -10.611279
Q std: 14.599657
Actor loss: 10.615265
Action reg: 0.003987
  l1.weight: grad_norm = 0.033891
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.026397
Total gradient norm: 0.117659
=== Actor Training Debug (Iteration 2755) ===
Q mean: -10.783188
Q std: 13.768113
Actor loss: 10.787169
Action reg: 0.003981
  l1.weight: grad_norm = 0.028705
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.022499
Total gradient norm: 0.077243
=== Actor Training Debug (Iteration 2756) ===
Q mean: -12.638253
Q std: 14.487725
Actor loss: 12.642234
Action reg: 0.003981
  l1.weight: grad_norm = 0.090877
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.075087
Total gradient norm: 0.283528
=== Actor Training Debug (Iteration 2757) ===
Q mean: -10.887436
Q std: 13.672204
Actor loss: 10.891409
Action reg: 0.003973
  l1.weight: grad_norm = 0.079821
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.073647
Total gradient norm: 0.352473
=== Actor Training Debug (Iteration 2758) ===
Q mean: -11.908146
Q std: 14.090298
Actor loss: 11.912129
Action reg: 0.003983
  l1.weight: grad_norm = 0.050592
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.036762
Total gradient norm: 0.151123
=== Actor Training Debug (Iteration 2759) ===
Q mean: -8.790394
Q std: 12.558058
Actor loss: 8.794377
Action reg: 0.003984
  l1.weight: grad_norm = 0.069253
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.044522
Total gradient norm: 0.189936
=== Actor Training Debug (Iteration 2760) ===
Q mean: -10.729301
Q std: 13.240186
Actor loss: 10.733274
Action reg: 0.003972
  l1.weight: grad_norm = 0.045128
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.037263
Total gradient norm: 0.170725
=== Actor Training Debug (Iteration 2761) ===
Q mean: -12.283472
Q std: 13.994444
Actor loss: 12.287455
Action reg: 0.003983
  l1.weight: grad_norm = 0.076252
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.067403
Total gradient norm: 0.217846
=== Actor Training Debug (Iteration 2762) ===
Q mean: -11.608715
Q std: 14.604342
Actor loss: 11.612691
Action reg: 0.003976
  l1.weight: grad_norm = 0.094983
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.070535
Total gradient norm: 0.298081
=== Actor Training Debug (Iteration 2763) ===
Q mean: -10.701235
Q std: 13.868996
Actor loss: 10.705204
Action reg: 0.003969
  l1.weight: grad_norm = 0.057531
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.040185
Total gradient norm: 0.192919
=== Actor Training Debug (Iteration 2764) ===
Q mean: -10.535771
Q std: 13.514890
Actor loss: 10.539745
Action reg: 0.003974
  l1.weight: grad_norm = 0.051668
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.039142
Total gradient norm: 0.203433
=== Actor Training Debug (Iteration 2765) ===
Q mean: -10.869867
Q std: 14.447241
Actor loss: 10.873838
Action reg: 0.003971
  l1.weight: grad_norm = 0.015371
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.012417
Total gradient norm: 0.049315
=== Actor Training Debug (Iteration 2766) ===
Q mean: -10.372768
Q std: 13.131740
Actor loss: 10.376750
Action reg: 0.003982
  l1.weight: grad_norm = 0.043363
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.031282
Total gradient norm: 0.146138
=== Actor Training Debug (Iteration 2767) ===
Q mean: -12.271910
Q std: 13.837491
Actor loss: 12.275900
Action reg: 0.003991
  l1.weight: grad_norm = 0.008052
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.005397
Total gradient norm: 0.022183
=== Actor Training Debug (Iteration 2768) ===
Q mean: -12.226121
Q std: 14.080866
Actor loss: 12.230104
Action reg: 0.003983
  l1.weight: grad_norm = 0.059779
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.043556
Total gradient norm: 0.171361
=== Actor Training Debug (Iteration 2769) ===
Q mean: -10.894426
Q std: 14.364450
Actor loss: 10.898418
Action reg: 0.003992
  l1.weight: grad_norm = 0.050981
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.036466
Total gradient norm: 0.159736
=== Actor Training Debug (Iteration 2770) ===
Q mean: -11.311356
Q std: 13.978444
Actor loss: 11.315331
Action reg: 0.003975
  l1.weight: grad_norm = 0.099245
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.078106
Total gradient norm: 0.351024
=== Actor Training Debug (Iteration 2771) ===
Q mean: -11.763516
Q std: 14.486195
Actor loss: 11.767498
Action reg: 0.003982
  l1.weight: grad_norm = 0.142626
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.116756
Total gradient norm: 0.530956
=== Actor Training Debug (Iteration 2772) ===
Q mean: -12.025547
Q std: 14.680616
Actor loss: 12.029528
Action reg: 0.003981
  l1.weight: grad_norm = 0.020840
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.016419
Total gradient norm: 0.070360
=== Actor Training Debug (Iteration 2773) ===
Q mean: -11.638788
Q std: 14.104831
Actor loss: 11.642755
Action reg: 0.003966
  l1.weight: grad_norm = 0.085513
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.080735
Total gradient norm: 0.327681
=== Actor Training Debug (Iteration 2774) ===
Q mean: -11.274551
Q std: 13.642591
Actor loss: 11.278539
Action reg: 0.003988
  l1.weight: grad_norm = 0.031903
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.022619
Total gradient norm: 0.098136
=== Actor Training Debug (Iteration 2775) ===
Q mean: -11.204490
Q std: 13.917010
Actor loss: 11.208461
Action reg: 0.003971
  l1.weight: grad_norm = 0.034930
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.027756
Total gradient norm: 0.098435
=== Actor Training Debug (Iteration 2776) ===
Q mean: -12.154240
Q std: 15.052479
Actor loss: 12.158208
Action reg: 0.003969
  l1.weight: grad_norm = 0.075718
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.067679
Total gradient norm: 0.281037
=== Actor Training Debug (Iteration 2777) ===
Q mean: -11.040323
Q std: 14.068974
Actor loss: 11.044298
Action reg: 0.003975
  l1.weight: grad_norm = 0.083538
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.058584
Total gradient norm: 0.227952
=== Actor Training Debug (Iteration 2778) ===
Q mean: -11.207532
Q std: 13.583544
Actor loss: 11.211493
Action reg: 0.003962
  l1.weight: grad_norm = 0.126397
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.105595
Total gradient norm: 0.316242
=== Actor Training Debug (Iteration 2779) ===
Q mean: -11.810966
Q std: 14.774671
Actor loss: 11.814952
Action reg: 0.003985
  l1.weight: grad_norm = 0.058349
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.045483
Total gradient norm: 0.276287
=== Actor Training Debug (Iteration 2780) ===
Q mean: -12.557400
Q std: 15.191709
Actor loss: 12.561389
Action reg: 0.003989
  l1.weight: grad_norm = 0.056175
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.049082
Total gradient norm: 0.205832
=== Actor Training Debug (Iteration 2781) ===
Q mean: -10.883742
Q std: 13.658727
Actor loss: 10.887723
Action reg: 0.003981
  l1.weight: grad_norm = 0.038042
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.023462
Total gradient norm: 0.109281
=== Actor Training Debug (Iteration 2782) ===
Q mean: -11.412184
Q std: 13.892928
Actor loss: 11.416162
Action reg: 0.003978
  l1.weight: grad_norm = 0.057175
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.047752
Total gradient norm: 0.183152
=== Actor Training Debug (Iteration 2783) ===
Q mean: -10.476065
Q std: 13.189774
Actor loss: 10.480051
Action reg: 0.003987
  l1.weight: grad_norm = 0.066924
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.052789
Total gradient norm: 0.241016
=== Actor Training Debug (Iteration 2784) ===
Q mean: -11.166945
Q std: 13.888462
Actor loss: 11.170910
Action reg: 0.003966
  l1.weight: grad_norm = 0.070039
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.060898
Total gradient norm: 0.299317
=== Actor Training Debug (Iteration 2785) ===
Q mean: -11.555746
Q std: 13.711986
Actor loss: 11.559731
Action reg: 0.003986
  l1.weight: grad_norm = 0.055548
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.054460
Total gradient norm: 0.175243
=== Actor Training Debug (Iteration 2786) ===
Q mean: -10.665829
Q std: 13.626478
Actor loss: 10.669798
Action reg: 0.003969
  l1.weight: grad_norm = 0.060314
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.046054
Total gradient norm: 0.201202
=== Actor Training Debug (Iteration 2787) ===
Q mean: -11.570679
Q std: 14.521607
Actor loss: 11.574659
Action reg: 0.003981
  l1.weight: grad_norm = 0.030422
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.021280
Total gradient norm: 0.084073
=== Actor Training Debug (Iteration 2788) ===
Q mean: -11.499895
Q std: 14.199815
Actor loss: 11.503877
Action reg: 0.003982
  l1.weight: grad_norm = 0.028465
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.019531
Total gradient norm: 0.089257
=== Actor Training Debug (Iteration 2789) ===
Q mean: -11.955933
Q std: 14.168446
Actor loss: 11.959908
Action reg: 0.003975
  l1.weight: grad_norm = 0.035610
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.026461
Total gradient norm: 0.107503
=== Actor Training Debug (Iteration 2790) ===
Q mean: -11.086472
Q std: 13.294746
Actor loss: 11.090442
Action reg: 0.003971
  l1.weight: grad_norm = 0.051047
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.045647
Total gradient norm: 0.259277
=== Actor Training Debug (Iteration 2791) ===
Q mean: -9.047175
Q std: 12.990813
Actor loss: 9.051140
Action reg: 0.003965
  l1.weight: grad_norm = 0.037251
  l1.bias: grad_norm = 0.000718
  l2.weight: grad_norm = 0.025866
Total gradient norm: 0.122542
=== Actor Training Debug (Iteration 2792) ===
Q mean: -11.846731
Q std: 13.647072
Actor loss: 11.850728
Action reg: 0.003997
  l1.weight: grad_norm = 0.013998
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.010637
Total gradient norm: 0.058810
=== Actor Training Debug (Iteration 2793) ===
Q mean: -11.183477
Q std: 13.902934
Actor loss: 11.187448
Action reg: 0.003970
  l1.weight: grad_norm = 0.035113
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.028956
Total gradient norm: 0.089942
=== Actor Training Debug (Iteration 2794) ===
Q mean: -10.547795
Q std: 13.319182
Actor loss: 10.551768
Action reg: 0.003973
  l1.weight: grad_norm = 0.060258
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.041524
Total gradient norm: 0.153574
=== Actor Training Debug (Iteration 2795) ===
Q mean: -10.359042
Q std: 13.701714
Actor loss: 10.363000
Action reg: 0.003958
  l1.weight: grad_norm = 0.033160
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.029893
Total gradient norm: 0.095632
=== Actor Training Debug (Iteration 2796) ===
Q mean: -12.137314
Q std: 14.125175
Actor loss: 12.141286
Action reg: 0.003972
  l1.weight: grad_norm = 0.035502
  l1.bias: grad_norm = 0.000529
  l2.weight: grad_norm = 0.028778
Total gradient norm: 0.095606
=== Actor Training Debug (Iteration 2797) ===
Q mean: -10.123297
Q std: 13.502127
Actor loss: 10.127275
Action reg: 0.003978
  l1.weight: grad_norm = 0.066271
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.050641
Total gradient norm: 0.170850
=== Actor Training Debug (Iteration 2798) ===
Q mean: -10.215030
Q std: 12.788829
Actor loss: 10.219000
Action reg: 0.003970
  l1.weight: grad_norm = 0.061148
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.058590
Total gradient norm: 0.194274
=== Actor Training Debug (Iteration 2799) ===
Q mean: -10.613455
Q std: 13.511310
Actor loss: 10.617445
Action reg: 0.003990
  l1.weight: grad_norm = 0.029241
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.022159
Total gradient norm: 0.091194
=== Actor Training Debug (Iteration 2800) ===
Q mean: -11.909281
Q std: 13.520507
Actor loss: 11.913262
Action reg: 0.003981
  l1.weight: grad_norm = 0.029613
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.028194
Total gradient norm: 0.110845
=== Actor Training Debug (Iteration 2801) ===
Q mean: -10.832643
Q std: 13.625665
Actor loss: 10.836617
Action reg: 0.003975
  l1.weight: grad_norm = 0.047264
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.036933
Total gradient norm: 0.159985
=== Actor Training Debug (Iteration 2802) ===
Q mean: -11.961458
Q std: 14.573087
Actor loss: 11.965448
Action reg: 0.003990
  l1.weight: grad_norm = 0.013702
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.010267
Total gradient norm: 0.037369
=== Actor Training Debug (Iteration 2803) ===
Q mean: -11.458651
Q std: 14.812050
Actor loss: 11.462636
Action reg: 0.003985
  l1.weight: grad_norm = 0.064640
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.039864
Total gradient norm: 0.179587
=== Actor Training Debug (Iteration 2804) ===
Q mean: -11.482372
Q std: 13.523170
Actor loss: 11.486345
Action reg: 0.003973
  l1.weight: grad_norm = 0.055670
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.035689
Total gradient norm: 0.142591
=== Actor Training Debug (Iteration 2805) ===
Q mean: -10.875243
Q std: 12.886874
Actor loss: 10.879224
Action reg: 0.003981
  l1.weight: grad_norm = 0.106387
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.089329
Total gradient norm: 0.375405
=== Actor Training Debug (Iteration 2806) ===
Q mean: -12.425789
Q std: 14.189367
Actor loss: 12.429775
Action reg: 0.003986
  l1.weight: grad_norm = 0.031796
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.023652
Total gradient norm: 0.111410
=== Actor Training Debug (Iteration 2807) ===
Q mean: -11.627380
Q std: 13.729608
Actor loss: 11.631353
Action reg: 0.003973
  l1.weight: grad_norm = 0.049856
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.034355
Total gradient norm: 0.135269
=== Actor Training Debug (Iteration 2808) ===
Q mean: -11.365444
Q std: 13.552027
Actor loss: 11.369419
Action reg: 0.003974
  l1.weight: grad_norm = 0.047447
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.032910
Total gradient norm: 0.131683
=== Actor Training Debug (Iteration 2809) ===
Q mean: -11.853783
Q std: 14.072443
Actor loss: 11.857777
Action reg: 0.003994
  l1.weight: grad_norm = 0.066281
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.046592
Total gradient norm: 0.168676
=== Actor Training Debug (Iteration 2810) ===
Q mean: -11.591279
Q std: 13.959264
Actor loss: 11.595269
Action reg: 0.003991
  l1.weight: grad_norm = 0.014525
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.009843
Total gradient norm: 0.050142
=== Actor Training Debug (Iteration 2811) ===
Q mean: -11.377538
Q std: 14.125435
Actor loss: 11.381513
Action reg: 0.003975
  l1.weight: grad_norm = 0.007447
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.005235
Total gradient norm: 0.025455
=== Actor Training Debug (Iteration 2812) ===
Q mean: -11.370317
Q std: 13.756891
Actor loss: 11.374304
Action reg: 0.003987
  l1.weight: grad_norm = 0.053925
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.041073
Total gradient norm: 0.161805
=== Actor Training Debug (Iteration 2813) ===
Q mean: -10.977813
Q std: 13.955842
Actor loss: 10.981781
Action reg: 0.003968
  l1.weight: grad_norm = 0.012382
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.009545
Total gradient norm: 0.042799
=== Actor Training Debug (Iteration 2814) ===
Q mean: -13.450378
Q std: 14.955763
Actor loss: 13.454350
Action reg: 0.003971
  l1.weight: grad_norm = 0.034165
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.025001
Total gradient norm: 0.088731
=== Actor Training Debug (Iteration 2815) ===
Q mean: -11.718801
Q std: 14.120820
Actor loss: 11.722795
Action reg: 0.003994
  l1.weight: grad_norm = 0.016267
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.013005
Total gradient norm: 0.048170
=== Actor Training Debug (Iteration 2816) ===
Q mean: -11.837147
Q std: 13.426969
Actor loss: 11.841126
Action reg: 0.003980
  l1.weight: grad_norm = 0.064789
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.047005
Total gradient norm: 0.190420
=== Actor Training Debug (Iteration 2817) ===
Q mean: -11.231894
Q std: 13.721362
Actor loss: 11.235875
Action reg: 0.003981
  l1.weight: grad_norm = 0.015189
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.010024
Total gradient norm: 0.050290
=== Actor Training Debug (Iteration 2818) ===
Q mean: -12.919479
Q std: 14.855705
Actor loss: 12.923438
Action reg: 0.003959
  l1.weight: grad_norm = 0.105897
  l1.bias: grad_norm = 0.000674
  l2.weight: grad_norm = 0.064697
Total gradient norm: 0.304501
=== Actor Training Debug (Iteration 2819) ===
Q mean: -12.621264
Q std: 15.169065
Actor loss: 12.625259
Action reg: 0.003995
  l1.weight: grad_norm = 0.111172
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.067399
Total gradient norm: 0.280085
=== Actor Training Debug (Iteration 2820) ===
Q mean: -10.100111
Q std: 12.835772
Actor loss: 10.104074
Action reg: 0.003962
  l1.weight: grad_norm = 0.072989
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.065056
Total gradient norm: 0.315564
=== Actor Training Debug (Iteration 2821) ===
Q mean: -11.113050
Q std: 14.385919
Actor loss: 11.117030
Action reg: 0.003980
  l1.weight: grad_norm = 0.063441
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.046878
Total gradient norm: 0.211724
=== Actor Training Debug (Iteration 2822) ===
Q mean: -12.494180
Q std: 14.202478
Actor loss: 12.498158
Action reg: 0.003978
  l1.weight: grad_norm = 0.006809
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.004828
Total gradient norm: 0.016711
=== Actor Training Debug (Iteration 2823) ===
Q mean: -11.303815
Q std: 13.856345
Actor loss: 11.307808
Action reg: 0.003993
  l1.weight: grad_norm = 0.078591
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.050547
Total gradient norm: 0.216133
=== Actor Training Debug (Iteration 2824) ===
Q mean: -11.544208
Q std: 13.075874
Actor loss: 11.548191
Action reg: 0.003984
  l1.weight: grad_norm = 0.060274
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.038128
Total gradient norm: 0.158050
=== Actor Training Debug (Iteration 2825) ===
Q mean: -11.453323
Q std: 13.945719
Actor loss: 11.457303
Action reg: 0.003979
  l1.weight: grad_norm = 0.008556
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.007083
Total gradient norm: 0.033890
=== Actor Training Debug (Iteration 2826) ===
Q mean: -13.147280
Q std: 14.554074
Actor loss: 13.151252
Action reg: 0.003972
  l1.weight: grad_norm = 0.053928
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.044637
Total gradient norm: 0.223872
=== Actor Training Debug (Iteration 2827) ===
Q mean: -11.553726
Q std: 14.070639
Actor loss: 11.557703
Action reg: 0.003977
  l1.weight: grad_norm = 0.051287
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.044067
Total gradient norm: 0.161868
=== Actor Training Debug (Iteration 2828) ===
Q mean: -13.409584
Q std: 14.367085
Actor loss: 13.413570
Action reg: 0.003986
  l1.weight: grad_norm = 0.054427
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.042045
Total gradient norm: 0.160167
=== Actor Training Debug (Iteration 2829) ===
Q mean: -10.805722
Q std: 13.765924
Actor loss: 10.809711
Action reg: 0.003988
  l1.weight: grad_norm = 0.022989
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.018956
Total gradient norm: 0.080313
=== Actor Training Debug (Iteration 2830) ===
Q mean: -10.428152
Q std: 13.150712
Actor loss: 10.432126
Action reg: 0.003974
  l1.weight: grad_norm = 0.030241
  l1.bias: grad_norm = 0.000634
  l2.weight: grad_norm = 0.021448
Total gradient norm: 0.093053
=== Actor Training Debug (Iteration 2831) ===
Q mean: -11.202183
Q std: 14.082253
Actor loss: 11.206164
Action reg: 0.003982
  l1.weight: grad_norm = 0.009068
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.006966
Total gradient norm: 0.029338
=== Actor Training Debug (Iteration 2832) ===
Q mean: -8.809140
Q std: 12.856868
Actor loss: 8.813120
Action reg: 0.003979
  l1.weight: grad_norm = 0.105315
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.081245
Total gradient norm: 0.313065
=== Actor Training Debug (Iteration 2833) ===
Q mean: -11.195438
Q std: 13.696277
Actor loss: 11.199419
Action reg: 0.003981
  l1.weight: grad_norm = 0.052759
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.043045
Total gradient norm: 0.204084
=== Actor Training Debug (Iteration 2834) ===
Q mean: -10.742401
Q std: 14.106530
Actor loss: 10.746383
Action reg: 0.003982
  l1.weight: grad_norm = 0.103853
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.074921
Total gradient norm: 0.327661
=== Actor Training Debug (Iteration 2835) ===
Q mean: -12.870916
Q std: 14.914034
Actor loss: 12.874900
Action reg: 0.003984
  l1.weight: grad_norm = 0.051568
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.050195
Total gradient norm: 0.174080
=== Actor Training Debug (Iteration 2836) ===
Q mean: -11.352716
Q std: 14.839623
Actor loss: 11.356697
Action reg: 0.003980
  l1.weight: grad_norm = 0.025462
  l1.bias: grad_norm = 0.000697
  l2.weight: grad_norm = 0.021091
Total gradient norm: 0.088367
=== Actor Training Debug (Iteration 2837) ===
Q mean: -12.656899
Q std: 14.522844
Actor loss: 12.660881
Action reg: 0.003982
  l1.weight: grad_norm = 0.110128
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.078864
Total gradient norm: 0.370542
=== Actor Training Debug (Iteration 2838) ===
Q mean: -13.158741
Q std: 14.895617
Actor loss: 13.162714
Action reg: 0.003973
  l1.weight: grad_norm = 0.026675
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.017599
Total gradient norm: 0.058559
=== Actor Training Debug (Iteration 2839) ===
Q mean: -13.205109
Q std: 14.884080
Actor loss: 13.209078
Action reg: 0.003969
  l1.weight: grad_norm = 0.047737
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.031861
Total gradient norm: 0.127899
=== Actor Training Debug (Iteration 2840) ===
Q mean: -12.138341
Q std: 14.280790
Actor loss: 12.142320
Action reg: 0.003979
  l1.weight: grad_norm = 0.039322
  l1.bias: grad_norm = 0.000533
  l2.weight: grad_norm = 0.034124
Total gradient norm: 0.155347
=== Actor Training Debug (Iteration 2841) ===
Q mean: -10.028814
Q std: 13.488185
Actor loss: 10.032794
Action reg: 0.003979
  l1.weight: grad_norm = 0.058198
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.047008
Total gradient norm: 0.203540
=== Actor Training Debug (Iteration 2842) ===
Q mean: -11.505808
Q std: 14.015780
Actor loss: 11.509783
Action reg: 0.003975
  l1.weight: grad_norm = 0.040390
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.028632
Total gradient norm: 0.119167
=== Actor Training Debug (Iteration 2843) ===
Q mean: -12.233861
Q std: 14.058581
Actor loss: 12.237848
Action reg: 0.003987
  l1.weight: grad_norm = 0.034645
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.027554
Total gradient norm: 0.107857
=== Actor Training Debug (Iteration 2844) ===
Q mean: -10.944414
Q std: 14.456805
Actor loss: 10.948406
Action reg: 0.003992
  l1.weight: grad_norm = 0.034010
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.027178
Total gradient norm: 0.117277
=== Actor Training Debug (Iteration 2845) ===
Q mean: -10.112089
Q std: 13.108373
Actor loss: 10.116075
Action reg: 0.003985
  l1.weight: grad_norm = 0.025438
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.021503
Total gradient norm: 0.114399
=== Actor Training Debug (Iteration 2846) ===
Q mean: -10.876241
Q std: 14.272316
Actor loss: 10.880189
Action reg: 0.003948
  l1.weight: grad_norm = 0.059615
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.051173
Total gradient norm: 0.246707
=== Actor Training Debug (Iteration 2847) ===
Q mean: -10.908298
Q std: 14.466253
Actor loss: 10.912286
Action reg: 0.003987
  l1.weight: grad_norm = 0.051038
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.033249
Total gradient norm: 0.122431
=== Actor Training Debug (Iteration 2848) ===
Q mean: -12.133270
Q std: 14.332783
Actor loss: 12.137244
Action reg: 0.003974
  l1.weight: grad_norm = 0.078593
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.061406
Total gradient norm: 0.194421
=== Actor Training Debug (Iteration 2849) ===
Q mean: -12.511880
Q std: 14.393569
Actor loss: 12.515852
Action reg: 0.003972
  l1.weight: grad_norm = 0.035123
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.025278
Total gradient norm: 0.097901
=== Actor Training Debug (Iteration 2850) ===
Q mean: -10.795543
Q std: 13.507856
Actor loss: 10.799516
Action reg: 0.003973
  l1.weight: grad_norm = 0.033742
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.026042
Total gradient norm: 0.107447
=== Actor Training Debug (Iteration 2851) ===
Q mean: -10.349444
Q std: 14.015418
Actor loss: 10.353431
Action reg: 0.003987
  l1.weight: grad_norm = 0.027024
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.024547
Total gradient norm: 0.111830
=== Actor Training Debug (Iteration 2852) ===
Q mean: -11.957542
Q std: 14.328320
Actor loss: 11.961535
Action reg: 0.003992
  l1.weight: grad_norm = 0.052125
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.040289
Total gradient norm: 0.141849
=== Actor Training Debug (Iteration 2853) ===
Q mean: -11.772816
Q std: 12.967768
Actor loss: 11.776793
Action reg: 0.003976
  l1.weight: grad_norm = 0.038272
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.030748
Total gradient norm: 0.103008
=== Actor Training Debug (Iteration 2854) ===
Q mean: -10.621998
Q std: 14.051819
Actor loss: 10.625978
Action reg: 0.003980
  l1.weight: grad_norm = 0.059492
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.046906
Total gradient norm: 0.152864
=== Actor Training Debug (Iteration 2855) ===
Q mean: -10.972887
Q std: 13.630888
Actor loss: 10.976861
Action reg: 0.003974
  l1.weight: grad_norm = 0.039750
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.027955
Total gradient norm: 0.127072
=== Actor Training Debug (Iteration 2856) ===
Q mean: -12.165079
Q std: 14.897731
Actor loss: 12.169061
Action reg: 0.003982
  l1.weight: grad_norm = 0.023531
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.017339
Total gradient norm: 0.054357
=== Actor Training Debug (Iteration 2857) ===
Q mean: -8.700180
Q std: 12.989150
Actor loss: 8.704157
Action reg: 0.003977
  l1.weight: grad_norm = 0.055637
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.040021
Total gradient norm: 0.182395
=== Actor Training Debug (Iteration 2858) ===
Q mean: -9.751863
Q std: 13.368246
Actor loss: 9.755837
Action reg: 0.003974
  l1.weight: grad_norm = 0.069450
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.061737
Total gradient norm: 0.204956
=== Actor Training Debug (Iteration 2859) ===
Q mean: -12.066532
Q std: 15.089025
Actor loss: 12.070515
Action reg: 0.003982
  l1.weight: grad_norm = 0.066188
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.057249
Total gradient norm: 0.220218
=== Actor Training Debug (Iteration 2860) ===
Q mean: -11.337234
Q std: 14.685953
Actor loss: 11.341225
Action reg: 0.003991
  l1.weight: grad_norm = 0.027042
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.018244
Total gradient norm: 0.062614
=== Actor Training Debug (Iteration 2861) ===
Q mean: -12.142032
Q std: 14.714468
Actor loss: 12.146006
Action reg: 0.003974
  l1.weight: grad_norm = 0.006104
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.003914
Total gradient norm: 0.018020
=== Actor Training Debug (Iteration 2862) ===
Q mean: -11.112774
Q std: 13.585982
Actor loss: 11.116757
Action reg: 0.003984
  l1.weight: grad_norm = 0.060472
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.041409
Total gradient norm: 0.173838
=== Actor Training Debug (Iteration 2863) ===
Q mean: -10.414080
Q std: 13.913713
Actor loss: 10.418059
Action reg: 0.003979
  l1.weight: grad_norm = 0.050960
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.041852
Total gradient norm: 0.207766
=== Actor Training Debug (Iteration 2864) ===
Q mean: -10.512106
Q std: 14.205068
Actor loss: 10.516088
Action reg: 0.003982
  l1.weight: grad_norm = 0.035118
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.026184
Total gradient norm: 0.084271
=== Actor Training Debug (Iteration 2865) ===
Q mean: -11.673239
Q std: 14.776462
Actor loss: 11.677230
Action reg: 0.003991
  l1.weight: grad_norm = 0.063219
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.040549
Total gradient norm: 0.178756
=== Actor Training Debug (Iteration 2866) ===
Q mean: -10.832500
Q std: 13.410600
Actor loss: 10.836485
Action reg: 0.003985
  l1.weight: grad_norm = 0.062825
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.044658
Total gradient norm: 0.157966
=== Actor Training Debug (Iteration 2867) ===
Q mean: -11.082222
Q std: 13.448406
Actor loss: 11.086208
Action reg: 0.003986
  l1.weight: grad_norm = 0.033380
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.026157
Total gradient norm: 0.105026
=== Actor Training Debug (Iteration 2868) ===
Q mean: -11.857723
Q std: 14.586291
Actor loss: 11.861701
Action reg: 0.003978
  l1.weight: grad_norm = 0.088058
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.064784
Total gradient norm: 0.315392
=== Actor Training Debug (Iteration 2869) ===
Q mean: -11.897004
Q std: 14.232248
Actor loss: 11.900979
Action reg: 0.003975
  l1.weight: grad_norm = 0.038285
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.032472
Total gradient norm: 0.110919
=== Actor Training Debug (Iteration 2870) ===
Q mean: -11.140529
Q std: 14.137954
Actor loss: 11.144512
Action reg: 0.003983
  l1.weight: grad_norm = 0.053338
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.040621
Total gradient norm: 0.144102
=== Actor Training Debug (Iteration 2871) ===
Q mean: -11.157598
Q std: 14.483114
Actor loss: 11.161565
Action reg: 0.003968
  l1.weight: grad_norm = 0.057876
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.039781
Total gradient norm: 0.163677
=== Actor Training Debug (Iteration 2872) ===
Q mean: -10.127580
Q std: 13.468359
Actor loss: 10.131561
Action reg: 0.003981
  l1.weight: grad_norm = 0.028184
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.018971
Total gradient norm: 0.079867
=== Actor Training Debug (Iteration 2873) ===
Q mean: -12.995365
Q std: 14.123766
Actor loss: 12.999351
Action reg: 0.003985
  l1.weight: grad_norm = 0.026752
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.022727
Total gradient norm: 0.090374
=== Actor Training Debug (Iteration 2874) ===
Q mean: -10.540953
Q std: 14.030687
Actor loss: 10.544929
Action reg: 0.003975
  l1.weight: grad_norm = 0.055151
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.043236
Total gradient norm: 0.179488
=== Actor Training Debug (Iteration 2875) ===
Q mean: -10.557726
Q std: 12.850274
Actor loss: 10.561701
Action reg: 0.003975
  l1.weight: grad_norm = 0.030322
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.024246
Total gradient norm: 0.111797
=== Actor Training Debug (Iteration 2876) ===
Q mean: -10.840724
Q std: 13.758887
Actor loss: 10.844700
Action reg: 0.003976
  l1.weight: grad_norm = 0.085916
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.065508
Total gradient norm: 0.275960
=== Actor Training Debug (Iteration 2877) ===
Q mean: -10.872725
Q std: 13.748943
Actor loss: 10.876701
Action reg: 0.003977
  l1.weight: grad_norm = 0.079525
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.056825
Total gradient norm: 0.281065
=== Actor Training Debug (Iteration 2878) ===
Q mean: -12.388357
Q std: 14.151330
Actor loss: 12.392340
Action reg: 0.003983
  l1.weight: grad_norm = 0.041258
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.029705
Total gradient norm: 0.115602
=== Actor Training Debug (Iteration 2879) ===
Q mean: -11.208122
Q std: 14.409770
Actor loss: 11.212102
Action reg: 0.003980
  l1.weight: grad_norm = 0.042463
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.032108
Total gradient norm: 0.138498
=== Actor Training Debug (Iteration 2880) ===
Q mean: -10.514729
Q std: 14.014324
Actor loss: 10.518698
Action reg: 0.003969
  l1.weight: grad_norm = 0.069296
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.055962
Total gradient norm: 0.195122
=== Actor Training Debug (Iteration 2881) ===
Q mean: -10.670710
Q std: 13.510322
Actor loss: 10.674691
Action reg: 0.003982
  l1.weight: grad_norm = 0.066320
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.053326
Total gradient norm: 0.208180
=== Actor Training Debug (Iteration 2882) ===
Q mean: -11.717911
Q std: 14.416039
Actor loss: 11.721898
Action reg: 0.003988
  l1.weight: grad_norm = 0.038110
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.030349
Total gradient norm: 0.124574
=== Actor Training Debug (Iteration 2883) ===
Q mean: -12.411131
Q std: 14.495528
Actor loss: 12.415111
Action reg: 0.003980
  l1.weight: grad_norm = 0.091883
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.072741
Total gradient norm: 0.193598
=== Actor Training Debug (Iteration 2884) ===
Q mean: -10.937421
Q std: 14.435158
Actor loss: 10.941393
Action reg: 0.003972
  l1.weight: grad_norm = 0.055716
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.039987
Total gradient norm: 0.216999
=== Actor Training Debug (Iteration 2885) ===
Q mean: -13.213156
Q std: 14.636168
Actor loss: 13.217146
Action reg: 0.003990
  l1.weight: grad_norm = 0.021252
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.019794
Total gradient norm: 0.067597
=== Actor Training Debug (Iteration 2886) ===
Q mean: -12.597243
Q std: 14.472665
Actor loss: 12.601217
Action reg: 0.003974
  l1.weight: grad_norm = 0.028763
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.024564
Total gradient norm: 0.097709
=== Actor Training Debug (Iteration 2887) ===
Q mean: -12.082340
Q std: 14.029350
Actor loss: 12.086311
Action reg: 0.003971
  l1.weight: grad_norm = 0.031861
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.022250
Total gradient norm: 0.089420
=== Actor Training Debug (Iteration 2888) ===
Q mean: -11.637461
Q std: 14.426343
Actor loss: 11.641428
Action reg: 0.003967
  l1.weight: grad_norm = 0.016222
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.014793
Total gradient norm: 0.046081
=== Actor Training Debug (Iteration 2889) ===
Q mean: -11.548606
Q std: 14.533145
Actor loss: 11.552577
Action reg: 0.003971
  l1.weight: grad_norm = 0.091007
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.069337
Total gradient norm: 0.314683
=== Actor Training Debug (Iteration 2890) ===
Q mean: -10.607440
Q std: 14.037957
Actor loss: 10.611416
Action reg: 0.003976
  l1.weight: grad_norm = 0.056365
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.036153
Total gradient norm: 0.173669
=== Actor Training Debug (Iteration 2891) ===
Q mean: -10.874057
Q std: 13.490565
Actor loss: 10.878027
Action reg: 0.003970
  l1.weight: grad_norm = 0.127672
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.105570
Total gradient norm: 0.387076
=== Actor Training Debug (Iteration 2892) ===
Q mean: -11.562677
Q std: 13.303211
Actor loss: 11.566658
Action reg: 0.003981
  l1.weight: grad_norm = 0.047968
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.036399
Total gradient norm: 0.162208
=== Actor Training Debug (Iteration 2893) ===
Q mean: -10.172018
Q std: 13.866537
Actor loss: 10.175979
Action reg: 0.003961
  l1.weight: grad_norm = 0.055013
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.047138
Total gradient norm: 0.234093
=== Actor Training Debug (Iteration 2894) ===
Q mean: -11.386189
Q std: 14.427928
Actor loss: 11.390176
Action reg: 0.003988
  l1.weight: grad_norm = 0.024009
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.018666
Total gradient norm: 0.062732
=== Actor Training Debug (Iteration 2895) ===
Q mean: -9.116021
Q std: 12.731076
Actor loss: 9.119988
Action reg: 0.003967
  l1.weight: grad_norm = 0.027592
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.023025
Total gradient norm: 0.088291
=== Actor Training Debug (Iteration 2896) ===
Q mean: -10.652039
Q std: 14.342431
Actor loss: 10.656018
Action reg: 0.003979
  l1.weight: grad_norm = 0.083360
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.063290
Total gradient norm: 0.251172
=== Actor Training Debug (Iteration 2897) ===
Q mean: -11.501513
Q std: 14.626348
Actor loss: 11.505498
Action reg: 0.003984
  l1.weight: grad_norm = 0.123347
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.086650
Total gradient norm: 0.338216
=== Actor Training Debug (Iteration 2898) ===
Q mean: -10.377905
Q std: 13.479924
Actor loss: 10.381879
Action reg: 0.003974
  l1.weight: grad_norm = 0.037953
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.027499
Total gradient norm: 0.110712
=== Actor Training Debug (Iteration 2899) ===
Q mean: -11.907574
Q std: 13.929220
Actor loss: 11.911553
Action reg: 0.003980
  l1.weight: grad_norm = 0.012869
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.011736
Total gradient norm: 0.036135
=== Actor Training Debug (Iteration 2900) ===
Q mean: -13.598820
Q std: 15.375958
Actor loss: 13.602803
Action reg: 0.003983
  l1.weight: grad_norm = 0.036243
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.027887
Total gradient norm: 0.106961
=== Actor Training Debug (Iteration 2901) ===
Q mean: -12.140897
Q std: 14.667273
Actor loss: 12.144881
Action reg: 0.003985
  l1.weight: grad_norm = 0.059783
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.039815
Total gradient norm: 0.161564
=== Actor Training Debug (Iteration 2902) ===
Q mean: -10.220860
Q std: 13.459835
Actor loss: 10.224823
Action reg: 0.003964
  l1.weight: grad_norm = 0.024483
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.019553
Total gradient norm: 0.075533
=== Actor Training Debug (Iteration 2903) ===
Q mean: -10.802376
Q std: 13.702811
Actor loss: 10.806352
Action reg: 0.003976
  l1.weight: grad_norm = 0.144047
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.100807
Total gradient norm: 0.396046
=== Actor Training Debug (Iteration 2904) ===
Q mean: -12.005807
Q std: 14.842467
Actor loss: 12.009783
Action reg: 0.003976
  l1.weight: grad_norm = 0.034644
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.026988
Total gradient norm: 0.080301
=== Actor Training Debug (Iteration 2905) ===
Q mean: -10.502482
Q std: 13.683749
Actor loss: 10.506454
Action reg: 0.003973
  l1.weight: grad_norm = 0.095702
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.072631
Total gradient norm: 0.302072
=== Actor Training Debug (Iteration 2906) ===
Q mean: -10.058159
Q std: 14.504767
Actor loss: 10.062140
Action reg: 0.003981
  l1.weight: grad_norm = 0.082626
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.054991
Total gradient norm: 0.238258
=== Actor Training Debug (Iteration 2907) ===
Q mean: -11.008960
Q std: 14.378366
Actor loss: 11.012938
Action reg: 0.003979
  l1.weight: grad_norm = 0.024905
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.020173
Total gradient norm: 0.074102
=== Actor Training Debug (Iteration 2908) ===
Q mean: -11.107800
Q std: 14.667208
Actor loss: 11.111779
Action reg: 0.003979
  l1.weight: grad_norm = 0.028378
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.018913
Total gradient norm: 0.074866
=== Actor Training Debug (Iteration 2909) ===
Q mean: -12.085316
Q std: 14.357995
Actor loss: 12.089300
Action reg: 0.003984
  l1.weight: grad_norm = 0.033139
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.032171
Total gradient norm: 0.128923
=== Actor Training Debug (Iteration 2910) ===
Q mean: -11.551245
Q std: 14.267488
Actor loss: 11.555230
Action reg: 0.003985
  l1.weight: grad_norm = 0.037681
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.036339
Total gradient norm: 0.132557
=== Actor Training Debug (Iteration 2911) ===
Q mean: -10.708355
Q std: 14.036954
Actor loss: 10.712340
Action reg: 0.003985
  l1.weight: grad_norm = 0.044061
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.029623
Total gradient norm: 0.109037
=== Actor Training Debug (Iteration 2912) ===
Q mean: -11.806183
Q std: 14.223104
Actor loss: 11.810166
Action reg: 0.003983
  l1.weight: grad_norm = 0.052195
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.035760
Total gradient norm: 0.159225
=== Actor Training Debug (Iteration 2913) ===
Q mean: -12.470428
Q std: 14.954595
Actor loss: 12.474407
Action reg: 0.003980
  l1.weight: grad_norm = 0.045405
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.030744
Total gradient norm: 0.124565
=== Actor Training Debug (Iteration 2914) ===
Q mean: -10.829634
Q std: 14.255030
Actor loss: 10.833602
Action reg: 0.003968
  l1.weight: grad_norm = 0.097564
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.070122
Total gradient norm: 0.260099
=== Actor Training Debug (Iteration 2915) ===
Q mean: -10.967074
Q std: 14.180014
Actor loss: 10.971045
Action reg: 0.003971
  l1.weight: grad_norm = 0.170966
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.187757
Total gradient norm: 0.739127
=== Actor Training Debug (Iteration 2916) ===
Q mean: -10.203222
Q std: 13.937233
Actor loss: 10.207195
Action reg: 0.003973
  l1.weight: grad_norm = 0.054830
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.051052
Total gradient norm: 0.221601
=== Actor Training Debug (Iteration 2917) ===
Q mean: -10.570911
Q std: 13.852898
Actor loss: 10.574879
Action reg: 0.003968
  l1.weight: grad_norm = 0.078690
  l1.bias: grad_norm = 0.000546
  l2.weight: grad_norm = 0.062243
Total gradient norm: 0.302844
=== Actor Training Debug (Iteration 2918) ===
Q mean: -11.914970
Q std: 14.091396
Actor loss: 11.918954
Action reg: 0.003983
  l1.weight: grad_norm = 0.038472
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.031066
Total gradient norm: 0.130794
=== Actor Training Debug (Iteration 2919) ===
Q mean: -12.010162
Q std: 14.381131
Actor loss: 12.014144
Action reg: 0.003982
  l1.weight: grad_norm = 0.023482
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.021377
Total gradient norm: 0.100322
=== Actor Training Debug (Iteration 2920) ===
Q mean: -10.335302
Q std: 14.103182
Actor loss: 10.339263
Action reg: 0.003961
  l1.weight: grad_norm = 0.018761
  l1.bias: grad_norm = 0.000630
  l2.weight: grad_norm = 0.017479
Total gradient norm: 0.075772
=== Actor Training Debug (Iteration 2921) ===
Q mean: -10.321296
Q std: 13.448730
Actor loss: 10.325286
Action reg: 0.003990
  l1.weight: grad_norm = 0.043079
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.032358
Total gradient norm: 0.119229
=== Actor Training Debug (Iteration 2922) ===
Q mean: -10.436014
Q std: 14.761834
Actor loss: 10.439968
Action reg: 0.003954
  l1.weight: grad_norm = 0.097411
  l1.bias: grad_norm = 0.000724
  l2.weight: grad_norm = 0.064619
Total gradient norm: 0.241045
=== Actor Training Debug (Iteration 2923) ===
Q mean: -11.777635
Q std: 14.954397
Actor loss: 11.781607
Action reg: 0.003972
  l1.weight: grad_norm = 0.158780
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.130487
Total gradient norm: 0.671066
=== Actor Training Debug (Iteration 2924) ===
Q mean: -12.025129
Q std: 15.162082
Actor loss: 12.029107
Action reg: 0.003978
  l1.weight: grad_norm = 0.072494
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.056359
Total gradient norm: 0.231537
=== Actor Training Debug (Iteration 2925) ===
Q mean: -11.217163
Q std: 14.071131
Actor loss: 11.221140
Action reg: 0.003977
  l1.weight: grad_norm = 0.027602
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.022342
Total gradient norm: 0.088200
=== Actor Training Debug (Iteration 2926) ===
Q mean: -11.772193
Q std: 14.265464
Actor loss: 11.776175
Action reg: 0.003983
  l1.weight: grad_norm = 0.050179
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.036277
Total gradient norm: 0.117160
=== Actor Training Debug (Iteration 2927) ===
Q mean: -11.439181
Q std: 14.450006
Actor loss: 11.443167
Action reg: 0.003986
  l1.weight: grad_norm = 0.047757
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.039269
Total gradient norm: 0.169085
=== Actor Training Debug (Iteration 2928) ===
Q mean: -10.272184
Q std: 13.608521
Actor loss: 10.276155
Action reg: 0.003970
  l1.weight: grad_norm = 0.080033
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.066221
Total gradient norm: 0.335687
=== Actor Training Debug (Iteration 2929) ===
Q mean: -12.333288
Q std: 14.241765
Actor loss: 12.337262
Action reg: 0.003974
  l1.weight: grad_norm = 0.102928
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.088226
Total gradient norm: 0.348098
=== Actor Training Debug (Iteration 2930) ===
Q mean: -11.932409
Q std: 14.377652
Actor loss: 11.936392
Action reg: 0.003982
  l1.weight: grad_norm = 0.037135
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.028164
Total gradient norm: 0.094039
=== Actor Training Debug (Iteration 2931) ===
Q mean: -11.641705
Q std: 14.276502
Actor loss: 11.645679
Action reg: 0.003975
  l1.weight: grad_norm = 0.052653
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.034603
Total gradient norm: 0.148981
=== Actor Training Debug (Iteration 2932) ===
Q mean: -12.689676
Q std: 15.169953
Actor loss: 12.693660
Action reg: 0.003984
  l1.weight: grad_norm = 0.072945
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.067473
Total gradient norm: 0.255122
=== Actor Training Debug (Iteration 2933) ===
Q mean: -11.720509
Q std: 14.466413
Actor loss: 11.724490
Action reg: 0.003982
  l1.weight: grad_norm = 0.105268
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.079222
Total gradient norm: 0.361765
=== Actor Training Debug (Iteration 2934) ===
Q mean: -11.595163
Q std: 14.468711
Actor loss: 11.599143
Action reg: 0.003980
  l1.weight: grad_norm = 0.063530
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.045760
Total gradient norm: 0.186811
=== Actor Training Debug (Iteration 2935) ===
Q mean: -12.219226
Q std: 14.735331
Actor loss: 12.223197
Action reg: 0.003971
  l1.weight: grad_norm = 0.064388
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.048700
Total gradient norm: 0.169487
=== Actor Training Debug (Iteration 2936) ===
Q mean: -11.673828
Q std: 14.188719
Actor loss: 11.677814
Action reg: 0.003986
  l1.weight: grad_norm = 0.034484
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.023756
Total gradient norm: 0.120214
=== Actor Training Debug (Iteration 2937) ===
Q mean: -10.534952
Q std: 14.272163
Actor loss: 10.538919
Action reg: 0.003967
  l1.weight: grad_norm = 0.082845
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.063560
Total gradient norm: 0.267312
=== Actor Training Debug (Iteration 2938) ===
Q mean: -12.543053
Q std: 14.499710
Actor loss: 12.547017
Action reg: 0.003965
  l1.weight: grad_norm = 0.064948
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.050179
Total gradient norm: 0.218781
=== Actor Training Debug (Iteration 2939) ===
Q mean: -11.411158
Q std: 14.179427
Actor loss: 11.415138
Action reg: 0.003980
  l1.weight: grad_norm = 0.029121
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.025921
Total gradient norm: 0.073886
=== Actor Training Debug (Iteration 2940) ===
Q mean: -12.287411
Q std: 15.132987
Actor loss: 12.291399
Action reg: 0.003988
  l1.weight: grad_norm = 0.079784
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.064507
Total gradient norm: 0.216519
=== Actor Training Debug (Iteration 2941) ===
Q mean: -10.751951
Q std: 14.218320
Actor loss: 10.755940
Action reg: 0.003989
  l1.weight: grad_norm = 0.086559
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.061641
Total gradient norm: 0.242977
=== Actor Training Debug (Iteration 2942) ===
Q mean: -10.742163
Q std: 15.207992
Actor loss: 10.746138
Action reg: 0.003975
  l1.weight: grad_norm = 0.100652
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.095065
Total gradient norm: 0.406974
=== Actor Training Debug (Iteration 2943) ===
Q mean: -12.675280
Q std: 15.141391
Actor loss: 12.679266
Action reg: 0.003986
  l1.weight: grad_norm = 0.053269
  l1.bias: grad_norm = 0.000581
  l2.weight: grad_norm = 0.037066
Total gradient norm: 0.167093
=== Actor Training Debug (Iteration 2944) ===
Q mean: -11.745929
Q std: 14.868509
Actor loss: 11.749901
Action reg: 0.003972
  l1.weight: grad_norm = 0.023276
  l1.bias: grad_norm = 0.000771
  l2.weight: grad_norm = 0.017868
Total gradient norm: 0.072305
=== Actor Training Debug (Iteration 2945) ===
Q mean: -12.046604
Q std: 14.270368
Actor loss: 12.050597
Action reg: 0.003993
  l1.weight: grad_norm = 0.025934
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.016935
Total gradient norm: 0.078699
=== Actor Training Debug (Iteration 2946) ===
Q mean: -10.661304
Q std: 13.833353
Actor loss: 10.665296
Action reg: 0.003992
  l1.weight: grad_norm = 0.034698
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.026801
Total gradient norm: 0.110747
=== Actor Training Debug (Iteration 2947) ===
Q mean: -9.065046
Q std: 12.727671
Actor loss: 9.069005
Action reg: 0.003959
  l1.weight: grad_norm = 0.092585
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.068951
Total gradient norm: 0.267192
=== Actor Training Debug (Iteration 2948) ===
Q mean: -12.333979
Q std: 15.206881
Actor loss: 12.337949
Action reg: 0.003970
  l1.weight: grad_norm = 0.110476
  l1.bias: grad_norm = 0.000752
  l2.weight: grad_norm = 0.072572
Total gradient norm: 0.273454
=== Actor Training Debug (Iteration 2949) ===
Q mean: -10.118766
Q std: 13.740741
Actor loss: 10.122746
Action reg: 0.003981
  l1.weight: grad_norm = 0.058966
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.042697
Total gradient norm: 0.206127
=== Actor Training Debug (Iteration 2950) ===
Q mean: -10.620224
Q std: 14.308092
Actor loss: 10.624205
Action reg: 0.003981
  l1.weight: grad_norm = 0.089261
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.061441
Total gradient norm: 0.306317
=== Actor Training Debug (Iteration 2951) ===
Q mean: -10.877602
Q std: 14.201997
Actor loss: 10.881580
Action reg: 0.003979
  l1.weight: grad_norm = 0.069272
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.048874
Total gradient norm: 0.184553
=== Actor Training Debug (Iteration 2952) ===
Q mean: -13.186844
Q std: 14.843791
Actor loss: 13.190825
Action reg: 0.003980
  l1.weight: grad_norm = 0.037220
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.030330
Total gradient norm: 0.122303
=== Actor Training Debug (Iteration 2953) ===
Q mean: -11.682708
Q std: 15.031884
Actor loss: 11.686690
Action reg: 0.003983
  l1.weight: grad_norm = 0.081649
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.055211
Total gradient norm: 0.218888
=== Actor Training Debug (Iteration 2954) ===
Q mean: -13.772033
Q std: 15.361805
Actor loss: 13.776004
Action reg: 0.003971
  l1.weight: grad_norm = 0.060733
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.048873
Total gradient norm: 0.163729
=== Actor Training Debug (Iteration 2955) ===
Q mean: -9.000229
Q std: 13.152404
Actor loss: 9.004211
Action reg: 0.003983
  l1.weight: grad_norm = 0.033072
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.025187
Total gradient norm: 0.110109
=== Actor Training Debug (Iteration 2956) ===
Q mean: -12.038616
Q std: 14.329003
Actor loss: 12.042594
Action reg: 0.003977
  l1.weight: grad_norm = 0.051571
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.043454
Total gradient norm: 0.169981
=== Actor Training Debug (Iteration 2957) ===
Q mean: -13.954820
Q std: 15.524162
Actor loss: 13.958775
Action reg: 0.003955
  l1.weight: grad_norm = 0.070366
  l1.bias: grad_norm = 0.000698
  l2.weight: grad_norm = 0.057278
Total gradient norm: 0.232828
=== Actor Training Debug (Iteration 2958) ===
Q mean: -12.658488
Q std: 14.992151
Actor loss: 12.662479
Action reg: 0.003991
  l1.weight: grad_norm = 0.020730
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.019456
Total gradient norm: 0.114950
=== Actor Training Debug (Iteration 2959) ===
Q mean: -11.356627
Q std: 14.361973
Actor loss: 11.360606
Action reg: 0.003979
  l1.weight: grad_norm = 0.179826
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.122368
Total gradient norm: 0.461395
=== Actor Training Debug (Iteration 2960) ===
Q mean: -9.932678
Q std: 13.175329
Actor loss: 9.936661
Action reg: 0.003983
  l1.weight: grad_norm = 0.034212
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.022689
Total gradient norm: 0.091725
=== Actor Training Debug (Iteration 2961) ===
Q mean: -12.347755
Q std: 14.737845
Actor loss: 12.351747
Action reg: 0.003991
  l1.weight: grad_norm = 0.041273
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.038505
Total gradient norm: 0.129570
=== Actor Training Debug (Iteration 2962) ===
Q mean: -12.626703
Q std: 15.472017
Actor loss: 12.630683
Action reg: 0.003980
  l1.weight: grad_norm = 0.072318
  l1.bias: grad_norm = 0.001005
  l2.weight: grad_norm = 0.056177
Total gradient norm: 0.281520
=== Actor Training Debug (Iteration 2963) ===
Q mean: -11.492577
Q std: 14.675018
Actor loss: 11.496564
Action reg: 0.003987
  l1.weight: grad_norm = 0.035054
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.028728
Total gradient norm: 0.129589
=== Actor Training Debug (Iteration 2964) ===
Q mean: -12.523211
Q std: 14.568806
Actor loss: 12.527199
Action reg: 0.003988
  l1.weight: grad_norm = 0.037141
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.024138
Total gradient norm: 0.092305
=== Actor Training Debug (Iteration 2965) ===
Q mean: -11.154657
Q std: 14.990643
Actor loss: 11.158636
Action reg: 0.003979
  l1.weight: grad_norm = 0.094845
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.062631
Total gradient norm: 0.223233
=== Actor Training Debug (Iteration 2966) ===
Q mean: -11.822654
Q std: 14.816886
Actor loss: 11.826645
Action reg: 0.003992
  l1.weight: grad_norm = 0.060469
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.041579
Total gradient norm: 0.190150
=== Actor Training Debug (Iteration 2967) ===
Q mean: -11.961330
Q std: 14.558876
Actor loss: 11.965304
Action reg: 0.003974
  l1.weight: grad_norm = 0.091413
  l1.bias: grad_norm = 0.000727
  l2.weight: grad_norm = 0.067709
Total gradient norm: 0.273483
=== Actor Training Debug (Iteration 2968) ===
Q mean: -10.882406
Q std: 14.641312
Actor loss: 10.886384
Action reg: 0.003978
  l1.weight: grad_norm = 0.107170
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.073957
Total gradient norm: 0.381300
=== Actor Training Debug (Iteration 2969) ===
Q mean: -12.287113
Q std: 14.895761
Actor loss: 12.291096
Action reg: 0.003983
  l1.weight: grad_norm = 0.049147
  l1.bias: grad_norm = 0.000600
  l2.weight: grad_norm = 0.040193
Total gradient norm: 0.149629
=== Actor Training Debug (Iteration 2970) ===
Q mean: -11.308712
Q std: 14.404061
Actor loss: 11.312677
Action reg: 0.003966
  l1.weight: grad_norm = 0.059792
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.043246
Total gradient norm: 0.171801
=== Actor Training Debug (Iteration 2971) ===
Q mean: -10.526661
Q std: 14.085389
Actor loss: 10.530644
Action reg: 0.003984
  l1.weight: grad_norm = 0.037897
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.033229
Total gradient norm: 0.155025
=== Actor Training Debug (Iteration 2972) ===
Q mean: -12.250177
Q std: 15.057879
Actor loss: 12.254153
Action reg: 0.003976
  l1.weight: grad_norm = 0.106590
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.079735
Total gradient norm: 0.289165
=== Actor Training Debug (Iteration 2973) ===
Q mean: -11.050386
Q std: 14.828904
Actor loss: 11.054353
Action reg: 0.003966
  l1.weight: grad_norm = 0.060505
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.044319
Total gradient norm: 0.205756
=== Actor Training Debug (Iteration 2974) ===
Q mean: -11.867798
Q std: 14.732433
Actor loss: 11.871780
Action reg: 0.003982
  l1.weight: grad_norm = 0.047477
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.043007
Total gradient norm: 0.264547
=== Actor Training Debug (Iteration 2975) ===
Q mean: -12.448847
Q std: 14.910217
Actor loss: 12.452823
Action reg: 0.003976
  l1.weight: grad_norm = 0.053660
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.043288
Total gradient norm: 0.213493
=== Actor Training Debug (Iteration 2976) ===
Q mean: -10.252127
Q std: 13.672328
Actor loss: 10.256099
Action reg: 0.003972
  l1.weight: grad_norm = 0.070867
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.053725
Total gradient norm: 0.203979
=== Actor Training Debug (Iteration 2977) ===
Q mean: -13.805948
Q std: 15.821103
Actor loss: 13.809928
Action reg: 0.003980
  l1.weight: grad_norm = 0.067498
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.050233
Total gradient norm: 0.200882
=== Actor Training Debug (Iteration 2978) ===
Q mean: -11.882224
Q std: 14.327347
Actor loss: 11.886189
Action reg: 0.003965
  l1.weight: grad_norm = 0.107359
  l1.bias: grad_norm = 0.000611
  l2.weight: grad_norm = 0.080989
Total gradient norm: 0.288754
=== Actor Training Debug (Iteration 2979) ===
Q mean: -11.780617
Q std: 14.539834
Actor loss: 11.784599
Action reg: 0.003982
  l1.weight: grad_norm = 0.014844
  l1.bias: grad_norm = 0.000484
  l2.weight: grad_norm = 0.012010
Total gradient norm: 0.057716
=== Actor Training Debug (Iteration 2980) ===
Q mean: -11.447933
Q std: 14.859131
Actor loss: 11.451922
Action reg: 0.003989
  l1.weight: grad_norm = 0.046782
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.037843
Total gradient norm: 0.175758
=== Actor Training Debug (Iteration 2981) ===
Q mean: -12.382689
Q std: 14.447744
Actor loss: 12.386662
Action reg: 0.003974
  l1.weight: grad_norm = 0.091151
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.065597
Total gradient norm: 0.257929
=== Actor Training Debug (Iteration 2982) ===
Q mean: -11.760898
Q std: 14.411715
Actor loss: 11.764890
Action reg: 0.003992
  l1.weight: grad_norm = 0.013032
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.009978
Total gradient norm: 0.030780
=== Actor Training Debug (Iteration 2983) ===
Q mean: -10.378460
Q std: 13.783074
Actor loss: 10.382427
Action reg: 0.003967
  l1.weight: grad_norm = 0.042667
  l1.bias: grad_norm = 0.000998
  l2.weight: grad_norm = 0.029536
Total gradient norm: 0.122908
=== Actor Training Debug (Iteration 2984) ===
Q mean: -10.804745
Q std: 14.309993
Actor loss: 10.808714
Action reg: 0.003969
  l1.weight: grad_norm = 0.057952
  l1.bias: grad_norm = 0.000662
  l2.weight: grad_norm = 0.048429
Total gradient norm: 0.188905
=== Actor Training Debug (Iteration 2985) ===
Q mean: -11.917845
Q std: 14.606737
Actor loss: 11.921821
Action reg: 0.003976
  l1.weight: grad_norm = 0.075495
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.058905
Total gradient norm: 0.232131
=== Actor Training Debug (Iteration 2986) ===
Q mean: -11.654894
Q std: 14.419014
Actor loss: 11.658867
Action reg: 0.003973
  l1.weight: grad_norm = 0.031733
  l1.bias: grad_norm = 0.000776
  l2.weight: grad_norm = 0.025138
Total gradient norm: 0.097058
=== Actor Training Debug (Iteration 2987) ===
Q mean: -12.679016
Q std: 15.097877
Actor loss: 12.683002
Action reg: 0.003987
  l1.weight: grad_norm = 0.146008
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.101200
Total gradient norm: 0.413650
=== Actor Training Debug (Iteration 2988) ===
Q mean: -13.413696
Q std: 15.369476
Actor loss: 13.417677
Action reg: 0.003980
  l1.weight: grad_norm = 0.045795
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.032138
Total gradient norm: 0.147976
=== Actor Training Debug (Iteration 2989) ===
Q mean: -13.212214
Q std: 15.082482
Actor loss: 13.216198
Action reg: 0.003984
  l1.weight: grad_norm = 0.062164
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.057351
Total gradient norm: 0.199057
=== Actor Training Debug (Iteration 2990) ===
Q mean: -11.132694
Q std: 14.612612
Actor loss: 11.136672
Action reg: 0.003978
  l1.weight: grad_norm = 0.074400
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.054259
Total gradient norm: 0.265475
=== Actor Training Debug (Iteration 2991) ===
Q mean: -12.378160
Q std: 14.786153
Actor loss: 12.382143
Action reg: 0.003983
  l1.weight: grad_norm = 0.016545
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.012495
Total gradient norm: 0.048947
=== Actor Training Debug (Iteration 2992) ===
Q mean: -11.457722
Q std: 14.359046
Actor loss: 11.461699
Action reg: 0.003978
  l1.weight: grad_norm = 0.143735
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.093363
Total gradient norm: 0.341875
=== Actor Training Debug (Iteration 2993) ===
Q mean: -11.906630
Q std: 15.356134
Actor loss: 11.910613
Action reg: 0.003984
  l1.weight: grad_norm = 0.020634
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.015772
Total gradient norm: 0.090495
=== Actor Training Debug (Iteration 2994) ===
Q mean: -10.303862
Q std: 13.770019
Actor loss: 10.307858
Action reg: 0.003996
  l1.weight: grad_norm = 0.051528
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.035353
Total gradient norm: 0.102651
=== Actor Training Debug (Iteration 2995) ===
Q mean: -9.818743
Q std: 13.326664
Actor loss: 9.822725
Action reg: 0.003983
  l1.weight: grad_norm = 0.093284
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.071279
Total gradient norm: 0.291251
=== Actor Training Debug (Iteration 2996) ===
Q mean: -12.482341
Q std: 14.354547
Actor loss: 12.486331
Action reg: 0.003990
  l1.weight: grad_norm = 0.013757
  l1.bias: grad_norm = 0.000435
  l2.weight: grad_norm = 0.010982
Total gradient norm: 0.060962
=== Actor Training Debug (Iteration 2997) ===
Q mean: -11.690285
Q std: 14.413744
Actor loss: 11.694263
Action reg: 0.003978
  l1.weight: grad_norm = 0.058242
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.040028
Total gradient norm: 0.161427
=== Actor Training Debug (Iteration 2998) ===
Q mean: -12.312513
Q std: 14.913254
Actor loss: 12.316498
Action reg: 0.003985
  l1.weight: grad_norm = 0.024347
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.020448
Total gradient norm: 0.063500
=== Actor Training Debug (Iteration 2999) ===
Q mean: -11.760092
Q std: 14.824572
Actor loss: 11.764060
Action reg: 0.003969
  l1.weight: grad_norm = 0.057113
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.040835
Total gradient norm: 0.150178
=== Actor Training Debug (Iteration 3000) ===
Q mean: -11.848433
Q std: 14.899712
Actor loss: 11.852415
Action reg: 0.003983
  l1.weight: grad_norm = 0.015782
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.009891
Total gradient norm: 0.034512
Step 8000: Critic Loss: 1.3445, Actor Loss: 11.8524, Q Value: -11.8484
  Average reward: -330.657 | Average length: 100.0
Evaluation at episode 80: -330.657
=== Actor Training Debug (Iteration 3001) ===
Q mean: -12.211282
Q std: 14.851822
Actor loss: 12.215258
Action reg: 0.003976
  l1.weight: grad_norm = 0.057149
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.044781
Total gradient norm: 0.193111
=== Actor Training Debug (Iteration 3002) ===
Q mean: -11.755819
Q std: 14.744799
Actor loss: 11.759808
Action reg: 0.003988
  l1.weight: grad_norm = 0.042070
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.033430
Total gradient norm: 0.142980
=== Actor Training Debug (Iteration 3003) ===
Q mean: -11.104720
Q std: 13.412518
Actor loss: 11.108692
Action reg: 0.003972
  l1.weight: grad_norm = 0.043299
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.039866
Total gradient norm: 0.151261
=== Actor Training Debug (Iteration 3004) ===
Q mean: -12.268640
Q std: 15.218098
Actor loss: 12.272618
Action reg: 0.003978
  l1.weight: grad_norm = 0.122650
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.096452
Total gradient norm: 0.364334
=== Actor Training Debug (Iteration 3005) ===
Q mean: -10.227779
Q std: 13.825233
Actor loss: 10.231761
Action reg: 0.003981
  l1.weight: grad_norm = 0.049153
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.041623
Total gradient norm: 0.165521
=== Actor Training Debug (Iteration 3006) ===
Q mean: -10.204067
Q std: 13.767859
Actor loss: 10.208030
Action reg: 0.003963
  l1.weight: grad_norm = 0.073883
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.054387
Total gradient norm: 0.195256
=== Actor Training Debug (Iteration 3007) ===
Q mean: -9.637947
Q std: 12.844200
Actor loss: 9.641926
Action reg: 0.003979
  l1.weight: grad_norm = 0.052256
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.040197
Total gradient norm: 0.153374
=== Actor Training Debug (Iteration 3008) ===
Q mean: -11.920116
Q std: 14.748284
Actor loss: 11.924104
Action reg: 0.003987
  l1.weight: grad_norm = 0.020041
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.015613
Total gradient norm: 0.063622
=== Actor Training Debug (Iteration 3009) ===
Q mean: -11.413165
Q std: 14.536784
Actor loss: 11.417136
Action reg: 0.003971
  l1.weight: grad_norm = 0.038219
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.030642
Total gradient norm: 0.096568
=== Actor Training Debug (Iteration 3010) ===
Q mean: -11.643384
Q std: 15.856534
Actor loss: 11.647354
Action reg: 0.003970
  l1.weight: grad_norm = 0.038608
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.032505
Total gradient norm: 0.101272
=== Actor Training Debug (Iteration 3011) ===
Q mean: -11.729074
Q std: 14.925584
Actor loss: 11.733054
Action reg: 0.003980
  l1.weight: grad_norm = 0.069427
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.057683
Total gradient norm: 0.199399
=== Actor Training Debug (Iteration 3012) ===
Q mean: -12.935066
Q std: 15.309921
Actor loss: 12.939058
Action reg: 0.003992
  l1.weight: grad_norm = 0.022652
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.019572
Total gradient norm: 0.092335
=== Actor Training Debug (Iteration 3013) ===
Q mean: -12.734064
Q std: 15.338735
Actor loss: 12.738041
Action reg: 0.003976
  l1.weight: grad_norm = 0.014687
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.011114
Total gradient norm: 0.044556
=== Actor Training Debug (Iteration 3014) ===
Q mean: -11.708986
Q std: 14.528753
Actor loss: 11.712973
Action reg: 0.003987
  l1.weight: grad_norm = 0.028320
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.020254
Total gradient norm: 0.067820
=== Actor Training Debug (Iteration 3015) ===
Q mean: -12.178620
Q std: 14.192837
Actor loss: 12.182595
Action reg: 0.003974
  l1.weight: grad_norm = 0.059430
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.045768
Total gradient norm: 0.210697
=== Actor Training Debug (Iteration 3016) ===
Q mean: -11.004864
Q std: 13.826710
Actor loss: 11.008852
Action reg: 0.003988
  l1.weight: grad_norm = 0.043763
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.024327
Total gradient norm: 0.093280
=== Actor Training Debug (Iteration 3017) ===
Q mean: -11.771407
Q std: 14.509358
Actor loss: 11.775367
Action reg: 0.003960
  l1.weight: grad_norm = 0.070963
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.056365
Total gradient norm: 0.206703
=== Actor Training Debug (Iteration 3018) ===
Q mean: -12.228624
Q std: 15.213237
Actor loss: 12.232581
Action reg: 0.003957
  l1.weight: grad_norm = 0.023372
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.015558
Total gradient norm: 0.065787
=== Actor Training Debug (Iteration 3019) ===
Q mean: -11.016325
Q std: 14.453521
Actor loss: 11.020296
Action reg: 0.003971
  l1.weight: grad_norm = 0.062911
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.046369
Total gradient norm: 0.220169
=== Actor Training Debug (Iteration 3020) ===
Q mean: -9.969355
Q std: 13.271728
Actor loss: 9.973326
Action reg: 0.003971
  l1.weight: grad_norm = 0.048042
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.036115
Total gradient norm: 0.164235
=== Actor Training Debug (Iteration 3021) ===
Q mean: -12.835010
Q std: 14.921315
Actor loss: 12.838995
Action reg: 0.003986
  l1.weight: grad_norm = 0.014579
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.010056
Total gradient norm: 0.038970
=== Actor Training Debug (Iteration 3022) ===
Q mean: -12.743903
Q std: 13.712959
Actor loss: 12.747881
Action reg: 0.003978
  l1.weight: grad_norm = 0.014099
  l1.bias: grad_norm = 0.000691
  l2.weight: grad_norm = 0.012340
Total gradient norm: 0.064481
=== Actor Training Debug (Iteration 3023) ===
Q mean: -11.460405
Q std: 14.066084
Actor loss: 11.464387
Action reg: 0.003981
  l1.weight: grad_norm = 0.022727
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.014799
Total gradient norm: 0.068829
=== Actor Training Debug (Iteration 3024) ===
Q mean: -12.334527
Q std: 15.277019
Actor loss: 12.338508
Action reg: 0.003981
  l1.weight: grad_norm = 0.035556
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.022940
Total gradient norm: 0.108642
=== Actor Training Debug (Iteration 3025) ===
Q mean: -10.178310
Q std: 14.288378
Actor loss: 10.182292
Action reg: 0.003982
  l1.weight: grad_norm = 0.088510
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.053736
Total gradient norm: 0.221723
=== Actor Training Debug (Iteration 3026) ===
Q mean: -11.611784
Q std: 14.630278
Actor loss: 11.615764
Action reg: 0.003980
  l1.weight: grad_norm = 0.102630
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.065030
Total gradient norm: 0.259938
=== Actor Training Debug (Iteration 3027) ===
Q mean: -10.275133
Q std: 13.202423
Actor loss: 10.279121
Action reg: 0.003988
  l1.weight: grad_norm = 0.020928
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.015809
Total gradient norm: 0.056477
=== Actor Training Debug (Iteration 3028) ===
Q mean: -13.527315
Q std: 15.876732
Actor loss: 13.531292
Action reg: 0.003977
  l1.weight: grad_norm = 0.057282
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.032426
Total gradient norm: 0.123382
=== Actor Training Debug (Iteration 3029) ===
Q mean: -12.821230
Q std: 15.156652
Actor loss: 12.825217
Action reg: 0.003987
  l1.weight: grad_norm = 0.061340
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.047959
Total gradient norm: 0.189321
=== Actor Training Debug (Iteration 3030) ===
Q mean: -13.214883
Q std: 15.158462
Actor loss: 13.218865
Action reg: 0.003983
  l1.weight: grad_norm = 0.004903
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.004376
Total gradient norm: 0.023799
=== Actor Training Debug (Iteration 3031) ===
Q mean: -11.800802
Q std: 13.960084
Actor loss: 11.804784
Action reg: 0.003982
  l1.weight: grad_norm = 0.049093
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.040793
Total gradient norm: 0.161381
=== Actor Training Debug (Iteration 3032) ===
Q mean: -12.856380
Q std: 15.336288
Actor loss: 12.860360
Action reg: 0.003979
  l1.weight: grad_norm = 0.075656
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.057430
Total gradient norm: 0.212300
=== Actor Training Debug (Iteration 3033) ===
Q mean: -12.622821
Q std: 14.509044
Actor loss: 12.626807
Action reg: 0.003987
  l1.weight: grad_norm = 0.072699
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.055064
Total gradient norm: 0.184770
=== Actor Training Debug (Iteration 3034) ===
Q mean: -11.422535
Q std: 15.208348
Actor loss: 11.426517
Action reg: 0.003982
  l1.weight: grad_norm = 0.036462
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.030663
Total gradient norm: 0.097502
=== Actor Training Debug (Iteration 3035) ===
Q mean: -12.521962
Q std: 15.297947
Actor loss: 12.525935
Action reg: 0.003973
  l1.weight: grad_norm = 0.039099
  l1.bias: grad_norm = 0.000713
  l2.weight: grad_norm = 0.027879
Total gradient norm: 0.112031
=== Actor Training Debug (Iteration 3036) ===
Q mean: -9.634949
Q std: 14.327220
Actor loss: 9.638940
Action reg: 0.003991
  l1.weight: grad_norm = 0.018723
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.014062
Total gradient norm: 0.050392
=== Actor Training Debug (Iteration 3037) ===
Q mean: -12.214819
Q std: 15.607187
Actor loss: 12.218798
Action reg: 0.003979
  l1.weight: grad_norm = 0.104778
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.082589
Total gradient norm: 0.327031
=== Actor Training Debug (Iteration 3038) ===
Q mean: -11.412781
Q std: 14.352616
Actor loss: 11.416762
Action reg: 0.003982
  l1.weight: grad_norm = 0.061651
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.046300
Total gradient norm: 0.172961
=== Actor Training Debug (Iteration 3039) ===
Q mean: -11.784298
Q std: 14.670352
Actor loss: 11.788267
Action reg: 0.003969
  l1.weight: grad_norm = 0.093944
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.075009
Total gradient norm: 0.243878
=== Actor Training Debug (Iteration 3040) ===
Q mean: -11.707702
Q std: 14.451428
Actor loss: 11.711681
Action reg: 0.003980
  l1.weight: grad_norm = 0.026423
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.020152
Total gradient norm: 0.074356
=== Actor Training Debug (Iteration 3041) ===
Q mean: -10.222286
Q std: 13.964747
Actor loss: 10.226269
Action reg: 0.003983
  l1.weight: grad_norm = 0.049121
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.034751
Total gradient norm: 0.167273
=== Actor Training Debug (Iteration 3042) ===
Q mean: -12.230354
Q std: 15.161477
Actor loss: 12.234342
Action reg: 0.003987
  l1.weight: grad_norm = 0.018425
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.015597
Total gradient norm: 0.063893
=== Actor Training Debug (Iteration 3043) ===
Q mean: -8.852747
Q std: 12.995627
Actor loss: 8.856718
Action reg: 0.003971
  l1.weight: grad_norm = 0.093918
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.080464
Total gradient norm: 0.314020
=== Actor Training Debug (Iteration 3044) ===
Q mean: -11.922069
Q std: 14.535372
Actor loss: 11.926056
Action reg: 0.003988
  l1.weight: grad_norm = 0.092849
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.057978
Total gradient norm: 0.264854
=== Actor Training Debug (Iteration 3045) ===
Q mean: -11.942469
Q std: 13.962407
Actor loss: 11.946442
Action reg: 0.003973
  l1.weight: grad_norm = 0.043178
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.036220
Total gradient norm: 0.116323
=== Actor Training Debug (Iteration 3046) ===
Q mean: -11.576765
Q std: 15.050668
Actor loss: 11.580753
Action reg: 0.003988
  l1.weight: grad_norm = 0.054098
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.045522
Total gradient norm: 0.223881
=== Actor Training Debug (Iteration 3047) ===
Q mean: -12.682197
Q std: 15.040145
Actor loss: 12.686172
Action reg: 0.003975
  l1.weight: grad_norm = 0.028546
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.023432
Total gradient norm: 0.107819
=== Actor Training Debug (Iteration 3048) ===
Q mean: -10.485653
Q std: 14.503721
Actor loss: 10.489635
Action reg: 0.003981
  l1.weight: grad_norm = 0.027004
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.021388
Total gradient norm: 0.063101
=== Actor Training Debug (Iteration 3049) ===
Q mean: -11.517409
Q std: 14.980474
Actor loss: 11.521387
Action reg: 0.003978
  l1.weight: grad_norm = 0.055714
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.043816
Total gradient norm: 0.161763
=== Actor Training Debug (Iteration 3050) ===
Q mean: -12.515376
Q std: 15.127135
Actor loss: 12.519369
Action reg: 0.003993
  l1.weight: grad_norm = 0.053969
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.042217
Total gradient norm: 0.174519
=== Actor Training Debug (Iteration 3051) ===
Q mean: -12.748032
Q std: 14.871839
Actor loss: 12.752011
Action reg: 0.003980
  l1.weight: grad_norm = 0.080895
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.069176
Total gradient norm: 0.283882
=== Actor Training Debug (Iteration 3052) ===
Q mean: -12.898675
Q std: 15.038551
Actor loss: 12.902667
Action reg: 0.003992
  l1.weight: grad_norm = 0.014254
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.014007
Total gradient norm: 0.051038
=== Actor Training Debug (Iteration 3053) ===
Q mean: -10.800688
Q std: 14.143682
Actor loss: 10.804662
Action reg: 0.003974
  l1.weight: grad_norm = 0.087038
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.068118
Total gradient norm: 0.311204
=== Actor Training Debug (Iteration 3054) ===
Q mean: -14.230312
Q std: 15.941711
Actor loss: 14.234300
Action reg: 0.003987
  l1.weight: grad_norm = 0.041826
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.028904
Total gradient norm: 0.103373
=== Actor Training Debug (Iteration 3055) ===
Q mean: -10.920669
Q std: 13.550320
Actor loss: 10.924650
Action reg: 0.003982
  l1.weight: grad_norm = 0.097761
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.082130
Total gradient norm: 0.300876
=== Actor Training Debug (Iteration 3056) ===
Q mean: -10.168875
Q std: 14.389557
Actor loss: 10.172857
Action reg: 0.003982
  l1.weight: grad_norm = 0.059410
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.038842
Total gradient norm: 0.153567
=== Actor Training Debug (Iteration 3057) ===
Q mean: -11.699266
Q std: 14.197784
Actor loss: 11.703244
Action reg: 0.003978
  l1.weight: grad_norm = 0.093673
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.066813
Total gradient norm: 0.245872
=== Actor Training Debug (Iteration 3058) ===
Q mean: -11.979406
Q std: 14.949591
Actor loss: 11.983381
Action reg: 0.003975
  l1.weight: grad_norm = 0.074428
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.055195
Total gradient norm: 0.178886
=== Actor Training Debug (Iteration 3059) ===
Q mean: -13.099072
Q std: 15.455172
Actor loss: 13.103056
Action reg: 0.003985
  l1.weight: grad_norm = 0.061684
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.046530
Total gradient norm: 0.188780
=== Actor Training Debug (Iteration 3060) ===
Q mean: -12.517721
Q std: 14.724092
Actor loss: 12.521696
Action reg: 0.003975
  l1.weight: grad_norm = 0.051322
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.041692
Total gradient norm: 0.154611
=== Actor Training Debug (Iteration 3061) ===
Q mean: -12.002269
Q std: 15.091889
Actor loss: 12.006241
Action reg: 0.003972
  l1.weight: grad_norm = 0.063019
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.045786
Total gradient norm: 0.201249
=== Actor Training Debug (Iteration 3062) ===
Q mean: -12.771732
Q std: 15.014965
Actor loss: 12.775706
Action reg: 0.003974
  l1.weight: grad_norm = 0.083342
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.060406
Total gradient norm: 0.244040
=== Actor Training Debug (Iteration 3063) ===
Q mean: -13.880775
Q std: 15.513701
Actor loss: 13.884761
Action reg: 0.003985
  l1.weight: grad_norm = 0.020355
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.015697
Total gradient norm: 0.061498
=== Actor Training Debug (Iteration 3064) ===
Q mean: -10.288012
Q std: 14.194067
Actor loss: 10.291988
Action reg: 0.003977
  l1.weight: grad_norm = 0.086144
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.062516
Total gradient norm: 0.268032
=== Actor Training Debug (Iteration 3065) ===
Q mean: -11.485397
Q std: 14.939411
Actor loss: 11.489386
Action reg: 0.003988
  l1.weight: grad_norm = 0.065266
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.048793
Total gradient norm: 0.231913
=== Actor Training Debug (Iteration 3066) ===
Q mean: -12.317156
Q std: 14.729949
Actor loss: 12.321129
Action reg: 0.003973
  l1.weight: grad_norm = 0.077038
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.054853
Total gradient norm: 0.226748
=== Actor Training Debug (Iteration 3067) ===
Q mean: -10.704819
Q std: 14.094903
Actor loss: 10.708804
Action reg: 0.003985
  l1.weight: grad_norm = 0.082236
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.054237
Total gradient norm: 0.235771
=== Actor Training Debug (Iteration 3068) ===
Q mean: -12.240351
Q std: 15.630830
Actor loss: 12.244327
Action reg: 0.003976
  l1.weight: grad_norm = 0.084841
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.059395
Total gradient norm: 0.254285
=== Actor Training Debug (Iteration 3069) ===
Q mean: -10.600180
Q std: 14.018023
Actor loss: 10.604157
Action reg: 0.003978
  l1.weight: grad_norm = 0.084627
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.063252
Total gradient norm: 0.249830
=== Actor Training Debug (Iteration 3070) ===
Q mean: -11.839619
Q std: 15.283134
Actor loss: 11.843600
Action reg: 0.003981
  l1.weight: grad_norm = 0.040267
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.034080
Total gradient norm: 0.171717
=== Actor Training Debug (Iteration 3071) ===
Q mean: -12.298941
Q std: 13.846496
Actor loss: 12.302932
Action reg: 0.003992
  l1.weight: grad_norm = 0.027949
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.021405
Total gradient norm: 0.087764
=== Actor Training Debug (Iteration 3072) ===
Q mean: -12.478344
Q std: 15.110170
Actor loss: 12.482329
Action reg: 0.003986
  l1.weight: grad_norm = 0.052046
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.037346
Total gradient norm: 0.156296
=== Actor Training Debug (Iteration 3073) ===
Q mean: -12.007599
Q std: 14.658930
Actor loss: 12.011583
Action reg: 0.003985
  l1.weight: grad_norm = 0.019614
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.014006
Total gradient norm: 0.055342
=== Actor Training Debug (Iteration 3074) ===
Q mean: -11.179790
Q std: 14.432015
Actor loss: 11.183755
Action reg: 0.003965
  l1.weight: grad_norm = 0.043354
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.034272
Total gradient norm: 0.154326
=== Actor Training Debug (Iteration 3075) ===
Q mean: -11.116123
Q std: 14.296162
Actor loss: 11.120114
Action reg: 0.003991
  l1.weight: grad_norm = 0.019295
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.012841
Total gradient norm: 0.044707
=== Actor Training Debug (Iteration 3076) ===
Q mean: -12.029598
Q std: 15.022148
Actor loss: 12.033579
Action reg: 0.003981
  l1.weight: grad_norm = 0.033150
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.024300
Total gradient norm: 0.083269
=== Actor Training Debug (Iteration 3077) ===
Q mean: -11.320818
Q std: 13.984314
Actor loss: 11.324806
Action reg: 0.003988
  l1.weight: grad_norm = 0.045694
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.039575
Total gradient norm: 0.132824
=== Actor Training Debug (Iteration 3078) ===
Q mean: -11.103786
Q std: 14.306313
Actor loss: 11.107762
Action reg: 0.003976
  l1.weight: grad_norm = 0.037807
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.027663
Total gradient norm: 0.101859
=== Actor Training Debug (Iteration 3079) ===
Q mean: -12.323813
Q std: 15.447519
Actor loss: 12.327810
Action reg: 0.003997
  l1.weight: grad_norm = 0.006664
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004424
Total gradient norm: 0.019215
=== Actor Training Debug (Iteration 3080) ===
Q mean: -13.349118
Q std: 14.704920
Actor loss: 13.353097
Action reg: 0.003979
  l1.weight: grad_norm = 0.083515
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.056475
Total gradient norm: 0.270693
=== Actor Training Debug (Iteration 3081) ===
Q mean: -10.340055
Q std: 14.005530
Actor loss: 10.344045
Action reg: 0.003990
  l1.weight: grad_norm = 0.030369
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.022393
Total gradient norm: 0.096048
=== Actor Training Debug (Iteration 3082) ===
Q mean: -13.002077
Q std: 16.061583
Actor loss: 13.006067
Action reg: 0.003990
  l1.weight: grad_norm = 0.042305
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.028591
Total gradient norm: 0.112953
=== Actor Training Debug (Iteration 3083) ===
Q mean: -11.433149
Q std: 15.002205
Actor loss: 11.437141
Action reg: 0.003992
  l1.weight: grad_norm = 0.026952
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.020035
Total gradient norm: 0.088780
=== Actor Training Debug (Iteration 3084) ===
Q mean: -11.112005
Q std: 15.221708
Actor loss: 11.115983
Action reg: 0.003977
  l1.weight: grad_norm = 0.031985
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.026913
Total gradient norm: 0.108554
=== Actor Training Debug (Iteration 3085) ===
Q mean: -10.261113
Q std: 13.994547
Actor loss: 10.265089
Action reg: 0.003976
  l1.weight: grad_norm = 0.118096
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.089458
Total gradient norm: 0.420141
=== Actor Training Debug (Iteration 3086) ===
Q mean: -13.035977
Q std: 15.333379
Actor loss: 13.039958
Action reg: 0.003981
  l1.weight: grad_norm = 0.057700
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.037111
Total gradient norm: 0.145600
=== Actor Training Debug (Iteration 3087) ===
Q mean: -11.290270
Q std: 14.752448
Actor loss: 11.294254
Action reg: 0.003985
  l1.weight: grad_norm = 0.057397
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.040994
Total gradient norm: 0.186492
=== Actor Training Debug (Iteration 3088) ===
Q mean: -12.007198
Q std: 15.429715
Actor loss: 12.011175
Action reg: 0.003977
  l1.weight: grad_norm = 0.044921
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.037228
Total gradient norm: 0.120779
=== Actor Training Debug (Iteration 3089) ===
Q mean: -11.134258
Q std: 14.384429
Actor loss: 11.138238
Action reg: 0.003980
  l1.weight: grad_norm = 0.044764
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.029543
Total gradient norm: 0.109978
=== Actor Training Debug (Iteration 3090) ===
Q mean: -11.941897
Q std: 14.806990
Actor loss: 11.945860
Action reg: 0.003963
  l1.weight: grad_norm = 0.124646
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.085071
Total gradient norm: 0.347390
=== Actor Training Debug (Iteration 3091) ===
Q mean: -12.368935
Q std: 15.866501
Actor loss: 12.372912
Action reg: 0.003977
  l1.weight: grad_norm = 0.164524
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.115179
Total gradient norm: 0.469650
=== Actor Training Debug (Iteration 3092) ===
Q mean: -13.844664
Q std: 15.472993
Actor loss: 13.848643
Action reg: 0.003980
  l1.weight: grad_norm = 0.040736
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.035555
Total gradient norm: 0.135784
=== Actor Training Debug (Iteration 3093) ===
Q mean: -10.366615
Q std: 14.346302
Actor loss: 10.370597
Action reg: 0.003981
  l1.weight: grad_norm = 0.021073
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.016462
Total gradient norm: 0.068899
=== Actor Training Debug (Iteration 3094) ===
Q mean: -9.726007
Q std: 13.941895
Actor loss: 9.729991
Action reg: 0.003985
  l1.weight: grad_norm = 0.050722
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.038329
Total gradient norm: 0.189828
=== Actor Training Debug (Iteration 3095) ===
Q mean: -10.856762
Q std: 14.977056
Actor loss: 10.860739
Action reg: 0.003976
  l1.weight: grad_norm = 0.024062
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.021425
Total gradient norm: 0.091765
=== Actor Training Debug (Iteration 3096) ===
Q mean: -11.742117
Q std: 14.696973
Actor loss: 11.746097
Action reg: 0.003980
  l1.weight: grad_norm = 0.018787
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.014969
Total gradient norm: 0.069679
=== Actor Training Debug (Iteration 3097) ===
Q mean: -11.778618
Q std: 14.922008
Actor loss: 11.782578
Action reg: 0.003959
  l1.weight: grad_norm = 0.034206
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.023485
Total gradient norm: 0.094807
=== Actor Training Debug (Iteration 3098) ===
Q mean: -10.871395
Q std: 14.557807
Actor loss: 10.875374
Action reg: 0.003978
  l1.weight: grad_norm = 0.029523
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.022873
Total gradient norm: 0.101704
=== Actor Training Debug (Iteration 3099) ===
Q mean: -12.108605
Q std: 15.465612
Actor loss: 12.112581
Action reg: 0.003976
  l1.weight: grad_norm = 0.077335
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.060183
Total gradient norm: 0.241123
=== Actor Training Debug (Iteration 3100) ===
Q mean: -11.176117
Q std: 14.576567
Actor loss: 11.180095
Action reg: 0.003978
  l1.weight: grad_norm = 0.107945
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.078405
Total gradient norm: 0.278595
Episode 81: Steps=100, Reward=-271.215, Buffer_size=8100
=== Actor Training Debug (Iteration 3101) ===
Q mean: -12.652919
Q std: 15.495321
Actor loss: 12.656890
Action reg: 0.003971
  l1.weight: grad_norm = 0.039680
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.034523
Total gradient norm: 0.135441
=== Actor Training Debug (Iteration 3102) ===
Q mean: -12.685940
Q std: 15.209955
Actor loss: 12.689923
Action reg: 0.003984
  l1.weight: grad_norm = 0.034656
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.024697
Total gradient norm: 0.110139
=== Actor Training Debug (Iteration 3103) ===
Q mean: -13.272398
Q std: 15.411671
Actor loss: 13.276369
Action reg: 0.003971
  l1.weight: grad_norm = 0.070648
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.053049
Total gradient norm: 0.215416
=== Actor Training Debug (Iteration 3104) ===
Q mean: -11.168337
Q std: 14.595348
Actor loss: 11.172301
Action reg: 0.003964
  l1.weight: grad_norm = 0.115907
  l1.bias: grad_norm = 0.000829
  l2.weight: grad_norm = 0.086129
Total gradient norm: 0.260704
=== Actor Training Debug (Iteration 3105) ===
Q mean: -11.173437
Q std: 13.964713
Actor loss: 11.177409
Action reg: 0.003972
  l1.weight: grad_norm = 0.025512
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.019343
Total gradient norm: 0.085065
=== Actor Training Debug (Iteration 3106) ===
Q mean: -10.821838
Q std: 14.557239
Actor loss: 10.825830
Action reg: 0.003991
  l1.weight: grad_norm = 0.004168
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.002970
Total gradient norm: 0.011261
=== Actor Training Debug (Iteration 3107) ===
Q mean: -10.644896
Q std: 13.945606
Actor loss: 10.648877
Action reg: 0.003982
  l1.weight: grad_norm = 0.048508
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.038253
Total gradient norm: 0.135652
=== Actor Training Debug (Iteration 3108) ===
Q mean: -12.995382
Q std: 15.391068
Actor loss: 12.999360
Action reg: 0.003977
  l1.weight: grad_norm = 0.048306
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.032865
Total gradient norm: 0.116519
=== Actor Training Debug (Iteration 3109) ===
Q mean: -12.476730
Q std: 15.665684
Actor loss: 12.480710
Action reg: 0.003980
  l1.weight: grad_norm = 0.074908
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.056098
Total gradient norm: 0.209805
=== Actor Training Debug (Iteration 3110) ===
Q mean: -12.142096
Q std: 14.714358
Actor loss: 12.146073
Action reg: 0.003978
  l1.weight: grad_norm = 0.013725
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.009672
Total gradient norm: 0.048067
=== Actor Training Debug (Iteration 3111) ===
Q mean: -10.614853
Q std: 14.897345
Actor loss: 10.618841
Action reg: 0.003989
  l1.weight: grad_norm = 0.013547
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.009280
Total gradient norm: 0.031040
=== Actor Training Debug (Iteration 3112) ===
Q mean: -11.736788
Q std: 15.333271
Actor loss: 11.740767
Action reg: 0.003979
  l1.weight: grad_norm = 0.067507
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.058020
Total gradient norm: 0.231274
=== Actor Training Debug (Iteration 3113) ===
Q mean: -11.883087
Q std: 15.021464
Actor loss: 11.887053
Action reg: 0.003965
  l1.weight: grad_norm = 0.029964
  l1.bias: grad_norm = 0.001121
  l2.weight: grad_norm = 0.022553
Total gradient norm: 0.097440
=== Actor Training Debug (Iteration 3114) ===
Q mean: -11.686714
Q std: 14.418638
Actor loss: 11.690693
Action reg: 0.003979
  l1.weight: grad_norm = 0.049388
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.040315
Total gradient norm: 0.151192
=== Actor Training Debug (Iteration 3115) ===
Q mean: -13.918812
Q std: 15.274671
Actor loss: 13.922796
Action reg: 0.003984
  l1.weight: grad_norm = 0.037406
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.028183
Total gradient norm: 0.117815
=== Actor Training Debug (Iteration 3116) ===
Q mean: -11.959875
Q std: 14.827401
Actor loss: 11.963848
Action reg: 0.003973
  l1.weight: grad_norm = 0.088250
  l1.bias: grad_norm = 0.000881
  l2.weight: grad_norm = 0.058721
Total gradient norm: 0.279185
=== Actor Training Debug (Iteration 3117) ===
Q mean: -9.622828
Q std: 14.254995
Actor loss: 9.626809
Action reg: 0.003981
  l1.weight: grad_norm = 0.075039
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.073470
Total gradient norm: 0.219527
=== Actor Training Debug (Iteration 3118) ===
Q mean: -12.982334
Q std: 15.842983
Actor loss: 12.986320
Action reg: 0.003986
  l1.weight: grad_norm = 0.029173
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.021791
Total gradient norm: 0.084582
=== Actor Training Debug (Iteration 3119) ===
Q mean: -11.473433
Q std: 15.032207
Actor loss: 11.477425
Action reg: 0.003993
  l1.weight: grad_norm = 0.031763
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.021087
Total gradient norm: 0.086335
=== Actor Training Debug (Iteration 3120) ===
Q mean: -12.247730
Q std: 15.086261
Actor loss: 12.251714
Action reg: 0.003984
  l1.weight: grad_norm = 0.067709
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.053932
Total gradient norm: 0.255439
=== Actor Training Debug (Iteration 3121) ===
Q mean: -13.430260
Q std: 15.559879
Actor loss: 13.434241
Action reg: 0.003981
  l1.weight: grad_norm = 0.070670
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.048510
Total gradient norm: 0.217331
=== Actor Training Debug (Iteration 3122) ===
Q mean: -12.196936
Q std: 15.376965
Actor loss: 12.200908
Action reg: 0.003972
  l1.weight: grad_norm = 0.043497
  l1.bias: grad_norm = 0.000612
  l2.weight: grad_norm = 0.033567
Total gradient norm: 0.140471
=== Actor Training Debug (Iteration 3123) ===
Q mean: -10.420421
Q std: 13.761534
Actor loss: 10.424398
Action reg: 0.003978
  l1.weight: grad_norm = 0.096832
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.058253
Total gradient norm: 0.307752
=== Actor Training Debug (Iteration 3124) ===
Q mean: -12.293974
Q std: 15.869975
Actor loss: 12.297966
Action reg: 0.003992
  l1.weight: grad_norm = 0.028634
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.025526
Total gradient norm: 0.115805
=== Actor Training Debug (Iteration 3125) ===
Q mean: -12.019309
Q std: 14.572349
Actor loss: 12.023290
Action reg: 0.003981
  l1.weight: grad_norm = 0.049387
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.040243
Total gradient norm: 0.146367
=== Actor Training Debug (Iteration 3126) ===
Q mean: -11.870836
Q std: 14.703623
Actor loss: 11.874819
Action reg: 0.003983
  l1.weight: grad_norm = 0.032884
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.023865
Total gradient norm: 0.119490
=== Actor Training Debug (Iteration 3127) ===
Q mean: -11.707339
Q std: 15.106518
Actor loss: 11.711327
Action reg: 0.003987
  l1.weight: grad_norm = 0.033522
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.024075
Total gradient norm: 0.096831
=== Actor Training Debug (Iteration 3128) ===
Q mean: -12.209839
Q std: 14.852866
Actor loss: 12.213830
Action reg: 0.003991
  l1.weight: grad_norm = 0.036853
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.029112
Total gradient norm: 0.120481
=== Actor Training Debug (Iteration 3129) ===
Q mean: -11.524889
Q std: 14.999011
Actor loss: 11.528853
Action reg: 0.003964
  l1.weight: grad_norm = 0.030346
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.018875
Total gradient norm: 0.065878
=== Actor Training Debug (Iteration 3130) ===
Q mean: -11.119358
Q std: 14.781119
Actor loss: 11.123336
Action reg: 0.003977
  l1.weight: grad_norm = 0.051757
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.047391
Total gradient norm: 0.180804
=== Actor Training Debug (Iteration 3131) ===
Q mean: -9.554319
Q std: 14.434340
Actor loss: 9.558301
Action reg: 0.003982
  l1.weight: grad_norm = 0.065667
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.053264
Total gradient norm: 0.170253
=== Actor Training Debug (Iteration 3132) ===
Q mean: -12.000171
Q std: 15.134911
Actor loss: 12.004159
Action reg: 0.003988
  l1.weight: grad_norm = 0.012248
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.009949
Total gradient norm: 0.036756
=== Actor Training Debug (Iteration 3133) ===
Q mean: -11.312970
Q std: 15.166871
Actor loss: 11.316955
Action reg: 0.003984
  l1.weight: grad_norm = 0.074241
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.056365
Total gradient norm: 0.234204
=== Actor Training Debug (Iteration 3134) ===
Q mean: -12.470284
Q std: 15.143577
Actor loss: 12.474256
Action reg: 0.003972
  l1.weight: grad_norm = 0.028308
  l1.bias: grad_norm = 0.000649
  l2.weight: grad_norm = 0.025193
Total gradient norm: 0.093868
=== Actor Training Debug (Iteration 3135) ===
Q mean: -11.646666
Q std: 14.695495
Actor loss: 11.650660
Action reg: 0.003994
  l1.weight: grad_norm = 0.004654
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.003748
Total gradient norm: 0.017960
=== Actor Training Debug (Iteration 3136) ===
Q mean: -11.152190
Q std: 14.727059
Actor loss: 11.156162
Action reg: 0.003972
  l1.weight: grad_norm = 0.034143
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.022507
Total gradient norm: 0.093336
=== Actor Training Debug (Iteration 3137) ===
Q mean: -12.127768
Q std: 14.913721
Actor loss: 12.131748
Action reg: 0.003980
  l1.weight: grad_norm = 0.055018
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.036444
Total gradient norm: 0.165946
=== Actor Training Debug (Iteration 3138) ===
Q mean: -12.241116
Q std: 14.873417
Actor loss: 12.245105
Action reg: 0.003989
  l1.weight: grad_norm = 0.004614
  l1.bias: grad_norm = 0.000686
  l2.weight: grad_norm = 0.004361
Total gradient norm: 0.023730
=== Actor Training Debug (Iteration 3139) ===
Q mean: -13.608013
Q std: 15.945604
Actor loss: 13.611993
Action reg: 0.003980
  l1.weight: grad_norm = 0.058939
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.052542
Total gradient norm: 0.146284
=== Actor Training Debug (Iteration 3140) ===
Q mean: -11.185944
Q std: 13.850431
Actor loss: 11.189923
Action reg: 0.003980
  l1.weight: grad_norm = 0.022508
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.018665
Total gradient norm: 0.069510
=== Actor Training Debug (Iteration 3141) ===
Q mean: -11.886349
Q std: 14.725423
Actor loss: 11.890326
Action reg: 0.003978
  l1.weight: grad_norm = 0.034079
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.024657
Total gradient norm: 0.085264
=== Actor Training Debug (Iteration 3142) ===
Q mean: -13.375704
Q std: 15.840615
Actor loss: 13.379688
Action reg: 0.003985
  l1.weight: grad_norm = 0.100913
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.065887
Total gradient norm: 0.271972
=== Actor Training Debug (Iteration 3143) ===
Q mean: -8.886106
Q std: 13.459779
Actor loss: 8.890095
Action reg: 0.003988
  l1.weight: grad_norm = 0.058155
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.039508
Total gradient norm: 0.145099
=== Actor Training Debug (Iteration 3144) ===
Q mean: -12.355211
Q std: 16.011278
Actor loss: 12.359193
Action reg: 0.003982
  l1.weight: grad_norm = 0.045020
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.029703
Total gradient norm: 0.133799
=== Actor Training Debug (Iteration 3145) ===
Q mean: -9.711398
Q std: 14.241596
Actor loss: 9.715371
Action reg: 0.003973
  l1.weight: grad_norm = 0.087094
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.063779
Total gradient norm: 0.259396
=== Actor Training Debug (Iteration 3146) ===
Q mean: -13.257721
Q std: 15.459064
Actor loss: 13.261704
Action reg: 0.003983
  l1.weight: grad_norm = 0.023325
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.016157
Total gradient norm: 0.057987
=== Actor Training Debug (Iteration 3147) ===
Q mean: -13.330374
Q std: 14.823487
Actor loss: 13.334353
Action reg: 0.003979
  l1.weight: grad_norm = 0.089974
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.071750
Total gradient norm: 0.252919
=== Actor Training Debug (Iteration 3148) ===
Q mean: -13.465420
Q std: 16.254086
Actor loss: 13.469402
Action reg: 0.003982
  l1.weight: grad_norm = 0.049706
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.044211
Total gradient norm: 0.153173
=== Actor Training Debug (Iteration 3149) ===
Q mean: -9.660442
Q std: 14.370001
Actor loss: 9.664428
Action reg: 0.003985
  l1.weight: grad_norm = 0.010858
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.010835
Total gradient norm: 0.040121
=== Actor Training Debug (Iteration 3150) ===
Q mean: -11.276113
Q std: 14.676157
Actor loss: 11.280077
Action reg: 0.003965
  l1.weight: grad_norm = 0.101681
  l1.bias: grad_norm = 0.000754
  l2.weight: grad_norm = 0.070094
Total gradient norm: 0.250310
=== Actor Training Debug (Iteration 3151) ===
Q mean: -10.094535
Q std: 14.310570
Actor loss: 10.098519
Action reg: 0.003985
  l1.weight: grad_norm = 0.027099
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.020360
Total gradient norm: 0.074428
=== Actor Training Debug (Iteration 3152) ===
Q mean: -11.144753
Q std: 14.831381
Actor loss: 11.148744
Action reg: 0.003990
  l1.weight: grad_norm = 0.027504
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.021838
Total gradient norm: 0.080351
=== Actor Training Debug (Iteration 3153) ===
Q mean: -11.401838
Q std: 14.919975
Actor loss: 11.405818
Action reg: 0.003980
  l1.weight: grad_norm = 0.090530
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.064510
Total gradient norm: 0.264440
=== Actor Training Debug (Iteration 3154) ===
Q mean: -11.043108
Q std: 14.399214
Actor loss: 11.047098
Action reg: 0.003990
  l1.weight: grad_norm = 0.039985
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.029367
Total gradient norm: 0.157390
=== Actor Training Debug (Iteration 3155) ===
Q mean: -11.121118
Q std: 14.061113
Actor loss: 11.125107
Action reg: 0.003989
  l1.weight: grad_norm = 0.046764
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.034583
Total gradient norm: 0.143724
=== Actor Training Debug (Iteration 3156) ===
Q mean: -11.050130
Q std: 14.510874
Actor loss: 11.054119
Action reg: 0.003989
  l1.weight: grad_norm = 0.040358
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.029133
Total gradient norm: 0.112921
=== Actor Training Debug (Iteration 3157) ===
Q mean: -11.276735
Q std: 15.077765
Actor loss: 11.280714
Action reg: 0.003979
  l1.weight: grad_norm = 0.056123
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.037667
Total gradient norm: 0.151393
=== Actor Training Debug (Iteration 3158) ===
Q mean: -11.938210
Q std: 14.455931
Actor loss: 11.942195
Action reg: 0.003984
  l1.weight: grad_norm = 0.068741
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.062113
Total gradient norm: 0.200049
=== Actor Training Debug (Iteration 3159) ===
Q mean: -12.645086
Q std: 16.295334
Actor loss: 12.649068
Action reg: 0.003982
  l1.weight: grad_norm = 0.046407
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.032146
Total gradient norm: 0.114345
=== Actor Training Debug (Iteration 3160) ===
Q mean: -11.150400
Q std: 14.361172
Actor loss: 11.154375
Action reg: 0.003975
  l1.weight: grad_norm = 0.041579
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.032623
Total gradient norm: 0.112483
=== Actor Training Debug (Iteration 3161) ===
Q mean: -10.421916
Q std: 13.520758
Actor loss: 10.425910
Action reg: 0.003994
  l1.weight: grad_norm = 0.066875
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.047307
Total gradient norm: 0.171334
=== Actor Training Debug (Iteration 3162) ===
Q mean: -11.768943
Q std: 15.498143
Actor loss: 11.772929
Action reg: 0.003986
  l1.weight: grad_norm = 0.048584
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.040463
Total gradient norm: 0.193167
=== Actor Training Debug (Iteration 3163) ===
Q mean: -11.794397
Q std: 15.633231
Actor loss: 11.798379
Action reg: 0.003982
  l1.weight: grad_norm = 0.031162
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.022823
Total gradient norm: 0.083021
=== Actor Training Debug (Iteration 3164) ===
Q mean: -14.031353
Q std: 16.797312
Actor loss: 14.035336
Action reg: 0.003983
  l1.weight: grad_norm = 0.079599
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.055126
Total gradient norm: 0.267408
=== Actor Training Debug (Iteration 3165) ===
Q mean: -12.368227
Q std: 15.677464
Actor loss: 12.372198
Action reg: 0.003971
  l1.weight: grad_norm = 0.054110
  l1.bias: grad_norm = 0.001276
  l2.weight: grad_norm = 0.039679
Total gradient norm: 0.149949
=== Actor Training Debug (Iteration 3166) ===
Q mean: -11.374096
Q std: 15.576834
Actor loss: 11.378075
Action reg: 0.003978
  l1.weight: grad_norm = 0.071559
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.054009
Total gradient norm: 0.205359
=== Actor Training Debug (Iteration 3167) ===
Q mean: -12.243956
Q std: 15.250471
Actor loss: 12.247941
Action reg: 0.003986
  l1.weight: grad_norm = 0.043196
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.027906
Total gradient norm: 0.128885
=== Actor Training Debug (Iteration 3168) ===
Q mean: -10.753451
Q std: 14.706820
Actor loss: 10.757434
Action reg: 0.003983
  l1.weight: grad_norm = 0.032249
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.023006
Total gradient norm: 0.086379
=== Actor Training Debug (Iteration 3169) ===
Q mean: -12.300030
Q std: 15.533418
Actor loss: 12.303994
Action reg: 0.003965
  l1.weight: grad_norm = 0.079091
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.053898
Total gradient norm: 0.218028
=== Actor Training Debug (Iteration 3170) ===
Q mean: -11.999001
Q std: 14.942910
Actor loss: 12.002961
Action reg: 0.003961
  l1.weight: grad_norm = 0.103800
  l1.bias: grad_norm = 0.000706
  l2.weight: grad_norm = 0.080375
Total gradient norm: 0.323519
=== Actor Training Debug (Iteration 3171) ===
Q mean: -12.726522
Q std: 15.876492
Actor loss: 12.730498
Action reg: 0.003976
  l1.weight: grad_norm = 0.157550
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.097734
Total gradient norm: 0.394218
=== Actor Training Debug (Iteration 3172) ===
Q mean: -12.357897
Q std: 14.856160
Actor loss: 12.361876
Action reg: 0.003979
  l1.weight: grad_norm = 0.028454
  l1.bias: grad_norm = 0.000658
  l2.weight: grad_norm = 0.020826
Total gradient norm: 0.099591
=== Actor Training Debug (Iteration 3173) ===
Q mean: -11.403053
Q std: 15.295995
Actor loss: 11.407036
Action reg: 0.003982
  l1.weight: grad_norm = 0.048930
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.036024
Total gradient norm: 0.184169
=== Actor Training Debug (Iteration 3174) ===
Q mean: -13.247141
Q std: 16.608046
Actor loss: 13.251112
Action reg: 0.003971
  l1.weight: grad_norm = 0.059276
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.042983
Total gradient norm: 0.166817
=== Actor Training Debug (Iteration 3175) ===
Q mean: -12.092019
Q std: 15.150269
Actor loss: 12.096003
Action reg: 0.003984
  l1.weight: grad_norm = 0.038303
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.027051
Total gradient norm: 0.105070
=== Actor Training Debug (Iteration 3176) ===
Q mean: -10.841703
Q std: 14.393919
Actor loss: 10.845685
Action reg: 0.003981
  l1.weight: grad_norm = 0.074725
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.064723
Total gradient norm: 0.243083
=== Actor Training Debug (Iteration 3177) ===
Q mean: -11.133307
Q std: 15.110923
Actor loss: 11.137280
Action reg: 0.003973
  l1.weight: grad_norm = 0.037605
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.028794
Total gradient norm: 0.129785
=== Actor Training Debug (Iteration 3178) ===
Q mean: -14.145262
Q std: 16.500866
Actor loss: 14.149238
Action reg: 0.003976
  l1.weight: grad_norm = 0.052264
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.042504
Total gradient norm: 0.157674
=== Actor Training Debug (Iteration 3179) ===
Q mean: -12.293532
Q std: 15.025486
Actor loss: 12.297505
Action reg: 0.003973
  l1.weight: grad_norm = 0.041977
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.030159
Total gradient norm: 0.125535
=== Actor Training Debug (Iteration 3180) ===
Q mean: -14.302055
Q std: 16.349232
Actor loss: 14.306040
Action reg: 0.003984
  l1.weight: grad_norm = 0.110622
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.087735
Total gradient norm: 0.342427
=== Actor Training Debug (Iteration 3181) ===
Q mean: -12.477483
Q std: 15.729832
Actor loss: 12.481469
Action reg: 0.003986
  l1.weight: grad_norm = 0.027565
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.019640
Total gradient norm: 0.074517
=== Actor Training Debug (Iteration 3182) ===
Q mean: -11.230350
Q std: 14.563581
Actor loss: 11.234341
Action reg: 0.003990
  l1.weight: grad_norm = 0.033095
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.028391
Total gradient norm: 0.100485
=== Actor Training Debug (Iteration 3183) ===
Q mean: -13.376124
Q std: 16.287493
Actor loss: 13.380100
Action reg: 0.003976
  l1.weight: grad_norm = 0.175080
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.128717
Total gradient norm: 0.495834
=== Actor Training Debug (Iteration 3184) ===
Q mean: -11.957439
Q std: 15.021969
Actor loss: 11.961426
Action reg: 0.003987
  l1.weight: grad_norm = 0.039692
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.030819
Total gradient norm: 0.117554
=== Actor Training Debug (Iteration 3185) ===
Q mean: -8.870502
Q std: 13.447342
Actor loss: 8.874488
Action reg: 0.003987
  l1.weight: grad_norm = 0.035439
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.022882
Total gradient norm: 0.087090
=== Actor Training Debug (Iteration 3186) ===
Q mean: -12.765854
Q std: 15.242044
Actor loss: 12.769843
Action reg: 0.003989
  l1.weight: grad_norm = 0.068705
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.049684
Total gradient norm: 0.228132
=== Actor Training Debug (Iteration 3187) ===
Q mean: -13.063443
Q std: 16.249420
Actor loss: 13.067430
Action reg: 0.003987
  l1.weight: grad_norm = 0.015917
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.012000
Total gradient norm: 0.044114
=== Actor Training Debug (Iteration 3188) ===
Q mean: -12.384039
Q std: 15.471084
Actor loss: 12.388022
Action reg: 0.003983
  l1.weight: grad_norm = 0.095134
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.062316
Total gradient norm: 0.278239
=== Actor Training Debug (Iteration 3189) ===
Q mean: -12.663041
Q std: 15.506814
Actor loss: 12.667017
Action reg: 0.003976
  l1.weight: grad_norm = 0.048974
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.039556
Total gradient norm: 0.149581
=== Actor Training Debug (Iteration 3190) ===
Q mean: -12.559284
Q std: 15.692225
Actor loss: 12.563249
Action reg: 0.003964
  l1.weight: grad_norm = 0.076103
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.070243
Total gradient norm: 0.207357
=== Actor Training Debug (Iteration 3191) ===
Q mean: -11.533249
Q std: 15.052196
Actor loss: 11.537222
Action reg: 0.003973
  l1.weight: grad_norm = 0.035993
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.024238
Total gradient norm: 0.081356
=== Actor Training Debug (Iteration 3192) ===
Q mean: -11.085687
Q std: 15.279711
Actor loss: 11.089653
Action reg: 0.003966
  l1.weight: grad_norm = 0.050168
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.042246
Total gradient norm: 0.140026
=== Actor Training Debug (Iteration 3193) ===
Q mean: -12.988159
Q std: 16.261990
Actor loss: 12.992126
Action reg: 0.003966
  l1.weight: grad_norm = 0.030711
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.024782
Total gradient norm: 0.104696
=== Actor Training Debug (Iteration 3194) ===
Q mean: -12.376351
Q std: 15.606808
Actor loss: 12.380345
Action reg: 0.003994
  l1.weight: grad_norm = 0.017507
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.012130
Total gradient norm: 0.046618
=== Actor Training Debug (Iteration 3195) ===
Q mean: -11.997511
Q std: 14.789993
Actor loss: 12.001508
Action reg: 0.003997
  l1.weight: grad_norm = 0.072652
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.044790
Total gradient norm: 0.157895
=== Actor Training Debug (Iteration 3196) ===
Q mean: -11.443512
Q std: 15.515406
Actor loss: 11.447491
Action reg: 0.003978
  l1.weight: grad_norm = 0.061711
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.058467
Total gradient norm: 0.178509
=== Actor Training Debug (Iteration 3197) ===
Q mean: -11.654852
Q std: 15.753553
Actor loss: 11.658829
Action reg: 0.003977
  l1.weight: grad_norm = 0.074341
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.063063
Total gradient norm: 0.283027
=== Actor Training Debug (Iteration 3198) ===
Q mean: -12.576707
Q std: 15.778246
Actor loss: 12.580675
Action reg: 0.003969
  l1.weight: grad_norm = 0.074964
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.054921
Total gradient norm: 0.183086
=== Actor Training Debug (Iteration 3199) ===
Q mean: -11.866244
Q std: 14.569221
Actor loss: 11.870222
Action reg: 0.003978
  l1.weight: grad_norm = 0.047151
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.029886
Total gradient norm: 0.118891
=== Actor Training Debug (Iteration 3200) ===
Q mean: -12.360667
Q std: 15.112333
Actor loss: 12.364648
Action reg: 0.003980
  l1.weight: grad_norm = 0.059767
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.047144
Total gradient norm: 0.195292
=== Actor Training Debug (Iteration 3201) ===
Q mean: -13.078875
Q std: 15.853538
Actor loss: 13.082854
Action reg: 0.003979
  l1.weight: grad_norm = 0.029053
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.023895
Total gradient norm: 0.086187
=== Actor Training Debug (Iteration 3202) ===
Q mean: -9.910070
Q std: 13.602156
Actor loss: 9.914051
Action reg: 0.003981
  l1.weight: grad_norm = 0.044309
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.036633
Total gradient norm: 0.125826
=== Actor Training Debug (Iteration 3203) ===
Q mean: -12.374886
Q std: 15.839406
Actor loss: 12.378878
Action reg: 0.003992
  l1.weight: grad_norm = 0.013987
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.011189
Total gradient norm: 0.053891
=== Actor Training Debug (Iteration 3204) ===
Q mean: -12.052351
Q std: 15.042660
Actor loss: 12.056338
Action reg: 0.003988
  l1.weight: grad_norm = 0.046433
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.036362
Total gradient norm: 0.149768
=== Actor Training Debug (Iteration 3205) ===
Q mean: -11.170002
Q std: 15.442289
Actor loss: 11.173982
Action reg: 0.003980
  l1.weight: grad_norm = 0.142943
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.104887
Total gradient norm: 0.330011
=== Actor Training Debug (Iteration 3206) ===
Q mean: -11.307311
Q std: 14.716801
Actor loss: 11.311296
Action reg: 0.003986
  l1.weight: grad_norm = 0.069089
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.052006
Total gradient norm: 0.240694
=== Actor Training Debug (Iteration 3207) ===
Q mean: -11.174217
Q std: 14.698762
Actor loss: 11.178192
Action reg: 0.003975
  l1.weight: grad_norm = 0.060557
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.056308
Total gradient norm: 0.169871
=== Actor Training Debug (Iteration 3208) ===
Q mean: -10.572933
Q std: 13.960432
Actor loss: 10.576911
Action reg: 0.003978
  l1.weight: grad_norm = 0.051600
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.036564
Total gradient norm: 0.123802
=== Actor Training Debug (Iteration 3209) ===
Q mean: -12.123041
Q std: 15.118965
Actor loss: 12.127024
Action reg: 0.003983
  l1.weight: grad_norm = 0.044801
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.031641
Total gradient norm: 0.102832
=== Actor Training Debug (Iteration 3210) ===
Q mean: -10.855646
Q std: 14.689407
Actor loss: 10.859615
Action reg: 0.003969
  l1.weight: grad_norm = 0.167594
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.113019
Total gradient norm: 0.455598
=== Actor Training Debug (Iteration 3211) ===
Q mean: -10.619027
Q std: 14.532407
Actor loss: 10.623019
Action reg: 0.003992
  l1.weight: grad_norm = 0.029794
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.023764
Total gradient norm: 0.083997
=== Actor Training Debug (Iteration 3212) ===
Q mean: -12.448918
Q std: 15.166683
Actor loss: 12.452903
Action reg: 0.003984
  l1.weight: grad_norm = 0.044985
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.031889
Total gradient norm: 0.116674
=== Actor Training Debug (Iteration 3213) ===
Q mean: -12.242098
Q std: 15.717209
Actor loss: 12.246082
Action reg: 0.003984
  l1.weight: grad_norm = 0.012470
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.010881
Total gradient norm: 0.041685
=== Actor Training Debug (Iteration 3214) ===
Q mean: -12.428316
Q std: 15.660493
Actor loss: 12.432302
Action reg: 0.003985
  l1.weight: grad_norm = 0.027626
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.025170
Total gradient norm: 0.135614
=== Actor Training Debug (Iteration 3215) ===
Q mean: -11.866028
Q std: 15.136370
Actor loss: 11.870011
Action reg: 0.003983
  l1.weight: grad_norm = 0.038920
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.031223
Total gradient norm: 0.114542
=== Actor Training Debug (Iteration 3216) ===
Q mean: -12.194241
Q std: 14.649671
Actor loss: 12.198220
Action reg: 0.003980
  l1.weight: grad_norm = 0.026660
  l1.bias: grad_norm = 0.000883
  l2.weight: grad_norm = 0.019758
Total gradient norm: 0.085698
=== Actor Training Debug (Iteration 3217) ===
Q mean: -12.339571
Q std: 14.904348
Actor loss: 12.343554
Action reg: 0.003983
  l1.weight: grad_norm = 0.048923
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.032348
Total gradient norm: 0.144481
=== Actor Training Debug (Iteration 3218) ===
Q mean: -12.977500
Q std: 14.524325
Actor loss: 12.981471
Action reg: 0.003971
  l1.weight: grad_norm = 0.067850
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.053006
Total gradient norm: 0.235541
=== Actor Training Debug (Iteration 3219) ===
Q mean: -12.807784
Q std: 15.539425
Actor loss: 12.811769
Action reg: 0.003984
  l1.weight: grad_norm = 0.035716
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.026459
Total gradient norm: 0.093214
=== Actor Training Debug (Iteration 3220) ===
Q mean: -12.283884
Q std: 15.448483
Actor loss: 12.287852
Action reg: 0.003968
  l1.weight: grad_norm = 0.081557
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.062103
Total gradient norm: 0.209346
=== Actor Training Debug (Iteration 3221) ===
Q mean: -12.779972
Q std: 15.507599
Actor loss: 12.783951
Action reg: 0.003979
  l1.weight: grad_norm = 0.033116
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.031305
Total gradient norm: 0.117956
=== Actor Training Debug (Iteration 3222) ===
Q mean: -10.988026
Q std: 15.608727
Actor loss: 10.992000
Action reg: 0.003974
  l1.weight: grad_norm = 0.072078
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.056455
Total gradient norm: 0.192586
=== Actor Training Debug (Iteration 3223) ===
Q mean: -13.435242
Q std: 15.180778
Actor loss: 13.439236
Action reg: 0.003994
  l1.weight: grad_norm = 0.012932
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.010648
Total gradient norm: 0.053394
=== Actor Training Debug (Iteration 3224) ===
Q mean: -10.893396
Q std: 14.797521
Actor loss: 10.897378
Action reg: 0.003982
  l1.weight: grad_norm = 0.071567
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.053797
Total gradient norm: 0.231600
=== Actor Training Debug (Iteration 3225) ===
Q mean: -11.488997
Q std: 14.557790
Actor loss: 11.492987
Action reg: 0.003989
  l1.weight: grad_norm = 0.061921
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.049723
Total gradient norm: 0.232218
=== Actor Training Debug (Iteration 3226) ===
Q mean: -11.110184
Q std: 14.630848
Actor loss: 11.114169
Action reg: 0.003985
  l1.weight: grad_norm = 0.099471
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.066866
Total gradient norm: 0.247771
=== Actor Training Debug (Iteration 3227) ===
Q mean: -12.592630
Q std: 15.592732
Actor loss: 12.596602
Action reg: 0.003972
  l1.weight: grad_norm = 0.058898
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.045428
Total gradient norm: 0.160494
=== Actor Training Debug (Iteration 3228) ===
Q mean: -10.729471
Q std: 14.991627
Actor loss: 10.733456
Action reg: 0.003984
  l1.weight: grad_norm = 0.054023
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.044050
Total gradient norm: 0.162694
=== Actor Training Debug (Iteration 3229) ===
Q mean: -11.742377
Q std: 14.760064
Actor loss: 11.746352
Action reg: 0.003975
  l1.weight: grad_norm = 0.082828
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.059923
Total gradient norm: 0.214567
=== Actor Training Debug (Iteration 3230) ===
Q mean: -12.015984
Q std: 14.887416
Actor loss: 12.019955
Action reg: 0.003971
  l1.weight: grad_norm = 0.094782
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.069144
Total gradient norm: 0.333230
=== Actor Training Debug (Iteration 3231) ===
Q mean: -12.443256
Q std: 15.120479
Actor loss: 12.447247
Action reg: 0.003990
  l1.weight: grad_norm = 0.024581
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.016315
Total gradient norm: 0.062789
=== Actor Training Debug (Iteration 3232) ===
Q mean: -12.441813
Q std: 15.670045
Actor loss: 12.445803
Action reg: 0.003990
  l1.weight: grad_norm = 0.030696
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.024571
Total gradient norm: 0.098128
=== Actor Training Debug (Iteration 3233) ===
Q mean: -11.525717
Q std: 15.163421
Actor loss: 11.529698
Action reg: 0.003982
  l1.weight: grad_norm = 0.035314
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.026723
Total gradient norm: 0.095640
=== Actor Training Debug (Iteration 3234) ===
Q mean: -11.906477
Q std: 15.210058
Actor loss: 11.910443
Action reg: 0.003966
  l1.weight: grad_norm = 0.017767
  l1.bias: grad_norm = 0.000623
  l2.weight: grad_norm = 0.013732
Total gradient norm: 0.055666
=== Actor Training Debug (Iteration 3235) ===
Q mean: -10.892590
Q std: 14.116635
Actor loss: 10.896577
Action reg: 0.003987
  l1.weight: grad_norm = 0.028972
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.023246
Total gradient norm: 0.138944
=== Actor Training Debug (Iteration 3236) ===
Q mean: -11.740008
Q std: 14.489611
Actor loss: 11.743984
Action reg: 0.003976
  l1.weight: grad_norm = 0.046087
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.032801
Total gradient norm: 0.164418
=== Actor Training Debug (Iteration 3237) ===
Q mean: -12.548248
Q std: 15.717365
Actor loss: 12.552230
Action reg: 0.003982
  l1.weight: grad_norm = 0.056963
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.046718
Total gradient norm: 0.170899
=== Actor Training Debug (Iteration 3238) ===
Q mean: -11.929956
Q std: 15.111811
Actor loss: 11.933940
Action reg: 0.003984
  l1.weight: grad_norm = 0.032359
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.020608
Total gradient norm: 0.077891
=== Actor Training Debug (Iteration 3239) ===
Q mean: -11.383942
Q std: 15.206112
Actor loss: 11.387925
Action reg: 0.003983
  l1.weight: grad_norm = 0.050667
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.038118
Total gradient norm: 0.147707
=== Actor Training Debug (Iteration 3240) ===
Q mean: -10.342731
Q std: 14.377967
Actor loss: 10.346695
Action reg: 0.003965
  l1.weight: grad_norm = 0.034644
  l1.bias: grad_norm = 0.000658
  l2.weight: grad_norm = 0.027267
Total gradient norm: 0.108770
=== Actor Training Debug (Iteration 3241) ===
Q mean: -13.456065
Q std: 15.993701
Actor loss: 13.460060
Action reg: 0.003995
  l1.weight: grad_norm = 0.035701
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.031174
Total gradient norm: 0.159805
=== Actor Training Debug (Iteration 3242) ===
Q mean: -11.653478
Q std: 14.392688
Actor loss: 11.657458
Action reg: 0.003981
  l1.weight: grad_norm = 0.054549
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.037766
Total gradient norm: 0.136077
=== Actor Training Debug (Iteration 3243) ===
Q mean: -13.602735
Q std: 16.478260
Actor loss: 13.606714
Action reg: 0.003980
  l1.weight: grad_norm = 0.030192
  l1.bias: grad_norm = 0.000876
  l2.weight: grad_norm = 0.023939
Total gradient norm: 0.081064
=== Actor Training Debug (Iteration 3244) ===
Q mean: -13.131254
Q std: 15.486978
Actor loss: 13.135242
Action reg: 0.003988
  l1.weight: grad_norm = 0.085654
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.061167
Total gradient norm: 0.211081
=== Actor Training Debug (Iteration 3245) ===
Q mean: -12.972052
Q std: 16.045156
Actor loss: 12.976044
Action reg: 0.003992
  l1.weight: grad_norm = 0.025390
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.017640
Total gradient norm: 0.058379
=== Actor Training Debug (Iteration 3246) ===
Q mean: -12.893261
Q std: 16.358555
Actor loss: 12.897247
Action reg: 0.003987
  l1.weight: grad_norm = 0.084734
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.053908
Total gradient norm: 0.268831
=== Actor Training Debug (Iteration 3247) ===
Q mean: -12.372981
Q std: 14.832421
Actor loss: 12.376964
Action reg: 0.003982
  l1.weight: grad_norm = 0.044663
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.031286
Total gradient norm: 0.133252
=== Actor Training Debug (Iteration 3248) ===
Q mean: -11.752219
Q std: 15.524379
Actor loss: 11.756198
Action reg: 0.003979
  l1.weight: grad_norm = 0.028071
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.020104
Total gradient norm: 0.079274
=== Actor Training Debug (Iteration 3249) ===
Q mean: -11.051065
Q std: 15.028011
Actor loss: 11.055034
Action reg: 0.003968
  l1.weight: grad_norm = 0.068585
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.057130
Total gradient norm: 0.218188
=== Actor Training Debug (Iteration 3250) ===
Q mean: -11.791979
Q std: 15.280068
Actor loss: 11.795961
Action reg: 0.003982
  l1.weight: grad_norm = 0.031343
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.024527
Total gradient norm: 0.095489
=== Actor Training Debug (Iteration 3251) ===
Q mean: -13.343450
Q std: 16.404654
Actor loss: 13.347438
Action reg: 0.003988
  l1.weight: grad_norm = 0.035469
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.035089
Total gradient norm: 0.150023
=== Actor Training Debug (Iteration 3252) ===
Q mean: -12.519022
Q std: 15.214886
Actor loss: 12.523018
Action reg: 0.003996
  l1.weight: grad_norm = 0.053095
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.048517
Total gradient norm: 0.159932
=== Actor Training Debug (Iteration 3253) ===
Q mean: -11.473348
Q std: 15.062854
Actor loss: 11.477323
Action reg: 0.003975
  l1.weight: grad_norm = 0.041040
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.027138
Total gradient norm: 0.095530
=== Actor Training Debug (Iteration 3254) ===
Q mean: -10.735041
Q std: 14.800199
Actor loss: 10.739020
Action reg: 0.003980
  l1.weight: grad_norm = 0.095137
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.064242
Total gradient norm: 0.236666
=== Actor Training Debug (Iteration 3255) ===
Q mean: -12.257599
Q std: 16.259567
Actor loss: 12.261575
Action reg: 0.003976
  l1.weight: grad_norm = 0.076165
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.052867
Total gradient norm: 0.217374
=== Actor Training Debug (Iteration 3256) ===
Q mean: -11.219687
Q std: 14.942220
Actor loss: 11.223674
Action reg: 0.003987
  l1.weight: grad_norm = 0.052362
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.042336
Total gradient norm: 0.184515
=== Actor Training Debug (Iteration 3257) ===
Q mean: -13.066048
Q std: 15.796535
Actor loss: 13.070031
Action reg: 0.003983
  l1.weight: grad_norm = 0.074764
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.060563
Total gradient norm: 0.322664
=== Actor Training Debug (Iteration 3258) ===
Q mean: -11.625845
Q std: 15.310260
Actor loss: 11.629818
Action reg: 0.003973
  l1.weight: grad_norm = 0.050129
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.048521
Total gradient norm: 0.164999
=== Actor Training Debug (Iteration 3259) ===
Q mean: -13.043207
Q std: 15.229289
Actor loss: 13.047192
Action reg: 0.003984
  l1.weight: grad_norm = 0.023535
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.017773
Total gradient norm: 0.067583
=== Actor Training Debug (Iteration 3260) ===
Q mean: -12.177394
Q std: 15.397193
Actor loss: 12.181387
Action reg: 0.003993
  l1.weight: grad_norm = 0.077612
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.066298
Total gradient norm: 0.248285
=== Actor Training Debug (Iteration 3261) ===
Q mean: -12.705793
Q std: 15.913600
Actor loss: 12.709785
Action reg: 0.003992
  l1.weight: grad_norm = 0.032668
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.022539
Total gradient norm: 0.116745
=== Actor Training Debug (Iteration 3262) ===
Q mean: -12.547516
Q std: 15.033846
Actor loss: 12.551508
Action reg: 0.003992
  l1.weight: grad_norm = 0.039793
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.032342
Total gradient norm: 0.096148
=== Actor Training Debug (Iteration 3263) ===
Q mean: -11.818415
Q std: 14.826632
Actor loss: 11.822394
Action reg: 0.003979
  l1.weight: grad_norm = 0.058209
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.043716
Total gradient norm: 0.142764
=== Actor Training Debug (Iteration 3264) ===
Q mean: -13.185135
Q std: 15.926280
Actor loss: 13.189124
Action reg: 0.003990
  l1.weight: grad_norm = 0.088212
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.068820
Total gradient norm: 0.308651
=== Actor Training Debug (Iteration 3265) ===
Q mean: -12.238976
Q std: 15.427123
Actor loss: 12.242963
Action reg: 0.003987
  l1.weight: grad_norm = 0.082987
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.062100
Total gradient norm: 0.269529
=== Actor Training Debug (Iteration 3266) ===
Q mean: -11.484242
Q std: 15.121777
Actor loss: 11.488231
Action reg: 0.003988
  l1.weight: grad_norm = 0.080966
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.063752
Total gradient norm: 0.245412
=== Actor Training Debug (Iteration 3267) ===
Q mean: -10.932442
Q std: 15.178540
Actor loss: 10.936419
Action reg: 0.003977
  l1.weight: grad_norm = 0.040829
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.029605
Total gradient norm: 0.088504
=== Actor Training Debug (Iteration 3268) ===
Q mean: -11.609961
Q std: 15.383321
Actor loss: 11.613951
Action reg: 0.003990
  l1.weight: grad_norm = 0.115628
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.089701
Total gradient norm: 0.389453
=== Actor Training Debug (Iteration 3269) ===
Q mean: -12.466482
Q std: 16.099384
Actor loss: 12.470468
Action reg: 0.003985
  l1.weight: grad_norm = 0.048063
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.042653
Total gradient norm: 0.207512
=== Actor Training Debug (Iteration 3270) ===
Q mean: -12.235504
Q std: 15.425016
Actor loss: 12.239493
Action reg: 0.003989
  l1.weight: grad_norm = 0.072601
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.066090
Total gradient norm: 0.238346
=== Actor Training Debug (Iteration 3271) ===
Q mean: -13.117224
Q std: 16.078459
Actor loss: 13.121207
Action reg: 0.003983
  l1.weight: grad_norm = 0.087695
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.074424
Total gradient norm: 0.403079
=== Actor Training Debug (Iteration 3272) ===
Q mean: -12.410292
Q std: 16.073895
Actor loss: 12.414277
Action reg: 0.003985
  l1.weight: grad_norm = 0.069566
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.051752
Total gradient norm: 0.211104
=== Actor Training Debug (Iteration 3273) ===
Q mean: -12.224770
Q std: 14.586519
Actor loss: 12.228749
Action reg: 0.003980
  l1.weight: grad_norm = 0.049684
  l1.bias: grad_norm = 0.000584
  l2.weight: grad_norm = 0.031981
Total gradient norm: 0.138545
=== Actor Training Debug (Iteration 3274) ===
Q mean: -11.906563
Q std: 15.920737
Actor loss: 11.910543
Action reg: 0.003981
  l1.weight: grad_norm = 0.096330
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.064613
Total gradient norm: 0.230863
=== Actor Training Debug (Iteration 3275) ===
Q mean: -11.255169
Q std: 14.870494
Actor loss: 11.259164
Action reg: 0.003995
  l1.weight: grad_norm = 0.035728
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.035646
Total gradient norm: 0.171931
=== Actor Training Debug (Iteration 3276) ===
Q mean: -10.956271
Q std: 14.857161
Actor loss: 10.960245
Action reg: 0.003974
  l1.weight: grad_norm = 0.147385
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.101248
Total gradient norm: 0.490882
=== Actor Training Debug (Iteration 3277) ===
Q mean: -12.240992
Q std: 15.630641
Actor loss: 12.244958
Action reg: 0.003967
  l1.weight: grad_norm = 0.137067
  l1.bias: grad_norm = 0.000674
  l2.weight: grad_norm = 0.117711
Total gradient norm: 0.628546
=== Actor Training Debug (Iteration 3278) ===
Q mean: -12.236797
Q std: 15.336325
Actor loss: 12.240775
Action reg: 0.003978
  l1.weight: grad_norm = 0.212020
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.168578
Total gradient norm: 0.902852
=== Actor Training Debug (Iteration 3279) ===
Q mean: -11.101196
Q std: 14.355122
Actor loss: 11.105170
Action reg: 0.003974
  l1.weight: grad_norm = 0.114894
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.088125
Total gradient norm: 0.390712
=== Actor Training Debug (Iteration 3280) ===
Q mean: -11.892784
Q std: 15.236673
Actor loss: 11.896754
Action reg: 0.003971
  l1.weight: grad_norm = 0.072031
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.066729
Total gradient norm: 0.392102
=== Actor Training Debug (Iteration 3281) ===
Q mean: -11.607051
Q std: 14.671784
Actor loss: 11.611030
Action reg: 0.003979
  l1.weight: grad_norm = 0.071321
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.056489
Total gradient norm: 0.257923
=== Actor Training Debug (Iteration 3282) ===
Q mean: -11.590609
Q std: 14.500261
Actor loss: 11.594584
Action reg: 0.003976
  l1.weight: grad_norm = 0.070942
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.056107
Total gradient norm: 0.210977
=== Actor Training Debug (Iteration 3283) ===
Q mean: -12.980369
Q std: 16.021103
Actor loss: 12.984349
Action reg: 0.003980
  l1.weight: grad_norm = 0.069315
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.060414
Total gradient norm: 0.236252
=== Actor Training Debug (Iteration 3284) ===
Q mean: -11.444839
Q std: 14.605494
Actor loss: 11.448822
Action reg: 0.003984
  l1.weight: grad_norm = 0.020718
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.018405
Total gradient norm: 0.065711
=== Actor Training Debug (Iteration 3285) ===
Q mean: -10.518698
Q std: 14.510671
Actor loss: 10.522666
Action reg: 0.003969
  l1.weight: grad_norm = 0.070986
  l1.bias: grad_norm = 0.000662
  l2.weight: grad_norm = 0.054230
Total gradient norm: 0.192316
=== Actor Training Debug (Iteration 3286) ===
Q mean: -11.537819
Q std: 14.565450
Actor loss: 11.541798
Action reg: 0.003979
  l1.weight: grad_norm = 0.033693
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.024959
Total gradient norm: 0.122061
=== Actor Training Debug (Iteration 3287) ===
Q mean: -12.249910
Q std: 16.215916
Actor loss: 12.253888
Action reg: 0.003977
  l1.weight: grad_norm = 0.022128
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.015917
Total gradient norm: 0.081536
=== Actor Training Debug (Iteration 3288) ===
Q mean: -14.081273
Q std: 16.669563
Actor loss: 14.085252
Action reg: 0.003979
  l1.weight: grad_norm = 0.027820
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.018057
Total gradient norm: 0.059958
=== Actor Training Debug (Iteration 3289) ===
Q mean: -11.028278
Q std: 15.236484
Actor loss: 11.032263
Action reg: 0.003985
  l1.weight: grad_norm = 0.027829
  l1.bias: grad_norm = 0.000736
  l2.weight: grad_norm = 0.021442
Total gradient norm: 0.071094
=== Actor Training Debug (Iteration 3290) ===
Q mean: -13.051943
Q std: 16.187923
Actor loss: 13.055921
Action reg: 0.003978
  l1.weight: grad_norm = 0.116431
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.089429
Total gradient norm: 0.347559
=== Actor Training Debug (Iteration 3291) ===
Q mean: -12.099833
Q std: 14.989038
Actor loss: 12.103807
Action reg: 0.003975
  l1.weight: grad_norm = 0.098000
  l1.bias: grad_norm = 0.000494
  l2.weight: grad_norm = 0.060504
Total gradient norm: 0.224007
=== Actor Training Debug (Iteration 3292) ===
Q mean: -12.253683
Q std: 15.282070
Actor loss: 12.257671
Action reg: 0.003988
  l1.weight: grad_norm = 0.061342
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.050014
Total gradient norm: 0.161465
=== Actor Training Debug (Iteration 3293) ===
Q mean: -12.977613
Q std: 15.978333
Actor loss: 12.981604
Action reg: 0.003990
  l1.weight: grad_norm = 0.100464
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.070087
Total gradient norm: 0.286106
=== Actor Training Debug (Iteration 3294) ===
Q mean: -14.752083
Q std: 16.241137
Actor loss: 14.756072
Action reg: 0.003989
  l1.weight: grad_norm = 0.113628
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.095163
Total gradient norm: 0.352721
=== Actor Training Debug (Iteration 3295) ===
Q mean: -12.818738
Q std: 15.814363
Actor loss: 12.822734
Action reg: 0.003996
  l1.weight: grad_norm = 0.038217
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.027865
Total gradient norm: 0.117460
=== Actor Training Debug (Iteration 3296) ===
Q mean: -10.921329
Q std: 15.387504
Actor loss: 10.925317
Action reg: 0.003988
  l1.weight: grad_norm = 0.014654
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.011387
Total gradient norm: 0.044390
=== Actor Training Debug (Iteration 3297) ===
Q mean: -13.385359
Q std: 16.035921
Actor loss: 13.389329
Action reg: 0.003970
  l1.weight: grad_norm = 0.059993
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.047180
Total gradient norm: 0.194027
=== Actor Training Debug (Iteration 3298) ===
Q mean: -12.859049
Q std: 16.079681
Actor loss: 12.863030
Action reg: 0.003981
  l1.weight: grad_norm = 0.049622
  l1.bias: grad_norm = 0.000981
  l2.weight: grad_norm = 0.036612
Total gradient norm: 0.138772
=== Actor Training Debug (Iteration 3299) ===
Q mean: -11.628698
Q std: 15.054497
Actor loss: 11.632678
Action reg: 0.003980
  l1.weight: grad_norm = 0.019813
  l1.bias: grad_norm = 0.000888
  l2.weight: grad_norm = 0.017019
Total gradient norm: 0.066751
=== Actor Training Debug (Iteration 3300) ===
Q mean: -11.098658
Q std: 14.367025
Actor loss: 11.102635
Action reg: 0.003978
  l1.weight: grad_norm = 0.053201
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.032839
Total gradient norm: 0.145564
=== Actor Training Debug (Iteration 3301) ===
Q mean: -13.625018
Q std: 16.532074
Actor loss: 13.629000
Action reg: 0.003982
  l1.weight: grad_norm = 0.066697
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.050344
Total gradient norm: 0.249539
=== Actor Training Debug (Iteration 3302) ===
Q mean: -11.505434
Q std: 15.851908
Actor loss: 11.509424
Action reg: 0.003990
  l1.weight: grad_norm = 0.023665
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.019932
Total gradient norm: 0.088154
=== Actor Training Debug (Iteration 3303) ===
Q mean: -10.606478
Q std: 14.901395
Actor loss: 10.610455
Action reg: 0.003977
  l1.weight: grad_norm = 0.105438
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.073343
Total gradient norm: 0.290405
=== Actor Training Debug (Iteration 3304) ===
Q mean: -14.745451
Q std: 16.044827
Actor loss: 14.749417
Action reg: 0.003966
  l1.weight: grad_norm = 0.107537
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.081824
Total gradient norm: 0.329422
=== Actor Training Debug (Iteration 3305) ===
Q mean: -11.882767
Q std: 14.144403
Actor loss: 11.886742
Action reg: 0.003975
  l1.weight: grad_norm = 0.110118
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.078498
Total gradient norm: 0.281602
=== Actor Training Debug (Iteration 3306) ===
Q mean: -11.730635
Q std: 15.110020
Actor loss: 11.734620
Action reg: 0.003985
  l1.weight: grad_norm = 0.020891
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.015571
Total gradient norm: 0.055573
=== Actor Training Debug (Iteration 3307) ===
Q mean: -12.628866
Q std: 16.246382
Actor loss: 12.632854
Action reg: 0.003988
  l1.weight: grad_norm = 0.060873
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.051592
Total gradient norm: 0.190976
=== Actor Training Debug (Iteration 3308) ===
Q mean: -11.871856
Q std: 15.796534
Actor loss: 11.875836
Action reg: 0.003980
  l1.weight: grad_norm = 0.159246
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.105949
Total gradient norm: 0.424384
=== Actor Training Debug (Iteration 3309) ===
Q mean: -11.581088
Q std: 15.141819
Actor loss: 11.585074
Action reg: 0.003986
  l1.weight: grad_norm = 0.049407
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.038310
Total gradient norm: 0.161694
=== Actor Training Debug (Iteration 3310) ===
Q mean: -12.448469
Q std: 15.989674
Actor loss: 12.452449
Action reg: 0.003980
  l1.weight: grad_norm = 0.027760
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.025399
Total gradient norm: 0.088684
=== Actor Training Debug (Iteration 3311) ===
Q mean: -10.381454
Q std: 15.413311
Actor loss: 10.385431
Action reg: 0.003978
  l1.weight: grad_norm = 0.031785
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.019788
Total gradient norm: 0.070262
=== Actor Training Debug (Iteration 3312) ===
Q mean: -12.788618
Q std: 15.255863
Actor loss: 12.792601
Action reg: 0.003983
  l1.weight: grad_norm = 0.058435
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.039436
Total gradient norm: 0.130729
=== Actor Training Debug (Iteration 3313) ===
Q mean: -11.723324
Q std: 15.505948
Actor loss: 11.727307
Action reg: 0.003984
  l1.weight: grad_norm = 0.110258
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.083151
Total gradient norm: 0.263780
=== Actor Training Debug (Iteration 3314) ===
Q mean: -10.478964
Q std: 15.236953
Actor loss: 10.482946
Action reg: 0.003982
  l1.weight: grad_norm = 0.014034
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.011323
Total gradient norm: 0.041268
=== Actor Training Debug (Iteration 3315) ===
Q mean: -11.758798
Q std: 15.189566
Actor loss: 11.762781
Action reg: 0.003983
  l1.weight: grad_norm = 0.113469
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.077057
Total gradient norm: 0.296584
=== Actor Training Debug (Iteration 3316) ===
Q mean: -13.435363
Q std: 16.128847
Actor loss: 13.439354
Action reg: 0.003991
  l1.weight: grad_norm = 0.097201
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.071462
Total gradient norm: 0.361965
=== Actor Training Debug (Iteration 3317) ===
Q mean: -12.406271
Q std: 15.364040
Actor loss: 12.410263
Action reg: 0.003992
  l1.weight: grad_norm = 0.094669
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.070616
Total gradient norm: 0.284028
=== Actor Training Debug (Iteration 3318) ===
Q mean: -10.742994
Q std: 14.762514
Actor loss: 10.746979
Action reg: 0.003985
  l1.weight: grad_norm = 0.058677
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.037090
Total gradient norm: 0.139595
=== Actor Training Debug (Iteration 3319) ===
Q mean: -11.654708
Q std: 15.739022
Actor loss: 11.658696
Action reg: 0.003988
  l1.weight: grad_norm = 0.156567
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.103452
Total gradient norm: 0.500795
=== Actor Training Debug (Iteration 3320) ===
Q mean: -12.697006
Q std: 15.952190
Actor loss: 12.700993
Action reg: 0.003986
  l1.weight: grad_norm = 0.079860
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.069730
Total gradient norm: 0.330028
=== Actor Training Debug (Iteration 3321) ===
Q mean: -12.642291
Q std: 15.420098
Actor loss: 12.646275
Action reg: 0.003983
  l1.weight: grad_norm = 0.046876
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.033896
Total gradient norm: 0.164870
=== Actor Training Debug (Iteration 3322) ===
Q mean: -11.732326
Q std: 14.645639
Actor loss: 11.736315
Action reg: 0.003989
  l1.weight: grad_norm = 0.031117
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.022071
Total gradient norm: 0.097802
=== Actor Training Debug (Iteration 3323) ===
Q mean: -12.387203
Q std: 15.778334
Actor loss: 12.391191
Action reg: 0.003988
  l1.weight: grad_norm = 0.036747
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.029129
Total gradient norm: 0.117282
=== Actor Training Debug (Iteration 3324) ===
Q mean: -12.952620
Q std: 16.146563
Actor loss: 12.956603
Action reg: 0.003983
  l1.weight: grad_norm = 0.017163
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.016056
Total gradient norm: 0.053828
=== Actor Training Debug (Iteration 3325) ===
Q mean: -12.122362
Q std: 16.202309
Actor loss: 12.126344
Action reg: 0.003982
  l1.weight: grad_norm = 0.038419
  l1.bias: grad_norm = 0.000414
  l2.weight: grad_norm = 0.025573
Total gradient norm: 0.108979
=== Actor Training Debug (Iteration 3326) ===
Q mean: -12.607718
Q std: 15.100536
Actor loss: 12.611697
Action reg: 0.003979
  l1.weight: grad_norm = 0.027775
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.019513
Total gradient norm: 0.068333
=== Actor Training Debug (Iteration 3327) ===
Q mean: -13.962740
Q std: 16.187614
Actor loss: 13.966728
Action reg: 0.003988
  l1.weight: grad_norm = 0.039037
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.025253
Total gradient norm: 0.098930
=== Actor Training Debug (Iteration 3328) ===
Q mean: -11.685914
Q std: 14.964262
Actor loss: 11.689889
Action reg: 0.003975
  l1.weight: grad_norm = 0.120531
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.083404
Total gradient norm: 0.336320
=== Actor Training Debug (Iteration 3329) ===
Q mean: -12.309096
Q std: 16.646627
Actor loss: 12.313075
Action reg: 0.003979
  l1.weight: grad_norm = 0.020782
  l1.bias: grad_norm = 0.001132
  l2.weight: grad_norm = 0.019446
Total gradient norm: 0.071022
=== Actor Training Debug (Iteration 3330) ===
Q mean: -13.178263
Q std: 15.760604
Actor loss: 13.182243
Action reg: 0.003980
  l1.weight: grad_norm = 0.038064
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.029776
Total gradient norm: 0.126928
=== Actor Training Debug (Iteration 3331) ===
Q mean: -12.584637
Q std: 16.077511
Actor loss: 12.588624
Action reg: 0.003987
  l1.weight: grad_norm = 0.156704
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.128697
Total gradient norm: 0.569462
=== Actor Training Debug (Iteration 3332) ===
Q mean: -11.778843
Q std: 15.562105
Actor loss: 11.782823
Action reg: 0.003980
  l1.weight: grad_norm = 0.069155
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.053812
Total gradient norm: 0.214875
=== Actor Training Debug (Iteration 3333) ===
Q mean: -10.737450
Q std: 14.893357
Actor loss: 10.741434
Action reg: 0.003985
  l1.weight: grad_norm = 0.062530
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.051679
Total gradient norm: 0.164587
=== Actor Training Debug (Iteration 3334) ===
Q mean: -11.862633
Q std: 14.444564
Actor loss: 11.866613
Action reg: 0.003981
  l1.weight: grad_norm = 0.033613
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.022726
Total gradient norm: 0.095460
=== Actor Training Debug (Iteration 3335) ===
Q mean: -13.469015
Q std: 15.316084
Actor loss: 13.472993
Action reg: 0.003978
  l1.weight: grad_norm = 0.075862
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.069732
Total gradient norm: 0.238159
=== Actor Training Debug (Iteration 3336) ===
Q mean: -12.413866
Q std: 15.193666
Actor loss: 12.417853
Action reg: 0.003987
  l1.weight: grad_norm = 0.180466
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.130109
Total gradient norm: 0.562069
=== Actor Training Debug (Iteration 3337) ===
Q mean: -12.270384
Q std: 15.589235
Actor loss: 12.274349
Action reg: 0.003966
  l1.weight: grad_norm = 0.163463
  l1.bias: grad_norm = 0.001114
  l2.weight: grad_norm = 0.115115
Total gradient norm: 0.424446
=== Actor Training Debug (Iteration 3338) ===
Q mean: -12.670492
Q std: 16.229048
Actor loss: 12.674471
Action reg: 0.003979
  l1.weight: grad_norm = 0.015742
  l1.bias: grad_norm = 0.000726
  l2.weight: grad_norm = 0.010903
Total gradient norm: 0.039393
=== Actor Training Debug (Iteration 3339) ===
Q mean: -11.377927
Q std: 15.483856
Actor loss: 11.381888
Action reg: 0.003962
  l1.weight: grad_norm = 0.017519
  l1.bias: grad_norm = 0.000712
  l2.weight: grad_norm = 0.015256
Total gradient norm: 0.074378
=== Actor Training Debug (Iteration 3340) ===
Q mean: -14.065546
Q std: 16.260893
Actor loss: 14.069520
Action reg: 0.003974
  l1.weight: grad_norm = 0.041282
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.034039
Total gradient norm: 0.131006
=== Actor Training Debug (Iteration 3341) ===
Q mean: -12.368729
Q std: 15.424602
Actor loss: 12.372718
Action reg: 0.003989
  l1.weight: grad_norm = 0.031520
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.025186
Total gradient norm: 0.100180
=== Actor Training Debug (Iteration 3342) ===
Q mean: -12.126745
Q std: 15.810119
Actor loss: 12.130732
Action reg: 0.003986
  l1.weight: grad_norm = 0.039721
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.026448
Total gradient norm: 0.102870
=== Actor Training Debug (Iteration 3343) ===
Q mean: -11.888467
Q std: 15.569172
Actor loss: 11.892450
Action reg: 0.003983
  l1.weight: grad_norm = 0.031820
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.024701
Total gradient norm: 0.088822
=== Actor Training Debug (Iteration 3344) ===
Q mean: -12.936528
Q std: 15.631798
Actor loss: 12.940510
Action reg: 0.003981
  l1.weight: grad_norm = 0.029239
  l1.bias: grad_norm = 0.000600
  l2.weight: grad_norm = 0.022564
Total gradient norm: 0.083356
=== Actor Training Debug (Iteration 3345) ===
Q mean: -12.395713
Q std: 16.651770
Actor loss: 12.399690
Action reg: 0.003977
  l1.weight: grad_norm = 0.054556
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.040604
Total gradient norm: 0.151812
=== Actor Training Debug (Iteration 3346) ===
Q mean: -10.333139
Q std: 15.175739
Actor loss: 10.337119
Action reg: 0.003980
  l1.weight: grad_norm = 0.041911
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.031415
Total gradient norm: 0.110115
=== Actor Training Debug (Iteration 3347) ===
Q mean: -12.009470
Q std: 15.470029
Actor loss: 12.013448
Action reg: 0.003978
  l1.weight: grad_norm = 0.042590
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.030941
Total gradient norm: 0.138809
=== Actor Training Debug (Iteration 3348) ===
Q mean: -12.068905
Q std: 15.349481
Actor loss: 12.072885
Action reg: 0.003980
  l1.weight: grad_norm = 0.057641
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.042058
Total gradient norm: 0.121932
=== Actor Training Debug (Iteration 3349) ===
Q mean: -11.459436
Q std: 15.649096
Actor loss: 11.463425
Action reg: 0.003989
  l1.weight: grad_norm = 0.055282
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.041755
Total gradient norm: 0.161655
=== Actor Training Debug (Iteration 3350) ===
Q mean: -11.262931
Q std: 14.949281
Actor loss: 11.266908
Action reg: 0.003977
  l1.weight: grad_norm = 0.080105
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.051974
Total gradient norm: 0.194385
=== Actor Training Debug (Iteration 3351) ===
Q mean: -12.448215
Q std: 15.412577
Actor loss: 12.452206
Action reg: 0.003991
  l1.weight: grad_norm = 0.032928
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.027293
Total gradient norm: 0.127709
=== Actor Training Debug (Iteration 3352) ===
Q mean: -11.288872
Q std: 15.808639
Actor loss: 11.292855
Action reg: 0.003983
  l1.weight: grad_norm = 0.104347
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.079363
Total gradient norm: 0.282412
=== Actor Training Debug (Iteration 3353) ===
Q mean: -12.706075
Q std: 15.322973
Actor loss: 12.710063
Action reg: 0.003988
  l1.weight: grad_norm = 0.038268
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.034267
Total gradient norm: 0.142758
=== Actor Training Debug (Iteration 3354) ===
Q mean: -12.294287
Q std: 16.159529
Actor loss: 12.298243
Action reg: 0.003956
  l1.weight: grad_norm = 0.065334
  l1.bias: grad_norm = 0.000740
  l2.weight: grad_norm = 0.045887
Total gradient norm: 0.166437
=== Actor Training Debug (Iteration 3355) ===
Q mean: -12.893045
Q std: 15.577292
Actor loss: 12.897034
Action reg: 0.003989
  l1.weight: grad_norm = 0.022146
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.017093
Total gradient norm: 0.054439
=== Actor Training Debug (Iteration 3356) ===
Q mean: -12.183331
Q std: 15.361588
Actor loss: 12.187312
Action reg: 0.003982
  l1.weight: grad_norm = 0.054027
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.034167
Total gradient norm: 0.171740
=== Actor Training Debug (Iteration 3357) ===
Q mean: -12.422722
Q std: 15.531321
Actor loss: 12.426702
Action reg: 0.003980
  l1.weight: grad_norm = 0.044644
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.032841
Total gradient norm: 0.107458
=== Actor Training Debug (Iteration 3358) ===
Q mean: -12.013777
Q std: 16.033058
Actor loss: 12.017755
Action reg: 0.003978
  l1.weight: grad_norm = 0.046374
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.037880
Total gradient norm: 0.117861
=== Actor Training Debug (Iteration 3359) ===
Q mean: -12.564795
Q std: 16.067345
Actor loss: 12.568781
Action reg: 0.003987
  l1.weight: grad_norm = 0.049642
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.032023
Total gradient norm: 0.134620
=== Actor Training Debug (Iteration 3360) ===
Q mean: -9.490541
Q std: 14.158248
Actor loss: 9.494529
Action reg: 0.003989
  l1.weight: grad_norm = 0.054867
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.041213
Total gradient norm: 0.109700
=== Actor Training Debug (Iteration 3361) ===
Q mean: -12.836383
Q std: 16.166325
Actor loss: 12.840374
Action reg: 0.003991
  l1.weight: grad_norm = 0.021291
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.018491
Total gradient norm: 0.065881
=== Actor Training Debug (Iteration 3362) ===
Q mean: -10.578300
Q std: 14.730144
Actor loss: 10.582282
Action reg: 0.003983
  l1.weight: grad_norm = 0.111005
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.084146
Total gradient norm: 0.324586
=== Actor Training Debug (Iteration 3363) ===
Q mean: -13.357618
Q std: 16.807245
Actor loss: 13.361605
Action reg: 0.003986
  l1.weight: grad_norm = 0.087663
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.061458
Total gradient norm: 0.341669
=== Actor Training Debug (Iteration 3364) ===
Q mean: -12.707289
Q std: 15.999933
Actor loss: 12.711283
Action reg: 0.003994
  l1.weight: grad_norm = 0.018172
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.015491
Total gradient norm: 0.052040
=== Actor Training Debug (Iteration 3365) ===
Q mean: -10.466454
Q std: 15.177738
Actor loss: 10.470439
Action reg: 0.003985
  l1.weight: grad_norm = 0.041412
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.033632
Total gradient norm: 0.129783
=== Actor Training Debug (Iteration 3366) ===
Q mean: -11.985331
Q std: 14.192688
Actor loss: 11.989317
Action reg: 0.003986
  l1.weight: grad_norm = 0.036926
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.026574
Total gradient norm: 0.080694
=== Actor Training Debug (Iteration 3367) ===
Q mean: -11.442575
Q std: 15.018736
Actor loss: 11.446559
Action reg: 0.003983
  l1.weight: grad_norm = 0.061268
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.043159
Total gradient norm: 0.179020
=== Actor Training Debug (Iteration 3368) ===
Q mean: -12.412467
Q std: 16.250078
Actor loss: 12.416448
Action reg: 0.003981
  l1.weight: grad_norm = 0.055817
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.039542
Total gradient norm: 0.145886
=== Actor Training Debug (Iteration 3369) ===
Q mean: -12.589180
Q std: 15.853444
Actor loss: 12.593169
Action reg: 0.003990
  l1.weight: grad_norm = 0.030053
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.025618
Total gradient norm: 0.080763
=== Actor Training Debug (Iteration 3370) ===
Q mean: -13.080124
Q std: 16.275595
Actor loss: 13.084105
Action reg: 0.003982
  l1.weight: grad_norm = 0.065014
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.057906
Total gradient norm: 0.228402
=== Actor Training Debug (Iteration 3371) ===
Q mean: -12.873441
Q std: 15.818153
Actor loss: 12.877427
Action reg: 0.003986
  l1.weight: grad_norm = 0.054467
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.037944
Total gradient norm: 0.133850
=== Actor Training Debug (Iteration 3372) ===
Q mean: -12.074832
Q std: 15.115418
Actor loss: 12.078820
Action reg: 0.003988
  l1.weight: grad_norm = 0.088199
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.062460
Total gradient norm: 0.234608
=== Actor Training Debug (Iteration 3373) ===
Q mean: -11.661613
Q std: 14.651258
Actor loss: 11.665604
Action reg: 0.003990
  l1.weight: grad_norm = 0.073891
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.051755
Total gradient norm: 0.157584
=== Actor Training Debug (Iteration 3374) ===
Q mean: -13.845463
Q std: 16.394836
Actor loss: 13.849450
Action reg: 0.003988
  l1.weight: grad_norm = 0.054771
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.039580
Total gradient norm: 0.139588
=== Actor Training Debug (Iteration 3375) ===
Q mean: -12.997437
Q std: 16.621780
Actor loss: 13.001410
Action reg: 0.003973
  l1.weight: grad_norm = 0.070874
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.051376
Total gradient norm: 0.239884
=== Actor Training Debug (Iteration 3376) ===
Q mean: -11.740965
Q std: 15.735969
Actor loss: 11.744954
Action reg: 0.003990
  l1.weight: grad_norm = 0.043418
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.039895
Total gradient norm: 0.148014
=== Actor Training Debug (Iteration 3377) ===
Q mean: -10.875422
Q std: 14.757719
Actor loss: 10.879412
Action reg: 0.003990
  l1.weight: grad_norm = 0.158793
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.104547
Total gradient norm: 0.407759
=== Actor Training Debug (Iteration 3378) ===
Q mean: -11.484076
Q std: 15.300635
Actor loss: 11.488053
Action reg: 0.003978
  l1.weight: grad_norm = 0.057435
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.042786
Total gradient norm: 0.181836
=== Actor Training Debug (Iteration 3379) ===
Q mean: -11.642464
Q std: 15.468942
Actor loss: 11.646437
Action reg: 0.003973
  l1.weight: grad_norm = 0.089791
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.068337
Total gradient norm: 0.258683
=== Actor Training Debug (Iteration 3380) ===
Q mean: -9.466618
Q std: 14.884644
Actor loss: 9.470604
Action reg: 0.003986
  l1.weight: grad_norm = 0.039885
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.034353
Total gradient norm: 0.116670
=== Actor Training Debug (Iteration 3381) ===
Q mean: -10.904551
Q std: 15.089181
Actor loss: 10.908546
Action reg: 0.003996
  l1.weight: grad_norm = 0.011940
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.009349
Total gradient norm: 0.036275
=== Actor Training Debug (Iteration 3382) ===
Q mean: -14.704105
Q std: 16.638334
Actor loss: 14.708087
Action reg: 0.003982
  l1.weight: grad_norm = 0.056127
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.044958
Total gradient norm: 0.127656
=== Actor Training Debug (Iteration 3383) ===
Q mean: -11.106622
Q std: 15.237599
Actor loss: 11.110609
Action reg: 0.003988
  l1.weight: grad_norm = 0.030711
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.026662
Total gradient norm: 0.079854
=== Actor Training Debug (Iteration 3384) ===
Q mean: -11.806703
Q std: 14.393936
Actor loss: 11.810685
Action reg: 0.003982
  l1.weight: grad_norm = 0.096431
  l1.bias: grad_norm = 0.000911
  l2.weight: grad_norm = 0.073007
Total gradient norm: 0.243206
=== Actor Training Debug (Iteration 3385) ===
Q mean: -14.376411
Q std: 16.438940
Actor loss: 14.380393
Action reg: 0.003981
  l1.weight: grad_norm = 0.063630
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.052442
Total gradient norm: 0.215598
=== Actor Training Debug (Iteration 3386) ===
Q mean: -13.902494
Q std: 16.203201
Actor loss: 13.906479
Action reg: 0.003985
  l1.weight: grad_norm = 0.035236
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.028541
Total gradient norm: 0.128069
=== Actor Training Debug (Iteration 3387) ===
Q mean: -11.209398
Q std: 15.012734
Actor loss: 11.213376
Action reg: 0.003978
  l1.weight: grad_norm = 0.292152
  l1.bias: grad_norm = 0.000726
  l2.weight: grad_norm = 0.180726
Total gradient norm: 0.598782
=== Actor Training Debug (Iteration 3388) ===
Q mean: -11.742069
Q std: 16.494785
Actor loss: 11.746057
Action reg: 0.003988
  l1.weight: grad_norm = 0.026502
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.022616
Total gradient norm: 0.079545
=== Actor Training Debug (Iteration 3389) ===
Q mean: -12.013525
Q std: 15.872621
Actor loss: 12.017513
Action reg: 0.003988
  l1.weight: grad_norm = 0.041552
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.030595
Total gradient norm: 0.125010
=== Actor Training Debug (Iteration 3390) ===
Q mean: -13.109135
Q std: 15.834809
Actor loss: 13.113122
Action reg: 0.003988
  l1.weight: grad_norm = 0.063311
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.037252
Total gradient norm: 0.153550
=== Actor Training Debug (Iteration 3391) ===
Q mean: -11.345021
Q std: 14.989447
Actor loss: 11.348995
Action reg: 0.003974
  l1.weight: grad_norm = 0.182832
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.132933
Total gradient norm: 0.549569
=== Actor Training Debug (Iteration 3392) ===
Q mean: -12.756767
Q std: 15.234889
Actor loss: 12.760753
Action reg: 0.003985
  l1.weight: grad_norm = 0.091590
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.074415
Total gradient norm: 0.260562
=== Actor Training Debug (Iteration 3393) ===
Q mean: -12.208752
Q std: 15.739173
Actor loss: 12.212725
Action reg: 0.003973
  l1.weight: grad_norm = 0.109839
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.099931
Total gradient norm: 0.334350
=== Actor Training Debug (Iteration 3394) ===
Q mean: -11.311778
Q std: 14.861314
Actor loss: 11.315754
Action reg: 0.003975
  l1.weight: grad_norm = 0.142591
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.107746
Total gradient norm: 0.390452
=== Actor Training Debug (Iteration 3395) ===
Q mean: -12.184826
Q std: 15.934720
Actor loss: 12.188801
Action reg: 0.003975
  l1.weight: grad_norm = 0.026793
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.023244
Total gradient norm: 0.070277
=== Actor Training Debug (Iteration 3396) ===
Q mean: -12.284606
Q std: 16.226553
Actor loss: 12.288596
Action reg: 0.003990
  l1.weight: grad_norm = 0.035609
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.024594
Total gradient norm: 0.097183
=== Actor Training Debug (Iteration 3397) ===
Q mean: -12.208832
Q std: 15.268593
Actor loss: 12.212809
Action reg: 0.003977
  l1.weight: grad_norm = 0.110790
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.096995
Total gradient norm: 0.254870
=== Actor Training Debug (Iteration 3398) ===
Q mean: -11.703533
Q std: 15.239524
Actor loss: 11.707520
Action reg: 0.003986
  l1.weight: grad_norm = 0.055223
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.038616
Total gradient norm: 0.141239
=== Actor Training Debug (Iteration 3399) ===
Q mean: -12.915479
Q std: 15.775090
Actor loss: 12.919468
Action reg: 0.003989
  l1.weight: grad_norm = 0.055150
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.044794
Total gradient norm: 0.171784
=== Actor Training Debug (Iteration 3400) ===
Q mean: -15.230436
Q std: 16.537163
Actor loss: 15.234422
Action reg: 0.003985
  l1.weight: grad_norm = 0.124694
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.106372
Total gradient norm: 0.329395
=== Actor Training Debug (Iteration 3401) ===
Q mean: -12.118017
Q std: 16.036398
Actor loss: 12.121994
Action reg: 0.003977
  l1.weight: grad_norm = 0.119526
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.086112
Total gradient norm: 0.333295
=== Actor Training Debug (Iteration 3402) ===
Q mean: -12.169907
Q std: 15.541448
Actor loss: 12.173888
Action reg: 0.003981
  l1.weight: grad_norm = 0.047629
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.039217
Total gradient norm: 0.141766
=== Actor Training Debug (Iteration 3403) ===
Q mean: -13.384228
Q std: 16.495350
Actor loss: 13.388217
Action reg: 0.003989
  l1.weight: grad_norm = 0.089838
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.081270
Total gradient norm: 0.244637
=== Actor Training Debug (Iteration 3404) ===
Q mean: -12.768019
Q std: 15.427889
Actor loss: 12.771998
Action reg: 0.003979
  l1.weight: grad_norm = 0.060324
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.043199
Total gradient norm: 0.148852
=== Actor Training Debug (Iteration 3405) ===
Q mean: -11.848029
Q std: 15.702881
Actor loss: 11.852013
Action reg: 0.003983
  l1.weight: grad_norm = 0.025955
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.021746
Total gradient norm: 0.076031
=== Actor Training Debug (Iteration 3406) ===
Q mean: -11.672922
Q std: 15.356785
Actor loss: 11.676905
Action reg: 0.003982
  l1.weight: grad_norm = 0.051185
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.046553
Total gradient norm: 0.249270
=== Actor Training Debug (Iteration 3407) ===
Q mean: -11.749330
Q std: 15.588173
Actor loss: 11.753318
Action reg: 0.003989
  l1.weight: grad_norm = 0.019991
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.013573
Total gradient norm: 0.068562
=== Actor Training Debug (Iteration 3408) ===
Q mean: -13.270030
Q std: 16.574326
Actor loss: 13.274025
Action reg: 0.003995
  l1.weight: grad_norm = 0.017367
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.013383
Total gradient norm: 0.046292
=== Actor Training Debug (Iteration 3409) ===
Q mean: -15.001312
Q std: 16.368065
Actor loss: 15.005293
Action reg: 0.003981
  l1.weight: grad_norm = 0.034717
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.027084
Total gradient norm: 0.113412
=== Actor Training Debug (Iteration 3410) ===
Q mean: -12.383793
Q std: 15.642140
Actor loss: 12.387764
Action reg: 0.003971
  l1.weight: grad_norm = 0.047396
  l1.bias: grad_norm = 0.000791
  l2.weight: grad_norm = 0.033778
Total gradient norm: 0.132978
=== Actor Training Debug (Iteration 3411) ===
Q mean: -13.283034
Q std: 15.903031
Actor loss: 13.287003
Action reg: 0.003968
  l1.weight: grad_norm = 0.237707
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.157440
Total gradient norm: 0.616712
=== Actor Training Debug (Iteration 3412) ===
Q mean: -12.527649
Q std: 16.013533
Actor loss: 12.531633
Action reg: 0.003984
  l1.weight: grad_norm = 0.022167
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.015654
Total gradient norm: 0.058408
=== Actor Training Debug (Iteration 3413) ===
Q mean: -12.839293
Q std: 16.012814
Actor loss: 12.843278
Action reg: 0.003985
  l1.weight: grad_norm = 0.051750
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.045704
Total gradient norm: 0.164076
=== Actor Training Debug (Iteration 3414) ===
Q mean: -11.538404
Q std: 15.969316
Actor loss: 11.542370
Action reg: 0.003966
  l1.weight: grad_norm = 0.048823
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.037595
Total gradient norm: 0.116405
=== Actor Training Debug (Iteration 3415) ===
Q mean: -10.450785
Q std: 14.625551
Actor loss: 10.454771
Action reg: 0.003986
  l1.weight: grad_norm = 0.056180
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.040891
Total gradient norm: 0.154287
=== Actor Training Debug (Iteration 3416) ===
Q mean: -11.166032
Q std: 14.717760
Actor loss: 11.170008
Action reg: 0.003975
  l1.weight: grad_norm = 0.039947
  l1.bias: grad_norm = 0.000806
  l2.weight: grad_norm = 0.025902
Total gradient norm: 0.098819
=== Actor Training Debug (Iteration 3417) ===
Q mean: -13.238068
Q std: 16.701624
Actor loss: 13.242050
Action reg: 0.003982
  l1.weight: grad_norm = 0.063610
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.042093
Total gradient norm: 0.161881
=== Actor Training Debug (Iteration 3418) ===
Q mean: -14.150255
Q std: 16.668463
Actor loss: 14.154238
Action reg: 0.003983
  l1.weight: grad_norm = 0.085272
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.060622
Total gradient norm: 0.259197
=== Actor Training Debug (Iteration 3419) ===
Q mean: -12.751767
Q std: 15.945778
Actor loss: 12.755747
Action reg: 0.003980
  l1.weight: grad_norm = 0.061991
  l1.bias: grad_norm = 0.001104
  l2.weight: grad_norm = 0.053479
Total gradient norm: 0.238340
=== Actor Training Debug (Iteration 3420) ===
Q mean: -12.003728
Q std: 16.239370
Actor loss: 12.007711
Action reg: 0.003984
  l1.weight: grad_norm = 0.052323
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.040207
Total gradient norm: 0.151236
=== Actor Training Debug (Iteration 3421) ===
Q mean: -11.658205
Q std: 14.952403
Actor loss: 11.662177
Action reg: 0.003972
  l1.weight: grad_norm = 0.134636
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.109701
Total gradient norm: 0.376818
=== Actor Training Debug (Iteration 3422) ===
Q mean: -11.625672
Q std: 14.970251
Actor loss: 11.629647
Action reg: 0.003975
  l1.weight: grad_norm = 0.105202
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.087876
Total gradient norm: 0.285147
=== Actor Training Debug (Iteration 3423) ===
Q mean: -13.889908
Q std: 16.469681
Actor loss: 13.893891
Action reg: 0.003983
  l1.weight: grad_norm = 0.029147
  l1.bias: grad_norm = 0.000509
  l2.weight: grad_norm = 0.022674
Total gradient norm: 0.114771
=== Actor Training Debug (Iteration 3424) ===
Q mean: -12.364475
Q std: 15.451736
Actor loss: 12.368459
Action reg: 0.003984
  l1.weight: grad_norm = 0.053154
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.044044
Total gradient norm: 0.211198
=== Actor Training Debug (Iteration 3425) ===
Q mean: -12.846062
Q std: 15.805893
Actor loss: 12.850040
Action reg: 0.003978
  l1.weight: grad_norm = 0.019725
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 0.014887
Total gradient norm: 0.051492
=== Actor Training Debug (Iteration 3426) ===
Q mean: -14.297737
Q std: 17.134003
Actor loss: 14.301722
Action reg: 0.003985
  l1.weight: grad_norm = 0.011693
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.007524
Total gradient norm: 0.027051
=== Actor Training Debug (Iteration 3427) ===
Q mean: -11.466539
Q std: 15.757293
Actor loss: 11.470524
Action reg: 0.003985
  l1.weight: grad_norm = 0.048473
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.037679
Total gradient norm: 0.124348
=== Actor Training Debug (Iteration 3428) ===
Q mean: -11.817720
Q std: 15.975815
Actor loss: 11.821693
Action reg: 0.003973
  l1.weight: grad_norm = 0.076947
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.051347
Total gradient norm: 0.188574
=== Actor Training Debug (Iteration 3429) ===
Q mean: -12.660291
Q std: 16.019245
Actor loss: 12.664270
Action reg: 0.003980
  l1.weight: grad_norm = 0.089226
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.069543
Total gradient norm: 0.240055
=== Actor Training Debug (Iteration 3430) ===
Q mean: -11.148044
Q std: 15.243095
Actor loss: 11.152035
Action reg: 0.003991
  l1.weight: grad_norm = 0.034980
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.023517
Total gradient norm: 0.092642
=== Actor Training Debug (Iteration 3431) ===
Q mean: -12.027401
Q std: 16.333273
Actor loss: 12.031384
Action reg: 0.003984
  l1.weight: grad_norm = 0.081284
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.075854
Total gradient norm: 0.216713
=== Actor Training Debug (Iteration 3432) ===
Q mean: -12.512419
Q std: 15.578844
Actor loss: 12.516395
Action reg: 0.003976
  l1.weight: grad_norm = 0.067241
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.052815
Total gradient norm: 0.159120
=== Actor Training Debug (Iteration 3433) ===
Q mean: -12.084966
Q std: 15.598253
Actor loss: 12.088951
Action reg: 0.003986
  l1.weight: grad_norm = 0.047687
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.040161
Total gradient norm: 0.137901
=== Actor Training Debug (Iteration 3434) ===
Q mean: -11.782187
Q std: 15.447391
Actor loss: 11.786175
Action reg: 0.003988
  l1.weight: grad_norm = 0.278120
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.225792
Total gradient norm: 0.788351
=== Actor Training Debug (Iteration 3435) ===
Q mean: -11.811359
Q std: 16.037302
Actor loss: 11.815347
Action reg: 0.003987
  l1.weight: grad_norm = 0.089209
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.067344
Total gradient norm: 0.283398
=== Actor Training Debug (Iteration 3436) ===
Q mean: -10.528586
Q std: 15.447998
Actor loss: 10.532564
Action reg: 0.003978
  l1.weight: grad_norm = 0.046116
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.035178
Total gradient norm: 0.159946
=== Actor Training Debug (Iteration 3437) ===
Q mean: -12.830601
Q std: 16.228638
Actor loss: 12.834575
Action reg: 0.003974
  l1.weight: grad_norm = 0.102576
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.082628
Total gradient norm: 0.283470
=== Actor Training Debug (Iteration 3438) ===
Q mean: -13.963339
Q std: 16.263567
Actor loss: 13.967325
Action reg: 0.003986
  l1.weight: grad_norm = 0.107281
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.075063
Total gradient norm: 0.253935
=== Actor Training Debug (Iteration 3439) ===
Q mean: -12.884604
Q std: 16.290943
Actor loss: 12.888588
Action reg: 0.003984
  l1.weight: grad_norm = 0.018052
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.014900
Total gradient norm: 0.053978
=== Actor Training Debug (Iteration 3440) ===
Q mean: -11.424353
Q std: 15.588981
Actor loss: 11.428327
Action reg: 0.003974
  l1.weight: grad_norm = 0.210882
  l1.bias: grad_norm = 0.000736
  l2.weight: grad_norm = 0.139249
Total gradient norm: 0.523327
=== Actor Training Debug (Iteration 3441) ===
Q mean: -12.851516
Q std: 16.404629
Actor loss: 12.855510
Action reg: 0.003994
  l1.weight: grad_norm = 0.015390
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.012251
Total gradient norm: 0.042996
=== Actor Training Debug (Iteration 3442) ===
Q mean: -11.438869
Q std: 14.691980
Actor loss: 11.442854
Action reg: 0.003985
  l1.weight: grad_norm = 0.082823
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.062912
Total gradient norm: 0.213051
=== Actor Training Debug (Iteration 3443) ===
Q mean: -13.333114
Q std: 16.249718
Actor loss: 13.337096
Action reg: 0.003983
  l1.weight: grad_norm = 0.043043
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.035094
Total gradient norm: 0.130577
=== Actor Training Debug (Iteration 3444) ===
Q mean: -10.442676
Q std: 15.771235
Actor loss: 10.446664
Action reg: 0.003988
  l1.weight: grad_norm = 0.031713
  l1.bias: grad_norm = 0.000639
  l2.weight: grad_norm = 0.028011
Total gradient norm: 0.097092
=== Actor Training Debug (Iteration 3445) ===
Q mean: -12.115499
Q std: 15.844618
Actor loss: 12.119488
Action reg: 0.003988
  l1.weight: grad_norm = 0.011260
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.009290
Total gradient norm: 0.044765
=== Actor Training Debug (Iteration 3446) ===
Q mean: -11.464223
Q std: 15.172943
Actor loss: 11.468218
Action reg: 0.003995
  l1.weight: grad_norm = 0.063809
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.044713
Total gradient norm: 0.145255
=== Actor Training Debug (Iteration 3447) ===
Q mean: -12.572552
Q std: 15.932023
Actor loss: 12.576516
Action reg: 0.003964
  l1.weight: grad_norm = 0.131883
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.094243
Total gradient norm: 0.297289
=== Actor Training Debug (Iteration 3448) ===
Q mean: -11.363793
Q std: 15.086365
Actor loss: 11.367781
Action reg: 0.003987
  l1.weight: grad_norm = 0.054597
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.046089
Total gradient norm: 0.190793
=== Actor Training Debug (Iteration 3449) ===
Q mean: -13.078388
Q std: 15.826033
Actor loss: 13.082365
Action reg: 0.003977
  l1.weight: grad_norm = 0.055876
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.043645
Total gradient norm: 0.158069
=== Actor Training Debug (Iteration 3450) ===
Q mean: -14.319596
Q std: 16.623997
Actor loss: 14.323578
Action reg: 0.003981
  l1.weight: grad_norm = 0.029802
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.022932
Total gradient norm: 0.083118
=== Actor Training Debug (Iteration 3451) ===
Q mean: -12.767105
Q std: 15.925403
Actor loss: 12.771086
Action reg: 0.003981
  l1.weight: grad_norm = 0.058854
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.042434
Total gradient norm: 0.159014
=== Actor Training Debug (Iteration 3452) ===
Q mean: -14.955462
Q std: 16.810041
Actor loss: 14.959451
Action reg: 0.003988
  l1.weight: grad_norm = 0.025969
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.019473
Total gradient norm: 0.065594
=== Actor Training Debug (Iteration 3453) ===
Q mean: -13.408323
Q std: 15.402848
Actor loss: 13.412313
Action reg: 0.003990
  l1.weight: grad_norm = 0.122038
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.100287
Total gradient norm: 0.379359
=== Actor Training Debug (Iteration 3454) ===
Q mean: -11.834553
Q std: 15.295732
Actor loss: 11.838532
Action reg: 0.003980
  l1.weight: grad_norm = 0.046037
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.034839
Total gradient norm: 0.128768
=== Actor Training Debug (Iteration 3455) ===
Q mean: -11.086442
Q std: 15.150880
Actor loss: 11.090428
Action reg: 0.003987
  l1.weight: grad_norm = 0.080932
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.058718
Total gradient norm: 0.161047
=== Actor Training Debug (Iteration 3456) ===
Q mean: -11.994552
Q std: 15.514724
Actor loss: 11.998535
Action reg: 0.003984
  l1.weight: grad_norm = 0.095571
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.073054
Total gradient norm: 0.239094
=== Actor Training Debug (Iteration 3457) ===
Q mean: -13.099442
Q std: 16.336075
Actor loss: 13.103429
Action reg: 0.003988
  l1.weight: grad_norm = 0.038997
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.035677
Total gradient norm: 0.127911
=== Actor Training Debug (Iteration 3458) ===
Q mean: -11.437077
Q std: 15.955482
Actor loss: 11.441054
Action reg: 0.003977
  l1.weight: grad_norm = 0.059299
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.046906
Total gradient norm: 0.181270
=== Actor Training Debug (Iteration 3459) ===
Q mean: -11.492143
Q std: 16.701857
Actor loss: 11.496107
Action reg: 0.003965
  l1.weight: grad_norm = 0.132953
  l1.bias: grad_norm = 0.000783
  l2.weight: grad_norm = 0.098218
Total gradient norm: 0.386943
=== Actor Training Debug (Iteration 3460) ===
Q mean: -13.759069
Q std: 16.788139
Actor loss: 13.763044
Action reg: 0.003975
  l1.weight: grad_norm = 0.221153
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.168496
Total gradient norm: 0.580728
=== Actor Training Debug (Iteration 3461) ===
Q mean: -11.941687
Q std: 15.217611
Actor loss: 11.945674
Action reg: 0.003988
  l1.weight: grad_norm = 0.039088
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.030373
Total gradient norm: 0.101301
=== Actor Training Debug (Iteration 3462) ===
Q mean: -11.816923
Q std: 14.775284
Actor loss: 11.820909
Action reg: 0.003985
  l1.weight: grad_norm = 0.172578
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.133137
Total gradient norm: 0.423789
=== Actor Training Debug (Iteration 3463) ===
Q mean: -12.817180
Q std: 15.694985
Actor loss: 12.821169
Action reg: 0.003989
  l1.weight: grad_norm = 0.091211
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.077457
Total gradient norm: 0.255334
=== Actor Training Debug (Iteration 3464) ===
Q mean: -12.570904
Q std: 15.850434
Actor loss: 12.574897
Action reg: 0.003993
  l1.weight: grad_norm = 0.026305
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.018444
Total gradient norm: 0.061381
=== Actor Training Debug (Iteration 3465) ===
Q mean: -11.051408
Q std: 15.541282
Actor loss: 11.055397
Action reg: 0.003989
  l1.weight: grad_norm = 0.026013
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.018675
Total gradient norm: 0.063671
=== Actor Training Debug (Iteration 3466) ===
Q mean: -13.272200
Q std: 16.894203
Actor loss: 13.276181
Action reg: 0.003982
  l1.weight: grad_norm = 0.074773
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.056488
Total gradient norm: 0.202672
=== Actor Training Debug (Iteration 3467) ===
Q mean: -13.672441
Q std: 15.812190
Actor loss: 13.676417
Action reg: 0.003976
  l1.weight: grad_norm = 0.055264
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.038922
Total gradient norm: 0.123519
=== Actor Training Debug (Iteration 3468) ===
Q mean: -13.133111
Q std: 16.917955
Actor loss: 13.137097
Action reg: 0.003986
  l1.weight: grad_norm = 0.054715
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.046770
Total gradient norm: 0.168060
=== Actor Training Debug (Iteration 3469) ===
Q mean: -12.646894
Q std: 16.295286
Actor loss: 12.650873
Action reg: 0.003979
  l1.weight: grad_norm = 0.046196
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.030000
Total gradient norm: 0.125130
=== Actor Training Debug (Iteration 3470) ===
Q mean: -12.160126
Q std: 15.873776
Actor loss: 12.164108
Action reg: 0.003982
  l1.weight: grad_norm = 0.153026
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.101517
Total gradient norm: 0.386267
=== Actor Training Debug (Iteration 3471) ===
Q mean: -13.343506
Q std: 15.393254
Actor loss: 13.347484
Action reg: 0.003978
  l1.weight: grad_norm = 0.099985
  l1.bias: grad_norm = 0.000611
  l2.weight: grad_norm = 0.068365
Total gradient norm: 0.261070
=== Actor Training Debug (Iteration 3472) ===
Q mean: -13.409271
Q std: 17.024715
Actor loss: 13.413262
Action reg: 0.003991
  l1.weight: grad_norm = 0.061577
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.047783
Total gradient norm: 0.171037
=== Actor Training Debug (Iteration 3473) ===
Q mean: -10.546518
Q std: 14.776085
Actor loss: 10.550496
Action reg: 0.003977
  l1.weight: grad_norm = 0.141951
  l1.bias: grad_norm = 0.000796
  l2.weight: grad_norm = 0.108100
Total gradient norm: 0.323358
=== Actor Training Debug (Iteration 3474) ===
Q mean: -13.827881
Q std: 16.330872
Actor loss: 13.831866
Action reg: 0.003985
  l1.weight: grad_norm = 0.071022
  l1.bias: grad_norm = 0.001016
  l2.weight: grad_norm = 0.048014
Total gradient norm: 0.156145
=== Actor Training Debug (Iteration 3475) ===
Q mean: -11.660389
Q std: 15.644307
Actor loss: 11.664378
Action reg: 0.003989
  l1.weight: grad_norm = 0.142622
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.098879
Total gradient norm: 0.335375
=== Actor Training Debug (Iteration 3476) ===
Q mean: -11.591280
Q std: 15.628352
Actor loss: 11.595266
Action reg: 0.003987
  l1.weight: grad_norm = 0.028885
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.023460
Total gradient norm: 0.075578
=== Actor Training Debug (Iteration 3477) ===
Q mean: -12.023619
Q std: 15.833080
Actor loss: 12.027604
Action reg: 0.003985
  l1.weight: grad_norm = 0.072722
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.052030
Total gradient norm: 0.182191
=== Actor Training Debug (Iteration 3478) ===
Q mean: -11.016054
Q std: 14.954456
Actor loss: 11.020046
Action reg: 0.003992
  l1.weight: grad_norm = 0.039407
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.035646
Total gradient norm: 0.106480
=== Actor Training Debug (Iteration 3479) ===
Q mean: -13.029798
Q std: 16.490534
Actor loss: 13.033786
Action reg: 0.003988
  l1.weight: grad_norm = 0.022384
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.020336
Total gradient norm: 0.073487
=== Actor Training Debug (Iteration 3480) ===
Q mean: -12.158863
Q std: 15.575003
Actor loss: 12.162840
Action reg: 0.003977
  l1.weight: grad_norm = 0.031028
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.026070
Total gradient norm: 0.092888
=== Actor Training Debug (Iteration 3481) ===
Q mean: -12.612871
Q std: 15.900763
Actor loss: 12.616846
Action reg: 0.003975
  l1.weight: grad_norm = 0.075304
  l1.bias: grad_norm = 0.000557
  l2.weight: grad_norm = 0.055357
Total gradient norm: 0.262833
=== Actor Training Debug (Iteration 3482) ===
Q mean: -11.832651
Q std: 16.194448
Actor loss: 11.836636
Action reg: 0.003984
  l1.weight: grad_norm = 0.155865
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.094111
Total gradient norm: 0.342216
=== Actor Training Debug (Iteration 3483) ===
Q mean: -11.202386
Q std: 16.067425
Actor loss: 11.206373
Action reg: 0.003987
  l1.weight: grad_norm = 0.047997
  l1.bias: grad_norm = 0.000679
  l2.weight: grad_norm = 0.041738
Total gradient norm: 0.150368
=== Actor Training Debug (Iteration 3484) ===
Q mean: -10.030209
Q std: 14.565018
Actor loss: 10.034195
Action reg: 0.003986
  l1.weight: grad_norm = 0.074008
  l1.bias: grad_norm = 0.000648
  l2.weight: grad_norm = 0.048966
Total gradient norm: 0.150828
=== Actor Training Debug (Iteration 3485) ===
Q mean: -14.368635
Q std: 16.786879
Actor loss: 14.372620
Action reg: 0.003984
  l1.weight: grad_norm = 0.125022
  l1.bias: grad_norm = 0.000932
  l2.weight: grad_norm = 0.075754
Total gradient norm: 0.266587
=== Actor Training Debug (Iteration 3486) ===
Q mean: -13.726332
Q std: 15.741823
Actor loss: 13.730308
Action reg: 0.003975
  l1.weight: grad_norm = 0.064334
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.043953
Total gradient norm: 0.183366
=== Actor Training Debug (Iteration 3487) ===
Q mean: -12.857973
Q std: 16.051710
Actor loss: 12.861953
Action reg: 0.003980
  l1.weight: grad_norm = 0.121095
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.083661
Total gradient norm: 0.335378
=== Actor Training Debug (Iteration 3488) ===
Q mean: -11.797535
Q std: 16.269686
Actor loss: 11.801525
Action reg: 0.003990
  l1.weight: grad_norm = 0.030940
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.026343
Total gradient norm: 0.103555
=== Actor Training Debug (Iteration 3489) ===
Q mean: -10.843912
Q std: 15.027622
Actor loss: 10.847894
Action reg: 0.003982
  l1.weight: grad_norm = 0.041101
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.029499
Total gradient norm: 0.113997
=== Actor Training Debug (Iteration 3490) ===
Q mean: -13.336758
Q std: 16.560368
Actor loss: 13.340743
Action reg: 0.003986
  l1.weight: grad_norm = 0.085409
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.051083
Total gradient norm: 0.178931
=== Actor Training Debug (Iteration 3491) ===
Q mean: -13.003258
Q std: 16.518375
Actor loss: 13.007244
Action reg: 0.003986
  l1.weight: grad_norm = 0.022829
  l1.bias: grad_norm = 0.001474
  l2.weight: grad_norm = 0.017511
Total gradient norm: 0.072580
=== Actor Training Debug (Iteration 3492) ===
Q mean: -9.052708
Q std: 14.081477
Actor loss: 9.056683
Action reg: 0.003975
  l1.weight: grad_norm = 0.277974
  l1.bias: grad_norm = 0.000650
  l2.weight: grad_norm = 0.163760
Total gradient norm: 0.534998
=== Actor Training Debug (Iteration 3493) ===
Q mean: -12.577358
Q std: 17.357285
Actor loss: 12.581347
Action reg: 0.003989
  l1.weight: grad_norm = 0.023168
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.020628
Total gradient norm: 0.090222
=== Actor Training Debug (Iteration 3494) ===
Q mean: -13.323167
Q std: 16.259611
Actor loss: 13.327149
Action reg: 0.003982
  l1.weight: grad_norm = 0.070184
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.048338
Total gradient norm: 0.194605
=== Actor Training Debug (Iteration 3495) ===
Q mean: -13.920800
Q std: 16.609190
Actor loss: 13.924784
Action reg: 0.003984
  l1.weight: grad_norm = 0.038674
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.029469
Total gradient norm: 0.090979
=== Actor Training Debug (Iteration 3496) ===
Q mean: -11.776161
Q std: 16.026756
Actor loss: 11.780146
Action reg: 0.003985
  l1.weight: grad_norm = 0.058458
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.042315
Total gradient norm: 0.136506
=== Actor Training Debug (Iteration 3497) ===
Q mean: -12.630132
Q std: 16.007027
Actor loss: 12.634114
Action reg: 0.003982
  l1.weight: grad_norm = 0.092095
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.065530
Total gradient norm: 0.285441
=== Actor Training Debug (Iteration 3498) ===
Q mean: -10.273847
Q std: 15.455526
Actor loss: 10.277817
Action reg: 0.003970
  l1.weight: grad_norm = 0.152428
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.112754
Total gradient norm: 0.511566
=== Actor Training Debug (Iteration 3499) ===
Q mean: -11.569860
Q std: 15.765590
Actor loss: 11.573847
Action reg: 0.003987
  l1.weight: grad_norm = 0.051127
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.041347
Total gradient norm: 0.150300
=== Actor Training Debug (Iteration 3500) ===
Q mean: -12.814780
Q std: 15.949651
Actor loss: 12.818767
Action reg: 0.003987
  l1.weight: grad_norm = 0.081594
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.046855
Total gradient norm: 0.177178
  Average reward: -313.749 | Average length: 100.0
Evaluation at episode 85: -313.749
=== Actor Training Debug (Iteration 3501) ===
Q mean: -12.708261
Q std: 16.607325
Actor loss: 12.712245
Action reg: 0.003984
  l1.weight: grad_norm = 0.025085
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.018704
Total gradient norm: 0.075940
=== Actor Training Debug (Iteration 3502) ===
Q mean: -11.370928
Q std: 14.464679
Actor loss: 11.374916
Action reg: 0.003989
  l1.weight: grad_norm = 0.078367
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.066855
Total gradient norm: 0.192915
=== Actor Training Debug (Iteration 3503) ===
Q mean: -13.894265
Q std: 17.375900
Actor loss: 13.898251
Action reg: 0.003985
  l1.weight: grad_norm = 0.129838
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.083092
Total gradient norm: 0.297586
=== Actor Training Debug (Iteration 3504) ===
Q mean: -11.687988
Q std: 15.868997
Actor loss: 11.691971
Action reg: 0.003982
  l1.weight: grad_norm = 0.029851
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.021696
Total gradient norm: 0.074966
=== Actor Training Debug (Iteration 3505) ===
Q mean: -11.590732
Q std: 15.918142
Actor loss: 11.594725
Action reg: 0.003993
  l1.weight: grad_norm = 0.015389
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.010589
Total gradient norm: 0.040387
=== Actor Training Debug (Iteration 3506) ===
Q mean: -14.033443
Q std: 16.573204
Actor loss: 14.037432
Action reg: 0.003988
  l1.weight: grad_norm = 0.037808
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.026856
Total gradient norm: 0.113204
=== Actor Training Debug (Iteration 3507) ===
Q mean: -12.320824
Q std: 16.013233
Actor loss: 12.324817
Action reg: 0.003993
  l1.weight: grad_norm = 0.060488
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.051813
Total gradient norm: 0.212481
=== Actor Training Debug (Iteration 3508) ===
Q mean: -13.006394
Q std: 16.664061
Actor loss: 13.010383
Action reg: 0.003988
  l1.weight: grad_norm = 0.011676
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.009813
Total gradient norm: 0.030702
=== Actor Training Debug (Iteration 3509) ===
Q mean: -12.291758
Q std: 15.707058
Actor loss: 12.295753
Action reg: 0.003995
  l1.weight: grad_norm = 0.170873
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.151615
Total gradient norm: 0.687240
=== Actor Training Debug (Iteration 3510) ===
Q mean: -12.803928
Q std: 15.903944
Actor loss: 12.807924
Action reg: 0.003996
  l1.weight: grad_norm = 0.026408
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.022141
Total gradient norm: 0.088950
=== Actor Training Debug (Iteration 3511) ===
Q mean: -12.980810
Q std: 16.743349
Actor loss: 12.984787
Action reg: 0.003977
  l1.weight: grad_norm = 0.047820
  l1.bias: grad_norm = 0.000827
  l2.weight: grad_norm = 0.038844
Total gradient norm: 0.113388
=== Actor Training Debug (Iteration 3512) ===
Q mean: -12.836155
Q std: 16.330284
Actor loss: 12.840147
Action reg: 0.003992
  l1.weight: grad_norm = 0.014911
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.009917
Total gradient norm: 0.050279
=== Actor Training Debug (Iteration 3545) ===
Q mean: -10.939150
Q std: 15.568217
Actor loss: 10.943134
Action reg: 0.003985
  l1.weight: grad_norm = 0.136990
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.102594
Total gradient norm: 0.318227
=== Actor Training Debug (Iteration 3546) ===
Q mean: -12.572677
Q std: 15.823550
Actor loss: 12.576664
Action reg: 0.003988
  l1.weight: grad_norm = 0.009746
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.007428
Total gradient norm: 0.028289
=== Actor Training Debug (Iteration 3547) ===
Q mean: -11.610091
Q std: 16.118090
Actor loss: 11.614079
Action reg: 0.003988
  l1.weight: grad_norm = 0.136674
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.113458
Total gradient norm: 0.340793
=== Actor Training Debug (Iteration 3548) ===
Q mean: -9.720744
Q std: 14.941647
Actor loss: 9.724728
Action reg: 0.003984
  l1.weight: grad_norm = 0.091093
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.060788
Total gradient norm: 0.221030
=== Actor Training Debug (Iteration 3549) ===
Q mean: -12.200899
Q std: 16.431997
Actor loss: 12.204863
Action reg: 0.003963
  l1.weight: grad_norm = 0.132510
  l1.bias: grad_norm = 0.001053
  l2.weight: grad_norm = 0.096507
Total gradient norm: 0.358357
=== Actor Training Debug (Iteration 3550) ===
Q mean: -12.168393
Q std: 15.407645
Actor loss: 12.172378
Action reg: 0.003984
  l1.weight: grad_norm = 0.223384
  l1.bias: grad_norm = 0.000842
  l2.weight: grad_norm = 0.170248
Total gradient norm: 0.622101
=== Actor Training Debug (Iteration 3551) ===
Q mean: -13.540586
Q std: 16.047672
Actor loss: 13.544573
Action reg: 0.003986
  l1.weight: grad_norm = 0.039070
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.026665
Total gradient norm: 0.102820
=== Actor Training Debug (Iteration 3552) ===
Q mean: -10.624189
Q std: 15.730643
Actor loss: 10.628174
Action reg: 0.003985
  l1.weight: grad_norm = 0.060255
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.043080
Total gradient norm: 0.150213
=== Actor Training Debug (Iteration 3553) ===
Q mean: -11.881470
Q std: 15.158760
Actor loss: 11.885448
Action reg: 0.003978
  l1.weight: grad_norm = 0.117880
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.096755
Total gradient norm: 0.408166
=== Actor Training Debug (Iteration 3554) ===
Q mean: -11.870155
Q std: 16.178055
Actor loss: 11.874139
Action reg: 0.003983
  l1.weight: grad_norm = 0.078164
  l1.bias: grad_norm = 0.000553
  l2.weight: grad_norm = 0.056982
Total gradient norm: 0.179051
=== Actor Training Debug (Iteration 3555) ===
Q mean: -12.592573
Q std: 15.892668
Actor loss: 12.596555
Action reg: 0.003981
  l1.weight: grad_norm = 0.097383
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.079668
Total gradient norm: 0.255512
=== Actor Training Debug (Iteration 3556) ===
Q mean: -10.088037
Q std: 14.696541
Actor loss: 10.092024
Action reg: 0.003987
  l1.weight: grad_norm = 0.052339
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.035561
Total gradient norm: 0.143985
=== Actor Training Debug (Iteration 3557) ===
Q mean: -13.164964
Q std: 16.422813
Actor loss: 13.168947
Action reg: 0.003984
  l1.weight: grad_norm = 0.086826
  l1.bias: grad_norm = 0.000961
  l2.weight: grad_norm = 0.074348
Total gradient norm: 0.241556
=== Actor Training Debug (Iteration 3558) ===
Q mean: -12.120922
Q std: 16.468573
Actor loss: 12.124902
Action reg: 0.003979
  l1.weight: grad_norm = 0.067127
  l1.bias: grad_norm = 0.000478
  l2.weight: grad_norm = 0.051351
Total gradient norm: 0.248116
=== Actor Training Debug (Iteration 3559) ===
Q mean: -13.721775
Q std: 16.569666
Actor loss: 13.725758
Action reg: 0.003982
  l1.weight: grad_norm = 0.074106
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.054790
Total gradient norm: 0.165490
=== Actor Training Debug (Iteration 3560) ===
Q mean: -12.085941
Q std: 16.943964
Actor loss: 12.089926
Action reg: 0.003984
  l1.weight: grad_norm = 0.116116
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.095893
Total gradient norm: 0.347476
=== Actor Training Debug (Iteration 3561) ===
Q mean: -11.424402
Q std: 15.378280
Actor loss: 11.428382
Action reg: 0.003980
  l1.weight: grad_norm = 0.100436
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.077755
Total gradient norm: 0.315367
=== Actor Training Debug (Iteration 3562) ===
Q mean: -12.583111
Q std: 15.985651
Actor loss: 12.587097
Action reg: 0.003987
  l1.weight: grad_norm = 0.094021
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.079277
Total gradient norm: 0.352078
=== Actor Training Debug (Iteration 3563) ===
Q mean: -12.144196
Q std: 16.770376
Actor loss: 12.148179
Action reg: 0.003984
  l1.weight: grad_norm = 0.065024
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.039599
Total gradient norm: 0.119090
=== Actor Training Debug (Iteration 3564) ===
Q mean: -13.291386
Q std: 16.806862
Actor loss: 13.295360
Action reg: 0.003974
  l1.weight: grad_norm = 0.063385
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.049587
Total gradient norm: 0.196178
=== Actor Training Debug (Iteration 3565) ===
Q mean: -13.210978
Q std: 15.451861
Actor loss: 13.214954
Action reg: 0.003977
  l1.weight: grad_norm = 0.106293
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.082976
Total gradient norm: 0.302280
=== Actor Training Debug (Iteration 3566) ===
Q mean: -14.249793
Q std: 16.139599
Actor loss: 14.253773
Action reg: 0.003980
  l1.weight: grad_norm = 0.164001
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.107350
Total gradient norm: 0.381331
=== Actor Training Debug (Iteration 3567) ===
Q mean: -11.961289
Q std: 15.918379
Actor loss: 11.965281
Action reg: 0.003991
  l1.weight: grad_norm = 0.199097
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.162644
Total gradient norm: 0.824936
=== Actor Training Debug (Iteration 3568) ===
Q mean: -11.109909
Q std: 15.646824
Actor loss: 11.113902
Action reg: 0.003993
  l1.weight: grad_norm = 0.052629
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.040381
Total gradient norm: 0.167504
=== Actor Training Debug (Iteration 3569) ===
Q mean: -12.972849
Q std: 16.422621
Actor loss: 12.976837
Action reg: 0.003988
  l1.weight: grad_norm = 0.080636
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.059154
Total gradient norm: 0.223298
=== Actor Training Debug (Iteration 3570) ===
Q mean: -12.946781
Q std: 16.245176
Actor loss: 12.950776
Action reg: 0.003995
  l1.weight: grad_norm = 0.058659
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.039487
Total gradient norm: 0.130774
=== Actor Training Debug (Iteration 3571) ===
Q mean: -11.996843
Q std: 15.907746
Actor loss: 12.000823
Action reg: 0.003979
  l1.weight: grad_norm = 0.104551
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.098532
Total gradient norm: 0.402774
=== Actor Training Debug (Iteration 3572) ===
Q mean: -12.961267
Q std: 16.350060
Actor loss: 12.965259
Action reg: 0.003992
  l1.weight: grad_norm = 0.028831
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.023010
Total gradient norm: 0.074550
=== Actor Training Debug (Iteration 3573) ===
Q mean: -11.243528
Q std: 14.988196
Actor loss: 11.247519
Action reg: 0.003991
  l1.weight: grad_norm = 0.052262
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.041178
Total gradient norm: 0.173404
=== Actor Training Debug (Iteration 3574) ===
Q mean: -12.342537
Q std: 16.114758
Actor loss: 12.346526
Action reg: 0.003989
  l1.weight: grad_norm = 0.035796
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.029800
Total gradient norm: 0.112589
=== Actor Training Debug (Iteration 3575) ===
Q mean: -11.876486
Q std: 15.263951
Actor loss: 11.880472
Action reg: 0.003986
  l1.weight: grad_norm = 0.020458
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.016514
Total gradient norm: 0.057119
=== Actor Training Debug (Iteration 3576) ===
Q mean: -12.918246
Q std: 16.515509
Actor loss: 12.922235
Action reg: 0.003989
  l1.weight: grad_norm = 0.029745
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.019025
Total gradient norm: 0.067907
=== Actor Training Debug (Iteration 3577) ===
Q mean: -11.829489
Q std: 15.881476
Actor loss: 11.833479
Action reg: 0.003990
  l1.weight: grad_norm = 0.029923
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.022364
Total gradient norm: 0.075626
=== Actor Training Debug (Iteration 3578) ===
Q mean: -11.021287
Q std: 15.682006
Actor loss: 11.025274
Action reg: 0.003987
  l1.weight: grad_norm = 0.052938
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.035931
Total gradient norm: 0.140256
=== Actor Training Debug (Iteration 3579) ===
Q mean: -10.328581
Q std: 15.145297
Actor loss: 10.332567
Action reg: 0.003986
  l1.weight: grad_norm = 0.067444
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.049711
Total gradient norm: 0.180211
=== Actor Training Debug (Iteration 3580) ===
Q mean: -12.063297
Q std: 16.041996
Actor loss: 12.067273
Action reg: 0.003976
  l1.weight: grad_norm = 0.051160
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.033901
Total gradient norm: 0.131338
=== Actor Training Debug (Iteration 3581) ===
Q mean: -12.516176
Q std: 16.327469
Actor loss: 12.520160
Action reg: 0.003984
  l1.weight: grad_norm = 0.094742
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.067258
Total gradient norm: 0.286348
=== Actor Training Debug (Iteration 3582) ===
Q mean: -12.659405
Q std: 16.535528
Actor loss: 12.663391
Action reg: 0.003986
  l1.weight: grad_norm = 0.056529
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.038555
Total gradient norm: 0.176398
=== Actor Training Debug (Iteration 3583) ===
Q mean: -11.663212
Q std: 16.264921
Actor loss: 11.667205
Action reg: 0.003993
  l1.weight: grad_norm = 0.062216
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.060935
Total gradient norm: 0.164985
=== Actor Training Debug (Iteration 3584) ===
Q mean: -12.729181
Q std: 15.811791
Actor loss: 12.733169
Action reg: 0.003987
  l1.weight: grad_norm = 0.026257
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.018892
Total gradient norm: 0.065628
=== Actor Training Debug (Iteration 3585) ===
Q mean: -13.569080
Q std: 16.956976
Actor loss: 13.573061
Action reg: 0.003980
  l1.weight: grad_norm = 0.084978
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.079678
Total gradient norm: 0.259301
=== Actor Training Debug (Iteration 3586) ===
Q mean: -12.963915
Q std: 16.465216
Actor loss: 12.967902
Action reg: 0.003987
  l1.weight: grad_norm = 0.043113
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.035771
Total gradient norm: 0.153886
=== Actor Training Debug (Iteration 3587) ===
Q mean: -13.961906
Q std: 17.082075
Actor loss: 13.965899
Action reg: 0.003993
  l1.weight: grad_norm = 0.007282
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.004109
Total gradient norm: 0.014880
=== Actor Training Debug (Iteration 3588) ===
Q mean: -11.075589
Q std: 15.457435
Actor loss: 11.079576
Action reg: 0.003986
  l1.weight: grad_norm = 0.043880
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.039163
Total gradient norm: 0.117176
=== Actor Training Debug (Iteration 3589) ===
Q mean: -12.113229
Q std: 16.087936
Actor loss: 12.117212
Action reg: 0.003984
  l1.weight: grad_norm = 0.060864
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.043684
Total gradient norm: 0.144177
=== Actor Training Debug (Iteration 3590) ===
Q mean: -13.257362
Q std: 16.564789
Actor loss: 13.261345
Action reg: 0.003983
  l1.weight: grad_norm = 0.080650
  l1.bias: grad_norm = 0.000807
  l2.weight: grad_norm = 0.070395
Total gradient norm: 0.344290
=== Actor Training Debug (Iteration 3591) ===
Q mean: -11.879871
Q std: 16.138763
Actor loss: 11.883852
Action reg: 0.003981
  l1.weight: grad_norm = 0.067145
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.062307
Total gradient norm: 0.184705
=== Actor Training Debug (Iteration 3592) ===
Q mean: -14.436491
Q std: 17.347338
Actor loss: 14.440476
Action reg: 0.003985
  l1.weight: grad_norm = 0.115399
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.079204
Total gradient norm: 0.248614
=== Actor Training Debug (Iteration 3593) ===
Q mean: -11.714508
Q std: 15.924126
Actor loss: 11.718501
Action reg: 0.003993
  l1.weight: grad_norm = 0.007038
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.006306
Total gradient norm: 0.022134
=== Actor Training Debug (Iteration 3594) ===
Q mean: -14.241074
Q std: 16.310194
Actor loss: 14.245059
Action reg: 0.003985
  l1.weight: grad_norm = 0.022541
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.016133
Total gradient norm: 0.066404
=== Actor Training Debug (Iteration 3595) ===
Q mean: -10.926135
Q std: 14.838044
Actor loss: 10.930108
Action reg: 0.003973
  l1.weight: grad_norm = 0.067616
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.048260
Total gradient norm: 0.186722
=== Actor Training Debug (Iteration 3596) ===
Q mean: -12.032204
Q std: 16.531300
Actor loss: 12.036198
Action reg: 0.003994
  l1.weight: grad_norm = 0.029053
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.022389
Total gradient norm: 0.082903
=== Actor Training Debug (Iteration 3597) ===
Q mean: -13.616049
Q std: 16.569447
Actor loss: 13.620036
Action reg: 0.003988
  l1.weight: grad_norm = 0.052102
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.036299
Total gradient norm: 0.146219
=== Actor Training Debug (Iteration 3598) ===
Q mean: -13.106628
Q std: 16.398256
Actor loss: 13.110621
Action reg: 0.003993
  l1.weight: grad_norm = 0.025891
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.023966
Total gradient norm: 0.073779
=== Actor Training Debug (Iteration 3599) ===
Q mean: -13.866240
Q std: 16.808712
Actor loss: 13.870224
Action reg: 0.003985
  l1.weight: grad_norm = 0.101266
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.074253
Total gradient norm: 0.281590
=== Actor Training Debug (Iteration 3600) ===
Q mean: -12.764477
Q std: 16.731842
Actor loss: 12.768467
Action reg: 0.003990
  l1.weight: grad_norm = 0.046013
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.038083
Total gradient norm: 0.126471
=== Actor Training Debug (Iteration 3601) ===
Q mean: -14.130693
Q std: 16.306923
Actor loss: 14.134674
Action reg: 0.003981
  l1.weight: grad_norm = 0.075977
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.052925
Total gradient norm: 0.174108
=== Actor Training Debug (Iteration 3602) ===
Q mean: -13.195248
Q std: 16.398218
Actor loss: 13.199224
Action reg: 0.003976
  l1.weight: grad_norm = 0.040165
  l1.bias: grad_norm = 0.000857
  l2.weight: grad_norm = 0.030180
Total gradient norm: 0.122493
=== Actor Training Debug (Iteration 3603) ===
Q mean: -12.630506
Q std: 16.976545
Actor loss: 12.634487
Action reg: 0.003982
  l1.weight: grad_norm = 0.145099
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.097119
Total gradient norm: 0.372551
=== Actor Training Debug (Iteration 3604) ===
Q mean: -11.728067
Q std: 16.287689
Actor loss: 11.732054
Action reg: 0.003987
  l1.weight: grad_norm = 0.100108
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.067275
Total gradient norm: 0.241900
=== Actor Training Debug (Iteration 3605) ===
Q mean: -13.171022
Q std: 15.865185
Actor loss: 13.175008
Action reg: 0.003986
  l1.weight: grad_norm = 0.017323
  l1.bias: grad_norm = 0.000809
  l2.weight: grad_norm = 0.013384
Total gradient norm: 0.051793
=== Actor Training Debug (Iteration 3606) ===
Q mean: -11.725154
Q std: 16.193699
Actor loss: 11.729138
Action reg: 0.003985
  l1.weight: grad_norm = 0.104421
  l1.bias: grad_norm = 0.000454
  l2.weight: grad_norm = 0.079279
Total gradient norm: 0.304000
=== Actor Training Debug (Iteration 3607) ===
Q mean: -12.093037
Q std: 15.824542
Actor loss: 12.097015
Action reg: 0.003979
  l1.weight: grad_norm = 0.122822
  l1.bias: grad_norm = 0.000874
  l2.weight: grad_norm = 0.079259
Total gradient norm: 0.255156
=== Actor Training Debug (Iteration 3608) ===
Q mean: -13.941957
Q std: 16.141933
Actor loss: 13.945947
Action reg: 0.003990
  l1.weight: grad_norm = 0.053691
  l1.bias: grad_norm = 0.000720
  l2.weight: grad_norm = 0.035870
Total gradient norm: 0.114086
=== Actor Training Debug (Iteration 3609) ===
Q mean: -11.672184
Q std: 15.818921
Actor loss: 11.676169
Action reg: 0.003986
  l1.weight: grad_norm = 0.081966
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.050824
Total gradient norm: 0.191402
=== Actor Training Debug (Iteration 3610) ===
Q mean: -13.138777
Q std: 16.877972
Actor loss: 13.142767
Action reg: 0.003990
  l1.weight: grad_norm = 0.032367
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.034673
Total gradient norm: 0.145294
=== Actor Training Debug (Iteration 3611) ===
Q mean: -12.951845
Q std: 16.176741
Actor loss: 12.955830
Action reg: 0.003984
  l1.weight: grad_norm = 0.212237
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.128914
Total gradient norm: 0.445854
=== Actor Training Debug (Iteration 3612) ===
Q mean: -12.901018
Q std: 16.176327
Actor loss: 12.905009
Action reg: 0.003991
  l1.weight: grad_norm = 0.036479
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.029756
Total gradient norm: 0.118764
=== Actor Training Debug (Iteration 3613) ===
Q mean: -13.298650
Q std: 16.269787
Actor loss: 13.302625
Action reg: 0.003975
  l1.weight: grad_norm = 0.104130
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.066204
Total gradient norm: 0.275659
=== Actor Training Debug (Iteration 3614) ===
Q mean: -11.828981
Q std: 16.660128
Actor loss: 11.832961
Action reg: 0.003979
  l1.weight: grad_norm = 0.095281
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.078313
Total gradient norm: 0.266035
=== Actor Training Debug (Iteration 3615) ===
Q mean: -11.571423
Q std: 16.226559
Actor loss: 11.575395
Action reg: 0.003972
  l1.weight: grad_norm = 0.111074
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.091183
Total gradient norm: 0.338972
=== Actor Training Debug (Iteration 3616) ===
Q mean: -12.943658
Q std: 16.492147
Actor loss: 12.947643
Action reg: 0.003985
  l1.weight: grad_norm = 0.078418
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.049232
Total gradient norm: 0.236233
=== Actor Training Debug (Iteration 3617) ===
Q mean: -12.988665
Q std: 15.977751
Actor loss: 12.992641
Action reg: 0.003977
  l1.weight: grad_norm = 0.056625
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.040760
Total gradient norm: 0.134445
=== Actor Training Debug (Iteration 3618) ===
Q mean: -9.665369
Q std: 14.191391
Actor loss: 9.669353
Action reg: 0.003983
  l1.weight: grad_norm = 0.062423
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.050793
Total gradient norm: 0.208497
=== Actor Training Debug (Iteration 3619) ===
Q mean: -12.702291
Q std: 15.716204
Actor loss: 12.706285
Action reg: 0.003993
  l1.weight: grad_norm = 0.029971
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.020514
Total gradient norm: 0.068819
=== Actor Training Debug (Iteration 3620) ===
Q mean: -15.431473
Q std: 16.731188
Actor loss: 15.435462
Action reg: 0.003989
  l1.weight: grad_norm = 0.040458
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.033492
Total gradient norm: 0.149115
=== Actor Training Debug (Iteration 3621) ===
Q mean: -12.767427
Q std: 16.092022
Actor loss: 12.771417
Action reg: 0.003990
  l1.weight: grad_norm = 0.056570
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.047855
Total gradient norm: 0.176110
=== Actor Training Debug (Iteration 3622) ===
Q mean: -12.079494
Q std: 16.317667
Actor loss: 12.083472
Action reg: 0.003978
  l1.weight: grad_norm = 0.093758
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.059266
Total gradient norm: 0.209812
=== Actor Training Debug (Iteration 3623) ===
Q mean: -12.345915
Q std: 15.977060
Actor loss: 12.349900
Action reg: 0.003985
  l1.weight: grad_norm = 0.041355
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.030632
Total gradient norm: 0.116119
=== Actor Training Debug (Iteration 3624) ===
Q mean: -12.501684
Q std: 15.687939
Actor loss: 12.505670
Action reg: 0.003985
  l1.weight: grad_norm = 0.054654
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.044321
Total gradient norm: 0.182341
=== Actor Training Debug (Iteration 3625) ===
Q mean: -12.599407
Q std: 16.701836
Actor loss: 12.603381
Action reg: 0.003974
  l1.weight: grad_norm = 0.056361
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.046277
Total gradient norm: 0.139458
=== Actor Training Debug (Iteration 3626) ===
Q mean: -11.489607
Q std: 16.254128
Actor loss: 11.493589
Action reg: 0.003983
  l1.weight: grad_norm = 0.126699
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.089078
Total gradient norm: 0.428584
=== Actor Training Debug (Iteration 3627) ===
Q mean: -15.604279
Q std: 18.299538
Actor loss: 15.608264
Action reg: 0.003985
  l1.weight: grad_norm = 0.153465
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.095903
Total gradient norm: 0.316055
=== Actor Training Debug (Iteration 3628) ===
Q mean: -13.574595
Q std: 16.880098
Actor loss: 13.578578
Action reg: 0.003983
  l1.weight: grad_norm = 0.105616
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.077889
Total gradient norm: 0.326526
=== Actor Training Debug (Iteration 3629) ===
Q mean: -14.595215
Q std: 16.682066
Actor loss: 14.599205
Action reg: 0.003991
  l1.weight: grad_norm = 0.021729
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.017343
Total gradient norm: 0.057094
=== Actor Training Debug (Iteration 3630) ===
Q mean: -13.050760
Q std: 17.527199
Actor loss: 13.054738
Action reg: 0.003978
  l1.weight: grad_norm = 0.218477
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.153006
Total gradient norm: 0.499748
=== Actor Training Debug (Iteration 3631) ===
Q mean: -12.383350
Q std: 16.293730
Actor loss: 12.387334
Action reg: 0.003984
  l1.weight: grad_norm = 0.063934
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.051818
Total gradient norm: 0.155106
=== Actor Training Debug (Iteration 3632) ===
Q mean: -13.653292
Q std: 17.077089
Actor loss: 13.657282
Action reg: 0.003990
  l1.weight: grad_norm = 0.091021
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.059519
Total gradient norm: 0.191562
=== Actor Training Debug (Iteration 3633) ===
Q mean: -10.898009
Q std: 15.346712
Actor loss: 10.901995
Action reg: 0.003985
  l1.weight: grad_norm = 0.103219
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.076206
Total gradient norm: 0.253493
=== Actor Training Debug (Iteration 3634) ===
Q mean: -12.054087
Q std: 16.315903
Actor loss: 12.058064
Action reg: 0.003978
  l1.weight: grad_norm = 0.244290
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.157391
Total gradient norm: 0.701750
=== Actor Training Debug (Iteration 3635) ===
Q mean: -12.932783
Q std: 16.926123
Actor loss: 12.936755
Action reg: 0.003972
  l1.weight: grad_norm = 0.081643
  l1.bias: grad_norm = 0.001050
  l2.weight: grad_norm = 0.059275
Total gradient norm: 0.221386
=== Actor Training Debug (Iteration 3636) ===
Q mean: -13.311230
Q std: 16.981071
Actor loss: 13.315221
Action reg: 0.003991
  l1.weight: grad_norm = 0.049161
  l1.bias: grad_norm = 0.000552
  l2.weight: grad_norm = 0.030174
Total gradient norm: 0.108949
=== Actor Training Debug (Iteration 3637) ===
Q mean: -10.945518
Q std: 15.201530
Actor loss: 10.949503
Action reg: 0.003985
  l1.weight: grad_norm = 0.132113
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.086276
Total gradient norm: 0.386529
=== Actor Training Debug (Iteration 3638) ===
Q mean: -11.539606
Q std: 15.824568
Actor loss: 11.543588
Action reg: 0.003981
  l1.weight: grad_norm = 0.065726
  l1.bias: grad_norm = 0.001057
  l2.weight: grad_norm = 0.056963
Total gradient norm: 0.202677
=== Actor Training Debug (Iteration 3639) ===
Q mean: -12.518564
Q std: 16.267584
Actor loss: 12.522556
Action reg: 0.003992
  l1.weight: grad_norm = 0.132459
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.084667
Total gradient norm: 0.313066
=== Actor Training Debug (Iteration 3640) ===
Q mean: -12.698221
Q std: 16.606064
Actor loss: 12.702201
Action reg: 0.003979
  l1.weight: grad_norm = 0.120926
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.083543
Total gradient norm: 0.296826
=== Actor Training Debug (Iteration 3641) ===
Q mean: -10.792072
Q std: 15.350166
Actor loss: 10.796055
Action reg: 0.003983
  l1.weight: grad_norm = 0.059339
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.046406
Total gradient norm: 0.154068
=== Actor Training Debug (Iteration 3642) ===
Q mean: -12.922287
Q std: 16.485924
Actor loss: 12.926270
Action reg: 0.003983
  l1.weight: grad_norm = 0.043041
  l1.bias: grad_norm = 0.001122
  l2.weight: grad_norm = 0.041223
Total gradient norm: 0.122726
=== Actor Training Debug (Iteration 3643) ===
Q mean: -11.516943
Q std: 15.157092
Actor loss: 11.520928
Action reg: 0.003986
  l1.weight: grad_norm = 0.103017
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.077951
Total gradient norm: 0.324646
=== Actor Training Debug (Iteration 3644) ===
Q mean: -10.871435
Q std: 14.988647
Actor loss: 10.875418
Action reg: 0.003982
  l1.weight: grad_norm = 0.248813
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.148038
Total gradient norm: 0.596370
=== Actor Training Debug (Iteration 3645) ===
Q mean: -11.937016
Q std: 16.177872
Actor loss: 11.941007
Action reg: 0.003991
  l1.weight: grad_norm = 0.052858
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.033643
Total gradient norm: 0.132759
=== Actor Training Debug (Iteration 3646) ===
Q mean: -12.506977
Q std: 16.110962
Actor loss: 12.510962
Action reg: 0.003984
  l1.weight: grad_norm = 0.048986
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.036658
Total gradient norm: 0.151936
=== Actor Training Debug (Iteration 3647) ===
Q mean: -13.859418
Q std: 17.051229
Actor loss: 13.863412
Action reg: 0.003994
  l1.weight: grad_norm = 0.043231
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.031511
Total gradient norm: 0.129560
=== Actor Training Debug (Iteration 3648) ===
Q mean: -12.384140
Q std: 15.932220
Actor loss: 12.388127
Action reg: 0.003987
  l1.weight: grad_norm = 0.085546
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.065594
Total gradient norm: 0.230266
=== Actor Training Debug (Iteration 3649) ===
Q mean: -12.178127
Q std: 15.795665
Actor loss: 12.182110
Action reg: 0.003982
  l1.weight: grad_norm = 0.051757
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.037784
Total gradient norm: 0.127830
=== Actor Training Debug (Iteration 3650) ===
Q mean: -10.840095
Q std: 14.895358
Actor loss: 10.844078
Action reg: 0.003983
  l1.weight: grad_norm = 0.101434
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.069249
Total gradient norm: 0.238245
=== Actor Training Debug (Iteration 3651) ===
Q mean: -11.398964
Q std: 15.323641
Actor loss: 11.402946
Action reg: 0.003982
  l1.weight: grad_norm = 0.077652
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.056319
Total gradient norm: 0.226817
=== Actor Training Debug (Iteration 3652) ===
Q mean: -12.785471
Q std: 17.218561
Actor loss: 12.789455
Action reg: 0.003985
  l1.weight: grad_norm = 0.049013
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.036568
Total gradient norm: 0.128075
=== Actor Training Debug (Iteration 3653) ===
Q mean: -12.909253
Q std: 15.927146
Actor loss: 12.913246
Action reg: 0.003993
  l1.weight: grad_norm = 0.015164
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.011713
Total gradient norm: 0.050091
=== Actor Training Debug (Iteration 3654) ===
Q mean: -12.721300
Q std: 16.775845
Actor loss: 12.725291
Action reg: 0.003991
  l1.weight: grad_norm = 0.058364
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.046960
Total gradient norm: 0.189474
=== Actor Training Debug (Iteration 3655) ===
Q mean: -10.792307
Q std: 15.238126
Actor loss: 10.796294
Action reg: 0.003988
  l1.weight: grad_norm = 0.074611
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.046134
Total gradient norm: 0.232778
=== Actor Training Debug (Iteration 3656) ===
Q mean: -12.526500
Q std: 16.108519
Actor loss: 12.530488
Action reg: 0.003988
  l1.weight: grad_norm = 0.095429
  l1.bias: grad_norm = 0.000630
  l2.weight: grad_norm = 0.066807
Total gradient norm: 0.211191
=== Actor Training Debug (Iteration 3657) ===
Q mean: -11.671150
Q std: 15.465413
Actor loss: 11.675138
Action reg: 0.003987
  l1.weight: grad_norm = 0.033539
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.025741
Total gradient norm: 0.076268
=== Actor Training Debug (Iteration 3658) ===
Q mean: -11.747261
Q std: 15.552637
Actor loss: 11.751245
Action reg: 0.003984
  l1.weight: grad_norm = 0.171908
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.121092
Total gradient norm: 0.450164
=== Actor Training Debug (Iteration 3659) ===
Q mean: -13.620502
Q std: 16.891851
Actor loss: 13.624478
Action reg: 0.003977
  l1.weight: grad_norm = 0.080257
  l1.bias: grad_norm = 0.000712
  l2.weight: grad_norm = 0.056498
Total gradient norm: 0.187261
=== Actor Training Debug (Iteration 3660) ===
Q mean: -12.926172
Q std: 16.320267
Actor loss: 12.930162
Action reg: 0.003990
  l1.weight: grad_norm = 0.094920
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.057338
Total gradient norm: 0.218292
=== Actor Training Debug (Iteration 3661) ===
Q mean: -13.256746
Q std: 17.217438
Actor loss: 13.260731
Action reg: 0.003985
  l1.weight: grad_norm = 0.031301
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.021296
Total gradient norm: 0.072164
=== Actor Training Debug (Iteration 3662) ===
Q mean: -13.042723
Q std: 16.798954
Actor loss: 13.046703
Action reg: 0.003980
  l1.weight: grad_norm = 0.067387
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.048123
Total gradient norm: 0.173588
=== Actor Training Debug (Iteration 3663) ===
Q mean: -12.661371
Q std: 16.712177
Actor loss: 12.665348
Action reg: 0.003977
  l1.weight: grad_norm = 0.077632
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.062136
Total gradient norm: 0.261171
=== Actor Training Debug (Iteration 3664) ===
Q mean: -12.456911
Q std: 15.668895
Actor loss: 12.460894
Action reg: 0.003982
  l1.weight: grad_norm = 0.092662
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.065720
Total gradient norm: 0.239010
=== Actor Training Debug (Iteration 3665) ===
Q mean: -11.950991
Q std: 16.403034
Actor loss: 11.954969
Action reg: 0.003979
  l1.weight: grad_norm = 0.263348
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.175186
Total gradient norm: 0.587891
=== Actor Training Debug (Iteration 3666) ===
Q mean: -14.002064
Q std: 15.921792
Actor loss: 14.006053
Action reg: 0.003989
  l1.weight: grad_norm = 0.061563
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.041779
Total gradient norm: 0.138742
=== Actor Training Debug (Iteration 3667) ===
Q mean: -13.090459
Q std: 17.206982
Actor loss: 13.094431
Action reg: 0.003972
  l1.weight: grad_norm = 0.102310
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.073255
Total gradient norm: 0.228907
=== Actor Training Debug (Iteration 3668) ===
Q mean: -11.452772
Q std: 16.674395
Actor loss: 11.456758
Action reg: 0.003986
  l1.weight: grad_norm = 0.071617
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.039585
Total gradient norm: 0.140254
=== Actor Training Debug (Iteration 3669) ===
Q mean: -13.010348
Q std: 16.748526
Actor loss: 13.014331
Action reg: 0.003982
  l1.weight: grad_norm = 0.110660
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.087679
Total gradient norm: 0.343978
=== Actor Training Debug (Iteration 3670) ===
Q mean: -14.177016
Q std: 17.493561
Actor loss: 14.181004
Action reg: 0.003987
  l1.weight: grad_norm = 0.076635
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.054215
Total gradient norm: 0.183283
=== Actor Training Debug (Iteration 3671) ===
Q mean: -13.068445
Q std: 17.087954
Actor loss: 13.072433
Action reg: 0.003988
  l1.weight: grad_norm = 0.064055
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.060973
Total gradient norm: 0.193893
=== Actor Training Debug (Iteration 3672) ===
Q mean: -10.789639
Q std: 16.462708
Actor loss: 10.793619
Action reg: 0.003980
  l1.weight: grad_norm = 0.156737
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.111082
Total gradient norm: 0.409072
=== Actor Training Debug (Iteration 3673) ===
Q mean: -13.034889
Q std: 17.279470
Actor loss: 13.038862
Action reg: 0.003973
  l1.weight: grad_norm = 0.122310
  l1.bias: grad_norm = 0.001053
  l2.weight: grad_norm = 0.081072
Total gradient norm: 0.297821
=== Actor Training Debug (Iteration 3674) ===
Q mean: -13.025690
Q std: 16.875212
Actor loss: 13.029660
Action reg: 0.003970
  l1.weight: grad_norm = 0.055973
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.038507
Total gradient norm: 0.127626
=== Actor Training Debug (Iteration 3675) ===
Q mean: -13.430852
Q std: 17.284454
Actor loss: 13.434834
Action reg: 0.003983
  l1.weight: grad_norm = 0.052208
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.041327
Total gradient norm: 0.126651
=== Actor Training Debug (Iteration 3676) ===
Q mean: -11.985516
Q std: 16.087337
Actor loss: 11.989495
Action reg: 0.003980
  l1.weight: grad_norm = 0.093990
  l1.bias: grad_norm = 0.000951
  l2.weight: grad_norm = 0.066899
Total gradient norm: 0.261503
=== Actor Training Debug (Iteration 3677) ===
Q mean: -12.173954
Q std: 14.770922
Actor loss: 12.177941
Action reg: 0.003987
  l1.weight: grad_norm = 0.052473
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.034830
Total gradient norm: 0.119829
=== Actor Training Debug (Iteration 3678) ===
Q mean: -11.897448
Q std: 15.964145
Actor loss: 11.901434
Action reg: 0.003986
  l1.weight: grad_norm = 0.128246
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.098359
Total gradient norm: 0.452543
=== Actor Training Debug (Iteration 3679) ===
Q mean: -13.490565
Q std: 16.673929
Actor loss: 13.494553
Action reg: 0.003988
  l1.weight: grad_norm = 0.111036
  l1.bias: grad_norm = 0.000683
  l2.weight: grad_norm = 0.081455
Total gradient norm: 0.263268
=== Actor Training Debug (Iteration 3680) ===
Q mean: -13.946955
Q std: 18.014050
Actor loss: 13.950938
Action reg: 0.003983
  l1.weight: grad_norm = 0.040412
  l1.bias: grad_norm = 0.000944
  l2.weight: grad_norm = 0.028302
Total gradient norm: 0.121527
=== Actor Training Debug (Iteration 3681) ===
Q mean: -11.766998
Q std: 15.956848
Actor loss: 11.770990
Action reg: 0.003992
  l1.weight: grad_norm = 0.031734
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.018222
Total gradient norm: 0.060464
=== Actor Training Debug (Iteration 3682) ===
Q mean: -12.718640
Q std: 16.423277
Actor loss: 12.722625
Action reg: 0.003985
  l1.weight: grad_norm = 0.139196
  l1.bias: grad_norm = 0.000743
  l2.weight: grad_norm = 0.108802
Total gradient norm: 0.304706
=== Actor Training Debug (Iteration 3683) ===
Q mean: -12.932581
Q std: 17.147465
Actor loss: 12.936569
Action reg: 0.003988
  l1.weight: grad_norm = 0.027748
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.026480
Total gradient norm: 0.106862
=== Actor Training Debug (Iteration 3684) ===
Q mean: -12.569101
Q std: 15.771763
Actor loss: 12.573082
Action reg: 0.003981
  l1.weight: grad_norm = 0.081632
  l1.bias: grad_norm = 0.000571
  l2.weight: grad_norm = 0.060359
Total gradient norm: 0.303573
=== Actor Training Debug (Iteration 3685) ===
Q mean: -12.375696
Q std: 15.847378
Actor loss: 12.379680
Action reg: 0.003984
  l1.weight: grad_norm = 0.057496
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.037410
Total gradient norm: 0.142713
=== Actor Training Debug (Iteration 3686) ===
Q mean: -14.166313
Q std: 17.173368
Actor loss: 14.170301
Action reg: 0.003988
  l1.weight: grad_norm = 0.041261
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.036378
Total gradient norm: 0.138168
=== Actor Training Debug (Iteration 3687) ===
Q mean: -12.519693
Q std: 16.459427
Actor loss: 12.523680
Action reg: 0.003986
  l1.weight: grad_norm = 0.041051
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.034716
Total gradient norm: 0.118894
=== Actor Training Debug (Iteration 3688) ===
Q mean: -12.747422
Q std: 16.852457
Actor loss: 12.751406
Action reg: 0.003983
  l1.weight: grad_norm = 0.032141
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.024375
Total gradient norm: 0.091780
=== Actor Training Debug (Iteration 3689) ===
Q mean: -12.411451
Q std: 16.629824
Actor loss: 12.415418
Action reg: 0.003967
  l1.weight: grad_norm = 0.146572
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.093554
Total gradient norm: 0.285072
=== Actor Training Debug (Iteration 3690) ===
Q mean: -12.260334
Q std: 16.193514
Actor loss: 12.264319
Action reg: 0.003985
  l1.weight: grad_norm = 0.079877
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.063394
Total gradient norm: 0.352410
=== Actor Training Debug (Iteration 3691) ===
Q mean: -13.959694
Q std: 17.286188
Actor loss: 13.963667
Action reg: 0.003973
  l1.weight: grad_norm = 0.099685
  l1.bias: grad_norm = 0.000725
  l2.weight: grad_norm = 0.090850
Total gradient norm: 0.292914
=== Actor Training Debug (Iteration 3692) ===
Q mean: -13.605687
Q std: 16.873072
Actor loss: 13.609665
Action reg: 0.003977
  l1.weight: grad_norm = 0.114365
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.085753
Total gradient norm: 0.345181
=== Actor Training Debug (Iteration 3693) ===
Q mean: -14.544680
Q std: 17.925100
Actor loss: 14.548664
Action reg: 0.003984
  l1.weight: grad_norm = 0.104679
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.078674
Total gradient norm: 0.302768
=== Actor Training Debug (Iteration 3694) ===
Q mean: -13.380358
Q std: 16.848663
Actor loss: 13.384342
Action reg: 0.003985
  l1.weight: grad_norm = 0.043322
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.034946
Total gradient norm: 0.109320
=== Actor Training Debug (Iteration 3695) ===
Q mean: -11.752682
Q std: 16.179401
Actor loss: 11.756665
Action reg: 0.003983
  l1.weight: grad_norm = 0.070556
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.056361
Total gradient norm: 0.237485
=== Actor Training Debug (Iteration 3696) ===
Q mean: -10.583779
Q std: 15.194018
Actor loss: 10.587767
Action reg: 0.003987
  l1.weight: grad_norm = 0.109266
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.087982
Total gradient norm: 0.356270
=== Actor Training Debug (Iteration 3697) ===
Q mean: -12.884054
Q std: 16.028936
Actor loss: 12.888036
Action reg: 0.003982
  l1.weight: grad_norm = 0.088128
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.061860
Total gradient norm: 0.243914
=== Actor Training Debug (Iteration 3698) ===
Q mean: -13.450165
Q std: 16.799185
Actor loss: 13.454150
Action reg: 0.003985
  l1.weight: grad_norm = 0.112445
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.079927
Total gradient norm: 0.284745
=== Actor Training Debug (Iteration 3699) ===
Q mean: -12.260183
Q std: 16.582634
Actor loss: 12.264168
Action reg: 0.003985
  l1.weight: grad_norm = 0.054093
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.041631
Total gradient norm: 0.148282
=== Actor Training Debug (Iteration 3700) ===
Q mean: -13.978674
Q std: 17.431971
Actor loss: 13.982656
Action reg: 0.003982
  l1.weight: grad_norm = 0.078773
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.064341
Total gradient norm: 0.197489
=== Actor Training Debug (Iteration 3701) ===
Q mean: -10.977757
Q std: 15.526054
Actor loss: 10.981745
Action reg: 0.003988
  l1.weight: grad_norm = 0.044450
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.031118
Total gradient norm: 0.104147
=== Actor Training Debug (Iteration 3702) ===
Q mean: -11.069877
Q std: 15.755232
Actor loss: 11.073869
Action reg: 0.003992
  l1.weight: grad_norm = 0.013323
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.009864
Total gradient norm: 0.034517
=== Actor Training Debug (Iteration 3703) ===
Q mean: -15.299913
Q std: 16.678585
Actor loss: 15.303905
Action reg: 0.003992
  l1.weight: grad_norm = 0.096735
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.090846
Total gradient norm: 0.369649
=== Actor Training Debug (Iteration 3704) ===
Q mean: -11.136738
Q std: 15.860433
Actor loss: 11.140715
Action reg: 0.003977
  l1.weight: grad_norm = 0.040287
  l1.bias: grad_norm = 0.000669
  l2.weight: grad_norm = 0.032140
Total gradient norm: 0.115027
=== Actor Training Debug (Iteration 3705) ===
Q mean: -12.652910
Q std: 17.439760
Actor loss: 12.656899
Action reg: 0.003989
  l1.weight: grad_norm = 0.126264
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.072293
Total gradient norm: 0.208781
=== Actor Training Debug (Iteration 3706) ===
Q mean: -12.537740
Q std: 16.702257
Actor loss: 12.541722
Action reg: 0.003982
  l1.weight: grad_norm = 0.059570
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.048468
Total gradient norm: 0.175767
=== Actor Training Debug (Iteration 3707) ===
Q mean: -11.827539
Q std: 16.026379
Actor loss: 11.831525
Action reg: 0.003985
  l1.weight: grad_norm = 0.165197
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.120899
Total gradient norm: 0.504325
=== Actor Training Debug (Iteration 3708) ===
Q mean: -13.230500
Q std: 16.458263
Actor loss: 13.234475
Action reg: 0.003975
  l1.weight: grad_norm = 0.123934
  l1.bias: grad_norm = 0.001185
  l2.weight: grad_norm = 0.071940
Total gradient norm: 0.249292
=== Actor Training Debug (Iteration 3709) ===
Q mean: -11.701441
Q std: 15.694942
Actor loss: 11.705429
Action reg: 0.003988
  l1.weight: grad_norm = 0.058322
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.038718
Total gradient norm: 0.149021
=== Actor Training Debug (Iteration 3710) ===
Q mean: -13.997664
Q std: 17.056187
Actor loss: 14.001655
Action reg: 0.003990
  l1.weight: grad_norm = 0.010059
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.006315
Total gradient norm: 0.024466
=== Actor Training Debug (Iteration 3711) ===
Q mean: -12.382227
Q std: 16.247141
Actor loss: 12.386212
Action reg: 0.003985
  l1.weight: grad_norm = 0.082999
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.057182
Total gradient norm: 0.206536
=== Actor Training Debug (Iteration 3712) ===
Q mean: -13.167434
Q std: 15.932376
Actor loss: 13.171420
Action reg: 0.003986
  l1.weight: grad_norm = 0.071242
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.052879
Total gradient norm: 0.240196
=== Actor Training Debug (Iteration 3713) ===
Q mean: -9.784678
Q std: 14.591523
Actor loss: 9.788666
Action reg: 0.003988
  l1.weight: grad_norm = 0.096224
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.067353
Total gradient norm: 0.240237
=== Actor Training Debug (Iteration 3714) ===
Q mean: -12.643360
Q std: 15.669398
Actor loss: 12.647349
Action reg: 0.003990
  l1.weight: grad_norm = 0.063272
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.050640
Total gradient norm: 0.175586
=== Actor Training Debug (Iteration 3715) ===
Q mean: -11.960034
Q std: 16.552658
Actor loss: 11.964016
Action reg: 0.003981
  l1.weight: grad_norm = 0.085353
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.065896
Total gradient norm: 0.249920
=== Actor Training Debug (Iteration 3716) ===
Q mean: -11.808214
Q std: 15.308784
Actor loss: 11.812197
Action reg: 0.003982
  l1.weight: grad_norm = 0.049392
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.041503
Total gradient norm: 0.142643
=== Actor Training Debug (Iteration 3717) ===
Q mean: -12.792474
Q std: 16.098246
Actor loss: 12.796460
Action reg: 0.003986
  l1.weight: grad_norm = 0.446244
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.302601
Total gradient norm: 0.924336
=== Actor Training Debug (Iteration 3718) ===
Q mean: -11.474960
Q std: 15.583024
Actor loss: 11.478945
Action reg: 0.003985
  l1.weight: grad_norm = 0.053139
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.038862
Total gradient norm: 0.143425
=== Actor Training Debug (Iteration 3719) ===
Q mean: -14.064573
Q std: 17.289005
Actor loss: 14.068561
Action reg: 0.003987
  l1.weight: grad_norm = 0.057215
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.042740
Total gradient norm: 0.182586
=== Actor Training Debug (Iteration 3720) ===
Q mean: -13.752275
Q std: 17.279057
Actor loss: 13.756253
Action reg: 0.003979
  l1.weight: grad_norm = 0.115936
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.072313
Total gradient norm: 0.235466
=== Actor Training Debug (Iteration 3721) ===
Q mean: -12.568732
Q std: 16.661057
Actor loss: 12.572715
Action reg: 0.003983
  l1.weight: grad_norm = 0.078896
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.062563
Total gradient norm: 0.198523
=== Actor Training Debug (Iteration 3722) ===
Q mean: -13.321294
Q std: 16.478392
Actor loss: 13.325279
Action reg: 0.003985
  l1.weight: grad_norm = 0.070807
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.068787
Total gradient norm: 0.305056
=== Actor Training Debug (Iteration 3723) ===
Q mean: -11.401296
Q std: 15.996990
Actor loss: 11.405272
Action reg: 0.003976
  l1.weight: grad_norm = 0.183963
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.142475
Total gradient norm: 0.441683
=== Actor Training Debug (Iteration 3724) ===
Q mean: -12.501458
Q std: 15.807796
Actor loss: 12.505454
Action reg: 0.003996
  l1.weight: grad_norm = 0.035838
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.032784
Total gradient norm: 0.107337
=== Actor Training Debug (Iteration 3725) ===
Q mean: -12.578931
Q std: 16.362635
Actor loss: 12.582927
Action reg: 0.003996
  l1.weight: grad_norm = 0.025260
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.018089
Total gradient norm: 0.070359
=== Actor Training Debug (Iteration 3726) ===
Q mean: -12.718266
Q std: 17.016846
Actor loss: 12.722241
Action reg: 0.003975
  l1.weight: grad_norm = 0.056352
  l1.bias: grad_norm = 0.000623
  l2.weight: grad_norm = 0.040468
Total gradient norm: 0.186195
=== Actor Training Debug (Iteration 3727) ===
Q mean: -13.038556
Q std: 16.309961
Actor loss: 13.042544
Action reg: 0.003988
  l1.weight: grad_norm = 0.072496
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.051138
Total gradient norm: 0.184493
=== Actor Training Debug (Iteration 3728) ===
Q mean: -11.484587
Q std: 15.894177
Actor loss: 11.488570
Action reg: 0.003983
  l1.weight: grad_norm = 0.083708
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.062221
Total gradient norm: 0.254196
=== Actor Training Debug (Iteration 3729) ===
Q mean: -14.712951
Q std: 17.299377
Actor loss: 14.716942
Action reg: 0.003991
  l1.weight: grad_norm = 0.133602
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.093166
Total gradient norm: 0.365065
=== Actor Training Debug (Iteration 3730) ===
Q mean: -14.361069
Q std: 16.488361
Actor loss: 14.365055
Action reg: 0.003986
  l1.weight: grad_norm = 0.016047
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.010225
Total gradient norm: 0.034369
=== Actor Training Debug (Iteration 3731) ===
Q mean: -13.001476
Q std: 16.775467
Actor loss: 13.005462
Action reg: 0.003985
  l1.weight: grad_norm = 0.135561
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.105674
Total gradient norm: 0.388353
=== Actor Training Debug (Iteration 3732) ===
Q mean: -12.168421
Q std: 17.247440
Actor loss: 12.172408
Action reg: 0.003987
  l1.weight: grad_norm = 0.125416
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.086902
Total gradient norm: 0.326426
=== Actor Training Debug (Iteration 3733) ===
Q mean: -12.314810
Q std: 16.397205
Actor loss: 12.318794
Action reg: 0.003985
  l1.weight: grad_norm = 0.121619
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.094414
Total gradient norm: 0.397487
=== Actor Training Debug (Iteration 3734) ===
Q mean: -11.095734
Q std: 15.463250
Actor loss: 11.099717
Action reg: 0.003984
  l1.weight: grad_norm = 0.058305
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.046230
Total gradient norm: 0.147343
=== Actor Training Debug (Iteration 3735) ===
Q mean: -12.636007
Q std: 16.102160
Actor loss: 12.639986
Action reg: 0.003978
  l1.weight: grad_norm = 0.106047
  l1.bias: grad_norm = 0.000428
  l2.weight: grad_norm = 0.086833
Total gradient norm: 0.312180
=== Actor Training Debug (Iteration 3736) ===
Q mean: -11.950562
Q std: 16.495171
Actor loss: 11.954547
Action reg: 0.003985
  l1.weight: grad_norm = 0.103929
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.072910
Total gradient norm: 0.249805
=== Actor Training Debug (Iteration 3737) ===
Q mean: -13.598455
Q std: 16.243889
Actor loss: 13.602440
Action reg: 0.003985
  l1.weight: grad_norm = 0.077273
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.059803
Total gradient norm: 0.213819
=== Actor Training Debug (Iteration 3738) ===
Q mean: -11.811792
Q std: 15.762845
Actor loss: 11.815777
Action reg: 0.003985
  l1.weight: grad_norm = 0.061674
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.052817
Total gradient norm: 0.165337
=== Actor Training Debug (Iteration 3739) ===
Q mean: -12.916258
Q std: 16.556604
Actor loss: 12.920245
Action reg: 0.003987
  l1.weight: grad_norm = 0.076324
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.058241
Total gradient norm: 0.193951
=== Actor Training Debug (Iteration 3740) ===
Q mean: -12.835238
Q std: 16.756407
Actor loss: 12.839223
Action reg: 0.003985
  l1.weight: grad_norm = 0.038672
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.031073
Total gradient norm: 0.146030
=== Actor Training Debug (Iteration 3741) ===
Q mean: -12.101003
Q std: 16.121466
Actor loss: 12.104986
Action reg: 0.003984
  l1.weight: grad_norm = 0.032865
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.030666
Total gradient norm: 0.105022
=== Actor Training Debug (Iteration 3742) ===
Q mean: -11.068414
Q std: 16.736403
Actor loss: 11.072398
Action reg: 0.003984
  l1.weight: grad_norm = 0.099114
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.062340
Total gradient norm: 0.220239
=== Actor Training Debug (Iteration 3743) ===
Q mean: -11.097528
Q std: 15.422755
Actor loss: 11.101515
Action reg: 0.003986
  l1.weight: grad_norm = 0.091818
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.082292
Total gradient norm: 0.357008
=== Actor Training Debug (Iteration 3744) ===
Q mean: -13.007066
Q std: 16.718409
Actor loss: 13.011054
Action reg: 0.003988
  l1.weight: grad_norm = 0.062566
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.041905
Total gradient norm: 0.145537
=== Actor Training Debug (Iteration 3745) ===
Q mean: -13.772995
Q std: 17.353926
Actor loss: 13.776955
Action reg: 0.003960
  l1.weight: grad_norm = 0.088231
  l1.bias: grad_norm = 0.000780
  l2.weight: grad_norm = 0.068388
Total gradient norm: 0.248939
=== Actor Training Debug (Iteration 3746) ===
Q mean: -13.394773
Q std: 17.651512
Actor loss: 13.398756
Action reg: 0.003982
  l1.weight: grad_norm = 0.098135
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.060435
Total gradient norm: 0.252324
=== Actor Training Debug (Iteration 3747) ===
Q mean: -13.289669
Q std: 17.358418
Actor loss: 13.293644
Action reg: 0.003975
  l1.weight: grad_norm = 0.128221
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.108419
Total gradient norm: 0.441075
=== Actor Training Debug (Iteration 3748) ===
Q mean: -11.996203
Q std: 16.854879
Actor loss: 12.000187
Action reg: 0.003983
  l1.weight: grad_norm = 0.164774
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.122062
Total gradient norm: 0.414048
=== Actor Training Debug (Iteration 3749) ===
Q mean: -14.642746
Q std: 16.626657
Actor loss: 14.646729
Action reg: 0.003982
  l1.weight: grad_norm = 0.084540
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.065792
Total gradient norm: 0.273634
=== Actor Training Debug (Iteration 3750) ===
Q mean: -11.818934
Q std: 16.587257
Actor loss: 11.822911
Action reg: 0.003977
  l1.weight: grad_norm = 0.089378
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.060034
Total gradient norm: 0.181244
=== Actor Training Debug (Iteration 3751) ===
Q mean: -13.272205
Q std: 16.805058
Actor loss: 13.276198
Action reg: 0.003993
  l1.weight: grad_norm = 0.045838
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.028078
Total gradient norm: 0.091870
=== Actor Training Debug (Iteration 3752) ===
Q mean: -12.242393
Q std: 16.097029
Actor loss: 12.246381
Action reg: 0.003987
  l1.weight: grad_norm = 0.080921
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.068303
Total gradient norm: 0.225278
=== Actor Training Debug (Iteration 3753) ===
Q mean: -13.915949
Q std: 17.901628
Actor loss: 13.919934
Action reg: 0.003985
  l1.weight: grad_norm = 0.054804
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.038162
Total gradient norm: 0.126845
=== Actor Training Debug (Iteration 3754) ===
Q mean: -10.422382
Q std: 15.424342
Actor loss: 10.426359
Action reg: 0.003977
  l1.weight: grad_norm = 0.108004
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.081308
Total gradient norm: 0.253065
=== Actor Training Debug (Iteration 3755) ===
Q mean: -13.154913
Q std: 16.455770
Actor loss: 13.158895
Action reg: 0.003982
  l1.weight: grad_norm = 0.079952
  l1.bias: grad_norm = 0.000707
  l2.weight: grad_norm = 0.051701
Total gradient norm: 0.241273
=== Actor Training Debug (Iteration 3756) ===
Q mean: -12.824030
Q std: 17.589664
Actor loss: 12.828018
Action reg: 0.003989
  l1.weight: grad_norm = 0.067329
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.053577
Total gradient norm: 0.207529
=== Actor Training Debug (Iteration 3757) ===
Q mean: -13.898169
Q std: 16.996222
Actor loss: 13.902154
Action reg: 0.003986
  l1.weight: grad_norm = 0.032482
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.029051
Total gradient norm: 0.102633
=== Actor Training Debug (Iteration 3758) ===
Q mean: -11.643522
Q std: 16.831726
Actor loss: 11.647508
Action reg: 0.003985
  l1.weight: grad_norm = 0.118270
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.080118
Total gradient norm: 0.267176
=== Actor Training Debug (Iteration 3759) ===
Q mean: -13.399429
Q std: 17.598272
Actor loss: 13.403414
Action reg: 0.003985
  l1.weight: grad_norm = 0.039111
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.031262
Total gradient norm: 0.101375
=== Actor Training Debug (Iteration 3760) ===
Q mean: -12.080776
Q std: 16.003826
Actor loss: 12.084761
Action reg: 0.003985
  l1.weight: grad_norm = 0.055182
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.046426
Total gradient norm: 0.191452
=== Actor Training Debug (Iteration 3761) ===
Q mean: -13.177639
Q std: 16.115786
Actor loss: 13.181633
Action reg: 0.003994
  l1.weight: grad_norm = 0.063508
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.045785
Total gradient norm: 0.189571
=== Actor Training Debug (Iteration 3762) ===
Q mean: -13.480211
Q std: 16.937349
Actor loss: 13.484203
Action reg: 0.003992
  l1.weight: grad_norm = 0.015162
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.013548
Total gradient norm: 0.047347
=== Actor Training Debug (Iteration 3763) ===
Q mean: -11.483426
Q std: 16.330595
Actor loss: 11.487412
Action reg: 0.003987
  l1.weight: grad_norm = 0.012571
  l1.bias: grad_norm = 0.001432
  l2.weight: grad_norm = 0.010444
Total gradient norm: 0.041521
=== Actor Training Debug (Iteration 3764) ===
Q mean: -14.862267
Q std: 17.163624
Actor loss: 14.866257
Action reg: 0.003990
  l1.weight: grad_norm = 0.151516
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.111115
Total gradient norm: 0.340573
=== Actor Training Debug (Iteration 3765) ===
Q mean: -11.800031
Q std: 15.825763
Actor loss: 11.804020
Action reg: 0.003990
  l1.weight: grad_norm = 0.062697
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.042953
Total gradient norm: 0.158039
=== Actor Training Debug (Iteration 3766) ===
Q mean: -13.221649
Q std: 16.750940
Actor loss: 13.225633
Action reg: 0.003984
  l1.weight: grad_norm = 0.091826
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.048637
Total gradient norm: 0.198485
=== Actor Training Debug (Iteration 3767) ===
Q mean: -11.768269
Q std: 16.041294
Actor loss: 11.772250
Action reg: 0.003982
  l1.weight: grad_norm = 0.055076
  l1.bias: grad_norm = 0.000596
  l2.weight: grad_norm = 0.041520
Total gradient norm: 0.147928
=== Actor Training Debug (Iteration 3768) ===
Q mean: -12.839167
Q std: 15.852831
Actor loss: 12.843145
Action reg: 0.003979
  l1.weight: grad_norm = 0.100617
  l1.bias: grad_norm = 0.000684
  l2.weight: grad_norm = 0.072686
Total gradient norm: 0.294576
=== Actor Training Debug (Iteration 3769) ===
Q mean: -12.691288
Q std: 16.474857
Actor loss: 12.695283
Action reg: 0.003995
  l1.weight: grad_norm = 0.044257
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.032568
Total gradient norm: 0.112469
=== Actor Training Debug (Iteration 3770) ===
Q mean: -13.745401
Q std: 17.626266
Actor loss: 13.749370
Action reg: 0.003968
  l1.weight: grad_norm = 0.090698
  l1.bias: grad_norm = 0.002268
  l2.weight: grad_norm = 0.072327
Total gradient norm: 0.326452
=== Actor Training Debug (Iteration 3771) ===
Q mean: -12.227086
Q std: 16.764874
Actor loss: 12.231077
Action reg: 0.003991
  l1.weight: grad_norm = 0.038605
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.036617
Total gradient norm: 0.111723
=== Actor Training Debug (Iteration 3772) ===
Q mean: -12.373661
Q std: 16.915451
Actor loss: 12.377648
Action reg: 0.003987
  l1.weight: grad_norm = 0.082724
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.068326
Total gradient norm: 0.237120
=== Actor Training Debug (Iteration 3773) ===
Q mean: -13.629879
Q std: 16.334427
Actor loss: 13.633863
Action reg: 0.003985
  l1.weight: grad_norm = 0.067348
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.054916
Total gradient norm: 0.186909
=== Actor Training Debug (Iteration 3774) ===
Q mean: -14.838887
Q std: 17.910109
Actor loss: 14.842869
Action reg: 0.003982
  l1.weight: grad_norm = 0.038834
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.026449
Total gradient norm: 0.094272
=== Actor Training Debug (Iteration 3775) ===
Q mean: -12.539310
Q std: 16.732002
Actor loss: 12.543297
Action reg: 0.003986
  l1.weight: grad_norm = 0.107671
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.074981
Total gradient norm: 0.327458
=== Actor Training Debug (Iteration 3776) ===
Q mean: -15.180371
Q std: 17.851728
Actor loss: 15.184362
Action reg: 0.003991
  l1.weight: grad_norm = 0.046449
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.030256
Total gradient norm: 0.095156
=== Actor Training Debug (Iteration 3777) ===
Q mean: -11.647756
Q std: 15.891719
Actor loss: 11.651750
Action reg: 0.003994
  l1.weight: grad_norm = 0.042616
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.037456
Total gradient norm: 0.127204
=== Actor Training Debug (Iteration 3778) ===
Q mean: -12.550540
Q std: 17.004164
Actor loss: 12.554520
Action reg: 0.003980
  l1.weight: grad_norm = 0.118122
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.071774
Total gradient norm: 0.337223
=== Actor Training Debug (Iteration 3779) ===
Q mean: -13.745205
Q std: 16.593317
Actor loss: 13.749190
Action reg: 0.003985
  l1.weight: grad_norm = 0.085956
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.071638
Total gradient norm: 0.345402
=== Actor Training Debug (Iteration 3780) ===
Q mean: -13.288987
Q std: 17.450947
Actor loss: 13.292967
Action reg: 0.003980
  l1.weight: grad_norm = 0.054669
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.042425
Total gradient norm: 0.138646
=== Actor Training Debug (Iteration 3781) ===
Q mean: -14.182178
Q std: 17.328770
Actor loss: 14.186156
Action reg: 0.003979
  l1.weight: grad_norm = 0.236669
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.156474
Total gradient norm: 0.608452
=== Actor Training Debug (Iteration 3782) ===
Q mean: -12.234318
Q std: 16.441137
Actor loss: 12.238302
Action reg: 0.003985
  l1.weight: grad_norm = 0.075494
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.052642
Total gradient norm: 0.202322
=== Actor Training Debug (Iteration 3783) ===
Q mean: -12.800852
Q std: 16.483265
Actor loss: 12.804846
Action reg: 0.003994
  l1.weight: grad_norm = 0.063480
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.050347
Total gradient norm: 0.188360
=== Actor Training Debug (Iteration 3784) ===
Q mean: -13.600520
Q std: 17.462360
Actor loss: 13.604508
Action reg: 0.003988
  l1.weight: grad_norm = 0.046484
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.029953
Total gradient norm: 0.112150
=== Actor Training Debug (Iteration 3785) ===
Q mean: -11.823697
Q std: 17.026028
Actor loss: 11.827689
Action reg: 0.003992
  l1.weight: grad_norm = 0.048777
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.033932
Total gradient norm: 0.119851
=== Actor Training Debug (Iteration 3786) ===
Q mean: -12.416473
Q std: 16.520424
Actor loss: 12.420465
Action reg: 0.003991
  l1.weight: grad_norm = 0.032065
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.023461
Total gradient norm: 0.078556
=== Actor Training Debug (Iteration 3787) ===
Q mean: -13.639889
Q std: 17.656797
Actor loss: 13.643877
Action reg: 0.003988
  l1.weight: grad_norm = 0.064113
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.046463
Total gradient norm: 0.183914
=== Actor Training Debug (Iteration 3788) ===
Q mean: -13.464456
Q std: 15.955709
Actor loss: 13.468441
Action reg: 0.003986
  l1.weight: grad_norm = 0.086739
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.059534
Total gradient norm: 0.221668
=== Actor Training Debug (Iteration 3789) ===
Q mean: -12.376370
Q std: 16.396866
Actor loss: 12.380358
Action reg: 0.003988
  l1.weight: grad_norm = 0.081808
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.062582
Total gradient norm: 0.243332
=== Actor Training Debug (Iteration 3790) ===
Q mean: -11.109625
Q std: 15.566544
Actor loss: 11.113608
Action reg: 0.003984
  l1.weight: grad_norm = 0.147061
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.077955
Total gradient norm: 0.280283
=== Actor Training Debug (Iteration 3791) ===
Q mean: -14.168766
Q std: 18.440762
Actor loss: 14.172752
Action reg: 0.003987
  l1.weight: grad_norm = 0.085722
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.064857
Total gradient norm: 0.275767
=== Actor Training Debug (Iteration 3792) ===
Q mean: -15.138603
Q std: 18.081141
Actor loss: 15.142579
Action reg: 0.003976
  l1.weight: grad_norm = 0.114939
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.097024
Total gradient norm: 0.373768
=== Actor Training Debug (Iteration 3793) ===
Q mean: -12.780702
Q std: 16.917366
Actor loss: 12.784691
Action reg: 0.003989
  l1.weight: grad_norm = 0.020089
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.015215
Total gradient norm: 0.050108
=== Actor Training Debug (Iteration 3794) ===
Q mean: -12.156230
Q std: 15.680177
Actor loss: 12.160214
Action reg: 0.003985
  l1.weight: grad_norm = 0.068707
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.053572
Total gradient norm: 0.188824
=== Actor Training Debug (Iteration 3795) ===
Q mean: -14.108251
Q std: 17.613605
Actor loss: 14.112238
Action reg: 0.003987
  l1.weight: grad_norm = 0.154124
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.111215
Total gradient norm: 0.319605
=== Actor Training Debug (Iteration 3796) ===
Q mean: -12.268000
Q std: 16.230234
Actor loss: 12.271990
Action reg: 0.003990
  l1.weight: grad_norm = 0.107960
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.064005
Total gradient norm: 0.234669
=== Actor Training Debug (Iteration 3797) ===
Q mean: -13.610632
Q std: 16.561218
Actor loss: 13.614616
Action reg: 0.003985
  l1.weight: grad_norm = 0.087921
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.058043
Total gradient norm: 0.229340
=== Actor Training Debug (Iteration 3798) ===
Q mean: -12.429439
Q std: 16.424292
Actor loss: 12.433414
Action reg: 0.003975
  l1.weight: grad_norm = 0.046371
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.036204
Total gradient norm: 0.121661
=== Actor Training Debug (Iteration 3799) ===
Q mean: -13.191393
Q std: 17.112089
Actor loss: 13.195383
Action reg: 0.003990
  l1.weight: grad_norm = 0.020531
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.014367
Total gradient norm: 0.055129
=== Actor Training Debug (Iteration 3800) ===
Q mean: -12.941130
Q std: 17.739159
Actor loss: 12.945117
Action reg: 0.003988
  l1.weight: grad_norm = 0.081233
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.058597
Total gradient norm: 0.189299
=== Actor Training Debug (Iteration 3801) ===
Q mean: -12.371232
Q std: 15.225596
Actor loss: 12.375223
Action reg: 0.003991
  l1.weight: grad_norm = 0.018324
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.014943
Total gradient norm: 0.067667
=== Actor Training Debug (Iteration 3802) ===
Q mean: -12.778782
Q std: 16.382416
Actor loss: 12.782766
Action reg: 0.003984
  l1.weight: grad_norm = 0.122494
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.090880
Total gradient norm: 0.338682
=== Actor Training Debug (Iteration 3803) ===
Q mean: -12.445785
Q std: 16.485537
Actor loss: 12.449767
Action reg: 0.003982
  l1.weight: grad_norm = 0.175375
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.129243
Total gradient norm: 0.581038
=== Actor Training Debug (Iteration 3804) ===
Q mean: -12.891428
Q std: 16.744497
Actor loss: 12.895406
Action reg: 0.003978
  l1.weight: grad_norm = 0.148976
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.101211
Total gradient norm: 0.321686
=== Actor Training Debug (Iteration 3805) ===
Q mean: -12.898169
Q std: 18.057447
Actor loss: 12.902153
Action reg: 0.003985
  l1.weight: grad_norm = 0.095243
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.067630
Total gradient norm: 0.264878
=== Actor Training Debug (Iteration 3806) ===
Q mean: -12.004902
Q std: 16.014978
Actor loss: 12.008883
Action reg: 0.003982
  l1.weight: grad_norm = 0.092288
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.077200
Total gradient norm: 0.271604
=== Actor Training Debug (Iteration 3807) ===
Q mean: -14.573830
Q std: 16.808697
Actor loss: 14.577807
Action reg: 0.003978
  l1.weight: grad_norm = 0.154428
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.131432
Total gradient norm: 0.377737
=== Actor Training Debug (Iteration 3808) ===
Q mean: -13.103370
Q std: 17.047480
Actor loss: 13.107363
Action reg: 0.003993
  l1.weight: grad_norm = 0.021651
  l1.bias: grad_norm = 0.000731
  l2.weight: grad_norm = 0.015448
Total gradient norm: 0.059986
=== Actor Training Debug (Iteration 3809) ===
Q mean: -12.131411
Q std: 15.587256
Actor loss: 12.135401
Action reg: 0.003990
  l1.weight: grad_norm = 0.069038
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.046217
Total gradient norm: 0.151828
=== Actor Training Debug (Iteration 3810) ===
Q mean: -11.197342
Q std: 16.261660
Actor loss: 11.201323
Action reg: 0.003981
  l1.weight: grad_norm = 0.091613
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.060845
Total gradient norm: 0.200476
=== Actor Training Debug (Iteration 3811) ===
Q mean: -14.747547
Q std: 17.697104
Actor loss: 14.751529
Action reg: 0.003982
  l1.weight: grad_norm = 0.089108
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.070299
Total gradient norm: 0.278781
=== Actor Training Debug (Iteration 3812) ===
Q mean: -11.450967
Q std: 16.072634
Actor loss: 11.454946
Action reg: 0.003979
  l1.weight: grad_norm = 0.119581
  l1.bias: grad_norm = 0.000428
  l2.weight: grad_norm = 0.081130
Total gradient norm: 0.251560
=== Actor Training Debug (Iteration 3813) ===
Q mean: -12.864021
Q std: 17.535652
Actor loss: 12.867995
Action reg: 0.003974
  l1.weight: grad_norm = 0.144229
  l1.bias: grad_norm = 0.000612
  l2.weight: grad_norm = 0.114237
Total gradient norm: 0.432419
=== Actor Training Debug (Iteration 3814) ===
Q mean: -12.100574
Q std: 17.179070
Actor loss: 12.104564
Action reg: 0.003989
  l1.weight: grad_norm = 0.030467
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.022461
Total gradient norm: 0.084193
=== Actor Training Debug (Iteration 3815) ===
Q mean: -10.613217
Q std: 16.288948
Actor loss: 10.617189
Action reg: 0.003972
  l1.weight: grad_norm = 0.120153
  l1.bias: grad_norm = 0.000855
  l2.weight: grad_norm = 0.099566
Total gradient norm: 0.478179
=== Actor Training Debug (Iteration 3816) ===
Q mean: -10.895905
Q std: 15.491015
Actor loss: 10.899882
Action reg: 0.003978
  l1.weight: grad_norm = 0.078819
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.054445
Total gradient norm: 0.162395
=== Actor Training Debug (Iteration 3817) ===
Q mean: -12.082286
Q std: 16.475224
Actor loss: 12.086271
Action reg: 0.003985
  l1.weight: grad_norm = 0.107040
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.076154
Total gradient norm: 0.217492
=== Actor Training Debug (Iteration 3818) ===
Q mean: -11.735493
Q std: 16.187473
Actor loss: 11.739473
Action reg: 0.003980
  l1.weight: grad_norm = 0.046853
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.032967
Total gradient norm: 0.140038
=== Actor Training Debug (Iteration 3819) ===
Q mean: -13.524679
Q std: 16.889956
Actor loss: 13.528661
Action reg: 0.003982
  l1.weight: grad_norm = 0.056179
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.045156
Total gradient norm: 0.167498
=== Actor Training Debug (Iteration 3820) ===
Q mean: -11.502504
Q std: 16.341633
Actor loss: 11.506493
Action reg: 0.003988
  l1.weight: grad_norm = 0.046908
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.028320
Total gradient norm: 0.102865
=== Actor Training Debug (Iteration 3821) ===
Q mean: -13.631645
Q std: 17.641972
Actor loss: 13.635629
Action reg: 0.003983
  l1.weight: grad_norm = 0.073851
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.062208
Total gradient norm: 0.215530
=== Actor Training Debug (Iteration 3822) ===
Q mean: -12.616161
Q std: 17.311022
Actor loss: 12.620150
Action reg: 0.003988
  l1.weight: grad_norm = 0.081570
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.064666
Total gradient norm: 0.214303
=== Actor Training Debug (Iteration 3823) ===
Q mean: -14.079298
Q std: 17.177357
Actor loss: 14.083279
Action reg: 0.003980
  l1.weight: grad_norm = 0.087332
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.061036
Total gradient norm: 0.196541
=== Actor Training Debug (Iteration 3824) ===
Q mean: -11.474648
Q std: 15.356618
Actor loss: 11.478615
Action reg: 0.003967
  l1.weight: grad_norm = 0.191696
  l1.bias: grad_norm = 0.000900
  l2.weight: grad_norm = 0.120648
Total gradient norm: 0.415161
=== Actor Training Debug (Iteration 3825) ===
Q mean: -14.316206
Q std: 17.242527
Actor loss: 14.320197
Action reg: 0.003991
  l1.weight: grad_norm = 0.021410
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.015013
Total gradient norm: 0.055691
=== Actor Training Debug (Iteration 3826) ===
Q mean: -10.989354
Q std: 14.783087
Actor loss: 10.993346
Action reg: 0.003992
  l1.weight: grad_norm = 0.047015
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.037252
Total gradient norm: 0.144309
=== Actor Training Debug (Iteration 3827) ===
Q mean: -12.985499
Q std: 17.994936
Actor loss: 12.989481
Action reg: 0.003981
  l1.weight: grad_norm = 0.097555
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.068441
Total gradient norm: 0.307112
=== Actor Training Debug (Iteration 3828) ===
Q mean: -13.231240
Q std: 16.686005
Actor loss: 13.235225
Action reg: 0.003984
  l1.weight: grad_norm = 0.038331
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.027057
Total gradient norm: 0.123405
=== Actor Training Debug (Iteration 3829) ===
Q mean: -12.036949
Q std: 16.593277
Actor loss: 12.040925
Action reg: 0.003976
  l1.weight: grad_norm = 0.174237
  l1.bias: grad_norm = 0.000721
  l2.weight: grad_norm = 0.108209
Total gradient norm: 0.341182
=== Actor Training Debug (Iteration 3830) ===
Q mean: -14.601690
Q std: 17.523001
Actor loss: 14.605684
Action reg: 0.003994
  l1.weight: grad_norm = 0.049172
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.039784
Total gradient norm: 0.194998
=== Actor Training Debug (Iteration 3831) ===
Q mean: -14.179092
Q std: 17.190859
Actor loss: 14.183082
Action reg: 0.003989
  l1.weight: grad_norm = 0.049027
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.032804
Total gradient norm: 0.137496
=== Actor Training Debug (Iteration 3832) ===
Q mean: -11.720716
Q std: 16.143625
Actor loss: 11.724695
Action reg: 0.003979
  l1.weight: grad_norm = 0.085361
  l1.bias: grad_norm = 0.000732
  l2.weight: grad_norm = 0.063188
Total gradient norm: 0.218349
=== Actor Training Debug (Iteration 3833) ===
Q mean: -11.310328
Q std: 15.309160
Actor loss: 11.314311
Action reg: 0.003983
  l1.weight: grad_norm = 0.060069
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.049191
Total gradient norm: 0.147973
=== Actor Training Debug (Iteration 3834) ===
Q mean: -11.243088
Q std: 16.040901
Actor loss: 11.247074
Action reg: 0.003987
  l1.weight: grad_norm = 0.148014
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.097457
Total gradient norm: 0.356398
=== Actor Training Debug (Iteration 3835) ===
Q mean: -13.451498
Q std: 17.330044
Actor loss: 13.455479
Action reg: 0.003981
  l1.weight: grad_norm = 0.108032
  l1.bias: grad_norm = 0.000719
  l2.weight: grad_norm = 0.068904
Total gradient norm: 0.259268
=== Actor Training Debug (Iteration 3836) ===
Q mean: -13.712828
Q std: 17.495274
Actor loss: 13.716804
Action reg: 0.003976
  l1.weight: grad_norm = 0.044223
  l1.bias: grad_norm = 0.001091
  l2.weight: grad_norm = 0.029628
Total gradient norm: 0.106818
=== Actor Training Debug (Iteration 3837) ===
Q mean: -12.940706
Q std: 17.547348
Actor loss: 12.944700
Action reg: 0.003994
  l1.weight: grad_norm = 0.119627
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.066015
Total gradient norm: 0.229080
=== Actor Training Debug (Iteration 3838) ===
Q mean: -13.359306
Q std: 16.046043
Actor loss: 13.363288
Action reg: 0.003982
  l1.weight: grad_norm = 0.109097
  l1.bias: grad_norm = 0.000885
  l2.weight: grad_norm = 0.060699
Total gradient norm: 0.221284
=== Actor Training Debug (Iteration 3839) ===
Q mean: -15.849018
Q std: 17.864990
Actor loss: 15.853005
Action reg: 0.003988
  l1.weight: grad_norm = 0.112305
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.086601
Total gradient norm: 0.370567
=== Actor Training Debug (Iteration 3840) ===
Q mean: -13.804573
Q std: 17.182487
Actor loss: 13.808559
Action reg: 0.003986
  l1.weight: grad_norm = 0.053693
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.033185
Total gradient norm: 0.103259
=== Actor Training Debug (Iteration 3841) ===
Q mean: -13.807485
Q std: 17.179750
Actor loss: 13.811460
Action reg: 0.003975
  l1.weight: grad_norm = 0.127019
  l1.bias: grad_norm = 0.000602
  l2.weight: grad_norm = 0.086252
Total gradient norm: 0.293937
=== Actor Training Debug (Iteration 3842) ===
Q mean: -11.485518
Q std: 16.155127
Actor loss: 11.489504
Action reg: 0.003986
  l1.weight: grad_norm = 0.089191
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.071020
Total gradient norm: 0.277530
=== Actor Training Debug (Iteration 3843) ===
Q mean: -10.982161
Q std: 15.996677
Actor loss: 10.986148
Action reg: 0.003987
  l1.weight: grad_norm = 0.065563
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.052938
Total gradient norm: 0.180713
=== Actor Training Debug (Iteration 3844) ===
Q mean: -14.253191
Q std: 17.497873
Actor loss: 14.257174
Action reg: 0.003983
  l1.weight: grad_norm = 0.226844
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.178098
Total gradient norm: 0.727469
=== Actor Training Debug (Iteration 3845) ===
Q mean: -13.727299
Q std: 17.784157
Actor loss: 13.731287
Action reg: 0.003988
  l1.weight: grad_norm = 0.135156
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.105232
Total gradient norm: 0.325045
=== Actor Training Debug (Iteration 3846) ===
Q mean: -11.728208
Q std: 16.368761
Actor loss: 11.732192
Action reg: 0.003985
  l1.weight: grad_norm = 0.167020
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.125590
Total gradient norm: 0.436590
=== Actor Training Debug (Iteration 3847) ===
Q mean: -14.030333
Q std: 16.464291
Actor loss: 14.034322
Action reg: 0.003989
  l1.weight: grad_norm = 0.084966
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.070634
Total gradient norm: 0.306576
=== Actor Training Debug (Iteration 3848) ===
Q mean: -14.795510
Q std: 17.728180
Actor loss: 14.799499
Action reg: 0.003988
  l1.weight: grad_norm = 0.085575
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.068509
Total gradient norm: 0.192544
=== Actor Training Debug (Iteration 3849) ===
Q mean: -13.456063
Q std: 17.107216
Actor loss: 13.460046
Action reg: 0.003982
  l1.weight: grad_norm = 0.096673
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.095246
Total gradient norm: 0.287609
=== Actor Training Debug (Iteration 3850) ===
Q mean: -12.340526
Q std: 16.783995
Actor loss: 12.344519
Action reg: 0.003993
  l1.weight: grad_norm = 0.045506
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.032768
Total gradient norm: 0.107733
=== Actor Training Debug (Iteration 3851) ===
Q mean: -11.036430
Q std: 15.441298
Actor loss: 11.040413
Action reg: 0.003983
  l1.weight: grad_norm = 0.159031
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.095104
Total gradient norm: 0.316136
=== Actor Training Debug (Iteration 3852) ===
Q mean: -12.044114
Q std: 17.570566
Actor loss: 12.048095
Action reg: 0.003980
  l1.weight: grad_norm = 0.051366
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.036724
Total gradient norm: 0.140952
=== Actor Training Debug (Iteration 3853) ===
Q mean: -14.525335
Q std: 17.737492
Actor loss: 14.529322
Action reg: 0.003986
  l1.weight: grad_norm = 0.008746
  l1.bias: grad_norm = 0.000619
  l2.weight: grad_norm = 0.007436
Total gradient norm: 0.028913
=== Actor Training Debug (Iteration 3854) ===
Q mean: -12.647756
Q std: 16.223951
Actor loss: 12.651744
Action reg: 0.003989
  l1.weight: grad_norm = 0.067793
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.055918
Total gradient norm: 0.234517
=== Actor Training Debug (Iteration 3855) ===
Q mean: -12.656201
Q std: 16.854393
Actor loss: 12.660191
Action reg: 0.003990
  l1.weight: grad_norm = 0.040829
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.025176
Total gradient norm: 0.084173
=== Actor Training Debug (Iteration 3856) ===
Q mean: -11.986637
Q std: 16.939440
Actor loss: 11.990614
Action reg: 0.003977
  l1.weight: grad_norm = 0.097791
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.075368
Total gradient norm: 0.310633
=== Actor Training Debug (Iteration 3857) ===
Q mean: -13.221968
Q std: 16.943016
Actor loss: 13.225952
Action reg: 0.003984
  l1.weight: grad_norm = 0.163685
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.139829
Total gradient norm: 0.525429
=== Actor Training Debug (Iteration 3858) ===
Q mean: -13.292179
Q std: 17.274391
Actor loss: 13.296169
Action reg: 0.003990
  l1.weight: grad_norm = 0.081229
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.069398
Total gradient norm: 0.292620
=== Actor Training Debug (Iteration 3859) ===
Q mean: -13.191978
Q std: 16.414301
Actor loss: 13.195965
Action reg: 0.003987
  l1.weight: grad_norm = 0.093223
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.093958
Total gradient norm: 0.402742
=== Actor Training Debug (Iteration 3860) ===
Q mean: -12.045197
Q std: 16.563288
Actor loss: 12.049179
Action reg: 0.003981
  l1.weight: grad_norm = 0.069636
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.061208
Total gradient norm: 0.204012
=== Actor Training Debug (Iteration 3861) ===
Q mean: -12.786022
Q std: 16.704582
Actor loss: 12.790009
Action reg: 0.003986
  l1.weight: grad_norm = 0.133416
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.090751
Total gradient norm: 0.338353
=== Actor Training Debug (Iteration 3862) ===
Q mean: -15.125197
Q std: 17.195927
Actor loss: 15.129179
Action reg: 0.003982
  l1.weight: grad_norm = 0.113596
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.065146
Total gradient norm: 0.195281
=== Actor Training Debug (Iteration 3863) ===
Q mean: -13.395236
Q std: 17.201897
Actor loss: 13.399227
Action reg: 0.003991
  l1.weight: grad_norm = 0.059355
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.051437
Total gradient norm: 0.149673
=== Actor Training Debug (Iteration 3864) ===
Q mean: -12.420797
Q std: 16.021351
Actor loss: 12.424790
Action reg: 0.003993
  l1.weight: grad_norm = 0.068980
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.044822
Total gradient norm: 0.137806
=== Actor Training Debug (Iteration 3865) ===
Q mean: -12.895075
Q std: 16.927446
Actor loss: 12.899060
Action reg: 0.003985
  l1.weight: grad_norm = 0.163147
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.143072
Total gradient norm: 0.519711
=== Actor Training Debug (Iteration 3866) ===
Q mean: -11.322994
Q std: 16.795998
Actor loss: 11.326975
Action reg: 0.003981
  l1.weight: grad_norm = 0.119439
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.088655
Total gradient norm: 0.380241
=== Actor Training Debug (Iteration 3867) ===
Q mean: -12.831423
Q std: 17.751574
Actor loss: 12.835415
Action reg: 0.003992
  l1.weight: grad_norm = 0.271341
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.243199
Total gradient norm: 0.705418
=== Actor Training Debug (Iteration 3868) ===
Q mean: -12.552744
Q std: 16.152134
Actor loss: 12.556725
Action reg: 0.003980
  l1.weight: grad_norm = 0.111606
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.069564
Total gradient norm: 0.214383
=== Actor Training Debug (Iteration 3869) ===
Q mean: -11.699289
Q std: 15.659162
Actor loss: 11.703278
Action reg: 0.003988
  l1.weight: grad_norm = 0.141324
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.081841
Total gradient norm: 0.301769
=== Actor Training Debug (Iteration 3870) ===
Q mean: -11.121889
Q std: 15.022395
Actor loss: 11.125878
Action reg: 0.003989
  l1.weight: grad_norm = 0.144238
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.112849
Total gradient norm: 0.363870
=== Actor Training Debug (Iteration 3871) ===
Q mean: -12.984776
Q std: 17.458479
Actor loss: 12.988754
Action reg: 0.003977
  l1.weight: grad_norm = 0.135499
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.114920
Total gradient norm: 0.410439
=== Actor Training Debug (Iteration 3872) ===
Q mean: -12.898208
Q std: 17.204178
Actor loss: 12.902185
Action reg: 0.003978
  l1.weight: grad_norm = 0.087943
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.087693
Total gradient norm: 0.431115
=== Actor Training Debug (Iteration 3873) ===
Q mean: -10.561587
Q std: 15.411962
Actor loss: 10.565569
Action reg: 0.003982
  l1.weight: grad_norm = 0.072112
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.050896
Total gradient norm: 0.220901
=== Actor Training Debug (Iteration 3874) ===
Q mean: -11.936887
Q std: 16.736891
Actor loss: 11.940864
Action reg: 0.003977
  l1.weight: grad_norm = 0.200806
  l1.bias: grad_norm = 0.000565
  l2.weight: grad_norm = 0.107981
Total gradient norm: 0.355344
=== Actor Training Debug (Iteration 3875) ===
Q mean: -12.732816
Q std: 16.421383
Actor loss: 12.736804
Action reg: 0.003988
  l1.weight: grad_norm = 0.101372
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.075777
Total gradient norm: 0.284221
=== Actor Training Debug (Iteration 3876) ===
Q mean: -13.588408
Q std: 16.931389
Actor loss: 13.592391
Action reg: 0.003984
  l1.weight: grad_norm = 0.144182
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.091832
Total gradient norm: 0.327291
=== Actor Training Debug (Iteration 3877) ===
Q mean: -13.816607
Q std: 17.496552
Actor loss: 13.820596
Action reg: 0.003989
  l1.weight: grad_norm = 0.035430
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.025680
Total gradient norm: 0.114008
=== Actor Training Debug (Iteration 3878) ===
Q mean: -13.992216
Q std: 17.939798
Actor loss: 13.996205
Action reg: 0.003990
  l1.weight: grad_norm = 0.078439
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.051992
Total gradient norm: 0.159277
=== Actor Training Debug (Iteration 3879) ===
Q mean: -14.697820
Q std: 17.870842
Actor loss: 14.701803
Action reg: 0.003984
  l1.weight: grad_norm = 0.078841
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.053690
Total gradient norm: 0.208666
=== Actor Training Debug (Iteration 3880) ===
Q mean: -11.796472
Q std: 16.206587
Actor loss: 11.800453
Action reg: 0.003982
  l1.weight: grad_norm = 0.066045
  l1.bias: grad_norm = 0.001330
  l2.weight: grad_norm = 0.044065
Total gradient norm: 0.170512
=== Actor Training Debug (Iteration 3881) ===
Q mean: -14.015719
Q std: 16.798559
Actor loss: 14.019701
Action reg: 0.003982
  l1.weight: grad_norm = 0.066663
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.042486
Total gradient norm: 0.151373
=== Actor Training Debug (Iteration 3882) ===
Q mean: -12.410793
Q std: 16.623968
Actor loss: 12.414783
Action reg: 0.003991
  l1.weight: grad_norm = 0.033829
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.023380
Total gradient norm: 0.104609
=== Actor Training Debug (Iteration 3883) ===
Q mean: -11.717987
Q std: 15.793614
Actor loss: 11.721973
Action reg: 0.003987
  l1.weight: grad_norm = 0.082724
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.056566
Total gradient norm: 0.193210
=== Actor Training Debug (Iteration 3884) ===
Q mean: -14.788128
Q std: 17.585094
Actor loss: 14.792115
Action reg: 0.003987
  l1.weight: grad_norm = 0.090520
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.074335
Total gradient norm: 0.222347
=== Actor Training Debug (Iteration 3885) ===
Q mean: -14.829315
Q std: 17.513018
Actor loss: 14.833306
Action reg: 0.003991
  l1.weight: grad_norm = 0.062751
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.043532
Total gradient norm: 0.121007
=== Actor Training Debug (Iteration 3886) ===
Q mean: -12.985073
Q std: 17.183847
Actor loss: 12.989055
Action reg: 0.003982
  l1.weight: grad_norm = 0.109732
  l1.bias: grad_norm = 0.000524
  l2.weight: grad_norm = 0.067797
Total gradient norm: 0.246814
=== Actor Training Debug (Iteration 3887) ===
Q mean: -11.823280
Q std: 16.886766
Actor loss: 11.827261
Action reg: 0.003981
  l1.weight: grad_norm = 0.088763
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.079665
Total gradient norm: 0.233535
=== Actor Training Debug (Iteration 3888) ===
Q mean: -12.701115
Q std: 16.951050
Actor loss: 12.705102
Action reg: 0.003988
  l1.weight: grad_norm = 0.113992
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.080689
Total gradient norm: 0.258961
=== Actor Training Debug (Iteration 3889) ===
Q mean: -13.061226
Q std: 16.517872
Actor loss: 13.065206
Action reg: 0.003979
  l1.weight: grad_norm = 0.042658
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.035163
Total gradient norm: 0.107408
=== Actor Training Debug (Iteration 3890) ===
Q mean: -11.977347
Q std: 16.488604
Actor loss: 11.981330
Action reg: 0.003982
  l1.weight: grad_norm = 0.140126
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.087608
Total gradient norm: 0.284505
=== Actor Training Debug (Iteration 3891) ===
Q mean: -12.102207
Q std: 16.569185
Actor loss: 12.106193
Action reg: 0.003985
  l1.weight: grad_norm = 0.147203
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.095828
Total gradient norm: 0.379783
=== Actor Training Debug (Iteration 3892) ===
Q mean: -13.282343
Q std: 16.885441
Actor loss: 13.286329
Action reg: 0.003986
  l1.weight: grad_norm = 0.043327
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.027277
Total gradient norm: 0.103395
=== Actor Training Debug (Iteration 3893) ===
Q mean: -12.217229
Q std: 16.567417
Actor loss: 12.221223
Action reg: 0.003994
  l1.weight: grad_norm = 0.049042
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.033578
Total gradient norm: 0.123237
=== Actor Training Debug (Iteration 3894) ===
Q mean: -12.248890
Q std: 16.121607
Actor loss: 12.252885
Action reg: 0.003995
  l1.weight: grad_norm = 0.028123
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.023344
Total gradient norm: 0.074115
=== Actor Training Debug (Iteration 3895) ===
Q mean: -13.623322
Q std: 17.495213
Actor loss: 13.627305
Action reg: 0.003983
  l1.weight: grad_norm = 0.155226
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.090578
Total gradient norm: 0.303179
=== Actor Training Debug (Iteration 3896) ===
Q mean: -12.886470
Q std: 16.644663
Actor loss: 12.890450
Action reg: 0.003980
  l1.weight: grad_norm = 0.135050
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.103155
Total gradient norm: 0.337418
=== Actor Training Debug (Iteration 3897) ===
Q mean: -12.476870
Q std: 16.709593
Actor loss: 12.480851
Action reg: 0.003982
  l1.weight: grad_norm = 0.049512
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.037705
Total gradient norm: 0.137118
=== Actor Training Debug (Iteration 3898) ===
Q mean: -12.996205
Q std: 17.921463
Actor loss: 13.000189
Action reg: 0.003983
  l1.weight: grad_norm = 0.097022
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.071856
Total gradient norm: 0.236878
=== Actor Training Debug (Iteration 3899) ===
Q mean: -11.844961
Q std: 16.057987
Actor loss: 11.848952
Action reg: 0.003991
  l1.weight: grad_norm = 0.066116
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.052703
Total gradient norm: 0.167029
=== Actor Training Debug (Iteration 3900) ===
Q mean: -11.220042
Q std: 15.606604
Actor loss: 11.224026
Action reg: 0.003984
  l1.weight: grad_norm = 0.134556
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.091655
Total gradient norm: 0.329428
=== Actor Training Debug (Iteration 3901) ===
Q mean: -14.081900
Q std: 17.792532
Actor loss: 14.085890
Action reg: 0.003990
  l1.weight: grad_norm = 0.031577
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.025237
Total gradient norm: 0.087573
=== Actor Training Debug (Iteration 3902) ===
Q mean: -11.947304
Q std: 16.617640
Actor loss: 11.951292
Action reg: 0.003988
  l1.weight: grad_norm = 0.087872
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.072294
Total gradient norm: 0.350836
=== Actor Training Debug (Iteration 3903) ===
Q mean: -12.649302
Q std: 17.371677
Actor loss: 12.653293
Action reg: 0.003991
  l1.weight: grad_norm = 0.032183
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.020205
Total gradient norm: 0.071711
=== Actor Training Debug (Iteration 3904) ===
Q mean: -13.427013
Q std: 17.628504
Actor loss: 13.431008
Action reg: 0.003995
  l1.weight: grad_norm = 0.068648
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.051293
Total gradient norm: 0.159018
=== Actor Training Debug (Iteration 3905) ===
Q mean: -12.213970
Q std: 16.755533
Actor loss: 12.217947
Action reg: 0.003977
  l1.weight: grad_norm = 0.102304
  l1.bias: grad_norm = 0.000786
  l2.weight: grad_norm = 0.070819
Total gradient norm: 0.264857
=== Actor Training Debug (Iteration 3906) ===
Q mean: -13.017561
Q std: 17.161238
Actor loss: 13.021541
Action reg: 0.003980
  l1.weight: grad_norm = 0.106466
  l1.bias: grad_norm = 0.000983
  l2.weight: grad_norm = 0.087517
Total gradient norm: 0.284584
=== Actor Training Debug (Iteration 3907) ===
Q mean: -12.984076
Q std: 17.025873
Actor loss: 12.988065
Action reg: 0.003989
  l1.weight: grad_norm = 0.186508
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.115356
Total gradient norm: 0.385476
=== Actor Training Debug (Iteration 3908) ===
Q mean: -11.838326
Q std: 16.288734
Actor loss: 11.842301
Action reg: 0.003976
  l1.weight: grad_norm = 0.054752
  l1.bias: grad_norm = 0.000575
  l2.weight: grad_norm = 0.036353
Total gradient norm: 0.135279
=== Actor Training Debug (Iteration 3909) ===
Q mean: -12.419073
Q std: 16.677736
Actor loss: 12.423057
Action reg: 0.003984
  l1.weight: grad_norm = 0.226600
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.135019
Total gradient norm: 0.646725
=== Actor Training Debug (Iteration 3910) ===
Q mean: -13.968691
Q std: 18.119755
Actor loss: 13.972672
Action reg: 0.003981
  l1.weight: grad_norm = 0.124736
  l1.bias: grad_norm = 0.000744
  l2.weight: grad_norm = 0.067424
Total gradient norm: 0.205371
=== Actor Training Debug (Iteration 3911) ===
Q mean: -12.843526
Q std: 17.154907
Actor loss: 12.847509
Action reg: 0.003983
  l1.weight: grad_norm = 0.113421
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.077982
Total gradient norm: 0.237242
=== Actor Training Debug (Iteration 3912) ===
Q mean: -13.472660
Q std: 17.454031
Actor loss: 13.476644
Action reg: 0.003984
  l1.weight: grad_norm = 0.049266
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.045201
Total gradient norm: 0.209693
=== Actor Training Debug (Iteration 3913) ===
Q mean: -11.090879
Q std: 15.882217
Actor loss: 11.094865
Action reg: 0.003985
  l1.weight: grad_norm = 0.053763
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.037205
Total gradient norm: 0.147328
=== Actor Training Debug (Iteration 3914) ===
Q mean: -10.631840
Q std: 15.526628
Actor loss: 10.635829
Action reg: 0.003989
  l1.weight: grad_norm = 0.116138
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.080157
Total gradient norm: 0.240808
=== Actor Training Debug (Iteration 3915) ===
Q mean: -14.232653
Q std: 18.092674
Actor loss: 14.236628
Action reg: 0.003975
  l1.weight: grad_norm = 0.050677
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.038715
Total gradient norm: 0.155364
=== Actor Training Debug (Iteration 3916) ===
Q mean: -12.336195
Q std: 16.173887
Actor loss: 12.340177
Action reg: 0.003982
  l1.weight: grad_norm = 0.063285
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.048299
Total gradient norm: 0.144457
=== Actor Training Debug (Iteration 3917) ===
Q mean: -13.592436
Q std: 18.287704
Actor loss: 13.596414
Action reg: 0.003978
  l1.weight: grad_norm = 0.109597
  l1.bias: grad_norm = 0.000492
  l2.weight: grad_norm = 0.091708
Total gradient norm: 0.378543
=== Actor Training Debug (Iteration 3918) ===
Q mean: -12.886402
Q std: 17.129293
Actor loss: 12.890379
Action reg: 0.003977
  l1.weight: grad_norm = 0.251620
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.184536
Total gradient norm: 0.786959
=== Actor Training Debug (Iteration 3919) ===
Q mean: -13.849502
Q std: 17.487862
Actor loss: 13.853492
Action reg: 0.003990
  l1.weight: grad_norm = 0.044982
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.032884
Total gradient norm: 0.101871
=== Actor Training Debug (Iteration 3920) ===
Q mean: -12.345503
Q std: 16.878531
Actor loss: 12.349483
Action reg: 0.003980
  l1.weight: grad_norm = 0.338135
  l1.bias: grad_norm = 0.000616
  l2.weight: grad_norm = 0.233271
Total gradient norm: 0.949981
=== Actor Training Debug (Iteration 3921) ===
Q mean: -13.625563
Q std: 17.096008
Actor loss: 13.629535
Action reg: 0.003972
  l1.weight: grad_norm = 0.049277
  l1.bias: grad_norm = 0.000626
  l2.weight: grad_norm = 0.037180
Total gradient norm: 0.118853
=== Actor Training Debug (Iteration 3922) ===
Q mean: -13.017996
Q std: 16.957975
Actor loss: 13.021976
Action reg: 0.003981
  l1.weight: grad_norm = 0.084957
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.064244
Total gradient norm: 0.243162
=== Actor Training Debug (Iteration 3923) ===
Q mean: -13.553589
Q std: 17.428659
Actor loss: 13.557565
Action reg: 0.003976
  l1.weight: grad_norm = 0.104414
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.072558
Total gradient norm: 0.324875
=== Actor Training Debug (Iteration 3924) ===
Q mean: -11.949639
Q std: 16.666351
Actor loss: 11.953626
Action reg: 0.003986
  l1.weight: grad_norm = 0.174841
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.134560
Total gradient norm: 0.551887
=== Actor Training Debug (Iteration 3925) ===
Q mean: -11.719733
Q std: 16.456856
Actor loss: 11.723720
Action reg: 0.003987
  l1.weight: grad_norm = 0.092034
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.064330
Total gradient norm: 0.207245
=== Actor Training Debug (Iteration 3926) ===
Q mean: -11.575692
Q std: 16.491970
Actor loss: 11.579681
Action reg: 0.003990
  l1.weight: grad_norm = 0.088329
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.058316
Total gradient norm: 0.197349
=== Actor Training Debug (Iteration 3927) ===
Q mean: -12.047432
Q std: 17.218224
Actor loss: 12.051414
Action reg: 0.003982
  l1.weight: grad_norm = 0.073441
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.056759
Total gradient norm: 0.233071
=== Actor Training Debug (Iteration 3928) ===
Q mean: -12.365507
Q std: 16.596380
Actor loss: 12.369491
Action reg: 0.003983
  l1.weight: grad_norm = 0.105415
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.087733
Total gradient norm: 0.377082
=== Actor Training Debug (Iteration 3929) ===
Q mean: -14.908037
Q std: 17.176123
Actor loss: 14.912025
Action reg: 0.003988
  l1.weight: grad_norm = 0.097879
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.075727
Total gradient norm: 0.315030
=== Actor Training Debug (Iteration 3930) ===
Q mean: -14.898233
Q std: 18.436262
Actor loss: 14.902218
Action reg: 0.003985
  l1.weight: grad_norm = 0.060360
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.049505
Total gradient norm: 0.151038
=== Actor Training Debug (Iteration 3931) ===
Q mean: -13.025547
Q std: 16.584068
Actor loss: 13.029531
Action reg: 0.003984
  l1.weight: grad_norm = 0.018541
  l1.bias: grad_norm = 0.000837
  l2.weight: grad_norm = 0.015609
Total gradient norm: 0.070743
=== Actor Training Debug (Iteration 3932) ===
Q mean: -14.203059
Q std: 17.262184
Actor loss: 14.207038
Action reg: 0.003979
  l1.weight: grad_norm = 0.108342
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.093296
Total gradient norm: 0.344828
=== Actor Training Debug (Iteration 3933) ===
Q mean: -13.334885
Q std: 17.137825
Actor loss: 13.338877
Action reg: 0.003992
  l1.weight: grad_norm = 0.493712
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.363117
Total gradient norm: 1.633000
=== Actor Training Debug (Iteration 3934) ===
Q mean: -13.200041
Q std: 17.906290
Actor loss: 13.204032
Action reg: 0.003991
  l1.weight: grad_norm = 0.080308
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.056963
Total gradient norm: 0.172546
=== Actor Training Debug (Iteration 3935) ===
Q mean: -14.125845
Q std: 18.521114
Actor loss: 14.129833
Action reg: 0.003988
  l1.weight: grad_norm = 0.134529
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.095498
Total gradient norm: 0.283187
=== Actor Training Debug (Iteration 3936) ===
Q mean: -11.721021
Q std: 16.860546
Actor loss: 11.725009
Action reg: 0.003988
  l1.weight: grad_norm = 0.065997
  l1.bias: grad_norm = 0.000425
  l2.weight: grad_norm = 0.055478
Total gradient norm: 0.142858
=== Actor Training Debug (Iteration 3937) ===
Q mean: -13.024652
Q std: 17.222582
Actor loss: 13.028644
Action reg: 0.003991
  l1.weight: grad_norm = 0.047815
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.038885
Total gradient norm: 0.156314
=== Actor Training Debug (Iteration 3938) ===
Q mean: -12.268828
Q std: 17.675013
Actor loss: 12.272812
Action reg: 0.003984
  l1.weight: grad_norm = 0.069979
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.058688
Total gradient norm: 0.236531
=== Actor Training Debug (Iteration 3939) ===
Q mean: -13.414051
Q std: 16.064384
Actor loss: 13.418036
Action reg: 0.003985
  l1.weight: grad_norm = 0.139640
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.087663
Total gradient norm: 0.297827
=== Actor Training Debug (Iteration 3940) ===
Q mean: -10.271601
Q std: 15.189017
Actor loss: 10.275580
Action reg: 0.003980
  l1.weight: grad_norm = 0.127278
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.103175
Total gradient norm: 0.303875
=== Actor Training Debug (Iteration 3941) ===
Q mean: -14.069245
Q std: 17.777855
Actor loss: 14.073226
Action reg: 0.003981
  l1.weight: grad_norm = 0.047581
  l1.bias: grad_norm = 0.000677
  l2.weight: grad_norm = 0.031362
Total gradient norm: 0.098967
=== Actor Training Debug (Iteration 3942) ===
Q mean: -11.233109
Q std: 15.821165
Actor loss: 11.237088
Action reg: 0.003980
  l1.weight: grad_norm = 0.081842
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.058031
Total gradient norm: 0.243203
=== Actor Training Debug (Iteration 3943) ===
Q mean: -12.865683
Q std: 17.222805
Actor loss: 12.869664
Action reg: 0.003982
  l1.weight: grad_norm = 0.109800
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.083432
Total gradient norm: 0.258882
=== Actor Training Debug (Iteration 3944) ===
Q mean: -13.348696
Q std: 16.987259
Actor loss: 13.352682
Action reg: 0.003986
  l1.weight: grad_norm = 0.130428
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.119723
Total gradient norm: 0.488452
=== Actor Training Debug (Iteration 3945) ===
Q mean: -12.905823
Q std: 17.570065
Actor loss: 12.909809
Action reg: 0.003986
  l1.weight: grad_norm = 0.089169
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.065579
Total gradient norm: 0.234137
=== Actor Training Debug (Iteration 3946) ===
Q mean: -13.772889
Q std: 17.395411
Actor loss: 13.776876
Action reg: 0.003988
  l1.weight: grad_norm = 0.179052
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.100729
Total gradient norm: 0.328707
=== Actor Training Debug (Iteration 3947) ===
Q mean: -11.765181
Q std: 16.920715
Actor loss: 11.769163
Action reg: 0.003983
  l1.weight: grad_norm = 0.115560
  l1.bias: grad_norm = 0.000976
  l2.weight: grad_norm = 0.106203
Total gradient norm: 0.325837
=== Actor Training Debug (Iteration 3948) ===
Q mean: -15.647274
Q std: 17.660620
Actor loss: 15.651267
Action reg: 0.003993
  l1.weight: grad_norm = 0.047704
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.030443
Total gradient norm: 0.102816
=== Actor Training Debug (Iteration 3949) ===
Q mean: -12.909359
Q std: 16.409023
Actor loss: 12.913346
Action reg: 0.003988
  l1.weight: grad_norm = 0.081110
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.058974
Total gradient norm: 0.232890
=== Actor Training Debug (Iteration 3950) ===
Q mean: -11.821657
Q std: 16.418146
Actor loss: 11.825642
Action reg: 0.003984
  l1.weight: grad_norm = 0.079059
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.055359
Total gradient norm: 0.174444
=== Actor Training Debug (Iteration 3951) ===
Q mean: -12.747718
Q std: 17.913429
Actor loss: 12.751706
Action reg: 0.003988
  l1.weight: grad_norm = 0.183961
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.148175
Total gradient norm: 0.737972
=== Actor Training Debug (Iteration 3952) ===
Q mean: -13.087777
Q std: 17.641907
Actor loss: 13.091770
Action reg: 0.003993
  l1.weight: grad_norm = 0.041391
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.031551
Total gradient norm: 0.138597
=== Actor Training Debug (Iteration 3953) ===
Q mean: -13.720613
Q std: 18.040363
Actor loss: 13.724592
Action reg: 0.003979
  l1.weight: grad_norm = 0.059826
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.043914
Total gradient norm: 0.186871
=== Actor Training Debug (Iteration 3954) ===
Q mean: -12.853934
Q std: 17.332918
Actor loss: 12.857923
Action reg: 0.003989
  l1.weight: grad_norm = 0.099569
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.056255
Total gradient norm: 0.185713
=== Actor Training Debug (Iteration 3955) ===
Q mean: -14.052941
Q std: 18.300709
Actor loss: 14.056930
Action reg: 0.003989
  l1.weight: grad_norm = 0.050508
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.046655
Total gradient norm: 0.235579
=== Actor Training Debug (Iteration 3956) ===
Q mean: -14.696726
Q std: 17.146973
Actor loss: 14.700710
Action reg: 0.003984
  l1.weight: grad_norm = 0.173272
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.127462
Total gradient norm: 0.466737
=== Actor Training Debug (Iteration 3957) ===
Q mean: -12.126478
Q std: 17.130449
Actor loss: 12.130457
Action reg: 0.003979
  l1.weight: grad_norm = 0.181150
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.113102
Total gradient norm: 0.385344
=== Actor Training Debug (Iteration 3958) ===
Q mean: -10.865400
Q std: 16.059299
Actor loss: 10.869390
Action reg: 0.003990
  l1.weight: grad_norm = 0.050977
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.030505
Total gradient norm: 0.101191
=== Actor Training Debug (Iteration 3959) ===
Q mean: -11.418653
Q std: 16.533743
Actor loss: 11.422628
Action reg: 0.003975
  l1.weight: grad_norm = 0.093409
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.057013
Total gradient norm: 0.182672
=== Actor Training Debug (Iteration 3960) ===
Q mean: -14.863529
Q std: 18.744244
Actor loss: 14.867513
Action reg: 0.003983
  l1.weight: grad_norm = 0.109694
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.086276
Total gradient norm: 0.356270
=== Actor Training Debug (Iteration 3961) ===
Q mean: -12.452170
Q std: 16.315842
Actor loss: 12.456161
Action reg: 0.003990
  l1.weight: grad_norm = 0.056587
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.038985
Total gradient norm: 0.152900
=== Actor Training Debug (Iteration 3962) ===
Q mean: -12.994301
Q std: 17.523365
Actor loss: 12.998285
Action reg: 0.003984
  l1.weight: grad_norm = 0.056453
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.037802
Total gradient norm: 0.113084
=== Actor Training Debug (Iteration 3963) ===
Q mean: -13.788836
Q std: 17.519606
Actor loss: 13.792817
Action reg: 0.003980
  l1.weight: grad_norm = 0.028123
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.022045
Total gradient norm: 0.079806
=== Actor Training Debug (Iteration 3964) ===
Q mean: -13.127281
Q std: 17.602736
Actor loss: 13.131264
Action reg: 0.003983
  l1.weight: grad_norm = 0.071278
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.049777
Total gradient norm: 0.167764
=== Actor Training Debug (Iteration 3965) ===
Q mean: -11.775992
Q std: 16.555431
Actor loss: 11.779982
Action reg: 0.003989
  l1.weight: grad_norm = 0.046994
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.039955
Total gradient norm: 0.120683
=== Actor Training Debug (Iteration 3966) ===
Q mean: -11.843296
Q std: 16.866301
Actor loss: 11.847280
Action reg: 0.003984
  l1.weight: grad_norm = 0.137996
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.124972
Total gradient norm: 0.521175
=== Actor Training Debug (Iteration 3967) ===
Q mean: -14.398796
Q std: 17.483398
Actor loss: 14.402778
Action reg: 0.003982
  l1.weight: grad_norm = 0.123429
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.116170
Total gradient norm: 0.611577
=== Actor Training Debug (Iteration 3968) ===
Q mean: -13.709206
Q std: 17.581100
Actor loss: 13.713194
Action reg: 0.003988
  l1.weight: grad_norm = 0.071361
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.064366
Total gradient norm: 0.316052
=== Actor Training Debug (Iteration 3969) ===
Q mean: -12.952900
Q std: 17.455091
Actor loss: 12.956871
Action reg: 0.003971
  l1.weight: grad_norm = 0.084593
  l1.bias: grad_norm = 0.000613
  l2.weight: grad_norm = 0.057423
Total gradient norm: 0.188228
=== Actor Training Debug (Iteration 3970) ===
Q mean: -12.723807
Q std: 16.286875
Actor loss: 12.727788
Action reg: 0.003981
  l1.weight: grad_norm = 0.112753
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.075252
Total gradient norm: 0.237291
=== Actor Training Debug (Iteration 3971) ===
Q mean: -12.750059
Q std: 17.262875
Actor loss: 12.754047
Action reg: 0.003988
  l1.weight: grad_norm = 0.038126
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.028380
Total gradient norm: 0.104593
=== Actor Training Debug (Iteration 3972) ===
Q mean: -13.097345
Q std: 17.875113
Actor loss: 13.101319
Action reg: 0.003974
  l1.weight: grad_norm = 0.083224
  l1.bias: grad_norm = 0.000753
  l2.weight: grad_norm = 0.053547
Total gradient norm: 0.211097
=== Actor Training Debug (Iteration 3973) ===
Q mean: -13.362392
Q std: 17.141777
Actor loss: 13.366379
Action reg: 0.003986
  l1.weight: grad_norm = 0.199645
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.138691
Total gradient norm: 0.427297
=== Actor Training Debug (Iteration 3974) ===
Q mean: -12.290968
Q std: 16.116400
Actor loss: 12.294953
Action reg: 0.003985
  l1.weight: grad_norm = 0.150631
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.097285
Total gradient norm: 0.266441
=== Actor Training Debug (Iteration 3975) ===
Q mean: -15.443913
Q std: 18.641747
Actor loss: 15.447886
Action reg: 0.003973
  l1.weight: grad_norm = 0.122582
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.083299
Total gradient norm: 0.296934
=== Actor Training Debug (Iteration 3976) ===
Q mean: -13.213680
Q std: 17.308058
Actor loss: 13.217654
Action reg: 0.003974
  l1.weight: grad_norm = 0.240856
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.176061
Total gradient norm: 0.581203
=== Actor Training Debug (Iteration 3977) ===
Q mean: -11.594873
Q std: 16.779432
Actor loss: 11.598857
Action reg: 0.003983
  l1.weight: grad_norm = 0.083290
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.057584
Total gradient norm: 0.208073
=== Actor Training Debug (Iteration 3978) ===
Q mean: -13.360976
Q std: 16.901585
Actor loss: 13.364966
Action reg: 0.003990
  l1.weight: grad_norm = 0.059443
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.035459
Total gradient norm: 0.157987
=== Actor Training Debug (Iteration 3979) ===
Q mean: -11.922474
Q std: 16.660295
Actor loss: 11.926450
Action reg: 0.003976
  l1.weight: grad_norm = 0.107430
  l1.bias: grad_norm = 0.000546
  l2.weight: grad_norm = 0.076885
Total gradient norm: 0.230655
=== Actor Training Debug (Iteration 3980) ===
Q mean: -12.634883
Q std: 16.490721
Actor loss: 12.638865
Action reg: 0.003982
  l1.weight: grad_norm = 0.139948
  l1.bias: grad_norm = 0.000542
  l2.weight: grad_norm = 0.097411
Total gradient norm: 0.313354
=== Actor Training Debug (Iteration 3981) ===
Q mean: -10.161080
Q std: 16.016302
Actor loss: 10.165068
Action reg: 0.003987
  l1.weight: grad_norm = 0.076738
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.053983
Total gradient norm: 0.232195
=== Actor Training Debug (Iteration 3982) ===
Q mean: -12.817341
Q std: 16.011555
Actor loss: 12.821322
Action reg: 0.003981
  l1.weight: grad_norm = 0.078605
  l1.bias: grad_norm = 0.000728
  l2.weight: grad_norm = 0.059403
Total gradient norm: 0.219106
=== Actor Training Debug (Iteration 3983) ===
Q mean: -12.683000
Q std: 16.309734
Actor loss: 12.686982
Action reg: 0.003982
  l1.weight: grad_norm = 0.073050
  l1.bias: grad_norm = 0.000600
  l2.weight: grad_norm = 0.059666
Total gradient norm: 0.175651
=== Actor Training Debug (Iteration 3984) ===
Q mean: -11.419212
Q std: 16.868668
Actor loss: 11.423201
Action reg: 0.003988
  l1.weight: grad_norm = 0.081236
  l1.bias: grad_norm = 0.000748
  l2.weight: grad_norm = 0.049040
Total gradient norm: 0.161242
=== Actor Training Debug (Iteration 3985) ===
Q mean: -11.338433
Q std: 16.300627
Actor loss: 11.342413
Action reg: 0.003980
  l1.weight: grad_norm = 0.115528
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.092801
Total gradient norm: 0.335027
=== Actor Training Debug (Iteration 3986) ===
Q mean: -12.711770
Q std: 17.688976
Actor loss: 12.715750
Action reg: 0.003979
  l1.weight: grad_norm = 0.037837
  l1.bias: grad_norm = 0.000764
  l2.weight: grad_norm = 0.024885
Total gradient norm: 0.087279
=== Actor Training Debug (Iteration 3987) ===
Q mean: -13.789007
Q std: 18.002642
Actor loss: 13.792979
Action reg: 0.003972
  l1.weight: grad_norm = 0.093609
  l1.bias: grad_norm = 0.000868
  l2.weight: grad_norm = 0.069976
Total gradient norm: 0.253955
=== Actor Training Debug (Iteration 3988) ===
Q mean: -13.397955
Q std: 16.924488
Actor loss: 13.401937
Action reg: 0.003982
  l1.weight: grad_norm = 0.136872
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.086514
Total gradient norm: 0.315624
=== Actor Training Debug (Iteration 3989) ===
Q mean: -14.500397
Q std: 18.768759
Actor loss: 14.504384
Action reg: 0.003987
  l1.weight: grad_norm = 0.064223
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.042494
Total gradient norm: 0.155378
=== Actor Training Debug (Iteration 3990) ===
Q mean: -11.744141
Q std: 17.008057
Actor loss: 11.748122
Action reg: 0.003981
  l1.weight: grad_norm = 0.045776
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.039179
Total gradient norm: 0.127153
=== Actor Training Debug (Iteration 3991) ===
Q mean: -14.755087
Q std: 17.903502
Actor loss: 14.759066
Action reg: 0.003979
  l1.weight: grad_norm = 0.131829
  l1.bias: grad_norm = 0.000744
  l2.weight: grad_norm = 0.082727
Total gradient norm: 0.262007
=== Actor Training Debug (Iteration 3992) ===
Q mean: -10.651406
Q std: 16.380251
Actor loss: 10.655389
Action reg: 0.003983
  l1.weight: grad_norm = 0.032574
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.027811
Total gradient norm: 0.078100
=== Actor Training Debug (Iteration 3993) ===
Q mean: -10.747411
Q std: 15.852666
Actor loss: 10.751390
Action reg: 0.003979
  l1.weight: grad_norm = 0.094212
  l1.bias: grad_norm = 0.000712
  l2.weight: grad_norm = 0.062374
Total gradient norm: 0.202296
=== Actor Training Debug (Iteration 3994) ===
Q mean: -11.577858
Q std: 16.179127
Actor loss: 11.581837
Action reg: 0.003979
  l1.weight: grad_norm = 0.127786
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.087944
Total gradient norm: 0.256993
=== Actor Training Debug (Iteration 3995) ===
Q mean: -11.748817
Q std: 17.361130
Actor loss: 11.752789
Action reg: 0.003971
  l1.weight: grad_norm = 0.151829
  l1.bias: grad_norm = 0.000666
  l2.weight: grad_norm = 0.134714
Total gradient norm: 0.588775
=== Actor Training Debug (Iteration 3996) ===
Q mean: -15.788862
Q std: 18.284569
Actor loss: 15.792846
Action reg: 0.003984
  l1.weight: grad_norm = 0.077392
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.073760
Total gradient norm: 0.304552
=== Actor Training Debug (Iteration 3997) ===
Q mean: -12.631953
Q std: 17.623426
Actor loss: 12.635944
Action reg: 0.003991
  l1.weight: grad_norm = 0.022881
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.017358
Total gradient norm: 0.082571
=== Actor Training Debug (Iteration 3998) ===
Q mean: -12.940187
Q std: 16.994230
Actor loss: 12.944161
Action reg: 0.003974
  l1.weight: grad_norm = 0.069123
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.053875
Total gradient norm: 0.170136
=== Actor Training Debug (Iteration 3999) ===
Q mean: -13.862641
Q std: 17.740475
Actor loss: 13.866609
Action reg: 0.003967
  l1.weight: grad_norm = 0.126934
  l1.bias: grad_norm = 0.001110
  l2.weight: grad_norm = 0.096790
Total gradient norm: 0.450985
=== Actor Training Debug (Iteration 4000) ===
Q mean: -13.312756
Q std: 17.068430
Actor loss: 13.316740
Action reg: 0.003985
  l1.weight: grad_norm = 0.043683
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.028244
Total gradient norm: 0.092111
Step 9000: Critic Loss: 1.3062, Actor Loss: 13.3167, Q Value: -13.3128
  Average reward: -321.274 | Average length: 100.0
Evaluation at episode 90: -321.274
=== Actor Training Debug (Iteration 4001) ===
Q mean: -12.853043
Q std: 17.362087
Actor loss: 12.857026
Action reg: 0.003983
  l1.weight: grad_norm = 0.048671
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.041135
Total gradient norm: 0.121962
=== Actor Training Debug (Iteration 4002) ===
Q mean: -12.320642
Q std: 17.491711
Actor loss: 12.324614
Action reg: 0.003971
  l1.weight: grad_norm = 0.089194
  l1.bias: grad_norm = 0.000794
  l2.weight: grad_norm = 0.057977
Total gradient norm: 0.179918
=== Actor Training Debug (Iteration 4003) ===
Q mean: -12.164393
Q std: 16.780239
Actor loss: 12.168379
Action reg: 0.003985
  l1.weight: grad_norm = 0.071110
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.048550
Total gradient norm: 0.171317
=== Actor Training Debug (Iteration 4004) ===
Q mean: -13.838688
Q std: 17.537119
Actor loss: 13.842668
Action reg: 0.003980
  l1.weight: grad_norm = 0.059026
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.039689
Total gradient norm: 0.122404
=== Actor Training Debug (Iteration 4005) ===
Q mean: -12.890360
Q std: 17.871876
Actor loss: 12.894354
Action reg: 0.003994
  l1.weight: grad_norm = 0.094418
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.062530
Total gradient norm: 0.235079
=== Actor Training Debug (Iteration 4006) ===
Q mean: -14.721027
Q std: 18.073746
Actor loss: 14.724999
Action reg: 0.003972
  l1.weight: grad_norm = 0.098871
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.085044
Total gradient norm: 0.650368
=== Actor Training Debug (Iteration 4007) ===
Q mean: -13.816599
Q std: 17.832779
Actor loss: 13.820574
Action reg: 0.003975
  l1.weight: grad_norm = 0.168190
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.160837
Total gradient norm: 1.311648
=== Actor Training Debug (Iteration 4008) ===
Q mean: -14.066987
Q std: 16.957041
Actor loss: 14.070969
Action reg: 0.003981
  l1.weight: grad_norm = 0.214358
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.153719
Total gradient norm: 0.503599
=== Actor Training Debug (Iteration 4009) ===
Q mean: -11.732304
Q std: 16.805820
Actor loss: 11.736281
Action reg: 0.003978
  l1.weight: grad_norm = 0.272234
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.204681
Total gradient norm: 0.746592
=== Actor Training Debug (Iteration 4010) ===
Q mean: -11.781212
Q std: 16.934052
Actor loss: 11.785175
Action reg: 0.003963
  l1.weight: grad_norm = 0.169814
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.133477
Total gradient norm: 0.439598
=== Actor Training Debug (Iteration 4011) ===
Q mean: -12.264569
Q std: 16.759758
Actor loss: 12.268538
Action reg: 0.003968
  l1.weight: grad_norm = 0.142078
  l1.bias: grad_norm = 0.000518
  l2.weight: grad_norm = 0.124107
Total gradient norm: 0.578798
=== Actor Training Debug (Iteration 4012) ===
Q mean: -14.850167
Q std: 19.050587
Actor loss: 14.854138
Action reg: 0.003972
  l1.weight: grad_norm = 0.128111
  l1.bias: grad_norm = 0.000484
  l2.weight: grad_norm = 0.097199
Total gradient norm: 0.369834
=== Actor Training Debug (Iteration 4013) ===
Q mean: -13.728850
Q std: 18.326838
Actor loss: 13.732831
Action reg: 0.003981
  l1.weight: grad_norm = 0.281738
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.190388
Total gradient norm: 0.763685
=== Actor Training Debug (Iteration 4014) ===
Q mean: -13.455126
Q std: 17.888189
Actor loss: 13.459105
Action reg: 0.003978
  l1.weight: grad_norm = 0.085036
  l1.bias: grad_norm = 0.000528
  l2.weight: grad_norm = 0.062016
Total gradient norm: 0.302822
=== Actor Training Debug (Iteration 4015) ===
Q mean: -12.780275
Q std: 16.657728
Actor loss: 12.784233
Action reg: 0.003957
  l1.weight: grad_norm = 0.176877
  l1.bias: grad_norm = 0.001803
  l2.weight: grad_norm = 0.154480
Total gradient norm: 0.986875
=== Actor Training Debug (Iteration 4016) ===
Q mean: -14.113359
Q std: 17.411135
Actor loss: 14.117335
Action reg: 0.003976
  l1.weight: grad_norm = 0.095874
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.070649
Total gradient norm: 0.239087
=== Actor Training Debug (Iteration 4017) ===
Q mean: -12.119653
Q std: 17.167549
Actor loss: 12.123625
Action reg: 0.003972
  l1.weight: grad_norm = 0.130415
  l1.bias: grad_norm = 0.000405
  l2.weight: grad_norm = 0.110809
Total gradient norm: 0.546520
=== Actor Training Debug (Iteration 4018) ===
Q mean: -11.705242
Q std: 16.796736
Actor loss: 11.709219
Action reg: 0.003976
  l1.weight: grad_norm = 0.122585
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.082577
Total gradient norm: 0.445535
=== Actor Training Debug (Iteration 4019) ===
Q mean: -14.574468
Q std: 17.150377
Actor loss: 14.578421
Action reg: 0.003953
  l1.weight: grad_norm = 0.178930
  l1.bias: grad_norm = 0.001635
  l2.weight: grad_norm = 0.118340
Total gradient norm: 0.336574
=== Actor Training Debug (Iteration 4020) ===
Q mean: -13.555819
Q std: 17.390493
Actor loss: 13.559802
Action reg: 0.003983
  l1.weight: grad_norm = 0.089176
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.063788
Total gradient norm: 0.223642
=== Actor Training Debug (Iteration 4021) ===
Q mean: -13.430245
Q std: 17.736919
Actor loss: 13.434235
Action reg: 0.003990
  l1.weight: grad_norm = 0.099803
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.065228
Total gradient norm: 0.181923
=== Actor Training Debug (Iteration 4022) ===
Q mean: -12.981007
Q std: 17.138882
Actor loss: 12.984975
Action reg: 0.003968
  l1.weight: grad_norm = 0.213619
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.118628
Total gradient norm: 0.415832
=== Actor Training Debug (Iteration 4023) ===
Q mean: -14.734726
Q std: 17.193867
Actor loss: 14.738679
Action reg: 0.003953
  l1.weight: grad_norm = 0.081285
  l1.bias: grad_norm = 0.002015
  l2.weight: grad_norm = 0.071078
Total gradient norm: 0.250755
=== Actor Training Debug (Iteration 4024) ===
Q mean: -12.180484
Q std: 17.158625
Actor loss: 12.184456
Action reg: 0.003972
  l1.weight: grad_norm = 0.072922
  l1.bias: grad_norm = 0.000918
  l2.weight: grad_norm = 0.066736
Total gradient norm: 0.209478
=== Actor Training Debug (Iteration 4025) ===
Q mean: -12.275129
Q std: 16.020851
Actor loss: 12.279117
Action reg: 0.003987
  l1.weight: grad_norm = 0.120370
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.084240
Total gradient norm: 0.371336
=== Actor Training Debug (Iteration 4026) ===
Q mean: -13.791958
Q std: 17.831629
Actor loss: 13.795927
Action reg: 0.003970
  l1.weight: grad_norm = 0.162700
  l1.bias: grad_norm = 0.000724
  l2.weight: grad_norm = 0.115170
Total gradient norm: 0.619114
=== Actor Training Debug (Iteration 4027) ===
Q mean: -14.032801
Q std: 17.725351
Actor loss: 14.036771
Action reg: 0.003970
  l1.weight: grad_norm = 0.300816
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.207648
Total gradient norm: 1.052648
=== Actor Training Debug (Iteration 4028) ===
Q mean: -14.055271
Q std: 18.155075
Actor loss: 14.059231
Action reg: 0.003960
  l1.weight: grad_norm = 0.225224
  l1.bias: grad_norm = 0.001071
  l2.weight: grad_norm = 0.180647
Total gradient norm: 0.811715
=== Actor Training Debug (Iteration 4029) ===
Q mean: -12.099708
Q std: 16.412191
Actor loss: 12.103686
Action reg: 0.003979
  l1.weight: grad_norm = 0.052651
  l1.bias: grad_norm = 0.000930
  l2.weight: grad_norm = 0.034334
Total gradient norm: 0.116923
=== Actor Training Debug (Iteration 4030) ===
Q mean: -12.663799
Q std: 16.274860
Actor loss: 12.667761
Action reg: 0.003962
  l1.weight: grad_norm = 0.100861
  l1.bias: grad_norm = 0.000944
  l2.weight: grad_norm = 0.091567
Total gradient norm: 0.311499
=== Actor Training Debug (Iteration 4031) ===
Q mean: -13.401498
Q std: 17.753788
Actor loss: 13.405473
Action reg: 0.003975
  l1.weight: grad_norm = 0.147585
  l1.bias: grad_norm = 0.000578
  l2.weight: grad_norm = 0.111665
Total gradient norm: 0.407102
=== Actor Training Debug (Iteration 4032) ===
Q mean: -11.989479
Q std: 17.576277
Actor loss: 11.993461
Action reg: 0.003981
  l1.weight: grad_norm = 0.053049
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.041642
Total gradient norm: 0.125213
=== Actor Training Debug (Iteration 4033) ===
Q mean: -12.140220
Q std: 17.639879
Actor loss: 12.144197
Action reg: 0.003978
  l1.weight: grad_norm = 0.141081
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.107310
Total gradient norm: 0.379124
=== Actor Training Debug (Iteration 4034) ===
Q mean: -12.224596
Q std: 17.456303
Actor loss: 12.228556
Action reg: 0.003959
  l1.weight: grad_norm = 0.203227
  l1.bias: grad_norm = 0.001259
  l2.weight: grad_norm = 0.180313
Total gradient norm: 0.687889
=== Actor Training Debug (Iteration 4035) ===
Q mean: -11.557774
Q std: 15.801277
Actor loss: 11.561755
Action reg: 0.003982
  l1.weight: grad_norm = 0.124985
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.115358
Total gradient norm: 0.408717
=== Actor Training Debug (Iteration 4036) ===
Q mean: -13.132296
Q std: 17.836384
Actor loss: 13.136271
Action reg: 0.003976
  l1.weight: grad_norm = 0.150321
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.111467
Total gradient norm: 0.375200
=== Actor Training Debug (Iteration 4037) ===
Q mean: -11.063608
Q std: 15.900759
Actor loss: 11.067592
Action reg: 0.003983
  l1.weight: grad_norm = 0.194837
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.167665
Total gradient norm: 0.483658
=== Actor Training Debug (Iteration 4038) ===
Q mean: -14.327857
Q std: 17.181858
Actor loss: 14.331841
Action reg: 0.003984
  l1.weight: grad_norm = 0.195265
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.160887
Total gradient norm: 0.575474
=== Actor Training Debug (Iteration 4039) ===
Q mean: -12.216537
Q std: 17.324045
Actor loss: 12.220515
Action reg: 0.003979
  l1.weight: grad_norm = 0.207754
  l1.bias: grad_norm = 0.000584
  l2.weight: grad_norm = 0.214712
Total gradient norm: 0.994063
=== Actor Training Debug (Iteration 4040) ===
Q mean: -13.832656
Q std: 18.497435
Actor loss: 13.836628
Action reg: 0.003972
  l1.weight: grad_norm = 0.088768
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.074099
Total gradient norm: 0.243353
=== Actor Training Debug (Iteration 4041) ===
Q mean: -12.889547
Q std: 17.262653
Actor loss: 12.893529
Action reg: 0.003981
  l1.weight: grad_norm = 0.097155
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.084828
Total gradient norm: 0.259814
=== Actor Training Debug (Iteration 4042) ===
Q mean: -13.381133
Q std: 18.050755
Actor loss: 13.385115
Action reg: 0.003981
  l1.weight: grad_norm = 0.070139
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.057070
Total gradient norm: 0.209320
=== Actor Training Debug (Iteration 4043) ===
Q mean: -11.789566
Q std: 16.920288
Actor loss: 11.793546
Action reg: 0.003980
  l1.weight: grad_norm = 0.126599
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.089699
Total gradient norm: 0.327543
=== Actor Training Debug (Iteration 4044) ===
Q mean: -12.313012
Q std: 17.207705
Actor loss: 12.316983
Action reg: 0.003971
  l1.weight: grad_norm = 0.175742
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.129194
Total gradient norm: 0.416745
=== Actor Training Debug (Iteration 4045) ===
Q mean: -12.938409
Q std: 16.999079
Actor loss: 12.942389
Action reg: 0.003981
  l1.weight: grad_norm = 0.122585
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.089649
Total gradient norm: 0.263603
=== Actor Training Debug (Iteration 4046) ===
Q mean: -13.560489
Q std: 17.062105
Actor loss: 13.564473
Action reg: 0.003985
  l1.weight: grad_norm = 0.061417
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.050047
Total gradient norm: 0.204578
=== Actor Training Debug (Iteration 4047) ===
Q mean: -12.281620
Q std: 17.171642
Actor loss: 12.285606
Action reg: 0.003987
  l1.weight: grad_norm = 0.122832
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.096918
Total gradient norm: 0.278373
=== Actor Training Debug (Iteration 4048) ===
Q mean: -12.738811
Q std: 17.415340
Actor loss: 12.742788
Action reg: 0.003977
  l1.weight: grad_norm = 0.043326
  l1.bias: grad_norm = 0.000607
  l2.weight: grad_norm = 0.032690
Total gradient norm: 0.115483
=== Actor Training Debug (Iteration 4049) ===
Q mean: -14.157562
Q std: 17.534872
Actor loss: 14.161552
Action reg: 0.003990
  l1.weight: grad_norm = 0.048397
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.040388
Total gradient norm: 0.140260
=== Actor Training Debug (Iteration 4050) ===
Q mean: -13.246685
Q std: 17.557253
Actor loss: 13.250667
Action reg: 0.003981
  l1.weight: grad_norm = 0.025820
  l1.bias: grad_norm = 0.000743
  l2.weight: grad_norm = 0.020920
Total gradient norm: 0.086774
=== Actor Training Debug (Iteration 4051) ===
Q mean: -12.801363
Q std: 18.042257
Actor loss: 12.805346
Action reg: 0.003983
  l1.weight: grad_norm = 0.122879
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.095303
Total gradient norm: 0.277073
=== Actor Training Debug (Iteration 4052) ===
Q mean: -14.949414
Q std: 17.480707
Actor loss: 14.953400
Action reg: 0.003985
  l1.weight: grad_norm = 0.080832
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.049311
Total gradient norm: 0.173460
=== Actor Training Debug (Iteration 4053) ===
Q mean: -12.648506
Q std: 17.344900
Actor loss: 12.652483
Action reg: 0.003977
  l1.weight: grad_norm = 0.104044
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.074521
Total gradient norm: 0.251890
=== Actor Training Debug (Iteration 4054) ===
Q mean: -13.824492
Q std: 18.226301
Actor loss: 13.828457
Action reg: 0.003966
  l1.weight: grad_norm = 0.145475
  l1.bias: grad_norm = 0.001223
  l2.weight: grad_norm = 0.093455
Total gradient norm: 0.293455
=== Actor Training Debug (Iteration 4055) ===
Q mean: -13.357559
Q std: 17.240284
Actor loss: 13.361527
Action reg: 0.003968
  l1.weight: grad_norm = 0.113310
  l1.bias: grad_norm = 0.000733
  l2.weight: grad_norm = 0.076054
Total gradient norm: 0.278425
=== Actor Training Debug (Iteration 4056) ===
Q mean: -12.817036
Q std: 17.050751
Actor loss: 12.821012
Action reg: 0.003976
  l1.weight: grad_norm = 0.197806
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.175487
Total gradient norm: 0.737093
=== Actor Training Debug (Iteration 4057) ===
Q mean: -12.930218
Q std: 18.335875
Actor loss: 12.934198
Action reg: 0.003980
  l1.weight: grad_norm = 0.172525
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.120836
Total gradient norm: 0.471469
=== Actor Training Debug (Iteration 4058) ===
Q mean: -15.096401
Q std: 18.843403
Actor loss: 15.100390
Action reg: 0.003990
  l1.weight: grad_norm = 0.059300
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.044806
Total gradient norm: 0.145039
=== Actor Training Debug (Iteration 4059) ===
Q mean: -11.833508
Q std: 16.267298
Actor loss: 11.837496
Action reg: 0.003988
  l1.weight: grad_norm = 0.142955
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.098326
Total gradient norm: 0.365214
=== Actor Training Debug (Iteration 4060) ===
Q mean: -13.640276
Q std: 18.073156
Actor loss: 13.644258
Action reg: 0.003982
  l1.weight: grad_norm = 0.129670
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.097465
Total gradient norm: 0.347693
=== Actor Training Debug (Iteration 4061) ===
Q mean: -13.003676
Q std: 17.019825
Actor loss: 13.007656
Action reg: 0.003980
  l1.weight: grad_norm = 0.086879
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.058655
Total gradient norm: 0.222547
=== Actor Training Debug (Iteration 4062) ===
Q mean: -12.695932
Q std: 16.879152
Actor loss: 12.699903
Action reg: 0.003971
  l1.weight: grad_norm = 0.101094
  l1.bias: grad_norm = 0.000751
  l2.weight: grad_norm = 0.069901
Total gradient norm: 0.272143
=== Actor Training Debug (Iteration 4063) ===
Q mean: -12.876427
Q std: 16.137369
Actor loss: 12.880403
Action reg: 0.003976
  l1.weight: grad_norm = 0.124859
  l1.bias: grad_norm = 0.000682
  l2.weight: grad_norm = 0.087998
Total gradient norm: 0.300488
=== Actor Training Debug (Iteration 4064) ===
Q mean: -11.559848
Q std: 17.012886
Actor loss: 11.563826
Action reg: 0.003978
  l1.weight: grad_norm = 0.173413
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.118228
Total gradient norm: 0.406929
=== Actor Training Debug (Iteration 4065) ===
Q mean: -13.005814
Q std: 17.823246
Actor loss: 13.009770
Action reg: 0.003957
  l1.weight: grad_norm = 0.245677
  l1.bias: grad_norm = 0.002094
  l2.weight: grad_norm = 0.193017
Total gradient norm: 0.719670
=== Actor Training Debug (Iteration 4066) ===
Q mean: -11.769773
Q std: 16.969063
Actor loss: 11.773741
Action reg: 0.003967
  l1.weight: grad_norm = 0.102487
  l1.bias: grad_norm = 0.001079
  l2.weight: grad_norm = 0.086003
Total gradient norm: 0.317021
=== Actor Training Debug (Iteration 4067) ===
Q mean: -13.806772
Q std: 18.211868
Actor loss: 13.810748
Action reg: 0.003976
  l1.weight: grad_norm = 0.168400
  l1.bias: grad_norm = 0.000590
  l2.weight: grad_norm = 0.139552
Total gradient norm: 0.656876
=== Actor Training Debug (Iteration 4068) ===
Q mean: -12.548367
Q std: 16.556612
Actor loss: 12.552345
Action reg: 0.003979
  l1.weight: grad_norm = 0.125396
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.097661
Total gradient norm: 0.384572
=== Actor Training Debug (Iteration 4069) ===
Q mean: -13.574266
Q std: 17.335997
Actor loss: 13.578250
Action reg: 0.003983
  l1.weight: grad_norm = 0.083283
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.056912
Total gradient norm: 0.195979
=== Actor Training Debug (Iteration 4070) ===
Q mean: -11.601297
Q std: 16.699175
Actor loss: 11.605268
Action reg: 0.003971
  l1.weight: grad_norm = 0.086169
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.064246
Total gradient norm: 0.292681
=== Actor Training Debug (Iteration 4071) ===
Q mean: -12.050053
Q std: 15.698924
Actor loss: 12.054040
Action reg: 0.003987
  l1.weight: grad_norm = 0.203074
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.110603
Total gradient norm: 0.321463
=== Actor Training Debug (Iteration 4072) ===
Q mean: -10.803974
Q std: 16.822073
Actor loss: 10.807954
Action reg: 0.003979
  l1.weight: grad_norm = 0.197561
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.134490
Total gradient norm: 0.506373
=== Actor Training Debug (Iteration 4073) ===
Q mean: -12.288286
Q std: 16.944521
Actor loss: 12.292256
Action reg: 0.003970
  l1.weight: grad_norm = 0.199265
  l1.bias: grad_norm = 0.000990
  l2.weight: grad_norm = 0.159456
Total gradient norm: 0.559654
=== Actor Training Debug (Iteration 4074) ===
Q mean: -14.456612
Q std: 18.190197
Actor loss: 14.460593
Action reg: 0.003982
  l1.weight: grad_norm = 0.067141
  l1.bias: grad_norm = 0.000756
  l2.weight: grad_norm = 0.047744
Total gradient norm: 0.165863
=== Actor Training Debug (Iteration 4075) ===
Q mean: -14.735653
Q std: 17.710087
Actor loss: 14.739643
Action reg: 0.003991
  l1.weight: grad_norm = 0.101123
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.076211
Total gradient norm: 0.321710
=== Actor Training Debug (Iteration 4076) ===
Q mean: -11.840816
Q std: 17.046556
Actor loss: 11.844784
Action reg: 0.003967
  l1.weight: grad_norm = 0.093274
  l1.bias: grad_norm = 0.001121
  l2.weight: grad_norm = 0.070766
Total gradient norm: 0.304629
=== Actor Training Debug (Iteration 4077) ===
Q mean: -12.214048
Q std: 16.368868
Actor loss: 12.218022
Action reg: 0.003974
  l1.weight: grad_norm = 0.015348
  l1.bias: grad_norm = 0.001651
  l2.weight: grad_norm = 0.013300
Total gradient norm: 0.050924
=== Actor Training Debug (Iteration 4078) ===
Q mean: -12.550852
Q std: 18.394058
Actor loss: 12.554815
Action reg: 0.003964
  l1.weight: grad_norm = 0.189264
  l1.bias: grad_norm = 0.002238
  l2.weight: grad_norm = 0.128620
Total gradient norm: 0.411364
=== Actor Training Debug (Iteration 4079) ===
Q mean: -12.599848
Q std: 17.283468
Actor loss: 12.603822
Action reg: 0.003974
  l1.weight: grad_norm = 0.191070
  l1.bias: grad_norm = 0.000768
  l2.weight: grad_norm = 0.152243
Total gradient norm: 0.671815
=== Actor Training Debug (Iteration 4080) ===
Q mean: -14.583621
Q std: 16.731314
Actor loss: 14.587602
Action reg: 0.003981
  l1.weight: grad_norm = 0.119544
  l1.bias: grad_norm = 0.001260
  l2.weight: grad_norm = 0.093320
Total gradient norm: 0.433397
=== Actor Training Debug (Iteration 4081) ===
Q mean: -12.886147
Q std: 17.362976
Actor loss: 12.890127
Action reg: 0.003980
  l1.weight: grad_norm = 0.085321
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.066120
Total gradient norm: 0.260125
=== Actor Training Debug (Iteration 4082) ===
Q mean: -12.144663
Q std: 16.996407
Actor loss: 12.148628
Action reg: 0.003965
  l1.weight: grad_norm = 0.058887
  l1.bias: grad_norm = 0.001715
  l2.weight: grad_norm = 0.035868
Total gradient norm: 0.146058
=== Actor Training Debug (Iteration 4083) ===
Q mean: -12.355780
Q std: 17.299191
Actor loss: 12.359751
Action reg: 0.003971
  l1.weight: grad_norm = 0.102681
  l1.bias: grad_norm = 0.001058
  l2.weight: grad_norm = 0.091848
Total gradient norm: 0.234706
=== Actor Training Debug (Iteration 4084) ===
Q mean: -11.945396
Q std: 16.905081
Actor loss: 11.949366
Action reg: 0.003969
  l1.weight: grad_norm = 0.122611
  l1.bias: grad_norm = 0.001282
  l2.weight: grad_norm = 0.070556
Total gradient norm: 0.250515
=== Actor Training Debug (Iteration 4085) ===
Q mean: -11.384095
Q std: 16.863817
Actor loss: 11.388076
Action reg: 0.003981
  l1.weight: grad_norm = 0.097218
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.069596
Total gradient norm: 0.252408
=== Actor Training Debug (Iteration 4086) ===
Q mean: -12.004862
Q std: 17.385529
Actor loss: 12.008828
Action reg: 0.003967
  l1.weight: grad_norm = 0.082898
  l1.bias: grad_norm = 0.002603
  l2.weight: grad_norm = 0.061194
Total gradient norm: 0.215401
=== Actor Training Debug (Iteration 4087) ===
Q mean: -9.970513
Q std: 16.324120
Actor loss: 9.974466
Action reg: 0.003953
  l1.weight: grad_norm = 0.093171
  l1.bias: grad_norm = 0.001373
  l2.weight: grad_norm = 0.074146
Total gradient norm: 0.206661
=== Actor Training Debug (Iteration 4088) ===
Q mean: -13.297088
Q std: 17.798969
Actor loss: 13.301062
Action reg: 0.003974
  l1.weight: grad_norm = 0.123737
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.071871
Total gradient norm: 0.213844
=== Actor Training Debug (Iteration 4089) ===
Q mean: -13.663882
Q std: 17.449463
Actor loss: 13.667860
Action reg: 0.003978
  l1.weight: grad_norm = 0.073493
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.049215
Total gradient norm: 0.152589
=== Actor Training Debug (Iteration 4090) ===
Q mean: -15.698900
Q std: 18.570995
Actor loss: 15.702869
Action reg: 0.003969
  l1.weight: grad_norm = 0.027163
  l1.bias: grad_norm = 0.001314
  l2.weight: grad_norm = 0.019361
Total gradient norm: 0.068173
=== Actor Training Debug (Iteration 4091) ===
Q mean: -13.310760
Q std: 16.880821
Actor loss: 13.314731
Action reg: 0.003970
  l1.weight: grad_norm = 0.060235
  l1.bias: grad_norm = 0.001027
  l2.weight: grad_norm = 0.041330
Total gradient norm: 0.158785
=== Actor Training Debug (Iteration 4092) ===
Q mean: -10.521002
Q std: 15.649045
Actor loss: 10.524980
Action reg: 0.003978
  l1.weight: grad_norm = 0.100124
  l1.bias: grad_norm = 0.001051
  l2.weight: grad_norm = 0.061751
Total gradient norm: 0.213714
=== Actor Training Debug (Iteration 4093) ===
Q mean: -14.771078
Q std: 17.697660
Actor loss: 14.775064
Action reg: 0.003986
  l1.weight: grad_norm = 0.127009
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.103684
Total gradient norm: 0.302083
=== Actor Training Debug (Iteration 4094) ===
Q mean: -14.108931
Q std: 18.202675
Actor loss: 14.112888
Action reg: 0.003958
  l1.weight: grad_norm = 0.191689
  l1.bias: grad_norm = 0.001634
  l2.weight: grad_norm = 0.133101
Total gradient norm: 0.494027
=== Actor Training Debug (Iteration 4095) ===
Q mean: -13.129489
Q std: 17.238102
Actor loss: 13.133461
Action reg: 0.003972
  l1.weight: grad_norm = 0.089324
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.076275
Total gradient norm: 0.238725
=== Actor Training Debug (Iteration 4096) ===
Q mean: -13.380489
Q std: 18.247837
Actor loss: 13.384467
Action reg: 0.003978
  l1.weight: grad_norm = 0.131124
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.108584
Total gradient norm: 0.359752
=== Actor Training Debug (Iteration 4097) ===
Q mean: -12.947322
Q std: 17.244701
Actor loss: 12.951287
Action reg: 0.003966
  l1.weight: grad_norm = 0.066912
  l1.bias: grad_norm = 0.001901
  l2.weight: grad_norm = 0.064445
Total gradient norm: 0.219373
=== Actor Training Debug (Iteration 4098) ===
Q mean: -11.996214
Q std: 17.351601
Actor loss: 12.000178
Action reg: 0.003964
  l1.weight: grad_norm = 0.171481
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.121847
Total gradient norm: 0.411364
=== Actor Training Debug (Iteration 4099) ===
Q mean: -11.745539
Q std: 15.814704
Actor loss: 11.749501
Action reg: 0.003962
  l1.weight: grad_norm = 0.088333
  l1.bias: grad_norm = 0.001275
  l2.weight: grad_norm = 0.073326
Total gradient norm: 0.244264
=== Actor Training Debug (Iteration 4100) ===
Q mean: -14.044367
Q std: 17.801100
Actor loss: 14.048354
Action reg: 0.003987
  l1.weight: grad_norm = 0.066538
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.047260
Total gradient norm: 0.162573
Episode 91: Steps=100, Reward=-286.391, Buffer_size=9100
=== Actor Training Debug (Iteration 4101) ===
Q mean: -12.957989
Q std: 17.971533
Actor loss: 12.961977
Action reg: 0.003988
  l1.weight: grad_norm = 0.067346
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.054721
Total gradient norm: 0.168296
=== Actor Training Debug (Iteration 4102) ===
Q mean: -12.539334
Q std: 17.936874
Actor loss: 12.543312
Action reg: 0.003978
  l1.weight: grad_norm = 0.018953
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.014445
Total gradient norm: 0.053725
=== Actor Training Debug (Iteration 4103) ===
Q mean: -13.958484
Q std: 18.185394
Actor loss: 13.962468
Action reg: 0.003984
  l1.weight: grad_norm = 0.054618
  l1.bias: grad_norm = 0.001556
  l2.weight: grad_norm = 0.047288
Total gradient norm: 0.190366
=== Actor Training Debug (Iteration 4104) ===
Q mean: -12.833378
Q std: 17.889303
Actor loss: 12.837346
Action reg: 0.003969
  l1.weight: grad_norm = 0.114943
  l1.bias: grad_norm = 0.001583
  l2.weight: grad_norm = 0.084572
Total gradient norm: 0.294290
=== Actor Training Debug (Iteration 4105) ===
Q mean: -13.202653
Q std: 17.468725
Actor loss: 13.206624
Action reg: 0.003971
  l1.weight: grad_norm = 0.111780
  l1.bias: grad_norm = 0.000981
  l2.weight: grad_norm = 0.095900
Total gradient norm: 0.315662
=== Actor Training Debug (Iteration 4106) ===
Q mean: -13.642962
Q std: 18.629246
Actor loss: 13.646950
Action reg: 0.003988
  l1.weight: grad_norm = 0.065993
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.046955
Total gradient norm: 0.171628
=== Actor Training Debug (Iteration 4107) ===
Q mean: -11.953772
Q std: 17.220770
Actor loss: 11.957745
Action reg: 0.003973
  l1.weight: grad_norm = 0.125865
  l1.bias: grad_norm = 0.000673
  l2.weight: grad_norm = 0.082817
Total gradient norm: 0.332338
=== Actor Training Debug (Iteration 4108) ===
Q mean: -12.482286
Q std: 17.130449
Actor loss: 12.486259
Action reg: 0.003972
  l1.weight: grad_norm = 0.131567
  l1.bias: grad_norm = 0.000839
  l2.weight: grad_norm = 0.112230
Total gradient norm: 0.351439
=== Actor Training Debug (Iteration 4109) ===
Q mean: -14.731532
Q std: 17.915966
Actor loss: 14.735497
Action reg: 0.003965
  l1.weight: grad_norm = 0.356402
  l1.bias: grad_norm = 0.001475
  l2.weight: grad_norm = 0.284552
Total gradient norm: 1.088316
=== Actor Training Debug (Iteration 4110) ===
Q mean: -13.652893
Q std: 17.470222
Actor loss: 13.656862
Action reg: 0.003969
  l1.weight: grad_norm = 0.125644
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.094168
Total gradient norm: 0.352641
=== Actor Training Debug (Iteration 4111) ===
Q mean: -12.978542
Q std: 16.676617
Actor loss: 12.982505
Action reg: 0.003963
  l1.weight: grad_norm = 0.093750
  l1.bias: grad_norm = 0.000907
  l2.weight: grad_norm = 0.074737
Total gradient norm: 0.297429
=== Actor Training Debug (Iteration 4112) ===
Q mean: -12.854798
Q std: 17.273628
Actor loss: 12.858769
Action reg: 0.003971
  l1.weight: grad_norm = 0.146471
  l1.bias: grad_norm = 0.000672
  l2.weight: grad_norm = 0.086951
Total gradient norm: 0.287578
=== Actor Training Debug (Iteration 4113) ===
Q mean: -13.672352
Q std: 17.560537
Actor loss: 13.676325
Action reg: 0.003973
  l1.weight: grad_norm = 0.116252
  l1.bias: grad_norm = 0.001332
  l2.weight: grad_norm = 0.080095
Total gradient norm: 0.269440
=== Actor Training Debug (Iteration 4114) ===
Q mean: -15.739734
Q std: 19.091761
Actor loss: 15.743706
Action reg: 0.003973
  l1.weight: grad_norm = 0.067274
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.044146
Total gradient norm: 0.146241
=== Actor Training Debug (Iteration 4115) ===
Q mean: -12.155678
Q std: 16.169785
Actor loss: 12.159648
Action reg: 0.003971
  l1.weight: grad_norm = 0.050033
  l1.bias: grad_norm = 0.001072
  l2.weight: grad_norm = 0.031961
Total gradient norm: 0.105857
=== Actor Training Debug (Iteration 4116) ===
Q mean: -11.866452
Q std: 16.902855
Actor loss: 11.870424
Action reg: 0.003972
  l1.weight: grad_norm = 0.097090
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.070570
Total gradient norm: 0.253432
=== Actor Training Debug (Iteration 4117) ===
Q mean: -15.370775
Q std: 18.325321
Actor loss: 15.374732
Action reg: 0.003957
  l1.weight: grad_norm = 0.140098
  l1.bias: grad_norm = 0.001261
  l2.weight: grad_norm = 0.104080
Total gradient norm: 0.363475
=== Actor Training Debug (Iteration 4118) ===
Q mean: -12.939378
Q std: 17.751024
Actor loss: 12.943344
Action reg: 0.003966
  l1.weight: grad_norm = 0.151881
  l1.bias: grad_norm = 0.001191
  l2.weight: grad_norm = 0.118956
Total gradient norm: 0.480752
=== Actor Training Debug (Iteration 4119) ===
Q mean: -11.771167
Q std: 16.949495
Actor loss: 11.775142
Action reg: 0.003975
  l1.weight: grad_norm = 0.081342
  l1.bias: grad_norm = 0.000617
  l2.weight: grad_norm = 0.080887
Total gradient norm: 0.265019
=== Actor Training Debug (Iteration 4120) ===
Q mean: -13.504386
Q std: 18.455236
Actor loss: 13.508359
Action reg: 0.003973
  l1.weight: grad_norm = 0.103268
  l1.bias: grad_norm = 0.001020
  l2.weight: grad_norm = 0.076473
Total gradient norm: 0.266187
=== Actor Training Debug (Iteration 4121) ===
Q mean: -13.599468
Q std: 18.183441
Actor loss: 13.603435
Action reg: 0.003966
  l1.weight: grad_norm = 0.090928
  l1.bias: grad_norm = 0.001430
  l2.weight: grad_norm = 0.073716
Total gradient norm: 0.345326
=== Actor Training Debug (Iteration 4122) ===
Q mean: -14.251266
Q std: 18.010469
Actor loss: 14.255236
Action reg: 0.003969
  l1.weight: grad_norm = 0.113876
  l1.bias: grad_norm = 0.001573
  l2.weight: grad_norm = 0.080612
Total gradient norm: 0.255523
=== Actor Training Debug (Iteration 4123) ===
Q mean: -12.837749
Q std: 17.595745
Actor loss: 12.841733
Action reg: 0.003984
  l1.weight: grad_norm = 0.067728
  l1.bias: grad_norm = 0.000578
  l2.weight: grad_norm = 0.047794
Total gradient norm: 0.159020
=== Actor Training Debug (Iteration 4124) ===
Q mean: -13.271852
Q std: 17.670128
Actor loss: 13.275831
Action reg: 0.003979
  l1.weight: grad_norm = 0.152732
  l1.bias: grad_norm = 0.000484
  l2.weight: grad_norm = 0.104227
Total gradient norm: 0.356073
=== Actor Training Debug (Iteration 4125) ===
Q mean: -14.029091
Q std: 17.492472
Actor loss: 14.033082
Action reg: 0.003991
  l1.weight: grad_norm = 0.038074
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.026763
Total gradient norm: 0.123362
=== Actor Training Debug (Iteration 4126) ===
Q mean: -14.126137
Q std: 18.901394
Actor loss: 14.130092
Action reg: 0.003955
  l1.weight: grad_norm = 0.147003
  l1.bias: grad_norm = 0.001222
  l2.weight: grad_norm = 0.107769
Total gradient norm: 0.447063
=== Actor Training Debug (Iteration 4127) ===
Q mean: -12.453520
Q std: 18.504572
Actor loss: 12.457467
Action reg: 0.003947
  l1.weight: grad_norm = 0.193420
  l1.bias: grad_norm = 0.001426
  l2.weight: grad_norm = 0.162653
Total gradient norm: 0.626411
=== Actor Training Debug (Iteration 4128) ===
Q mean: -12.885813
Q std: 17.681557
Actor loss: 12.889784
Action reg: 0.003971
  l1.weight: grad_norm = 0.083383
  l1.bias: grad_norm = 0.000565
  l2.weight: grad_norm = 0.061821
Total gradient norm: 0.235787
=== Actor Training Debug (Iteration 4129) ===
Q mean: -11.983236
Q std: 17.447512
Actor loss: 11.987213
Action reg: 0.003976
  l1.weight: grad_norm = 0.115496
  l1.bias: grad_norm = 0.000575
  l2.weight: grad_norm = 0.088033
Total gradient norm: 0.391581
=== Actor Training Debug (Iteration 4130) ===
Q mean: -13.901411
Q std: 17.684772
Actor loss: 13.905397
Action reg: 0.003986
  l1.weight: grad_norm = 0.031976
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.020403
Total gradient norm: 0.078958
=== Actor Training Debug (Iteration 4131) ===
Q mean: -13.611727
Q std: 18.964785
Actor loss: 13.615712
Action reg: 0.003985
  l1.weight: grad_norm = 0.132449
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.099818
Total gradient norm: 0.355168
=== Actor Training Debug (Iteration 4132) ===
Q mean: -13.024477
Q std: 17.769699
Actor loss: 13.028460
Action reg: 0.003983
  l1.weight: grad_norm = 0.119270
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.077649
Total gradient norm: 0.218987
=== Actor Training Debug (Iteration 4133) ===
Q mean: -12.586361
Q std: 17.365250
Actor loss: 12.590341
Action reg: 0.003980
  l1.weight: grad_norm = 0.124365
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.095767
Total gradient norm: 0.321913
=== Actor Training Debug (Iteration 4134) ===
Q mean: -12.319644
Q std: 18.067352
Actor loss: 12.323606
Action reg: 0.003962
  l1.weight: grad_norm = 0.091100
  l1.bias: grad_norm = 0.002065
  l2.weight: grad_norm = 0.070749
Total gradient norm: 0.247074
=== Actor Training Debug (Iteration 4135) ===
Q mean: -12.730163
Q std: 16.645466
Actor loss: 12.734148
Action reg: 0.003985
  l1.weight: grad_norm = 0.052325
  l1.bias: grad_norm = 0.001331
  l2.weight: grad_norm = 0.038425
Total gradient norm: 0.130562
=== Actor Training Debug (Iteration 4136) ===
Q mean: -13.062063
Q std: 17.607935
Actor loss: 13.066036
Action reg: 0.003973
  l1.weight: grad_norm = 0.104863
  l1.bias: grad_norm = 0.001108
  l2.weight: grad_norm = 0.067389
Total gradient norm: 0.219161
=== Actor Training Debug (Iteration 4137) ===
Q mean: -13.305302
Q std: 17.490923
Actor loss: 13.309283
Action reg: 0.003982
  l1.weight: grad_norm = 0.090488
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.059853
Total gradient norm: 0.229073
=== Actor Training Debug (Iteration 4138) ===
Q mean: -12.476587
Q std: 16.629354
Actor loss: 12.480558
Action reg: 0.003971
  l1.weight: grad_norm = 0.065154
  l1.bias: grad_norm = 0.001010
  l2.weight: grad_norm = 0.056492
Total gradient norm: 0.198792
=== Actor Training Debug (Iteration 4139) ===
Q mean: -12.564867
Q std: 17.377819
Actor loss: 12.568833
Action reg: 0.003966
  l1.weight: grad_norm = 0.123593
  l1.bias: grad_norm = 0.001109
  l2.weight: grad_norm = 0.121556
Total gradient norm: 0.409854
=== Actor Training Debug (Iteration 4140) ===
Q mean: -13.651203
Q std: 17.654188
Actor loss: 13.655178
Action reg: 0.003975
  l1.weight: grad_norm = 0.064160
  l1.bias: grad_norm = 0.000832
  l2.weight: grad_norm = 0.049476
Total gradient norm: 0.157845
=== Actor Training Debug (Iteration 4141) ===
Q mean: -12.891044
Q std: 17.045139
Actor loss: 12.895026
Action reg: 0.003983
  l1.weight: grad_norm = 0.019155
  l1.bias: grad_norm = 0.000777
  l2.weight: grad_norm = 0.015783
Total gradient norm: 0.062392
=== Actor Training Debug (Iteration 4142) ===
Q mean: -14.794162
Q std: 17.954245
Actor loss: 14.798148
Action reg: 0.003987
  l1.weight: grad_norm = 0.057769
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.041743
Total gradient norm: 0.131428
=== Actor Training Debug (Iteration 4143) ===
Q mean: -12.511889
Q std: 17.041609
Actor loss: 12.515864
Action reg: 0.003976
  l1.weight: grad_norm = 0.171982
  l1.bias: grad_norm = 0.000849
  l2.weight: grad_norm = 0.130900
Total gradient norm: 0.461473
=== Actor Training Debug (Iteration 4144) ===
Q mean: -13.745345
Q std: 17.297668
Actor loss: 13.749317
Action reg: 0.003972
  l1.weight: grad_norm = 0.080311
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.052944
Total gradient norm: 0.177948
=== Actor Training Debug (Iteration 4145) ===
Q mean: -12.835409
Q std: 17.476690
Actor loss: 12.839394
Action reg: 0.003985
  l1.weight: grad_norm = 0.076184
  l1.bias: grad_norm = 0.000929
  l2.weight: grad_norm = 0.048370
Total gradient norm: 0.156498
=== Actor Training Debug (Iteration 4146) ===
Q mean: -13.796626
Q std: 17.253830
Actor loss: 13.800609
Action reg: 0.003982
  l1.weight: grad_norm = 0.077803
  l1.bias: grad_norm = 0.000679
  l2.weight: grad_norm = 0.071777
Total gradient norm: 0.314564
=== Actor Training Debug (Iteration 4147) ===
Q mean: -11.756699
Q std: 17.723114
Actor loss: 11.760647
Action reg: 0.003949
  l1.weight: grad_norm = 0.050726
  l1.bias: grad_norm = 0.001487
  l2.weight: grad_norm = 0.043042
Total gradient norm: 0.161803
=== Actor Training Debug (Iteration 4148) ===
Q mean: -13.226273
Q std: 17.690725
Actor loss: 13.230253
Action reg: 0.003981
  l1.weight: grad_norm = 0.067879
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.046407
Total gradient norm: 0.136726
=== Actor Training Debug (Iteration 4149) ===
Q mean: -12.072755
Q std: 16.720116
Actor loss: 12.076731
Action reg: 0.003976
  l1.weight: grad_norm = 0.043755
  l1.bias: grad_norm = 0.000492
  l2.weight: grad_norm = 0.034514
Total gradient norm: 0.140907
=== Actor Training Debug (Iteration 4150) ===
Q mean: -11.239595
Q std: 15.032468
Actor loss: 11.243579
Action reg: 0.003983
  l1.weight: grad_norm = 0.095492
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.078153
Total gradient norm: 0.312559
=== Actor Training Debug (Iteration 4151) ===
Q mean: -12.423805
Q std: 17.344841
Actor loss: 12.427749
Action reg: 0.003944
  l1.weight: grad_norm = 0.068851
  l1.bias: grad_norm = 0.002263
  l2.weight: grad_norm = 0.057229
Total gradient norm: 0.228504
=== Actor Training Debug (Iteration 4152) ===
Q mean: -11.933110
Q std: 17.457293
Actor loss: 11.937088
Action reg: 0.003977
  l1.weight: grad_norm = 0.084028
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.053118
Total gradient norm: 0.204157
=== Actor Training Debug (Iteration 4153) ===
Q mean: -12.940310
Q std: 18.299608
Actor loss: 12.944284
Action reg: 0.003975
  l1.weight: grad_norm = 0.043808
  l1.bias: grad_norm = 0.001211
  l2.weight: grad_norm = 0.032442
Total gradient norm: 0.115064
=== Actor Training Debug (Iteration 4154) ===
Q mean: -12.184368
Q std: 17.098387
Actor loss: 12.188341
Action reg: 0.003973
  l1.weight: grad_norm = 0.110027
  l1.bias: grad_norm = 0.000806
  l2.weight: grad_norm = 0.078145
Total gradient norm: 0.295174
=== Actor Training Debug (Iteration 4155) ===
Q mean: -13.953840
Q std: 18.202200
Actor loss: 13.957833
Action reg: 0.003993
  l1.weight: grad_norm = 0.076507
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.059721
Total gradient norm: 0.153792
=== Actor Training Debug (Iteration 4156) ===
Q mean: -15.278941
Q std: 17.853792
Actor loss: 15.282918
Action reg: 0.003977
  l1.weight: grad_norm = 0.167608
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.112351
Total gradient norm: 0.360535
=== Actor Training Debug (Iteration 4157) ===
Q mean: -14.715065
Q std: 18.931599
Actor loss: 14.719035
Action reg: 0.003970
  l1.weight: grad_norm = 0.086375
  l1.bias: grad_norm = 0.001215
  l2.weight: grad_norm = 0.072471
Total gradient norm: 0.242396
=== Actor Training Debug (Iteration 4158) ===
Q mean: -12.392769
Q std: 17.088511
Actor loss: 12.396737
Action reg: 0.003969
  l1.weight: grad_norm = 0.108823
  l1.bias: grad_norm = 0.000911
  l2.weight: grad_norm = 0.095103
Total gradient norm: 0.327296
=== Actor Training Debug (Iteration 4159) ===
Q mean: -12.979689
Q std: 16.758373
Actor loss: 12.983660
Action reg: 0.003971
  l1.weight: grad_norm = 0.166340
  l1.bias: grad_norm = 0.000773
  l2.weight: grad_norm = 0.116035
Total gradient norm: 0.349052
=== Actor Training Debug (Iteration 4160) ===
Q mean: -12.793552
Q std: 17.237148
Actor loss: 12.797538
Action reg: 0.003985
  l1.weight: grad_norm = 0.166514
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.126902
Total gradient norm: 0.314647
=== Actor Training Debug (Iteration 4161) ===
Q mean: -12.742064
Q std: 17.610277
Actor loss: 12.746016
Action reg: 0.003951
  l1.weight: grad_norm = 0.143745
  l1.bias: grad_norm = 0.001068
  l2.weight: grad_norm = 0.106925
Total gradient norm: 0.388901
=== Actor Training Debug (Iteration 4162) ===
Q mean: -12.423995
Q std: 17.320127
Actor loss: 12.427955
Action reg: 0.003960
  l1.weight: grad_norm = 0.035685
  l1.bias: grad_norm = 0.001086
  l2.weight: grad_norm = 0.026491
Total gradient norm: 0.097314
=== Actor Training Debug (Iteration 4163) ===
Q mean: -11.232778
Q std: 16.942163
Actor loss: 11.236745
Action reg: 0.003967
  l1.weight: grad_norm = 0.153206
  l1.bias: grad_norm = 0.000741
  l2.weight: grad_norm = 0.120415
Total gradient norm: 0.392904
=== Actor Training Debug (Iteration 4164) ===
Q mean: -10.955059
Q std: 16.347355
Actor loss: 10.959025
Action reg: 0.003967
  l1.weight: grad_norm = 0.111146
  l1.bias: grad_norm = 0.000598
  l2.weight: grad_norm = 0.083858
Total gradient norm: 0.296772
=== Actor Training Debug (Iteration 4165) ===
Q mean: -11.394869
Q std: 16.288754
Actor loss: 11.398850
Action reg: 0.003982
  l1.weight: grad_norm = 0.099018
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.079891
Total gradient norm: 0.305400
=== Actor Training Debug (Iteration 4166) ===
Q mean: -13.663910
Q std: 19.115999
Actor loss: 13.667872
Action reg: 0.003963
  l1.weight: grad_norm = 0.066273
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.060544
Total gradient norm: 0.213711
=== Actor Training Debug (Iteration 4167) ===
Q mean: -12.382259
Q std: 17.347349
Actor loss: 12.386221
Action reg: 0.003962
  l1.weight: grad_norm = 0.113818
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.085246
Total gradient norm: 0.341052
=== Actor Training Debug (Iteration 4168) ===
Q mean: -12.327703
Q std: 16.949026
Actor loss: 12.331674
Action reg: 0.003970
  l1.weight: grad_norm = 0.071198
  l1.bias: grad_norm = 0.000643
  l2.weight: grad_norm = 0.061569
Total gradient norm: 0.224279
=== Actor Training Debug (Iteration 4169) ===
Q mean: -12.856614
Q std: 18.586042
Actor loss: 12.860576
Action reg: 0.003961
  l1.weight: grad_norm = 0.062837
  l1.bias: grad_norm = 0.001500
  l2.weight: grad_norm = 0.052773
Total gradient norm: 0.151888
=== Actor Training Debug (Iteration 4170) ===
Q mean: -12.386815
Q std: 16.282192
Actor loss: 12.390788
Action reg: 0.003973
  l1.weight: grad_norm = 0.078048
  l1.bias: grad_norm = 0.001626
  l2.weight: grad_norm = 0.064919
Total gradient norm: 0.252949
=== Actor Training Debug (Iteration 4171) ===
Q mean: -12.529298
Q std: 17.455215
Actor loss: 12.533273
Action reg: 0.003975
  l1.weight: grad_norm = 0.081320
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.054723
Total gradient norm: 0.195486
=== Actor Training Debug (Iteration 4172) ===
Q mean: -13.800566
Q std: 18.107533
Actor loss: 13.804548
Action reg: 0.003983
  l1.weight: grad_norm = 0.091438
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.065163
Total gradient norm: 0.200114
=== Actor Training Debug (Iteration 4173) ===
Q mean: -11.996052
Q std: 17.070999
Actor loss: 12.000032
Action reg: 0.003980
  l1.weight: grad_norm = 0.089408
  l1.bias: grad_norm = 0.001988
  l2.weight: grad_norm = 0.062560
Total gradient norm: 0.196797
=== Actor Training Debug (Iteration 4174) ===
Q mean: -14.020741
Q std: 17.019753
Actor loss: 14.024702
Action reg: 0.003962
  l1.weight: grad_norm = 0.330986
  l1.bias: grad_norm = 0.000913
  l2.weight: grad_norm = 0.247185
Total gradient norm: 1.056627
=== Actor Training Debug (Iteration 4175) ===
Q mean: -13.800421
Q std: 17.727089
Actor loss: 13.804406
Action reg: 0.003985
  l1.weight: grad_norm = 0.042763
  l1.bias: grad_norm = 0.001065
  l2.weight: grad_norm = 0.029693
Total gradient norm: 0.100145
=== Actor Training Debug (Iteration 4176) ===
Q mean: -9.742725
Q std: 15.378356
Actor loss: 9.746688
Action reg: 0.003963
  l1.weight: grad_norm = 0.035264
  l1.bias: grad_norm = 0.001237
  l2.weight: grad_norm = 0.026777
Total gradient norm: 0.094577
=== Actor Training Debug (Iteration 4177) ===
Q mean: -12.072491
Q std: 17.452646
Actor loss: 12.076463
Action reg: 0.003972
  l1.weight: grad_norm = 0.089973
  l1.bias: grad_norm = 0.000582
  l2.weight: grad_norm = 0.073229
Total gradient norm: 0.251364
=== Actor Training Debug (Iteration 4178) ===
Q mean: -11.574472
Q std: 16.929747
Actor loss: 11.578442
Action reg: 0.003969
  l1.weight: grad_norm = 0.094434
  l1.bias: grad_norm = 0.001147
  l2.weight: grad_norm = 0.063349
Total gradient norm: 0.232591
=== Actor Training Debug (Iteration 4179) ===
Q mean: -12.162319
Q std: 17.160252
Actor loss: 12.166286
Action reg: 0.003968
  l1.weight: grad_norm = 0.048519
  l1.bias: grad_norm = 0.001100
  l2.weight: grad_norm = 0.032810
Total gradient norm: 0.117038
=== Actor Training Debug (Iteration 4180) ===
Q mean: -13.152598
Q std: 17.284599
Actor loss: 13.156559
Action reg: 0.003960
  l1.weight: grad_norm = 0.072145
  l1.bias: grad_norm = 0.001198
  l2.weight: grad_norm = 0.071050
Total gradient norm: 0.235459
=== Actor Training Debug (Iteration 4181) ===
Q mean: -13.244563
Q std: 17.802477
Actor loss: 13.248540
Action reg: 0.003977
  l1.weight: grad_norm = 0.140070
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.100183
Total gradient norm: 0.365879
=== Actor Training Debug (Iteration 4182) ===
Q mean: -12.497413
Q std: 16.656439
Actor loss: 12.501392
Action reg: 0.003980
  l1.weight: grad_norm = 0.083760
  l1.bias: grad_norm = 0.000517
  l2.weight: grad_norm = 0.059755
Total gradient norm: 0.222371
=== Actor Training Debug (Iteration 4183) ===
Q mean: -13.686143
Q std: 18.901514
Actor loss: 13.690094
Action reg: 0.003951
  l1.weight: grad_norm = 0.112677
  l1.bias: grad_norm = 0.001120
  l2.weight: grad_norm = 0.087623
Total gradient norm: 0.318507
=== Actor Training Debug (Iteration 4184) ===
Q mean: -13.047623
Q std: 17.514502
Actor loss: 13.051614
Action reg: 0.003992
  l1.weight: grad_norm = 0.086944
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.064543
Total gradient norm: 0.244787
=== Actor Training Debug (Iteration 4185) ===
Q mean: -13.918980
Q std: 18.189625
Actor loss: 13.922924
Action reg: 0.003944
  l1.weight: grad_norm = 0.073336
  l1.bias: grad_norm = 0.002283
  l2.weight: grad_norm = 0.057170
Total gradient norm: 0.241647
=== Actor Training Debug (Iteration 4186) ===
Q mean: -12.514363
Q std: 17.163437
Actor loss: 12.518344
Action reg: 0.003981
  l1.weight: grad_norm = 0.082319
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.063409
Total gradient norm: 0.211976
=== Actor Training Debug (Iteration 4187) ===
Q mean: -10.943563
Q std: 17.100456
Actor loss: 10.947519
Action reg: 0.003955
  l1.weight: grad_norm = 0.080286
  l1.bias: grad_norm = 0.001102
  l2.weight: grad_norm = 0.057018
Total gradient norm: 0.224334
=== Actor Training Debug (Iteration 4188) ===
Q mean: -12.297003
Q std: 17.111004
Actor loss: 12.300983
Action reg: 0.003981
  l1.weight: grad_norm = 0.107777
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.078535
Total gradient norm: 0.246749
=== Actor Training Debug (Iteration 4189) ===
Q mean: -12.087889
Q std: 17.064922
Actor loss: 12.091846
Action reg: 0.003957
  l1.weight: grad_norm = 0.119653
  l1.bias: grad_norm = 0.001855
  l2.weight: grad_norm = 0.113907
Total gradient norm: 0.392882
=== Actor Training Debug (Iteration 4190) ===
Q mean: -13.394429
Q std: 17.976463
Actor loss: 13.398396
Action reg: 0.003966
  l1.weight: grad_norm = 0.102084
  l1.bias: grad_norm = 0.000960
  l2.weight: grad_norm = 0.096353
Total gradient norm: 0.345192
=== Actor Training Debug (Iteration 4191) ===
Q mean: -10.880499
Q std: 16.041052
Actor loss: 10.884458
Action reg: 0.003959
  l1.weight: grad_norm = 0.226498
  l1.bias: grad_norm = 0.001537
  l2.weight: grad_norm = 0.184418
Total gradient norm: 0.684342
=== Actor Training Debug (Iteration 4192) ===
Q mean: -10.904781
Q std: 16.201689
Actor loss: 10.908745
Action reg: 0.003963
  l1.weight: grad_norm = 0.223212
  l1.bias: grad_norm = 0.000729
  l2.weight: grad_norm = 0.137258
Total gradient norm: 0.420628
=== Actor Training Debug (Iteration 4193) ===
Q mean: -13.745811
Q std: 17.812237
Actor loss: 13.749792
Action reg: 0.003981
  l1.weight: grad_norm = 0.155644
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.111107
Total gradient norm: 0.375542
=== Actor Training Debug (Iteration 4194) ===
Q mean: -15.461950
Q std: 18.689896
Actor loss: 15.465934
Action reg: 0.003983
  l1.weight: grad_norm = 0.072990
  l1.bias: grad_norm = 0.000719
  l2.weight: grad_norm = 0.057734
Total gradient norm: 0.220857
=== Actor Training Debug (Iteration 4195) ===
Q mean: -14.086210
Q std: 18.870564
Actor loss: 14.090171
Action reg: 0.003961
  l1.weight: grad_norm = 0.062942
  l1.bias: grad_norm = 0.003542
  l2.weight: grad_norm = 0.056090
Total gradient norm: 0.204309
=== Actor Training Debug (Iteration 4196) ===
Q mean: -11.188180
Q std: 15.934416
Actor loss: 11.192159
Action reg: 0.003979
  l1.weight: grad_norm = 0.052590
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.035665
Total gradient norm: 0.136793
=== Actor Training Debug (Iteration 4197) ===
Q mean: -12.216853
Q std: 16.156509
Actor loss: 12.220819
Action reg: 0.003965
  l1.weight: grad_norm = 0.042866
  l1.bias: grad_norm = 0.001678
  l2.weight: grad_norm = 0.033650
Total gradient norm: 0.118446
=== Actor Training Debug (Iteration 4198) ===
Q mean: -12.971547
Q std: 16.745346
Actor loss: 12.975512
Action reg: 0.003965
  l1.weight: grad_norm = 0.121950
  l1.bias: grad_norm = 0.000765
  l2.weight: grad_norm = 0.094186
Total gradient norm: 0.345315
=== Actor Training Debug (Iteration 4199) ===
Q mean: -12.636236
Q std: 17.901062
Actor loss: 12.640217
Action reg: 0.003981
  l1.weight: grad_norm = 0.144649
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.106713
Total gradient norm: 0.358190
=== Actor Training Debug (Iteration 4200) ===
Q mean: -12.415236
Q std: 17.154348
Actor loss: 12.419225
Action reg: 0.003989
  l1.weight: grad_norm = 0.089224
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.058875
Total gradient norm: 0.176216
=== Actor Training Debug (Iteration 4201) ===
Q mean: -14.521269
Q std: 17.991499
Actor loss: 14.525227
Action reg: 0.003958
  l1.weight: grad_norm = 0.157400
  l1.bias: grad_norm = 0.000975
  l2.weight: grad_norm = 0.126765
Total gradient norm: 0.537706
=== Actor Training Debug (Iteration 4202) ===
Q mean: -12.151808
Q std: 17.298618
Actor loss: 12.155789
Action reg: 0.003982
  l1.weight: grad_norm = 0.048782
  l1.bias: grad_norm = 0.000507
  l2.weight: grad_norm = 0.037446
Total gradient norm: 0.119163
=== Actor Training Debug (Iteration 4203) ===
Q mean: -13.791321
Q std: 17.639523
Actor loss: 13.795302
Action reg: 0.003981
  l1.weight: grad_norm = 0.071035
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.061011
Total gradient norm: 0.218958
=== Actor Training Debug (Iteration 4204) ===
Q mean: -15.132362
Q std: 17.909254
Actor loss: 15.136339
Action reg: 0.003977
  l1.weight: grad_norm = 0.133751
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.104578
Total gradient norm: 0.283871
=== Actor Training Debug (Iteration 4205) ===
Q mean: -12.622710
Q std: 17.523190
Actor loss: 12.626676
Action reg: 0.003965
  l1.weight: grad_norm = 0.214084
  l1.bias: grad_norm = 0.001600
  l2.weight: grad_norm = 0.152048
Total gradient norm: 0.564478
=== Actor Training Debug (Iteration 4206) ===
Q mean: -11.992906
Q std: 17.097729
Actor loss: 11.996871
Action reg: 0.003965
  l1.weight: grad_norm = 0.158184
  l1.bias: grad_norm = 0.000673
  l2.weight: grad_norm = 0.140687
Total gradient norm: 0.491432
=== Actor Training Debug (Iteration 4207) ===
Q mean: -13.836931
Q std: 18.337860
Actor loss: 13.840876
Action reg: 0.003945
  l1.weight: grad_norm = 0.111183
  l1.bias: grad_norm = 0.000881
  l2.weight: grad_norm = 0.094231
Total gradient norm: 0.263621
=== Actor Training Debug (Iteration 4208) ===
Q mean: -14.093178
Q std: 19.159676
Actor loss: 14.097143
Action reg: 0.003966
  l1.weight: grad_norm = 0.050722
  l1.bias: grad_norm = 0.000905
  l2.weight: grad_norm = 0.040308
Total gradient norm: 0.131141
=== Actor Training Debug (Iteration 4209) ===
Q mean: -12.875368
Q std: 18.050949
Actor loss: 12.879322
Action reg: 0.003954
  l1.weight: grad_norm = 0.069808
  l1.bias: grad_norm = 0.001505
  l2.weight: grad_norm = 0.042379
Total gradient norm: 0.130363
=== Actor Training Debug (Iteration 4210) ===
Q mean: -14.643806
Q std: 18.764578
Actor loss: 14.647785
Action reg: 0.003979
  l1.weight: grad_norm = 0.244649
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.205414
Total gradient norm: 0.785299
=== Actor Training Debug (Iteration 4211) ===
Q mean: -13.253160
Q std: 18.206100
Actor loss: 13.257118
Action reg: 0.003958
  l1.weight: grad_norm = 0.153318
  l1.bias: grad_norm = 0.000591
  l2.weight: grad_norm = 0.108938
Total gradient norm: 0.384108
=== Actor Training Debug (Iteration 4212) ===
Q mean: -12.413298
Q std: 16.652431
Actor loss: 12.417272
Action reg: 0.003974
  l1.weight: grad_norm = 0.050805
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.040322
Total gradient norm: 0.136800
=== Actor Training Debug (Iteration 4213) ===
Q mean: -12.748569
Q std: 16.377857
Actor loss: 12.752546
Action reg: 0.003977
  l1.weight: grad_norm = 0.165295
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.160620
Total gradient norm: 0.472986
=== Actor Training Debug (Iteration 4214) ===
Q mean: -12.055241
Q std: 16.719229
Actor loss: 12.059208
Action reg: 0.003967
  l1.weight: grad_norm = 0.083700
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.070310
Total gradient norm: 0.226234
=== Actor Training Debug (Iteration 4215) ===
Q mean: -13.382837
Q std: 18.287962
Actor loss: 13.386807
Action reg: 0.003970
  l1.weight: grad_norm = 0.119882
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.102359
Total gradient norm: 0.316243
=== Actor Training Debug (Iteration 4216) ===
Q mean: -13.659480
Q std: 17.881096
Actor loss: 13.663452
Action reg: 0.003972
  l1.weight: grad_norm = 0.051389
  l1.bias: grad_norm = 0.000761
  l2.weight: grad_norm = 0.041209
Total gradient norm: 0.185257
=== Actor Training Debug (Iteration 4217) ===
Q mean: -13.657636
Q std: 17.966127
Actor loss: 13.661609
Action reg: 0.003973
  l1.weight: grad_norm = 0.135170
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.101828
Total gradient norm: 0.401547
=== Actor Training Debug (Iteration 4218) ===
Q mean: -11.960650
Q std: 16.760841
Actor loss: 11.964625
Action reg: 0.003975
  l1.weight: grad_norm = 0.112161
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.081921
Total gradient norm: 0.302822
=== Actor Training Debug (Iteration 4219) ===
Q mean: -11.423265
Q std: 16.128742
Actor loss: 11.427230
Action reg: 0.003965
  l1.weight: grad_norm = 0.102644
  l1.bias: grad_norm = 0.000652
  l2.weight: grad_norm = 0.067830
Total gradient norm: 0.236652
=== Actor Training Debug (Iteration 4220) ===
Q mean: -12.439514
Q std: 17.115303
Actor loss: 12.443474
Action reg: 0.003960
  l1.weight: grad_norm = 0.157445
  l1.bias: grad_norm = 0.002426
  l2.weight: grad_norm = 0.117337
Total gradient norm: 0.441280
=== Actor Training Debug (Iteration 4221) ===
Q mean: -13.209152
Q std: 17.743258
Actor loss: 13.213114
Action reg: 0.003961
  l1.weight: grad_norm = 0.103753
  l1.bias: grad_norm = 0.000842
  l2.weight: grad_norm = 0.075071
Total gradient norm: 0.252398
=== Actor Training Debug (Iteration 4222) ===
Q mean: -9.940228
Q std: 16.115149
Actor loss: 9.944201
Action reg: 0.003974
  l1.weight: grad_norm = 0.057182
  l1.bias: grad_norm = 0.000678
  l2.weight: grad_norm = 0.047418
Total gradient norm: 0.171373
=== Actor Training Debug (Iteration 4223) ===
Q mean: -13.668568
Q std: 18.724419
Actor loss: 13.672519
Action reg: 0.003951
  l1.weight: grad_norm = 0.038949
  l1.bias: grad_norm = 0.000812
  l2.weight: grad_norm = 0.030963
Total gradient norm: 0.111684
=== Actor Training Debug (Iteration 4224) ===
Q mean: -14.845939
Q std: 18.181862
Actor loss: 14.849906
Action reg: 0.003967
  l1.weight: grad_norm = 0.135512
  l1.bias: grad_norm = 0.000717
  l2.weight: grad_norm = 0.098612
Total gradient norm: 0.326599
=== Actor Training Debug (Iteration 4225) ===
Q mean: -15.286498
Q std: 18.689878
Actor loss: 15.290476
Action reg: 0.003978
  l1.weight: grad_norm = 0.100060
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.060513
Total gradient norm: 0.210173
=== Actor Training Debug (Iteration 4226) ===
Q mean: -12.365016
Q std: 18.368509
Actor loss: 12.368993
Action reg: 0.003977
  l1.weight: grad_norm = 0.121341
  l1.bias: grad_norm = 0.000634
  l2.weight: grad_norm = 0.096583
Total gradient norm: 0.342863
=== Actor Training Debug (Iteration 4227) ===
Q mean: -12.301140
Q std: 17.066282
Actor loss: 12.305126
Action reg: 0.003986
  l1.weight: grad_norm = 0.084887
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.063338
Total gradient norm: 0.217516
=== Actor Training Debug (Iteration 4228) ===
Q mean: -12.336086
Q std: 17.715143
Actor loss: 12.340042
Action reg: 0.003956
  l1.weight: grad_norm = 0.064215
  l1.bias: grad_norm = 0.000847
  l2.weight: grad_norm = 0.047286
Total gradient norm: 0.152923
=== Actor Training Debug (Iteration 4229) ===
Q mean: -12.770820
Q std: 17.589062
Actor loss: 12.774785
Action reg: 0.003966
  l1.weight: grad_norm = 0.139911
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.093940
Total gradient norm: 0.324975
=== Actor Training Debug (Iteration 4230) ===
Q mean: -12.677765
Q std: 18.244879
Actor loss: 12.681728
Action reg: 0.003963
  l1.weight: grad_norm = 0.090986
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.073053
Total gradient norm: 0.238767
=== Actor Training Debug (Iteration 4231) ===
Q mean: -11.686623
Q std: 16.483997
Actor loss: 11.690583
Action reg: 0.003960
  l1.weight: grad_norm = 0.061050
  l1.bias: grad_norm = 0.001201
  l2.weight: grad_norm = 0.045875
Total gradient norm: 0.167241
=== Actor Training Debug (Iteration 4232) ===
Q mean: -12.703209
Q std: 16.852859
Actor loss: 12.707176
Action reg: 0.003968
  l1.weight: grad_norm = 0.077228
  l1.bias: grad_norm = 0.000912
  l2.weight: grad_norm = 0.051806
Total gradient norm: 0.170798
=== Actor Training Debug (Iteration 4233) ===
Q mean: -12.518515
Q std: 16.386826
Actor loss: 12.522475
Action reg: 0.003961
  l1.weight: grad_norm = 0.064701
  l1.bias: grad_norm = 0.001049
  l2.weight: grad_norm = 0.048627
Total gradient norm: 0.208463
=== Actor Training Debug (Iteration 4234) ===
Q mean: -13.097355
Q std: 17.419880
Actor loss: 13.101335
Action reg: 0.003979
  l1.weight: grad_norm = 0.091019
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.064640
Total gradient norm: 0.220276
=== Actor Training Debug (Iteration 4235) ===
Q mean: -13.262219
Q std: 18.756025
Actor loss: 13.266198
Action reg: 0.003978
  l1.weight: grad_norm = 0.057519
  l1.bias: grad_norm = 0.002776
  l2.weight: grad_norm = 0.049880
Total gradient norm: 0.168236
=== Actor Training Debug (Iteration 4236) ===
Q mean: -11.850718
Q std: 17.180973
Actor loss: 11.854703
Action reg: 0.003985
  l1.weight: grad_norm = 0.377037
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.278944
Total gradient norm: 0.939887
=== Actor Training Debug (Iteration 4237) ===
Q mean: -12.326038
Q std: 16.375345
Actor loss: 12.329985
Action reg: 0.003947
  l1.weight: grad_norm = 0.085330
  l1.bias: grad_norm = 0.002074
  l2.weight: grad_norm = 0.061595
Total gradient norm: 0.194270
=== Actor Training Debug (Iteration 4238) ===
Q mean: -12.613401
Q std: 17.547031
Actor loss: 12.617345
Action reg: 0.003943
  l1.weight: grad_norm = 0.173416
  l1.bias: grad_norm = 0.000986
  l2.weight: grad_norm = 0.129034
Total gradient norm: 0.382427
=== Actor Training Debug (Iteration 4239) ===
Q mean: -13.915521
Q std: 18.586578
Actor loss: 13.919483
Action reg: 0.003962
  l1.weight: grad_norm = 0.205137
  l1.bias: grad_norm = 0.000652
  l2.weight: grad_norm = 0.161208
Total gradient norm: 0.608013
=== Actor Training Debug (Iteration 4240) ===
Q mean: -13.364758
Q std: 18.155548
Actor loss: 13.368710
Action reg: 0.003952
  l1.weight: grad_norm = 0.155054
  l1.bias: grad_norm = 0.000591
  l2.weight: grad_norm = 0.127145
Total gradient norm: 0.503026
=== Actor Training Debug (Iteration 4241) ===
Q mean: -12.866442
Q std: 18.392662
Actor loss: 12.870426
Action reg: 0.003985
  l1.weight: grad_norm = 0.078196
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.073711
Total gradient norm: 0.304084
=== Actor Training Debug (Iteration 4242) ===
Q mean: -12.363953
Q std: 16.817654
Actor loss: 12.367907
Action reg: 0.003954
  l1.weight: grad_norm = 0.126765
  l1.bias: grad_norm = 0.000869
  l2.weight: grad_norm = 0.090248
Total gradient norm: 0.287418
=== Actor Training Debug (Iteration 4243) ===
Q mean: -14.117982
Q std: 18.755651
Actor loss: 14.121970
Action reg: 0.003988
  l1.weight: grad_norm = 0.190272
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.139431
Total gradient norm: 0.536357
=== Actor Training Debug (Iteration 4244) ===
Q mean: -13.076279
Q std: 18.278975
Actor loss: 13.080255
Action reg: 0.003976
  l1.weight: grad_norm = 0.079751
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.059834
Total gradient norm: 0.177697
=== Actor Training Debug (Iteration 4245) ===
Q mean: -13.562422
Q std: 17.548748
Actor loss: 13.566393
Action reg: 0.003971
  l1.weight: grad_norm = 0.153333
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.109834
Total gradient norm: 0.398723
=== Actor Training Debug (Iteration 4246) ===
Q mean: -14.080439
Q std: 17.790787
Actor loss: 14.084417
Action reg: 0.003979
  l1.weight: grad_norm = 0.047437
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 0.046581
Total gradient norm: 0.153546
=== Actor Training Debug (Iteration 4247) ===
Q mean: -14.088242
Q std: 18.267242
Actor loss: 14.092221
Action reg: 0.003980
  l1.weight: grad_norm = 0.059017
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.042894
Total gradient norm: 0.127223
=== Actor Training Debug (Iteration 4248) ===
Q mean: -12.932287
Q std: 17.894434
Actor loss: 12.936264
Action reg: 0.003976
  l1.weight: grad_norm = 0.084002
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.062396
Total gradient norm: 0.185894
=== Actor Training Debug (Iteration 4249) ===
Q mean: -14.711828
Q std: 19.283588
Actor loss: 14.715807
Action reg: 0.003979
  l1.weight: grad_norm = 0.061710
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.058987
Total gradient norm: 0.198079
=== Actor Training Debug (Iteration 4250) ===
Q mean: -12.083158
Q std: 17.662952
Actor loss: 12.087120
Action reg: 0.003961
  l1.weight: grad_norm = 0.112325
  l1.bias: grad_norm = 0.000804
  l2.weight: grad_norm = 0.087974
Total gradient norm: 0.321919
=== Actor Training Debug (Iteration 4251) ===
Q mean: -14.755749
Q std: 18.552357
Actor loss: 14.759723
Action reg: 0.003974
  l1.weight: grad_norm = 0.118605
  l1.bias: grad_norm = 0.002932
  l2.weight: grad_norm = 0.110108
Total gradient norm: 0.371958
=== Actor Training Debug (Iteration 4252) ===
Q mean: -13.524494
Q std: 17.434187
Actor loss: 13.528468
Action reg: 0.003974
  l1.weight: grad_norm = 0.205160
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.170460
Total gradient norm: 0.624918
=== Actor Training Debug (Iteration 4253) ===
Q mean: -12.869080
Q std: 16.166136
Actor loss: 12.873064
Action reg: 0.003985
  l1.weight: grad_norm = 0.036935
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.032367
Total gradient norm: 0.115202
=== Actor Training Debug (Iteration 4254) ===
Q mean: -12.220503
Q std: 18.038231
Actor loss: 12.224482
Action reg: 0.003979
  l1.weight: grad_norm = 0.131455
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.107390
Total gradient norm: 0.355038
=== Actor Training Debug (Iteration 4255) ===
Q mean: -14.837036
Q std: 19.331993
Actor loss: 14.841023
Action reg: 0.003987
  l1.weight: grad_norm = 0.087712
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.059173
Total gradient norm: 0.259982
=== Actor Training Debug (Iteration 4256) ===
Q mean: -11.118090
Q std: 17.066641
Actor loss: 11.122043
Action reg: 0.003953
  l1.weight: grad_norm = 0.263188
  l1.bias: grad_norm = 0.000751
  l2.weight: grad_norm = 0.206291
Total gradient norm: 0.581608
=== Actor Training Debug (Iteration 4257) ===
Q mean: -13.124328
Q std: 18.095793
Actor loss: 13.128284
Action reg: 0.003957
  l1.weight: grad_norm = 0.173388
  l1.bias: grad_norm = 0.000786
  l2.weight: grad_norm = 0.146111
Total gradient norm: 0.405345
=== Actor Training Debug (Iteration 4258) ===
Q mean: -13.990492
Q std: 17.471117
Actor loss: 13.994476
Action reg: 0.003985
  l1.weight: grad_norm = 0.155585
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.099028
Total gradient norm: 0.315068
=== Actor Training Debug (Iteration 4259) ===
Q mean: -14.520819
Q std: 18.596226
Actor loss: 14.524798
Action reg: 0.003980
  l1.weight: grad_norm = 0.054961
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.037078
Total gradient norm: 0.124326
=== Actor Training Debug (Iteration 4260) ===
Q mean: -13.956318
Q std: 18.551205
Actor loss: 13.960281
Action reg: 0.003963
  l1.weight: grad_norm = 0.057516
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.046283
Total gradient norm: 0.215893
=== Actor Training Debug (Iteration 4261) ===
Q mean: -11.969000
Q std: 17.808609
Actor loss: 11.972970
Action reg: 0.003971
  l1.weight: grad_norm = 0.159232
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.118923
Total gradient norm: 0.373821
=== Actor Training Debug (Iteration 4262) ===
Q mean: -14.634481
Q std: 18.188690
Actor loss: 14.638453
Action reg: 0.003971
  l1.weight: grad_norm = 0.133331
  l1.bias: grad_norm = 0.000556
  l2.weight: grad_norm = 0.098180
Total gradient norm: 0.314524
=== Actor Training Debug (Iteration 4263) ===
Q mean: -11.698701
Q std: 16.665661
Actor loss: 11.702673
Action reg: 0.003972
  l1.weight: grad_norm = 0.086706
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.071437
Total gradient norm: 0.246730
=== Actor Training Debug (Iteration 4264) ===
Q mean: -13.231407
Q std: 18.106342
Actor loss: 13.235374
Action reg: 0.003967
  l1.weight: grad_norm = 0.097755
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.070807
Total gradient norm: 0.245364
=== Actor Training Debug (Iteration 4265) ===
Q mean: -11.919683
Q std: 18.241051
Actor loss: 11.923648
Action reg: 0.003964
  l1.weight: grad_norm = 0.059754
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.052533
Total gradient norm: 0.209737
=== Actor Training Debug (Iteration 4266) ===
Q mean: -12.483736
Q std: 17.579407
Actor loss: 12.487715
Action reg: 0.003979
  l1.weight: grad_norm = 0.084053
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.054722
Total gradient norm: 0.181954
=== Actor Training Debug (Iteration 4267) ===
Q mean: -14.212757
Q std: 18.692434
Actor loss: 14.216726
Action reg: 0.003969
  l1.weight: grad_norm = 0.232565
  l1.bias: grad_norm = 0.000813
  l2.weight: grad_norm = 0.162582
Total gradient norm: 0.641464
=== Actor Training Debug (Iteration 4268) ===
Q mean: -14.296021
Q std: 17.523268
Actor loss: 14.300004
Action reg: 0.003982
  l1.weight: grad_norm = 0.102045
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.074775
Total gradient norm: 0.268323
=== Actor Training Debug (Iteration 4269) ===
Q mean: -12.953794
Q std: 18.008678
Actor loss: 12.957766
Action reg: 0.003972
  l1.weight: grad_norm = 0.041210
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.034032
Total gradient norm: 0.117669
=== Actor Training Debug (Iteration 4270) ===
Q mean: -14.130396
Q std: 17.975662
Actor loss: 14.134374
Action reg: 0.003977
  l1.weight: grad_norm = 0.232596
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.173898
Total gradient norm: 0.559152
=== Actor Training Debug (Iteration 4271) ===
Q mean: -13.471739
Q std: 18.221869
Actor loss: 13.475710
Action reg: 0.003971
  l1.weight: grad_norm = 0.133098
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.111788
Total gradient norm: 0.346811
=== Actor Training Debug (Iteration 4272) ===
Q mean: -11.207489
Q std: 17.271879
Actor loss: 11.211464
Action reg: 0.003974
  l1.weight: grad_norm = 0.150388
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.105072
Total gradient norm: 0.343757
=== Actor Training Debug (Iteration 4273) ===
Q mean: -12.438032
Q std: 16.290960
Actor loss: 12.441991
Action reg: 0.003959
  l1.weight: grad_norm = 0.164230
  l1.bias: grad_norm = 0.000809
  l2.weight: grad_norm = 0.125877
Total gradient norm: 0.459909
=== Actor Training Debug (Iteration 4274) ===
Q mean: -13.479027
Q std: 18.346405
Actor loss: 13.482972
Action reg: 0.003946
  l1.weight: grad_norm = 0.078722
  l1.bias: grad_norm = 0.001167
  l2.weight: grad_norm = 0.068971
Total gradient norm: 0.198030
=== Actor Training Debug (Iteration 4275) ===
Q mean: -13.906441
Q std: 17.380287
Actor loss: 13.910392
Action reg: 0.003951
  l1.weight: grad_norm = 0.066840
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.063006
Total gradient norm: 0.248219
=== Actor Training Debug (Iteration 4276) ===
Q mean: -13.869390
Q std: 18.635326
Actor loss: 13.873362
Action reg: 0.003971
  l1.weight: grad_norm = 0.091040
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.071087
Total gradient norm: 0.250583
=== Actor Training Debug (Iteration 4277) ===
Q mean: -13.571377
Q std: 18.055353
Actor loss: 13.575353
Action reg: 0.003976
  l1.weight: grad_norm = 0.022733
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.019264
Total gradient norm: 0.073184
=== Actor Training Debug (Iteration 4278) ===
Q mean: -11.368936
Q std: 16.385218
Actor loss: 11.372914
Action reg: 0.003978
  l1.weight: grad_norm = 0.059565
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.043489
Total gradient norm: 0.165598
=== Actor Training Debug (Iteration 4279) ===
Q mean: -14.715637
Q std: 18.287914
Actor loss: 14.719600
Action reg: 0.003963
  l1.weight: grad_norm = 0.039972
  l1.bias: grad_norm = 0.000816
  l2.weight: grad_norm = 0.030339
Total gradient norm: 0.099914
=== Actor Training Debug (Iteration 4280) ===
Q mean: -16.279072
Q std: 18.600977
Actor loss: 16.283030
Action reg: 0.003957
  l1.weight: grad_norm = 0.104886
  l1.bias: grad_norm = 0.000682
  l2.weight: grad_norm = 0.070296
Total gradient norm: 0.203687
=== Actor Training Debug (Iteration 4281) ===
Q mean: -12.833780
Q std: 17.663204
Actor loss: 12.837770
Action reg: 0.003989
  l1.weight: grad_norm = 0.072177
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.059522
Total gradient norm: 0.261049
=== Actor Training Debug (Iteration 4282) ===
Q mean: -12.876694
Q std: 17.730665
Actor loss: 12.880668
Action reg: 0.003974
  l1.weight: grad_norm = 0.084993
  l1.bias: grad_norm = 0.000613
  l2.weight: grad_norm = 0.065864
Total gradient norm: 0.215402
=== Actor Training Debug (Iteration 4283) ===
Q mean: -13.589556
Q std: 18.176456
Actor loss: 13.593540
Action reg: 0.003984
  l1.weight: grad_norm = 0.078866
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.060756
Total gradient norm: 0.249376
=== Actor Training Debug (Iteration 4284) ===
Q mean: -13.994977
Q std: 18.868237
Actor loss: 13.998943
Action reg: 0.003967
  l1.weight: grad_norm = 0.172090
  l1.bias: grad_norm = 0.000414
  l2.weight: grad_norm = 0.144176
Total gradient norm: 0.519392
=== Actor Training Debug (Iteration 4285) ===
Q mean: -14.979866
Q std: 18.014109
Actor loss: 14.983850
Action reg: 0.003985
  l1.weight: grad_norm = 0.052240
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.040190
Total gradient norm: 0.155518
=== Actor Training Debug (Iteration 4286) ===
Q mean: -14.165070
Q std: 18.262148
Actor loss: 14.169050
Action reg: 0.003981
  l1.weight: grad_norm = 0.217450
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.158641
Total gradient norm: 0.477507
=== Actor Training Debug (Iteration 4287) ===
Q mean: -14.975154
Q std: 18.966925
Actor loss: 14.979130
Action reg: 0.003976
  l1.weight: grad_norm = 0.119649
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.091583
Total gradient norm: 0.279935
=== Actor Training Debug (Iteration 4288) ===
Q mean: -11.911740
Q std: 17.257950
Actor loss: 11.915709
Action reg: 0.003969
  l1.weight: grad_norm = 0.231586
  l1.bias: grad_norm = 0.000729
  l2.weight: grad_norm = 0.199385
Total gradient norm: 0.978502
=== Actor Training Debug (Iteration 4289) ===
Q mean: -12.186790
Q std: 16.736359
Actor loss: 12.190758
Action reg: 0.003968
  l1.weight: grad_norm = 0.110431
  l1.bias: grad_norm = 0.000756
  l2.weight: grad_norm = 0.077043
Total gradient norm: 0.233240
=== Actor Training Debug (Iteration 4290) ===
Q mean: -14.368185
Q std: 18.853512
Actor loss: 14.372167
Action reg: 0.003981
  l1.weight: grad_norm = 0.077571
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.062935
Total gradient norm: 0.219570
=== Actor Training Debug (Iteration 4291) ===
Q mean: -13.998650
Q std: 18.256170
Actor loss: 14.002621
Action reg: 0.003971
  l1.weight: grad_norm = 0.150834
  l1.bias: grad_norm = 0.000724
  l2.weight: grad_norm = 0.115034
Total gradient norm: 0.370003
=== Actor Training Debug (Iteration 4292) ===
Q mean: -13.690357
Q std: 18.065176
Actor loss: 13.694324
Action reg: 0.003966
  l1.weight: grad_norm = 0.107953
  l1.bias: grad_norm = 0.002025
  l2.weight: grad_norm = 0.095305
Total gradient norm: 0.313336
=== Actor Training Debug (Iteration 4293) ===
Q mean: -11.222183
Q std: 15.537643
Actor loss: 11.226160
Action reg: 0.003977
  l1.weight: grad_norm = 0.086805
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.066567
Total gradient norm: 0.271735
=== Actor Training Debug (Iteration 4294) ===
Q mean: -11.681890
Q std: 17.280989
Actor loss: 11.685863
Action reg: 0.003973
  l1.weight: grad_norm = 0.134812
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.116118
Total gradient norm: 0.399963
=== Actor Training Debug (Iteration 4295) ===
Q mean: -13.138977
Q std: 17.405350
Actor loss: 13.142944
Action reg: 0.003967
  l1.weight: grad_norm = 0.128424
  l1.bias: grad_norm = 0.000569
  l2.weight: grad_norm = 0.099018
Total gradient norm: 0.296948
=== Actor Training Debug (Iteration 4296) ===
Q mean: -14.557945
Q std: 19.254202
Actor loss: 14.561935
Action reg: 0.003990
  l1.weight: grad_norm = 0.159022
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.114682
Total gradient norm: 0.341615
=== Actor Training Debug (Iteration 4297) ===
Q mean: -12.457119
Q std: 16.468327
Actor loss: 12.461089
Action reg: 0.003970
  l1.weight: grad_norm = 0.210587
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.172563
Total gradient norm: 0.741517
=== Actor Training Debug (Iteration 4298) ===
Q mean: -14.413977
Q std: 18.028227
Actor loss: 14.417963
Action reg: 0.003986
  l1.weight: grad_norm = 0.090366
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.069493
Total gradient norm: 0.256431
=== Actor Training Debug (Iteration 4299) ===
Q mean: -12.322236
Q std: 17.257528
Actor loss: 12.326211
Action reg: 0.003975
  l1.weight: grad_norm = 0.139010
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.080546
Total gradient norm: 0.243151
=== Actor Training Debug (Iteration 4300) ===
Q mean: -12.311758
Q std: 18.051903
Actor loss: 12.315724
Action reg: 0.003966
  l1.weight: grad_norm = 0.069289
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.050476
Total gradient norm: 0.179650
=== Actor Training Debug (Iteration 4301) ===
Q mean: -14.483521
Q std: 18.532063
Actor loss: 14.487496
Action reg: 0.003975
  l1.weight: grad_norm = 0.117399
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.108321
Total gradient norm: 0.414141
=== Actor Training Debug (Iteration 4302) ===
Q mean: -12.736917
Q std: 17.323578
Actor loss: 12.740870
Action reg: 0.003954
  l1.weight: grad_norm = 0.137144
  l1.bias: grad_norm = 0.000986
  l2.weight: grad_norm = 0.101959
Total gradient norm: 0.346706
=== Actor Training Debug (Iteration 4303) ===
Q mean: -12.541405
Q std: 17.154812
Actor loss: 12.545383
Action reg: 0.003979
  l1.weight: grad_norm = 0.137553
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.101135
Total gradient norm: 0.317956
=== Actor Training Debug (Iteration 4304) ===
Q mean: -13.866573
Q std: 17.381872
Actor loss: 13.870548
Action reg: 0.003975
  l1.weight: grad_norm = 0.080524
  l1.bias: grad_norm = 0.000588
  l2.weight: grad_norm = 0.066166
Total gradient norm: 0.245847
=== Actor Training Debug (Iteration 4305) ===
Q mean: -11.987943
Q std: 17.322138
Actor loss: 11.991917
Action reg: 0.003973
  l1.weight: grad_norm = 0.069307
  l1.bias: grad_norm = 0.001260
  l2.weight: grad_norm = 0.052407
Total gradient norm: 0.165551
=== Actor Training Debug (Iteration 4306) ===
Q mean: -13.304222
Q std: 18.353542
Actor loss: 13.308192
Action reg: 0.003970
  l1.weight: grad_norm = 0.236928
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.173620
Total gradient norm: 0.768530
=== Actor Training Debug (Iteration 4307) ===
Q mean: -12.858097
Q std: 16.892439
Actor loss: 12.862075
Action reg: 0.003978
  l1.weight: grad_norm = 0.081698
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.066500
Total gradient norm: 0.239235
=== Actor Training Debug (Iteration 4308) ===
Q mean: -13.536806
Q std: 17.058275
Actor loss: 13.540772
Action reg: 0.003966
  l1.weight: grad_norm = 0.123414
  l1.bias: grad_norm = 0.002560
  l2.weight: grad_norm = 0.093268
Total gradient norm: 0.351060
=== Actor Training Debug (Iteration 4309) ===
Q mean: -12.811972
Q std: 17.169067
Actor loss: 12.815921
Action reg: 0.003949
  l1.weight: grad_norm = 0.150465
  l1.bias: grad_norm = 0.000911
  l2.weight: grad_norm = 0.117690
Total gradient norm: 0.384379
=== Actor Training Debug (Iteration 4310) ===
Q mean: -13.771530
Q std: 17.718733
Actor loss: 13.775506
Action reg: 0.003976
  l1.weight: grad_norm = 0.125433
  l1.bias: grad_norm = 0.000428
  l2.weight: grad_norm = 0.105924
Total gradient norm: 0.327692
=== Actor Training Debug (Iteration 4311) ===
Q mean: -11.017465
Q std: 17.204222
Actor loss: 11.021415
Action reg: 0.003950
  l1.weight: grad_norm = 0.154215
  l1.bias: grad_norm = 0.000802
  l2.weight: grad_norm = 0.111107
Total gradient norm: 0.370292
=== Actor Training Debug (Iteration 4312) ===
Q mean: -10.252298
Q std: 16.182394
Actor loss: 10.256269
Action reg: 0.003971
  l1.weight: grad_norm = 0.067677
  l1.bias: grad_norm = 0.002418
  l2.weight: grad_norm = 0.048416
Total gradient norm: 0.164120
=== Actor Training Debug (Iteration 4313) ===
Q mean: -12.358530
Q std: 18.277006
Actor loss: 12.362496
Action reg: 0.003966
  l1.weight: grad_norm = 0.124223
  l1.bias: grad_norm = 0.000509
  l2.weight: grad_norm = 0.112096
Total gradient norm: 0.356673
=== Actor Training Debug (Iteration 4314) ===
Q mean: -14.346712
Q std: 17.704853
Actor loss: 14.350680
Action reg: 0.003968
  l1.weight: grad_norm = 0.061414
  l1.bias: grad_norm = 0.001980
  l2.weight: grad_norm = 0.049473
Total gradient norm: 0.183935
=== Actor Training Debug (Iteration 4315) ===
Q mean: -11.658472
Q std: 16.820311
Actor loss: 11.662454
Action reg: 0.003982
  l1.weight: grad_norm = 0.085807
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.078787
Total gradient norm: 0.278224
=== Actor Training Debug (Iteration 4316) ===
Q mean: -12.909481
Q std: 17.410681
Actor loss: 12.913438
Action reg: 0.003956
  l1.weight: grad_norm = 0.107412
  l1.bias: grad_norm = 0.001054
  l2.weight: grad_norm = 0.092456
Total gradient norm: 0.270445
=== Actor Training Debug (Iteration 4317) ===
Q mean: -13.002554
Q std: 17.046883
Actor loss: 13.006539
Action reg: 0.003985
  l1.weight: grad_norm = 0.049798
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.044751
Total gradient norm: 0.150201
=== Actor Training Debug (Iteration 4318) ===
Q mean: -14.382444
Q std: 18.374695
Actor loss: 14.386423
Action reg: 0.003978
  l1.weight: grad_norm = 0.166040
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.118568
Total gradient norm: 0.375668
=== Actor Training Debug (Iteration 4319) ===
Q mean: -13.108303
Q std: 18.406443
Actor loss: 13.112267
Action reg: 0.003963
  l1.weight: grad_norm = 0.078027
  l1.bias: grad_norm = 0.000654
  l2.weight: grad_norm = 0.059265
Total gradient norm: 0.207259
=== Actor Training Debug (Iteration 4320) ===
Q mean: -14.285312
Q std: 19.182953
Actor loss: 14.289279
Action reg: 0.003968
  l1.weight: grad_norm = 0.183413
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.148186
Total gradient norm: 0.450326
=== Actor Training Debug (Iteration 4321) ===
Q mean: -13.560214
Q std: 17.897089
Actor loss: 13.564205
Action reg: 0.003991
  l1.weight: grad_norm = 0.076874
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.074164
Total gradient norm: 0.274151
=== Actor Training Debug (Iteration 4322) ===
Q mean: -15.280457
Q std: 18.942144
Actor loss: 15.284445
Action reg: 0.003987
  l1.weight: grad_norm = 0.094618
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.078815
Total gradient norm: 0.244895
=== Actor Training Debug (Iteration 4323) ===
Q mean: -13.421244
Q std: 17.739683
Actor loss: 13.425221
Action reg: 0.003978
  l1.weight: grad_norm = 0.112759
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.094032
Total gradient norm: 0.322703
=== Actor Training Debug (Iteration 4324) ===
Q mean: -12.228353
Q std: 17.926315
Actor loss: 12.232329
Action reg: 0.003976
  l1.weight: grad_norm = 0.108598
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.080939
Total gradient norm: 0.272176
=== Actor Training Debug (Iteration 4325) ===
Q mean: -12.498543
Q std: 16.569181
Actor loss: 12.502509
Action reg: 0.003966
  l1.weight: grad_norm = 0.514975
  l1.bias: grad_norm = 0.001018
  l2.weight: grad_norm = 0.367062
Total gradient norm: 1.594541
=== Actor Training Debug (Iteration 4326) ===
Q mean: -12.888535
Q std: 18.246653
Actor loss: 12.892485
Action reg: 0.003950
  l1.weight: grad_norm = 0.197357
  l1.bias: grad_norm = 0.000870
  l2.weight: grad_norm = 0.169758
Total gradient norm: 0.751702
=== Actor Training Debug (Iteration 4327) ===
Q mean: -13.038044
Q std: 17.291616
Actor loss: 13.042027
Action reg: 0.003984
  l1.weight: grad_norm = 0.223789
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.165284
Total gradient norm: 0.742452
=== Actor Training Debug (Iteration 4328) ===
Q mean: -11.690197
Q std: 16.960794
Actor loss: 11.694174
Action reg: 0.003977
  l1.weight: grad_norm = 0.177140
  l1.bias: grad_norm = 0.000703
  l2.weight: grad_norm = 0.155905
Total gradient norm: 0.511677
=== Actor Training Debug (Iteration 4329) ===
Q mean: -12.345573
Q std: 17.032404
Actor loss: 12.349551
Action reg: 0.003978
  l1.weight: grad_norm = 0.074111
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.068493
Total gradient norm: 0.251679
=== Actor Training Debug (Iteration 4330) ===
Q mean: -14.021605
Q std: 18.890944
Actor loss: 14.025583
Action reg: 0.003978
  l1.weight: grad_norm = 0.187558
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.103690
Total gradient norm: 0.324706
=== Actor Training Debug (Iteration 4331) ===
Q mean: -13.583914
Q std: 16.707039
Actor loss: 13.587903
Action reg: 0.003989
  l1.weight: grad_norm = 0.120636
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.097525
Total gradient norm: 0.323345
=== Actor Training Debug (Iteration 4332) ===
Q mean: -12.036834
Q std: 17.848120
Actor loss: 12.040806
Action reg: 0.003972
  l1.weight: grad_norm = 0.135090
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.098008
Total gradient norm: 0.343377
=== Actor Training Debug (Iteration 4333) ===
Q mean: -13.084599
Q std: 17.914698
Actor loss: 13.088567
Action reg: 0.003968
  l1.weight: grad_norm = 0.087055
  l1.bias: grad_norm = 0.000536
  l2.weight: grad_norm = 0.068312
Total gradient norm: 0.199518
=== Actor Training Debug (Iteration 4334) ===
Q mean: -13.302930
Q std: 17.342525
Actor loss: 13.306881
Action reg: 0.003951
  l1.weight: grad_norm = 0.086499
  l1.bias: grad_norm = 0.001221
  l2.weight: grad_norm = 0.065513
Total gradient norm: 0.192993
=== Actor Training Debug (Iteration 4335) ===
Q mean: -14.970270
Q std: 18.609270
Actor loss: 14.974230
Action reg: 0.003960
  l1.weight: grad_norm = 0.133077
  l1.bias: grad_norm = 0.001023
  l2.weight: grad_norm = 0.100270
Total gradient norm: 0.370708
=== Actor Training Debug (Iteration 4336) ===
Q mean: -13.482069
Q std: 17.653088
Actor loss: 13.486052
Action reg: 0.003983
  l1.weight: grad_norm = 0.037341
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.031932
Total gradient norm: 0.104071
=== Actor Training Debug (Iteration 4337) ===
Q mean: -13.374113
Q std: 17.806791
Actor loss: 13.378097
Action reg: 0.003984
  l1.weight: grad_norm = 0.101873
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.070697
Total gradient norm: 0.274821
=== Actor Training Debug (Iteration 4338) ===
Q mean: -12.426016
Q std: 18.258438
Actor loss: 12.429984
Action reg: 0.003968
  l1.weight: grad_norm = 0.113550
  l1.bias: grad_norm = 0.000839
  l2.weight: grad_norm = 0.093542
Total gradient norm: 0.318042
=== Actor Training Debug (Iteration 4339) ===
Q mean: -13.738888
Q std: 18.629623
Actor loss: 13.742862
Action reg: 0.003974
  l1.weight: grad_norm = 0.066614
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.062816
Total gradient norm: 0.250153
=== Actor Training Debug (Iteration 4340) ===
Q mean: -13.264837
Q std: 18.618210
Actor loss: 13.268818
Action reg: 0.003980
  l1.weight: grad_norm = 0.269439
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.196194
Total gradient norm: 0.742688
=== Actor Training Debug (Iteration 4341) ===
Q mean: -12.409498
Q std: 18.258152
Actor loss: 12.413484
Action reg: 0.003985
  l1.weight: grad_norm = 0.044209
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.037452
Total gradient norm: 0.121308
=== Actor Training Debug (Iteration 4342) ===
Q mean: -11.925182
Q std: 17.226517
Actor loss: 11.929158
Action reg: 0.003976
  l1.weight: grad_norm = 0.108350
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.079774
Total gradient norm: 0.256316
=== Actor Training Debug (Iteration 4343) ===
Q mean: -12.584482
Q std: 18.630947
Actor loss: 12.588442
Action reg: 0.003959
  l1.weight: grad_norm = 0.184133
  l1.bias: grad_norm = 0.000672
  l2.weight: grad_norm = 0.156746
Total gradient norm: 0.483189
=== Actor Training Debug (Iteration 4344) ===
Q mean: -14.119341
Q std: 18.507820
Actor loss: 14.123320
Action reg: 0.003979
  l1.weight: grad_norm = 0.109897
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.069952
Total gradient norm: 0.218911
=== Actor Training Debug (Iteration 4345) ===
Q mean: -14.330893
Q std: 18.491650
Actor loss: 14.334853
Action reg: 0.003960
  l1.weight: grad_norm = 0.101874
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.080867
Total gradient norm: 0.281252
=== Actor Training Debug (Iteration 4346) ===
Q mean: -13.788270
Q std: 19.069752
Actor loss: 13.792223
Action reg: 0.003953
  l1.weight: grad_norm = 0.083531
  l1.bias: grad_norm = 0.001213
  l2.weight: grad_norm = 0.073961
Total gradient norm: 0.234178
=== Actor Training Debug (Iteration 4347) ===
Q mean: -11.119016
Q std: 16.101294
Actor loss: 11.122984
Action reg: 0.003968
  l1.weight: grad_norm = 0.050288
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.038941
Total gradient norm: 0.129388
=== Actor Training Debug (Iteration 4348) ===
Q mean: -13.414820
Q std: 18.801008
Actor loss: 13.418807
Action reg: 0.003987
  l1.weight: grad_norm = 0.019849
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.016802
Total gradient norm: 0.049517
=== Actor Training Debug (Iteration 4349) ===
Q mean: -13.724241
Q std: 18.982323
Actor loss: 13.728210
Action reg: 0.003969
  l1.weight: grad_norm = 0.167680
  l1.bias: grad_norm = 0.000681
  l2.weight: grad_norm = 0.133083
Total gradient norm: 0.432886
=== Actor Training Debug (Iteration 4350) ===
Q mean: -14.344854
Q std: 19.293295
Actor loss: 14.348838
Action reg: 0.003984
  l1.weight: grad_norm = 0.094062
  l1.bias: grad_norm = 0.000611
  l2.weight: grad_norm = 0.083086
Total gradient norm: 0.212111
=== Actor Training Debug (Iteration 4351) ===
Q mean: -12.832251
Q std: 17.261261
Actor loss: 12.836224
Action reg: 0.003973
  l1.weight: grad_norm = 0.096050
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.062707
Total gradient norm: 0.194569
=== Actor Training Debug (Iteration 4352) ===
Q mean: -14.584150
Q std: 18.718128
Actor loss: 14.588124
Action reg: 0.003974
  l1.weight: grad_norm = 0.060363
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.049835
Total gradient norm: 0.168745
=== Actor Training Debug (Iteration 4353) ===
Q mean: -14.713263
Q std: 19.066940
Actor loss: 14.717238
Action reg: 0.003976
  l1.weight: grad_norm = 0.123762
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.101998
Total gradient norm: 0.312583
=== Actor Training Debug (Iteration 4354) ===
Q mean: -12.288422
Q std: 17.276600
Actor loss: 12.292399
Action reg: 0.003978
  l1.weight: grad_norm = 0.138015
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.112187
Total gradient norm: 0.345645
=== Actor Training Debug (Iteration 4355) ===
Q mean: -12.170107
Q std: 17.657059
Actor loss: 12.174088
Action reg: 0.003981
  l1.weight: grad_norm = 0.320926
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.268898
Total gradient norm: 0.748781
=== Actor Training Debug (Iteration 4356) ===
Q mean: -14.530519
Q std: 18.435392
Actor loss: 14.534490
Action reg: 0.003970
  l1.weight: grad_norm = 0.115807
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.092621
Total gradient norm: 0.367057
=== Actor Training Debug (Iteration 4357) ===
Q mean: -12.742333
Q std: 17.293079
Actor loss: 12.746301
Action reg: 0.003967
  l1.weight: grad_norm = 0.100285
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.073045
Total gradient norm: 0.221203
=== Actor Training Debug (Iteration 4358) ===
Q mean: -12.520691
Q std: 17.392029
Actor loss: 12.524669
Action reg: 0.003978
  l1.weight: grad_norm = 0.102796
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.079800
Total gradient norm: 0.281780
=== Actor Training Debug (Iteration 4359) ===
Q mean: -10.906594
Q std: 16.566322
Actor loss: 10.910563
Action reg: 0.003968
  l1.weight: grad_norm = 0.123371
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.095371
Total gradient norm: 0.310036
=== Actor Training Debug (Iteration 4360) ===
Q mean: -11.597546
Q std: 17.596601
Actor loss: 11.601512
Action reg: 0.003966
  l1.weight: grad_norm = 0.140239
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.111868
Total gradient norm: 0.365740
=== Actor Training Debug (Iteration 4361) ===
Q mean: -13.066182
Q std: 17.835821
Actor loss: 13.070164
Action reg: 0.003981
  l1.weight: grad_norm = 0.111702
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.082313
Total gradient norm: 0.327333
=== Actor Training Debug (Iteration 4362) ===
Q mean: -11.982962
Q std: 17.680384
Actor loss: 11.986929
Action reg: 0.003967
  l1.weight: grad_norm = 0.287367
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.203565
Total gradient norm: 0.653122
=== Actor Training Debug (Iteration 4363) ===
Q mean: -13.091118
Q std: 18.467939
Actor loss: 13.095096
Action reg: 0.003978
  l1.weight: grad_norm = 0.104700
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.082424
Total gradient norm: 0.242398
=== Actor Training Debug (Iteration 4364) ===
Q mean: -11.690411
Q std: 17.546019
Actor loss: 11.694390
Action reg: 0.003979
  l1.weight: grad_norm = 0.123335
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.081274
Total gradient norm: 0.239714
=== Actor Training Debug (Iteration 4365) ===
Q mean: -12.321295
Q std: 17.820023
Actor loss: 12.325272
Action reg: 0.003977
  l1.weight: grad_norm = 0.069517
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.063311
Total gradient norm: 0.175703
=== Actor Training Debug (Iteration 4366) ===
Q mean: -12.385745
Q std: 17.047693
Actor loss: 12.389712
Action reg: 0.003967
  l1.weight: grad_norm = 0.174595
  l1.bias: grad_norm = 0.000542
  l2.weight: grad_norm = 0.149846
Total gradient norm: 0.578779
=== Actor Training Debug (Iteration 4367) ===
Q mean: -12.976390
Q std: 17.999481
Actor loss: 12.980360
Action reg: 0.003970
  l1.weight: grad_norm = 0.164477
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.137400
Total gradient norm: 0.422587
=== Actor Training Debug (Iteration 4368) ===
Q mean: -13.762718
Q std: 19.176077
Actor loss: 13.766685
Action reg: 0.003967
  l1.weight: grad_norm = 0.145520
  l1.bias: grad_norm = 0.000633
  l2.weight: grad_norm = 0.104590
Total gradient norm: 0.310613
=== Actor Training Debug (Iteration 4369) ===
Q mean: -12.782860
Q std: 17.615103
Actor loss: 12.786836
Action reg: 0.003976
  l1.weight: grad_norm = 0.169911
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.105343
Total gradient norm: 0.339313
=== Actor Training Debug (Iteration 4370) ===
Q mean: -15.049926
Q std: 18.876360
Actor loss: 15.053887
Action reg: 0.003961
  l1.weight: grad_norm = 0.108757
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.081662
Total gradient norm: 0.266289
=== Actor Training Debug (Iteration 4371) ===
Q mean: -11.678606
Q std: 17.006922
Actor loss: 11.682582
Action reg: 0.003976
  l1.weight: grad_norm = 0.083978
  l1.bias: grad_norm = 0.000668
  l2.weight: grad_norm = 0.062410
Total gradient norm: 0.193876
=== Actor Training Debug (Iteration 4372) ===
Q mean: -11.960924
Q std: 17.817822
Actor loss: 11.964894
Action reg: 0.003970
  l1.weight: grad_norm = 0.125700
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.078655
Total gradient norm: 0.228620
=== Actor Training Debug (Iteration 4373) ===
Q mean: -13.325699
Q std: 17.974552
Actor loss: 13.329682
Action reg: 0.003983
  l1.weight: grad_norm = 0.114445
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.094043
Total gradient norm: 0.330455
=== Actor Training Debug (Iteration 4374) ===
Q mean: -15.633810
Q std: 19.757397
Actor loss: 15.637795
Action reg: 0.003986
  l1.weight: grad_norm = 0.025023
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.018752
Total gradient norm: 0.060689
=== Actor Training Debug (Iteration 4375) ===
Q mean: -10.129936
Q std: 15.991207
Actor loss: 10.133908
Action reg: 0.003972
  l1.weight: grad_norm = 0.055257
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.045830
Total gradient norm: 0.133405
=== Actor Training Debug (Iteration 4376) ===
Q mean: -10.229603
Q std: 15.673999
Actor loss: 10.233560
Action reg: 0.003957
  l1.weight: grad_norm = 0.147331
  l1.bias: grad_norm = 0.000670
  l2.weight: grad_norm = 0.113247
Total gradient norm: 0.342157
=== Actor Training Debug (Iteration 4377) ===
Q mean: -12.369828
Q std: 17.269236
Actor loss: 12.373796
Action reg: 0.003968
  l1.weight: grad_norm = 0.077734
  l1.bias: grad_norm = 0.000669
  l2.weight: grad_norm = 0.055236
Total gradient norm: 0.162709
=== Actor Training Debug (Iteration 4378) ===
Q mean: -13.655282
Q std: 18.409180
Actor loss: 13.659259
Action reg: 0.003977
  l1.weight: grad_norm = 0.180926
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.129759
Total gradient norm: 0.485724
=== Actor Training Debug (Iteration 4379) ===
Q mean: -12.484451
Q std: 17.973442
Actor loss: 12.488428
Action reg: 0.003977
  l1.weight: grad_norm = 0.102284
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.079657
Total gradient norm: 0.335417
=== Actor Training Debug (Iteration 4380) ===
Q mean: -14.055912
Q std: 18.125792
Actor loss: 14.059879
Action reg: 0.003968
  l1.weight: grad_norm = 0.112956
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.097609
Total gradient norm: 0.308915
=== Actor Training Debug (Iteration 4381) ===
Q mean: -14.566207
Q std: 19.008139
Actor loss: 14.570176
Action reg: 0.003969
  l1.weight: grad_norm = 0.093693
  l1.bias: grad_norm = 0.000563
  l2.weight: grad_norm = 0.079781
Total gradient norm: 0.276348
=== Actor Training Debug (Iteration 4382) ===
Q mean: -14.568231
Q std: 19.284142
Actor loss: 14.572206
Action reg: 0.003975
  l1.weight: grad_norm = 0.093680
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.072272
Total gradient norm: 0.223040
=== Actor Training Debug (Iteration 4383) ===
Q mean: -13.903219
Q std: 17.513014
Actor loss: 13.907185
Action reg: 0.003965
  l1.weight: grad_norm = 0.077468
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.068119
Total gradient norm: 0.249311
=== Actor Training Debug (Iteration 4384) ===
Q mean: -12.174930
Q std: 17.949865
Actor loss: 12.178902
Action reg: 0.003972
  l1.weight: grad_norm = 0.140613
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.104061
Total gradient norm: 0.342035
=== Actor Training Debug (Iteration 4385) ===
Q mean: -14.910257
Q std: 19.308949
Actor loss: 14.914248
Action reg: 0.003991
  l1.weight: grad_norm = 0.090345
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.060933
Total gradient norm: 0.174451
=== Actor Training Debug (Iteration 4386) ===
Q mean: -11.823599
Q std: 17.692879
Actor loss: 11.827566
Action reg: 0.003967
  l1.weight: grad_norm = 0.106211
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.081674
Total gradient norm: 0.285483
=== Actor Training Debug (Iteration 4387) ===
Q mean: -12.497698
Q std: 17.577044
Actor loss: 12.501683
Action reg: 0.003985
  l1.weight: grad_norm = 0.063499
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.046450
Total gradient norm: 0.176733
=== Actor Training Debug (Iteration 4388) ===
Q mean: -13.375862
Q std: 18.350075
Actor loss: 13.379830
Action reg: 0.003968
  l1.weight: grad_norm = 0.099278
  l1.bias: grad_norm = 0.000993
  l2.weight: grad_norm = 0.065768
Total gradient norm: 0.234788
=== Actor Training Debug (Iteration 4389) ===
Q mean: -12.414129
Q std: 17.112816
Actor loss: 12.418098
Action reg: 0.003969
  l1.weight: grad_norm = 0.116632
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.094868
Total gradient norm: 0.366993
=== Actor Training Debug (Iteration 4390) ===
Q mean: -14.056673
Q std: 17.953876
Actor loss: 14.060624
Action reg: 0.003951
  l1.weight: grad_norm = 0.143960
  l1.bias: grad_norm = 0.000589
  l2.weight: grad_norm = 0.101546
Total gradient norm: 0.347814
=== Actor Training Debug (Iteration 4391) ===
Q mean: -12.907413
Q std: 18.773537
Actor loss: 12.911368
Action reg: 0.003955
  l1.weight: grad_norm = 0.229877
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.180914
Total gradient norm: 0.714725
=== Actor Training Debug (Iteration 4392) ===
Q mean: -13.141556
Q std: 18.516451
Actor loss: 13.145516
Action reg: 0.003961
  l1.weight: grad_norm = 0.189780
  l1.bias: grad_norm = 0.000579
  l2.weight: grad_norm = 0.133439
Total gradient norm: 0.378677
=== Actor Training Debug (Iteration 4393) ===
Q mean: -12.355989
Q std: 17.229053
Actor loss: 12.359964
Action reg: 0.003975
  l1.weight: grad_norm = 0.081079
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.068493
Total gradient norm: 0.225892
=== Actor Training Debug (Iteration 4394) ===
Q mean: -15.493574
Q std: 19.799875
Actor loss: 15.497551
Action reg: 0.003977
  l1.weight: grad_norm = 0.134951
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.087684
Total gradient norm: 0.268356
=== Actor Training Debug (Iteration 4395) ===
Q mean: -14.018496
Q std: 17.817694
Actor loss: 14.022471
Action reg: 0.003976
  l1.weight: grad_norm = 0.055741
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.050281
Total gradient norm: 0.189606
=== Actor Training Debug (Iteration 4396) ===
Q mean: -12.422820
Q std: 17.951382
Actor loss: 12.426801
Action reg: 0.003980
  l1.weight: grad_norm = 0.152204
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.143671
Total gradient norm: 0.450048
=== Actor Training Debug (Iteration 4397) ===
Q mean: -13.261276
Q std: 18.369354
Actor loss: 13.265243
Action reg: 0.003966
  l1.weight: grad_norm = 0.084662
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.063819
Total gradient norm: 0.243761
=== Actor Training Debug (Iteration 4398) ===
Q mean: -13.173791
Q std: 17.409796
Actor loss: 13.177752
Action reg: 0.003961
  l1.weight: grad_norm = 0.118889
  l1.bias: grad_norm = 0.001794
  l2.weight: grad_norm = 0.093495
Total gradient norm: 0.267267
=== Actor Training Debug (Iteration 4399) ===
Q mean: -13.048543
Q std: 18.567352
Actor loss: 13.052517
Action reg: 0.003974
  l1.weight: grad_norm = 0.078553
  l1.bias: grad_norm = 0.001065
  l2.weight: grad_norm = 0.060856
Total gradient norm: 0.251753
=== Actor Training Debug (Iteration 4400) ===
Q mean: -12.805183
Q std: 17.227955
Actor loss: 12.809158
Action reg: 0.003975
  l1.weight: grad_norm = 0.137345
  l1.bias: grad_norm = 0.000939
  l2.weight: grad_norm = 0.098417
Total gradient norm: 0.325059
=== Actor Training Debug (Iteration 4401) ===
Q mean: -14.545483
Q std: 18.275978
Actor loss: 14.549447
Action reg: 0.003964
  l1.weight: grad_norm = 0.111359
  l1.bias: grad_norm = 0.000519
  l2.weight: grad_norm = 0.093684
Total gradient norm: 0.327239
=== Actor Training Debug (Iteration 4402) ===
Q mean: -14.186530
Q std: 18.874527
Actor loss: 14.190507
Action reg: 0.003977
  l1.weight: grad_norm = 0.071527
  l1.bias: grad_norm = 0.001890
  l2.weight: grad_norm = 0.058604
Total gradient norm: 0.202389
=== Actor Training Debug (Iteration 4403) ===
Q mean: -12.598829
Q std: 18.086811
Actor loss: 12.602794
Action reg: 0.003965
  l1.weight: grad_norm = 0.115893
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.092011
Total gradient norm: 0.317814
=== Actor Training Debug (Iteration 4404) ===
Q mean: -10.523417
Q std: 17.078579
Actor loss: 10.527398
Action reg: 0.003981
  l1.weight: grad_norm = 0.080654
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.059319
Total gradient norm: 0.182442
=== Actor Training Debug (Iteration 4405) ===
Q mean: -11.910697
Q std: 17.452684
Actor loss: 11.914670
Action reg: 0.003973
  l1.weight: grad_norm = 0.056578
  l1.bias: grad_norm = 0.001288
  l2.weight: grad_norm = 0.042467
Total gradient norm: 0.135236
=== Actor Training Debug (Iteration 4406) ===
Q mean: -12.126657
Q std: 17.519676
Actor loss: 12.130634
Action reg: 0.003976
  l1.weight: grad_norm = 0.103172
  l1.bias: grad_norm = 0.001462
  l2.weight: grad_norm = 0.087053
Total gradient norm: 0.298522
=== Actor Training Debug (Iteration 4407) ===
Q mean: -13.382660
Q std: 17.830576
Actor loss: 13.386645
Action reg: 0.003985
  l1.weight: grad_norm = 0.127973
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.088184
Total gradient norm: 0.244063
=== Actor Training Debug (Iteration 4408) ===
Q mean: -12.106281
Q std: 17.797783
Actor loss: 12.110261
Action reg: 0.003979
  l1.weight: grad_norm = 0.130500
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.088424
Total gradient norm: 0.288691
=== Actor Training Debug (Iteration 4409) ===
Q mean: -11.501688
Q std: 16.686127
Actor loss: 11.505670
Action reg: 0.003982
  l1.weight: grad_norm = 0.143517
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.087638
Total gradient norm: 0.282320
=== Actor Training Debug (Iteration 4410) ===
Q mean: -11.581864
Q std: 16.688473
Actor loss: 11.585834
Action reg: 0.003969
  l1.weight: grad_norm = 0.166840
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.115394
Total gradient norm: 0.490928
=== Actor Training Debug (Iteration 4411) ===
Q mean: -12.799003
Q std: 17.540001
Actor loss: 12.802979
Action reg: 0.003975
  l1.weight: grad_norm = 0.105428
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.073151
Total gradient norm: 0.208422
=== Actor Training Debug (Iteration 4412) ===
Q mean: -14.796515
Q std: 18.916466
Actor loss: 14.800489
Action reg: 0.003975
  l1.weight: grad_norm = 0.228747
  l1.bias: grad_norm = 0.000791
  l2.weight: grad_norm = 0.172209
Total gradient norm: 0.625393
=== Actor Training Debug (Iteration 4413) ===
Q mean: -15.148556
Q std: 19.070211
Actor loss: 15.152533
Action reg: 0.003977
  l1.weight: grad_norm = 0.038986
  l1.bias: grad_norm = 0.001121
  l2.weight: grad_norm = 0.030106
Total gradient norm: 0.091758
=== Actor Training Debug (Iteration 4414) ===
Q mean: -15.568737
Q std: 19.599928
Actor loss: 15.572715
Action reg: 0.003978
  l1.weight: grad_norm = 0.131438
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.102740
Total gradient norm: 0.311464
=== Actor Training Debug (Iteration 4415) ===
Q mean: -11.446672
Q std: 17.549288
Actor loss: 11.450653
Action reg: 0.003980
  l1.weight: grad_norm = 0.174842
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.131811
Total gradient norm: 0.563689
=== Actor Training Debug (Iteration 4416) ===
Q mean: -13.003777
Q std: 18.366310
Actor loss: 13.007754
Action reg: 0.003978
  l1.weight: grad_norm = 0.094907
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.067799
Total gradient norm: 0.219926
=== Actor Training Debug (Iteration 4417) ===
Q mean: -13.065537
Q std: 18.396919
Actor loss: 13.069524
Action reg: 0.003987
  l1.weight: grad_norm = 0.210866
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.165021
Total gradient norm: 0.480557
=== Actor Training Debug (Iteration 4418) ===
Q mean: -12.005341
Q std: 17.034367
Actor loss: 12.009320
Action reg: 0.003980
  l1.weight: grad_norm = 0.092668
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.063503
Total gradient norm: 0.213291
=== Actor Training Debug (Iteration 4419) ===
Q mean: -12.016296
Q std: 17.990396
Actor loss: 12.020268
Action reg: 0.003972
  l1.weight: grad_norm = 0.126458
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.092144
Total gradient norm: 0.338892
=== Actor Training Debug (Iteration 4420) ===
Q mean: -13.896194
Q std: 18.673790
Actor loss: 13.900176
Action reg: 0.003983
  l1.weight: grad_norm = 0.052319
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.051093
Total gradient norm: 0.182123
=== Actor Training Debug (Iteration 4421) ===
Q mean: -14.058711
Q std: 18.202438
Actor loss: 14.062686
Action reg: 0.003975
  l1.weight: grad_norm = 0.112186
  l1.bias: grad_norm = 0.000534
  l2.weight: grad_norm = 0.084065
Total gradient norm: 0.329543
=== Actor Training Debug (Iteration 4422) ===
Q mean: -13.276442
Q std: 17.666649
Actor loss: 13.280427
Action reg: 0.003986
  l1.weight: grad_norm = 0.049932
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.038711
Total gradient norm: 0.129133
=== Actor Training Debug (Iteration 4423) ===
Q mean: -13.188938
Q std: 18.189840
Actor loss: 13.192894
Action reg: 0.003956
  l1.weight: grad_norm = 0.128051
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.088694
Total gradient norm: 0.295998
=== Actor Training Debug (Iteration 4424) ===
Q mean: -14.109824
Q std: 18.560661
Actor loss: 14.113811
Action reg: 0.003986
  l1.weight: grad_norm = 0.059027
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.050005
Total gradient norm: 0.173969
=== Actor Training Debug (Iteration 4425) ===
Q mean: -14.134560
Q std: 19.918390
Actor loss: 14.138537
Action reg: 0.003978
  l1.weight: grad_norm = 0.096363
  l1.bias: grad_norm = 0.000577
  l2.weight: grad_norm = 0.067714
Total gradient norm: 0.225849
=== Actor Training Debug (Iteration 4426) ===
Q mean: -14.099297
Q std: 17.804457
Actor loss: 14.103261
Action reg: 0.003964
  l1.weight: grad_norm = 0.125305
  l1.bias: grad_norm = 0.000686
  l2.weight: grad_norm = 0.091924
Total gradient norm: 0.270636
=== Actor Training Debug (Iteration 4427) ===
Q mean: -13.191018
Q std: 18.181187
Actor loss: 13.194997
Action reg: 0.003979
  l1.weight: grad_norm = 0.213484
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.150855
Total gradient norm: 0.569386
=== Actor Training Debug (Iteration 4428) ===
Q mean: -12.877291
Q std: 17.900827
Actor loss: 12.881275
Action reg: 0.003985
  l1.weight: grad_norm = 0.060466
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.048718
Total gradient norm: 0.180635
=== Actor Training Debug (Iteration 4429) ===
Q mean: -12.704946
Q std: 18.419310
Actor loss: 12.708916
Action reg: 0.003970
  l1.weight: grad_norm = 0.279324
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.170278
Total gradient norm: 0.568867
=== Actor Training Debug (Iteration 4430) ===
Q mean: -12.317260
Q std: 17.633659
Actor loss: 12.321235
Action reg: 0.003975
  l1.weight: grad_norm = 0.233904
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.181275
Total gradient norm: 0.591337
=== Actor Training Debug (Iteration 4431) ===
Q mean: -12.437915
Q std: 17.301140
Actor loss: 12.441901
Action reg: 0.003986
  l1.weight: grad_norm = 0.079296
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.069084
Total gradient norm: 0.216401
=== Actor Training Debug (Iteration 4432) ===
Q mean: -15.703886
Q std: 19.505432
Actor loss: 15.707858
Action reg: 0.003973
  l1.weight: grad_norm = 0.144177
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.106853
Total gradient norm: 0.383594
=== Actor Training Debug (Iteration 4433) ===
Q mean: -14.572071
Q std: 19.157257
Actor loss: 14.576037
Action reg: 0.003966
  l1.weight: grad_norm = 0.311280
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.175198
Total gradient norm: 0.543875
=== Actor Training Debug (Iteration 4434) ===
Q mean: -14.126098
Q std: 19.747330
Actor loss: 14.130080
Action reg: 0.003982
  l1.weight: grad_norm = 0.132807
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.111197
Total gradient norm: 0.363159
=== Actor Training Debug (Iteration 4435) ===
Q mean: -11.800548
Q std: 16.790770
Actor loss: 11.804528
Action reg: 0.003981
  l1.weight: grad_norm = 0.123742
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.097159
Total gradient norm: 0.325366
=== Actor Training Debug (Iteration 4436) ===
Q mean: -12.213623
Q std: 18.292431
Actor loss: 12.217591
Action reg: 0.003968
  l1.weight: grad_norm = 0.130364
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.105374
Total gradient norm: 0.347415
=== Actor Training Debug (Iteration 4437) ===
Q mean: -14.406805
Q std: 18.370501
Actor loss: 14.410777
Action reg: 0.003972
  l1.weight: grad_norm = 0.097435
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.081235
Total gradient norm: 0.321910
=== Actor Training Debug (Iteration 4438) ===
Q mean: -12.887375
Q std: 18.472345
Actor loss: 12.891361
Action reg: 0.003986
  l1.weight: grad_norm = 0.180066
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.131802
Total gradient norm: 0.434643
=== Actor Training Debug (Iteration 4439) ===
Q mean: -11.959475
Q std: 17.525625
Actor loss: 11.963439
Action reg: 0.003964
  l1.weight: grad_norm = 0.118535
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.084837
Total gradient norm: 0.291297
=== Actor Training Debug (Iteration 4440) ===
Q mean: -11.701762
Q std: 17.359934
Actor loss: 11.705728
Action reg: 0.003965
  l1.weight: grad_norm = 0.153295
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.131292
Total gradient norm: 0.426573
=== Actor Training Debug (Iteration 4441) ===
Q mean: -15.152781
Q std: 19.646814
Actor loss: 15.156752
Action reg: 0.003970
  l1.weight: grad_norm = 0.128042
  l1.bias: grad_norm = 0.000432
  l2.weight: grad_norm = 0.087589
Total gradient norm: 0.290711
=== Actor Training Debug (Iteration 4442) ===
Q mean: -13.580683
Q std: 18.005144
Actor loss: 13.584650
Action reg: 0.003967
  l1.weight: grad_norm = 0.155969
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.115630
Total gradient norm: 0.350897
=== Actor Training Debug (Iteration 4443) ===
Q mean: -13.022471
Q std: 17.663979
Actor loss: 13.026451
Action reg: 0.003980
  l1.weight: grad_norm = 0.105581
  l1.bias: grad_norm = 0.001709
  l2.weight: grad_norm = 0.079845
Total gradient norm: 0.260882
=== Actor Training Debug (Iteration 4444) ===
Q mean: -12.250589
Q std: 17.527168
Actor loss: 12.254550
Action reg: 0.003961
  l1.weight: grad_norm = 0.123153
  l1.bias: grad_norm = 0.001847
  l2.weight: grad_norm = 0.103984
Total gradient norm: 0.378548
=== Actor Training Debug (Iteration 4445) ===
Q mean: -12.373763
Q std: 17.714273
Actor loss: 12.377726
Action reg: 0.003962
  l1.weight: grad_norm = 0.082986
  l1.bias: grad_norm = 0.002173
  l2.weight: grad_norm = 0.061940
Total gradient norm: 0.210436
=== Actor Training Debug (Iteration 4446) ===
Q mean: -13.496305
Q std: 17.667765
Actor loss: 13.500284
Action reg: 0.003978
  l1.weight: grad_norm = 0.092438
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.066637
Total gradient norm: 0.206759
=== Actor Training Debug (Iteration 4447) ===
Q mean: -11.799553
Q std: 17.589842
Actor loss: 11.803530
Action reg: 0.003977
  l1.weight: grad_norm = 0.077060
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.063121
Total gradient norm: 0.236231
=== Actor Training Debug (Iteration 4448) ===
Q mean: -11.332096
Q std: 17.376114
Actor loss: 11.336082
Action reg: 0.003986
  l1.weight: grad_norm = 0.143886
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.093841
Total gradient norm: 0.262238
=== Actor Training Debug (Iteration 4449) ===
Q mean: -12.627716
Q std: 17.931875
Actor loss: 12.631695
Action reg: 0.003978
  l1.weight: grad_norm = 0.103046
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.079021
Total gradient norm: 0.210209
=== Actor Training Debug (Iteration 4450) ===
Q mean: -13.611797
Q std: 18.999092
Actor loss: 13.615782
Action reg: 0.003984
  l1.weight: grad_norm = 0.058759
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.046686
Total gradient norm: 0.139154
=== Actor Training Debug (Iteration 4451) ===
Q mean: -11.872878
Q std: 16.194138
Actor loss: 11.876857
Action reg: 0.003978
  l1.weight: grad_norm = 0.110025
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.073566
Total gradient norm: 0.201091
=== Actor Training Debug (Iteration 4452) ===
Q mean: -13.555315
Q std: 18.462334
Actor loss: 13.559278
Action reg: 0.003962
  l1.weight: grad_norm = 0.137421
  l1.bias: grad_norm = 0.000584
  l2.weight: grad_norm = 0.098090
Total gradient norm: 0.303577
=== Actor Training Debug (Iteration 4453) ===
Q mean: -14.360965
Q std: 19.078255
Actor loss: 14.364928
Action reg: 0.003963
  l1.weight: grad_norm = 0.042814
  l1.bias: grad_norm = 0.000665
  l2.weight: grad_norm = 0.033493
Total gradient norm: 0.107065
=== Actor Training Debug (Iteration 4454) ===
Q mean: -11.439965
Q std: 17.665897
Actor loss: 11.443937
Action reg: 0.003972
  l1.weight: grad_norm = 0.114016
  l1.bias: grad_norm = 0.000510
  l2.weight: grad_norm = 0.077349
Total gradient norm: 0.291457
=== Actor Training Debug (Iteration 4455) ===
Q mean: -12.079715
Q std: 17.032759
Actor loss: 12.083688
Action reg: 0.003973
  l1.weight: grad_norm = 0.187422
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.161227
Total gradient norm: 0.644397
=== Actor Training Debug (Iteration 4456) ===
Q mean: -12.992342
Q std: 18.637213
Actor loss: 12.996321
Action reg: 0.003978
  l1.weight: grad_norm = 0.145831
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.097007
Total gradient norm: 0.322898
=== Actor Training Debug (Iteration 4457) ===
Q mean: -10.334813
Q std: 16.564772
Actor loss: 10.338786
Action reg: 0.003973
  l1.weight: grad_norm = 0.158503
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.112207
Total gradient norm: 0.350457
=== Actor Training Debug (Iteration 4458) ===
Q mean: -11.672029
Q std: 16.979427
Actor loss: 11.675999
Action reg: 0.003970
  l1.weight: grad_norm = 0.054980
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.037126
Total gradient norm: 0.130570
=== Actor Training Debug (Iteration 4459) ===
Q mean: -11.539764
Q std: 18.195770
Actor loss: 11.543749
Action reg: 0.003984
  l1.weight: grad_norm = 0.049238
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.029153
Total gradient norm: 0.083764
=== Actor Training Debug (Iteration 4460) ===
Q mean: -12.440081
Q std: 18.448444
Actor loss: 12.444040
Action reg: 0.003960
  l1.weight: grad_norm = 0.197411
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.142611
Total gradient norm: 0.473453
=== Actor Training Debug (Iteration 4461) ===
Q mean: -13.777207
Q std: 18.226301
Actor loss: 13.781192
Action reg: 0.003985
  l1.weight: grad_norm = 0.038606
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.024826
Total gradient norm: 0.067809
=== Actor Training Debug (Iteration 4462) ===
Q mean: -12.229557
Q std: 17.318644
Actor loss: 12.233515
Action reg: 0.003958
  l1.weight: grad_norm = 0.182427
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.133229
Total gradient norm: 0.548662
=== Actor Training Debug (Iteration 4463) ===
Q mean: -13.177224
Q std: 18.034775
Actor loss: 13.181207
Action reg: 0.003983
  l1.weight: grad_norm = 0.076823
  l1.bias: grad_norm = 0.000414
  l2.weight: grad_norm = 0.056797
Total gradient norm: 0.202777
=== Actor Training Debug (Iteration 4464) ===
Q mean: -14.175638
Q std: 18.969793
Actor loss: 14.179608
Action reg: 0.003970
  l1.weight: grad_norm = 0.099580
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.067126
Total gradient norm: 0.267497
=== Actor Training Debug (Iteration 4465) ===
Q mean: -13.778055
Q std: 18.415731
Actor loss: 13.782036
Action reg: 0.003981
  l1.weight: grad_norm = 0.099504
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.081736
Total gradient norm: 0.246916
=== Actor Training Debug (Iteration 4466) ===
Q mean: -13.495682
Q std: 18.827259
Actor loss: 13.499661
Action reg: 0.003980
  l1.weight: grad_norm = 0.125122
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.095618
Total gradient norm: 0.369141
=== Actor Training Debug (Iteration 4467) ===
Q mean: -15.305542
Q std: 18.735880
Actor loss: 15.309519
Action reg: 0.003977
  l1.weight: grad_norm = 0.104906
  l1.bias: grad_norm = 0.000595
  l2.weight: grad_norm = 0.069592
Total gradient norm: 0.277622
=== Actor Training Debug (Iteration 4468) ===
Q mean: -13.709309
Q std: 17.287275
Actor loss: 13.713274
Action reg: 0.003965
  l1.weight: grad_norm = 0.104747
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.074651
Total gradient norm: 0.251647
=== Actor Training Debug (Iteration 4469) ===
Q mean: -16.292332
Q std: 18.905565
Actor loss: 16.296303
Action reg: 0.003971
  l1.weight: grad_norm = 0.061982
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.054479
Total gradient norm: 0.169915
=== Actor Training Debug (Iteration 4470) ===
Q mean: -12.830667
Q std: 17.746748
Actor loss: 12.834644
Action reg: 0.003977
  l1.weight: grad_norm = 0.135812
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.118245
Total gradient norm: 0.354980
=== Actor Training Debug (Iteration 4471) ===
Q mean: -12.314895
Q std: 17.862446
Actor loss: 12.318872
Action reg: 0.003978
  l1.weight: grad_norm = 0.085970
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.067104
Total gradient norm: 0.201595
=== Actor Training Debug (Iteration 4472) ===
Q mean: -11.736808
Q std: 17.088833
Actor loss: 11.740772
Action reg: 0.003964
  l1.weight: grad_norm = 0.132437
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.102933
Total gradient norm: 0.282252
=== Actor Training Debug (Iteration 4473) ===
Q mean: -13.651175
Q std: 17.463427
Actor loss: 13.655144
Action reg: 0.003969
  l1.weight: grad_norm = 0.079324
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.057031
Total gradient norm: 0.181192
=== Actor Training Debug (Iteration 4474) ===
Q mean: -13.590796
Q std: 17.632027
Actor loss: 13.594767
Action reg: 0.003970
  l1.weight: grad_norm = 0.108398
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.064653
Total gradient norm: 0.210214
=== Actor Training Debug (Iteration 4475) ===
Q mean: -13.742817
Q std: 18.553955
Actor loss: 13.746796
Action reg: 0.003979
  l1.weight: grad_norm = 0.097650
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.073086
Total gradient norm: 0.242381
=== Actor Training Debug (Iteration 4476) ===
Q mean: -13.183399
Q std: 18.214195
Actor loss: 13.187370
Action reg: 0.003971
  l1.weight: grad_norm = 0.092912
  l1.bias: grad_norm = 0.000585
  l2.weight: grad_norm = 0.068234
Total gradient norm: 0.218363
=== Actor Training Debug (Iteration 4477) ===
Q mean: -13.659378
Q std: 18.676613
Actor loss: 13.663351
Action reg: 0.003973
  l1.weight: grad_norm = 0.070908
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.054225
Total gradient norm: 0.197365
=== Actor Training Debug (Iteration 4478) ===
Q mean: -14.052267
Q std: 18.684296
Actor loss: 14.056249
Action reg: 0.003981
  l1.weight: grad_norm = 0.109137
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.074812
Total gradient norm: 0.266701
=== Actor Training Debug (Iteration 4479) ===
Q mean: -12.752163
Q std: 18.130459
Actor loss: 12.756141
Action reg: 0.003978
  l1.weight: grad_norm = 0.160912
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.096451
Total gradient norm: 0.284195
=== Actor Training Debug (Iteration 4480) ===
Q mean: -12.782445
Q std: 17.020842
Actor loss: 12.786426
Action reg: 0.003981
  l1.weight: grad_norm = 0.110583
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.085841
Total gradient norm: 0.319876
=== Actor Training Debug (Iteration 4481) ===
Q mean: -12.659722
Q std: 17.955055
Actor loss: 12.663702
Action reg: 0.003979
  l1.weight: grad_norm = 0.186422
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.150519
Total gradient norm: 0.525123
=== Actor Training Debug (Iteration 4482) ===
Q mean: -12.691200
Q std: 17.849058
Actor loss: 12.695185
Action reg: 0.003985
  l1.weight: grad_norm = 0.224110
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.150016
Total gradient norm: 0.707454
=== Actor Training Debug (Iteration 4483) ===
Q mean: -11.140465
Q std: 16.483786
Actor loss: 11.144414
Action reg: 0.003949
  l1.weight: grad_norm = 0.124156
  l1.bias: grad_norm = 0.000745
  l2.weight: grad_norm = 0.099720
Total gradient norm: 0.299228
=== Actor Training Debug (Iteration 4484) ===
Q mean: -12.929657
Q std: 17.579174
Actor loss: 12.933644
Action reg: 0.003988
  l1.weight: grad_norm = 0.082510
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.066883
Total gradient norm: 0.197633
=== Actor Training Debug (Iteration 4485) ===
Q mean: -10.497089
Q std: 16.854231
Actor loss: 10.501060
Action reg: 0.003970
  l1.weight: grad_norm = 0.078388
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.063647
Total gradient norm: 0.218570
=== Actor Training Debug (Iteration 4486) ===
Q mean: -10.559952
Q std: 16.734879
Actor loss: 10.563925
Action reg: 0.003973
  l1.weight: grad_norm = 0.358231
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.321435
Total gradient norm: 1.079652
=== Actor Training Debug (Iteration 4487) ===
Q mean: -12.637413
Q std: 17.371838
Actor loss: 12.641384
Action reg: 0.003971
  l1.weight: grad_norm = 0.178083
  l1.bias: grad_norm = 0.000702
  l2.weight: grad_norm = 0.145621
Total gradient norm: 0.496462
=== Actor Training Debug (Iteration 4488) ===
Q mean: -11.011814
Q std: 17.051834
Actor loss: 11.015785
Action reg: 0.003971
  l1.weight: grad_norm = 0.060322
  l1.bias: grad_norm = 0.001305
  l2.weight: grad_norm = 0.054437
Total gradient norm: 0.173220
=== Actor Training Debug (Iteration 4489) ===
Q mean: -13.936493
Q std: 18.627228
Actor loss: 13.940467
Action reg: 0.003974
  l1.weight: grad_norm = 0.116148
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.098557
Total gradient norm: 0.314542
=== Actor Training Debug (Iteration 4490) ===
Q mean: -12.960056
Q std: 18.588390
Actor loss: 12.964025
Action reg: 0.003969
  l1.weight: grad_norm = 0.129543
  l1.bias: grad_norm = 0.000648
  l2.weight: grad_norm = 0.105762
Total gradient norm: 0.413731
=== Actor Training Debug (Iteration 4491) ===
Q mean: -13.630091
Q std: 19.249840
Actor loss: 13.634062
Action reg: 0.003971
  l1.weight: grad_norm = 0.096049
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.072708
Total gradient norm: 0.265224
=== Actor Training Debug (Iteration 4492) ===
Q mean: -14.428034
Q std: 18.281542
Actor loss: 14.432019
Action reg: 0.003985
  l1.weight: grad_norm = 0.029286
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.020976
Total gradient norm: 0.069446
=== Actor Training Debug (Iteration 4493) ===
Q mean: -11.206541
Q std: 17.920740
Actor loss: 11.210499
Action reg: 0.003958
  l1.weight: grad_norm = 0.095091
  l1.bias: grad_norm = 0.000640
  l2.weight: grad_norm = 0.071100
Total gradient norm: 0.231882
=== Actor Training Debug (Iteration 4494) ===
Q mean: -12.342710
Q std: 16.705698
Actor loss: 12.346681
Action reg: 0.003970
  l1.weight: grad_norm = 0.201059
  l1.bias: grad_norm = 0.000859
  l2.weight: grad_norm = 0.141557
Total gradient norm: 0.496550
=== Actor Training Debug (Iteration 4495) ===
Q mean: -13.258419
Q std: 18.210581
Actor loss: 13.262397
Action reg: 0.003978
  l1.weight: grad_norm = 0.122482
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.108509
Total gradient norm: 0.384927
=== Actor Training Debug (Iteration 4496) ===
Q mean: -11.694698
Q std: 18.169556
Actor loss: 11.698688
Action reg: 0.003989
  l1.weight: grad_norm = 0.047708
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.037790
Total gradient norm: 0.108851
=== Actor Training Debug (Iteration 4497) ===
Q mean: -13.376984
Q std: 18.032232
Actor loss: 13.380966
Action reg: 0.003983
  l1.weight: grad_norm = 0.159301
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.114390
Total gradient norm: 0.408124
=== Actor Training Debug (Iteration 4498) ===
Q mean: -15.020809
Q std: 19.902939
Actor loss: 15.024772
Action reg: 0.003963
  l1.weight: grad_norm = 0.105768
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.089654
Total gradient norm: 0.308081
=== Actor Training Debug (Iteration 4499) ===
Q mean: -11.894725
Q std: 17.874826
Actor loss: 11.898709
Action reg: 0.003985
  l1.weight: grad_norm = 0.164052
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.102404
Total gradient norm: 0.317233
=== Actor Training Debug (Iteration 4500) ===
Q mean: -14.008352
Q std: 18.242743
Actor loss: 14.012323
Action reg: 0.003972
  l1.weight: grad_norm = 0.071844
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.049333
Total gradient norm: 0.191033
  Average reward: -331.328 | Average length: 100.0
Evaluation at episode 95: -331.328
=== Actor Training Debug (Iteration 4501) ===
Q mean: -14.280725
Q std: 18.542719
Actor loss: 14.284694
Action reg: 0.003969
  l1.weight: grad_norm = 0.060903
  l1.bias: grad_norm = 0.000628
  l2.weight: grad_norm = 0.052896
Total gradient norm: 0.161054
=== Actor Training Debug (Iteration 4502) ===
Q mean: -14.698284
Q std: 19.130623
Actor loss: 14.702253
Action reg: 0.003970
  l1.weight: grad_norm = 0.139794
  l1.bias: grad_norm = 0.001051
  l2.weight: grad_norm = 0.123045
Total gradient norm: 0.531721
=== Actor Training Debug (Iteration 4503) ===
Q mean: -14.213947
Q std: 18.871254
Actor loss: 14.217915
Action reg: 0.003967
  l1.weight: grad_norm = 0.155865
  l1.bias: grad_norm = 0.001980
  l2.weight: grad_norm = 0.120625
Total gradient norm: 0.401927
=== Actor Training Debug (Iteration 4504) ===
Q mean: -14.036062
Q std: 18.122656
Actor loss: 14.040052
Action reg: 0.003990
  l1.weight: grad_norm = 0.147650
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.115703
Total gradient norm: 0.358832
=== Actor Training Debug (Iteration 4505) ===
Q mean: -12.606748
Q std: 18.014277
Actor loss: 12.610721
Action reg: 0.003973
  l1.weight: grad_norm = 0.103646
  l1.bias: grad_norm = 0.001010
  l2.weight: grad_norm = 0.097763
Total gradient norm: 0.243853
=== Actor Training Debug (Iteration 4506) ===
Q mean: -12.945841
Q std: 17.586266
Actor loss: 12.949804
Action reg: 0.003964
  l1.weight: grad_norm = 0.131674
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.100029
Total gradient norm: 0.361790
=== Actor Training Debug (Iteration 4507) ===
Q mean: -13.600647
Q std: 18.168856
Actor loss: 13.604625
Action reg: 0.003978
  l1.weight: grad_norm = 0.120645
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.073192
Total gradient norm: 0.202511
=== Actor Training Debug (Iteration 4508) ===
Q mean: -12.309769
Q std: 17.763166
Actor loss: 12.313745
Action reg: 0.003977
  l1.weight: grad_norm = 0.156104
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.098127
Total gradient norm: 0.304078
=== Actor Training Debug (Iteration 4509) ===
Q mean: -13.411245
Q std: 18.020102
Actor loss: 13.415230
Action reg: 0.003984
  l1.weight: grad_norm = 0.142320
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.112268
Total gradient norm: 0.392533
=== Actor Training Debug (Iteration 4510) ===
Q mean: -12.981459
Q std: 17.738022
Actor loss: 12.985431
Action reg: 0.003972
  l1.weight: grad_norm = 0.173337
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.149299
Total gradient norm: 0.493965
=== Actor Training Debug (Iteration 4511) ===
Q mean: -14.041758
Q std: 18.292299
Actor loss: 14.045735
Action reg: 0.003978
  l1.weight: grad_norm = 0.075123
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.056660
Total gradient norm: 0.205018
=== Actor Training Debug (Iteration 4512) ===
Q mean: -14.222212
Q std: 19.607544
Actor loss: 14.226185
Action reg: 0.003973
  l1.weight: grad_norm = 0.393588
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.286900
Total gradient norm: 1.045676
=== Actor Training Debug (Iteration 4513) ===
Q mean: -12.749168
Q std: 18.352789
Actor loss: 12.753137
Action reg: 0.003968
  l1.weight: grad_norm = 0.189684
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.128936
Total gradient norm: 0.384179
=== Actor Training Debug (Iteration 4514) ===
Q mean: -10.794474
Q std: 16.232544
Actor loss: 10.798450
Action reg: 0.003976
  l1.weight: grad_norm = 0.098225
  l1.bias: grad_norm = 0.001571
  l2.weight: grad_norm = 0.073499
Total gradient norm: 0.254514
=== Actor Training Debug (Iteration 4515) ===
Q mean: -14.687960
Q std: 19.656191
Actor loss: 14.691910
Action reg: 0.003950
  l1.weight: grad_norm = 0.133169
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.081183
Total gradient norm: 0.272686
=== Actor Training Debug (Iteration 4516) ===
Q mean: -12.755110
Q std: 18.968313
Actor loss: 12.759091
Action reg: 0.003981
  l1.weight: grad_norm = 0.116776
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.087688
Total gradient norm: 0.285895
=== Actor Training Debug (Iteration 4517) ===
Q mean: -14.859726
Q std: 19.510801
Actor loss: 14.863706
Action reg: 0.003980
  l1.weight: grad_norm = 0.026802
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.018270
Total gradient norm: 0.061786
=== Actor Training Debug (Iteration 4518) ===
Q mean: -11.442595
Q std: 18.111191
Actor loss: 11.446576
Action reg: 0.003981
  l1.weight: grad_norm = 0.101005
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.081971
Total gradient norm: 0.271478
=== Actor Training Debug (Iteration 4519) ===
Q mean: -15.162060
Q std: 20.030037
Actor loss: 15.166036
Action reg: 0.003976
  l1.weight: grad_norm = 0.058325
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.043482
Total gradient norm: 0.168788
=== Actor Training Debug (Iteration 4520) ===
Q mean: -13.261539
Q std: 18.194735
Actor loss: 13.265515
Action reg: 0.003977
  l1.weight: grad_norm = 1.288946
  l1.bias: grad_norm = 0.001325
  l2.weight: grad_norm = 0.953507
Total gradient norm: 3.983393
=== Actor Training Debug (Iteration 4521) ===
Q mean: -13.869320
Q std: 19.230042
Actor loss: 13.873302
Action reg: 0.003982
  l1.weight: grad_norm = 0.091064
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.071209
Total gradient norm: 0.252803
=== Actor Training Debug (Iteration 4522) ===
Q mean: -12.672234
Q std: 18.512754
Actor loss: 12.676209
Action reg: 0.003976
  l1.weight: grad_norm = 0.058088
  l1.bias: grad_norm = 0.000529
  l2.weight: grad_norm = 0.039868
Total gradient norm: 0.124690
=== Actor Training Debug (Iteration 4523) ===
Q mean: -13.020557
Q std: 18.183807
Actor loss: 13.024525
Action reg: 0.003967
  l1.weight: grad_norm = 0.201536
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.135794
Total gradient norm: 0.444536
=== Actor Training Debug (Iteration 4524) ===
Q mean: -12.602621
Q std: 18.569031
Actor loss: 12.606603
Action reg: 0.003981
  l1.weight: grad_norm = 0.130039
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.099005
Total gradient norm: 0.286151
=== Actor Training Debug (Iteration 4525) ===
Q mean: -13.164581
Q std: 18.129807
Actor loss: 13.168559
Action reg: 0.003978
  l1.weight: grad_norm = 0.203188
  l1.bias: grad_norm = 0.001274
  l2.weight: grad_norm = 0.145689
Total gradient norm: 0.473530
=== Actor Training Debug (Iteration 4526) ===
Q mean: -12.710426
Q std: 18.121292
Actor loss: 12.714393
Action reg: 0.003966
  l1.weight: grad_norm = 0.078374
  l1.bias: grad_norm = 0.000819
  l2.weight: grad_norm = 0.061869
Total gradient norm: 0.203217
=== Actor Training Debug (Iteration 4527) ===
Q mean: -12.981796
Q std: 18.492786
Actor loss: 12.985768
Action reg: 0.003972
  l1.weight: grad_norm = 0.103207
  l1.bias: grad_norm = 0.000650
  l2.weight: grad_norm = 0.082405
Total gradient norm: 0.229658
=== Actor Training Debug (Iteration 4528) ===
Q mean: -11.604630
Q std: 17.234894
Actor loss: 11.608604
Action reg: 0.003975
  l1.weight: grad_norm = 0.091078
  l1.bias: grad_norm = 0.001022
  l2.weight: grad_norm = 0.062367
Total gradient norm: 0.183625
=== Actor Training Debug (Iteration 4529) ===
Q mean: -11.936962
Q std: 17.469337
Actor loss: 11.940941
Action reg: 0.003978
  l1.weight: grad_norm = 0.087864
  l1.bias: grad_norm = 0.000521
  l2.weight: grad_norm = 0.069326
Total gradient norm: 0.262420
=== Actor Training Debug (Iteration 4530) ===
Q mean: -13.369671
Q std: 17.179430
Actor loss: 13.373641
Action reg: 0.003970
  l1.weight: grad_norm = 0.389237
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.274647
Total gradient norm: 0.808897
=== Actor Training Debug (Iteration 4531) ===
Q mean: -13.692228
Q std: 18.460247
Actor loss: 13.696195
Action reg: 0.003966
  l1.weight: grad_norm = 0.128598
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.101397
Total gradient norm: 0.424871
=== Actor Training Debug (Iteration 4532) ===
Q mean: -13.619610
Q std: 19.281342
Actor loss: 13.623570
Action reg: 0.003960
  l1.weight: grad_norm = 0.254490
  l1.bias: grad_norm = 0.000845
  l2.weight: grad_norm = 0.196478
Total gradient norm: 0.582435
=== Actor Training Debug (Iteration 4533) ===
Q mean: -12.550178
Q std: 17.982306
Actor loss: 12.554149
Action reg: 0.003971
  l1.weight: grad_norm = 0.158021
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.120560
Total gradient norm: 0.363876
=== Actor Training Debug (Iteration 4534) ===
Q mean: -14.358736
Q std: 19.358471
Actor loss: 14.362703
Action reg: 0.003967
  l1.weight: grad_norm = 0.101272
  l1.bias: grad_norm = 0.000894
  l2.weight: grad_norm = 0.073899
Total gradient norm: 0.218532
=== Actor Training Debug (Iteration 4535) ===
Q mean: -14.356385
Q std: 19.050142
Actor loss: 14.360345
Action reg: 0.003960
  l1.weight: grad_norm = 0.215512
  l1.bias: grad_norm = 0.001584
  l2.weight: grad_norm = 0.167324
Total gradient norm: 0.685649
=== Actor Training Debug (Iteration 4536) ===
Q mean: -14.769018
Q std: 17.938915
Actor loss: 14.772990
Action reg: 0.003972
  l1.weight: grad_norm = 0.115791
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.080416
Total gradient norm: 0.280421
=== Actor Training Debug (Iteration 4537) ===
Q mean: -14.834078
Q std: 18.975336
Actor loss: 14.838039
Action reg: 0.003962
  l1.weight: grad_norm = 0.121487
  l1.bias: grad_norm = 0.000875
  l2.weight: grad_norm = 0.087599
Total gradient norm: 0.311914
=== Actor Training Debug (Iteration 4538) ===
Q mean: -13.072263
Q std: 18.450411
Actor loss: 13.076236
Action reg: 0.003973
  l1.weight: grad_norm = 0.036306
  l1.bias: grad_norm = 0.000956
  l2.weight: grad_norm = 0.026185
Total gradient norm: 0.088051
=== Actor Training Debug (Iteration 4539) ===
Q mean: -12.237593
Q std: 17.576857
Actor loss: 12.241571
Action reg: 0.003978
  l1.weight: grad_norm = 0.228595
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.203314
Total gradient norm: 0.588827
=== Actor Training Debug (Iteration 4540) ===
Q mean: -13.952359
Q std: 19.430252
Actor loss: 13.956342
Action reg: 0.003983
  l1.weight: grad_norm = 0.072558
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.049716
Total gradient norm: 0.156083
=== Actor Training Debug (Iteration 4541) ===
Q mean: -11.865414
Q std: 18.019402
Actor loss: 11.869384
Action reg: 0.003970
  l1.weight: grad_norm = 0.090575
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.066694
Total gradient norm: 0.238323
=== Actor Training Debug (Iteration 4542) ===
Q mean: -12.717848
Q std: 18.808695
Actor loss: 12.721837
Action reg: 0.003990
  l1.weight: grad_norm = 0.071159
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.059663
Total gradient norm: 0.172214
=== Actor Training Debug (Iteration 4543) ===
Q mean: -12.736150
Q std: 17.911623
Actor loss: 12.740139
Action reg: 0.003989
  l1.weight: grad_norm = 0.107370
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.080383
Total gradient norm: 0.271844
=== Actor Training Debug (Iteration 4544) ===
Q mean: -13.781798
Q std: 18.164665
Actor loss: 13.785781
Action reg: 0.003982
  l1.weight: grad_norm = 0.134456
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.098521
Total gradient norm: 0.325872
=== Actor Training Debug (Iteration 4545) ===
Q mean: -13.884537
Q std: 18.478189
Actor loss: 13.888509
Action reg: 0.003972
  l1.weight: grad_norm = 0.275207
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.193173
Total gradient norm: 0.633592
=== Actor Training Debug (Iteration 4546) ===
Q mean: -14.687677
Q std: 18.775290
Actor loss: 14.691660
Action reg: 0.003982
  l1.weight: grad_norm = 0.111875
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.108659
Total gradient norm: 0.324583
=== Actor Training Debug (Iteration 4547) ===
Q mean: -13.466933
Q std: 18.556276
Actor loss: 13.470899
Action reg: 0.003965
  l1.weight: grad_norm = 0.141300
  l1.bias: grad_norm = 0.001482
  l2.weight: grad_norm = 0.116480
Total gradient norm: 0.433713
=== Actor Training Debug (Iteration 4548) ===
Q mean: -13.152121
Q std: 18.880009
Actor loss: 13.156090
Action reg: 0.003969
  l1.weight: grad_norm = 0.190339
  l1.bias: grad_norm = 0.001200
  l2.weight: grad_norm = 0.147923
Total gradient norm: 0.528500
=== Actor Training Debug (Iteration 4549) ===
Q mean: -10.772486
Q std: 17.028818
Actor loss: 10.776453
Action reg: 0.003967
  l1.weight: grad_norm = 0.126592
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.088434
Total gradient norm: 0.300421
=== Actor Training Debug (Iteration 4550) ===
Q mean: -13.052486
Q std: 18.580383
Actor loss: 13.056468
Action reg: 0.003982
  l1.weight: grad_norm = 0.054267
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.046657
Total gradient norm: 0.143320
=== Actor Training Debug (Iteration 4551) ===
Q mean: -14.115105
Q std: 18.765463
Actor loss: 14.119074
Action reg: 0.003970
  l1.weight: grad_norm = 0.281807
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.166370
Total gradient norm: 0.553886
=== Actor Training Debug (Iteration 4552) ===
Q mean: -14.135197
Q std: 18.409203
Actor loss: 14.139185
Action reg: 0.003988
  l1.weight: grad_norm = 0.107900
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.087166
Total gradient norm: 0.320162
=== Actor Training Debug (Iteration 4553) ===
Q mean: -11.408346
Q std: 16.735342
Actor loss: 11.412314
Action reg: 0.003969
  l1.weight: grad_norm = 0.186890
  l1.bias: grad_norm = 0.001589
  l2.weight: grad_norm = 0.130598
Total gradient norm: 0.373621
=== Actor Training Debug (Iteration 4554) ===
Q mean: -10.234672
Q std: 16.271936
Actor loss: 10.238644
Action reg: 0.003972
  l1.weight: grad_norm = 0.150347
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.119411
Total gradient norm: 0.389687
=== Actor Training Debug (Iteration 4555) ===
Q mean: -11.271236
Q std: 16.381107
Actor loss: 11.275200
Action reg: 0.003964
  l1.weight: grad_norm = 0.150437
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.118289
Total gradient norm: 0.416485
=== Actor Training Debug (Iteration 4556) ===
Q mean: -13.406694
Q std: 19.080851
Actor loss: 13.410671
Action reg: 0.003977
  l1.weight: grad_norm = 0.132656
  l1.bias: grad_norm = 0.000528
  l2.weight: grad_norm = 0.096038
Total gradient norm: 0.305089
=== Actor Training Debug (Iteration 4557) ===
Q mean: -12.729744
Q std: 17.729773
Actor loss: 12.733707
Action reg: 0.003963
  l1.weight: grad_norm = 0.320986
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.213728
Total gradient norm: 0.707166
=== Actor Training Debug (Iteration 4558) ===
Q mean: -16.094904
Q std: 20.035980
Actor loss: 16.098877
Action reg: 0.003972
  l1.weight: grad_norm = 0.125186
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.094239
Total gradient norm: 0.343232
=== Actor Training Debug (Iteration 4559) ===
Q mean: -13.678137
Q std: 18.245506
Actor loss: 13.682105
Action reg: 0.003969
  l1.weight: grad_norm = 0.176867
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.151875
Total gradient norm: 0.501315
=== Actor Training Debug (Iteration 4560) ===
Q mean: -11.225741
Q std: 18.525091
Actor loss: 11.229709
Action reg: 0.003967
  l1.weight: grad_norm = 0.102473
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.076355
Total gradient norm: 0.261212
=== Actor Training Debug (Iteration 4561) ===
Q mean: -12.248608
Q std: 17.612642
Actor loss: 12.252580
Action reg: 0.003972
  l1.weight: grad_norm = 0.146984
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.125086
Total gradient norm: 0.341920
=== Actor Training Debug (Iteration 4562) ===
Q mean: -13.468780
Q std: 18.727873
Actor loss: 13.472754
Action reg: 0.003974
  l1.weight: grad_norm = 0.099613
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.089595
Total gradient norm: 0.326401
=== Actor Training Debug (Iteration 4563) ===
Q mean: -14.859297
Q std: 18.915993
Actor loss: 14.863276
Action reg: 0.003980
  l1.weight: grad_norm = 0.083095
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.069336
Total gradient norm: 0.199402
=== Actor Training Debug (Iteration 4564) ===
Q mean: -12.396787
Q std: 18.167439
Actor loss: 12.400764
Action reg: 0.003977
  l1.weight: grad_norm = 0.106291
  l1.bias: grad_norm = 0.000638
  l2.weight: grad_norm = 0.076061
Total gradient norm: 0.262663
=== Actor Training Debug (Iteration 4565) ===
Q mean: -11.179899
Q std: 17.153379
Actor loss: 11.183872
Action reg: 0.003973
  l1.weight: grad_norm = 0.270031
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.212274
Total gradient norm: 0.712406
=== Actor Training Debug (Iteration 4566) ===
Q mean: -12.259509
Q std: 16.659025
Actor loss: 12.263491
Action reg: 0.003982
  l1.weight: grad_norm = 0.068510
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.042783
Total gradient norm: 0.130996
=== Actor Training Debug (Iteration 4567) ===
Q mean: -13.837551
Q std: 18.921608
Actor loss: 13.841534
Action reg: 0.003983
  l1.weight: grad_norm = 0.077955
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.059952
Total gradient norm: 0.193045
=== Actor Training Debug (Iteration 4568) ===
Q mean: -13.421396
Q std: 17.884031
Actor loss: 13.425379
Action reg: 0.003983
  l1.weight: grad_norm = 0.191037
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.121372
Total gradient norm: 0.388484
=== Actor Training Debug (Iteration 4569) ===
Q mean: -14.285224
Q std: 19.110634
Actor loss: 14.289203
Action reg: 0.003978
  l1.weight: grad_norm = 0.122110
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.091820
Total gradient norm: 0.321754
=== Actor Training Debug (Iteration 4570) ===
Q mean: -15.150557
Q std: 18.536041
Actor loss: 15.154531
Action reg: 0.003974
  l1.weight: grad_norm = 0.123409
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.073472
Total gradient norm: 0.227854
=== Actor Training Debug (Iteration 4571) ===
Q mean: -14.381262
Q std: 19.127714
Actor loss: 14.385232
Action reg: 0.003970
  l1.weight: grad_norm = 0.311663
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.204762
Total gradient norm: 0.628494
=== Actor Training Debug (Iteration 4572) ===
Q mean: -12.215391
Q std: 17.078711
Actor loss: 12.219357
Action reg: 0.003965
  l1.weight: grad_norm = 0.132903
  l1.bias: grad_norm = 0.000628
  l2.weight: grad_norm = 0.109804
Total gradient norm: 0.406573
=== Actor Training Debug (Iteration 4573) ===
Q mean: -13.282243
Q std: 17.339327
Actor loss: 13.286217
Action reg: 0.003974
  l1.weight: grad_norm = 0.145013
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.106213
Total gradient norm: 0.349521
=== Actor Training Debug (Iteration 4574) ===
Q mean: -12.577297
Q std: 17.560326
Actor loss: 12.581278
Action reg: 0.003980
  l1.weight: grad_norm = 0.043835
  l1.bias: grad_norm = 0.000676
  l2.weight: grad_norm = 0.036858
Total gradient norm: 0.130597
=== Actor Training Debug (Iteration 4575) ===
Q mean: -11.645744
Q std: 18.234964
Actor loss: 11.649724
Action reg: 0.003979
  l1.weight: grad_norm = 0.130922
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.108646
Total gradient norm: 0.373457
=== Actor Training Debug (Iteration 4576) ===
Q mean: -13.595064
Q std: 17.893200
Actor loss: 13.599047
Action reg: 0.003982
  l1.weight: grad_norm = 0.065890
  l1.bias: grad_norm = 0.001832
  l2.weight: grad_norm = 0.057433
Total gradient norm: 0.256601
=== Actor Training Debug (Iteration 4577) ===
Q mean: -12.783192
Q std: 17.770752
Actor loss: 12.787163
Action reg: 0.003971
  l1.weight: grad_norm = 0.143231
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.086870
Total gradient norm: 0.280303
=== Actor Training Debug (Iteration 4578) ===
Q mean: -14.072959
Q std: 17.900061
Actor loss: 14.076926
Action reg: 0.003967
  l1.weight: grad_norm = 0.260763
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.197686
Total gradient norm: 0.610432
=== Actor Training Debug (Iteration 4579) ===
Q mean: -12.110230
Q std: 17.653191
Actor loss: 12.114198
Action reg: 0.003968
  l1.weight: grad_norm = 0.062149
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.045210
Total gradient norm: 0.159151
=== Actor Training Debug (Iteration 4580) ===
Q mean: -14.557445
Q std: 18.628313
Actor loss: 14.561410
Action reg: 0.003965
  l1.weight: grad_norm = 0.149435
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.110500
Total gradient norm: 0.347163
=== Actor Training Debug (Iteration 4581) ===
Q mean: -12.013071
Q std: 17.942738
Actor loss: 12.017043
Action reg: 0.003972
  l1.weight: grad_norm = 0.142285
  l1.bias: grad_norm = 0.001972
  l2.weight: grad_norm = 0.095589
Total gradient norm: 0.355661
=== Actor Training Debug (Iteration 4582) ===
Q mean: -13.422295
Q std: 17.707041
Actor loss: 13.426278
Action reg: 0.003983
  l1.weight: grad_norm = 0.095270
  l1.bias: grad_norm = 0.001143
  l2.weight: grad_norm = 0.082117
Total gradient norm: 0.293785
=== Actor Training Debug (Iteration 4583) ===
Q mean: -14.180252
Q std: 18.072773
Actor loss: 14.184235
Action reg: 0.003983
  l1.weight: grad_norm = 0.160323
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.122611
Total gradient norm: 0.414213
=== Actor Training Debug (Iteration 4584) ===
Q mean: -12.790863
Q std: 18.447294
Actor loss: 12.794831
Action reg: 0.003968
  l1.weight: grad_norm = 0.167938
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.131585
Total gradient norm: 0.411008
=== Actor Training Debug (Iteration 4585) ===
Q mean: -12.404886
Q std: 17.902061
Actor loss: 12.408854
Action reg: 0.003968
  l1.weight: grad_norm = 0.156709
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.117256
Total gradient norm: 0.287116
=== Actor Training Debug (Iteration 4586) ===
Q mean: -13.773027
Q std: 19.088140
Actor loss: 13.777011
Action reg: 0.003983
  l1.weight: grad_norm = 0.064377
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.048257
Total gradient norm: 0.159386
=== Actor Training Debug (Iteration 4587) ===
Q mean: -13.862219
Q std: 18.913437
Actor loss: 13.866198
Action reg: 0.003978
  l1.weight: grad_norm = 0.078314
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.059036
Total gradient norm: 0.206776
=== Actor Training Debug (Iteration 4588) ===
Q mean: -11.976881
Q std: 17.683178
Actor loss: 11.980861
Action reg: 0.003979
  l1.weight: grad_norm = 0.041410
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.035930
Total gradient norm: 0.136465
=== Actor Training Debug (Iteration 4589) ===
Q mean: -12.388018
Q std: 18.392710
Actor loss: 12.391969
Action reg: 0.003951
  l1.weight: grad_norm = 0.228613
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.182123
Total gradient norm: 0.584130
=== Actor Training Debug (Iteration 4590) ===
Q mean: -12.335823
Q std: 18.371914
Actor loss: 12.339797
Action reg: 0.003974
  l1.weight: grad_norm = 0.141825
  l1.bias: grad_norm = 0.000563
  l2.weight: grad_norm = 0.107463
Total gradient norm: 0.322145
=== Actor Training Debug (Iteration 4591) ===
Q mean: -12.537766
Q std: 17.824360
Actor loss: 12.541724
Action reg: 0.003959
  l1.weight: grad_norm = 0.316052
  l1.bias: grad_norm = 0.000566
  l2.weight: grad_norm = 0.243520
Total gradient norm: 0.930085
=== Actor Training Debug (Iteration 4592) ===
Q mean: -12.564262
Q std: 18.188051
Actor loss: 12.568235
Action reg: 0.003973
  l1.weight: grad_norm = 0.194078
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.160812
Total gradient norm: 0.522425
=== Actor Training Debug (Iteration 4593) ===
Q mean: -12.973295
Q std: 18.010679
Actor loss: 12.977282
Action reg: 0.003986
  l1.weight: grad_norm = 0.091260
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.068937
Total gradient norm: 0.237508
=== Actor Training Debug (Iteration 4594) ===
Q mean: -13.134927
Q std: 17.676466
Actor loss: 13.138906
Action reg: 0.003980
  l1.weight: grad_norm = 0.088644
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.058078
Total gradient norm: 0.201602
=== Actor Training Debug (Iteration 4595) ===
Q mean: -13.475611
Q std: 18.057434
Actor loss: 13.479601
Action reg: 0.003990
  l1.weight: grad_norm = 0.112410
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.101372
Total gradient norm: 0.350085
=== Actor Training Debug (Iteration 4596) ===
Q mean: -13.942884
Q std: 18.143841
Actor loss: 13.946863
Action reg: 0.003979
  l1.weight: grad_norm = 0.149662
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.111644
Total gradient norm: 0.325993
=== Actor Training Debug (Iteration 4597) ===
Q mean: -13.192150
Q std: 17.713762
Actor loss: 13.196126
Action reg: 0.003976
  l1.weight: grad_norm = 0.128378
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.093175
Total gradient norm: 0.309869
=== Actor Training Debug (Iteration 4598) ===
Q mean: -12.036606
Q std: 17.506544
Actor loss: 12.040577
Action reg: 0.003971
  l1.weight: grad_norm = 0.151921
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.112859
Total gradient norm: 0.347190
=== Actor Training Debug (Iteration 4599) ===
Q mean: -12.441635
Q std: 18.326666
Actor loss: 12.445617
Action reg: 0.003981
  l1.weight: grad_norm = 0.139211
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.118679
Total gradient norm: 0.404954
=== Actor Training Debug (Iteration 4600) ===
Q mean: -12.845942
Q std: 18.198582
Actor loss: 12.849912
Action reg: 0.003970
  l1.weight: grad_norm = 0.130742
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.103444
Total gradient norm: 0.328394
=== Actor Training Debug (Iteration 4601) ===
Q mean: -13.545322
Q std: 18.036158
Actor loss: 13.549305
Action reg: 0.003982
  l1.weight: grad_norm = 0.153326
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.120368
Total gradient norm: 0.357301
=== Actor Training Debug (Iteration 4602) ===
Q mean: -10.835093
Q std: 17.901493
Actor loss: 10.839067
Action reg: 0.003974
  l1.weight: grad_norm = 0.164708
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.123683
Total gradient norm: 0.399138
=== Actor Training Debug (Iteration 4603) ===
Q mean: -13.809961
Q std: 19.057230
Actor loss: 13.813939
Action reg: 0.003978
  l1.weight: grad_norm = 0.077889
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.070879
Total gradient norm: 0.249828
=== Actor Training Debug (Iteration 4604) ===
Q mean: -12.218461
Q std: 17.333359
Actor loss: 12.222443
Action reg: 0.003981
  l1.weight: grad_norm = 0.057292
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.043041
Total gradient norm: 0.144933
=== Actor Training Debug (Iteration 4605) ===
Q mean: -11.667479
Q std: 17.514854
Actor loss: 11.671457
Action reg: 0.003979
  l1.weight: grad_norm = 0.063597
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.046894
Total gradient norm: 0.187020
=== Actor Training Debug (Iteration 4606) ===
Q mean: -11.930706
Q std: 18.174234
Actor loss: 11.934684
Action reg: 0.003978
  l1.weight: grad_norm = 0.098517
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.077629
Total gradient norm: 0.287928
=== Actor Training Debug (Iteration 4607) ===
Q mean: -12.792488
Q std: 17.992411
Actor loss: 12.796463
Action reg: 0.003975
  l1.weight: grad_norm = 0.082155
  l1.bias: grad_norm = 0.000543
  l2.weight: grad_norm = 0.055529
Total gradient norm: 0.208979
=== Actor Training Debug (Iteration 4608) ===
Q mean: -12.928115
Q std: 17.770834
Actor loss: 12.932096
Action reg: 0.003981
  l1.weight: grad_norm = 0.144985
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.114317
Total gradient norm: 0.371839
=== Actor Training Debug (Iteration 4609) ===
Q mean: -15.641459
Q std: 19.713589
Actor loss: 15.645439
Action reg: 0.003981
  l1.weight: grad_norm = 0.044910
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.031096
Total gradient norm: 0.104081
=== Actor Training Debug (Iteration 4610) ===
Q mean: -13.623453
Q std: 17.930248
Actor loss: 13.627427
Action reg: 0.003974
  l1.weight: grad_norm = 0.146636
  l1.bias: grad_norm = 0.000618
  l2.weight: grad_norm = 0.100888
Total gradient norm: 0.310206
=== Actor Training Debug (Iteration 4611) ===
Q mean: -12.280416
Q std: 17.780430
Actor loss: 12.284392
Action reg: 0.003976
  l1.weight: grad_norm = 0.126795
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.096715
Total gradient norm: 0.346959
=== Actor Training Debug (Iteration 4612) ===
Q mean: -12.807671
Q std: 17.682053
Actor loss: 12.811649
Action reg: 0.003978
  l1.weight: grad_norm = 0.128365
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.115925
Total gradient norm: 0.524229
=== Actor Training Debug (Iteration 4613) ===
Q mean: -12.221716
Q std: 17.574011
Actor loss: 12.225697
Action reg: 0.003981
  l1.weight: grad_norm = 0.119395
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.093348
Total gradient norm: 0.253841
=== Actor Training Debug (Iteration 4614) ===
Q mean: -13.275696
Q std: 18.765135
Actor loss: 13.279663
Action reg: 0.003967
  l1.weight: grad_norm = 0.235335
  l1.bias: grad_norm = 0.000679
  l2.weight: grad_norm = 0.172356
Total gradient norm: 0.594222
=== Actor Training Debug (Iteration 4615) ===
Q mean: -12.102682
Q std: 17.088402
Actor loss: 12.106657
Action reg: 0.003975
  l1.weight: grad_norm = 0.114642
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.091041
Total gradient norm: 0.353370
=== Actor Training Debug (Iteration 4616) ===
Q mean: -12.109629
Q std: 18.112114
Actor loss: 12.113607
Action reg: 0.003978
  l1.weight: grad_norm = 0.133554
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.096815
Total gradient norm: 0.349676
=== Actor Training Debug (Iteration 4617) ===
Q mean: -12.983371
Q std: 18.650204
Actor loss: 12.987349
Action reg: 0.003977
  l1.weight: grad_norm = 0.074392
  l1.bias: grad_norm = 0.000629
  l2.weight: grad_norm = 0.058099
Total gradient norm: 0.177548
=== Actor Training Debug (Iteration 4618) ===
Q mean: -13.777474
Q std: 18.876909
Actor loss: 13.781446
Action reg: 0.003972
  l1.weight: grad_norm = 0.083077
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.060027
Total gradient norm: 0.239921
=== Actor Training Debug (Iteration 4619) ===
Q mean: -14.088740
Q std: 18.650913
Actor loss: 14.092719
Action reg: 0.003979
  l1.weight: grad_norm = 0.088310
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.067647
Total gradient norm: 0.221680
=== Actor Training Debug (Iteration 4620) ===
Q mean: -13.703806
Q std: 18.879129
Actor loss: 13.707788
Action reg: 0.003982
  l1.weight: grad_norm = 0.284051
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.230899
Total gradient norm: 0.746670
=== Actor Training Debug (Iteration 4621) ===
Q mean: -12.889576
Q std: 18.590494
Actor loss: 12.893549
Action reg: 0.003973
  l1.weight: grad_norm = 0.112218
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.080660
Total gradient norm: 0.323290
=== Actor Training Debug (Iteration 4622) ===
Q mean: -13.181030
Q std: 18.112122
Actor loss: 13.185005
Action reg: 0.003975
  l1.weight: grad_norm = 0.079982
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.064460
Total gradient norm: 0.217910
=== Actor Training Debug (Iteration 4623) ===
Q mean: -14.718391
Q std: 19.371323
Actor loss: 14.722354
Action reg: 0.003963
  l1.weight: grad_norm = 0.232567
  l1.bias: grad_norm = 0.002049
  l2.weight: grad_norm = 0.161747
Total gradient norm: 0.495696
=== Actor Training Debug (Iteration 4624) ===
Q mean: -15.538857
Q std: 20.569687
Actor loss: 15.542825
Action reg: 0.003968
  l1.weight: grad_norm = 0.115200
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.084842
Total gradient norm: 0.256353
=== Actor Training Debug (Iteration 4625) ===
Q mean: -12.642565
Q std: 17.714979
Actor loss: 12.646539
Action reg: 0.003974
  l1.weight: grad_norm = 0.102292
  l1.bias: grad_norm = 0.001718
  l2.weight: grad_norm = 0.083394
Total gradient norm: 0.281144
=== Actor Training Debug (Iteration 4626) ===
Q mean: -13.020992
Q std: 18.626392
Actor loss: 13.024961
Action reg: 0.003969
  l1.weight: grad_norm = 0.156242
  l1.bias: grad_norm = 0.001756
  l2.weight: grad_norm = 0.115384
Total gradient norm: 0.337600
=== Actor Training Debug (Iteration 4627) ===
Q mean: -12.529503
Q std: 17.481773
Actor loss: 12.533490
Action reg: 0.003987
  l1.weight: grad_norm = 0.122344
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.085871
Total gradient norm: 0.245194
=== Actor Training Debug (Iteration 4628) ===
Q mean: -14.815060
Q std: 19.240437
Actor loss: 14.819046
Action reg: 0.003986
  l1.weight: grad_norm = 0.044882
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.031903
Total gradient norm: 0.092273
=== Actor Training Debug (Iteration 4629) ===
Q mean: -14.124670
Q std: 19.589739
Actor loss: 14.128639
Action reg: 0.003970
  l1.weight: grad_norm = 0.128301
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.093339
Total gradient norm: 0.285195
=== Actor Training Debug (Iteration 4630) ===
Q mean: -15.381641
Q std: 18.859863
Actor loss: 15.385618
Action reg: 0.003977
  l1.weight: grad_norm = 0.158065
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.133581
Total gradient norm: 0.526986
=== Actor Training Debug (Iteration 4631) ===
Q mean: -13.172909
Q std: 18.853933
Actor loss: 13.176890
Action reg: 0.003981
  l1.weight: grad_norm = 0.051464
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.037900
Total gradient norm: 0.116475
=== Actor Training Debug (Iteration 4632) ===
Q mean: -12.214128
Q std: 17.530958
Actor loss: 12.218095
Action reg: 0.003967
  l1.weight: grad_norm = 0.159819
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.107724
Total gradient norm: 0.318567
=== Actor Training Debug (Iteration 4633) ===
Q mean: -12.620094
Q std: 18.968719
Actor loss: 12.624070
Action reg: 0.003976
  l1.weight: grad_norm = 0.085295
  l1.bias: grad_norm = 0.000432
  l2.weight: grad_norm = 0.084590
Total gradient norm: 0.247856
=== Actor Training Debug (Iteration 4634) ===
Q mean: -14.346704
Q std: 18.384901
Actor loss: 14.350683
Action reg: 0.003978
  l1.weight: grad_norm = 0.069882
  l1.bias: grad_norm = 0.000432
  l2.weight: grad_norm = 0.049486
Total gradient norm: 0.167225
=== Actor Training Debug (Iteration 4635) ===
Q mean: -11.813835
Q std: 17.614960
Actor loss: 11.817804
Action reg: 0.003969
  l1.weight: grad_norm = 0.067313
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.047682
Total gradient norm: 0.140087
=== Actor Training Debug (Iteration 4636) ===
Q mean: -12.546492
Q std: 17.433418
Actor loss: 12.550466
Action reg: 0.003974
  l1.weight: grad_norm = 0.104859
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.070624
Total gradient norm: 0.228866
=== Actor Training Debug (Iteration 4637) ===
Q mean: -14.902718
Q std: 19.193033
Actor loss: 14.906692
Action reg: 0.003974
  l1.weight: grad_norm = 0.159723
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.135709
Total gradient norm: 0.550770
=== Actor Training Debug (Iteration 4638) ===
Q mean: -15.218292
Q std: 19.738699
Actor loss: 15.222268
Action reg: 0.003976
  l1.weight: grad_norm = 0.064029
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.043824
Total gradient norm: 0.153865
=== Actor Training Debug (Iteration 4639) ===
Q mean: -13.562730
Q std: 18.627476
Actor loss: 13.566706
Action reg: 0.003976
  l1.weight: grad_norm = 0.062399
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.055430
Total gradient norm: 0.191932
=== Actor Training Debug (Iteration 4640) ===
Q mean: -12.187576
Q std: 18.382210
Actor loss: 12.191547
Action reg: 0.003971
  l1.weight: grad_norm = 0.040295
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.031018
Total gradient norm: 0.116738
=== Actor Training Debug (Iteration 4641) ===
Q mean: -14.546932
Q std: 18.611320
Actor loss: 14.550907
Action reg: 0.003975
  l1.weight: grad_norm = 0.141053
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.106504
Total gradient norm: 0.342622
=== Actor Training Debug (Iteration 4642) ===
Q mean: -12.573330
Q std: 19.084782
Actor loss: 12.577302
Action reg: 0.003972
  l1.weight: grad_norm = 0.137135
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.120973
Total gradient norm: 0.346597
=== Actor Training Debug (Iteration 4643) ===
Q mean: -13.738916
Q std: 17.676704
Actor loss: 13.742900
Action reg: 0.003983
  l1.weight: grad_norm = 0.052065
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.040416
Total gradient norm: 0.148869
=== Actor Training Debug (Iteration 4644) ===
Q mean: -13.296547
Q std: 18.422266
Actor loss: 13.300507
Action reg: 0.003959
  l1.weight: grad_norm = 0.115565
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.098397
Total gradient norm: 0.288166
=== Actor Training Debug (Iteration 4645) ===
Q mean: -12.900963
Q std: 17.827633
Actor loss: 12.904938
Action reg: 0.003975
  l1.weight: grad_norm = 0.183425
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.137529
Total gradient norm: 0.432976
=== Actor Training Debug (Iteration 4646) ===
Q mean: -14.294764
Q std: 19.690826
Actor loss: 14.298744
Action reg: 0.003981
  l1.weight: grad_norm = 0.166320
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.111725
Total gradient norm: 0.359313
=== Actor Training Debug (Iteration 4647) ===
Q mean: -13.547428
Q std: 18.074652
Actor loss: 13.551394
Action reg: 0.003967
  l1.weight: grad_norm = 0.146685
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.134962
Total gradient norm: 0.397715
=== Actor Training Debug (Iteration 4648) ===
Q mean: -13.363651
Q std: 18.691851
Actor loss: 13.367641
Action reg: 0.003990
  l1.weight: grad_norm = 0.061040
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.047891
Total gradient norm: 0.134256
=== Actor Training Debug (Iteration 4649) ===
Q mean: -12.826535
Q std: 18.170422
Actor loss: 12.830511
Action reg: 0.003976
  l1.weight: grad_norm = 0.156760
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.112340
Total gradient norm: 0.394490
=== Actor Training Debug (Iteration 4650) ===
Q mean: -11.558925
Q std: 16.787035
Actor loss: 11.562902
Action reg: 0.003977
  l1.weight: grad_norm = 0.106213
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.075208
Total gradient norm: 0.260727
=== Actor Training Debug (Iteration 4651) ===
Q mean: -14.063438
Q std: 18.432041
Actor loss: 14.067411
Action reg: 0.003973
  l1.weight: grad_norm = 0.182221
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.140596
Total gradient norm: 0.449662
=== Actor Training Debug (Iteration 4652) ===
Q mean: -13.544580
Q std: 17.995680
Actor loss: 13.548552
Action reg: 0.003971
  l1.weight: grad_norm = 0.127509
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.088886
Total gradient norm: 0.306889
=== Actor Training Debug (Iteration 4653) ===
Q mean: -12.089832
Q std: 17.271309
Actor loss: 12.093811
Action reg: 0.003979
  l1.weight: grad_norm = 0.135520
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.103702
Total gradient norm: 0.340412
=== Actor Training Debug (Iteration 4654) ===
Q mean: -12.910869
Q std: 18.111305
Actor loss: 12.914843
Action reg: 0.003974
  l1.weight: grad_norm = 0.105296
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.077463
Total gradient norm: 0.269088
=== Actor Training Debug (Iteration 4655) ===
Q mean: -13.356106
Q std: 18.350239
Actor loss: 13.360085
Action reg: 0.003980
  l1.weight: grad_norm = 0.047907
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.044797
Total gradient norm: 0.158403
=== Actor Training Debug (Iteration 4656) ===
Q mean: -16.410622
Q std: 20.141676
Actor loss: 16.414602
Action reg: 0.003981
  l1.weight: grad_norm = 0.047398
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.039276
Total gradient norm: 0.134837
=== Actor Training Debug (Iteration 4657) ===
Q mean: -12.859188
Q std: 17.390514
Actor loss: 12.863170
Action reg: 0.003982
  l1.weight: grad_norm = 0.081552
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.067489
Total gradient norm: 0.215621
=== Actor Training Debug (Iteration 4658) ===
Q mean: -11.042223
Q std: 16.107901
Actor loss: 11.046197
Action reg: 0.003974
  l1.weight: grad_norm = 0.044588
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.030778
Total gradient norm: 0.105238
=== Actor Training Debug (Iteration 4659) ===
Q mean: -13.431646
Q std: 18.018349
Actor loss: 13.435613
Action reg: 0.003966
  l1.weight: grad_norm = 0.134570
  l1.bias: grad_norm = 0.002346
  l2.weight: grad_norm = 0.084466
Total gradient norm: 0.288471
=== Actor Training Debug (Iteration 4660) ===
Q mean: -12.597588
Q std: 18.137922
Actor loss: 12.601568
Action reg: 0.003981
  l1.weight: grad_norm = 0.068651
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.060201
Total gradient norm: 0.171146
=== Actor Training Debug (Iteration 4661) ===
Q mean: -14.347226
Q std: 18.003613
Actor loss: 14.351214
Action reg: 0.003989
  l1.weight: grad_norm = 0.115231
  l1.bias: grad_norm = 0.001438
  l2.weight: grad_norm = 0.080927
Total gradient norm: 0.285695
=== Actor Training Debug (Iteration 4662) ===
Q mean: -13.338232
Q std: 18.121098
Actor loss: 13.342217
Action reg: 0.003985
  l1.weight: grad_norm = 0.006741
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.004644
Total gradient norm: 0.014875
=== Actor Training Debug (Iteration 4663) ===
Q mean: -13.107828
Q std: 18.394573
Actor loss: 13.111801
Action reg: 0.003973
  l1.weight: grad_norm = 0.074041
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.059140
Total gradient norm: 0.195772
=== Actor Training Debug (Iteration 4664) ===
Q mean: -12.783767
Q std: 18.213232
Actor loss: 12.787725
Action reg: 0.003959
  l1.weight: grad_norm = 0.079342
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.057757
Total gradient norm: 0.191127
=== Actor Training Debug (Iteration 4665) ===
Q mean: -12.192863
Q std: 17.877211
Actor loss: 12.196830
Action reg: 0.003966
  l1.weight: grad_norm = 0.109728
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.075716
Total gradient norm: 0.228402
=== Actor Training Debug (Iteration 4666) ===
Q mean: -13.459734
Q std: 18.138288
Actor loss: 13.463711
Action reg: 0.003977
  l1.weight: grad_norm = 0.092526
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.083110
Total gradient norm: 0.241344
=== Actor Training Debug (Iteration 4667) ===
Q mean: -12.891935
Q std: 18.319387
Actor loss: 12.895905
Action reg: 0.003969
  l1.weight: grad_norm = 0.178799
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.126809
Total gradient norm: 0.483652
=== Actor Training Debug (Iteration 4668) ===
Q mean: -13.827090
Q std: 18.419945
Actor loss: 13.831061
Action reg: 0.003971
  l1.weight: grad_norm = 0.195974
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.141575
Total gradient norm: 0.514853
=== Actor Training Debug (Iteration 4669) ===
Q mean: -12.075791
Q std: 16.923519
Actor loss: 12.079759
Action reg: 0.003967
  l1.weight: grad_norm = 0.074775
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.058883
Total gradient norm: 0.174336
=== Actor Training Debug (Iteration 4670) ===
Q mean: -11.251348
Q std: 17.596664
Actor loss: 11.255320
Action reg: 0.003971
  l1.weight: grad_norm = 0.069244
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.055644
Total gradient norm: 0.181927
=== Actor Training Debug (Iteration 4671) ===
Q mean: -13.548267
Q std: 19.507082
Actor loss: 13.552244
Action reg: 0.003977
  l1.weight: grad_norm = 0.078266
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.058211
Total gradient norm: 0.203796
=== Actor Training Debug (Iteration 4672) ===
Q mean: -16.022690
Q std: 19.210371
Actor loss: 16.026663
Action reg: 0.003974
  l1.weight: grad_norm = 0.048938
  l1.bias: grad_norm = 0.000651
  l2.weight: grad_norm = 0.039111
Total gradient norm: 0.118812
=== Actor Training Debug (Iteration 4673) ===
Q mean: -12.822060
Q std: 18.560200
Actor loss: 12.826030
Action reg: 0.003970
  l1.weight: grad_norm = 0.086433
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.067874
Total gradient norm: 0.163181
=== Actor Training Debug (Iteration 4674) ===
Q mean: -12.568029
Q std: 19.071753
Actor loss: 12.572002
Action reg: 0.003973
  l1.weight: grad_norm = 0.080815
  l1.bias: grad_norm = 0.000687
  l2.weight: grad_norm = 0.061170
Total gradient norm: 0.219119
=== Actor Training Debug (Iteration 4675) ===
Q mean: -12.887804
Q std: 18.697285
Actor loss: 12.891758
Action reg: 0.003954
  l1.weight: grad_norm = 0.202399
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.142547
Total gradient norm: 0.507541
=== Actor Training Debug (Iteration 4676) ===
Q mean: -12.001579
Q std: 18.039261
Actor loss: 12.005541
Action reg: 0.003961
  l1.weight: grad_norm = 0.133229
  l1.bias: grad_norm = 0.000855
  l2.weight: grad_norm = 0.090955
Total gradient norm: 0.385496
=== Actor Training Debug (Iteration 4677) ===
Q mean: -15.389773
Q std: 19.304684
Actor loss: 15.393757
Action reg: 0.003984
  l1.weight: grad_norm = 0.116525
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.087753
Total gradient norm: 0.248007
=== Actor Training Debug (Iteration 4678) ===
Q mean: -13.632277
Q std: 18.872442
Actor loss: 13.636257
Action reg: 0.003980
  l1.weight: grad_norm = 0.170767
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.110693
Total gradient norm: 0.363378
=== Actor Training Debug (Iteration 4679) ===
Q mean: -14.081966
Q std: 18.558432
Actor loss: 14.085936
Action reg: 0.003970
  l1.weight: grad_norm = 0.153671
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.120559
Total gradient norm: 0.406620
=== Actor Training Debug (Iteration 4680) ===
Q mean: -13.098783
Q std: 17.549339
Actor loss: 13.102757
Action reg: 0.003975
  l1.weight: grad_norm = 0.183360
  l1.bias: grad_norm = 0.000757
  l2.weight: grad_norm = 0.124226
Total gradient norm: 0.421398
=== Actor Training Debug (Iteration 4681) ===
Q mean: -13.108475
Q std: 17.682747
Actor loss: 13.112456
Action reg: 0.003981
  l1.weight: grad_norm = 0.060159
  l1.bias: grad_norm = 0.001181
  l2.weight: grad_norm = 0.041177
Total gradient norm: 0.137964
=== Actor Training Debug (Iteration 4682) ===
Q mean: -10.868625
Q std: 17.253508
Actor loss: 10.872607
Action reg: 0.003982
  l1.weight: grad_norm = 0.182244
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.153923
Total gradient norm: 0.504963
=== Actor Training Debug (Iteration 4683) ===
Q mean: -13.663836
Q std: 19.303341
Actor loss: 13.667804
Action reg: 0.003968
  l1.weight: grad_norm = 0.108757
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.095977
Total gradient norm: 0.244277
=== Actor Training Debug (Iteration 4684) ===
Q mean: -13.846997
Q std: 18.618261
Actor loss: 13.850960
Action reg: 0.003962
  l1.weight: grad_norm = 0.122907
  l1.bias: grad_norm = 0.002340
  l2.weight: grad_norm = 0.099322
Total gradient norm: 0.402603
=== Actor Training Debug (Iteration 4685) ===
Q mean: -12.109470
Q std: 17.658789
Actor loss: 12.113457
Action reg: 0.003986
  l1.weight: grad_norm = 0.137535
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.111369
Total gradient norm: 0.343747
=== Actor Training Debug (Iteration 4686) ===
Q mean: -12.370553
Q std: 18.012611
Actor loss: 12.374516
Action reg: 0.003963
  l1.weight: grad_norm = 0.087286
  l1.bias: grad_norm = 0.001312
  l2.weight: grad_norm = 0.073694
Total gradient norm: 0.280892
=== Actor Training Debug (Iteration 4687) ===
Q mean: -12.932346
Q std: 17.703465
Actor loss: 12.936325
Action reg: 0.003979
  l1.weight: grad_norm = 0.111912
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.088557
Total gradient norm: 0.265738
=== Actor Training Debug (Iteration 4688) ===
Q mean: -11.365063
Q std: 16.922731
Actor loss: 11.369039
Action reg: 0.003976
  l1.weight: grad_norm = 0.057577
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.046565
Total gradient norm: 0.167493
=== Actor Training Debug (Iteration 4689) ===
Q mean: -14.025928
Q std: 18.159822
Actor loss: 14.029884
Action reg: 0.003956
  l1.weight: grad_norm = 0.167965
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.111944
Total gradient norm: 0.392332
=== Actor Training Debug (Iteration 4690) ===
Q mean: -14.944042
Q std: 19.455767
Actor loss: 14.948015
Action reg: 0.003973
  l1.weight: grad_norm = 0.063933
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 0.053577
Total gradient norm: 0.165070
=== Actor Training Debug (Iteration 4691) ===
Q mean: -13.637808
Q std: 18.359623
Actor loss: 13.641765
Action reg: 0.003956
  l1.weight: grad_norm = 0.106990
  l1.bias: grad_norm = 0.000972
  l2.weight: grad_norm = 0.079998
Total gradient norm: 0.282214
=== Actor Training Debug (Iteration 4692) ===
Q mean: -14.729450
Q std: 19.894054
Actor loss: 14.733428
Action reg: 0.003978
  l1.weight: grad_norm = 0.139778
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.092852
Total gradient norm: 0.303281
=== Actor Training Debug (Iteration 4693) ===
Q mean: -11.885884
Q std: 16.418438
Actor loss: 11.889844
Action reg: 0.003960
  l1.weight: grad_norm = 0.161672
  l1.bias: grad_norm = 0.000494
  l2.weight: grad_norm = 0.132636
Total gradient norm: 0.570210
=== Actor Training Debug (Iteration 4694) ===
Q mean: -14.416726
Q std: 18.617579
Actor loss: 14.420706
Action reg: 0.003980
  l1.weight: grad_norm = 0.203249
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.146326
Total gradient norm: 0.432090
=== Actor Training Debug (Iteration 4695) ===
Q mean: -12.037567
Q std: 17.803347
Actor loss: 12.041535
Action reg: 0.003968
  l1.weight: grad_norm = 0.153923
  l1.bias: grad_norm = 0.000875
  l2.weight: grad_norm = 0.115855
Total gradient norm: 0.455715
=== Actor Training Debug (Iteration 4696) ===
Q mean: -12.018897
Q std: 17.022881
Actor loss: 12.022875
Action reg: 0.003978
  l1.weight: grad_norm = 0.045805
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.031726
Total gradient norm: 0.089507
=== Actor Training Debug (Iteration 4697) ===
Q mean: -12.725500
Q std: 19.366310
Actor loss: 12.729476
Action reg: 0.003976
  l1.weight: grad_norm = 0.141946
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.114391
Total gradient norm: 0.322939
=== Actor Training Debug (Iteration 4698) ===
Q mean: -13.573647
Q std: 19.039104
Actor loss: 13.577622
Action reg: 0.003974
  l1.weight: grad_norm = 0.177307
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.137466
Total gradient norm: 0.393078
=== Actor Training Debug (Iteration 4699) ===
Q mean: -15.030666
Q std: 18.468338
Actor loss: 15.034655
Action reg: 0.003989
  l1.weight: grad_norm = 0.062066
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.042119
Total gradient norm: 0.125627
=== Actor Training Debug (Iteration 4700) ===
Q mean: -13.027061
Q std: 16.952326
Actor loss: 13.031034
Action reg: 0.003973
  l1.weight: grad_norm = 0.068599
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.055726
Total gradient norm: 0.158526
=== Actor Training Debug (Iteration 4701) ===
Q mean: -13.777685
Q std: 19.092268
Actor loss: 13.781654
Action reg: 0.003970
  l1.weight: grad_norm = 0.067435
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.053273
Total gradient norm: 0.148039
=== Actor Training Debug (Iteration 4702) ===
Q mean: -14.005478
Q std: 18.533611
Actor loss: 14.009453
Action reg: 0.003975
  l1.weight: grad_norm = 0.047366
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.036891
Total gradient norm: 0.133687
=== Actor Training Debug (Iteration 4703) ===
Q mean: -11.545723
Q std: 17.118025
Actor loss: 11.549693
Action reg: 0.003970
  l1.weight: grad_norm = 0.141487
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.132089
Total gradient norm: 0.508122
=== Actor Training Debug (Iteration 4704) ===
Q mean: -12.046814
Q std: 18.585159
Actor loss: 12.050779
Action reg: 0.003965
  l1.weight: grad_norm = 0.120850
  l1.bias: grad_norm = 0.000519
  l2.weight: grad_norm = 0.078892
Total gradient norm: 0.235595
=== Actor Training Debug (Iteration 4705) ===
Q mean: -14.786362
Q std: 20.091341
Actor loss: 14.790339
Action reg: 0.003977
  l1.weight: grad_norm = 0.164504
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.118845
Total gradient norm: 0.349508
=== Actor Training Debug (Iteration 4706) ===
Q mean: -12.350595
Q std: 17.682434
Actor loss: 12.354567
Action reg: 0.003971
  l1.weight: grad_norm = 0.136176
  l1.bias: grad_norm = 0.000571
  l2.weight: grad_norm = 0.117452
Total gradient norm: 0.350605
=== Actor Training Debug (Iteration 4707) ===
Q mean: -11.097857
Q std: 17.191624
Actor loss: 11.101835
Action reg: 0.003978
  l1.weight: grad_norm = 0.072767
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.057978
Total gradient norm: 0.178598
=== Actor Training Debug (Iteration 4708) ===
Q mean: -12.209690
Q std: 17.427011
Actor loss: 12.213674
Action reg: 0.003984
  l1.weight: grad_norm = 0.131837
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.098024
Total gradient norm: 0.369067
=== Actor Training Debug (Iteration 4709) ===
Q mean: -15.588824
Q std: 20.151194
Actor loss: 15.592802
Action reg: 0.003977
  l1.weight: grad_norm = 0.087235
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.065319
Total gradient norm: 0.248914
=== Actor Training Debug (Iteration 4710) ===
Q mean: -15.489487
Q std: 19.240782
Actor loss: 15.493460
Action reg: 0.003973
  l1.weight: grad_norm = 0.232571
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.180599
Total gradient norm: 0.666080
=== Actor Training Debug (Iteration 4711) ===
Q mean: -13.768577
Q std: 18.646908
Actor loss: 13.772537
Action reg: 0.003961
  l1.weight: grad_norm = 0.217596
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.173951
Total gradient norm: 0.536826
=== Actor Training Debug (Iteration 4712) ===
Q mean: -14.122663
Q std: 19.061323
Actor loss: 14.126619
Action reg: 0.003956
  l1.weight: grad_norm = 0.145959
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.119588
Total gradient norm: 0.389174
=== Actor Training Debug (Iteration 4713) ===
Q mean: -12.739696
Q std: 18.596495
Actor loss: 12.743683
Action reg: 0.003988
  l1.weight: grad_norm = 0.058718
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.050811
Total gradient norm: 0.161719
=== Actor Training Debug (Iteration 4714) ===
Q mean: -12.621363
Q std: 17.040810
Actor loss: 12.625322
Action reg: 0.003959
  l1.weight: grad_norm = 0.095817
  l1.bias: grad_norm = 0.000720
  l2.weight: grad_norm = 0.070318
Total gradient norm: 0.217019
=== Actor Training Debug (Iteration 4715) ===
Q mean: -13.776884
Q std: 18.733786
Actor loss: 13.780862
Action reg: 0.003978
  l1.weight: grad_norm = 0.106839
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.090105
Total gradient norm: 0.278681
=== Actor Training Debug (Iteration 4716) ===
Q mean: -12.908415
Q std: 18.186619
Actor loss: 12.912373
Action reg: 0.003957
  l1.weight: grad_norm = 0.068914
  l1.bias: grad_norm = 0.000550
  l2.weight: grad_norm = 0.051142
Total gradient norm: 0.174138
=== Actor Training Debug (Iteration 4717) ===
Q mean: -12.562845
Q std: 18.497921
Actor loss: 12.566819
Action reg: 0.003974
  l1.weight: grad_norm = 0.081653
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.057677
Total gradient norm: 0.177048
=== Actor Training Debug (Iteration 4718) ===
Q mean: -11.871867
Q std: 17.324465
Actor loss: 11.875846
Action reg: 0.003979
  l1.weight: grad_norm = 0.049212
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.035429
Total gradient norm: 0.112618
=== Actor Training Debug (Iteration 4719) ===
Q mean: -14.365083
Q std: 18.639679
Actor loss: 14.369048
Action reg: 0.003966
  l1.weight: grad_norm = 0.126390
  l1.bias: grad_norm = 0.001715
  l2.weight: grad_norm = 0.095012
Total gradient norm: 0.294122
=== Actor Training Debug (Iteration 4720) ===
Q mean: -14.053637
Q std: 18.523247
Actor loss: 14.057617
Action reg: 0.003980
  l1.weight: grad_norm = 0.107792
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.096143
Total gradient norm: 0.304366
=== Actor Training Debug (Iteration 4721) ===
Q mean: -15.000477
Q std: 18.992533
Actor loss: 15.004455
Action reg: 0.003978
  l1.weight: grad_norm = 0.114916
  l1.bias: grad_norm = 0.000941
  l2.weight: grad_norm = 0.089697
Total gradient norm: 0.310708
=== Actor Training Debug (Iteration 4722) ===
Q mean: -12.750446
Q std: 18.598730
Actor loss: 12.754426
Action reg: 0.003979
  l1.weight: grad_norm = 0.047421
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.037845
Total gradient norm: 0.128098
=== Actor Training Debug (Iteration 4723) ===
Q mean: -14.350273
Q std: 19.283854
Actor loss: 14.354248
Action reg: 0.003975
  l1.weight: grad_norm = 0.148080
  l1.bias: grad_norm = 0.002288
  l2.weight: grad_norm = 0.119896
Total gradient norm: 0.454089
=== Actor Training Debug (Iteration 4724) ===
Q mean: -12.946634
Q std: 18.115812
Actor loss: 12.950611
Action reg: 0.003976
  l1.weight: grad_norm = 0.099877
  l1.bias: grad_norm = 0.000612
  l2.weight: grad_norm = 0.085530
Total gradient norm: 0.325187
=== Actor Training Debug (Iteration 4725) ===
Q mean: -13.372981
Q std: 18.715349
Actor loss: 13.376969
Action reg: 0.003988
  l1.weight: grad_norm = 0.085822
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.058674
Total gradient norm: 0.198289
=== Actor Training Debug (Iteration 4726) ===
Q mean: -11.290505
Q std: 16.611677
Actor loss: 11.294464
Action reg: 0.003959
  l1.weight: grad_norm = 0.138053
  l1.bias: grad_norm = 0.000724
  l2.weight: grad_norm = 0.085108
Total gradient norm: 0.268460
=== Actor Training Debug (Iteration 4727) ===
Q mean: -13.834717
Q std: 18.252434
Actor loss: 13.838696
Action reg: 0.003980
  l1.weight: grad_norm = 0.114022
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.102137
Total gradient norm: 0.336195
=== Actor Training Debug (Iteration 4728) ===
Q mean: -15.006334
Q std: 18.506210
Actor loss: 15.010316
Action reg: 0.003981
  l1.weight: grad_norm = 0.106805
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.083069
Total gradient norm: 0.207821
=== Actor Training Debug (Iteration 4729) ===
Q mean: -10.569529
Q std: 16.095802
Actor loss: 10.573503
Action reg: 0.003974
  l1.weight: grad_norm = 0.050786
  l1.bias: grad_norm = 0.001154
  l2.weight: grad_norm = 0.041118
Total gradient norm: 0.150202
=== Actor Training Debug (Iteration 4730) ===
Q mean: -12.569020
Q std: 17.899094
Actor loss: 12.572994
Action reg: 0.003974
  l1.weight: grad_norm = 0.084848
  l1.bias: grad_norm = 0.000643
  l2.weight: grad_norm = 0.067800
Total gradient norm: 0.224252
=== Actor Training Debug (Iteration 4731) ===
Q mean: -12.509382
Q std: 18.922134
Actor loss: 12.513352
Action reg: 0.003970
  l1.weight: grad_norm = 0.122097
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.074446
Total gradient norm: 0.259729
=== Actor Training Debug (Iteration 4732) ===
Q mean: -13.971197
Q std: 19.407833
Actor loss: 13.975179
Action reg: 0.003981
  l1.weight: grad_norm = 0.133951
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.110471
Total gradient norm: 0.402424
=== Actor Training Debug (Iteration 4733) ===
Q mean: -13.919497
Q std: 18.442966
Actor loss: 13.923470
Action reg: 0.003974
  l1.weight: grad_norm = 0.086245
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.065425
Total gradient norm: 0.218123
=== Actor Training Debug (Iteration 4734) ===
Q mean: -14.592310
Q std: 19.202290
Actor loss: 14.596290
Action reg: 0.003979
  l1.weight: grad_norm = 0.077556
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.062224
Total gradient norm: 0.184880
=== Actor Training Debug (Iteration 4735) ===
Q mean: -12.564671
Q std: 18.149963
Actor loss: 12.568643
Action reg: 0.003972
  l1.weight: grad_norm = 0.095851
  l1.bias: grad_norm = 0.000816
  l2.weight: grad_norm = 0.063514
Total gradient norm: 0.260448
=== Actor Training Debug (Iteration 4736) ===
Q mean: -13.279315
Q std: 17.715416
Actor loss: 13.283289
Action reg: 0.003974
  l1.weight: grad_norm = 0.041397
  l1.bias: grad_norm = 0.001250
  l2.weight: grad_norm = 0.031718
Total gradient norm: 0.107342
=== Actor Training Debug (Iteration 4737) ===
Q mean: -15.233686
Q std: 19.788273
Actor loss: 15.237653
Action reg: 0.003967
  l1.weight: grad_norm = 0.175343
  l1.bias: grad_norm = 0.000989
  l2.weight: grad_norm = 0.145288
Total gradient norm: 0.608131
=== Actor Training Debug (Iteration 4738) ===
Q mean: -14.482859
Q std: 18.184370
Actor loss: 14.486845
Action reg: 0.003987
  l1.weight: grad_norm = 0.046372
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.042645
Total gradient norm: 0.139224
=== Actor Training Debug (Iteration 4739) ===
Q mean: -11.215877
Q std: 17.066879
Actor loss: 11.219819
Action reg: 0.003942
  l1.weight: grad_norm = 0.141267
  l1.bias: grad_norm = 0.001097
  l2.weight: grad_norm = 0.100385
Total gradient norm: 0.367038
=== Actor Training Debug (Iteration 4740) ===
Q mean: -13.939017
Q std: 18.172600
Actor loss: 13.942999
Action reg: 0.003982
  l1.weight: grad_norm = 0.085720
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.057362
Total gradient norm: 0.168317
=== Actor Training Debug (Iteration 4741) ===
Q mean: -16.482452
Q std: 20.128456
Actor loss: 16.486431
Action reg: 0.003979
  l1.weight: grad_norm = 0.129534
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.103576
Total gradient norm: 0.355559
=== Actor Training Debug (Iteration 4742) ===
Q mean: -14.401981
Q std: 18.749910
Actor loss: 14.405948
Action reg: 0.003967
  l1.weight: grad_norm = 0.078841
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.063334
Total gradient norm: 0.205499
=== Actor Training Debug (Iteration 4743) ===
Q mean: -13.397362
Q std: 18.861475
Actor loss: 13.401342
Action reg: 0.003981
  l1.weight: grad_norm = 0.111599
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.095628
Total gradient norm: 0.297653
=== Actor Training Debug (Iteration 4744) ===
Q mean: -12.727175
Q std: 19.561157
Actor loss: 12.731142
Action reg: 0.003967
  l1.weight: grad_norm = 0.117956
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.088165
Total gradient norm: 0.285625
=== Actor Training Debug (Iteration 4745) ===
Q mean: -12.385472
Q std: 17.455517
Actor loss: 12.389443
Action reg: 0.003971
  l1.weight: grad_norm = 0.069684
  l1.bias: grad_norm = 0.000638
  l2.weight: grad_norm = 0.053881
Total gradient norm: 0.163464
=== Actor Training Debug (Iteration 4746) ===
Q mean: -12.335057
Q std: 17.858456
Actor loss: 12.339032
Action reg: 0.003975
  l1.weight: grad_norm = 0.101720
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.082514
Total gradient norm: 0.279422
=== Actor Training Debug (Iteration 4747) ===
Q mean: -15.400199
Q std: 19.499609
Actor loss: 15.404169
Action reg: 0.003970
  l1.weight: grad_norm = 0.152980
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.121950
Total gradient norm: 0.388970
=== Actor Training Debug (Iteration 4748) ===
Q mean: -14.076450
Q std: 19.561386
Actor loss: 14.080417
Action reg: 0.003967
  l1.weight: grad_norm = 0.079699
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.056143
Total gradient norm: 0.171031
=== Actor Training Debug (Iteration 4749) ===
Q mean: -13.051674
Q std: 17.765451
Actor loss: 13.055658
Action reg: 0.003985
  l1.weight: grad_norm = 0.180728
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.127113
Total gradient norm: 0.403454
=== Actor Training Debug (Iteration 4750) ===
Q mean: -13.794756
Q std: 18.187937
Actor loss: 13.798734
Action reg: 0.003978
  l1.weight: grad_norm = 0.147666
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.111730
Total gradient norm: 0.383552
=== Actor Training Debug (Iteration 4751) ===
Q mean: -12.376693
Q std: 18.176474
Actor loss: 12.380681
Action reg: 0.003988
  l1.weight: grad_norm = 0.056560
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.045336
Total gradient norm: 0.155912
=== Actor Training Debug (Iteration 4752) ===
Q mean: -13.378750
Q std: 17.505487
Actor loss: 13.382726
Action reg: 0.003976
  l1.weight: grad_norm = 0.095313
  l1.bias: grad_norm = 0.001908
  l2.weight: grad_norm = 0.077675
Total gradient norm: 0.272252
=== Actor Training Debug (Iteration 4753) ===
Q mean: -14.671173
Q std: 19.180225
Actor loss: 14.675152
Action reg: 0.003979
  l1.weight: grad_norm = 0.095990
  l1.bias: grad_norm = 0.001710
  l2.weight: grad_norm = 0.080190
Total gradient norm: 0.295508
=== Actor Training Debug (Iteration 4754) ===
Q mean: -12.179829
Q std: 17.860476
Actor loss: 12.183814
Action reg: 0.003985
  l1.weight: grad_norm = 0.081297
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.065506
Total gradient norm: 0.198499
=== Actor Training Debug (Iteration 4755) ===
Q mean: -13.179380
Q std: 18.605818
Actor loss: 13.183353
Action reg: 0.003973
  l1.weight: grad_norm = 0.082243
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.072163
Total gradient norm: 0.214716
=== Actor Training Debug (Iteration 4756) ===
Q mean: -12.375027
Q std: 19.301958
Actor loss: 12.378994
Action reg: 0.003967
  l1.weight: grad_norm = 0.093789
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.061872
Total gradient norm: 0.218412
=== Actor Training Debug (Iteration 4757) ===
Q mean: -14.063622
Q std: 18.724335
Actor loss: 14.067588
Action reg: 0.003966
  l1.weight: grad_norm = 0.129147
  l1.bias: grad_norm = 0.001910
  l2.weight: grad_norm = 0.112033
Total gradient norm: 0.341139
=== Actor Training Debug (Iteration 4758) ===
Q mean: -12.192012
Q std: 17.241285
Actor loss: 12.195972
Action reg: 0.003961
  l1.weight: grad_norm = 0.106592
  l1.bias: grad_norm = 0.000625
  l2.weight: grad_norm = 0.092255
Total gradient norm: 0.314915
=== Actor Training Debug (Iteration 4759) ===
Q mean: -13.966852
Q std: 19.133280
Actor loss: 13.970822
Action reg: 0.003970
  l1.weight: grad_norm = 0.102714
  l1.bias: grad_norm = 0.000717
  l2.weight: grad_norm = 0.077441
Total gradient norm: 0.290730
=== Actor Training Debug (Iteration 4760) ===
Q mean: -11.804385
Q std: 18.282557
Actor loss: 11.808338
Action reg: 0.003953
  l1.weight: grad_norm = 0.157475
  l1.bias: grad_norm = 0.000966
  l2.weight: grad_norm = 0.098959
Total gradient norm: 0.292365
=== Actor Training Debug (Iteration 4761) ===
Q mean: -12.462469
Q std: 18.761679
Actor loss: 12.466453
Action reg: 0.003984
  l1.weight: grad_norm = 0.109186
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.080973
Total gradient norm: 0.226423
=== Actor Training Debug (Iteration 4762) ===
Q mean: -15.440446
Q std: 20.596149
Actor loss: 15.444423
Action reg: 0.003977
  l1.weight: grad_norm = 0.106713
  l1.bias: grad_norm = 0.001094
  l2.weight: grad_norm = 0.096165
Total gradient norm: 0.375219
=== Actor Training Debug (Iteration 4763) ===
Q mean: -12.256539
Q std: 18.310537
Actor loss: 12.260493
Action reg: 0.003954
  l1.weight: grad_norm = 0.108962
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.079035
Total gradient norm: 0.268554
=== Actor Training Debug (Iteration 4764) ===
Q mean: -11.494316
Q std: 17.168118
Actor loss: 11.498296
Action reg: 0.003979
  l1.weight: grad_norm = 0.111891
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.095938
Total gradient norm: 0.348332
=== Actor Training Debug (Iteration 4765) ===
Q mean: -13.584022
Q std: 18.021301
Actor loss: 13.587999
Action reg: 0.003978
  l1.weight: grad_norm = 0.173450
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.118465
Total gradient norm: 0.369435
=== Actor Training Debug (Iteration 4766) ===
Q mean: -14.643070
Q std: 18.743376
Actor loss: 14.647050
Action reg: 0.003979
  l1.weight: grad_norm = 0.175726
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.131463
Total gradient norm: 0.473593
=== Actor Training Debug (Iteration 4767) ===
Q mean: -14.835892
Q std: 18.804050
Actor loss: 14.839877
Action reg: 0.003985
  l1.weight: grad_norm = 0.030732
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.023048
Total gradient norm: 0.069033
=== Actor Training Debug (Iteration 4768) ===
Q mean: -14.662479
Q std: 20.583982
Actor loss: 14.666459
Action reg: 0.003980
  l1.weight: grad_norm = 0.116621
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.090154
Total gradient norm: 0.264896
=== Actor Training Debug (Iteration 4769) ===
Q mean: -12.144337
Q std: 17.135395
Actor loss: 12.148293
Action reg: 0.003957
  l1.weight: grad_norm = 0.108366
  l1.bias: grad_norm = 0.000670
  l2.weight: grad_norm = 0.079901
Total gradient norm: 0.277253
=== Actor Training Debug (Iteration 4770) ===
Q mean: -12.766855
Q std: 17.855915
Actor loss: 12.770823
Action reg: 0.003969
  l1.weight: grad_norm = 0.395396
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.286250
Total gradient norm: 1.008856
=== Actor Training Debug (Iteration 4771) ===
Q mean: -10.724407
Q std: 17.043526
Actor loss: 10.728381
Action reg: 0.003974
  l1.weight: grad_norm = 0.115810
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.074530
Total gradient norm: 0.212525
=== Actor Training Debug (Iteration 4772) ===
Q mean: -14.586061
Q std: 18.647461
Actor loss: 14.590025
Action reg: 0.003964
  l1.weight: grad_norm = 0.138203
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.105435
Total gradient norm: 0.365265
=== Actor Training Debug (Iteration 4773) ===
Q mean: -13.827514
Q std: 19.174295
Actor loss: 13.831491
Action reg: 0.003978
  l1.weight: grad_norm = 0.077946
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.063697
Total gradient norm: 0.231399
=== Actor Training Debug (Iteration 4774) ===
Q mean: -12.019995
Q std: 17.190781
Actor loss: 12.023971
Action reg: 0.003975
  l1.weight: grad_norm = 0.166612
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.140299
Total gradient norm: 0.457650
=== Actor Training Debug (Iteration 4775) ===
Q mean: -14.025264
Q std: 19.466055
Actor loss: 14.029231
Action reg: 0.003968
  l1.weight: grad_norm = 0.092680
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.071408
Total gradient norm: 0.238108
=== Actor Training Debug (Iteration 4776) ===
Q mean: -12.053318
Q std: 17.940929
Actor loss: 12.057300
Action reg: 0.003981
  l1.weight: grad_norm = 0.250000
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.198644
Total gradient norm: 0.815779
=== Actor Training Debug (Iteration 4777) ===
Q mean: -13.727328
Q std: 18.673130
Actor loss: 13.731298
Action reg: 0.003970
  l1.weight: grad_norm = 0.094606
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.067741
Total gradient norm: 0.202878
=== Actor Training Debug (Iteration 4778) ===
Q mean: -13.591804
Q std: 19.420145
Actor loss: 13.595769
Action reg: 0.003965
  l1.weight: grad_norm = 0.184020
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.129999
Total gradient norm: 0.532804
=== Actor Training Debug (Iteration 4779) ===
Q mean: -13.367544
Q std: 18.011944
Actor loss: 13.371513
Action reg: 0.003969
  l1.weight: grad_norm = 0.135973
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.102141
Total gradient norm: 0.343073
=== Actor Training Debug (Iteration 4780) ===
Q mean: -14.074889
Q std: 19.694536
Actor loss: 14.078868
Action reg: 0.003979
  l1.weight: grad_norm = 0.177482
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.147071
Total gradient norm: 0.446913
=== Actor Training Debug (Iteration 4781) ===
Q mean: -12.833333
Q std: 18.150339
Actor loss: 12.837321
Action reg: 0.003989
  l1.weight: grad_norm = 0.104271
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.088562
Total gradient norm: 0.238647
=== Actor Training Debug (Iteration 4782) ===
Q mean: -15.549136
Q std: 19.653383
Actor loss: 15.553120
Action reg: 0.003983
  l1.weight: grad_norm = 0.124634
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.115281
Total gradient norm: 0.374538
=== Actor Training Debug (Iteration 4783) ===
Q mean: -11.932235
Q std: 17.748829
Actor loss: 11.936215
Action reg: 0.003980
  l1.weight: grad_norm = 0.109172
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.086835
Total gradient norm: 0.269450
=== Actor Training Debug (Iteration 4784) ===
Q mean: -12.052141
Q std: 17.695086
Actor loss: 12.056114
Action reg: 0.003973
  l1.weight: grad_norm = 0.133882
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.104212
Total gradient norm: 0.309355
=== Actor Training Debug (Iteration 4785) ===
Q mean: -16.717787
Q std: 19.906033
Actor loss: 16.721760
Action reg: 0.003974
  l1.weight: grad_norm = 0.118842
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.097508
Total gradient norm: 0.322293
=== Actor Training Debug (Iteration 4786) ===
Q mean: -13.104729
Q std: 18.524923
Actor loss: 13.108698
Action reg: 0.003969
  l1.weight: grad_norm = 0.151789
  l1.bias: grad_norm = 0.000821
  l2.weight: grad_norm = 0.133956
Total gradient norm: 0.535616
=== Actor Training Debug (Iteration 4787) ===
Q mean: -12.877907
Q std: 18.478565
Actor loss: 12.881874
Action reg: 0.003968
  l1.weight: grad_norm = 0.149317
  l1.bias: grad_norm = 0.000803
  l2.weight: grad_norm = 0.113521
Total gradient norm: 0.390071
=== Actor Training Debug (Iteration 4788) ===
Q mean: -14.160964
Q std: 20.215038
Actor loss: 14.164934
Action reg: 0.003971
  l1.weight: grad_norm = 0.071689
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.056202
Total gradient norm: 0.179263
=== Actor Training Debug (Iteration 4789) ===
Q mean: -15.902384
Q std: 20.029440
Actor loss: 15.906370
Action reg: 0.003986
  l1.weight: grad_norm = 0.037815
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.021514
Total gradient norm: 0.071610
=== Actor Training Debug (Iteration 4790) ===
Q mean: -12.042017
Q std: 18.087463
Actor loss: 12.045991
Action reg: 0.003974
  l1.weight: grad_norm = 0.087513
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.066822
Total gradient norm: 0.217590
=== Actor Training Debug (Iteration 4791) ===
Q mean: -13.701321
Q std: 19.702738
Actor loss: 13.705299
Action reg: 0.003979
  l1.weight: grad_norm = 0.059929
  l1.bias: grad_norm = 0.001431
  l2.weight: grad_norm = 0.050452
Total gradient norm: 0.193661
=== Actor Training Debug (Iteration 4792) ===
Q mean: -13.991114
Q std: 19.137001
Actor loss: 13.995090
Action reg: 0.003977
  l1.weight: grad_norm = 0.202746
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.188609
Total gradient norm: 0.552902
=== Actor Training Debug (Iteration 4793) ===
Q mean: -14.740955
Q std: 19.407742
Actor loss: 14.744931
Action reg: 0.003976
  l1.weight: grad_norm = 0.182184
  l1.bias: grad_norm = 0.000425
  l2.weight: grad_norm = 0.143494
Total gradient norm: 0.409315
=== Actor Training Debug (Iteration 4794) ===
Q mean: -14.297586
Q std: 18.708035
Actor loss: 14.301566
Action reg: 0.003979
  l1.weight: grad_norm = 0.130563
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.114050
Total gradient norm: 0.542838
=== Actor Training Debug (Iteration 4795) ===
Q mean: -13.168297
Q std: 17.783381
Actor loss: 13.172277
Action reg: 0.003981
  l1.weight: grad_norm = 0.139157
  l1.bias: grad_norm = 0.001556
  l2.weight: grad_norm = 0.115031
Total gradient norm: 0.362900
=== Actor Training Debug (Iteration 4796) ===
Q mean: -14.114255
Q std: 19.551554
Actor loss: 14.118237
Action reg: 0.003983
  l1.weight: grad_norm = 0.088644
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.073996
Total gradient norm: 0.237591
=== Actor Training Debug (Iteration 4797) ===
Q mean: -15.618313
Q std: 19.561049
Actor loss: 15.622291
Action reg: 0.003978
  l1.weight: grad_norm = 0.206368
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.142935
Total gradient norm: 0.426969
=== Actor Training Debug (Iteration 4798) ===
Q mean: -13.468765
Q std: 18.864592
Actor loss: 13.472726
Action reg: 0.003961
  l1.weight: grad_norm = 0.117193
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.074862
Total gradient norm: 0.223998
=== Actor Training Debug (Iteration 4799) ===
Q mean: -14.787368
Q std: 20.019945
Actor loss: 14.791343
Action reg: 0.003975
  l1.weight: grad_norm = 0.418869
  l1.bias: grad_norm = 0.001418
  l2.weight: grad_norm = 0.343026
Total gradient norm: 1.062955
=== Actor Training Debug (Iteration 4800) ===
Q mean: -12.327268
Q std: 18.556740
Actor loss: 12.331239
Action reg: 0.003971
  l1.weight: grad_norm = 0.090967
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.066272
Total gradient norm: 0.203003
=== Actor Training Debug (Iteration 4801) ===
Q mean: -14.626798
Q std: 19.586473
Actor loss: 14.630785
Action reg: 0.003987
  l1.weight: grad_norm = 0.060258
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.044743
Total gradient norm: 0.173908
=== Actor Training Debug (Iteration 4802) ===
Q mean: -13.053438
Q std: 19.107796
Actor loss: 13.057418
Action reg: 0.003980
  l1.weight: grad_norm = 0.067277
  l1.bias: grad_norm = 0.000461
  l2.weight: grad_norm = 0.053627
Total gradient norm: 0.189325
=== Actor Training Debug (Iteration 4803) ===
Q mean: -12.829620
Q std: 17.567873
Actor loss: 12.833604
Action reg: 0.003983
  l1.weight: grad_norm = 0.106327
  l1.bias: grad_norm = 0.000738
  l2.weight: grad_norm = 0.087089
Total gradient norm: 0.307923
=== Actor Training Debug (Iteration 4804) ===
Q mean: -12.052481
Q std: 17.037716
Actor loss: 12.056441
Action reg: 0.003961
  l1.weight: grad_norm = 0.155045
  l1.bias: grad_norm = 0.001219
  l2.weight: grad_norm = 0.122874
Total gradient norm: 0.438730
=== Actor Training Debug (Iteration 4805) ===
Q mean: -14.380621
Q std: 18.761208
Actor loss: 14.384592
Action reg: 0.003971
  l1.weight: grad_norm = 0.144609
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.107629
Total gradient norm: 0.431296
=== Actor Training Debug (Iteration 4806) ===
Q mean: -11.522190
Q std: 17.596710
Actor loss: 11.526177
Action reg: 0.003987
  l1.weight: grad_norm = 0.042908
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.033200
Total gradient norm: 0.100104
=== Actor Training Debug (Iteration 4807) ===
Q mean: -12.776083
Q std: 18.718056
Actor loss: 12.780055
Action reg: 0.003972
  l1.weight: grad_norm = 0.142414
  l1.bias: grad_norm = 0.000683
  l2.weight: grad_norm = 0.118604
Total gradient norm: 0.354677
=== Actor Training Debug (Iteration 4808) ===
Q mean: -12.668938
Q std: 18.140787
Actor loss: 12.672915
Action reg: 0.003977
  l1.weight: grad_norm = 0.094879
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.077687
Total gradient norm: 0.287189
=== Actor Training Debug (Iteration 4809) ===
Q mean: -13.170831
Q std: 18.555325
Actor loss: 13.174809
Action reg: 0.003979
  l1.weight: grad_norm = 0.305770
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.226738
Total gradient norm: 1.021436
=== Actor Training Debug (Iteration 4810) ===
Q mean: -12.404819
Q std: 17.680441
Actor loss: 12.408801
Action reg: 0.003982
  l1.weight: grad_norm = 0.153888
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.106109
Total gradient norm: 0.347475
=== Actor Training Debug (Iteration 4811) ===
Q mean: -11.273727
Q std: 18.569921
Actor loss: 11.277698
Action reg: 0.003971
  l1.weight: grad_norm = 0.070162
  l1.bias: grad_norm = 0.000953
  l2.weight: grad_norm = 0.064496
Total gradient norm: 0.208459
=== Actor Training Debug (Iteration 4812) ===
Q mean: -10.770063
Q std: 16.303041
Actor loss: 10.774030
Action reg: 0.003966
  l1.weight: grad_norm = 0.113279
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.089734
Total gradient norm: 0.300030
=== Actor Training Debug (Iteration 4813) ===
Q mean: -14.686182
Q std: 19.264433
Actor loss: 14.690165
Action reg: 0.003983
  l1.weight: grad_norm = 0.078218
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.065004
Total gradient norm: 0.211910
=== Actor Training Debug (Iteration 4814) ===
Q mean: -13.831525
Q std: 18.489902
Actor loss: 13.835493
Action reg: 0.003968
  l1.weight: grad_norm = 0.195834
  l1.bias: grad_norm = 0.003746
  l2.weight: grad_norm = 0.126635
Total gradient norm: 0.426790
=== Actor Training Debug (Iteration 4815) ===
Q mean: -12.969864
Q std: 17.871222
Actor loss: 12.973825
Action reg: 0.003962
  l1.weight: grad_norm = 0.082272
  l1.bias: grad_norm = 0.000601
  l2.weight: grad_norm = 0.074395
Total gradient norm: 0.224964
=== Actor Training Debug (Iteration 4816) ===
Q mean: -11.962578
Q std: 17.076395
Actor loss: 11.966545
Action reg: 0.003967
  l1.weight: grad_norm = 0.133054
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.101027
Total gradient norm: 0.365951
=== Actor Training Debug (Iteration 4817) ===
Q mean: -12.563303
Q std: 17.828209
Actor loss: 12.567280
Action reg: 0.003976
  l1.weight: grad_norm = 0.152473
  l1.bias: grad_norm = 0.001612
  l2.weight: grad_norm = 0.105501
Total gradient norm: 0.367942
=== Actor Training Debug (Iteration 4818) ===
Q mean: -13.355852
Q std: 19.369738
Actor loss: 13.359827
Action reg: 0.003975
  l1.weight: grad_norm = 0.097054
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.070287
Total gradient norm: 0.210337
=== Actor Training Debug (Iteration 4819) ===
Q mean: -12.255665
Q std: 18.115591
Actor loss: 12.259640
Action reg: 0.003975
  l1.weight: grad_norm = 0.128185
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.090103
Total gradient norm: 0.303067
=== Actor Training Debug (Iteration 4820) ===
Q mean: -14.130941
Q std: 20.174599
Actor loss: 14.134918
Action reg: 0.003977
  l1.weight: grad_norm = 0.080129
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.054632
Total gradient norm: 0.198021
=== Actor Training Debug (Iteration 4821) ===
Q mean: -10.538557
Q std: 16.930681
Actor loss: 10.542528
Action reg: 0.003971
  l1.weight: grad_norm = 0.120782
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.094475
Total gradient norm: 0.437688
=== Actor Training Debug (Iteration 4822) ===
Q mean: -14.124182
Q std: 19.072050
Actor loss: 14.128145
Action reg: 0.003964
  l1.weight: grad_norm = 0.166518
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.136982
Total gradient norm: 0.500974
=== Actor Training Debug (Iteration 4823) ===
Q mean: -13.198429
Q std: 18.530510
Actor loss: 13.202402
Action reg: 0.003973
  l1.weight: grad_norm = 0.094546
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.073429
Total gradient norm: 0.263048
=== Actor Training Debug (Iteration 4824) ===
Q mean: -12.713932
Q std: 19.828081
Actor loss: 12.717902
Action reg: 0.003970
  l1.weight: grad_norm = 0.193553
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.157010
Total gradient norm: 0.415211
=== Actor Training Debug (Iteration 4825) ===
Q mean: -13.414080
Q std: 19.062342
Actor loss: 13.418065
Action reg: 0.003985
  l1.weight: grad_norm = 0.106822
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.075473
Total gradient norm: 0.273137
=== Actor Training Debug (Iteration 4826) ===
Q mean: -12.613188
Q std: 17.510910
Actor loss: 12.617166
Action reg: 0.003978
  l1.weight: grad_norm = 0.183300
  l1.bias: grad_norm = 0.001404
  l2.weight: grad_norm = 0.160520
Total gradient norm: 0.489910
=== Actor Training Debug (Iteration 4827) ===
Q mean: -13.564505
Q std: 19.026861
Actor loss: 13.568460
Action reg: 0.003956
  l1.weight: grad_norm = 0.161951
  l1.bias: grad_norm = 0.000612
  l2.weight: grad_norm = 0.151041
Total gradient norm: 0.621396
=== Actor Training Debug (Iteration 4828) ===
Q mean: -13.463024
Q std: 19.054916
Actor loss: 13.466988
Action reg: 0.003964
  l1.weight: grad_norm = 0.124329
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.118188
Total gradient norm: 0.621396
=== Actor Training Debug (Iteration 4829) ===
Q mean: -13.018722
Q std: 18.598593
Actor loss: 13.022706
Action reg: 0.003984
  l1.weight: grad_norm = 0.111592
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.094083
Total gradient norm: 0.294826
=== Actor Training Debug (Iteration 4830) ===
Q mean: -12.788359
Q std: 19.087198
Actor loss: 12.792322
Action reg: 0.003963
  l1.weight: grad_norm = 0.082856
  l1.bias: grad_norm = 0.000705
  l2.weight: grad_norm = 0.062345
Total gradient norm: 0.201093
=== Actor Training Debug (Iteration 4831) ===
Q mean: -10.692797
Q std: 17.344576
Actor loss: 10.696779
Action reg: 0.003982
  l1.weight: grad_norm = 0.067022
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.053380
Total gradient norm: 0.181175
=== Actor Training Debug (Iteration 4832) ===
Q mean: -12.323135
Q std: 16.830975
Actor loss: 12.327122
Action reg: 0.003987
  l1.weight: grad_norm = 0.083783
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.072986
Total gradient norm: 0.254057
=== Actor Training Debug (Iteration 4833) ===
Q mean: -13.287424
Q std: 18.200104
Actor loss: 13.291399
Action reg: 0.003975
  l1.weight: grad_norm = 0.283277
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.193543
Total gradient norm: 0.677940
=== Actor Training Debug (Iteration 4834) ===
Q mean: -13.151807
Q std: 18.860964
Actor loss: 13.155787
Action reg: 0.003980
  l1.weight: grad_norm = 0.024739
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.021530
Total gradient norm: 0.075252
=== Actor Training Debug (Iteration 4835) ===
Q mean: -12.507848
Q std: 18.526489
Actor loss: 12.511830
Action reg: 0.003982
  l1.weight: grad_norm = 0.135780
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.112337
Total gradient norm: 0.339109
=== Actor Training Debug (Iteration 4836) ===
Q mean: -14.635256
Q std: 19.291088
Actor loss: 14.639227
Action reg: 0.003971
  l1.weight: grad_norm = 0.071276
  l1.bias: grad_norm = 0.000615
  l2.weight: grad_norm = 0.055756
Total gradient norm: 0.153897
=== Actor Training Debug (Iteration 4837) ===
Q mean: -12.911472
Q std: 19.332075
Actor loss: 12.915452
Action reg: 0.003979
  l1.weight: grad_norm = 0.176324
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.127270
Total gradient norm: 0.414839
=== Actor Training Debug (Iteration 4838) ===
Q mean: -11.790374
Q std: 18.061249
Actor loss: 11.794341
Action reg: 0.003967
  l1.weight: grad_norm = 0.143999
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.121394
Total gradient norm: 0.419893
=== Actor Training Debug (Iteration 4839) ===
Q mean: -13.321382
Q std: 18.941233
Actor loss: 13.325355
Action reg: 0.003973
  l1.weight: grad_norm = 0.090123
  l1.bias: grad_norm = 0.000559
  l2.weight: grad_norm = 0.069243
Total gradient norm: 0.211004
=== Actor Training Debug (Iteration 4840) ===
Q mean: -11.718357
Q std: 17.861639
Actor loss: 11.722327
Action reg: 0.003970
  l1.weight: grad_norm = 0.209935
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.174927
Total gradient norm: 0.616541
=== Actor Training Debug (Iteration 4841) ===
Q mean: -12.373289
Q std: 18.117001
Actor loss: 12.377260
Action reg: 0.003971
  l1.weight: grad_norm = 0.137641
  l1.bias: grad_norm = 0.001434
  l2.weight: grad_norm = 0.121656
Total gradient norm: 0.423303
=== Actor Training Debug (Iteration 4842) ===
Q mean: -14.656969
Q std: 19.118496
Actor loss: 14.660949
Action reg: 0.003979
  l1.weight: grad_norm = 0.136636
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.102433
Total gradient norm: 0.367234
=== Actor Training Debug (Iteration 4843) ===
Q mean: -12.685320
Q std: 19.712582
Actor loss: 12.689307
Action reg: 0.003988
  l1.weight: grad_norm = 0.635822
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.472059
Total gradient norm: 2.037283
=== Actor Training Debug (Iteration 4844) ===
Q mean: -11.006282
Q std: 18.233458
Actor loss: 11.010258
Action reg: 0.003976
  l1.weight: grad_norm = 0.138159
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.106340
Total gradient norm: 0.356102
=== Actor Training Debug (Iteration 4845) ===
Q mean: -13.519655
Q std: 18.364891
Actor loss: 13.523637
Action reg: 0.003982
  l1.weight: grad_norm = 0.091946
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.077306
Total gradient norm: 0.236677
=== Actor Training Debug (Iteration 4846) ===
Q mean: -13.085436
Q std: 18.072996
Actor loss: 13.089409
Action reg: 0.003973
  l1.weight: grad_norm = 0.163163
  l1.bias: grad_norm = 0.000584
  l2.weight: grad_norm = 0.149168
Total gradient norm: 0.398652
=== Actor Training Debug (Iteration 4847) ===
Q mean: -12.865408
Q std: 18.602026
Actor loss: 12.869384
Action reg: 0.003976
  l1.weight: grad_norm = 0.093355
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.084888
Total gradient norm: 0.312205
=== Actor Training Debug (Iteration 4848) ===
Q mean: -16.071468
Q std: 19.503674
Actor loss: 16.075445
Action reg: 0.003977
  l1.weight: grad_norm = 0.110383
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.091078
Total gradient norm: 0.346518
=== Actor Training Debug (Iteration 4849) ===
Q mean: -13.772936
Q std: 19.460541
Actor loss: 13.776910
Action reg: 0.003974
  l1.weight: grad_norm = 0.136996
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.098361
Total gradient norm: 0.341513
=== Actor Training Debug (Iteration 4850) ===
Q mean: -12.192324
Q std: 18.500242
Actor loss: 12.196305
Action reg: 0.003982
  l1.weight: grad_norm = 0.102014
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.096246
Total gradient norm: 0.325080
=== Actor Training Debug (Iteration 4851) ===
Q mean: -15.777482
Q std: 20.484888
Actor loss: 15.781467
Action reg: 0.003985
  l1.weight: grad_norm = 0.277712
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.231368
Total gradient norm: 0.822480
=== Actor Training Debug (Iteration 4852) ===
Q mean: -13.484745
Q std: 18.663845
Actor loss: 13.488724
Action reg: 0.003978
  l1.weight: grad_norm = 0.085076
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.067316
Total gradient norm: 0.212730
=== Actor Training Debug (Iteration 4853) ===
Q mean: -13.865349
Q std: 19.674662
Actor loss: 13.869313
Action reg: 0.003964
  l1.weight: grad_norm = 0.126128
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.102874
Total gradient norm: 0.294049
=== Actor Training Debug (Iteration 4854) ===
Q mean: -14.400986
Q std: 19.677038
Actor loss: 14.404963
Action reg: 0.003978
  l1.weight: grad_norm = 0.147522
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.102350
Total gradient norm: 0.324563
=== Actor Training Debug (Iteration 4855) ===
Q mean: -12.869267
Q std: 17.795546
Actor loss: 12.873246
Action reg: 0.003980
  l1.weight: grad_norm = 0.057863
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.045630
Total gradient norm: 0.143536
=== Actor Training Debug (Iteration 4856) ===
Q mean: -14.828388
Q std: 19.665644
Actor loss: 14.832362
Action reg: 0.003974
  l1.weight: grad_norm = 0.125220
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.104384
Total gradient norm: 0.330585
=== Actor Training Debug (Iteration 4857) ===
Q mean: -11.872604
Q std: 17.742168
Actor loss: 11.876575
Action reg: 0.003971
  l1.weight: grad_norm = 0.072187
  l1.bias: grad_norm = 0.001715
  l2.weight: grad_norm = 0.061137
Total gradient norm: 0.194131
=== Actor Training Debug (Iteration 4858) ===
Q mean: -12.635300
Q std: 18.503864
Actor loss: 12.639274
Action reg: 0.003974
  l1.weight: grad_norm = 0.108382
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.091204
Total gradient norm: 0.279682
=== Actor Training Debug (Iteration 4859) ===
Q mean: -12.494731
Q std: 18.912357
Actor loss: 12.498702
Action reg: 0.003971
  l1.weight: grad_norm = 0.124049
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.090795
Total gradient norm: 0.270805
=== Actor Training Debug (Iteration 4860) ===
Q mean: -13.849948
Q std: 18.323059
Actor loss: 13.853931
Action reg: 0.003984
  l1.weight: grad_norm = 0.133935
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.086088
Total gradient norm: 0.273318
=== Actor Training Debug (Iteration 4861) ===
Q mean: -12.668480
Q std: 18.070578
Actor loss: 12.672459
Action reg: 0.003978
  l1.weight: grad_norm = 0.215563
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.196299
Total gradient norm: 0.618050
=== Actor Training Debug (Iteration 4862) ===
Q mean: -13.151500
Q std: 18.504454
Actor loss: 13.155482
Action reg: 0.003982
  l1.weight: grad_norm = 0.164008
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.137003
Total gradient norm: 0.493353
=== Actor Training Debug (Iteration 4863) ===
Q mean: -13.634619
Q std: 18.205070
Actor loss: 13.638588
Action reg: 0.003969
  l1.weight: grad_norm = 0.166899
  l1.bias: grad_norm = 0.001791
  l2.weight: grad_norm = 0.139134
Total gradient norm: 0.413455
=== Actor Training Debug (Iteration 4864) ===
Q mean: -15.021774
Q std: 20.581980
Actor loss: 15.025741
Action reg: 0.003967
  l1.weight: grad_norm = 0.226890
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.200639
Total gradient norm: 0.635595
=== Actor Training Debug (Iteration 4865) ===
Q mean: -13.048287
Q std: 19.291521
Actor loss: 13.052271
Action reg: 0.003983
  l1.weight: grad_norm = 0.195529
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.128065
Total gradient norm: 0.415831
=== Actor Training Debug (Iteration 4866) ===
Q mean: -12.567326
Q std: 19.175510
Actor loss: 12.571313
Action reg: 0.003988
  l1.weight: grad_norm = 0.108585
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.098977
Total gradient norm: 0.331279
=== Actor Training Debug (Iteration 4867) ===
Q mean: -11.136387
Q std: 17.163725
Actor loss: 11.140359
Action reg: 0.003972
  l1.weight: grad_norm = 0.110132
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.089957
Total gradient norm: 0.339968
=== Actor Training Debug (Iteration 4868) ===
Q mean: -11.081105
Q std: 16.485382
Actor loss: 11.085096
Action reg: 0.003991
  l1.weight: grad_norm = 0.018474
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.013044
Total gradient norm: 0.044762
=== Actor Training Debug (Iteration 4869) ===
Q mean: -11.157715
Q std: 16.861662
Actor loss: 11.161693
Action reg: 0.003977
  l1.weight: grad_norm = 0.066670
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.053464
Total gradient norm: 0.180737
=== Actor Training Debug (Iteration 4870) ===
Q mean: -12.974589
Q std: 18.802956
Actor loss: 12.978576
Action reg: 0.003986
  l1.weight: grad_norm = 0.181549
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.147578
Total gradient norm: 0.483117
=== Actor Training Debug (Iteration 4871) ===
Q mean: -11.812191
Q std: 17.993601
Actor loss: 11.816159
Action reg: 0.003968
  l1.weight: grad_norm = 0.102488
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.076084
Total gradient norm: 0.277130
=== Actor Training Debug (Iteration 4872) ===
Q mean: -14.504098
Q std: 19.725084
Actor loss: 14.508079
Action reg: 0.003980
  l1.weight: grad_norm = 0.067974
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.060027
Total gradient norm: 0.196652
=== Actor Training Debug (Iteration 4873) ===
Q mean: -13.042708
Q std: 18.520395
Actor loss: 13.046679
Action reg: 0.003970
  l1.weight: grad_norm = 0.132597
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.106017
Total gradient norm: 0.368577
=== Actor Training Debug (Iteration 4874) ===
Q mean: -14.762741
Q std: 19.660233
Actor loss: 14.766732
Action reg: 0.003991
  l1.weight: grad_norm = 0.042317
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.035552
Total gradient norm: 0.122950
=== Actor Training Debug (Iteration 4875) ===
Q mean: -14.055373
Q std: 18.163605
Actor loss: 14.059343
Action reg: 0.003970
  l1.weight: grad_norm = 0.046554
  l1.bias: grad_norm = 0.001512
  l2.weight: grad_norm = 0.034468
Total gradient norm: 0.120904
=== Actor Training Debug (Iteration 4876) ===
Q mean: -14.624529
Q std: 19.874201
Actor loss: 14.628510
Action reg: 0.003982
  l1.weight: grad_norm = 0.177579
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.102081
Total gradient norm: 0.320513
=== Actor Training Debug (Iteration 4877) ===
Q mean: -14.799099
Q std: 19.892508
Actor loss: 14.803072
Action reg: 0.003973
  l1.weight: grad_norm = 0.065994
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.048532
Total gradient norm: 0.184147
=== Actor Training Debug (Iteration 4878) ===
Q mean: -14.581789
Q std: 19.430111
Actor loss: 14.585761
Action reg: 0.003972
  l1.weight: grad_norm = 0.109252
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.099165
Total gradient norm: 0.353895
=== Actor Training Debug (Iteration 4879) ===
Q mean: -12.546631
Q std: 19.146849
Actor loss: 12.550591
Action reg: 0.003961
  l1.weight: grad_norm = 0.094437
  l1.bias: grad_norm = 0.000628
  l2.weight: grad_norm = 0.066838
Total gradient norm: 0.224448
=== Actor Training Debug (Iteration 4880) ===
Q mean: -11.478608
Q std: 17.153557
Actor loss: 11.482580
Action reg: 0.003972
  l1.weight: grad_norm = 0.272833
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.215883
Total gradient norm: 0.710899
=== Actor Training Debug (Iteration 4881) ===
Q mean: -13.333962
Q std: 18.687454
Actor loss: 13.337947
Action reg: 0.003985
  l1.weight: grad_norm = 0.105196
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.080691
Total gradient norm: 0.315383
=== Actor Training Debug (Iteration 4882) ===
Q mean: -13.818428
Q std: 18.649168
Actor loss: 13.822416
Action reg: 0.003989
  l1.weight: grad_norm = 0.118519
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.086247
Total gradient norm: 0.299670
=== Actor Training Debug (Iteration 4883) ===
Q mean: -14.884506
Q std: 19.775146
Actor loss: 14.888500
Action reg: 0.003994
  l1.weight: grad_norm = 0.029779
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.022797
Total gradient norm: 0.079658
=== Actor Training Debug (Iteration 4884) ===
Q mean: -13.404670
Q std: 19.171331
Actor loss: 13.408635
Action reg: 0.003965
  l1.weight: grad_norm = 0.077577
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.061897
Total gradient norm: 0.187688
=== Actor Training Debug (Iteration 4885) ===
Q mean: -13.560253
Q std: 19.140305
Actor loss: 13.564236
Action reg: 0.003983
  l1.weight: grad_norm = 0.101248
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.074131
Total gradient norm: 0.205106
=== Actor Training Debug (Iteration 4886) ===
Q mean: -13.590643
Q std: 18.179205
Actor loss: 13.594606
Action reg: 0.003963
  l1.weight: grad_norm = 0.177778
  l1.bias: grad_norm = 0.000884
  l2.weight: grad_norm = 0.139181
Total gradient norm: 0.446146
=== Actor Training Debug (Iteration 4887) ===
Q mean: -13.957012
Q std: 19.239258
Actor loss: 13.960994
Action reg: 0.003982
  l1.weight: grad_norm = 0.108039
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.085545
Total gradient norm: 0.302810
=== Actor Training Debug (Iteration 4888) ===
Q mean: -13.506086
Q std: 19.559820
Actor loss: 13.510060
Action reg: 0.003974
  l1.weight: grad_norm = 0.154765
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.134828
Total gradient norm: 0.419089
=== Actor Training Debug (Iteration 4889) ===
Q mean: -14.637695
Q std: 19.370098
Actor loss: 14.641665
Action reg: 0.003969
  l1.weight: grad_norm = 0.057541
  l1.bias: grad_norm = 0.001027
  l2.weight: grad_norm = 0.050724
Total gradient norm: 0.151870
=== Actor Training Debug (Iteration 4890) ===
Q mean: -13.186565
Q std: 19.107962
Actor loss: 13.190544
Action reg: 0.003979
  l1.weight: grad_norm = 0.133843
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.110819
Total gradient norm: 0.416693
=== Actor Training Debug (Iteration 4891) ===
Q mean: -16.349052
Q std: 20.223324
Actor loss: 16.353035
Action reg: 0.003982
  l1.weight: grad_norm = 0.154391
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.121737
Total gradient norm: 0.533046
=== Actor Training Debug (Iteration 4892) ===
Q mean: -13.001463
Q std: 19.004667
Actor loss: 13.005433
Action reg: 0.003970
  l1.weight: grad_norm = 0.323589
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.281041
Total gradient norm: 0.967209
=== Actor Training Debug (Iteration 4893) ===
Q mean: -13.644405
Q std: 19.181616
Actor loss: 13.648375
Action reg: 0.003969
  l1.weight: grad_norm = 0.098532
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.084725
Total gradient norm: 0.297890
=== Actor Training Debug (Iteration 4894) ===
Q mean: -15.147623
Q std: 20.058731
Actor loss: 15.151602
Action reg: 0.003979
  l1.weight: grad_norm = 0.056814
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.046911
Total gradient norm: 0.186073
=== Actor Training Debug (Iteration 4895) ===
Q mean: -12.949503
Q std: 18.881123
Actor loss: 12.953460
Action reg: 0.003956
  l1.weight: grad_norm = 0.123235
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.110150
Total gradient norm: 0.350078
=== Actor Training Debug (Iteration 4896) ===
Q mean: -11.986715
Q std: 17.764292
Actor loss: 11.990694
Action reg: 0.003979
  l1.weight: grad_norm = 0.122216
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.101426
Total gradient norm: 0.317891
=== Actor Training Debug (Iteration 4897) ===
Q mean: -14.214734
Q std: 19.178045
Actor loss: 14.218721
Action reg: 0.003988
  l1.weight: grad_norm = 0.061834
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.049351
Total gradient norm: 0.161585
=== Actor Training Debug (Iteration 4898) ===
Q mean: -14.835543
Q std: 19.543459
Actor loss: 14.839518
Action reg: 0.003975
  l1.weight: grad_norm = 0.220115
  l1.bias: grad_norm = 0.001784
  l2.weight: grad_norm = 0.182970
Total gradient norm: 0.610877
=== Actor Training Debug (Iteration 4899) ===
Q mean: -11.685126
Q std: 17.805946
Actor loss: 11.689104
Action reg: 0.003978
  l1.weight: grad_norm = 0.154965
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.114413
Total gradient norm: 0.375763
=== Actor Training Debug (Iteration 4900) ===
Q mean: -13.694650
Q std: 18.692474
Actor loss: 13.698623
Action reg: 0.003973
  l1.weight: grad_norm = 0.114702
  l1.bias: grad_norm = 0.001493
  l2.weight: grad_norm = 0.093345
Total gradient norm: 0.345879
=== Actor Training Debug (Iteration 4901) ===
Q mean: -12.890259
Q std: 19.110008
Actor loss: 12.894215
Action reg: 0.003956
  l1.weight: grad_norm = 0.129452
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.106521
Total gradient norm: 0.430625
=== Actor Training Debug (Iteration 4902) ===
Q mean: -12.536339
Q std: 18.483074
Actor loss: 12.540316
Action reg: 0.003977
  l1.weight: grad_norm = 0.210707
  l1.bias: grad_norm = 0.001209
  l2.weight: grad_norm = 0.146212
Total gradient norm: 0.542222
=== Actor Training Debug (Iteration 4903) ===
Q mean: -14.822884
Q std: 19.602327
Actor loss: 14.826853
Action reg: 0.003969
  l1.weight: grad_norm = 0.110125
  l1.bias: grad_norm = 0.000641
  l2.weight: grad_norm = 0.073874
Total gradient norm: 0.236359
=== Actor Training Debug (Iteration 4904) ===
Q mean: -12.317008
Q std: 18.212770
Actor loss: 12.320970
Action reg: 0.003962
  l1.weight: grad_norm = 0.126975
  l1.bias: grad_norm = 0.001043
  l2.weight: grad_norm = 0.121455
Total gradient norm: 0.315032
=== Actor Training Debug (Iteration 4905) ===
Q mean: -13.133041
Q std: 19.035801
Actor loss: 13.137012
Action reg: 0.003971
  l1.weight: grad_norm = 0.213164
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.194294
Total gradient norm: 0.745134
=== Actor Training Debug (Iteration 4906) ===
Q mean: -13.648316
Q std: 19.648397
Actor loss: 13.652285
Action reg: 0.003968
  l1.weight: grad_norm = 0.146824
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.121725
Total gradient norm: 0.527454
=== Actor Training Debug (Iteration 4907) ===
Q mean: -14.285313
Q std: 19.851858
Actor loss: 14.289289
Action reg: 0.003976
  l1.weight: grad_norm = 0.166088
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.134955
Total gradient norm: 0.481661
=== Actor Training Debug (Iteration 4908) ===
Q mean: -11.527996
Q std: 16.883308
Actor loss: 11.531966
Action reg: 0.003970
  l1.weight: grad_norm = 0.109464
  l1.bias: grad_norm = 0.000507
  l2.weight: grad_norm = 0.083299
Total gradient norm: 0.269468
=== Actor Training Debug (Iteration 4909) ===
Q mean: -13.203445
Q std: 20.321253
Actor loss: 13.207414
Action reg: 0.003968
  l1.weight: grad_norm = 0.075596
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.064346
Total gradient norm: 0.228447
=== Actor Training Debug (Iteration 4910) ===
Q mean: -14.683050
Q std: 19.779608
Actor loss: 14.687039
Action reg: 0.003989
  l1.weight: grad_norm = 0.062151
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.053090
Total gradient norm: 0.174662
=== Actor Training Debug (Iteration 4911) ===
Q mean: -13.985733
Q std: 18.573992
Actor loss: 13.989718
Action reg: 0.003985
  l1.weight: grad_norm = 0.049732
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.037072
Total gradient norm: 0.114270
=== Actor Training Debug (Iteration 4912) ===
Q mean: -13.166147
Q std: 18.364843
Actor loss: 13.170110
Action reg: 0.003962
  l1.weight: grad_norm = 0.087839
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.070691
Total gradient norm: 0.198711
=== Actor Training Debug (Iteration 4913) ===
Q mean: -13.131607
Q std: 18.066463
Actor loss: 13.135591
Action reg: 0.003983
  l1.weight: grad_norm = 0.134562
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.101139
Total gradient norm: 0.310037
=== Actor Training Debug (Iteration 4914) ===
Q mean: -14.695267
Q std: 18.779119
Actor loss: 14.699255
Action reg: 0.003988
  l1.weight: grad_norm = 0.155270
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.121172
Total gradient norm: 0.500826
=== Actor Training Debug (Iteration 4915) ===
Q mean: -13.785582
Q std: 18.917517
Actor loss: 13.789556
Action reg: 0.003974
  l1.weight: grad_norm = 0.093715
  l1.bias: grad_norm = 0.000449
  l2.weight: grad_norm = 0.066214
Total gradient norm: 0.271078
=== Actor Training Debug (Iteration 4916) ===
Q mean: -11.066174
Q std: 16.976814
Actor loss: 11.070135
Action reg: 0.003962
  l1.weight: grad_norm = 0.186069
  l1.bias: grad_norm = 0.002907
  l2.weight: grad_norm = 0.159021
Total gradient norm: 0.494341
=== Actor Training Debug (Iteration 4917) ===
Q mean: -13.403131
Q std: 18.253166
Actor loss: 13.407106
Action reg: 0.003975
  l1.weight: grad_norm = 0.102766
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.078942
Total gradient norm: 0.243676
=== Actor Training Debug (Iteration 4918) ===
Q mean: -12.211647
Q std: 19.607603
Actor loss: 12.215624
Action reg: 0.003977
  l1.weight: grad_norm = 0.162120
  l1.bias: grad_norm = 0.000550
  l2.weight: grad_norm = 0.129941
Total gradient norm: 0.443791
=== Actor Training Debug (Iteration 4919) ===
Q mean: -13.330885
Q std: 18.655485
Actor loss: 13.334874
Action reg: 0.003989
  l1.weight: grad_norm = 0.078693
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.065666
Total gradient norm: 0.185259
=== Actor Training Debug (Iteration 4920) ===
Q mean: -13.661999
Q std: 18.667353
Actor loss: 13.665968
Action reg: 0.003969
  l1.weight: grad_norm = 0.065807
  l1.bias: grad_norm = 0.003428
  l2.weight: grad_norm = 0.058828
Total gradient norm: 0.227968
=== Actor Training Debug (Iteration 4921) ===
Q mean: -16.894840
Q std: 21.093117
Actor loss: 16.898809
Action reg: 0.003970
  l1.weight: grad_norm = 0.067621
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.052315
Total gradient norm: 0.172894
=== Actor Training Debug (Iteration 4922) ===
Q mean: -12.423734
Q std: 17.650717
Actor loss: 12.427727
Action reg: 0.003993
  l1.weight: grad_norm = 0.015797
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.012075
Total gradient norm: 0.045820
=== Actor Training Debug (Iteration 4923) ===
Q mean: -13.445811
Q std: 18.662111
Actor loss: 13.449792
Action reg: 0.003981
  l1.weight: grad_norm = 0.084593
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.065698
Total gradient norm: 0.186501
=== Actor Training Debug (Iteration 4924) ===
Q mean: -14.202330
Q std: 18.976028
Actor loss: 14.206312
Action reg: 0.003983
  l1.weight: grad_norm = 0.053279
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.040339
Total gradient norm: 0.126556
=== Actor Training Debug (Iteration 4925) ===
Q mean: -13.605513
Q std: 18.424990
Actor loss: 13.609499
Action reg: 0.003986
  l1.weight: grad_norm = 0.073265
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.061822
Total gradient norm: 0.186691
=== Actor Training Debug (Iteration 4926) ===
Q mean: -11.035069
Q std: 18.255342
Actor loss: 11.039041
Action reg: 0.003972
  l1.weight: grad_norm = 0.089893
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.075710
Total gradient norm: 0.218356
=== Actor Training Debug (Iteration 4927) ===
Q mean: -15.463909
Q std: 20.007511
Actor loss: 15.467890
Action reg: 0.003981
  l1.weight: grad_norm = 0.067105
  l1.bias: grad_norm = 0.000559
  l2.weight: grad_norm = 0.051296
Total gradient norm: 0.184187
=== Actor Training Debug (Iteration 4928) ===
Q mean: -12.512499
Q std: 17.945038
Actor loss: 12.516481
Action reg: 0.003982
  l1.weight: grad_norm = 0.059131
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.049112
Total gradient norm: 0.172114
=== Actor Training Debug (Iteration 4929) ===
Q mean: -13.987690
Q std: 19.635441
Actor loss: 13.991662
Action reg: 0.003972
  l1.weight: grad_norm = 0.093685
  l1.bias: grad_norm = 0.000591
  l2.weight: grad_norm = 0.071417
Total gradient norm: 0.270773
=== Actor Training Debug (Iteration 4930) ===
Q mean: -11.368678
Q std: 17.980064
Actor loss: 11.372643
Action reg: 0.003965
  l1.weight: grad_norm = 0.227169
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.164877
Total gradient norm: 0.572983
=== Actor Training Debug (Iteration 4931) ===
Q mean: -12.734532
Q std: 18.579418
Actor loss: 12.738521
Action reg: 0.003988
  l1.weight: grad_norm = 0.038073
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.027554
Total gradient norm: 0.096212
=== Actor Training Debug (Iteration 4932) ===
Q mean: -11.401510
Q std: 18.540281
Actor loss: 11.405491
Action reg: 0.003981
  l1.weight: grad_norm = 0.075886
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.058555
Total gradient norm: 0.203550
=== Actor Training Debug (Iteration 4933) ===
Q mean: -13.694633
Q std: 19.050520
Actor loss: 13.698606
Action reg: 0.003973
  l1.weight: grad_norm = 0.096844
  l1.bias: grad_norm = 0.001178
  l2.weight: grad_norm = 0.078597
Total gradient norm: 0.254654
=== Actor Training Debug (Iteration 4934) ===
Q mean: -12.911066
Q std: 19.484015
Actor loss: 12.915037
Action reg: 0.003971
  l1.weight: grad_norm = 0.169414
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.110503
Total gradient norm: 0.359227
=== Actor Training Debug (Iteration 4935) ===
Q mean: -13.977852
Q std: 19.366320
Actor loss: 13.981824
Action reg: 0.003972
  l1.weight: grad_norm = 0.051976
  l1.bias: grad_norm = 0.003000
  l2.weight: grad_norm = 0.043877
Total gradient norm: 0.134522
=== Actor Training Debug (Iteration 4936) ===
Q mean: -12.739308
Q std: 18.066040
Actor loss: 12.743287
Action reg: 0.003979
  l1.weight: grad_norm = 0.230359
  l1.bias: grad_norm = 0.001510
  l2.weight: grad_norm = 0.140548
Total gradient norm: 0.414131
=== Actor Training Debug (Iteration 4937) ===
Q mean: -13.455948
Q std: 18.728371
Actor loss: 13.459926
Action reg: 0.003978
  l1.weight: grad_norm = 0.130840
  l1.bias: grad_norm = 0.001574
  l2.weight: grad_norm = 0.110422
Total gradient norm: 0.317808
=== Actor Training Debug (Iteration 4938) ===
Q mean: -13.961132
Q std: 19.208303
Actor loss: 13.965094
Action reg: 0.003962
  l1.weight: grad_norm = 0.115553
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.103373
Total gradient norm: 0.315355
=== Actor Training Debug (Iteration 4939) ===
Q mean: -14.151239
Q std: 20.591331
Actor loss: 14.155219
Action reg: 0.003980
  l1.weight: grad_norm = 0.102192
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.077527
Total gradient norm: 0.255681
=== Actor Training Debug (Iteration 4940) ===
Q mean: -13.751411
Q std: 18.401379
Actor loss: 13.755390
Action reg: 0.003979
  l1.weight: grad_norm = 0.181204
  l1.bias: grad_norm = 0.000561
  l2.weight: grad_norm = 0.135538
Total gradient norm: 0.574037
=== Actor Training Debug (Iteration 4941) ===
Q mean: -12.699362
Q std: 18.071056
Actor loss: 12.703339
Action reg: 0.003977
  l1.weight: grad_norm = 0.169779
  l1.bias: grad_norm = 0.000584
  l2.weight: grad_norm = 0.113695
Total gradient norm: 0.377174
=== Actor Training Debug (Iteration 4942) ===
Q mean: -11.921779
Q std: 18.639978
Actor loss: 11.925747
Action reg: 0.003969
  l1.weight: grad_norm = 0.172251
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.117340
Total gradient norm: 0.353594
=== Actor Training Debug (Iteration 4943) ===
Q mean: -15.273061
Q std: 20.377274
Actor loss: 15.277051
Action reg: 0.003990
  l1.weight: grad_norm = 0.060946
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.052147
Total gradient norm: 0.170588
=== Actor Training Debug (Iteration 4944) ===
Q mean: -14.510862
Q std: 19.133266
Actor loss: 14.514838
Action reg: 0.003976
  l1.weight: grad_norm = 0.165839
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.114106
Total gradient norm: 0.340790
=== Actor Training Debug (Iteration 4945) ===
Q mean: -12.474691
Q std: 18.646605
Actor loss: 12.478657
Action reg: 0.003966
  l1.weight: grad_norm = 0.221738
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.186817
Total gradient norm: 0.620444
=== Actor Training Debug (Iteration 4946) ===
Q mean: -11.949719
Q std: 19.465000
Actor loss: 11.953697
Action reg: 0.003978
  l1.weight: grad_norm = 0.113713
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.092792
Total gradient norm: 0.283115
=== Actor Training Debug (Iteration 4947) ===
Q mean: -12.818108
Q std: 18.047375
Actor loss: 12.822085
Action reg: 0.003978
  l1.weight: grad_norm = 0.087846
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.066488
Total gradient norm: 0.197277
=== Actor Training Debug (Iteration 4948) ===
Q mean: -14.152924
Q std: 19.199650
Actor loss: 14.156899
Action reg: 0.003976
  l1.weight: grad_norm = 0.110214
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.076510
Total gradient norm: 0.244942
=== Actor Training Debug (Iteration 4949) ===
Q mean: -12.022233
Q std: 17.741528
Actor loss: 12.026217
Action reg: 0.003984
  l1.weight: grad_norm = 0.056227
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.047329
Total gradient norm: 0.164523
=== Actor Training Debug (Iteration 4950) ===
Q mean: -14.394468
Q std: 19.641291
Actor loss: 14.398439
Action reg: 0.003971
  l1.weight: grad_norm = 0.081729
  l1.bias: grad_norm = 0.001102
  l2.weight: grad_norm = 0.075415
Total gradient norm: 0.239662
=== Actor Training Debug (Iteration 4951) ===
Q mean: -13.338046
Q std: 19.231619
Actor loss: 13.342008
Action reg: 0.003962
  l1.weight: grad_norm = 0.163128
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.118138
Total gradient norm: 0.354299
=== Actor Training Debug (Iteration 4952) ===
Q mean: -11.319338
Q std: 18.345615
Actor loss: 11.323319
Action reg: 0.003982
  l1.weight: grad_norm = 0.171979
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.137427
Total gradient norm: 0.416421
=== Actor Training Debug (Iteration 4953) ===
Q mean: -14.989336
Q std: 20.672688
Actor loss: 14.993313
Action reg: 0.003977
  l1.weight: grad_norm = 0.103737
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.082459
Total gradient norm: 0.268943
=== Actor Training Debug (Iteration 4954) ===
Q mean: -15.016893
Q std: 20.237225
Actor loss: 15.020858
Action reg: 0.003964
  l1.weight: grad_norm = 0.108982
  l1.bias: grad_norm = 0.001412
  l2.weight: grad_norm = 0.083198
Total gradient norm: 0.316211
=== Actor Training Debug (Iteration 4955) ===
Q mean: -15.856784
Q std: 20.460779
Actor loss: 15.860758
Action reg: 0.003974
  l1.weight: grad_norm = 0.073188
  l1.bias: grad_norm = 0.000835
  l2.weight: grad_norm = 0.063167
Total gradient norm: 0.238332
=== Actor Training Debug (Iteration 4956) ===
Q mean: -15.535686
Q std: 19.472807
Actor loss: 15.539660
Action reg: 0.003973
  l1.weight: grad_norm = 0.087883
  l1.bias: grad_norm = 0.000917
  l2.weight: grad_norm = 0.062093
Total gradient norm: 0.193933
=== Actor Training Debug (Iteration 4957) ===
Q mean: -12.578926
Q std: 18.939089
Actor loss: 12.582905
Action reg: 0.003979
  l1.weight: grad_norm = 0.122898
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.087362
Total gradient norm: 0.267384
=== Actor Training Debug (Iteration 4958) ===
Q mean: -13.273970
Q std: 19.315928
Actor loss: 13.277947
Action reg: 0.003977
  l1.weight: grad_norm = 0.198234
  l1.bias: grad_norm = 0.000661
  l2.weight: grad_norm = 0.147762
Total gradient norm: 0.621290
=== Actor Training Debug (Iteration 4959) ===
Q mean: -11.863579
Q std: 17.339727
Actor loss: 11.867554
Action reg: 0.003975
  l1.weight: grad_norm = 0.089495
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.066118
Total gradient norm: 0.247355
=== Actor Training Debug (Iteration 4960) ===
Q mean: -15.674916
Q std: 19.737913
Actor loss: 15.678882
Action reg: 0.003966
  l1.weight: grad_norm = 0.113539
  l1.bias: grad_norm = 0.000632
  l2.weight: grad_norm = 0.073099
Total gradient norm: 0.252523
=== Actor Training Debug (Iteration 4961) ===
Q mean: -14.170651
Q std: 18.923216
Actor loss: 14.174637
Action reg: 0.003985
  l1.weight: grad_norm = 0.137809
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.113338
Total gradient norm: 0.328346
=== Actor Training Debug (Iteration 4962) ===
Q mean: -15.468861
Q std: 20.327452
Actor loss: 15.472837
Action reg: 0.003977
  l1.weight: grad_norm = 0.128433
  l1.bias: grad_norm = 0.001282
  l2.weight: grad_norm = 0.096512
Total gradient norm: 0.381088
=== Actor Training Debug (Iteration 4963) ===
Q mean: -12.277452
Q std: 17.869411
Actor loss: 12.281427
Action reg: 0.003975
  l1.weight: grad_norm = 0.143281
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.122397
Total gradient norm: 0.366044
=== Actor Training Debug (Iteration 4964) ===
Q mean: -11.257830
Q std: 17.617304
Actor loss: 11.261813
Action reg: 0.003984
  l1.weight: grad_norm = 0.098237
  l1.bias: grad_norm = 0.001495
  l2.weight: grad_norm = 0.084580
Total gradient norm: 0.294129
=== Actor Training Debug (Iteration 4965) ===
Q mean: -12.616937
Q std: 17.772537
Actor loss: 12.620916
Action reg: 0.003980
  l1.weight: grad_norm = 0.117005
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.088620
Total gradient norm: 0.330059
=== Actor Training Debug (Iteration 4966) ===
Q mean: -11.806664
Q std: 17.188040
Actor loss: 11.810640
Action reg: 0.003977
  l1.weight: grad_norm = 0.110722
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.070963
Total gradient norm: 0.246649
=== Actor Training Debug (Iteration 4967) ===
Q mean: -13.019525
Q std: 18.116287
Actor loss: 13.023505
Action reg: 0.003980
  l1.weight: grad_norm = 0.112045
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.112894
Total gradient norm: 0.411711
=== Actor Training Debug (Iteration 4968) ===
Q mean: -13.538056
Q std: 18.814150
Actor loss: 13.542032
Action reg: 0.003976
  l1.weight: grad_norm = 0.112332
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.065216
Total gradient norm: 0.203030
=== Actor Training Debug (Iteration 4969) ===
Q mean: -12.879239
Q std: 18.580851
Actor loss: 12.883204
Action reg: 0.003965
  l1.weight: grad_norm = 0.125709
  l1.bias: grad_norm = 0.002164
  l2.weight: grad_norm = 0.100974
Total gradient norm: 0.364925
=== Actor Training Debug (Iteration 4970) ===
Q mean: -10.730922
Q std: 17.464691
Actor loss: 10.734897
Action reg: 0.003975
  l1.weight: grad_norm = 0.060879
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.045770
Total gradient norm: 0.137172
=== Actor Training Debug (Iteration 4971) ===
Q mean: -11.779634
Q std: 17.623327
Actor loss: 11.783610
Action reg: 0.003977
  l1.weight: grad_norm = 0.081925
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.063557
Total gradient norm: 0.234461
=== Actor Training Debug (Iteration 4972) ===
Q mean: -12.974118
Q std: 18.601818
Actor loss: 12.978091
Action reg: 0.003973
  l1.weight: grad_norm = 0.177914
  l1.bias: grad_norm = 0.002286
  l2.weight: grad_norm = 0.152813
Total gradient norm: 0.535542
=== Actor Training Debug (Iteration 4973) ===
Q mean: -13.570583
Q std: 19.042912
Actor loss: 13.574553
Action reg: 0.003970
  l1.weight: grad_norm = 0.106131
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.097487
Total gradient norm: 0.290827
=== Actor Training Debug (Iteration 4974) ===
Q mean: -12.796114
Q std: 18.306261
Actor loss: 12.800077
Action reg: 0.003963
  l1.weight: grad_norm = 0.204532
  l1.bias: grad_norm = 0.001572
  l2.weight: grad_norm = 0.152092
Total gradient norm: 0.521537
=== Actor Training Debug (Iteration 4975) ===
Q mean: -14.408266
Q std: 19.135172
Actor loss: 14.412239
Action reg: 0.003973
  l1.weight: grad_norm = 0.092845
  l1.bias: grad_norm = 0.000751
  l2.weight: grad_norm = 0.079174
Total gradient norm: 0.257674
=== Actor Training Debug (Iteration 4976) ===
Q mean: -13.084015
Q std: 18.699594
Actor loss: 13.087987
Action reg: 0.003972
  l1.weight: grad_norm = 0.066724
  l1.bias: grad_norm = 0.001451
  l2.weight: grad_norm = 0.064443
Total gradient norm: 0.197469
=== Actor Training Debug (Iteration 4977) ===
Q mean: -14.733593
Q std: 19.371969
Actor loss: 14.737565
Action reg: 0.003972
  l1.weight: grad_norm = 0.034044
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.027636
Total gradient norm: 0.097114
=== Actor Training Debug (Iteration 4978) ===
Q mean: -14.387277
Q std: 19.880173
Actor loss: 14.391252
Action reg: 0.003975
  l1.weight: grad_norm = 0.066346
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.051393
Total gradient norm: 0.164724
=== Actor Training Debug (Iteration 4979) ===
Q mean: -11.170326
Q std: 17.059353
Actor loss: 11.174302
Action reg: 0.003976
  l1.weight: grad_norm = 0.093327
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.083900
Total gradient norm: 0.292251
=== Actor Training Debug (Iteration 4980) ===
Q mean: -13.397736
Q std: 19.037188
Actor loss: 13.401711
Action reg: 0.003975
  l1.weight: grad_norm = 0.057101
  l1.bias: grad_norm = 0.000757
  l2.weight: grad_norm = 0.047935
Total gradient norm: 0.141233
=== Actor Training Debug (Iteration 4981) ===
Q mean: -13.315573
Q std: 18.694014
Actor loss: 13.319548
Action reg: 0.003975
  l1.weight: grad_norm = 0.084077
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.066331
Total gradient norm: 0.197319
=== Actor Training Debug (Iteration 4982) ===
Q mean: -13.802507
Q std: 19.822517
Actor loss: 13.806492
Action reg: 0.003984
  l1.weight: grad_norm = 0.042285
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.028746
Total gradient norm: 0.087411
=== Actor Training Debug (Iteration 4983) ===
Q mean: -12.941956
Q std: 19.451237
Actor loss: 12.945933
Action reg: 0.003978
  l1.weight: grad_norm = 0.089277
  l1.bias: grad_norm = 0.000528
  l2.weight: grad_norm = 0.081047
Total gradient norm: 0.281738
=== Actor Training Debug (Iteration 4984) ===
Q mean: -13.346130
Q std: 18.186445
Actor loss: 13.350103
Action reg: 0.003973
  l1.weight: grad_norm = 0.060417
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.051204
Total gradient norm: 0.179087
=== Actor Training Debug (Iteration 4985) ===
Q mean: -15.177418
Q std: 19.151461
Actor loss: 15.181394
Action reg: 0.003976
  l1.weight: grad_norm = 0.096370
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.083437
Total gradient norm: 0.310139
=== Actor Training Debug (Iteration 4986) ===
Q mean: -10.922449
Q std: 17.808561
Actor loss: 10.926437
Action reg: 0.003989
  l1.weight: grad_norm = 0.096902
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.074141
Total gradient norm: 0.230677
=== Actor Training Debug (Iteration 4987) ===
Q mean: -13.205217
Q std: 19.402388
Actor loss: 13.209199
Action reg: 0.003982
  l1.weight: grad_norm = 0.073416
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.050838
Total gradient norm: 0.158969
=== Actor Training Debug (Iteration 4988) ===
Q mean: -12.566292
Q std: 18.362993
Actor loss: 12.570267
Action reg: 0.003975
  l1.weight: grad_norm = 0.209095
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.149892
Total gradient norm: 0.455305
=== Actor Training Debug (Iteration 4989) ===
Q mean: -14.666354
Q std: 19.976540
Actor loss: 14.670338
Action reg: 0.003983
  l1.weight: grad_norm = 0.133671
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.107869
Total gradient norm: 0.352573
=== Actor Training Debug (Iteration 4990) ===
Q mean: -13.853247
Q std: 19.943726
Actor loss: 13.857210
Action reg: 0.003963
  l1.weight: grad_norm = 0.190314
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.154961
Total gradient norm: 0.495502
=== Actor Training Debug (Iteration 4991) ===
Q mean: -14.040859
Q std: 18.780092
Actor loss: 14.044830
Action reg: 0.003971
  l1.weight: grad_norm = 0.162545
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.110160
Total gradient norm: 0.409401
=== Actor Training Debug (Iteration 4992) ===
Q mean: -13.778221
Q std: 18.237562
Actor loss: 13.782202
Action reg: 0.003981
  l1.weight: grad_norm = 0.203776
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.139553
Total gradient norm: 0.507739
=== Actor Training Debug (Iteration 4993) ===
Q mean: -13.350494
Q std: 19.090673
Actor loss: 13.354472
Action reg: 0.003978
  l1.weight: grad_norm = 0.118370
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.087453
Total gradient norm: 0.296972
=== Actor Training Debug (Iteration 4994) ===
Q mean: -13.997881
Q std: 19.150583
Actor loss: 14.001858
Action reg: 0.003977
  l1.weight: grad_norm = 0.160314
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.121315
Total gradient norm: 0.374320
=== Actor Training Debug (Iteration 4995) ===
Q mean: -13.879736
Q std: 19.651861
Actor loss: 13.883704
Action reg: 0.003969
  l1.weight: grad_norm = 0.084333
  l1.bias: grad_norm = 0.000935
  l2.weight: grad_norm = 0.075582
Total gradient norm: 0.212247
=== Actor Training Debug (Iteration 4996) ===
Q mean: -13.560627
Q std: 19.184881
Actor loss: 13.564604
Action reg: 0.003976
  l1.weight: grad_norm = 0.073974
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.058482
Total gradient norm: 0.192544
=== Actor Training Debug (Iteration 4997) ===
Q mean: -12.499952
Q std: 19.651266
Actor loss: 12.503920
Action reg: 0.003968
  l1.weight: grad_norm = 0.122818
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.100917
Total gradient norm: 0.397065
=== Actor Training Debug (Iteration 4998) ===
Q mean: -14.129305
Q std: 19.628521
Actor loss: 14.133288
Action reg: 0.003983
  l1.weight: grad_norm = 0.069098
  l1.bias: grad_norm = 0.002930
  l2.weight: grad_norm = 0.066278
Total gradient norm: 0.231002
=== Actor Training Debug (Iteration 4999) ===
Q mean: -15.286010
Q std: 20.525877
Actor loss: 15.289986
Action reg: 0.003976
  l1.weight: grad_norm = 0.123922
  l1.bias: grad_norm = 0.005431
  l2.weight: grad_norm = 0.092891
Total gradient norm: 0.285024
=== Actor Training Debug (Iteration 5000) ===
Q mean: -12.996449
Q std: 19.038054
Actor loss: 13.000419
Action reg: 0.003970
  l1.weight: grad_norm = 0.281662
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.187843
Total gradient norm: 0.538503
Step 10000: Critic Loss: 0.7700, Actor Loss: 13.0004, Q Value: -12.9964
  Average reward: -319.997 | Average length: 100.0
Evaluation at episode 100: -319.997
=== Actor Training Debug (Iteration 5001) ===
Q mean: -14.516840
Q std: 18.510710
Actor loss: 14.520820
Action reg: 0.003980
  l1.weight: grad_norm = 0.050232
  l1.bias: grad_norm = 0.000684
  l2.weight: grad_norm = 0.036612
Total gradient norm: 0.141016
=== Actor Training Debug (Iteration 5002) ===
Q mean: -15.195350
Q std: 20.066090
Actor loss: 15.199317
Action reg: 0.003967
  l1.weight: grad_norm = 0.157739
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.111878
Total gradient norm: 0.448640
=== Actor Training Debug (Iteration 5003) ===
Q mean: -10.789946
Q std: 17.033117
Actor loss: 10.793915
Action reg: 0.003969
  l1.weight: grad_norm = 0.142914
  l1.bias: grad_norm = 0.000476
  l2.weight: grad_norm = 0.099580
Total gradient norm: 0.301217
=== Actor Training Debug (Iteration 5004) ===
Q mean: -12.925480
Q std: 19.610901
Actor loss: 12.929452
Action reg: 0.003972
  l1.weight: grad_norm = 0.060345
  l1.bias: grad_norm = 0.000774
  l2.weight: grad_norm = 0.045635
Total gradient norm: 0.158865
=== Actor Training Debug (Iteration 5005) ===
Q mean: -10.702450
Q std: 17.887968
Actor loss: 10.706427
Action reg: 0.003977
  l1.weight: grad_norm = 0.079135
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.049584
Total gradient norm: 0.153021
=== Actor Training Debug (Iteration 5006) ===
Q mean: -14.140008
Q std: 19.553694
Actor loss: 14.143985
Action reg: 0.003976
  l1.weight: grad_norm = 0.025629
  l1.bias: grad_norm = 0.000976
  l2.weight: grad_norm = 0.021962
Total gradient norm: 0.080250
=== Actor Training Debug (Iteration 5007) ===
Q mean: -12.533094
Q std: 18.444986
Actor loss: 12.537087
Action reg: 0.003993
  l1.weight: grad_norm = 0.029760
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.025336
Total gradient norm: 0.076646
=== Actor Training Debug (Iteration 5008) ===
Q mean: -13.880899
Q std: 19.760416
Actor loss: 13.884881
Action reg: 0.003981
  l1.weight: grad_norm = 0.029460
  l1.bias: grad_norm = 0.000609
  l2.weight: grad_norm = 0.023040
Total gradient norm: 0.077178
=== Actor Training Debug (Iteration 5009) ===
Q mean: -12.745051
Q std: 18.612898
Actor loss: 12.749026
Action reg: 0.003975
  l1.weight: grad_norm = 0.061686
  l1.bias: grad_norm = 0.000661
  l2.weight: grad_norm = 0.046574
Total gradient norm: 0.140483
=== Actor Training Debug (Iteration 5010) ===
Q mean: -13.109100
Q std: 19.002447
Actor loss: 13.113081
Action reg: 0.003980
  l1.weight: grad_norm = 0.136728
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.113795
Total gradient norm: 0.334783
=== Actor Training Debug (Iteration 5011) ===
Q mean: -14.804864
Q std: 21.295965
Actor loss: 14.808845
Action reg: 0.003981
  l1.weight: grad_norm = 0.090397
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.061698
Total gradient norm: 0.188498
=== Actor Training Debug (Iteration 5012) ===
Q mean: -14.414249
Q std: 19.415838
Actor loss: 14.418230
Action reg: 0.003980
  l1.weight: grad_norm = 0.120476
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.095863
Total gradient norm: 0.269934
=== Actor Training Debug (Iteration 5013) ===
Q mean: -13.564348
Q std: 18.930992
Actor loss: 13.568324
Action reg: 0.003976
  l1.weight: grad_norm = 0.076400
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.068952
Total gradient norm: 0.240113
=== Actor Training Debug (Iteration 5014) ===
Q mean: -15.180815
Q std: 20.645592
Actor loss: 15.184797
Action reg: 0.003983
  l1.weight: grad_norm = 0.124263
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.095971
Total gradient norm: 0.330770
=== Actor Training Debug (Iteration 5015) ===
Q mean: -15.111771
Q std: 20.437237
Actor loss: 15.115747
Action reg: 0.003977
  l1.weight: grad_norm = 0.038031
  l1.bias: grad_norm = 0.000513
  l2.weight: grad_norm = 0.030355
Total gradient norm: 0.109001
=== Actor Training Debug (Iteration 5016) ===
Q mean: -13.346858
Q std: 19.426603
Actor loss: 13.350831
Action reg: 0.003973
  l1.weight: grad_norm = 0.078552
  l1.bias: grad_norm = 0.001375
  l2.weight: grad_norm = 0.068823
Total gradient norm: 0.228774
=== Actor Training Debug (Iteration 5017) ===
Q mean: -13.937209
Q std: 18.759567
Actor loss: 13.941175
Action reg: 0.003967
  l1.weight: grad_norm = 0.125863
  l1.bias: grad_norm = 0.000551
  l2.weight: grad_norm = 0.098655
Total gradient norm: 0.327724
=== Actor Training Debug (Iteration 5018) ===
Q mean: -12.290453
Q std: 17.521055
Actor loss: 12.294428
Action reg: 0.003975
  l1.weight: grad_norm = 0.107870
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.075225
Total gradient norm: 0.223059
=== Actor Training Debug (Iteration 5019) ===
Q mean: -12.849630
Q std: 18.965471
Actor loss: 12.853606
Action reg: 0.003976
  l1.weight: grad_norm = 0.114032
  l1.bias: grad_norm = 0.002358
  l2.weight: grad_norm = 0.078892
Total gradient norm: 0.254361
=== Actor Training Debug (Iteration 5020) ===
Q mean: -15.088360
Q std: 20.521751
Actor loss: 15.092343
Action reg: 0.003983
  l1.weight: grad_norm = 0.176520
  l1.bias: grad_norm = 0.001549
  l2.weight: grad_norm = 0.159185
Total gradient norm: 0.497405
=== Actor Training Debug (Iteration 5021) ===
Q mean: -14.122740
Q std: 19.642960
Actor loss: 14.126704
Action reg: 0.003964
  l1.weight: grad_norm = 0.094047
  l1.bias: grad_norm = 0.001970
  l2.weight: grad_norm = 0.071285
Total gradient norm: 0.207572
=== Actor Training Debug (Iteration 5022) ===
Q mean: -13.880285
Q std: 18.825638
Actor loss: 13.884269
Action reg: 0.003984
  l1.weight: grad_norm = 0.099183
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.078769
Total gradient norm: 0.229822
=== Actor Training Debug (Iteration 5023) ===
Q mean: -13.899979
Q std: 19.031061
Actor loss: 13.903951
Action reg: 0.003972
  l1.weight: grad_norm = 0.055154
  l1.bias: grad_norm = 0.000529
  l2.weight: grad_norm = 0.058545
Total gradient norm: 0.182530
=== Actor Training Debug (Iteration 5024) ===
Q mean: -12.462283
Q std: 19.159332
Actor loss: 12.466251
Action reg: 0.003968
  l1.weight: grad_norm = 0.312062
  l1.bias: grad_norm = 0.000620
  l2.weight: grad_norm = 0.206510
Total gradient norm: 0.621372
=== Actor Training Debug (Iteration 5025) ===
Q mean: -13.394186
Q std: 19.240847
Actor loss: 13.398148
Action reg: 0.003962
  l1.weight: grad_norm = 0.192847
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.158454
Total gradient norm: 0.526613
=== Actor Training Debug (Iteration 5026) ===
Q mean: -13.609591
Q std: 19.716179
Actor loss: 13.613574
Action reg: 0.003982
  l1.weight: grad_norm = 0.146713
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.109517
Total gradient norm: 0.356914
=== Actor Training Debug (Iteration 5027) ===
Q mean: -13.259439
Q std: 19.200434
Actor loss: 13.263400
Action reg: 0.003961
  l1.weight: grad_norm = 0.139116
  l1.bias: grad_norm = 0.000609
  l2.weight: grad_norm = 0.106539
Total gradient norm: 0.286954
=== Actor Training Debug (Iteration 5028) ===
Q mean: -13.315737
Q std: 19.353378
Actor loss: 13.319725
Action reg: 0.003988
  l1.weight: grad_norm = 0.093697
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.067207
Total gradient norm: 0.173701
=== Actor Training Debug (Iteration 5029) ===
Q mean: -11.346181
Q std: 18.188198
Actor loss: 11.350147
Action reg: 0.003966
  l1.weight: grad_norm = 0.249814
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.161453
Total gradient norm: 0.560313
=== Actor Training Debug (Iteration 5030) ===
Q mean: -12.411587
Q std: 18.676128
Actor loss: 12.415577
Action reg: 0.003990
  l1.weight: grad_norm = 0.139967
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.107923
Total gradient norm: 0.247875
=== Actor Training Debug (Iteration 5031) ===
Q mean: -14.021473
Q std: 19.658142
Actor loss: 14.025447
Action reg: 0.003974
  l1.weight: grad_norm = 0.108303
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.091445
Total gradient norm: 0.305430
=== Actor Training Debug (Iteration 5032) ===
Q mean: -12.913935
Q std: 18.003813
Actor loss: 12.917909
Action reg: 0.003974
  l1.weight: grad_norm = 0.089019
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.072305
Total gradient norm: 0.316717
=== Actor Training Debug (Iteration 5033) ===
Q mean: -13.138777
Q std: 18.461296
Actor loss: 13.142749
Action reg: 0.003972
  l1.weight: grad_norm = 0.193298
  l1.bias: grad_norm = 0.000378
  l2.weight: grad_norm = 0.145057
Total gradient norm: 0.465202
=== Actor Training Debug (Iteration 5034) ===
Q mean: -13.763700
Q std: 19.237112
Actor loss: 13.767681
Action reg: 0.003982
  l1.weight: grad_norm = 0.194587
  l1.bias: grad_norm = 0.000764
  l2.weight: grad_norm = 0.154757
Total gradient norm: 0.471640
=== Actor Training Debug (Iteration 5035) ===
Q mean: -11.524360
Q std: 17.763525
Actor loss: 11.528328
Action reg: 0.003969
  l1.weight: grad_norm = 0.187336
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.136086
Total gradient norm: 0.522211
=== Actor Training Debug (Iteration 5036) ===
Q mean: -15.277297
Q std: 19.334602
Actor loss: 15.281270
Action reg: 0.003973
  l1.weight: grad_norm = 0.091121
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.079729
Total gradient norm: 0.256242
=== Actor Training Debug (Iteration 5037) ===
Q mean: -17.135984
Q std: 21.181576
Actor loss: 17.139961
Action reg: 0.003977
  l1.weight: grad_norm = 0.109598
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.091514
Total gradient norm: 0.290496
=== Actor Training Debug (Iteration 5038) ===
Q mean: -14.327487
Q std: 19.041983
Actor loss: 14.331442
Action reg: 0.003955
  l1.weight: grad_norm = 0.115407
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.089012
Total gradient norm: 0.278186
=== Actor Training Debug (Iteration 5039) ===
Q mean: -11.975378
Q std: 18.017960
Actor loss: 11.979350
Action reg: 0.003972
  l1.weight: grad_norm = 0.090359
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.073956
Total gradient norm: 0.233079
=== Actor Training Debug (Iteration 5040) ===
Q mean: -11.417403
Q std: 18.021017
Actor loss: 11.421363
Action reg: 0.003959
  l1.weight: grad_norm = 0.191618
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.134083
Total gradient norm: 0.413535
=== Actor Training Debug (Iteration 5041) ===
Q mean: -11.843483
Q std: 18.083805
Actor loss: 11.847467
Action reg: 0.003984
  l1.weight: grad_norm = 0.077047
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.059984
Total gradient norm: 0.186392
=== Actor Training Debug (Iteration 5042) ===
Q mean: -14.612703
Q std: 19.290440
Actor loss: 14.616682
Action reg: 0.003979
  l1.weight: grad_norm = 0.122730
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.092005
Total gradient norm: 0.319034
=== Actor Training Debug (Iteration 5043) ===
Q mean: -15.766869
Q std: 20.714897
Actor loss: 15.770840
Action reg: 0.003972
  l1.weight: grad_norm = 0.178961
  l1.bias: grad_norm = 0.001592
  l2.weight: grad_norm = 0.147297
Total gradient norm: 0.505708
=== Actor Training Debug (Iteration 5044) ===
Q mean: -12.034007
Q std: 17.349142
Actor loss: 12.037990
Action reg: 0.003982
  l1.weight: grad_norm = 0.149745
  l1.bias: grad_norm = 0.001665
  l2.weight: grad_norm = 0.111875
Total gradient norm: 0.384139
=== Actor Training Debug (Iteration 5045) ===
Q mean: -13.477869
Q std: 18.715410
Actor loss: 13.481834
Action reg: 0.003966
  l1.weight: grad_norm = 0.099129
  l1.bias: grad_norm = 0.000525
  l2.weight: grad_norm = 0.080821
Total gradient norm: 0.238193
=== Actor Training Debug (Iteration 5046) ===
Q mean: -15.669012
Q std: 19.842228
Actor loss: 15.672995
Action reg: 0.003982
  l1.weight: grad_norm = 0.088953
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.058480
Total gradient norm: 0.169027
=== Actor Training Debug (Iteration 5047) ===
Q mean: -13.480095
Q std: 19.890844
Actor loss: 13.484072
Action reg: 0.003977
  l1.weight: grad_norm = 0.127936
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.106120
Total gradient norm: 0.396162
=== Actor Training Debug (Iteration 5048) ===
Q mean: -12.635361
Q std: 18.844101
Actor loss: 12.639333
Action reg: 0.003972
  l1.weight: grad_norm = 0.076197
  l1.bias: grad_norm = 0.000650
  l2.weight: grad_norm = 0.064881
Total gradient norm: 0.199682
=== Actor Training Debug (Iteration 5049) ===
Q mean: -12.328592
Q std: 18.087915
Actor loss: 12.332556
Action reg: 0.003963
  l1.weight: grad_norm = 0.141766
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.105202
Total gradient norm: 0.310241
=== Actor Training Debug (Iteration 5050) ===
Q mean: -13.314595
Q std: 19.706127
Actor loss: 13.318573
Action reg: 0.003978
  l1.weight: grad_norm = 0.124453
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.104835
Total gradient norm: 0.346452
=== Actor Training Debug (Iteration 5051) ===
Q mean: -11.234598
Q std: 17.879448
Actor loss: 11.238572
Action reg: 0.003974
  l1.weight: grad_norm = 0.148142
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.115817
Total gradient norm: 0.346262
=== Actor Training Debug (Iteration 5052) ===
Q mean: -12.376356
Q std: 18.276226
Actor loss: 12.380327
Action reg: 0.003971
  l1.weight: grad_norm = 0.136661
  l1.bias: grad_norm = 0.000966
  l2.weight: grad_norm = 0.117964
Total gradient norm: 0.501042
=== Actor Training Debug (Iteration 5053) ===
Q mean: -15.986652
Q std: 20.297180
Actor loss: 15.990634
Action reg: 0.003982
  l1.weight: grad_norm = 0.077842
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.070448
Total gradient norm: 0.195474
=== Actor Training Debug (Iteration 5054) ===
Q mean: -13.122957
Q std: 18.452831
Actor loss: 13.126932
Action reg: 0.003975
  l1.weight: grad_norm = 0.199205
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.183608
Total gradient norm: 0.559683
=== Actor Training Debug (Iteration 5055) ===
Q mean: -12.607793
Q std: 17.981159
Actor loss: 12.611759
Action reg: 0.003966
  l1.weight: grad_norm = 0.107316
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.078020
Total gradient norm: 0.256145
=== Actor Training Debug (Iteration 5056) ===
Q mean: -14.822004
Q std: 19.674919
Actor loss: 14.825964
Action reg: 0.003959
  l1.weight: grad_norm = 0.139322
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.095414
Total gradient norm: 0.356820
=== Actor Training Debug (Iteration 5057) ===
Q mean: -12.069189
Q std: 19.105486
Actor loss: 12.073174
Action reg: 0.003984
  l1.weight: grad_norm = 0.124572
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.097049
Total gradient norm: 0.288977
=== Actor Training Debug (Iteration 5058) ===
Q mean: -14.135565
Q std: 20.290039
Actor loss: 14.139532
Action reg: 0.003967
  l1.weight: grad_norm = 0.163301
  l1.bias: grad_norm = 0.001383
  l2.weight: grad_norm = 0.131977
Total gradient norm: 0.426064
=== Actor Training Debug (Iteration 5059) ===
Q mean: -11.386180
Q std: 17.932680
Actor loss: 11.390158
Action reg: 0.003978
  l1.weight: grad_norm = 0.085392
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.061424
Total gradient norm: 0.175422
=== Actor Training Debug (Iteration 5060) ===
Q mean: -12.231817
Q std: 18.285763
Actor loss: 12.235784
Action reg: 0.003967
  l1.weight: grad_norm = 0.165491
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.126158
Total gradient norm: 0.494865
=== Actor Training Debug (Iteration 5061) ===
Q mean: -12.813665
Q std: 18.570518
Actor loss: 12.817637
Action reg: 0.003972
  l1.weight: grad_norm = 0.161503
  l1.bias: grad_norm = 0.001315
  l2.weight: grad_norm = 0.139858
Total gradient norm: 0.454191
=== Actor Training Debug (Iteration 5062) ===
Q mean: -14.873423
Q std: 20.541485
Actor loss: 14.877411
Action reg: 0.003989
  l1.weight: grad_norm = 0.087610
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.069651
Total gradient norm: 0.226070
=== Actor Training Debug (Iteration 5063) ===
Q mean: -12.353401
Q std: 19.106550
Actor loss: 12.357368
Action reg: 0.003966
  l1.weight: grad_norm = 0.190571
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.116018
Total gradient norm: 0.377751
=== Actor Training Debug (Iteration 5064) ===
Q mean: -14.625785
Q std: 20.537588
Actor loss: 14.629777
Action reg: 0.003992
  l1.weight: grad_norm = 0.106855
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.075388
Total gradient norm: 0.245411
=== Actor Training Debug (Iteration 5065) ===
Q mean: -12.455658
Q std: 18.200727
Actor loss: 12.459640
Action reg: 0.003981
  l1.weight: grad_norm = 0.218337
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.161838
Total gradient norm: 0.565534
=== Actor Training Debug (Iteration 5066) ===
Q mean: -14.452831
Q std: 19.278799
Actor loss: 14.456803
Action reg: 0.003972
  l1.weight: grad_norm = 0.148223
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.126078
Total gradient norm: 0.409358
=== Actor Training Debug (Iteration 5067) ===
Q mean: -13.539492
Q std: 19.808617
Actor loss: 13.543468
Action reg: 0.003976
  l1.weight: grad_norm = 0.084477
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.071723
Total gradient norm: 0.203643
=== Actor Training Debug (Iteration 5068) ===
Q mean: -12.939162
Q std: 19.745066
Actor loss: 12.943132
Action reg: 0.003970
  l1.weight: grad_norm = 0.107999
  l1.bias: grad_norm = 0.000858
  l2.weight: grad_norm = 0.081980
Total gradient norm: 0.247885
=== Actor Training Debug (Iteration 5069) ===
Q mean: -14.373164
Q std: 19.882559
Actor loss: 14.377153
Action reg: 0.003989
  l1.weight: grad_norm = 0.074781
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.053041
Total gradient norm: 0.176394
=== Actor Training Debug (Iteration 5070) ===
Q mean: -11.569414
Q std: 16.726135
Actor loss: 11.573391
Action reg: 0.003977
  l1.weight: grad_norm = 0.041304
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.031320
Total gradient norm: 0.105330
=== Actor Training Debug (Iteration 5071) ===
Q mean: -12.855708
Q std: 18.625984
Actor loss: 12.859694
Action reg: 0.003985
  l1.weight: grad_norm = 0.089976
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.092618
Total gradient norm: 0.253354
=== Actor Training Debug (Iteration 5072) ===
Q mean: -13.366319
Q std: 19.433262
Actor loss: 13.370278
Action reg: 0.003960
  l1.weight: grad_norm = 0.103068
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.075133
Total gradient norm: 0.265849
=== Actor Training Debug (Iteration 5073) ===
Q mean: -12.795924
Q std: 19.860415
Actor loss: 12.799893
Action reg: 0.003969
  l1.weight: grad_norm = 0.076357
  l1.bias: grad_norm = 0.001298
  l2.weight: grad_norm = 0.061743
Total gradient norm: 0.235574
=== Actor Training Debug (Iteration 5074) ===
Q mean: -13.096384
Q std: 19.468061
Actor loss: 13.100361
Action reg: 0.003977
  l1.weight: grad_norm = 0.189864
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.123663
Total gradient norm: 0.366184
=== Actor Training Debug (Iteration 5075) ===
Q mean: -15.675434
Q std: 20.789375
Actor loss: 15.679412
Action reg: 0.003977
  l1.weight: grad_norm = 0.119748
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.090554
Total gradient norm: 0.294154
=== Actor Training Debug (Iteration 5076) ===
Q mean: -15.582651
Q std: 20.299091
Actor loss: 15.586630
Action reg: 0.003979
  l1.weight: grad_norm = 0.123029
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.093820
Total gradient norm: 0.280497
=== Actor Training Debug (Iteration 5077) ===
Q mean: -12.422898
Q std: 18.337299
Actor loss: 12.426867
Action reg: 0.003968
  l1.weight: grad_norm = 0.119253
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.091958
Total gradient norm: 0.361972
=== Actor Training Debug (Iteration 5078) ===
Q mean: -12.835646
Q std: 18.472355
Actor loss: 12.839604
Action reg: 0.003959
  l1.weight: grad_norm = 0.177258
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.141173
Total gradient norm: 0.460671
=== Actor Training Debug (Iteration 5079) ===
Q mean: -13.957006
Q std: 20.475969
Actor loss: 13.960982
Action reg: 0.003976
  l1.weight: grad_norm = 0.133083
  l1.bias: grad_norm = 0.000746
  l2.weight: grad_norm = 0.120467
Total gradient norm: 0.407720
=== Actor Training Debug (Iteration 5080) ===
Q mean: -13.659680
Q std: 19.011143
Actor loss: 13.663664
Action reg: 0.003983
  l1.weight: grad_norm = 0.167027
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.132843
Total gradient norm: 0.367344
=== Actor Training Debug (Iteration 5081) ===
Q mean: -13.953871
Q std: 19.716003
Actor loss: 13.957853
Action reg: 0.003982
  l1.weight: grad_norm = 0.076838
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.072468
Total gradient norm: 0.249645
=== Actor Training Debug (Iteration 5082) ===
Q mean: -12.513734
Q std: 19.572124
Actor loss: 12.517715
Action reg: 0.003980
  l1.weight: grad_norm = 0.112529
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.086659
Total gradient norm: 0.218499
=== Actor Training Debug (Iteration 5083) ===
Q mean: -15.499946
Q std: 20.432688
Actor loss: 15.503920
Action reg: 0.003974
  l1.weight: grad_norm = 0.181933
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.154832
Total gradient norm: 0.404994
=== Actor Training Debug (Iteration 5084) ===
Q mean: -11.781722
Q std: 18.053272
Actor loss: 11.785694
Action reg: 0.003973
  l1.weight: grad_norm = 0.054653
  l1.bias: grad_norm = 0.000813
  l2.weight: grad_norm = 0.051889
Total gradient norm: 0.156767
=== Actor Training Debug (Iteration 5085) ===
Q mean: -12.295317
Q std: 18.279629
Actor loss: 12.299285
Action reg: 0.003968
  l1.weight: grad_norm = 0.098821
  l1.bias: grad_norm = 0.001922
  l2.weight: grad_norm = 0.077509
Total gradient norm: 0.229781
=== Actor Training Debug (Iteration 5086) ===
Q mean: -14.064354
Q std: 19.922033
Actor loss: 14.068329
Action reg: 0.003975
  l1.weight: grad_norm = 0.088888
  l1.bias: grad_norm = 0.002379
  l2.weight: grad_norm = 0.072462
Total gradient norm: 0.230315
=== Actor Training Debug (Iteration 5087) ===
Q mean: -12.434872
Q std: 18.635490
Actor loss: 12.438839
Action reg: 0.003968
  l1.weight: grad_norm = 0.148641
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.120242
Total gradient norm: 0.361012
=== Actor Training Debug (Iteration 5088) ===
Q mean: -13.476505
Q std: 19.395338
Actor loss: 13.480480
Action reg: 0.003975
  l1.weight: grad_norm = 0.090575
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.072599
Total gradient norm: 0.203628
=== Actor Training Debug (Iteration 5089) ===
Q mean: -12.278034
Q std: 18.642002
Actor loss: 12.282001
Action reg: 0.003967
  l1.weight: grad_norm = 0.170082
  l1.bias: grad_norm = 0.000613
  l2.weight: grad_norm = 0.136872
Total gradient norm: 0.485719
=== Actor Training Debug (Iteration 5090) ===
Q mean: -12.444205
Q std: 18.528156
Actor loss: 12.448197
Action reg: 0.003992
  l1.weight: grad_norm = 0.090160
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.068711
Total gradient norm: 0.195695
=== Actor Training Debug (Iteration 5091) ===
Q mean: -13.333405
Q std: 18.675987
Actor loss: 13.337388
Action reg: 0.003983
  l1.weight: grad_norm = 0.078489
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.068890
Total gradient norm: 0.189979
=== Actor Training Debug (Iteration 5092) ===
Q mean: -11.799487
Q std: 19.633757
Actor loss: 11.803467
Action reg: 0.003979
  l1.weight: grad_norm = 0.078721
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.059270
Total gradient norm: 0.200944
=== Actor Training Debug (Iteration 5093) ===
Q mean: -12.725536
Q std: 19.157856
Actor loss: 12.729513
Action reg: 0.003977
  l1.weight: grad_norm = 0.121187
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.098207
Total gradient norm: 0.261832
=== Actor Training Debug (Iteration 5094) ===
Q mean: -15.080716
Q std: 20.803625
Actor loss: 15.084696
Action reg: 0.003980
  l1.weight: grad_norm = 0.064352
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.055073
Total gradient norm: 0.237568
=== Actor Training Debug (Iteration 5095) ===
Q mean: -13.284344
Q std: 19.809061
Actor loss: 13.288320
Action reg: 0.003976
  l1.weight: grad_norm = 0.113418
  l1.bias: grad_norm = 0.001936
  l2.weight: grad_norm = 0.083679
Total gradient norm: 0.234976
=== Actor Training Debug (Iteration 5096) ===
Q mean: -13.898193
Q std: 20.347576
Actor loss: 13.902174
Action reg: 0.003980
  l1.weight: grad_norm = 0.289795
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.212729
Total gradient norm: 0.630751
=== Actor Training Debug (Iteration 5097) ===
Q mean: -11.848207
Q std: 19.273832
Actor loss: 11.852180
Action reg: 0.003972
  l1.weight: grad_norm = 0.153129
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.098860
Total gradient norm: 0.300498
=== Actor Training Debug (Iteration 5098) ===
Q mean: -14.216908
Q std: 20.424105
Actor loss: 14.220888
Action reg: 0.003980
  l1.weight: grad_norm = 0.066639
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.060430
Total gradient norm: 0.189090
=== Actor Training Debug (Iteration 5099) ===
Q mean: -15.027839
Q std: 20.630411
Actor loss: 15.031800
Action reg: 0.003962
  l1.weight: grad_norm = 0.064689
  l1.bias: grad_norm = 0.000594
  l2.weight: grad_norm = 0.054713
Total gradient norm: 0.160709
=== Actor Training Debug (Iteration 5100) ===
Q mean: -14.969820
Q std: 20.163349
Actor loss: 14.973801
Action reg: 0.003981
  l1.weight: grad_norm = 0.110614
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.090080
Total gradient norm: 0.255755
Episode 101: Steps=100, Reward=-274.350, Buffer_size=10100
=== Actor Training Debug (Iteration 5101) ===
Q mean: -14.566393
Q std: 19.463987
Actor loss: 14.570375
Action reg: 0.003983
  l1.weight: grad_norm = 0.092571
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.067918
Total gradient norm: 0.218276
=== Actor Training Debug (Iteration 5102) ===
Q mean: -12.624054
Q std: 19.482264
Actor loss: 12.628034
Action reg: 0.003980
  l1.weight: grad_norm = 0.113850
  l1.bias: grad_norm = 0.000679
  l2.weight: grad_norm = 0.081396
Total gradient norm: 0.222078
=== Actor Training Debug (Iteration 5103) ===
Q mean: -12.123950
Q std: 18.144861
Actor loss: 12.127899
Action reg: 0.003949
  l1.weight: grad_norm = 0.105742
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.081741
Total gradient norm: 0.318801
=== Actor Training Debug (Iteration 5104) ===
Q mean: -12.323130
Q std: 17.308062
Actor loss: 12.327112
Action reg: 0.003982
  l1.weight: grad_norm = 0.054772
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.039316
Total gradient norm: 0.115982
=== Actor Training Debug (Iteration 5105) ===
Q mean: -15.053988
Q std: 20.774900
Actor loss: 15.057964
Action reg: 0.003977
  l1.weight: grad_norm = 0.090512
  l1.bias: grad_norm = 0.000776
  l2.weight: grad_norm = 0.073065
Total gradient norm: 0.234923
=== Actor Training Debug (Iteration 5106) ===
Q mean: -13.204181
Q std: 18.792894
Actor loss: 13.208153
Action reg: 0.003972
  l1.weight: grad_norm = 0.099745
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.079789
Total gradient norm: 0.267197
=== Actor Training Debug (Iteration 5107) ===
Q mean: -13.344522
Q std: 18.644293
Actor loss: 13.348487
Action reg: 0.003965
  l1.weight: grad_norm = 0.128680
  l1.bias: grad_norm = 0.000710
  l2.weight: grad_norm = 0.100624
Total gradient norm: 0.305337
=== Actor Training Debug (Iteration 5108) ===
Q mean: -15.371901
Q std: 19.924713
Actor loss: 15.375882
Action reg: 0.003981
  l1.weight: grad_norm = 0.097897
  l1.bias: grad_norm = 0.003349
  l2.weight: grad_norm = 0.074777
Total gradient norm: 0.262567
=== Actor Training Debug (Iteration 5109) ===
Q mean: -13.050269
Q std: 18.524748
Actor loss: 13.054246
Action reg: 0.003977
  l1.weight: grad_norm = 0.118350
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.097383
Total gradient norm: 0.296912
=== Actor Training Debug (Iteration 5110) ===
Q mean: -14.111090
Q std: 19.831686
Actor loss: 14.115050
Action reg: 0.003961
  l1.weight: grad_norm = 0.204940
  l1.bias: grad_norm = 0.000689
  l2.weight: grad_norm = 0.176710
Total gradient norm: 0.563275
=== Actor Training Debug (Iteration 5111) ===
Q mean: -12.011293
Q std: 20.307196
Actor loss: 12.015275
Action reg: 0.003981
  l1.weight: grad_norm = 0.098977
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.077991
Total gradient norm: 0.260083
=== Actor Training Debug (Iteration 5112) ===
Q mean: -11.167751
Q std: 17.424629
Actor loss: 11.171735
Action reg: 0.003983
  l1.weight: grad_norm = 0.077529
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.056663
Total gradient norm: 0.210971
=== Actor Training Debug (Iteration 5113) ===
Q mean: -15.387250
Q std: 19.759565
Actor loss: 15.391231
Action reg: 0.003981
  l1.weight: grad_norm = 0.101676
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.076332
Total gradient norm: 0.235183
=== Actor Training Debug (Iteration 5114) ===
Q mean: -14.759126
Q std: 19.861507
Actor loss: 14.763115
Action reg: 0.003989
  l1.weight: grad_norm = 0.067272
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.045411
Total gradient norm: 0.164179
=== Actor Training Debug (Iteration 5115) ===
Q mean: -12.943102
Q std: 19.525768
Actor loss: 12.947079
Action reg: 0.003977
  l1.weight: grad_norm = 0.154354
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.121391
Total gradient norm: 0.490529
=== Actor Training Debug (Iteration 5116) ===
Q mean: -13.877975
Q std: 19.786024
Actor loss: 13.881941
Action reg: 0.003966
  l1.weight: grad_norm = 0.195256
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.135762
Total gradient norm: 0.415086
=== Actor Training Debug (Iteration 5117) ===
Q mean: -12.421110
Q std: 18.568514
Actor loss: 12.425078
Action reg: 0.003968
  l1.weight: grad_norm = 0.124934
  l1.bias: grad_norm = 0.002070
  l2.weight: grad_norm = 0.094930
Total gradient norm: 0.336091
=== Actor Training Debug (Iteration 5118) ===
Q mean: -13.252627
Q std: 19.347412
Actor loss: 13.256590
Action reg: 0.003963
  l1.weight: grad_norm = 0.138483
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.122264
Total gradient norm: 0.366435
=== Actor Training Debug (Iteration 5119) ===
Q mean: -14.517365
Q std: 20.380682
Actor loss: 14.521344
Action reg: 0.003979
  l1.weight: grad_norm = 0.180769
  l1.bias: grad_norm = 0.002038
  l2.weight: grad_norm = 0.154785
Total gradient norm: 0.483057
=== Actor Training Debug (Iteration 5120) ===
Q mean: -12.882319
Q std: 18.457241
Actor loss: 12.886288
Action reg: 0.003968
  l1.weight: grad_norm = 0.128772
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.106317
Total gradient norm: 0.343850
=== Actor Training Debug (Iteration 5121) ===
Q mean: -12.043781
Q std: 18.477961
Actor loss: 12.047762
Action reg: 0.003981
  l1.weight: grad_norm = 0.103535
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.084776
Total gradient norm: 0.283897
=== Actor Training Debug (Iteration 5122) ===
Q mean: -10.395912
Q std: 17.212664
Actor loss: 10.399886
Action reg: 0.003974
  l1.weight: grad_norm = 0.061525
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.046994
Total gradient norm: 0.155165
=== Actor Training Debug (Iteration 5123) ===
Q mean: -12.399241
Q std: 19.145033
Actor loss: 12.403223
Action reg: 0.003981
  l1.weight: grad_norm = 0.092669
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.061856
Total gradient norm: 0.186578
=== Actor Training Debug (Iteration 5124) ===
Q mean: -14.804115
Q std: 20.243696
Actor loss: 14.808097
Action reg: 0.003982
  l1.weight: grad_norm = 0.109283
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.089532
Total gradient norm: 0.407932
=== Actor Training Debug (Iteration 5125) ===
Q mean: -14.519699
Q std: 20.031488
Actor loss: 14.523675
Action reg: 0.003975
  l1.weight: grad_norm = 0.089155
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.067366
Total gradient norm: 0.254790
=== Actor Training Debug (Iteration 5126) ===
Q mean: -13.668528
Q std: 19.773592
Actor loss: 13.672497
Action reg: 0.003970
  l1.weight: grad_norm = 0.108087
  l1.bias: grad_norm = 0.000771
  l2.weight: grad_norm = 0.084573
Total gradient norm: 0.266749
=== Actor Training Debug (Iteration 5127) ===
Q mean: -12.714158
Q std: 17.931137
Actor loss: 12.718137
Action reg: 0.003979
  l1.weight: grad_norm = 0.145432
  l1.bias: grad_norm = 0.000792
  l2.weight: grad_norm = 0.129802
Total gradient norm: 0.495216
=== Actor Training Debug (Iteration 5128) ===
Q mean: -14.876273
Q std: 20.489849
Actor loss: 14.880248
Action reg: 0.003974
  l1.weight: grad_norm = 0.082603
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.061241
Total gradient norm: 0.194719
=== Actor Training Debug (Iteration 5129) ===
Q mean: -14.428042
Q std: 19.555126
Actor loss: 14.432006
Action reg: 0.003964
  l1.weight: grad_norm = 0.080321
  l1.bias: grad_norm = 0.001266
  l2.weight: grad_norm = 0.060186
Total gradient norm: 0.191801
=== Actor Training Debug (Iteration 5130) ===
Q mean: -13.754160
Q std: 19.520971
Actor loss: 13.758126
Action reg: 0.003967
  l1.weight: grad_norm = 0.180986
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.143253
Total gradient norm: 0.420170
=== Actor Training Debug (Iteration 5131) ===
Q mean: -11.249207
Q std: 19.032549
Actor loss: 11.253178
Action reg: 0.003971
  l1.weight: grad_norm = 0.228427
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.148613
Total gradient norm: 0.458733
=== Actor Training Debug (Iteration 5132) ===
Q mean: -13.498092
Q std: 19.252953
Actor loss: 13.502079
Action reg: 0.003987
  l1.weight: grad_norm = 0.082825
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.057195
Total gradient norm: 0.188516
=== Actor Training Debug (Iteration 5133) ===
Q mean: -12.603987
Q std: 19.485882
Actor loss: 12.607962
Action reg: 0.003975
  l1.weight: grad_norm = 0.112876
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.088827
Total gradient norm: 0.221718
=== Actor Training Debug (Iteration 5134) ===
Q mean: -13.840701
Q std: 18.792263
Actor loss: 13.844673
Action reg: 0.003972
  l1.weight: grad_norm = 0.139357
  l1.bias: grad_norm = 0.000688
  l2.weight: grad_norm = 0.105166
Total gradient norm: 0.405911
=== Actor Training Debug (Iteration 5135) ===
Q mean: -11.466276
Q std: 18.105520
Actor loss: 11.470254
Action reg: 0.003977
  l1.weight: grad_norm = 0.113522
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.095543
Total gradient norm: 0.274283
=== Actor Training Debug (Iteration 5136) ===
Q mean: -13.763699
Q std: 19.291201
Actor loss: 13.767682
Action reg: 0.003983
  l1.weight: grad_norm = 0.052034
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.043529
Total gradient norm: 0.135739
=== Actor Training Debug (Iteration 5137) ===
Q mean: -12.532169
Q std: 18.056929
Actor loss: 12.536157
Action reg: 0.003988
  l1.weight: grad_norm = 0.122878
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.103960
Total gradient norm: 0.262526
=== Actor Training Debug (Iteration 5138) ===
Q mean: -13.760691
Q std: 20.361622
Actor loss: 13.764666
Action reg: 0.003975
  l1.weight: grad_norm = 0.122390
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.104702
Total gradient norm: 0.411044
=== Actor Training Debug (Iteration 5139) ===
Q mean: -15.165602
Q std: 20.331150
Actor loss: 15.169577
Action reg: 0.003975
  l1.weight: grad_norm = 0.149910
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.111027
Total gradient norm: 0.365678
=== Actor Training Debug (Iteration 5140) ===
Q mean: -12.736767
Q std: 18.236979
Actor loss: 12.740736
Action reg: 0.003970
  l1.weight: grad_norm = 0.149563
  l1.bias: grad_norm = 0.000730
  l2.weight: grad_norm = 0.110226
Total gradient norm: 0.327578
=== Actor Training Debug (Iteration 5141) ===
Q mean: -11.732056
Q std: 19.099640
Actor loss: 11.736024
Action reg: 0.003968
  l1.weight: grad_norm = 0.119022
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.078738
Total gradient norm: 0.229246
=== Actor Training Debug (Iteration 5142) ===
Q mean: -14.533201
Q std: 21.024773
Actor loss: 14.537185
Action reg: 0.003984
  l1.weight: grad_norm = 0.092124
  l1.bias: grad_norm = 0.001334
  l2.weight: grad_norm = 0.074255
Total gradient norm: 0.262821
=== Actor Training Debug (Iteration 5143) ===
Q mean: -13.152716
Q std: 19.842920
Actor loss: 13.156691
Action reg: 0.003975
  l1.weight: grad_norm = 0.158089
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.099507
Total gradient norm: 0.322177
=== Actor Training Debug (Iteration 5144) ===
Q mean: -13.389479
Q std: 19.461905
Actor loss: 13.393456
Action reg: 0.003978
  l1.weight: grad_norm = 0.108328
  l1.bias: grad_norm = 0.001542
  l2.weight: grad_norm = 0.100069
Total gradient norm: 0.347160
=== Actor Training Debug (Iteration 5145) ===
Q mean: -11.556196
Q std: 19.528227
Actor loss: 11.560175
Action reg: 0.003979
  l1.weight: grad_norm = 0.203746
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.174696
Total gradient norm: 0.481059
=== Actor Training Debug (Iteration 5146) ===
Q mean: -14.606421
Q std: 20.378229
Actor loss: 14.610411
Action reg: 0.003990
  l1.weight: grad_norm = 0.063316
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.044050
Total gradient norm: 0.132967
=== Actor Training Debug (Iteration 5147) ===
Q mean: -13.720625
Q std: 19.387669
Actor loss: 13.724602
Action reg: 0.003977
  l1.weight: grad_norm = 0.219581
  l1.bias: grad_norm = 0.001617
  l2.weight: grad_norm = 0.157337
Total gradient norm: 0.463200
=== Actor Training Debug (Iteration 5148) ===
Q mean: -13.852050
Q std: 20.193405
Actor loss: 13.856028
Action reg: 0.003978
  l1.weight: grad_norm = 0.173798
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.116626
Total gradient norm: 0.344472
=== Actor Training Debug (Iteration 5149) ===
Q mean: -12.048841
Q std: 18.363459
Actor loss: 12.052820
Action reg: 0.003978
  l1.weight: grad_norm = 0.109735
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.090114
Total gradient norm: 0.249452
=== Actor Training Debug (Iteration 5150) ===
Q mean: -12.889954
Q std: 18.507637
Actor loss: 12.893924
Action reg: 0.003970
  l1.weight: grad_norm = 0.236352
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.213914
Total gradient norm: 0.652831
=== Actor Training Debug (Iteration 5151) ===
Q mean: -12.666648
Q std: 18.413002
Actor loss: 12.670616
Action reg: 0.003968
  l1.weight: grad_norm = 0.140132
  l1.bias: grad_norm = 0.000960
  l2.weight: grad_norm = 0.106937
Total gradient norm: 0.296749
=== Actor Training Debug (Iteration 5152) ===
Q mean: -13.393806
Q std: 20.162357
Actor loss: 13.397774
Action reg: 0.003967
  l1.weight: grad_norm = 0.142108
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.105161
Total gradient norm: 0.318584
=== Actor Training Debug (Iteration 5153) ===
Q mean: -12.556789
Q std: 19.170761
Actor loss: 12.560760
Action reg: 0.003970
  l1.weight: grad_norm = 0.137491
  l1.bias: grad_norm = 0.001000
  l2.weight: grad_norm = 0.098741
Total gradient norm: 0.316688
=== Actor Training Debug (Iteration 5154) ===
Q mean: -13.359406
Q std: 20.041790
Actor loss: 13.363387
Action reg: 0.003980
  l1.weight: grad_norm = 0.097474
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.081430
Total gradient norm: 0.251985
=== Actor Training Debug (Iteration 5155) ===
Q mean: -14.545989
Q std: 20.528870
Actor loss: 14.549971
Action reg: 0.003981
  l1.weight: grad_norm = 0.136823
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.108871
Total gradient norm: 0.402728
=== Actor Training Debug (Iteration 5156) ===
Q mean: -12.997009
Q std: 18.326839
Actor loss: 13.000983
Action reg: 0.003974
  l1.weight: grad_norm = 0.196920
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.161362
Total gradient norm: 0.568352
=== Actor Training Debug (Iteration 5157) ===
Q mean: -12.550607
Q std: 18.062998
Actor loss: 12.554593
Action reg: 0.003987
  l1.weight: grad_norm = 0.063132
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.039326
Total gradient norm: 0.111846
=== Actor Training Debug (Iteration 5158) ===
Q mean: -12.903899
Q std: 18.451723
Actor loss: 12.907887
Action reg: 0.003988
  l1.weight: grad_norm = 0.092290
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.082674
Total gradient norm: 0.264732
=== Actor Training Debug (Iteration 5159) ===
Q mean: -12.168293
Q std: 19.845726
Actor loss: 12.172266
Action reg: 0.003973
  l1.weight: grad_norm = 0.141798
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.108352
Total gradient norm: 0.364900
=== Actor Training Debug (Iteration 5160) ===
Q mean: -14.181164
Q std: 20.046820
Actor loss: 14.185147
Action reg: 0.003983
  l1.weight: grad_norm = 0.179622
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.098137
Total gradient norm: 0.311491
=== Actor Training Debug (Iteration 5161) ===
Q mean: -12.197310
Q std: 18.348978
Actor loss: 12.201278
Action reg: 0.003967
  l1.weight: grad_norm = 0.191210
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.138394
Total gradient norm: 0.433863
=== Actor Training Debug (Iteration 5162) ===
Q mean: -12.658014
Q std: 18.697992
Actor loss: 12.661989
Action reg: 0.003974
  l1.weight: grad_norm = 0.094003
  l1.bias: grad_norm = 0.003524
  l2.weight: grad_norm = 0.077992
Total gradient norm: 0.301863
=== Actor Training Debug (Iteration 5163) ===
Q mean: -11.745560
Q std: 18.223967
Actor loss: 11.749537
Action reg: 0.003977
  l1.weight: grad_norm = 0.080100
  l1.bias: grad_norm = 0.001447
  l2.weight: grad_norm = 0.072886
Total gradient norm: 0.247714
=== Actor Training Debug (Iteration 5164) ===
Q mean: -11.886344
Q std: 18.720076
Actor loss: 11.890326
Action reg: 0.003982
  l1.weight: grad_norm = 0.094634
  l1.bias: grad_norm = 0.002096
  l2.weight: grad_norm = 0.057519
Total gradient norm: 0.213696
=== Actor Training Debug (Iteration 5165) ===
Q mean: -10.811039
Q std: 16.935152
Actor loss: 10.815007
Action reg: 0.003969
  l1.weight: grad_norm = 0.069322
  l1.bias: grad_norm = 0.001373
  l2.weight: grad_norm = 0.054951
Total gradient norm: 0.198614
=== Actor Training Debug (Iteration 5166) ===
Q mean: -11.079349
Q std: 18.006422
Actor loss: 11.083320
Action reg: 0.003971
  l1.weight: grad_norm = 0.118635
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.088464
Total gradient norm: 0.278976
=== Actor Training Debug (Iteration 5167) ===
Q mean: -12.268082
Q std: 17.676775
Actor loss: 12.272072
Action reg: 0.003991
  l1.weight: grad_norm = 0.031192
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.024447
Total gradient norm: 0.080534
=== Actor Training Debug (Iteration 5168) ===
Q mean: -14.296700
Q std: 19.746878
Actor loss: 14.300684
Action reg: 0.003984
  l1.weight: grad_norm = 0.079355
  l1.bias: grad_norm = 0.001920
  l2.weight: grad_norm = 0.065354
Total gradient norm: 0.200828
=== Actor Training Debug (Iteration 5169) ===
Q mean: -14.096775
Q std: 20.708597
Actor loss: 14.100738
Action reg: 0.003963
  l1.weight: grad_norm = 0.138696
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.112305
Total gradient norm: 0.331805
=== Actor Training Debug (Iteration 5170) ===
Q mean: -12.582950
Q std: 19.661072
Actor loss: 12.586929
Action reg: 0.003980
  l1.weight: grad_norm = 0.038321
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.032957
Total gradient norm: 0.118763
=== Actor Training Debug (Iteration 5171) ===
Q mean: -13.349383
Q std: 19.866055
Actor loss: 13.353359
Action reg: 0.003975
  l1.weight: grad_norm = 0.085894
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.074245
Total gradient norm: 0.229390
=== Actor Training Debug (Iteration 5172) ===
Q mean: -10.816435
Q std: 17.479469
Actor loss: 10.820415
Action reg: 0.003980
  l1.weight: grad_norm = 0.075290
  l1.bias: grad_norm = 0.000533
  l2.weight: grad_norm = 0.059153
Total gradient norm: 0.207024
=== Actor Training Debug (Iteration 5173) ===
Q mean: -14.402233
Q std: 18.676382
Actor loss: 14.406219
Action reg: 0.003986
  l1.weight: grad_norm = 0.079433
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.066763
Total gradient norm: 0.188605
=== Actor Training Debug (Iteration 5174) ===
Q mean: -14.020404
Q std: 19.246187
Actor loss: 14.024387
Action reg: 0.003984
  l1.weight: grad_norm = 0.053752
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.041348
Total gradient norm: 0.123942
=== Actor Training Debug (Iteration 5175) ===
Q mean: -14.029022
Q std: 20.578676
Actor loss: 14.032989
Action reg: 0.003966
  l1.weight: grad_norm = 0.188421
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.142624
Total gradient norm: 0.490074
=== Actor Training Debug (Iteration 5176) ===
Q mean: -11.792109
Q std: 16.920658
Actor loss: 11.796066
Action reg: 0.003957
  l1.weight: grad_norm = 0.188015
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.133589
Total gradient norm: 0.425990
=== Actor Training Debug (Iteration 5177) ===
Q mean: -14.642319
Q std: 20.905884
Actor loss: 14.646303
Action reg: 0.003985
  l1.weight: grad_norm = 0.073058
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.050903
Total gradient norm: 0.166727
=== Actor Training Debug (Iteration 5178) ===
Q mean: -12.967580
Q std: 19.461830
Actor loss: 12.971536
Action reg: 0.003956
  l1.weight: grad_norm = 0.218205
  l1.bias: grad_norm = 0.000937
  l2.weight: grad_norm = 0.174550
Total gradient norm: 0.568319
=== Actor Training Debug (Iteration 5179) ===
Q mean: -13.550113
Q std: 18.970158
Actor loss: 13.554088
Action reg: 0.003975
  l1.weight: grad_norm = 0.072593
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.059034
Total gradient norm: 0.200144
=== Actor Training Debug (Iteration 5180) ===
Q mean: -14.081278
Q std: 19.476103
Actor loss: 14.085261
Action reg: 0.003984
  l1.weight: grad_norm = 0.125969
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.096799
Total gradient norm: 0.320075
=== Actor Training Debug (Iteration 5181) ===
Q mean: -13.322043
Q std: 19.903910
Actor loss: 13.326019
Action reg: 0.003976
  l1.weight: grad_norm = 0.124345
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.094567
Total gradient norm: 0.352407
=== Actor Training Debug (Iteration 5182) ===
Q mean: -11.732284
Q std: 18.493250
Actor loss: 11.736263
Action reg: 0.003979
  l1.weight: grad_norm = 0.116522
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.092535
Total gradient norm: 0.263290
=== Actor Training Debug (Iteration 5183) ===
Q mean: -12.335468
Q std: 18.541357
Actor loss: 12.339439
Action reg: 0.003971
  l1.weight: grad_norm = 0.071880
  l1.bias: grad_norm = 0.000835
  l2.weight: grad_norm = 0.053635
Total gradient norm: 0.175336
=== Actor Training Debug (Iteration 5184) ===
Q mean: -13.226892
Q std: 19.259634
Actor loss: 13.230864
Action reg: 0.003972
  l1.weight: grad_norm = 0.213890
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.155192
Total gradient norm: 0.439270
=== Actor Training Debug (Iteration 5185) ===
Q mean: -13.393085
Q std: 18.740849
Actor loss: 13.397061
Action reg: 0.003977
  l1.weight: grad_norm = 0.112537
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.080146
Total gradient norm: 0.237584
=== Actor Training Debug (Iteration 5186) ===
Q mean: -11.652683
Q std: 17.247646
Actor loss: 11.656655
Action reg: 0.003972
  l1.weight: grad_norm = 0.079324
  l1.bias: grad_norm = 0.001172
  l2.weight: grad_norm = 0.065775
Total gradient norm: 0.203407
=== Actor Training Debug (Iteration 5187) ===
Q mean: -12.681278
Q std: 19.200232
Actor loss: 12.685261
Action reg: 0.003983
  l1.weight: grad_norm = 0.069307
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.054996
Total gradient norm: 0.179785
=== Actor Training Debug (Iteration 5188) ===
Q mean: -13.304808
Q std: 19.663620
Actor loss: 13.308784
Action reg: 0.003977
  l1.weight: grad_norm = 0.135010
  l1.bias: grad_norm = 0.002552
  l2.weight: grad_norm = 0.105771
Total gradient norm: 0.335914
=== Actor Training Debug (Iteration 5189) ===
Q mean: -10.841532
Q std: 18.028822
Actor loss: 10.845516
Action reg: 0.003985
  l1.weight: grad_norm = 0.074246
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.055180
Total gradient norm: 0.158823
=== Actor Training Debug (Iteration 5190) ===
Q mean: -12.282684
Q std: 18.856737
Actor loss: 12.286654
Action reg: 0.003970
  l1.weight: grad_norm = 0.209794
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.186286
Total gradient norm: 0.683166
=== Actor Training Debug (Iteration 5191) ===
Q mean: -14.724613
Q std: 20.589859
Actor loss: 14.728596
Action reg: 0.003983
  l1.weight: grad_norm = 0.130940
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.098742
Total gradient norm: 0.300891
=== Actor Training Debug (Iteration 5192) ===
Q mean: -10.959023
Q std: 17.499004
Actor loss: 10.962993
Action reg: 0.003969
  l1.weight: grad_norm = 0.076815
  l1.bias: grad_norm = 0.000758
  l2.weight: grad_norm = 0.053832
Total gradient norm: 0.178478
=== Actor Training Debug (Iteration 5193) ===
Q mean: -11.697257
Q std: 18.940125
Actor loss: 11.701228
Action reg: 0.003971
  l1.weight: grad_norm = 0.103349
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.086091
Total gradient norm: 0.276470
=== Actor Training Debug (Iteration 5194) ===
Q mean: -12.846726
Q std: 18.863743
Actor loss: 12.850712
Action reg: 0.003986
  l1.weight: grad_norm = 0.234824
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.161881
Total gradient norm: 0.501621
=== Actor Training Debug (Iteration 5195) ===
Q mean: -14.422886
Q std: 20.400200
Actor loss: 14.426867
Action reg: 0.003982
  l1.weight: grad_norm = 0.081435
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.061585
Total gradient norm: 0.186431
=== Actor Training Debug (Iteration 5196) ===
Q mean: -15.537579
Q std: 20.649031
Actor loss: 15.541551
Action reg: 0.003972
  l1.weight: grad_norm = 0.129727
  l1.bias: grad_norm = 0.000732
  l2.weight: grad_norm = 0.100661
Total gradient norm: 0.282996
=== Actor Training Debug (Iteration 5197) ===
Q mean: -13.235394
Q std: 18.665310
Actor loss: 13.239382
Action reg: 0.003987
  l1.weight: grad_norm = 0.066601
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.046193
Total gradient norm: 0.150068
=== Actor Training Debug (Iteration 5198) ===
Q mean: -12.049942
Q std: 17.903574
Actor loss: 12.053916
Action reg: 0.003974
  l1.weight: grad_norm = 0.065578
  l1.bias: grad_norm = 0.000934
  l2.weight: grad_norm = 0.045692
Total gradient norm: 0.147151
=== Actor Training Debug (Iteration 5199) ===
Q mean: -12.175927
Q std: 17.165989
Actor loss: 12.179911
Action reg: 0.003984
  l1.weight: grad_norm = 0.064590
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.052989
Total gradient norm: 0.186833
=== Actor Training Debug (Iteration 5200) ===
Q mean: -13.567501
Q std: 20.488981
Actor loss: 13.571472
Action reg: 0.003971
  l1.weight: grad_norm = 0.139779
  l1.bias: grad_norm = 0.000510
  l2.weight: grad_norm = 0.092311
Total gradient norm: 0.283756
=== Actor Training Debug (Iteration 5201) ===
Q mean: -15.084639
Q std: 20.101856
Actor loss: 15.088609
Action reg: 0.003970
  l1.weight: grad_norm = 0.104045
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.073062
Total gradient norm: 0.253126
=== Actor Training Debug (Iteration 5202) ===
Q mean: -13.718039
Q std: 19.928801
Actor loss: 13.722006
Action reg: 0.003967
  l1.weight: grad_norm = 0.121271
  l1.bias: grad_norm = 0.000809
  l2.weight: grad_norm = 0.099915
Total gradient norm: 0.345408
=== Actor Training Debug (Iteration 5203) ===
Q mean: -12.846437
Q std: 20.164476
Actor loss: 12.850407
Action reg: 0.003970
  l1.weight: grad_norm = 0.123656
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.108305
Total gradient norm: 0.314400
=== Actor Training Debug (Iteration 5204) ===
Q mean: -13.548157
Q std: 19.104324
Actor loss: 13.552135
Action reg: 0.003978
  l1.weight: grad_norm = 0.159601
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.104377
Total gradient norm: 0.304998
=== Actor Training Debug (Iteration 5205) ===
Q mean: -12.034071
Q std: 17.371838
Actor loss: 12.038037
Action reg: 0.003966
  l1.weight: grad_norm = 0.227907
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.173420
Total gradient norm: 0.577668
=== Actor Training Debug (Iteration 5206) ===
Q mean: -14.697334
Q std: 20.614761
Actor loss: 14.701308
Action reg: 0.003974
  l1.weight: grad_norm = 0.041430
  l1.bias: grad_norm = 0.002838
  l2.weight: grad_norm = 0.042933
Total gradient norm: 0.120065
=== Actor Training Debug (Iteration 5207) ===
Q mean: -14.197708
Q std: 19.632441
Actor loss: 14.201676
Action reg: 0.003968
  l1.weight: grad_norm = 0.126756
  l1.bias: grad_norm = 0.000863
  l2.weight: grad_norm = 0.085285
Total gradient norm: 0.278416
=== Actor Training Debug (Iteration 5208) ===
Q mean: -14.096862
Q std: 20.409004
Actor loss: 14.100830
Action reg: 0.003968
  l1.weight: grad_norm = 0.220763
  l1.bias: grad_norm = 0.002094
  l2.weight: grad_norm = 0.151257
Total gradient norm: 0.466103
=== Actor Training Debug (Iteration 5209) ===
Q mean: -14.027589
Q std: 19.652128
Actor loss: 14.031564
Action reg: 0.003975
  l1.weight: grad_norm = 0.158075
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.109738
Total gradient norm: 0.385739
=== Actor Training Debug (Iteration 5210) ===
Q mean: -14.165484
Q std: 19.919641
Actor loss: 14.169457
Action reg: 0.003973
  l1.weight: grad_norm = 0.112230
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.099603
Total gradient norm: 0.251179
=== Actor Training Debug (Iteration 5211) ===
Q mean: -12.277096
Q std: 18.744133
Actor loss: 12.281070
Action reg: 0.003974
  l1.weight: grad_norm = 0.156112
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.144454
Total gradient norm: 0.418448
=== Actor Training Debug (Iteration 5212) ===
Q mean: -15.288014
Q std: 20.216848
Actor loss: 15.291982
Action reg: 0.003967
  l1.weight: grad_norm = 0.145224
  l1.bias: grad_norm = 0.000762
  l2.weight: grad_norm = 0.118730
Total gradient norm: 0.407260
=== Actor Training Debug (Iteration 5213) ===
Q mean: -13.767653
Q std: 19.486130
Actor loss: 13.771620
Action reg: 0.003967
  l1.weight: grad_norm = 0.147041
  l1.bias: grad_norm = 0.001085
  l2.weight: grad_norm = 0.111979
Total gradient norm: 0.360793
=== Actor Training Debug (Iteration 5214) ===
Q mean: -15.181010
Q std: 20.715656
Actor loss: 15.184993
Action reg: 0.003983
  l1.weight: grad_norm = 0.147181
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.122190
Total gradient norm: 0.451538
=== Actor Training Debug (Iteration 5215) ===
Q mean: -12.551403
Q std: 19.365660
Actor loss: 12.555378
Action reg: 0.003975
  l1.weight: grad_norm = 0.040293
  l1.bias: grad_norm = 0.000696
  l2.weight: grad_norm = 0.031552
Total gradient norm: 0.107114
=== Actor Training Debug (Iteration 5216) ===
Q mean: -12.304767
Q std: 18.888264
Actor loss: 12.308742
Action reg: 0.003975
  l1.weight: grad_norm = 0.166080
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.133370
Total gradient norm: 0.415471
=== Actor Training Debug (Iteration 5217) ===
Q mean: -14.731037
Q std: 20.166355
Actor loss: 14.735005
Action reg: 0.003968
  l1.weight: grad_norm = 0.177225
  l1.bias: grad_norm = 0.000872
  l2.weight: grad_norm = 0.157393
Total gradient norm: 0.462759
=== Actor Training Debug (Iteration 5218) ===
Q mean: -12.304726
Q std: 19.417650
Actor loss: 12.308706
Action reg: 0.003981
  l1.weight: grad_norm = 0.292504
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.177282
Total gradient norm: 0.469873
=== Actor Training Debug (Iteration 5219) ===
Q mean: -14.028061
Q std: 19.742928
Actor loss: 14.032046
Action reg: 0.003985
  l1.weight: grad_norm = 0.113851
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.106300
Total gradient norm: 0.295707
=== Actor Training Debug (Iteration 5220) ===
Q mean: -12.086348
Q std: 18.797358
Actor loss: 12.090327
Action reg: 0.003980
  l1.weight: grad_norm = 0.142795
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.130505
Total gradient norm: 0.396402
=== Actor Training Debug (Iteration 5221) ===
Q mean: -13.714323
Q std: 19.528334
Actor loss: 13.718316
Action reg: 0.003993
  l1.weight: grad_norm = 0.134278
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.135215
Total gradient norm: 0.444012
=== Actor Training Debug (Iteration 5222) ===
Q mean: -14.265055
Q std: 20.454634
Actor loss: 14.269037
Action reg: 0.003983
  l1.weight: grad_norm = 0.037240
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.033379
Total gradient norm: 0.094507
=== Actor Training Debug (Iteration 5223) ===
Q mean: -12.916756
Q std: 19.000332
Actor loss: 12.920735
Action reg: 0.003980
  l1.weight: grad_norm = 0.241648
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.169154
Total gradient norm: 0.506403
=== Actor Training Debug (Iteration 5224) ===
Q mean: -15.104197
Q std: 20.029228
Actor loss: 15.108175
Action reg: 0.003978
  l1.weight: grad_norm = 0.086521
  l1.bias: grad_norm = 0.002514
  l2.weight: grad_norm = 0.069480
Total gradient norm: 0.266271
=== Actor Training Debug (Iteration 5225) ===
Q mean: -13.284955
Q std: 19.306231
Actor loss: 13.288937
Action reg: 0.003982
  l1.weight: grad_norm = 0.102920
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.072343
Total gradient norm: 0.232653
=== Actor Training Debug (Iteration 5226) ===
Q mean: -13.084958
Q std: 19.210756
Actor loss: 13.088928
Action reg: 0.003971
  l1.weight: grad_norm = 0.179889
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.139323
Total gradient norm: 0.440585
=== Actor Training Debug (Iteration 5227) ===
Q mean: -13.832890
Q std: 19.424011
Actor loss: 13.836864
Action reg: 0.003974
  l1.weight: grad_norm = 0.093811
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.069084
Total gradient norm: 0.192365
=== Actor Training Debug (Iteration 5228) ===
Q mean: -11.950228
Q std: 19.497135
Actor loss: 11.954211
Action reg: 0.003984
  l1.weight: grad_norm = 0.136625
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.096770
Total gradient norm: 0.408801
=== Actor Training Debug (Iteration 5229) ===
Q mean: -13.117857
Q std: 19.755806
Actor loss: 13.121844
Action reg: 0.003987
  l1.weight: grad_norm = 0.287303
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.197810
Total gradient norm: 0.799435
=== Actor Training Debug (Iteration 5230) ===
Q mean: -15.070373
Q std: 19.328136
Actor loss: 15.074360
Action reg: 0.003987
  l1.weight: grad_norm = 0.075274
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.053767
Total gradient norm: 0.167237
=== Actor Training Debug (Iteration 5231) ===
Q mean: -12.439100
Q std: 19.672827
Actor loss: 12.443074
Action reg: 0.003974
  l1.weight: grad_norm = 0.218196
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.183187
Total gradient norm: 0.623577
=== Actor Training Debug (Iteration 5232) ===
Q mean: -12.049981
Q std: 19.000969
Actor loss: 12.053969
Action reg: 0.003988
  l1.weight: grad_norm = 0.164344
  l1.bias: grad_norm = 0.001555
  l2.weight: grad_norm = 0.123477
Total gradient norm: 0.411822
=== Actor Training Debug (Iteration 5233) ===
Q mean: -11.121498
Q std: 18.049442
Actor loss: 11.125482
Action reg: 0.003983
  l1.weight: grad_norm = 0.032553
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.027859
Total gradient norm: 0.080709
=== Actor Training Debug (Iteration 5234) ===
Q mean: -13.169547
Q std: 19.351154
Actor loss: 13.173533
Action reg: 0.003986
  l1.weight: grad_norm = 0.116040
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.094472
Total gradient norm: 0.262248
=== Actor Training Debug (Iteration 5235) ===
Q mean: -12.329620
Q std: 18.011415
Actor loss: 12.333587
Action reg: 0.003966
  l1.weight: grad_norm = 0.214717
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.145895
Total gradient norm: 0.418024
=== Actor Training Debug (Iteration 5236) ===
Q mean: -12.161658
Q std: 17.992500
Actor loss: 12.165629
Action reg: 0.003971
  l1.weight: grad_norm = 0.073595
  l1.bias: grad_norm = 0.001418
  l2.weight: grad_norm = 0.060804
Total gradient norm: 0.186427
=== Actor Training Debug (Iteration 5237) ===
Q mean: -14.368501
Q std: 20.375080
Actor loss: 14.372485
Action reg: 0.003984
  l1.weight: grad_norm = 0.093561
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.069770
Total gradient norm: 0.278689
=== Actor Training Debug (Iteration 5238) ===
Q mean: -15.213560
Q std: 20.466227
Actor loss: 15.217541
Action reg: 0.003980
  l1.weight: grad_norm = 0.150950
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.126886
Total gradient norm: 0.408874
=== Actor Training Debug (Iteration 5239) ===
Q mean: -13.969542
Q std: 20.016226
Actor loss: 13.973513
Action reg: 0.003971
  l1.weight: grad_norm = 0.127395
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.100266
Total gradient norm: 0.413600
=== Actor Training Debug (Iteration 5240) ===
Q mean: -13.861154
Q std: 19.391491
Actor loss: 13.865124
Action reg: 0.003970
  l1.weight: grad_norm = 0.075933
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.052467
Total gradient norm: 0.162460
=== Actor Training Debug (Iteration 5241) ===
Q mean: -13.960094
Q std: 19.826122
Actor loss: 13.964067
Action reg: 0.003973
  l1.weight: grad_norm = 0.051519
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.044937
Total gradient norm: 0.171253
=== Actor Training Debug (Iteration 5242) ===
Q mean: -11.940445
Q std: 19.086798
Actor loss: 11.944417
Action reg: 0.003972
  l1.weight: grad_norm = 0.104562
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.081618
Total gradient norm: 0.253915
=== Actor Training Debug (Iteration 5243) ===
Q mean: -16.274086
Q std: 21.970346
Actor loss: 16.278057
Action reg: 0.003971
  l1.weight: grad_norm = 0.118798
  l1.bias: grad_norm = 0.002609
  l2.weight: grad_norm = 0.092792
Total gradient norm: 0.283192
=== Actor Training Debug (Iteration 5244) ===
Q mean: -13.751705
Q std: 19.554533
Actor loss: 13.755680
Action reg: 0.003975
  l1.weight: grad_norm = 0.450502
  l1.bias: grad_norm = 0.002080
  l2.weight: grad_norm = 0.402170
Total gradient norm: 1.823900
=== Actor Training Debug (Iteration 5245) ===
Q mean: -12.048687
Q std: 18.653049
Actor loss: 12.052660
Action reg: 0.003973
  l1.weight: grad_norm = 0.094466
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.080018
Total gradient norm: 0.250666
=== Actor Training Debug (Iteration 5246) ===
Q mean: -14.248581
Q std: 19.052551
Actor loss: 14.252554
Action reg: 0.003973
  l1.weight: grad_norm = 0.124301
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.098457
Total gradient norm: 0.343795
=== Actor Training Debug (Iteration 5247) ===
Q mean: -14.042595
Q std: 19.339485
Actor loss: 14.046572
Action reg: 0.003977
  l1.weight: grad_norm = 0.114059
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.084576
Total gradient norm: 0.335694
=== Actor Training Debug (Iteration 5248) ===
Q mean: -13.163235
Q std: 19.592831
Actor loss: 13.167200
Action reg: 0.003965
  l1.weight: grad_norm = 0.165254
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.128082
Total gradient norm: 0.389355
=== Actor Training Debug (Iteration 5249) ===
Q mean: -14.021278
Q std: 18.689255
Actor loss: 14.025272
Action reg: 0.003994
  l1.weight: grad_norm = 0.062551
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.048683
Total gradient norm: 0.147324
=== Actor Training Debug (Iteration 5250) ===
Q mean: -12.181809
Q std: 18.778366
Actor loss: 12.185774
Action reg: 0.003965
  l1.weight: grad_norm = 0.088698
  l1.bias: grad_norm = 0.001254
  l2.weight: grad_norm = 0.068504
Total gradient norm: 0.231098
=== Actor Training Debug (Iteration 5251) ===
Q mean: -13.670487
Q std: 19.558256
Actor loss: 13.674477
Action reg: 0.003989
  l1.weight: grad_norm = 0.134196
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.113470
Total gradient norm: 0.337640
=== Actor Training Debug (Iteration 5252) ===
Q mean: -12.446507
Q std: 18.337160
Actor loss: 12.450487
Action reg: 0.003979
  l1.weight: grad_norm = 0.091453
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.066168
Total gradient norm: 0.221387
=== Actor Training Debug (Iteration 5253) ===
Q mean: -14.192072
Q std: 19.484581
Actor loss: 14.196054
Action reg: 0.003982
  l1.weight: grad_norm = 0.126709
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.103194
Total gradient norm: 0.338405
=== Actor Training Debug (Iteration 5254) ===
Q mean: -13.959793
Q std: 19.331938
Actor loss: 13.963765
Action reg: 0.003972
  l1.weight: grad_norm = 0.238958
  l1.bias: grad_norm = 0.000657
  l2.weight: grad_norm = 0.179675
Total gradient norm: 0.598889
=== Actor Training Debug (Iteration 5255) ===
Q mean: -10.962156
Q std: 18.053467
Actor loss: 10.966143
Action reg: 0.003987
  l1.weight: grad_norm = 0.154978
  l1.bias: grad_norm = 0.000662
  l2.weight: grad_norm = 0.112132
Total gradient norm: 0.363907
=== Actor Training Debug (Iteration 5256) ===
Q mean: -14.197765
Q std: 19.674545
Actor loss: 14.201751
Action reg: 0.003985
  l1.weight: grad_norm = 0.094954
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.082056
Total gradient norm: 0.223206
=== Actor Training Debug (Iteration 5257) ===
Q mean: -11.662259
Q std: 17.679850
Actor loss: 11.666241
Action reg: 0.003982
  l1.weight: grad_norm = 0.084320
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.070713
Total gradient norm: 0.218860
=== Actor Training Debug (Iteration 5258) ===
Q mean: -13.264845
Q std: 19.169334
Actor loss: 13.268825
Action reg: 0.003980
  l1.weight: grad_norm = 0.078129
  l1.bias: grad_norm = 0.001002
  l2.weight: grad_norm = 0.055888
Total gradient norm: 0.193775
=== Actor Training Debug (Iteration 5259) ===
Q mean: -13.671810
Q std: 19.072683
Actor loss: 13.675800
Action reg: 0.003990
  l1.weight: grad_norm = 0.071886
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.049181
Total gradient norm: 0.155637
=== Actor Training Debug (Iteration 5260) ===
Q mean: -13.984213
Q std: 19.875422
Actor loss: 13.988193
Action reg: 0.003980
  l1.weight: grad_norm = 0.072237
  l1.bias: grad_norm = 0.001099
  l2.weight: grad_norm = 0.065198
Total gradient norm: 0.219515
=== Actor Training Debug (Iteration 5261) ===
Q mean: -12.367655
Q std: 18.827162
Actor loss: 12.371632
Action reg: 0.003977
  l1.weight: grad_norm = 0.094091
  l1.bias: grad_norm = 0.000726
  l2.weight: grad_norm = 0.075697
Total gradient norm: 0.240298
=== Actor Training Debug (Iteration 5262) ===
Q mean: -13.624081
Q std: 19.189344
Actor loss: 13.628051
Action reg: 0.003970
  l1.weight: grad_norm = 0.098254
  l1.bias: grad_norm = 0.000780
  l2.weight: grad_norm = 0.067681
Total gradient norm: 0.233583
=== Actor Training Debug (Iteration 5263) ===
Q mean: -14.962057
Q std: 20.085987
Actor loss: 14.966037
Action reg: 0.003980
  l1.weight: grad_norm = 0.156070
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.106179
Total gradient norm: 0.340784
=== Actor Training Debug (Iteration 5264) ===
Q mean: -14.971976
Q std: 19.956966
Actor loss: 14.975945
Action reg: 0.003969
  l1.weight: grad_norm = 0.185036
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.151827
Total gradient norm: 0.548043
=== Actor Training Debug (Iteration 5265) ===
Q mean: -14.581671
Q std: 19.369362
Actor loss: 14.585651
Action reg: 0.003981
  l1.weight: grad_norm = 0.089360
  l1.bias: grad_norm = 0.000652
  l2.weight: grad_norm = 0.064380
Total gradient norm: 0.198322
=== Actor Training Debug (Iteration 5266) ===
Q mean: -14.135315
Q std: 19.446281
Actor loss: 14.139292
Action reg: 0.003977
  l1.weight: grad_norm = 0.135620
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.107727
Total gradient norm: 0.438610
=== Actor Training Debug (Iteration 5267) ===
Q mean: -13.270479
Q std: 19.810787
Actor loss: 13.274445
Action reg: 0.003966
  l1.weight: grad_norm = 0.159442
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.105904
Total gradient norm: 0.319460
=== Actor Training Debug (Iteration 5268) ===
Q mean: -12.261415
Q std: 18.650999
Actor loss: 12.265397
Action reg: 0.003982
  l1.weight: grad_norm = 0.091621
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.072723
Total gradient norm: 0.224388
=== Actor Training Debug (Iteration 5269) ===
Q mean: -12.520324
Q std: 19.323299
Actor loss: 12.524284
Action reg: 0.003960
  l1.weight: grad_norm = 0.091233
  l1.bias: grad_norm = 0.000604
  l2.weight: grad_norm = 0.067604
Total gradient norm: 0.225619
=== Actor Training Debug (Iteration 5270) ===
Q mean: -14.912345
Q std: 20.157736
Actor loss: 14.916326
Action reg: 0.003981
  l1.weight: grad_norm = 0.083892
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.068867
Total gradient norm: 0.216248
=== Actor Training Debug (Iteration 5271) ===
Q mean: -13.401046
Q std: 20.306158
Actor loss: 13.405015
Action reg: 0.003969
  l1.weight: grad_norm = 0.213436
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.148517
Total gradient norm: 0.455547
=== Actor Training Debug (Iteration 5272) ===
Q mean: -11.528189
Q std: 17.492249
Actor loss: 11.532174
Action reg: 0.003985
  l1.weight: grad_norm = 0.272160
  l1.bias: grad_norm = 0.002021
  l2.weight: grad_norm = 0.216028
Total gradient norm: 0.782864
=== Actor Training Debug (Iteration 5273) ===
Q mean: -11.531123
Q std: 17.883768
Actor loss: 11.535112
Action reg: 0.003989
  l1.weight: grad_norm = 0.056077
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.037879
Total gradient norm: 0.118362
=== Actor Training Debug (Iteration 5274) ===
Q mean: -14.847715
Q std: 19.784325
Actor loss: 14.851696
Action reg: 0.003981
  l1.weight: grad_norm = 0.094785
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.069475
Total gradient norm: 0.214734
=== Actor Training Debug (Iteration 5275) ===
Q mean: -12.365782
Q std: 17.760078
Actor loss: 12.369758
Action reg: 0.003976
  l1.weight: grad_norm = 0.124733
  l1.bias: grad_norm = 0.003500
  l2.weight: grad_norm = 0.090642
Total gradient norm: 0.301148
=== Actor Training Debug (Iteration 5276) ===
Q mean: -13.754379
Q std: 19.016703
Actor loss: 13.758363
Action reg: 0.003984
  l1.weight: grad_norm = 0.105986
  l1.bias: grad_norm = 0.002979
  l2.weight: grad_norm = 0.089077
Total gradient norm: 0.300096
=== Actor Training Debug (Iteration 5277) ===
Q mean: -13.386925
Q std: 19.954895
Actor loss: 13.390890
Action reg: 0.003966
  l1.weight: grad_norm = 0.135639
  l1.bias: grad_norm = 0.001730
  l2.weight: grad_norm = 0.101445
Total gradient norm: 0.349107
=== Actor Training Debug (Iteration 5278) ===
Q mean: -14.463121
Q std: 19.734159
Actor loss: 14.467111
Action reg: 0.003989
  l1.weight: grad_norm = 0.039940
  l1.bias: grad_norm = 0.002020
  l2.weight: grad_norm = 0.032516
Total gradient norm: 0.123286
=== Actor Training Debug (Iteration 5279) ===
Q mean: -13.016246
Q std: 19.907335
Actor loss: 13.020218
Action reg: 0.003972
  l1.weight: grad_norm = 0.133719
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.110807
Total gradient norm: 0.410266
=== Actor Training Debug (Iteration 5280) ===
Q mean: -13.504333
Q std: 19.380642
Actor loss: 13.508314
Action reg: 0.003982
  l1.weight: grad_norm = 0.090968
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.073085
Total gradient norm: 0.270040
=== Actor Training Debug (Iteration 5281) ===
Q mean: -12.714174
Q std: 19.597298
Actor loss: 12.718155
Action reg: 0.003981
  l1.weight: grad_norm = 0.105363
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.085651
Total gradient norm: 0.253262
=== Actor Training Debug (Iteration 5282) ===
Q mean: -13.912878
Q std: 19.340088
Actor loss: 13.916852
Action reg: 0.003974
  l1.weight: grad_norm = 0.115751
  l1.bias: grad_norm = 0.000645
  l2.weight: grad_norm = 0.093282
Total gradient norm: 0.296896
=== Actor Training Debug (Iteration 5283) ===
Q mean: -12.632774
Q std: 19.916836
Actor loss: 12.636758
Action reg: 0.003983
  l1.weight: grad_norm = 0.040591
  l1.bias: grad_norm = 0.001165
  l2.weight: grad_norm = 0.038144
Total gradient norm: 0.120374
=== Actor Training Debug (Iteration 5284) ===
Q mean: -14.997009
Q std: 19.828705
Actor loss: 15.000998
Action reg: 0.003990
  l1.weight: grad_norm = 0.059642
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.045362
Total gradient norm: 0.151821
=== Actor Training Debug (Iteration 5285) ===
Q mean: -15.185917
Q std: 19.658808
Actor loss: 15.189893
Action reg: 0.003976
  l1.weight: grad_norm = 0.056223
  l1.bias: grad_norm = 0.000633
  l2.weight: grad_norm = 0.044546
Total gradient norm: 0.143065
=== Actor Training Debug (Iteration 5286) ===
Q mean: -10.988665
Q std: 18.487301
Actor loss: 10.992628
Action reg: 0.003963
  l1.weight: grad_norm = 0.215937
  l1.bias: grad_norm = 0.000541
  l2.weight: grad_norm = 0.182090
Total gradient norm: 0.559921
=== Actor Training Debug (Iteration 5287) ===
Q mean: -13.059710
Q std: 17.844694
Actor loss: 13.063688
Action reg: 0.003979
  l1.weight: grad_norm = 0.071431
  l1.bias: grad_norm = 0.001071
  l2.weight: grad_norm = 0.057363
Total gradient norm: 0.178743
=== Actor Training Debug (Iteration 5288) ===
Q mean: -12.802445
Q std: 17.991751
Actor loss: 12.806427
Action reg: 0.003982
  l1.weight: grad_norm = 0.081656
  l1.bias: grad_norm = 0.000556
  l2.weight: grad_norm = 0.066213
Total gradient norm: 0.203870
=== Actor Training Debug (Iteration 5289) ===
Q mean: -14.293520
Q std: 19.394180
Actor loss: 14.297501
Action reg: 0.003981
  l1.weight: grad_norm = 0.246050
  l1.bias: grad_norm = 0.000706
  l2.weight: grad_norm = 0.153452
Total gradient norm: 0.421712
=== Actor Training Debug (Iteration 5290) ===
Q mean: -14.698719
Q std: 19.386866
Actor loss: 14.702693
Action reg: 0.003974
  l1.weight: grad_norm = 0.083671
  l1.bias: grad_norm = 0.000952
  l2.weight: grad_norm = 0.070796
Total gradient norm: 0.205276
=== Actor Training Debug (Iteration 5291) ===
Q mean: -12.060926
Q std: 18.188440
Actor loss: 12.064906
Action reg: 0.003980
  l1.weight: grad_norm = 0.103361
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.080229
Total gradient norm: 0.274417
=== Actor Training Debug (Iteration 5292) ===
Q mean: -14.045095
Q std: 20.166935
Actor loss: 14.049058
Action reg: 0.003962
  l1.weight: grad_norm = 0.078179
  l1.bias: grad_norm = 0.001069
  l2.weight: grad_norm = 0.067850
Total gradient norm: 0.237386
=== Actor Training Debug (Iteration 5293) ===
Q mean: -12.585997
Q std: 19.552168
Actor loss: 12.589973
Action reg: 0.003977
  l1.weight: grad_norm = 0.156346
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.118293
Total gradient norm: 0.432323
=== Actor Training Debug (Iteration 5294) ===
Q mean: -13.652894
Q std: 19.794100
Actor loss: 13.656868
Action reg: 0.003974
  l1.weight: grad_norm = 0.109245
  l1.bias: grad_norm = 0.000572
  l2.weight: grad_norm = 0.088154
Total gradient norm: 0.297123
=== Actor Training Debug (Iteration 5295) ===
Q mean: -11.173239
Q std: 18.626215
Actor loss: 11.177207
Action reg: 0.003968
  l1.weight: grad_norm = 0.143092
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.104121
Total gradient norm: 0.318847
=== Actor Training Debug (Iteration 5296) ===
Q mean: -12.267532
Q std: 19.497284
Actor loss: 12.271504
Action reg: 0.003972
  l1.weight: grad_norm = 0.144669
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.110199
Total gradient norm: 0.339349
=== Actor Training Debug (Iteration 5297) ===
Q mean: -12.233990
Q std: 18.064377
Actor loss: 12.237973
Action reg: 0.003984
  l1.weight: grad_norm = 0.125543
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.091758
Total gradient norm: 0.283227
=== Actor Training Debug (Iteration 5298) ===
Q mean: -12.577629
Q std: 19.266169
Actor loss: 12.581617
Action reg: 0.003988
  l1.weight: grad_norm = 0.092007
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.065666
Total gradient norm: 0.202453
=== Actor Training Debug (Iteration 5299) ===
Q mean: -11.171147
Q std: 18.241100
Actor loss: 11.175123
Action reg: 0.003976
  l1.weight: grad_norm = 0.089477
  l1.bias: grad_norm = 0.001680
  l2.weight: grad_norm = 0.066984
Total gradient norm: 0.223551
=== Actor Training Debug (Iteration 5300) ===
Q mean: -14.379050
Q std: 20.180876
Actor loss: 14.383028
Action reg: 0.003978
  l1.weight: grad_norm = 0.081946
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.058328
Total gradient norm: 0.211863
=== Actor Training Debug (Iteration 5301) ===
Q mean: -12.633679
Q std: 18.835493
Actor loss: 12.637654
Action reg: 0.003975
  l1.weight: grad_norm = 0.113092
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.083912
Total gradient norm: 0.285532
=== Actor Training Debug (Iteration 5302) ===
Q mean: -13.842140
Q std: 19.182392
Actor loss: 13.846114
Action reg: 0.003974
  l1.weight: grad_norm = 0.113536
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.103537
Total gradient norm: 0.301041
=== Actor Training Debug (Iteration 5303) ===
Q mean: -14.295866
Q std: 19.504297
Actor loss: 14.299830
Action reg: 0.003965
  l1.weight: grad_norm = 0.123710
  l1.bias: grad_norm = 0.002384
  l2.weight: grad_norm = 0.082857
Total gradient norm: 0.303431
=== Actor Training Debug (Iteration 5304) ===
Q mean: -14.122753
Q std: 19.817272
Actor loss: 14.126729
Action reg: 0.003976
  l1.weight: grad_norm = 0.145612
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.102729
Total gradient norm: 0.306959
=== Actor Training Debug (Iteration 5305) ===
Q mean: -13.933542
Q std: 20.630610
Actor loss: 13.937525
Action reg: 0.003983
  l1.weight: grad_norm = 0.180231
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.136875
Total gradient norm: 0.442359
=== Actor Training Debug (Iteration 5306) ===
Q mean: -14.490034
Q std: 19.477514
Actor loss: 14.494006
Action reg: 0.003972
  l1.weight: grad_norm = 0.060051
  l1.bias: grad_norm = 0.001075
  l2.weight: grad_norm = 0.045952
Total gradient norm: 0.173896
=== Actor Training Debug (Iteration 5307) ===
Q mean: -13.407871
Q std: 19.738651
Actor loss: 13.411853
Action reg: 0.003982
  l1.weight: grad_norm = 0.044610
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.040740
Total gradient norm: 0.166413
=== Actor Training Debug (Iteration 5308) ===
Q mean: -13.815088
Q std: 20.418951
Actor loss: 13.819070
Action reg: 0.003981
  l1.weight: grad_norm = 0.068429
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.054120
Total gradient norm: 0.183116
=== Actor Training Debug (Iteration 5309) ===
Q mean: -13.233036
Q std: 19.535555
Actor loss: 13.237022
Action reg: 0.003987
  l1.weight: grad_norm = 0.037725
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.032754
Total gradient norm: 0.097422
=== Actor Training Debug (Iteration 5310) ===
Q mean: -11.671873
Q std: 18.221848
Actor loss: 11.675824
Action reg: 0.003951
  l1.weight: grad_norm = 0.076538
  l1.bias: grad_norm = 0.002142
  l2.weight: grad_norm = 0.056846
Total gradient norm: 0.161631
=== Actor Training Debug (Iteration 5311) ===
Q mean: -13.038240
Q std: 19.227144
Actor loss: 13.042213
Action reg: 0.003973
  l1.weight: grad_norm = 0.339956
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.209281
Total gradient norm: 0.703676
=== Actor Training Debug (Iteration 5312) ===
Q mean: -13.798726
Q std: 19.254223
Actor loss: 13.802701
Action reg: 0.003975
  l1.weight: grad_norm = 0.044824
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.034084
Total gradient norm: 0.105933
=== Actor Training Debug (Iteration 5313) ===
Q mean: -14.142216
Q std: 20.149759
Actor loss: 14.146195
Action reg: 0.003980
  l1.weight: grad_norm = 0.159072
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.116697
Total gradient norm: 0.347356
=== Actor Training Debug (Iteration 5314) ===
Q mean: -14.174343
Q std: 19.803816
Actor loss: 14.178319
Action reg: 0.003975
  l1.weight: grad_norm = 0.155637
  l1.bias: grad_norm = 0.003018
  l2.weight: grad_norm = 0.099941
Total gradient norm: 0.371815
=== Actor Training Debug (Iteration 5315) ===
Q mean: -13.159052
Q std: 19.067934
Actor loss: 13.163034
Action reg: 0.003982
  l1.weight: grad_norm = 0.057150
  l1.bias: grad_norm = 0.000715
  l2.weight: grad_norm = 0.042140
Total gradient norm: 0.138830
=== Actor Training Debug (Iteration 5316) ===
Q mean: -15.702179
Q std: 21.129335
Actor loss: 15.706159
Action reg: 0.003979
  l1.weight: grad_norm = 0.130673
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.104143
Total gradient norm: 0.345442
=== Actor Training Debug (Iteration 5317) ===
Q mean: -13.496554
Q std: 18.782970
Actor loss: 13.500536
Action reg: 0.003982
  l1.weight: grad_norm = 0.078690
  l1.bias: grad_norm = 0.001800
  l2.weight: grad_norm = 0.063109
Total gradient norm: 0.259293
=== Actor Training Debug (Iteration 5318) ===
Q mean: -14.323466
Q std: 20.663729
Actor loss: 14.327447
Action reg: 0.003981
  l1.weight: grad_norm = 0.024692
  l1.bias: grad_norm = 0.000454
  l2.weight: grad_norm = 0.018814
Total gradient norm: 0.059985
=== Actor Training Debug (Iteration 5319) ===
Q mean: -13.645395
Q std: 19.426451
Actor loss: 13.649367
Action reg: 0.003972
  l1.weight: grad_norm = 0.152330
  l1.bias: grad_norm = 0.001755
  l2.weight: grad_norm = 0.116533
Total gradient norm: 0.417006
=== Actor Training Debug (Iteration 5320) ===
Q mean: -13.298113
Q std: 19.234491
Actor loss: 13.302087
Action reg: 0.003974
  l1.weight: grad_norm = 0.095346
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.076532
Total gradient norm: 0.245706
=== Actor Training Debug (Iteration 5321) ===
Q mean: -13.844864
Q std: 20.367077
Actor loss: 13.848845
Action reg: 0.003981
  l1.weight: grad_norm = 0.074467
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.061551
Total gradient norm: 0.200310
=== Actor Training Debug (Iteration 5322) ===
Q mean: -14.116636
Q std: 19.980724
Actor loss: 14.120616
Action reg: 0.003979
  l1.weight: grad_norm = 0.091380
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.076938
Total gradient norm: 0.325654
=== Actor Training Debug (Iteration 5323) ===
Q mean: -13.591066
Q std: 19.710360
Actor loss: 13.595022
Action reg: 0.003956
  l1.weight: grad_norm = 0.087716
  l1.bias: grad_norm = 0.001452
  l2.weight: grad_norm = 0.064127
Total gradient norm: 0.223382
=== Actor Training Debug (Iteration 5324) ===
Q mean: -11.695580
Q std: 18.125984
Actor loss: 11.699559
Action reg: 0.003979
  l1.weight: grad_norm = 0.332441
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.229098
Total gradient norm: 0.748823
=== Actor Training Debug (Iteration 5325) ===
Q mean: -13.058601
Q std: 19.213099
Actor loss: 13.062575
Action reg: 0.003974
  l1.weight: grad_norm = 0.141640
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.101900
Total gradient norm: 0.341786
=== Actor Training Debug (Iteration 5326) ===
Q mean: -14.111176
Q std: 20.342020
Actor loss: 14.115134
Action reg: 0.003958
  l1.weight: grad_norm = 0.169451
  l1.bias: grad_norm = 0.000655
  l2.weight: grad_norm = 0.127965
Total gradient norm: 0.396086
=== Actor Training Debug (Iteration 5327) ===
Q mean: -13.198407
Q std: 20.091671
Actor loss: 13.202388
Action reg: 0.003980
  l1.weight: grad_norm = 0.096791
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.078131
Total gradient norm: 0.213873
=== Actor Training Debug (Iteration 5328) ===
Q mean: -12.892399
Q std: 17.600441
Actor loss: 12.896358
Action reg: 0.003959
  l1.weight: grad_norm = 0.096312
  l1.bias: grad_norm = 0.000816
  l2.weight: grad_norm = 0.082222
Total gradient norm: 0.263452
=== Actor Training Debug (Iteration 5329) ===
Q mean: -13.643510
Q std: 19.022041
Actor loss: 13.647491
Action reg: 0.003981
  l1.weight: grad_norm = 0.131976
  l1.bias: grad_norm = 0.000461
  l2.weight: grad_norm = 0.093398
Total gradient norm: 0.290529
=== Actor Training Debug (Iteration 5330) ===
Q mean: -13.383116
Q std: 19.072109
Actor loss: 13.387100
Action reg: 0.003985
  l1.weight: grad_norm = 0.062063
  l1.bias: grad_norm = 0.000536
  l2.weight: grad_norm = 0.046377
Total gradient norm: 0.138636
=== Actor Training Debug (Iteration 5331) ===
Q mean: -11.067167
Q std: 16.475101
Actor loss: 11.071142
Action reg: 0.003975
  l1.weight: grad_norm = 0.070677
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.055524
Total gradient norm: 0.163394
=== Actor Training Debug (Iteration 5332) ===
Q mean: -12.607849
Q std: 19.200253
Actor loss: 12.611826
Action reg: 0.003977
  l1.weight: grad_norm = 0.177108
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.129968
Total gradient norm: 0.375033
=== Actor Training Debug (Iteration 5333) ===
Q mean: -15.362634
Q std: 20.464741
Actor loss: 15.366610
Action reg: 0.003976
  l1.weight: grad_norm = 0.050038
  l1.bias: grad_norm = 0.000649
  l2.weight: grad_norm = 0.037984
Total gradient norm: 0.143787
=== Actor Training Debug (Iteration 5334) ===
Q mean: -14.757594
Q std: 19.655399
Actor loss: 14.761555
Action reg: 0.003961
  l1.weight: grad_norm = 0.154378
  l1.bias: grad_norm = 0.000550
  l2.weight: grad_norm = 0.128292
Total gradient norm: 0.432432
=== Actor Training Debug (Iteration 5335) ===
Q mean: -12.863800
Q std: 19.321014
Actor loss: 12.867772
Action reg: 0.003972
  l1.weight: grad_norm = 0.172025
  l1.bias: grad_norm = 0.001644
  l2.weight: grad_norm = 0.115404
Total gradient norm: 0.370791
=== Actor Training Debug (Iteration 5336) ===
Q mean: -13.693676
Q std: 19.332327
Actor loss: 13.697656
Action reg: 0.003980
  l1.weight: grad_norm = 0.112868
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.102226
Total gradient norm: 0.337035
=== Actor Training Debug (Iteration 5337) ===
Q mean: -13.158442
Q std: 19.490749
Actor loss: 13.162415
Action reg: 0.003973
  l1.weight: grad_norm = 0.160945
  l1.bias: grad_norm = 0.000851
  l2.weight: grad_norm = 0.133057
Total gradient norm: 0.429977
=== Actor Training Debug (Iteration 5338) ===
Q mean: -14.484621
Q std: 19.909149
Actor loss: 14.488605
Action reg: 0.003983
  l1.weight: grad_norm = 0.055663
  l1.bias: grad_norm = 0.001264
  l2.weight: grad_norm = 0.039397
Total gradient norm: 0.137008
=== Actor Training Debug (Iteration 5339) ===
Q mean: -12.995681
Q std: 18.910366
Actor loss: 12.999653
Action reg: 0.003972
  l1.weight: grad_norm = 0.246990
  l1.bias: grad_norm = 0.000646
  l2.weight: grad_norm = 0.213979
Total gradient norm: 0.705487
=== Actor Training Debug (Iteration 5340) ===
Q mean: -13.409939
Q std: 19.371313
Actor loss: 13.413911
Action reg: 0.003972
  l1.weight: grad_norm = 0.181650
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.153441
Total gradient norm: 0.487616
=== Actor Training Debug (Iteration 5341) ===
Q mean: -12.587650
Q std: 19.862352
Actor loss: 12.591620
Action reg: 0.003970
  l1.weight: grad_norm = 0.215044
  l1.bias: grad_norm = 0.000651
  l2.weight: grad_norm = 0.166905
Total gradient norm: 0.513110
=== Actor Training Debug (Iteration 5342) ===
Q mean: -12.088329
Q std: 19.007999
Actor loss: 12.092293
Action reg: 0.003963
  l1.weight: grad_norm = 0.163527
  l1.bias: grad_norm = 0.001199
  l2.weight: grad_norm = 0.141225
Total gradient norm: 0.438817
=== Actor Training Debug (Iteration 5343) ===
Q mean: -14.885054
Q std: 20.525959
Actor loss: 14.889030
Action reg: 0.003976
  l1.weight: grad_norm = 0.272698
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.219921
Total gradient norm: 0.658343
=== Actor Training Debug (Iteration 5344) ===
Q mean: -12.733498
Q std: 20.093220
Actor loss: 12.737473
Action reg: 0.003976
  l1.weight: grad_norm = 0.167388
  l1.bias: grad_norm = 0.001836
  l2.weight: grad_norm = 0.127729
Total gradient norm: 0.423854
=== Actor Training Debug (Iteration 5345) ===
Q mean: -13.261630
Q std: 19.231371
Actor loss: 13.265598
Action reg: 0.003968
  l1.weight: grad_norm = 0.321124
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.202454
Total gradient norm: 0.570531
=== Actor Training Debug (Iteration 5346) ===
Q mean: -14.131562
Q std: 19.522873
Actor loss: 14.135529
Action reg: 0.003966
  l1.weight: grad_norm = 0.190174
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.159420
Total gradient norm: 0.530045
=== Actor Training Debug (Iteration 5347) ===
Q mean: -13.145897
Q std: 19.709312
Actor loss: 13.149873
Action reg: 0.003976
  l1.weight: grad_norm = 0.261940
  l1.bias: grad_norm = 0.001884
  l2.weight: grad_norm = 0.172686
Total gradient norm: 0.612581
=== Actor Training Debug (Iteration 5348) ===
Q mean: -14.036543
Q std: 19.804485
Actor loss: 14.040521
Action reg: 0.003977
  l1.weight: grad_norm = 0.143373
  l1.bias: grad_norm = 0.001586
  l2.weight: grad_norm = 0.102744
Total gradient norm: 0.287923
=== Actor Training Debug (Iteration 5349) ===
Q mean: -15.406200
Q std: 20.963543
Actor loss: 15.410151
Action reg: 0.003951
  l1.weight: grad_norm = 0.239523
  l1.bias: grad_norm = 0.001692
  l2.weight: grad_norm = 0.163955
Total gradient norm: 0.530859
=== Actor Training Debug (Iteration 5350) ===
Q mean: -11.267752
Q std: 19.442511
Actor loss: 11.271734
Action reg: 0.003982
  l1.weight: grad_norm = 0.076996
  l1.bias: grad_norm = 0.001874
  l2.weight: grad_norm = 0.059065
Total gradient norm: 0.175127
=== Actor Training Debug (Iteration 5351) ===
Q mean: -14.652884
Q std: 19.926502
Actor loss: 14.656853
Action reg: 0.003969
  l1.weight: grad_norm = 0.125726
  l1.bias: grad_norm = 0.002369
  l2.weight: grad_norm = 0.101722
Total gradient norm: 0.354740
=== Actor Training Debug (Iteration 5352) ===
Q mean: -12.561681
Q std: 18.427130
Actor loss: 12.565652
Action reg: 0.003971
  l1.weight: grad_norm = 0.212575
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.158761
Total gradient norm: 0.453337
=== Actor Training Debug (Iteration 5353) ===
Q mean: -10.425571
Q std: 16.908070
Actor loss: 10.429550
Action reg: 0.003979
  l1.weight: grad_norm = 0.289908
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.224115
Total gradient norm: 0.803501
=== Actor Training Debug (Iteration 5354) ===
Q mean: -13.411308
Q std: 20.385284
Actor loss: 13.415274
Action reg: 0.003965
  l1.weight: grad_norm = 0.083884
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.065284
Total gradient norm: 0.206575
=== Actor Training Debug (Iteration 5355) ===
Q mean: -13.409231
Q std: 19.041061
Actor loss: 13.413219
Action reg: 0.003988
  l1.weight: grad_norm = 0.057278
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.045995
Total gradient norm: 0.135079
=== Actor Training Debug (Iteration 5356) ===
Q mean: -12.881307
Q std: 19.076811
Actor loss: 12.885277
Action reg: 0.003970
  l1.weight: grad_norm = 0.150813
  l1.bias: grad_norm = 0.000578
  l2.weight: grad_norm = 0.125100
Total gradient norm: 0.378714
=== Actor Training Debug (Iteration 5357) ===
Q mean: -12.445326
Q std: 18.364992
Actor loss: 12.449302
Action reg: 0.003976
  l1.weight: grad_norm = 0.296089
  l1.bias: grad_norm = 0.000618
  l2.weight: grad_norm = 0.215747
Total gradient norm: 0.898926
=== Actor Training Debug (Iteration 5358) ===
Q mean: -15.589028
Q std: 20.174486
Actor loss: 15.593008
Action reg: 0.003979
  l1.weight: grad_norm = 0.131749
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.089301
Total gradient norm: 0.271723
=== Actor Training Debug (Iteration 5359) ===
Q mean: -12.472156
Q std: 19.115343
Actor loss: 12.476140
Action reg: 0.003984
  l1.weight: grad_norm = 0.077482
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.053299
Total gradient norm: 0.177440
=== Actor Training Debug (Iteration 5360) ===
Q mean: -13.221098
Q std: 19.067135
Actor loss: 13.225064
Action reg: 0.003966
  l1.weight: grad_norm = 0.181556
  l1.bias: grad_norm = 0.001061
  l2.weight: grad_norm = 0.127319
Total gradient norm: 0.366542
=== Actor Training Debug (Iteration 5361) ===
Q mean: -13.698408
Q std: 18.818794
Actor loss: 13.702384
Action reg: 0.003976
  l1.weight: grad_norm = 0.255914
  l1.bias: grad_norm = 0.000405
  l2.weight: grad_norm = 0.208851
Total gradient norm: 0.639964
=== Actor Training Debug (Iteration 5362) ===
Q mean: -10.995363
Q std: 17.546127
Actor loss: 10.999339
Action reg: 0.003976
  l1.weight: grad_norm = 0.111257
  l1.bias: grad_norm = 0.000708
  l2.weight: grad_norm = 0.090913
Total gradient norm: 0.256638
=== Actor Training Debug (Iteration 5363) ===
Q mean: -12.566183
Q std: 20.131441
Actor loss: 12.570148
Action reg: 0.003965
  l1.weight: grad_norm = 0.130456
  l1.bias: grad_norm = 0.001986
  l2.weight: grad_norm = 0.098013
Total gradient norm: 0.295042
=== Actor Training Debug (Iteration 5364) ===
Q mean: -15.056188
Q std: 19.817221
Actor loss: 15.060164
Action reg: 0.003976
  l1.weight: grad_norm = 0.042781
  l1.bias: grad_norm = 0.001421
  l2.weight: grad_norm = 0.031671
Total gradient norm: 0.099365
=== Actor Training Debug (Iteration 5365) ===
Q mean: -14.642300
Q std: 20.594959
Actor loss: 14.646266
Action reg: 0.003967
  l1.weight: grad_norm = 0.316247
  l1.bias: grad_norm = 0.000933
  l2.weight: grad_norm = 0.219228
Total gradient norm: 0.901757
=== Actor Training Debug (Iteration 5366) ===
Q mean: -12.204701
Q std: 19.827229
Actor loss: 12.208678
Action reg: 0.003977
  l1.weight: grad_norm = 0.061635
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.050346
Total gradient norm: 0.177586
=== Actor Training Debug (Iteration 5367) ===
Q mean: -12.423639
Q std: 18.993340
Actor loss: 12.427614
Action reg: 0.003975
  l1.weight: grad_norm = 0.091573
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.079427
Total gradient norm: 0.278904
=== Actor Training Debug (Iteration 5368) ===
Q mean: -13.539108
Q std: 19.561968
Actor loss: 13.543089
Action reg: 0.003980
  l1.weight: grad_norm = 0.109542
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.080519
Total gradient norm: 0.260152
=== Actor Training Debug (Iteration 5369) ===
Q mean: -12.590054
Q std: 19.100697
Actor loss: 12.594027
Action reg: 0.003973
  l1.weight: grad_norm = 0.183800
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.122293
Total gradient norm: 0.365805
=== Actor Training Debug (Iteration 5370) ===
Q mean: -11.609970
Q std: 17.759747
Actor loss: 11.613934
Action reg: 0.003963
  l1.weight: grad_norm = 0.121959
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.094272
Total gradient norm: 0.260467
=== Actor Training Debug (Iteration 5371) ===
Q mean: -15.638517
Q std: 19.861734
Actor loss: 15.642500
Action reg: 0.003982
  l1.weight: grad_norm = 0.113054
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.088145
Total gradient norm: 0.262884
=== Actor Training Debug (Iteration 5372) ===
Q mean: -12.788421
Q std: 19.225773
Actor loss: 12.792400
Action reg: 0.003980
  l1.weight: grad_norm = 0.114277
  l1.bias: grad_norm = 0.001045
  l2.weight: grad_norm = 0.085506
Total gradient norm: 0.302280
=== Actor Training Debug (Iteration 5373) ===
Q mean: -14.349069
Q std: 19.641518
Actor loss: 14.353046
Action reg: 0.003978
  l1.weight: grad_norm = 0.080528
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.062091
Total gradient norm: 0.177174
=== Actor Training Debug (Iteration 5374) ===
Q mean: -13.978885
Q std: 19.531160
Actor loss: 13.982862
Action reg: 0.003978
  l1.weight: grad_norm = 0.113510
  l1.bias: grad_norm = 0.002079
  l2.weight: grad_norm = 0.076389
Total gradient norm: 0.272240
=== Actor Training Debug (Iteration 5375) ===
Q mean: -13.139669
Q std: 18.126616
Actor loss: 13.143655
Action reg: 0.003986
  l1.weight: grad_norm = 0.099520
  l1.bias: grad_norm = 0.001527
  l2.weight: grad_norm = 0.075817
Total gradient norm: 0.232147
=== Actor Training Debug (Iteration 5376) ===
Q mean: -10.491978
Q std: 16.789024
Actor loss: 10.495966
Action reg: 0.003988
  l1.weight: grad_norm = 0.060230
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.047011
Total gradient norm: 0.144742
=== Actor Training Debug (Iteration 5377) ===
Q mean: -12.466457
Q std: 18.617167
Actor loss: 12.470437
Action reg: 0.003980
  l1.weight: grad_norm = 0.093094
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.065811
Total gradient norm: 0.203237
=== Actor Training Debug (Iteration 5378) ===
Q mean: -11.222774
Q std: 18.270796
Actor loss: 11.226739
Action reg: 0.003965
  l1.weight: grad_norm = 0.186090
  l1.bias: grad_norm = 0.002309
  l2.weight: grad_norm = 0.145700
Total gradient norm: 0.459219
=== Actor Training Debug (Iteration 5379) ===
Q mean: -13.188774
Q std: 18.929100
Actor loss: 13.192759
Action reg: 0.003985
  l1.weight: grad_norm = 0.148057
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.114587
Total gradient norm: 0.397534
=== Actor Training Debug (Iteration 5380) ===
Q mean: -14.669532
Q std: 19.862904
Actor loss: 14.673512
Action reg: 0.003980
  l1.weight: grad_norm = 0.127545
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.096245
Total gradient norm: 0.389105
=== Actor Training Debug (Iteration 5381) ===
Q mean: -13.001238
Q std: 20.457634
Actor loss: 13.005218
Action reg: 0.003980
  l1.weight: grad_norm = 0.067730
  l1.bias: grad_norm = 0.002781
  l2.weight: grad_norm = 0.056522
Total gradient norm: 0.177229
=== Actor Training Debug (Iteration 5382) ===
Q mean: -13.165238
Q std: 18.488556
Actor loss: 13.169217
Action reg: 0.003979
  l1.weight: grad_norm = 0.115860
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.098509
Total gradient norm: 0.272556
=== Actor Training Debug (Iteration 5383) ===
Q mean: -11.357052
Q std: 18.810299
Actor loss: 11.361015
Action reg: 0.003964
  l1.weight: grad_norm = 0.175654
  l1.bias: grad_norm = 0.002770
  l2.weight: grad_norm = 0.120415
Total gradient norm: 0.389313
=== Actor Training Debug (Iteration 5384) ===
Q mean: -13.371470
Q std: 19.207289
Actor loss: 13.375449
Action reg: 0.003979
  l1.weight: grad_norm = 0.127878
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.096986
Total gradient norm: 0.308791
=== Actor Training Debug (Iteration 5385) ===
Q mean: -12.754751
Q std: 20.051912
Actor loss: 12.758720
Action reg: 0.003969
  l1.weight: grad_norm = 0.133297
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.090402
Total gradient norm: 0.267447
=== Actor Training Debug (Iteration 5386) ===
Q mean: -12.850819
Q std: 19.657581
Actor loss: 12.854796
Action reg: 0.003977
  l1.weight: grad_norm = 0.154401
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.122079
Total gradient norm: 0.315293
=== Actor Training Debug (Iteration 5387) ===
Q mean: -15.287006
Q std: 19.435110
Actor loss: 15.290985
Action reg: 0.003979
  l1.weight: grad_norm = 0.134370
  l1.bias: grad_norm = 0.000652
  l2.weight: grad_norm = 0.106309
Total gradient norm: 0.356577
=== Actor Training Debug (Iteration 5388) ===
Q mean: -13.767307
Q std: 20.815166
Actor loss: 13.771288
Action reg: 0.003981
  l1.weight: grad_norm = 0.087222
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.073937
Total gradient norm: 0.219807
=== Actor Training Debug (Iteration 5389) ===
Q mean: -13.492773
Q std: 19.399084
Actor loss: 13.496744
Action reg: 0.003971
  l1.weight: grad_norm = 0.175856
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.124651
Total gradient norm: 0.385965
=== Actor Training Debug (Iteration 5390) ===
Q mean: -13.138819
Q std: 19.441505
Actor loss: 13.142797
Action reg: 0.003978
  l1.weight: grad_norm = 0.112769
  l1.bias: grad_norm = 0.000644
  l2.weight: grad_norm = 0.098281
Total gradient norm: 0.262076
=== Actor Training Debug (Iteration 5391) ===
Q mean: -12.339712
Q std: 19.421339
Actor loss: 12.343682
Action reg: 0.003970
  l1.weight: grad_norm = 0.156624
  l1.bias: grad_norm = 0.000715
  l2.weight: grad_norm = 0.129065
Total gradient norm: 0.393202
=== Actor Training Debug (Iteration 5392) ===
Q mean: -11.358442
Q std: 17.245390
Actor loss: 11.362423
Action reg: 0.003981
  l1.weight: grad_norm = 0.041867
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.031765
Total gradient norm: 0.083980
=== Actor Training Debug (Iteration 5393) ===
Q mean: -11.152645
Q std: 17.827641
Actor loss: 11.156618
Action reg: 0.003973
  l1.weight: grad_norm = 0.067223
  l1.bias: grad_norm = 0.001048
  l2.weight: grad_norm = 0.051304
Total gradient norm: 0.160243
=== Actor Training Debug (Iteration 5394) ===
Q mean: -13.138577
Q std: 19.548365
Actor loss: 13.142553
Action reg: 0.003977
  l1.weight: grad_norm = 0.226603
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.139582
Total gradient norm: 0.409674
=== Actor Training Debug (Iteration 5395) ===
Q mean: -13.805569
Q std: 17.926079
Actor loss: 13.809547
Action reg: 0.003979
  l1.weight: grad_norm = 0.141036
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.116910
Total gradient norm: 0.388150
=== Actor Training Debug (Iteration 5396) ===
Q mean: -12.357552
Q std: 20.200100
Actor loss: 12.361522
Action reg: 0.003970
  l1.weight: grad_norm = 0.206723
  l1.bias: grad_norm = 0.000903
  l2.weight: grad_norm = 0.155136
Total gradient norm: 0.430734
=== Actor Training Debug (Iteration 5397) ===
Q mean: -14.153183
Q std: 20.041227
Actor loss: 14.157166
Action reg: 0.003983
  l1.weight: grad_norm = 0.198252
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.126605
Total gradient norm: 0.401496
=== Actor Training Debug (Iteration 5398) ===
Q mean: -14.537598
Q std: 20.490450
Actor loss: 14.541551
Action reg: 0.003953
  l1.weight: grad_norm = 0.185400
  l1.bias: grad_norm = 0.000516
  l2.weight: grad_norm = 0.142835
Total gradient norm: 0.412477
=== Actor Training Debug (Iteration 5399) ===
Q mean: -14.918648
Q std: 20.395969
Actor loss: 14.922617
Action reg: 0.003969
  l1.weight: grad_norm = 0.137128
  l1.bias: grad_norm = 0.000498
  l2.weight: grad_norm = 0.102760
Total gradient norm: 0.356925
=== Actor Training Debug (Iteration 5400) ===
Q mean: -11.847967
Q std: 20.154215
Actor loss: 11.851930
Action reg: 0.003962
  l1.weight: grad_norm = 0.117682
  l1.bias: grad_norm = 0.001076
  l2.weight: grad_norm = 0.096264
Total gradient norm: 0.308722
=== Actor Training Debug (Iteration 5401) ===
Q mean: -11.260730
Q std: 18.267380
Actor loss: 11.264709
Action reg: 0.003979
  l1.weight: grad_norm = 0.148483
  l1.bias: grad_norm = 0.002341
  l2.weight: grad_norm = 0.106623
Total gradient norm: 0.380814
=== Actor Training Debug (Iteration 5402) ===
Q mean: -12.774071
Q std: 18.105993
Actor loss: 12.778048
Action reg: 0.003977
  l1.weight: grad_norm = 0.071178
  l1.bias: grad_norm = 0.000639
  l2.weight: grad_norm = 0.058755
Total gradient norm: 0.170043
=== Actor Training Debug (Iteration 5403) ===
Q mean: -14.461958
Q std: 19.803055
Actor loss: 14.465926
Action reg: 0.003968
  l1.weight: grad_norm = 0.112996
  l1.bias: grad_norm = 0.001901
  l2.weight: grad_norm = 0.088359
Total gradient norm: 0.285635
=== Actor Training Debug (Iteration 5404) ===
Q mean: -14.251276
Q std: 19.350916
Actor loss: 14.255243
Action reg: 0.003967
  l1.weight: grad_norm = 0.093064
  l1.bias: grad_norm = 0.002458
  l2.weight: grad_norm = 0.073432
Total gradient norm: 0.243644
=== Actor Training Debug (Iteration 5405) ===
Q mean: -14.198346
Q std: 19.694231
Actor loss: 14.202323
Action reg: 0.003977
  l1.weight: grad_norm = 0.204664
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.152563
Total gradient norm: 0.554083
=== Actor Training Debug (Iteration 5406) ===
Q mean: -15.943930
Q std: 21.734529
Actor loss: 15.947908
Action reg: 0.003978
  l1.weight: grad_norm = 0.205627
  l1.bias: grad_norm = 0.001549
  l2.weight: grad_norm = 0.154363
Total gradient norm: 0.471994
=== Actor Training Debug (Iteration 5407) ===
Q mean: -11.142186
Q std: 18.874077
Actor loss: 11.146164
Action reg: 0.003978
  l1.weight: grad_norm = 0.079790
  l1.bias: grad_norm = 0.002114
  l2.weight: grad_norm = 0.053840
Total gradient norm: 0.177819
=== Actor Training Debug (Iteration 5408) ===
Q mean: -13.888475
Q std: 21.062531
Actor loss: 13.892450
Action reg: 0.003975
  l1.weight: grad_norm = 0.053470
  l1.bias: grad_norm = 0.000635
  l2.weight: grad_norm = 0.041846
Total gradient norm: 0.138297
=== Actor Training Debug (Iteration 5409) ===
Q mean: -12.894067
Q std: 18.092583
Actor loss: 12.898035
Action reg: 0.003969
  l1.weight: grad_norm = 0.151476
  l1.bias: grad_norm = 0.001147
  l2.weight: grad_norm = 0.124209
Total gradient norm: 0.489493
=== Actor Training Debug (Iteration 5410) ===
Q mean: -14.312950
Q std: 19.681711
Actor loss: 14.316935
Action reg: 0.003985
  l1.weight: grad_norm = 0.120512
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.094620
Total gradient norm: 0.277478
=== Actor Training Debug (Iteration 5411) ===
Q mean: -12.474507
Q std: 19.478453
Actor loss: 12.478482
Action reg: 0.003975
  l1.weight: grad_norm = 0.154954
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.143349
Total gradient norm: 0.444515
=== Actor Training Debug (Iteration 5412) ===
Q mean: -15.235178
Q std: 19.693708
Actor loss: 15.239151
Action reg: 0.003973
  l1.weight: grad_norm = 0.111975
  l1.bias: grad_norm = 0.000804
  l2.weight: grad_norm = 0.090814
Total gradient norm: 0.272580
=== Actor Training Debug (Iteration 5413) ===
Q mean: -13.858989
Q std: 19.973692
Actor loss: 13.862972
Action reg: 0.003984
  l1.weight: grad_norm = 0.067140
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.051275
Total gradient norm: 0.142694
=== Actor Training Debug (Iteration 5414) ===
Q mean: -12.960335
Q std: 18.616005
Actor loss: 12.964315
Action reg: 0.003980
  l1.weight: grad_norm = 0.060813
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.048615
Total gradient norm: 0.153841
=== Actor Training Debug (Iteration 5415) ===
Q mean: -11.534178
Q std: 18.490412
Actor loss: 11.538139
Action reg: 0.003962
  l1.weight: grad_norm = 0.228968
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.169733
Total gradient norm: 0.579843
=== Actor Training Debug (Iteration 5416) ===
Q mean: -13.676988
Q std: 19.743412
Actor loss: 13.680947
Action reg: 0.003959
  l1.weight: grad_norm = 0.232900
  l1.bias: grad_norm = 0.000766
  l2.weight: grad_norm = 0.172683
Total gradient norm: 0.521049
=== Actor Training Debug (Iteration 5417) ===
Q mean: -12.183105
Q std: 18.857843
Actor loss: 12.187073
Action reg: 0.003967
  l1.weight: grad_norm = 0.167143
  l1.bias: grad_norm = 0.000863
  l2.weight: grad_norm = 0.125471
Total gradient norm: 0.412906
=== Actor Training Debug (Iteration 5418) ===
Q mean: -14.595928
Q std: 20.721708
Actor loss: 14.599904
Action reg: 0.003976
  l1.weight: grad_norm = 0.121359
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.089534
Total gradient norm: 0.241008
=== Actor Training Debug (Iteration 5419) ===
Q mean: -13.676926
Q std: 19.867393
Actor loss: 13.680909
Action reg: 0.003983
  l1.weight: grad_norm = 0.088631
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.065827
Total gradient norm: 0.227725
=== Actor Training Debug (Iteration 5420) ===
Q mean: -13.526033
Q std: 19.887943
Actor loss: 13.530009
Action reg: 0.003976
  l1.weight: grad_norm = 0.142534
  l1.bias: grad_norm = 0.000637
  l2.weight: grad_norm = 0.123462
Total gradient norm: 0.386325
=== Actor Training Debug (Iteration 5421) ===
Q mean: -12.681915
Q std: 19.689043
Actor loss: 12.685880
Action reg: 0.003965
  l1.weight: grad_norm = 0.265226
  l1.bias: grad_norm = 0.000521
  l2.weight: grad_norm = 0.160361
Total gradient norm: 0.521699
=== Actor Training Debug (Iteration 5422) ===
Q mean: -16.460373
Q std: 20.807987
Actor loss: 16.464359
Action reg: 0.003985
  l1.weight: grad_norm = 0.033604
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.027214
Total gradient norm: 0.089835
=== Actor Training Debug (Iteration 5423) ===
Q mean: -14.218146
Q std: 19.991510
Actor loss: 14.222133
Action reg: 0.003986
  l1.weight: grad_norm = 0.077413
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.057653
Total gradient norm: 0.181086
=== Actor Training Debug (Iteration 5424) ===
Q mean: -13.916922
Q std: 20.044460
Actor loss: 13.920861
Action reg: 0.003940
  l1.weight: grad_norm = 0.182244
  l1.bias: grad_norm = 0.001344
  l2.weight: grad_norm = 0.130350
Total gradient norm: 0.379890
=== Actor Training Debug (Iteration 5425) ===
Q mean: -13.213888
Q std: 19.491789
Actor loss: 13.217867
Action reg: 0.003978
  l1.weight: grad_norm = 0.066674
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.049615
Total gradient norm: 0.192472
=== Actor Training Debug (Iteration 5426) ===
Q mean: -13.902939
Q std: 19.278761
Actor loss: 13.906925
Action reg: 0.003986
  l1.weight: grad_norm = 0.057083
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.050657
Total gradient norm: 0.135995
=== Actor Training Debug (Iteration 5427) ===
Q mean: -13.272625
Q std: 19.319805
Actor loss: 13.276595
Action reg: 0.003970
  l1.weight: grad_norm = 0.247729
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.206934
Total gradient norm: 0.569666
=== Actor Training Debug (Iteration 5428) ===
Q mean: -14.001045
Q std: 20.138739
Actor loss: 14.005022
Action reg: 0.003977
  l1.weight: grad_norm = 0.177532
  l1.bias: grad_norm = 0.001478
  l2.weight: grad_norm = 0.119414
Total gradient norm: 0.353301
=== Actor Training Debug (Iteration 5429) ===
Q mean: -12.418207
Q std: 19.387196
Actor loss: 12.422176
Action reg: 0.003969
  l1.weight: grad_norm = 0.128788
  l1.bias: grad_norm = 0.001635
  l2.weight: grad_norm = 0.100928
Total gradient norm: 0.364446
=== Actor Training Debug (Iteration 5430) ===
Q mean: -16.910742
Q std: 21.481007
Actor loss: 16.914721
Action reg: 0.003979
  l1.weight: grad_norm = 0.278071
  l1.bias: grad_norm = 0.002043
  l2.weight: grad_norm = 0.176443
Total gradient norm: 0.615557
=== Actor Training Debug (Iteration 5431) ===
Q mean: -11.810002
Q std: 19.101061
Actor loss: 11.813969
Action reg: 0.003966
  l1.weight: grad_norm = 0.186098
  l1.bias: grad_norm = 0.001539
  l2.weight: grad_norm = 0.150521
Total gradient norm: 0.463088
=== Actor Training Debug (Iteration 5432) ===
Q mean: -13.016355
Q std: 18.746138
Actor loss: 13.020329
Action reg: 0.003975
  l1.weight: grad_norm = 0.211205
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.154361
Total gradient norm: 0.452678
=== Actor Training Debug (Iteration 5433) ===
Q mean: -12.237799
Q std: 20.080286
Actor loss: 12.241777
Action reg: 0.003979
  l1.weight: grad_norm = 0.081954
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.062117
Total gradient norm: 0.192457
=== Actor Training Debug (Iteration 5434) ===
Q mean: -14.394888
Q std: 19.611023
Actor loss: 14.398861
Action reg: 0.003973
  l1.weight: grad_norm = 0.194368
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.158303
Total gradient norm: 0.507010
=== Actor Training Debug (Iteration 5435) ===
Q mean: -13.189832
Q std: 19.266897
Actor loss: 13.193814
Action reg: 0.003982
  l1.weight: grad_norm = 0.168802
  l1.bias: grad_norm = 0.000633
  l2.weight: grad_norm = 0.122000
Total gradient norm: 0.376002
=== Actor Training Debug (Iteration 5436) ===
Q mean: -13.722638
Q std: 19.426771
Actor loss: 13.726620
Action reg: 0.003982
  l1.weight: grad_norm = 0.118290
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.078340
Total gradient norm: 0.243300
=== Actor Training Debug (Iteration 5437) ===
Q mean: -12.514360
Q std: 18.396996
Actor loss: 12.518338
Action reg: 0.003978
  l1.weight: grad_norm = 0.064444
  l1.bias: grad_norm = 0.000602
  l2.weight: grad_norm = 0.044337
Total gradient norm: 0.155510
=== Actor Training Debug (Iteration 5438) ===
Q mean: -12.343884
Q std: 18.554550
Actor loss: 12.347860
Action reg: 0.003976
  l1.weight: grad_norm = 0.153788
  l1.bias: grad_norm = 0.000781
  l2.weight: grad_norm = 0.120120
Total gradient norm: 0.415579
=== Actor Training Debug (Iteration 5439) ===
Q mean: -15.604331
Q std: 22.087137
Actor loss: 15.608306
Action reg: 0.003975
  l1.weight: grad_norm = 0.178186
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.141996
Total gradient norm: 0.437042
=== Actor Training Debug (Iteration 5440) ===
Q mean: -14.873478
Q std: 19.908140
Actor loss: 14.877448
Action reg: 0.003970
  l1.weight: grad_norm = 0.089350
  l1.bias: grad_norm = 0.004162
  l2.weight: grad_norm = 0.074305
Total gradient norm: 0.233285
=== Actor Training Debug (Iteration 5441) ===
Q mean: -15.643453
Q std: 20.814735
Actor loss: 15.647434
Action reg: 0.003982
  l1.weight: grad_norm = 0.100222
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.082800
Total gradient norm: 0.249030
=== Actor Training Debug (Iteration 5442) ===
Q mean: -12.985662
Q std: 19.592125
Actor loss: 12.989637
Action reg: 0.003975
  l1.weight: grad_norm = 0.232967
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.170230
Total gradient norm: 0.575106
=== Actor Training Debug (Iteration 5443) ===
Q mean: -11.287006
Q std: 17.084387
Actor loss: 11.290989
Action reg: 0.003983
  l1.weight: grad_norm = 0.133580
  l1.bias: grad_norm = 0.001422
  l2.weight: grad_norm = 0.105948
Total gradient norm: 0.272512
=== Actor Training Debug (Iteration 5444) ===
Q mean: -12.958813
Q std: 19.665873
Actor loss: 12.962780
Action reg: 0.003967
  l1.weight: grad_norm = 0.114260
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.079504
Total gradient norm: 0.255773
=== Actor Training Debug (Iteration 5445) ===
Q mean: -14.162037
Q std: 19.636162
Actor loss: 14.166014
Action reg: 0.003976
  l1.weight: grad_norm = 0.129770
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.096968
Total gradient norm: 0.388836
=== Actor Training Debug (Iteration 5446) ===
Q mean: -11.534285
Q std: 17.530342
Actor loss: 11.538244
Action reg: 0.003959
  l1.weight: grad_norm = 0.110661
  l1.bias: grad_norm = 0.000963
  l2.weight: grad_norm = 0.089543
Total gradient norm: 0.309569
=== Actor Training Debug (Iteration 5447) ===
Q mean: -13.386269
Q std: 19.602016
Actor loss: 13.390243
Action reg: 0.003974
  l1.weight: grad_norm = 0.149881
  l1.bias: grad_norm = 0.002523
  l2.weight: grad_norm = 0.115749
Total gradient norm: 0.294936
=== Actor Training Debug (Iteration 5448) ===
Q mean: -12.488973
Q std: 18.294455
Actor loss: 12.492937
Action reg: 0.003964
  l1.weight: grad_norm = 0.537795
  l1.bias: grad_norm = 0.002118
  l2.weight: grad_norm = 0.439317
Total gradient norm: 1.548083
=== Actor Training Debug (Iteration 5449) ===
Q mean: -14.080221
Q std: 20.311174
Actor loss: 14.084187
Action reg: 0.003965
  l1.weight: grad_norm = 0.273176
  l1.bias: grad_norm = 0.000708
  l2.weight: grad_norm = 0.249081
Total gradient norm: 0.824951
=== Actor Training Debug (Iteration 5450) ===
Q mean: -12.103315
Q std: 18.357962
Actor loss: 12.107296
Action reg: 0.003981
  l1.weight: grad_norm = 0.100900
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.081168
Total gradient norm: 0.248186
=== Actor Training Debug (Iteration 5451) ===
Q mean: -12.393716
Q std: 18.760592
Actor loss: 12.397684
Action reg: 0.003968
  l1.weight: grad_norm = 0.129469
  l1.bias: grad_norm = 0.000980
  l2.weight: grad_norm = 0.098401
Total gradient norm: 0.279747
=== Actor Training Debug (Iteration 5452) ===
Q mean: -13.410355
Q std: 19.925810
Actor loss: 13.414329
Action reg: 0.003974
  l1.weight: grad_norm = 0.124788
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.101647
Total gradient norm: 0.321927
=== Actor Training Debug (Iteration 5453) ===
Q mean: -13.708955
Q std: 19.611460
Actor loss: 13.712915
Action reg: 0.003961
  l1.weight: grad_norm = 0.215790
  l1.bias: grad_norm = 0.001547
  l2.weight: grad_norm = 0.169892
Total gradient norm: 0.490295
=== Actor Training Debug (Iteration 5454) ===
Q mean: -15.080561
Q std: 19.380161
Actor loss: 15.084529
Action reg: 0.003968
  l1.weight: grad_norm = 0.200375
  l1.bias: grad_norm = 0.001154
  l2.weight: grad_norm = 0.199938
Total gradient norm: 0.814758
=== Actor Training Debug (Iteration 5455) ===
Q mean: -13.288138
Q std: 19.281429
Actor loss: 13.292116
Action reg: 0.003978
  l1.weight: grad_norm = 0.119343
  l1.bias: grad_norm = 0.000691
  l2.weight: grad_norm = 0.087753
Total gradient norm: 0.267782
=== Actor Training Debug (Iteration 5456) ===
Q mean: -12.595131
Q std: 19.308338
Actor loss: 12.599102
Action reg: 0.003971
  l1.weight: grad_norm = 0.087072
  l1.bias: grad_norm = 0.001381
  l2.weight: grad_norm = 0.073663
Total gradient norm: 0.241053
=== Actor Training Debug (Iteration 5457) ===
Q mean: -13.532955
Q std: 19.278042
Actor loss: 13.536922
Action reg: 0.003967
  l1.weight: grad_norm = 0.143041
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.112349
Total gradient norm: 0.400154
=== Actor Training Debug (Iteration 5458) ===
Q mean: -14.952472
Q std: 20.941208
Actor loss: 14.956453
Action reg: 0.003982
  l1.weight: grad_norm = 0.199434
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.129609
Total gradient norm: 0.370669
=== Actor Training Debug (Iteration 5459) ===
Q mean: -11.114895
Q std: 17.422600
Actor loss: 11.118872
Action reg: 0.003976
  l1.weight: grad_norm = 0.097893
  l1.bias: grad_norm = 0.000654
  l2.weight: grad_norm = 0.085319
Total gradient norm: 0.265700
=== Actor Training Debug (Iteration 5460) ===
Q mean: -13.537123
Q std: 19.234219
Actor loss: 13.541095
Action reg: 0.003972
  l1.weight: grad_norm = 0.217859
  l1.bias: grad_norm = 0.000544
  l2.weight: grad_norm = 0.200874
Total gradient norm: 0.830913
=== Actor Training Debug (Iteration 5461) ===
Q mean: -11.518725
Q std: 18.564625
Actor loss: 11.522707
Action reg: 0.003982
  l1.weight: grad_norm = 0.138795
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.114826
Total gradient norm: 0.364776
=== Actor Training Debug (Iteration 5462) ===
Q mean: -13.168156
Q std: 19.951889
Actor loss: 13.172124
Action reg: 0.003969
  l1.weight: grad_norm = 0.133919
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.103925
Total gradient norm: 0.273484
=== Actor Training Debug (Iteration 5463) ===
Q mean: -14.209009
Q std: 19.972828
Actor loss: 14.212990
Action reg: 0.003980
  l1.weight: grad_norm = 0.431285
  l1.bias: grad_norm = 0.000763
  l2.weight: grad_norm = 0.368513
Total gradient norm: 1.494461
=== Actor Training Debug (Iteration 5464) ===
Q mean: -13.261163
Q std: 20.040590
Actor loss: 13.265141
Action reg: 0.003978
  l1.weight: grad_norm = 0.246624
  l1.bias: grad_norm = 0.000835
  l2.weight: grad_norm = 0.185393
Total gradient norm: 0.657813
=== Actor Training Debug (Iteration 5465) ===
Q mean: -13.496334
Q std: 19.072983
Actor loss: 13.500308
Action reg: 0.003974
  l1.weight: grad_norm = 0.154314
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.134133
Total gradient norm: 0.408687
=== Actor Training Debug (Iteration 5466) ===
Q mean: -12.838781
Q std: 18.574318
Actor loss: 12.842742
Action reg: 0.003961
  l1.weight: grad_norm = 0.313580
  l1.bias: grad_norm = 0.001345
  l2.weight: grad_norm = 0.188427
Total gradient norm: 0.536052
=== Actor Training Debug (Iteration 5467) ===
Q mean: -14.495455
Q std: 19.782795
Actor loss: 14.499420
Action reg: 0.003965
  l1.weight: grad_norm = 0.107599
  l1.bias: grad_norm = 0.002065
  l2.weight: grad_norm = 0.076172
Total gradient norm: 0.249109
=== Actor Training Debug (Iteration 5468) ===
Q mean: -14.880339
Q std: 20.629288
Actor loss: 14.884315
Action reg: 0.003976
  l1.weight: grad_norm = 0.183351
  l1.bias: grad_norm = 0.000971
  l2.weight: grad_norm = 0.132250
Total gradient norm: 0.438522
=== Actor Training Debug (Iteration 5469) ===
Q mean: -12.556709
Q std: 18.923584
Actor loss: 12.560685
Action reg: 0.003976
  l1.weight: grad_norm = 0.182727
  l1.bias: grad_norm = 0.002784
  l2.weight: grad_norm = 0.137544
Total gradient norm: 0.410452
=== Actor Training Debug (Iteration 5470) ===
Q mean: -13.333792
Q std: 19.370455
Actor loss: 13.337772
Action reg: 0.003981
  l1.weight: grad_norm = 0.152962
  l1.bias: grad_norm = 0.002590
  l2.weight: grad_norm = 0.101413
Total gradient norm: 0.325327
=== Actor Training Debug (Iteration 5471) ===
Q mean: -11.431179
Q std: 18.327637
Actor loss: 11.435157
Action reg: 0.003978
  l1.weight: grad_norm = 0.147350
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.123964
Total gradient norm: 0.371780
=== Actor Training Debug (Iteration 5472) ===
Q mean: -11.815001
Q std: 17.903872
Actor loss: 11.818985
Action reg: 0.003985
  l1.weight: grad_norm = 0.125955
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.079572
Total gradient norm: 0.246377
=== Actor Training Debug (Iteration 5473) ===
Q mean: -12.900839
Q std: 18.863400
Actor loss: 12.904811
Action reg: 0.003972
  l1.weight: grad_norm = 0.130787
  l1.bias: grad_norm = 0.002356
  l2.weight: grad_norm = 0.115408
Total gradient norm: 0.388117
=== Actor Training Debug (Iteration 5474) ===
Q mean: -13.156410
Q std: 20.102348
Actor loss: 13.160380
Action reg: 0.003970
  l1.weight: grad_norm = 0.112646
  l1.bias: grad_norm = 0.001746
  l2.weight: grad_norm = 0.100820
Total gradient norm: 0.355977
=== Actor Training Debug (Iteration 5475) ===
Q mean: -13.935391
Q std: 19.852806
Actor loss: 13.939374
Action reg: 0.003983
  l1.weight: grad_norm = 0.100545
  l1.bias: grad_norm = 0.000913
  l2.weight: grad_norm = 0.078799
Total gradient norm: 0.267289
=== Actor Training Debug (Iteration 5476) ===
Q mean: -13.596689
Q std: 19.944551
Actor loss: 13.600672
Action reg: 0.003983
  l1.weight: grad_norm = 0.116478
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.085420
Total gradient norm: 0.238966
=== Actor Training Debug (Iteration 5477) ===
Q mean: -13.118713
Q std: 19.254629
Actor loss: 13.122692
Action reg: 0.003979
  l1.weight: grad_norm = 0.218184
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.153108
Total gradient norm: 0.497875
=== Actor Training Debug (Iteration 5478) ===
Q mean: -12.743555
Q std: 20.120749
Actor loss: 12.747528
Action reg: 0.003973
  l1.weight: grad_norm = 0.116773
  l1.bias: grad_norm = 0.000763
  l2.weight: grad_norm = 0.096939
Total gradient norm: 0.316733
=== Actor Training Debug (Iteration 5479) ===
Q mean: -13.881992
Q std: 20.555401
Actor loss: 13.885976
Action reg: 0.003983
  l1.weight: grad_norm = 0.143003
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.113847
Total gradient norm: 0.373562
=== Actor Training Debug (Iteration 5480) ===
Q mean: -16.594908
Q std: 22.436775
Actor loss: 16.598886
Action reg: 0.003978
  l1.weight: grad_norm = 0.078402
  l1.bias: grad_norm = 0.003301
  l2.weight: grad_norm = 0.072173
Total gradient norm: 0.258894
=== Actor Training Debug (Iteration 5481) ===
Q mean: -11.785507
Q std: 18.404144
Actor loss: 11.789490
Action reg: 0.003982
  l1.weight: grad_norm = 0.049925
  l1.bias: grad_norm = 0.001020
  l2.weight: grad_norm = 0.040653
Total gradient norm: 0.129494
=== Actor Training Debug (Iteration 5482) ===
Q mean: -14.119609
Q std: 19.740135
Actor loss: 14.123589
Action reg: 0.003980
  l1.weight: grad_norm = 0.104978
  l1.bias: grad_norm = 0.001565
  l2.weight: grad_norm = 0.085028
Total gradient norm: 0.344856
=== Actor Training Debug (Iteration 5483) ===
Q mean: -14.998707
Q std: 19.940899
Actor loss: 15.002682
Action reg: 0.003975
  l1.weight: grad_norm = 0.182540
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.144843
Total gradient norm: 0.412327
=== Actor Training Debug (Iteration 5484) ===
Q mean: -12.875134
Q std: 18.139828
Actor loss: 12.879105
Action reg: 0.003971
  l1.weight: grad_norm = 0.202471
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.149921
Total gradient norm: 0.626685
=== Actor Training Debug (Iteration 5485) ===
Q mean: -14.581600
Q std: 19.956396
Actor loss: 14.585567
Action reg: 0.003966
  l1.weight: grad_norm = 0.158062
  l1.bias: grad_norm = 0.000895
  l2.weight: grad_norm = 0.129278
Total gradient norm: 0.367765
=== Actor Training Debug (Iteration 5486) ===
Q mean: -13.328068
Q std: 18.926804
Actor loss: 13.332044
Action reg: 0.003976
  l1.weight: grad_norm = 0.202754
  l1.bias: grad_norm = 0.000646
  l2.weight: grad_norm = 0.150389
Total gradient norm: 0.444607
=== Actor Training Debug (Iteration 5487) ===
Q mean: -13.447195
Q std: 19.818493
Actor loss: 13.451175
Action reg: 0.003980
  l1.weight: grad_norm = 0.081208
  l1.bias: grad_norm = 0.000758
  l2.weight: grad_norm = 0.067703
Total gradient norm: 0.209469
=== Actor Training Debug (Iteration 5488) ===
Q mean: -12.387239
Q std: 19.506508
Actor loss: 12.391219
Action reg: 0.003980
  l1.weight: grad_norm = 0.085433
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.063588
Total gradient norm: 0.223744
=== Actor Training Debug (Iteration 5489) ===
Q mean: -12.763012
Q std: 19.006563
Actor loss: 12.766976
Action reg: 0.003964
  l1.weight: grad_norm = 0.130794
  l1.bias: grad_norm = 0.002842
  l2.weight: grad_norm = 0.103766
Total gradient norm: 0.297906
=== Actor Training Debug (Iteration 5490) ===
Q mean: -16.057531
Q std: 21.722885
Actor loss: 16.061514
Action reg: 0.003982
  l1.weight: grad_norm = 0.095711
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.078828
Total gradient norm: 0.286440
=== Actor Training Debug (Iteration 5491) ===
Q mean: -14.077006
Q std: 21.740475
Actor loss: 14.080992
Action reg: 0.003985
  l1.weight: grad_norm = 0.124013
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.090021
Total gradient norm: 0.297038
=== Actor Training Debug (Iteration 5492) ===
Q mean: -13.598869
Q std: 19.472563
Actor loss: 13.602840
Action reg: 0.003971
  l1.weight: grad_norm = 0.177427
  l1.bias: grad_norm = 0.000919
  l2.weight: grad_norm = 0.141254
Total gradient norm: 0.403722
=== Actor Training Debug (Iteration 5493) ===
Q mean: -13.299922
Q std: 19.716957
Actor loss: 13.303903
Action reg: 0.003981
  l1.weight: grad_norm = 0.114304
  l1.bias: grad_norm = 0.000536
  l2.weight: grad_norm = 0.089582
Total gradient norm: 0.276406
=== Actor Training Debug (Iteration 5494) ===
Q mean: -13.796808
Q std: 19.650471
Actor loss: 13.800774
Action reg: 0.003966
  l1.weight: grad_norm = 0.211047
  l1.bias: grad_norm = 0.000900
  l2.weight: grad_norm = 0.141292
Total gradient norm: 0.464960
=== Actor Training Debug (Iteration 5495) ===
Q mean: -13.245518
Q std: 18.974119
Actor loss: 13.249496
Action reg: 0.003978
  l1.weight: grad_norm = 0.107135
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.084280
Total gradient norm: 0.310479
=== Actor Training Debug (Iteration 5496) ===
Q mean: -14.011001
Q std: 20.649679
Actor loss: 14.014973
Action reg: 0.003972
  l1.weight: grad_norm = 0.136557
  l1.bias: grad_norm = 0.000668
  l2.weight: grad_norm = 0.109011
Total gradient norm: 0.312397
=== Actor Training Debug (Iteration 5497) ===
Q mean: -13.184455
Q std: 19.750326
Actor loss: 13.188430
Action reg: 0.003975
  l1.weight: grad_norm = 0.107121
  l1.bias: grad_norm = 0.000582
  l2.weight: grad_norm = 0.089994
Total gradient norm: 0.332303
=== Actor Training Debug (Iteration 5498) ===
Q mean: -15.290502
Q std: 20.597404
Actor loss: 15.294477
Action reg: 0.003976
  l1.weight: grad_norm = 0.089006
  l1.bias: grad_norm = 0.001192
  l2.weight: grad_norm = 0.074488
Total gradient norm: 0.226780
=== Actor Training Debug (Iteration 5499) ===
Q mean: -14.568131
Q std: 20.784834
Actor loss: 14.572106
Action reg: 0.003975
  l1.weight: grad_norm = 0.165458
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.127925
Total gradient norm: 0.456209
=== Actor Training Debug (Iteration 5500) ===
Q mean: -11.735486
Q std: 18.891022
Actor loss: 11.739459
Action reg: 0.003973
  l1.weight: grad_norm = 0.105159
  l1.bias: grad_norm = 0.000844
  l2.weight: grad_norm = 0.086612
Total gradient norm: 0.260111
  Average reward: -315.698 | Average length: 100.0
Evaluation at episode 105: -315.698
=== Actor Training Debug (Iteration 5501) ===
Q mean: -14.984013
Q std: 19.903069
Actor loss: 14.987992
Action reg: 0.003980
  l1.weight: grad_norm = 0.148351
  l1.bias: grad_norm = 0.001548
  l2.weight: grad_norm = 0.120140
Total gradient norm: 0.382140
=== Actor Training Debug (Iteration 5502) ===
Q mean: -13.286776
Q std: 19.063337
Actor loss: 13.290751
Action reg: 0.003976
  l1.weight: grad_norm = 0.112323
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.106595
Total gradient norm: 0.334397
=== Actor Training Debug (Iteration 5503) ===
Q mean: -13.420584
Q std: 19.452049
Actor loss: 13.424560
Action reg: 0.003976
  l1.weight: grad_norm = 0.073278
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.057343
Total gradient norm: 0.190328
=== Actor Training Debug (Iteration 5504) ===
Q mean: -13.097000
Q std: 19.462765
Actor loss: 13.100981
Action reg: 0.003981
  l1.weight: grad_norm = 0.136976
  l1.bias: grad_norm = 0.002090
  l2.weight: grad_norm = 0.106693
Total gradient norm: 0.333360
=== Actor Training Debug (Iteration 5505) ===
Q mean: -14.903214
Q std: 20.513641
Actor loss: 14.907186
Action reg: 0.003971
  l1.weight: grad_norm = 0.218344
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.147795
Total gradient norm: 0.475149
=== Actor Training Debug (Iteration 5506) ===
Q mean: -14.159191
Q std: 20.151186
Actor loss: 14.163171
Action reg: 0.003980
  l1.weight: grad_norm = 0.051858
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.050678
Total gradient norm: 0.147920
=== Actor Training Debug (Iteration 5507) ===
Q mean: -15.653547
Q std: 20.293411
Actor loss: 15.657530
Action reg: 0.003982
  l1.weight: grad_norm = 0.233921
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.168564
Total gradient norm: 0.487622
=== Actor Training Debug (Iteration 5508) ===
Q mean: -15.434674
Q std: 20.289770
Actor loss: 15.438661
Action reg: 0.003986
  l1.weight: grad_norm = 0.095077
  l1.bias: grad_norm = 0.001867
  l2.weight: grad_norm = 0.074363
Total gradient norm: 0.239880
=== Actor Training Debug (Iteration 5509) ===
Q mean: -13.371153
Q std: 18.450480
Actor loss: 13.375121
Action reg: 0.003968
  l1.weight: grad_norm = 0.118138
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.081722
Total gradient norm: 0.248613
=== Actor Training Debug (Iteration 5510) ===
Q mean: -14.750717
Q std: 20.209843
Actor loss: 14.754690
Action reg: 0.003973
  l1.weight: grad_norm = 0.180465
  l1.bias: grad_norm = 0.002254
  l2.weight: grad_norm = 0.136186
Total gradient norm: 0.407827
=== Actor Training Debug (Iteration 5511) ===
Q mean: -15.444021
Q std: 20.769253
Actor loss: 15.447991
Action reg: 0.003970
  l1.weight: grad_norm = 0.146713
  l1.bias: grad_norm = 0.003099
  l2.weight: grad_norm = 0.103492
Total gradient norm: 0.326956
=== Actor Training Debug (Iteration 5512) ===
Q mean: -15.340221
Q std: 20.219093
Actor loss: 15.344195
Action reg: 0.003974
  l1.weight: grad_norm = 0.101304
  l1.bias: grad_norm = 0.001753
  l2.weight: grad_norm = 0.087002
Total gradient norm: 0.269831
=== Actor Training Debug (Iteration 5513) ===
Q mean: -14.171316
Q std: 19.542578
Actor loss: 14.175293
Action reg: 0.003977
  l1.weight: grad_norm = 0.124511
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.087309
Total gradient norm: 0.254883
=== Actor Training Debug (Iteration 5514) ===
Q mean: -13.103754
Q std: 18.864223
Actor loss: 13.107717
Action reg: 0.003963
  l1.weight: grad_norm = 0.260666
  l1.bias: grad_norm = 0.001426
  l2.weight: grad_norm = 0.209736
Total gradient norm: 0.642246
=== Actor Training Debug (Iteration 5515) ===
Q mean: -10.929829
Q std: 18.324228
Actor loss: 10.933805
Action reg: 0.003976
  l1.weight: grad_norm = 0.216133
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.170920
Total gradient norm: 0.513702
=== Actor Training Debug (Iteration 5516) ===
Q mean: -13.800133
Q std: 20.297047
Actor loss: 13.804099
Action reg: 0.003967
  l1.weight: grad_norm = 0.072074
  l1.bias: grad_norm = 0.001061
  l2.weight: grad_norm = 0.055347
Total gradient norm: 0.176215
=== Actor Training Debug (Iteration 5517) ===
Q mean: -11.752831
Q std: 18.249132
Actor loss: 11.756814
Action reg: 0.003982
  l1.weight: grad_norm = 0.105086
  l1.bias: grad_norm = 0.000650
  l2.weight: grad_norm = 0.085367
Total gradient norm: 0.248479
=== Actor Training Debug (Iteration 5518) ===
Q mean: -14.045799
Q std: 19.300844
Actor loss: 14.049767
Action reg: 0.003967
  l1.weight: grad_norm = 0.147747
  l1.bias: grad_norm = 0.001174
  l2.weight: grad_norm = 0.109705
Total gradient norm: 0.328768
=== Actor Training Debug (Iteration 5519) ===
Q mean: -11.138325
Q std: 18.864061
Actor loss: 11.142305
Action reg: 0.003981
  l1.weight: grad_norm = 0.096186
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.085056
Total gradient norm: 0.264201
=== Actor Training Debug (Iteration 5520) ===
Q mean: -13.490665
Q std: 20.202810
Actor loss: 13.494640
Action reg: 0.003975
  l1.weight: grad_norm = 0.225540
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.142750
Total gradient norm: 0.468778
=== Actor Training Debug (Iteration 5521) ===
Q mean: -11.699070
Q std: 19.313692
Actor loss: 11.703040
Action reg: 0.003970
  l1.weight: grad_norm = 0.133685
  l1.bias: grad_norm = 0.001298
  l2.weight: grad_norm = 0.104454
Total gradient norm: 0.320634
=== Actor Training Debug (Iteration 5522) ===
Q mean: -13.715334
Q std: 20.688429
Actor loss: 13.719317
Action reg: 0.003984
  l1.weight: grad_norm = 0.131735
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.098255
Total gradient norm: 0.276803
=== Actor Training Debug (Iteration 5523) ===
Q mean: -15.524348
Q std: 22.088186
Actor loss: 15.528318
Action reg: 0.003970
  l1.weight: grad_norm = 0.167646
  l1.bias: grad_norm = 0.001429
  l2.weight: grad_norm = 0.126309
Total gradient norm: 0.382858
=== Actor Training Debug (Iteration 5524) ===
Q mean: -12.605361
Q std: 18.909948
Actor loss: 12.609332
Action reg: 0.003971
  l1.weight: grad_norm = 0.090476
  l1.bias: grad_norm = 0.000846
  l2.weight: grad_norm = 0.074465
Total gradient norm: 0.225692
=== Actor Training Debug (Iteration 5525) ===
Q mean: -15.737680
Q std: 21.117327
Actor loss: 15.741658
Action reg: 0.003977
  l1.weight: grad_norm = 0.089296
  l1.bias: grad_norm = 0.001008
  l2.weight: grad_norm = 0.071409
Total gradient norm: 0.225209
=== Actor Training Debug (Iteration 5526) ===
Q mean: -15.357979
Q std: 20.556305
Actor loss: 15.361950
Action reg: 0.003971
  l1.weight: grad_norm = 0.167101
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.137968
Total gradient norm: 0.447597
=== Actor Training Debug (Iteration 5527) ===
Q mean: -14.240440
Q std: 18.818169
Actor loss: 14.244415
Action reg: 0.003975
  l1.weight: grad_norm = 0.127927
  l1.bias: grad_norm = 0.000874
  l2.weight: grad_norm = 0.093419
Total gradient norm: 0.340106
=== Actor Training Debug (Iteration 5528) ===
Q mean: -14.494491
Q std: 20.015097
Actor loss: 14.498450
Action reg: 0.003959
  l1.weight: grad_norm = 0.062834
  l1.bias: grad_norm = 0.001165
  l2.weight: grad_norm = 0.052507
Total gradient norm: 0.174736
=== Actor Training Debug (Iteration 5529) ===
Q mean: -13.368998
Q std: 20.898350
Actor loss: 13.372972
Action reg: 0.003975
  l1.weight: grad_norm = 0.176111
  l1.bias: grad_norm = 0.001203
  l2.weight: grad_norm = 0.113427
Total gradient norm: 0.359916
=== Actor Training Debug (Iteration 5530) ===
Q mean: -13.168083
Q std: 20.253487
Actor loss: 13.172037
Action reg: 0.003953
  l1.weight: grad_norm = 0.130649
  l1.bias: grad_norm = 0.003131
  l2.weight: grad_norm = 0.090738
Total gradient norm: 0.296277
=== Actor Training Debug (Iteration 5531) ===
Q mean: -13.583804
Q std: 19.322704
Actor loss: 13.587766
Action reg: 0.003961
  l1.weight: grad_norm = 0.160633
  l1.bias: grad_norm = 0.001662
  l2.weight: grad_norm = 0.126891
Total gradient norm: 0.420763
=== Actor Training Debug (Iteration 5532) ===
Q mean: -12.726963
Q std: 20.163689
Actor loss: 12.730941
Action reg: 0.003978
  l1.weight: grad_norm = 0.202628
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.160402
Total gradient norm: 0.485432
=== Actor Training Debug (Iteration 5533) ===
Q mean: -12.230448
Q std: 18.093430
Actor loss: 12.234427
Action reg: 0.003980
  l1.weight: grad_norm = 0.294076
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.216780
Total gradient norm: 0.812375
=== Actor Training Debug (Iteration 5534) ===
Q mean: -12.262127
Q std: 17.994896
Actor loss: 12.266102
Action reg: 0.003975
  l1.weight: grad_norm = 0.131233
  l1.bias: grad_norm = 0.000797
  l2.weight: grad_norm = 0.101884
Total gradient norm: 0.291227
=== Actor Training Debug (Iteration 5535) ===
Q mean: -13.323992
Q std: 18.453802
Actor loss: 13.327976
Action reg: 0.003984
  l1.weight: grad_norm = 0.119992
  l1.bias: grad_norm = 0.001490
  l2.weight: grad_norm = 0.076702
Total gradient norm: 0.240120
=== Actor Training Debug (Iteration 5536) ===
Q mean: -14.258438
Q std: 20.087934
Actor loss: 14.262416
Action reg: 0.003978
  l1.weight: grad_norm = 0.123483
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.113188
Total gradient norm: 0.393377
=== Actor Training Debug (Iteration 5537) ===
Q mean: -14.962533
Q std: 20.171604
Actor loss: 14.966512
Action reg: 0.003979
  l1.weight: grad_norm = 0.135777
  l1.bias: grad_norm = 0.002168
  l2.weight: grad_norm = 0.111041
Total gradient norm: 0.330951
=== Actor Training Debug (Iteration 5538) ===
Q mean: -13.008429
Q std: 19.657787
Actor loss: 13.012403
Action reg: 0.003975
  l1.weight: grad_norm = 0.165049
  l1.bias: grad_norm = 0.001011
  l2.weight: grad_norm = 0.126976
Total gradient norm: 0.398119
=== Actor Training Debug (Iteration 5539) ===
Q mean: -12.980048
Q std: 18.834614
Actor loss: 12.984030
Action reg: 0.003982
  l1.weight: grad_norm = 0.068546
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.062478
Total gradient norm: 0.187551
=== Actor Training Debug (Iteration 5540) ===
Q mean: -14.250551
Q std: 20.010178
Actor loss: 14.254515
Action reg: 0.003963
  l1.weight: grad_norm = 0.413880
  l1.bias: grad_norm = 0.000815
  l2.weight: grad_norm = 0.329505
Total gradient norm: 1.168426
=== Actor Training Debug (Iteration 5541) ===
Q mean: -13.590141
Q std: 19.754803
Actor loss: 13.594101
Action reg: 0.003959
  l1.weight: grad_norm = 0.140829
  l1.bias: grad_norm = 0.001823
  l2.weight: grad_norm = 0.113270
Total gradient norm: 0.358966
=== Actor Training Debug (Iteration 5542) ===
Q mean: -14.458041
Q std: 20.881001
Actor loss: 14.462025
Action reg: 0.003983
  l1.weight: grad_norm = 0.132942
  l1.bias: grad_norm = 0.001477
  l2.weight: grad_norm = 0.103329
Total gradient norm: 0.359829
=== Actor Training Debug (Iteration 5543) ===
Q mean: -11.889400
Q std: 18.851406
Actor loss: 11.893375
Action reg: 0.003975
  l1.weight: grad_norm = 0.192197
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.151282
Total gradient norm: 0.608446
=== Actor Training Debug (Iteration 5544) ===
Q mean: -12.613078
Q std: 18.427231
Actor loss: 12.617058
Action reg: 0.003980
  l1.weight: grad_norm = 0.096815
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.082376
Total gradient norm: 0.296990
=== Actor Training Debug (Iteration 5545) ===
Q mean: -13.781639
Q std: 20.084211
Actor loss: 13.785625
Action reg: 0.003986
  l1.weight: grad_norm = 0.082410
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.061000
Total gradient norm: 0.199571
=== Actor Training Debug (Iteration 5546) ===
Q mean: -12.543473
Q std: 19.767172
Actor loss: 12.547441
Action reg: 0.003967
  l1.weight: grad_norm = 0.165060
  l1.bias: grad_norm = 0.000378
  l2.weight: grad_norm = 0.141381
Total gradient norm: 0.396074
=== Actor Training Debug (Iteration 5547) ===
Q mean: -12.822882
Q std: 19.229301
Actor loss: 12.826874
Action reg: 0.003992
  l1.weight: grad_norm = 0.126581
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.102477
Total gradient norm: 0.322937
=== Actor Training Debug (Iteration 5548) ===
Q mean: -13.684620
Q std: 19.285519
Actor loss: 13.688598
Action reg: 0.003978
  l1.weight: grad_norm = 0.149409
  l1.bias: grad_norm = 0.002156
  l2.weight: grad_norm = 0.134461
Total gradient norm: 0.467702
=== Actor Training Debug (Iteration 5549) ===
Q mean: -14.441642
Q std: 21.315767
Actor loss: 14.445619
Action reg: 0.003977
  l1.weight: grad_norm = 0.165592
  l1.bias: grad_norm = 0.003052
  l2.weight: grad_norm = 0.121844
Total gradient norm: 0.400151
=== Actor Training Debug (Iteration 5550) ===
Q mean: -11.555440
Q std: 17.109415
Actor loss: 11.559417
Action reg: 0.003977
  l1.weight: grad_norm = 0.115903
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.085363
Total gradient norm: 0.281828
=== Actor Training Debug (Iteration 5551) ===
Q mean: -11.891922
Q std: 18.147003
Actor loss: 11.895898
Action reg: 0.003976
  l1.weight: grad_norm = 0.096634
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.076007
Total gradient norm: 0.263527
=== Actor Training Debug (Iteration 5552) ===
Q mean: -13.743707
Q std: 20.061918
Actor loss: 13.747675
Action reg: 0.003968
  l1.weight: grad_norm = 0.228315
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.161918
Total gradient norm: 0.514902
=== Actor Training Debug (Iteration 5553) ===
Q mean: -12.048798
Q std: 19.183786
Actor loss: 12.052777
Action reg: 0.003980
  l1.weight: grad_norm = 0.065073
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.056805
Total gradient norm: 0.159261
=== Actor Training Debug (Iteration 5554) ===
Q mean: -14.716633
Q std: 20.460024
Actor loss: 14.720612
Action reg: 0.003979
  l1.weight: grad_norm = 0.125431
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.088303
Total gradient norm: 0.274466
=== Actor Training Debug (Iteration 5555) ===
Q mean: -13.436240
Q std: 20.128971
Actor loss: 13.440217
Action reg: 0.003977
  l1.weight: grad_norm = 0.060665
  l1.bias: grad_norm = 0.001498
  l2.weight: grad_norm = 0.051092
Total gradient norm: 0.169031
=== Actor Training Debug (Iteration 5556) ===
Q mean: -10.548509
Q std: 17.673647
Actor loss: 10.552461
Action reg: 0.003952
  l1.weight: grad_norm = 0.181568
  l1.bias: grad_norm = 0.002294
  l2.weight: grad_norm = 0.149261
Total gradient norm: 0.527903
=== Actor Training Debug (Iteration 5557) ===
Q mean: -14.548740
Q std: 21.020226
Actor loss: 14.552726
Action reg: 0.003985
  l1.weight: grad_norm = 0.089566
  l1.bias: grad_norm = 0.000863
  l2.weight: grad_norm = 0.070851
Total gradient norm: 0.238570
=== Actor Training Debug (Iteration 5558) ===
Q mean: -14.052185
Q std: 19.851528
Actor loss: 14.056143
Action reg: 0.003957
  l1.weight: grad_norm = 0.143031
  l1.bias: grad_norm = 0.001076
  l2.weight: grad_norm = 0.106172
Total gradient norm: 0.339181
=== Actor Training Debug (Iteration 5559) ===
Q mean: -12.869333
Q std: 19.641527
Actor loss: 12.873301
Action reg: 0.003967
  l1.weight: grad_norm = 0.269365
  l1.bias: grad_norm = 0.000613
  l2.weight: grad_norm = 0.182935
Total gradient norm: 0.523573
=== Actor Training Debug (Iteration 5560) ===
Q mean: -12.822529
Q std: 19.222486
Actor loss: 12.826504
Action reg: 0.003975
  l1.weight: grad_norm = 0.216921
  l1.bias: grad_norm = 0.001684
  l2.weight: grad_norm = 0.157697
Total gradient norm: 0.506236
=== Actor Training Debug (Iteration 5561) ===
Q mean: -13.510106
Q std: 19.520197
Actor loss: 13.514081
Action reg: 0.003975
  l1.weight: grad_norm = 0.105933
  l1.bias: grad_norm = 0.000613
  l2.weight: grad_norm = 0.095297
Total gradient norm: 0.277825
=== Actor Training Debug (Iteration 5562) ===
Q mean: -13.416250
Q std: 19.577797
Actor loss: 13.420235
Action reg: 0.003985
  l1.weight: grad_norm = 0.089721
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.063967
Total gradient norm: 0.183923
=== Actor Training Debug (Iteration 5563) ===
Q mean: -14.640369
Q std: 19.468397
Actor loss: 14.644351
Action reg: 0.003981
  l1.weight: grad_norm = 0.091600
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.069933
Total gradient norm: 0.219452
=== Actor Training Debug (Iteration 5564) ===
Q mean: -12.976472
Q std: 18.823881
Actor loss: 12.980452
Action reg: 0.003979
  l1.weight: grad_norm = 0.162526
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.140975
Total gradient norm: 0.401275
=== Actor Training Debug (Iteration 5565) ===
Q mean: -14.329320
Q std: 20.233976
Actor loss: 14.333291
Action reg: 0.003971
  l1.weight: grad_norm = 0.106881
  l1.bias: grad_norm = 0.000901
  l2.weight: grad_norm = 0.074753
Total gradient norm: 0.234137
=== Actor Training Debug (Iteration 5566) ===
Q mean: -13.040641
Q std: 19.962467
Actor loss: 13.044621
Action reg: 0.003981
  l1.weight: grad_norm = 0.496789
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.388414
Total gradient norm: 1.127992
=== Actor Training Debug (Iteration 5567) ===
Q mean: -14.059526
Q std: 20.248531
Actor loss: 14.063499
Action reg: 0.003973
  l1.weight: grad_norm = 0.145876
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.108846
Total gradient norm: 0.301823
=== Actor Training Debug (Iteration 5568) ===
Q mean: -15.398212
Q std: 20.279953
Actor loss: 15.402195
Action reg: 0.003983
  l1.weight: grad_norm = 0.166718
  l1.bias: grad_norm = 0.000896
  l2.weight: grad_norm = 0.134122
Total gradient norm: 0.389689
=== Actor Training Debug (Iteration 5569) ===
Q mean: -12.322614
Q std: 18.600801
Actor loss: 12.326589
Action reg: 0.003975
  l1.weight: grad_norm = 0.256593
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.198655
Total gradient norm: 0.529743
=== Actor Training Debug (Iteration 5570) ===
Q mean: -10.646422
Q std: 16.815746
Actor loss: 10.650396
Action reg: 0.003974
  l1.weight: grad_norm = 0.457771
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.319219
Total gradient norm: 1.138508
=== Actor Training Debug (Iteration 5571) ===
Q mean: -11.469248
Q std: 17.982801
Actor loss: 11.473231
Action reg: 0.003984
  l1.weight: grad_norm = 0.187460
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.149489
Total gradient norm: 0.421312
=== Actor Training Debug (Iteration 5572) ===
Q mean: -13.370586
Q std: 19.062098
Actor loss: 13.374564
Action reg: 0.003978
  l1.weight: grad_norm = 0.150038
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.123961
Total gradient norm: 0.416149
=== Actor Training Debug (Iteration 5573) ===
Q mean: -14.487017
Q std: 20.311644
Actor loss: 14.490979
Action reg: 0.003963
  l1.weight: grad_norm = 0.151153
  l1.bias: grad_norm = 0.001349
  l2.weight: grad_norm = 0.135548
Total gradient norm: 0.442424
=== Actor Training Debug (Iteration 5574) ===
Q mean: -15.725883
Q std: 20.596735
Actor loss: 15.729855
Action reg: 0.003971
  l1.weight: grad_norm = 0.436190
  l1.bias: grad_norm = 0.001686
  l2.weight: grad_norm = 0.334279
Total gradient norm: 1.175609
=== Actor Training Debug (Iteration 5575) ===
Q mean: -14.132870
Q std: 20.140827
Actor loss: 14.136848
Action reg: 0.003978
  l1.weight: grad_norm = 0.119475
  l1.bias: grad_norm = 0.001405
  l2.weight: grad_norm = 0.098126
Total gradient norm: 0.334697
=== Actor Training Debug (Iteration 5576) ===
Q mean: -11.577399
Q std: 19.050905
Actor loss: 11.581372
Action reg: 0.003973
  l1.weight: grad_norm = 0.110631
  l1.bias: grad_norm = 0.000722
  l2.weight: grad_norm = 0.089081
Total gradient norm: 0.277871
=== Actor Training Debug (Iteration 5577) ===
Q mean: -14.157709
Q std: 20.929167
Actor loss: 14.161695
Action reg: 0.003987
  l1.weight: grad_norm = 0.096319
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.078516
Total gradient norm: 0.240020
=== Actor Training Debug (Iteration 5578) ===
Q mean: -12.196564
Q std: 19.871605
Actor loss: 12.200540
Action reg: 0.003976
  l1.weight: grad_norm = 0.143121
  l1.bias: grad_norm = 0.002114
  l2.weight: grad_norm = 0.126412
Total gradient norm: 0.365982
=== Actor Training Debug (Iteration 5579) ===
Q mean: -12.459482
Q std: 19.612841
Actor loss: 12.463464
Action reg: 0.003981
  l1.weight: grad_norm = 0.135601
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.083732
Total gradient norm: 0.252038
=== Actor Training Debug (Iteration 5580) ===
Q mean: -11.370650
Q std: 18.384375
Actor loss: 11.374628
Action reg: 0.003978
  l1.weight: grad_norm = 0.095590
  l1.bias: grad_norm = 0.002051
  l2.weight: grad_norm = 0.086078
Total gradient norm: 0.321845
=== Actor Training Debug (Iteration 5581) ===
Q mean: -14.968758
Q std: 20.654928
Actor loss: 14.972736
Action reg: 0.003978
  l1.weight: grad_norm = 0.161903
  l1.bias: grad_norm = 0.000611
  l2.weight: grad_norm = 0.126697
Total gradient norm: 0.406576
=== Actor Training Debug (Iteration 5582) ===
Q mean: -15.326859
Q std: 21.093781
Actor loss: 15.330818
Action reg: 0.003959
  l1.weight: grad_norm = 0.306041
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.257996
Total gradient norm: 0.729819
=== Actor Training Debug (Iteration 5583) ===
Q mean: -14.779445
Q std: 19.707058
Actor loss: 14.783418
Action reg: 0.003973
  l1.weight: grad_norm = 0.190180
  l1.bias: grad_norm = 0.000697
  l2.weight: grad_norm = 0.138041
Total gradient norm: 0.517377
=== Actor Training Debug (Iteration 5584) ===
Q mean: -12.138845
Q std: 18.975439
Actor loss: 12.142826
Action reg: 0.003980
  l1.weight: grad_norm = 0.153795
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.132370
Total gradient norm: 0.432148
=== Actor Training Debug (Iteration 5585) ===
Q mean: -13.345941
Q std: 18.827856
Actor loss: 13.349917
Action reg: 0.003977
  l1.weight: grad_norm = 0.146048
  l1.bias: grad_norm = 0.001058
  l2.weight: grad_norm = 0.102549
Total gradient norm: 0.310300
=== Actor Training Debug (Iteration 5586) ===
Q mean: -14.335899
Q std: 21.192827
Actor loss: 14.339872
Action reg: 0.003973
  l1.weight: grad_norm = 0.201561
  l1.bias: grad_norm = 0.001520
  l2.weight: grad_norm = 0.141745
Total gradient norm: 0.452946
=== Actor Training Debug (Iteration 5587) ===
Q mean: -14.130116
Q std: 20.145868
Actor loss: 14.134098
Action reg: 0.003983
  l1.weight: grad_norm = 0.079511
  l1.bias: grad_norm = 0.000815
  l2.weight: grad_norm = 0.058470
Total gradient norm: 0.165210
=== Actor Training Debug (Iteration 5588) ===
Q mean: -13.377707
Q std: 18.372818
Actor loss: 13.381668
Action reg: 0.003961
  l1.weight: grad_norm = 0.129661
  l1.bias: grad_norm = 0.004163
  l2.weight: grad_norm = 0.103837
Total gradient norm: 0.340773
=== Actor Training Debug (Iteration 5589) ===
Q mean: -13.955208
Q std: 20.477680
Actor loss: 13.959188
Action reg: 0.003981
  l1.weight: grad_norm = 0.168919
  l1.bias: grad_norm = 0.001377
  l2.weight: grad_norm = 0.109127
Total gradient norm: 0.339568
=== Actor Training Debug (Iteration 5590) ===
Q mean: -12.141908
Q std: 20.079502
Actor loss: 12.145880
Action reg: 0.003972
  l1.weight: grad_norm = 0.147228
  l1.bias: grad_norm = 0.002647
  l2.weight: grad_norm = 0.105861
Total gradient norm: 0.338042
=== Actor Training Debug (Iteration 5591) ===
Q mean: -13.327443
Q std: 19.130678
Actor loss: 13.331418
Action reg: 0.003975
  l1.weight: grad_norm = 0.090102
  l1.bias: grad_norm = 0.000954
  l2.weight: grad_norm = 0.066360
Total gradient norm: 0.207765
=== Actor Training Debug (Iteration 5592) ===
Q mean: -12.601910
Q std: 19.915813
Actor loss: 12.605884
Action reg: 0.003974
  l1.weight: grad_norm = 0.137601
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.111901
Total gradient norm: 0.340540
=== Actor Training Debug (Iteration 5593) ===
Q mean: -14.290304
Q std: 20.642056
Actor loss: 14.294283
Action reg: 0.003978
  l1.weight: grad_norm = 0.127854
  l1.bias: grad_norm = 0.000857
  l2.weight: grad_norm = 0.100885
Total gradient norm: 0.367590
=== Actor Training Debug (Iteration 5594) ===
Q mean: -13.135788
Q std: 18.688301
Actor loss: 13.139768
Action reg: 0.003979
  l1.weight: grad_norm = 0.089997
  l1.bias: grad_norm = 0.000677
  l2.weight: grad_norm = 0.071356
Total gradient norm: 0.258198
=== Actor Training Debug (Iteration 5595) ===
Q mean: -13.156406
Q std: 19.343519
Actor loss: 13.160384
Action reg: 0.003978
  l1.weight: grad_norm = 0.278324
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.228048
Total gradient norm: 0.762284
=== Actor Training Debug (Iteration 5596) ===
Q mean: -11.538931
Q std: 18.466946
Actor loss: 11.542908
Action reg: 0.003977
  l1.weight: grad_norm = 0.168568
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.127873
Total gradient norm: 0.436636
=== Actor Training Debug (Iteration 5597) ===
Q mean: -12.238338
Q std: 19.632710
Actor loss: 12.242317
Action reg: 0.003980
  l1.weight: grad_norm = 0.201766
  l1.bias: grad_norm = 0.000724
  l2.weight: grad_norm = 0.121325
Total gradient norm: 0.345799
=== Actor Training Debug (Iteration 5598) ===
Q mean: -14.936830
Q std: 19.517321
Actor loss: 14.940805
Action reg: 0.003976
  l1.weight: grad_norm = 0.151179
  l1.bias: grad_norm = 0.001326
  l2.weight: grad_norm = 0.109268
Total gradient norm: 0.340199
=== Actor Training Debug (Iteration 5599) ===
Q mean: -11.634039
Q std: 18.874548
Actor loss: 11.638017
Action reg: 0.003978
  l1.weight: grad_norm = 0.107857
  l1.bias: grad_norm = 0.000774
  l2.weight: grad_norm = 0.078787
Total gradient norm: 0.272827
=== Actor Training Debug (Iteration 5600) ===
Q mean: -13.069663
Q std: 20.678007
Actor loss: 13.073643
Action reg: 0.003979
  l1.weight: grad_norm = 0.215531
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.170137
Total gradient norm: 0.500816
=== Actor Training Debug (Iteration 5601) ===
Q mean: -12.832468
Q std: 19.714794
Actor loss: 12.836451
Action reg: 0.003983
  l1.weight: grad_norm = 0.091471
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.073511
Total gradient norm: 0.256579
=== Actor Training Debug (Iteration 5602) ===
Q mean: -13.692455
Q std: 19.668488
Actor loss: 13.696438
Action reg: 0.003983
  l1.weight: grad_norm = 0.132866
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.109894
Total gradient norm: 0.339890
=== Actor Training Debug (Iteration 5603) ===
Q mean: -13.192933
Q std: 19.671736
Actor loss: 13.196886
Action reg: 0.003953
  l1.weight: grad_norm = 0.143291
  l1.bias: grad_norm = 0.001603
  l2.weight: grad_norm = 0.108882
Total gradient norm: 0.372002
=== Actor Training Debug (Iteration 5604) ===
Q mean: -12.992857
Q std: 19.532047
Actor loss: 12.996824
Action reg: 0.003968
  l1.weight: grad_norm = 0.113562
  l1.bias: grad_norm = 0.000833
  l2.weight: grad_norm = 0.087277
Total gradient norm: 0.245197
=== Actor Training Debug (Iteration 5605) ===
Q mean: -13.955421
Q std: 20.520845
Actor loss: 13.959399
Action reg: 0.003978
  l1.weight: grad_norm = 0.288593
  l1.bias: grad_norm = 0.000766
  l2.weight: grad_norm = 0.206166
Total gradient norm: 0.629059
=== Actor Training Debug (Iteration 5606) ===
Q mean: -12.684586
Q std: 19.310549
Actor loss: 12.688563
Action reg: 0.003978
  l1.weight: grad_norm = 0.097142
  l1.bias: grad_norm = 0.000873
  l2.weight: grad_norm = 0.073836
Total gradient norm: 0.244102
=== Actor Training Debug (Iteration 5607) ===
Q mean: -13.846724
Q std: 19.811205
Actor loss: 13.850698
Action reg: 0.003975
  l1.weight: grad_norm = 0.105482
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.091767
Total gradient norm: 0.326968
=== Actor Training Debug (Iteration 5608) ===
Q mean: -11.669487
Q std: 18.704775
Actor loss: 11.673452
Action reg: 0.003965
  l1.weight: grad_norm = 0.169012
  l1.bias: grad_norm = 0.001696
  l2.weight: grad_norm = 0.135011
Total gradient norm: 0.576074
=== Actor Training Debug (Iteration 5609) ===
Q mean: -12.971666
Q std: 20.039108
Actor loss: 12.975639
Action reg: 0.003973
  l1.weight: grad_norm = 0.186427
  l1.bias: grad_norm = 0.002493
  l2.weight: grad_norm = 0.154275
Total gradient norm: 0.511491
=== Actor Training Debug (Iteration 5610) ===
Q mean: -15.395660
Q std: 21.121721
Actor loss: 15.399637
Action reg: 0.003977
  l1.weight: grad_norm = 0.104427
  l1.bias: grad_norm = 0.000675
  l2.weight: grad_norm = 0.081726
Total gradient norm: 0.241465
=== Actor Training Debug (Iteration 5611) ===
Q mean: -13.036474
Q std: 19.704638
Actor loss: 13.040453
Action reg: 0.003978
  l1.weight: grad_norm = 0.080134
  l1.bias: grad_norm = 0.001314
  l2.weight: grad_norm = 0.064303
Total gradient norm: 0.220176
=== Actor Training Debug (Iteration 5612) ===
Q mean: -11.955736
Q std: 18.829990
Actor loss: 11.959712
Action reg: 0.003976
  l1.weight: grad_norm = 0.118885
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.102177
Total gradient norm: 0.314406
=== Actor Training Debug (Iteration 5613) ===
Q mean: -13.910786
Q std: 19.794891
Actor loss: 13.914761
Action reg: 0.003975
  l1.weight: grad_norm = 0.169838
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.126148
Total gradient norm: 0.347669
=== Actor Training Debug (Iteration 5614) ===
Q mean: -15.600012
Q std: 20.503078
Actor loss: 15.603991
Action reg: 0.003978
  l1.weight: grad_norm = 0.104779
  l1.bias: grad_norm = 0.002377
  l2.weight: grad_norm = 0.091273
Total gradient norm: 0.292422
=== Actor Training Debug (Iteration 5615) ===
Q mean: -16.294758
Q std: 20.430811
Actor loss: 16.298727
Action reg: 0.003970
  l1.weight: grad_norm = 0.167882
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.120880
Total gradient norm: 0.504490
=== Actor Training Debug (Iteration 5616) ===
Q mean: -11.822880
Q std: 18.878506
Actor loss: 11.826837
Action reg: 0.003957
  l1.weight: grad_norm = 0.156423
  l1.bias: grad_norm = 0.002092
  l2.weight: grad_norm = 0.123320
Total gradient norm: 0.376956
=== Actor Training Debug (Iteration 5617) ===
Q mean: -11.256130
Q std: 19.161766
Actor loss: 11.260109
Action reg: 0.003979
  l1.weight: grad_norm = 0.130696
  l1.bias: grad_norm = 0.001119
  l2.weight: grad_norm = 0.098169
Total gradient norm: 0.309984
=== Actor Training Debug (Iteration 5618) ===
Q mean: -13.556194
Q std: 20.446297
Actor loss: 13.560173
Action reg: 0.003978
  l1.weight: grad_norm = 0.151367
  l1.bias: grad_norm = 0.000405
  l2.weight: grad_norm = 0.108825
Total gradient norm: 0.288658
=== Actor Training Debug (Iteration 5619) ===
Q mean: -13.220778
Q std: 20.033739
Actor loss: 13.224759
Action reg: 0.003981
  l1.weight: grad_norm = 0.126163
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.101883
Total gradient norm: 0.300383
=== Actor Training Debug (Iteration 5620) ===
Q mean: -14.074442
Q std: 19.165579
Actor loss: 14.078428
Action reg: 0.003986
  l1.weight: grad_norm = 0.033956
  l1.bias: grad_norm = 0.000947
  l2.weight: grad_norm = 0.024792
Total gradient norm: 0.079050
=== Actor Training Debug (Iteration 5621) ===
Q mean: -13.255204
Q std: 19.015574
Actor loss: 13.259185
Action reg: 0.003980
  l1.weight: grad_norm = 0.149880
  l1.bias: grad_norm = 0.000435
  l2.weight: grad_norm = 0.106436
Total gradient norm: 0.351210
=== Actor Training Debug (Iteration 5622) ===
Q mean: -16.383694
Q std: 22.309639
Actor loss: 16.387671
Action reg: 0.003978
  l1.weight: grad_norm = 0.247814
  l1.bias: grad_norm = 0.000454
  l2.weight: grad_norm = 0.201625
Total gradient norm: 0.710437
=== Actor Training Debug (Iteration 5623) ===
Q mean: -14.131765
Q std: 20.338829
Actor loss: 14.135748
Action reg: 0.003982
  l1.weight: grad_norm = 0.273596
  l1.bias: grad_norm = 0.000658
  l2.weight: grad_norm = 0.218927
Total gradient norm: 0.783596
=== Actor Training Debug (Iteration 5624) ===
Q mean: -11.765399
Q std: 18.979832
Actor loss: 11.769371
Action reg: 0.003972
  l1.weight: grad_norm = 0.192642
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.133519
Total gradient norm: 0.406122
=== Actor Training Debug (Iteration 5625) ===
Q mean: -13.712877
Q std: 20.594824
Actor loss: 13.716860
Action reg: 0.003983
  l1.weight: grad_norm = 0.053394
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.039562
Total gradient norm: 0.131852
=== Actor Training Debug (Iteration 5626) ===
Q mean: -12.890985
Q std: 19.899057
Actor loss: 12.894972
Action reg: 0.003987
  l1.weight: grad_norm = 0.061918
  l1.bias: grad_norm = 0.000693
  l2.weight: grad_norm = 0.050379
Total gradient norm: 0.145132
=== Actor Training Debug (Iteration 5627) ===
Q mean: -13.961497
Q std: 19.553038
Actor loss: 13.965476
Action reg: 0.003979
  l1.weight: grad_norm = 0.057773
  l1.bias: grad_norm = 0.000663
  l2.weight: grad_norm = 0.045626
Total gradient norm: 0.150698
=== Actor Training Debug (Iteration 5628) ===
Q mean: -13.156095
Q std: 20.129667
Actor loss: 13.160058
Action reg: 0.003964
  l1.weight: grad_norm = 0.126554
  l1.bias: grad_norm = 0.001013
  l2.weight: grad_norm = 0.094478
Total gradient norm: 0.298894
=== Actor Training Debug (Iteration 5629) ===
Q mean: -13.449400
Q std: 18.953299
Actor loss: 13.453373
Action reg: 0.003973
  l1.weight: grad_norm = 0.075881
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.071349
Total gradient norm: 0.222936
=== Actor Training Debug (Iteration 5630) ===
Q mean: -12.194768
Q std: 18.796183
Actor loss: 12.198731
Action reg: 0.003963
  l1.weight: grad_norm = 0.181981
  l1.bias: grad_norm = 0.000936
  l2.weight: grad_norm = 0.151598
Total gradient norm: 0.480612
=== Actor Training Debug (Iteration 5631) ===
Q mean: -15.004690
Q std: 20.063690
Actor loss: 15.008666
Action reg: 0.003976
  l1.weight: grad_norm = 0.111125
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.079046
Total gradient norm: 0.258962
=== Actor Training Debug (Iteration 5632) ===
Q mean: -12.087737
Q std: 19.849348
Actor loss: 12.091713
Action reg: 0.003976
  l1.weight: grad_norm = 0.104453
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.077869
Total gradient norm: 0.241994
=== Actor Training Debug (Iteration 5633) ===
Q mean: -12.408758
Q std: 19.080215
Actor loss: 12.412726
Action reg: 0.003969
  l1.weight: grad_norm = 0.156705
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.117953
Total gradient norm: 0.340592
=== Actor Training Debug (Iteration 5634) ===
Q mean: -12.201354
Q std: 18.880339
Actor loss: 12.205323
Action reg: 0.003969
  l1.weight: grad_norm = 0.112408
  l1.bias: grad_norm = 0.002247
  l2.weight: grad_norm = 0.100975
Total gradient norm: 0.309707
=== Actor Training Debug (Iteration 5635) ===
Q mean: -12.539430
Q std: 19.073446
Actor loss: 12.543404
Action reg: 0.003974
  l1.weight: grad_norm = 0.237202
  l1.bias: grad_norm = 0.000566
  l2.weight: grad_norm = 0.196431
Total gradient norm: 0.658196
=== Actor Training Debug (Iteration 5636) ===
Q mean: -13.748468
Q std: 19.669685
Actor loss: 13.752456
Action reg: 0.003987
  l1.weight: grad_norm = 0.109711
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.074068
Total gradient norm: 0.209564
=== Actor Training Debug (Iteration 5637) ===
Q mean: -13.800880
Q std: 19.107107
Actor loss: 13.804859
Action reg: 0.003979
  l1.weight: grad_norm = 0.131200
  l1.bias: grad_norm = 0.000579
  l2.weight: grad_norm = 0.099507
Total gradient norm: 0.318733
=== Actor Training Debug (Iteration 5638) ===
Q mean: -13.393467
Q std: 20.501669
Actor loss: 13.397449
Action reg: 0.003981
  l1.weight: grad_norm = 0.168394
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.120977
Total gradient norm: 0.364351
=== Actor Training Debug (Iteration 5639) ===
Q mean: -14.391195
Q std: 21.852200
Actor loss: 14.395185
Action reg: 0.003990
  l1.weight: grad_norm = 0.131982
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.090947
Total gradient norm: 0.268599
=== Actor Training Debug (Iteration 5640) ===
Q mean: -12.904385
Q std: 19.878794
Actor loss: 12.908370
Action reg: 0.003985
  l1.weight: grad_norm = 0.156693
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.136382
Total gradient norm: 0.426580
=== Actor Training Debug (Iteration 5641) ===
Q mean: -14.490298
Q std: 21.370138
Actor loss: 14.494263
Action reg: 0.003965
  l1.weight: grad_norm = 0.334105
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.216002
Total gradient norm: 0.654849
=== Actor Training Debug (Iteration 5642) ===
Q mean: -13.968016
Q std: 20.713392
Actor loss: 13.971984
Action reg: 0.003969
  l1.weight: grad_norm = 0.109880
  l1.bias: grad_norm = 0.001228
  l2.weight: grad_norm = 0.089520
Total gradient norm: 0.293292
=== Actor Training Debug (Iteration 5643) ===
Q mean: -11.656195
Q std: 18.249809
Actor loss: 11.660184
Action reg: 0.003990
  l1.weight: grad_norm = 0.078646
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.065539
Total gradient norm: 0.193915
=== Actor Training Debug (Iteration 5644) ===
Q mean: -13.705049
Q std: 20.409315
Actor loss: 13.709023
Action reg: 0.003974
  l1.weight: grad_norm = 0.273156
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.240642
Total gradient norm: 0.811897
=== Actor Training Debug (Iteration 5645) ===
Q mean: -14.176131
Q std: 21.455406
Actor loss: 14.180114
Action reg: 0.003982
  l1.weight: grad_norm = 0.085737
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.065611
Total gradient norm: 0.197008
=== Actor Training Debug (Iteration 5646) ===
Q mean: -13.224054
Q std: 19.675276
Actor loss: 13.228034
Action reg: 0.003979
  l1.weight: grad_norm = 0.303950
  l1.bias: grad_norm = 0.000526
  l2.weight: grad_norm = 0.223349
Total gradient norm: 0.758274
=== Actor Training Debug (Iteration 5647) ===
Q mean: -16.028332
Q std: 21.213196
Actor loss: 16.032309
Action reg: 0.003978
  l1.weight: grad_norm = 0.233790
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.182794
Total gradient norm: 0.621184
=== Actor Training Debug (Iteration 5648) ===
Q mean: -15.263293
Q std: 21.188890
Actor loss: 15.267280
Action reg: 0.003986
  l1.weight: grad_norm = 0.058459
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.036501
Total gradient norm: 0.111766
=== Actor Training Debug (Iteration 5649) ===
Q mean: -13.948363
Q std: 20.265789
Actor loss: 13.952340
Action reg: 0.003977
  l1.weight: grad_norm = 0.207071
  l1.bias: grad_norm = 0.002080
  l2.weight: grad_norm = 0.158555
Total gradient norm: 0.476684
=== Actor Training Debug (Iteration 5650) ===
Q mean: -14.902973
Q std: 20.920795
Actor loss: 14.906956
Action reg: 0.003982
  l1.weight: grad_norm = 0.176946
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.143779
Total gradient norm: 0.466192
=== Actor Training Debug (Iteration 5651) ===
Q mean: -11.668962
Q std: 18.080357
Actor loss: 11.672941
Action reg: 0.003980
  l1.weight: grad_norm = 0.131163
  l1.bias: grad_norm = 0.000594
  l2.weight: grad_norm = 0.102165
Total gradient norm: 0.345068
=== Actor Training Debug (Iteration 5652) ===
Q mean: -12.117838
Q std: 18.270929
Actor loss: 12.121819
Action reg: 0.003980
  l1.weight: grad_norm = 0.121757
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.100927
Total gradient norm: 0.343784
=== Actor Training Debug (Iteration 5653) ===
Q mean: -13.550829
Q std: 20.117476
Actor loss: 13.554811
Action reg: 0.003982
  l1.weight: grad_norm = 0.133906
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.093893
Total gradient norm: 0.262260
=== Actor Training Debug (Iteration 5654) ===
Q mean: -11.597322
Q std: 18.256588
Actor loss: 11.601300
Action reg: 0.003979
  l1.weight: grad_norm = 0.123206
  l1.bias: grad_norm = 0.000769
  l2.weight: grad_norm = 0.093030
Total gradient norm: 0.272353
=== Actor Training Debug (Iteration 5655) ===
Q mean: -13.585480
Q std: 19.064608
Actor loss: 13.589464
Action reg: 0.003984
  l1.weight: grad_norm = 0.081061
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.061196
Total gradient norm: 0.201258
=== Actor Training Debug (Iteration 5656) ===
Q mean: -12.745058
Q std: 19.814976
Actor loss: 12.749028
Action reg: 0.003970
  l1.weight: grad_norm = 0.128069
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.105048
Total gradient norm: 0.318980
=== Actor Training Debug (Iteration 5657) ===
Q mean: -13.949455
Q std: 19.484875
Actor loss: 13.953439
Action reg: 0.003984
  l1.weight: grad_norm = 0.146157
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.119919
Total gradient norm: 0.383699
=== Actor Training Debug (Iteration 5658) ===
Q mean: -12.989086
Q std: 18.922287
Actor loss: 12.993067
Action reg: 0.003980
  l1.weight: grad_norm = 0.103689
  l1.bias: grad_norm = 0.001195
  l2.weight: grad_norm = 0.078922
Total gradient norm: 0.234872
=== Actor Training Debug (Iteration 5659) ===
Q mean: -13.306314
Q std: 19.531778
Actor loss: 13.310295
Action reg: 0.003981
  l1.weight: grad_norm = 0.164670
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.141183
Total gradient norm: 0.423981
=== Actor Training Debug (Iteration 5660) ===
Q mean: -12.379884
Q std: 19.100382
Actor loss: 12.383853
Action reg: 0.003969
  l1.weight: grad_norm = 0.102224
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.073671
Total gradient norm: 0.245120
=== Actor Training Debug (Iteration 5661) ===
Q mean: -12.384432
Q std: 19.352144
Actor loss: 12.388412
Action reg: 0.003980
  l1.weight: grad_norm = 0.601923
  l1.bias: grad_norm = 0.001040
  l2.weight: grad_norm = 0.420198
Total gradient norm: 1.405952
=== Actor Training Debug (Iteration 5662) ===
Q mean: -13.830911
Q std: 18.941444
Actor loss: 13.834888
Action reg: 0.003977
  l1.weight: grad_norm = 0.158019
  l1.bias: grad_norm = 0.000551
  l2.weight: grad_norm = 0.120035
Total gradient norm: 0.329556
=== Actor Training Debug (Iteration 5663) ===
Q mean: -12.070646
Q std: 18.929443
Actor loss: 12.074624
Action reg: 0.003978
  l1.weight: grad_norm = 0.163702
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.137233
Total gradient norm: 0.422023
=== Actor Training Debug (Iteration 5664) ===
Q mean: -12.514814
Q std: 19.029966
Actor loss: 12.518787
Action reg: 0.003973
  l1.weight: grad_norm = 0.121963
  l1.bias: grad_norm = 0.000782
  l2.weight: grad_norm = 0.085006
Total gradient norm: 0.273005
=== Actor Training Debug (Iteration 5665) ===
Q mean: -14.169704
Q std: 20.399424
Actor loss: 14.173686
Action reg: 0.003982
  l1.weight: grad_norm = 0.093163
  l1.bias: grad_norm = 0.000516
  l2.weight: grad_norm = 0.078991
Total gradient norm: 0.277275
=== Actor Training Debug (Iteration 5666) ===
Q mean: -13.241610
Q std: 19.816252
Actor loss: 13.245585
Action reg: 0.003975
  l1.weight: grad_norm = 0.128145
  l1.bias: grad_norm = 0.001075
  l2.weight: grad_norm = 0.094844
Total gradient norm: 0.269234
=== Actor Training Debug (Iteration 5667) ===
Q mean: -13.247671
Q std: 19.107038
Actor loss: 13.251653
Action reg: 0.003982
  l1.weight: grad_norm = 0.140289
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.111964
Total gradient norm: 0.364748
=== Actor Training Debug (Iteration 5668) ===
Q mean: -13.003702
Q std: 19.314665
Actor loss: 13.007672
Action reg: 0.003970
  l1.weight: grad_norm = 0.164979
  l1.bias: grad_norm = 0.001018
  l2.weight: grad_norm = 0.122338
Total gradient norm: 0.422292
=== Actor Training Debug (Iteration 5669) ===
Q mean: -14.186914
Q std: 19.566530
Actor loss: 14.190881
Action reg: 0.003966
  l1.weight: grad_norm = 0.101178
  l1.bias: grad_norm = 0.001595
  l2.weight: grad_norm = 0.078643
Total gradient norm: 0.346831
=== Actor Training Debug (Iteration 5670) ===
Q mean: -14.692598
Q std: 20.414871
Actor loss: 14.696579
Action reg: 0.003980
  l1.weight: grad_norm = 0.146389
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.155075
Total gradient norm: 0.378969
=== Actor Training Debug (Iteration 5671) ===
Q mean: -14.135624
Q std: 19.889700
Actor loss: 14.139602
Action reg: 0.003978
  l1.weight: grad_norm = 0.175683
  l1.bias: grad_norm = 0.000716
  l2.weight: grad_norm = 0.159045
Total gradient norm: 0.483487
=== Actor Training Debug (Iteration 5672) ===
Q mean: -12.414021
Q std: 17.582916
Actor loss: 12.418009
Action reg: 0.003988
  l1.weight: grad_norm = 0.070473
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.052303
Total gradient norm: 0.177225
=== Actor Training Debug (Iteration 5673) ===
Q mean: -12.382381
Q std: 18.484737
Actor loss: 12.386358
Action reg: 0.003977
  l1.weight: grad_norm = 0.163995
  l1.bias: grad_norm = 0.000649
  l2.weight: grad_norm = 0.117071
Total gradient norm: 0.357483
=== Actor Training Debug (Iteration 5674) ===
Q mean: -13.638358
Q std: 20.059923
Actor loss: 13.642325
Action reg: 0.003968
  l1.weight: grad_norm = 0.247053
  l1.bias: grad_norm = 0.000892
  l2.weight: grad_norm = 0.201914
Total gradient norm: 0.608065
=== Actor Training Debug (Iteration 5675) ===
Q mean: -13.028960
Q std: 19.449806
Actor loss: 13.032941
Action reg: 0.003980
  l1.weight: grad_norm = 0.089927
  l1.bias: grad_norm = 0.000869
  l2.weight: grad_norm = 0.075649
Total gradient norm: 0.213906
=== Actor Training Debug (Iteration 5676) ===
Q mean: -13.152060
Q std: 18.885202
Actor loss: 13.156031
Action reg: 0.003971
  l1.weight: grad_norm = 0.136820
  l1.bias: grad_norm = 0.002390
  l2.weight: grad_norm = 0.091677
Total gradient norm: 0.268543
=== Actor Training Debug (Iteration 5677) ===
Q mean: -15.131291
Q std: 20.575558
Actor loss: 15.135274
Action reg: 0.003983
  l1.weight: grad_norm = 0.109937
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.079593
Total gradient norm: 0.239209
=== Actor Training Debug (Iteration 5678) ===
Q mean: -14.759586
Q std: 19.610262
Actor loss: 14.763565
Action reg: 0.003979
  l1.weight: grad_norm = 0.095320
  l1.bias: grad_norm = 0.001207
  l2.weight: grad_norm = 0.064824
Total gradient norm: 0.217862
=== Actor Training Debug (Iteration 5679) ===
Q mean: -13.385280
Q std: 19.634197
Actor loss: 13.389254
Action reg: 0.003974
  l1.weight: grad_norm = 0.160423
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.115638
Total gradient norm: 0.328671
=== Actor Training Debug (Iteration 5680) ===
Q mean: -13.466957
Q std: 19.757366
Actor loss: 13.470938
Action reg: 0.003981
  l1.weight: grad_norm = 0.110918
  l1.bias: grad_norm = 0.002122
  l2.weight: grad_norm = 0.081197
Total gradient norm: 0.267811
=== Actor Training Debug (Iteration 5681) ===
Q mean: -13.212069
Q std: 19.158035
Actor loss: 13.216062
Action reg: 0.003993
  l1.weight: grad_norm = 0.068302
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.061583
Total gradient norm: 0.170615
=== Actor Training Debug (Iteration 5682) ===
Q mean: -13.185891
Q std: 20.941879
Actor loss: 13.189872
Action reg: 0.003980
  l1.weight: grad_norm = 0.083559
  l1.bias: grad_norm = 0.000639
  l2.weight: grad_norm = 0.076550
Total gradient norm: 0.223610
=== Actor Training Debug (Iteration 5683) ===
Q mean: -15.968546
Q std: 22.464169
Actor loss: 15.972516
Action reg: 0.003970
  l1.weight: grad_norm = 0.117886
  l1.bias: grad_norm = 0.003413
  l2.weight: grad_norm = 0.086929
Total gradient norm: 0.274118
=== Actor Training Debug (Iteration 5684) ===
Q mean: -12.363485
Q std: 19.567360
Actor loss: 12.367468
Action reg: 0.003982
  l1.weight: grad_norm = 0.206573
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.154279
Total gradient norm: 0.464300
=== Actor Training Debug (Iteration 5685) ===
Q mean: -14.450250
Q std: 21.133184
Actor loss: 14.454209
Action reg: 0.003960
  l1.weight: grad_norm = 0.197097
  l1.bias: grad_norm = 0.002653
  l2.weight: grad_norm = 0.177945
Total gradient norm: 0.486595
=== Actor Training Debug (Iteration 5686) ===
Q mean: -11.713438
Q std: 18.447906
Actor loss: 11.717405
Action reg: 0.003967
  l1.weight: grad_norm = 0.173636
  l1.bias: grad_norm = 0.003121
  l2.weight: grad_norm = 0.136657
Total gradient norm: 0.413475
=== Actor Training Debug (Iteration 5687) ===
Q mean: -13.980403
Q std: 19.956524
Actor loss: 13.984374
Action reg: 0.003971
  l1.weight: grad_norm = 0.157617
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.115668
Total gradient norm: 0.389309
=== Actor Training Debug (Iteration 5688) ===
Q mean: -12.446148
Q std: 19.007868
Actor loss: 12.450118
Action reg: 0.003970
  l1.weight: grad_norm = 0.128647
  l1.bias: grad_norm = 0.000935
  l2.weight: grad_norm = 0.094830
Total gradient norm: 0.312188
=== Actor Training Debug (Iteration 5689) ===
Q mean: -13.322174
Q std: 20.132490
Actor loss: 13.326152
Action reg: 0.003978
  l1.weight: grad_norm = 0.105805
  l1.bias: grad_norm = 0.001622
  l2.weight: grad_norm = 0.093002
Total gradient norm: 0.314333
=== Actor Training Debug (Iteration 5690) ===
Q mean: -14.094064
Q std: 19.504944
Actor loss: 14.098042
Action reg: 0.003979
  l1.weight: grad_norm = 0.143008
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.117245
Total gradient norm: 0.344871
=== Actor Training Debug (Iteration 5691) ===
Q mean: -14.394218
Q std: 19.702469
Actor loss: 14.398207
Action reg: 0.003989
  l1.weight: grad_norm = 0.093131
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.070520
Total gradient norm: 0.200345
=== Actor Training Debug (Iteration 5692) ===
Q mean: -13.094898
Q std: 18.745260
Actor loss: 13.098872
Action reg: 0.003974
  l1.weight: grad_norm = 0.091099
  l1.bias: grad_norm = 0.000566
  l2.weight: grad_norm = 0.075984
Total gradient norm: 0.235197
=== Actor Training Debug (Iteration 5693) ===
Q mean: -11.650055
Q std: 18.092588
Actor loss: 11.654040
Action reg: 0.003985
  l1.weight: grad_norm = 0.162001
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.146808
Total gradient norm: 0.485451
=== Actor Training Debug (Iteration 5694) ===
Q mean: -12.794127
Q std: 19.455891
Actor loss: 12.798105
Action reg: 0.003979
  l1.weight: grad_norm = 0.135364
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.106403
Total gradient norm: 0.338317
=== Actor Training Debug (Iteration 5695) ===
Q mean: -12.905300
Q std: 20.152592
Actor loss: 12.909284
Action reg: 0.003984
  l1.weight: grad_norm = 0.117702
  l1.bias: grad_norm = 0.000521
  l2.weight: grad_norm = 0.092086
Total gradient norm: 0.295315
=== Actor Training Debug (Iteration 5696) ===
Q mean: -13.390996
Q std: 19.019127
Actor loss: 13.394987
Action reg: 0.003991
  l1.weight: grad_norm = 0.078221
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.073808
Total gradient norm: 0.234991
=== Actor Training Debug (Iteration 5697) ===
Q mean: -12.115473
Q std: 17.824076
Actor loss: 12.119446
Action reg: 0.003973
  l1.weight: grad_norm = 0.271305
  l1.bias: grad_norm = 0.000484
  l2.weight: grad_norm = 0.226063
Total gradient norm: 0.739532
=== Actor Training Debug (Iteration 5698) ===
Q mean: -11.378988
Q std: 19.303604
Actor loss: 11.382953
Action reg: 0.003964
  l1.weight: grad_norm = 0.112878
  l1.bias: grad_norm = 0.000965
  l2.weight: grad_norm = 0.097697
Total gradient norm: 0.294000
=== Actor Training Debug (Iteration 5699) ===
Q mean: -13.567554
Q std: 20.517382
Actor loss: 13.571541
Action reg: 0.003987
  l1.weight: grad_norm = 0.094642
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.071696
Total gradient norm: 0.225188
=== Actor Training Debug (Iteration 5700) ===
Q mean: -15.013255
Q std: 20.816782
Actor loss: 15.017236
Action reg: 0.003981
  l1.weight: grad_norm = 0.176529
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.146590
Total gradient norm: 0.539402
=== Actor Training Debug (Iteration 5701) ===
Q mean: -14.001301
Q std: 19.397264
Actor loss: 14.005274
Action reg: 0.003973
  l1.weight: grad_norm = 0.215785
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.135025
Total gradient norm: 0.378912
=== Actor Training Debug (Iteration 5702) ===
Q mean: -12.931317
Q std: 18.388165
Actor loss: 12.935302
Action reg: 0.003985
  l1.weight: grad_norm = 0.083419
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.065172
Total gradient norm: 0.223088
=== Actor Training Debug (Iteration 5703) ===
Q mean: -15.011936
Q std: 19.732035
Actor loss: 15.015909
Action reg: 0.003973
  l1.weight: grad_norm = 0.078045
  l1.bias: grad_norm = 0.001082
  l2.weight: grad_norm = 0.071965
Total gradient norm: 0.280937
=== Actor Training Debug (Iteration 5704) ===
Q mean: -13.342699
Q std: 20.651804
Actor loss: 13.346671
Action reg: 0.003972
  l1.weight: grad_norm = 0.095070
  l1.bias: grad_norm = 0.000613
  l2.weight: grad_norm = 0.071838
Total gradient norm: 0.237179
=== Actor Training Debug (Iteration 5705) ===
Q mean: -11.915941
Q std: 17.936377
Actor loss: 11.919915
Action reg: 0.003974
  l1.weight: grad_norm = 0.186361
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.139902
Total gradient norm: 0.447036
=== Actor Training Debug (Iteration 5706) ===
Q mean: -14.659454
Q std: 21.425381
Actor loss: 14.663435
Action reg: 0.003980
  l1.weight: grad_norm = 0.187360
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.172482
Total gradient norm: 0.542202
=== Actor Training Debug (Iteration 5707) ===
Q mean: -12.636888
Q std: 17.625679
Actor loss: 12.640859
Action reg: 0.003971
  l1.weight: grad_norm = 0.268531
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.178727
Total gradient norm: 0.523138
=== Actor Training Debug (Iteration 5708) ===
Q mean: -13.885889
Q std: 20.479450
Actor loss: 13.889858
Action reg: 0.003969
  l1.weight: grad_norm = 0.161190
  l1.bias: grad_norm = 0.000630
  l2.weight: grad_norm = 0.114879
Total gradient norm: 0.368569
=== Actor Training Debug (Iteration 5709) ===
Q mean: -14.088051
Q std: 19.105761
Actor loss: 14.092022
Action reg: 0.003971
  l1.weight: grad_norm = 0.229648
  l1.bias: grad_norm = 0.000478
  l2.weight: grad_norm = 0.165322
Total gradient norm: 0.483122
=== Actor Training Debug (Iteration 5710) ===
Q mean: -14.259591
Q std: 19.753832
Actor loss: 14.263569
Action reg: 0.003978
  l1.weight: grad_norm = 0.104281
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.071093
Total gradient norm: 0.225188
=== Actor Training Debug (Iteration 5711) ===
Q mean: -12.117318
Q std: 19.586622
Actor loss: 12.121305
Action reg: 0.003986
  l1.weight: grad_norm = 0.129358
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.086951
Total gradient norm: 0.247696
=== Actor Training Debug (Iteration 5712) ===
Q mean: -14.524000
Q std: 21.086399
Actor loss: 14.527973
Action reg: 0.003973
  l1.weight: grad_norm = 0.102975
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.090810
Total gradient norm: 0.339419
=== Actor Training Debug (Iteration 5713) ===
Q mean: -13.861584
Q std: 19.932152
Actor loss: 13.865565
Action reg: 0.003982
  l1.weight: grad_norm = 0.100310
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.075932
Total gradient norm: 0.217234
=== Actor Training Debug (Iteration 5714) ===
Q mean: -18.368717
Q std: 22.874836
Actor loss: 18.372694
Action reg: 0.003977
  l1.weight: grad_norm = 0.098117
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.074694
Total gradient norm: 0.216452
=== Actor Training Debug (Iteration 5715) ===
Q mean: -16.694099
Q std: 20.967731
Actor loss: 16.698076
Action reg: 0.003977
  l1.weight: grad_norm = 0.126070
  l1.bias: grad_norm = 0.000688
  l2.weight: grad_norm = 0.090560
Total gradient norm: 0.276878
=== Actor Training Debug (Iteration 5716) ===
Q mean: -12.796079
Q std: 19.368116
Actor loss: 12.800048
Action reg: 0.003969
  l1.weight: grad_norm = 0.138841
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.087520
Total gradient norm: 0.241821
=== Actor Training Debug (Iteration 5717) ===
Q mean: -15.105420
Q std: 20.934229
Actor loss: 15.109403
Action reg: 0.003983
  l1.weight: grad_norm = 0.054746
  l1.bias: grad_norm = 0.000557
  l2.weight: grad_norm = 0.047457
Total gradient norm: 0.131547
=== Actor Training Debug (Iteration 5718) ===
Q mean: -14.049348
Q std: 19.903088
Actor loss: 14.053301
Action reg: 0.003953
  l1.weight: grad_norm = 0.188155
  l1.bias: grad_norm = 0.001603
  l2.weight: grad_norm = 0.138137
Total gradient norm: 0.421162
=== Actor Training Debug (Iteration 5719) ===
Q mean: -14.162035
Q std: 20.726234
Actor loss: 14.166010
Action reg: 0.003975
  l1.weight: grad_norm = 0.180447
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.139858
Total gradient norm: 0.398589
=== Actor Training Debug (Iteration 5720) ===
Q mean: -12.948755
Q std: 19.440336
Actor loss: 12.952723
Action reg: 0.003968
  l1.weight: grad_norm = 0.114829
  l1.bias: grad_norm = 0.001304
  l2.weight: grad_norm = 0.085488
Total gradient norm: 0.263672
=== Actor Training Debug (Iteration 5721) ===
Q mean: -12.552326
Q std: 19.428936
Actor loss: 12.556301
Action reg: 0.003975
  l1.weight: grad_norm = 0.173908
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.146946
Total gradient norm: 0.410531
=== Actor Training Debug (Iteration 5722) ===
Q mean: -12.727736
Q std: 20.392715
Actor loss: 12.731713
Action reg: 0.003978
  l1.weight: grad_norm = 0.220627
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.187404
Total gradient norm: 0.759944
=== Actor Training Debug (Iteration 5723) ===
Q mean: -15.354033
Q std: 20.493603
Actor loss: 15.358016
Action reg: 0.003983
  l1.weight: grad_norm = 0.195701
  l1.bias: grad_norm = 0.001320
  l2.weight: grad_norm = 0.136680
Total gradient norm: 0.410421
=== Actor Training Debug (Iteration 5724) ===
Q mean: -14.164434
Q std: 19.748037
Actor loss: 14.168401
Action reg: 0.003967
  l1.weight: grad_norm = 0.132334
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.090339
Total gradient norm: 0.281457
=== Actor Training Debug (Iteration 5725) ===
Q mean: -12.441786
Q std: 18.622255
Actor loss: 12.445759
Action reg: 0.003973
  l1.weight: grad_norm = 0.122822
  l1.bias: grad_norm = 0.001187
  l2.weight: grad_norm = 0.097873
Total gradient norm: 0.298608
=== Actor Training Debug (Iteration 5726) ===
Q mean: -15.692109
Q std: 21.750780
Actor loss: 15.696077
Action reg: 0.003968
  l1.weight: grad_norm = 0.145011
  l1.bias: grad_norm = 0.002136
  l2.weight: grad_norm = 0.121224
Total gradient norm: 0.408545
=== Actor Training Debug (Iteration 5727) ===
Q mean: -15.034842
Q std: 21.424433
Actor loss: 15.038820
Action reg: 0.003978
  l1.weight: grad_norm = 0.191921
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.141864
Total gradient norm: 0.554324
=== Actor Training Debug (Iteration 5728) ===
Q mean: -12.385462
Q std: 18.145353
Actor loss: 12.389442
Action reg: 0.003980
  l1.weight: grad_norm = 0.170331
  l1.bias: grad_norm = 0.000790
  l2.weight: grad_norm = 0.125974
Total gradient norm: 0.381477
=== Actor Training Debug (Iteration 5729) ===
Q mean: -11.181427
Q std: 18.660110
Actor loss: 11.185398
Action reg: 0.003971
  l1.weight: grad_norm = 0.124609
  l1.bias: grad_norm = 0.001742
  l2.weight: grad_norm = 0.095985
Total gradient norm: 0.339968
=== Actor Training Debug (Iteration 5730) ===
Q mean: -15.220192
Q std: 20.192583
Actor loss: 15.224175
Action reg: 0.003983
  l1.weight: grad_norm = 0.100839
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.079725
Total gradient norm: 0.252273
=== Actor Training Debug (Iteration 5731) ===
Q mean: -14.442783
Q std: 20.825464
Actor loss: 14.446763
Action reg: 0.003980
  l1.weight: grad_norm = 0.065512
  l1.bias: grad_norm = 0.000910
  l2.weight: grad_norm = 0.057293
Total gradient norm: 0.151253
=== Actor Training Debug (Iteration 5732) ===
Q mean: -12.451799
Q std: 18.667023
Actor loss: 12.455778
Action reg: 0.003979
  l1.weight: grad_norm = 0.291434
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.273523
Total gradient norm: 0.802979
=== Actor Training Debug (Iteration 5733) ===
Q mean: -15.044163
Q std: 21.333664
Actor loss: 15.048135
Action reg: 0.003972
  l1.weight: grad_norm = 0.064871
  l1.bias: grad_norm = 0.001059
  l2.weight: grad_norm = 0.051470
Total gradient norm: 0.162315
=== Actor Training Debug (Iteration 5734) ===
Q mean: -10.166739
Q std: 18.420443
Actor loss: 10.170718
Action reg: 0.003980
  l1.weight: grad_norm = 0.157942
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.135057
Total gradient norm: 0.380163
=== Actor Training Debug (Iteration 5735) ===
Q mean: -14.193039
Q std: 20.399511
Actor loss: 14.197007
Action reg: 0.003968
  l1.weight: grad_norm = 0.220565
  l1.bias: grad_norm = 0.000983
  l2.weight: grad_norm = 0.152432
Total gradient norm: 0.384474
=== Actor Training Debug (Iteration 5736) ===
Q mean: -12.728298
Q std: 18.818130
Actor loss: 12.732275
Action reg: 0.003977
  l1.weight: grad_norm = 0.073540
  l1.bias: grad_norm = 0.002432
  l2.weight: grad_norm = 0.053054
Total gradient norm: 0.172174
=== Actor Training Debug (Iteration 5737) ===
Q mean: -10.695114
Q std: 15.824061
Actor loss: 10.699091
Action reg: 0.003977
  l1.weight: grad_norm = 0.046204
  l1.bias: grad_norm = 0.000598
  l2.weight: grad_norm = 0.037579
Total gradient norm: 0.111889
=== Actor Training Debug (Iteration 5738) ===
Q mean: -12.622875
Q std: 17.715935
Actor loss: 12.626846
Action reg: 0.003971
  l1.weight: grad_norm = 0.206816
  l1.bias: grad_norm = 0.002139
  l2.weight: grad_norm = 0.183335
Total gradient norm: 0.457743
=== Actor Training Debug (Iteration 5739) ===
Q mean: -14.463881
Q std: 20.669828
Actor loss: 14.467844
Action reg: 0.003964
  l1.weight: grad_norm = 0.229514
  l1.bias: grad_norm = 0.000877
  l2.weight: grad_norm = 0.179505
Total gradient norm: 0.550268
=== Actor Training Debug (Iteration 5740) ===
Q mean: -10.672113
Q std: 17.603601
Actor loss: 10.676082
Action reg: 0.003968
  l1.weight: grad_norm = 0.185531
  l1.bias: grad_norm = 0.000727
  l2.weight: grad_norm = 0.121772
Total gradient norm: 0.413465
=== Actor Training Debug (Iteration 5741) ===
Q mean: -14.287616
Q std: 21.321703
Actor loss: 14.291590
Action reg: 0.003974
  l1.weight: grad_norm = 0.219097
  l1.bias: grad_norm = 0.001046
  l2.weight: grad_norm = 0.176038
Total gradient norm: 0.479853
=== Actor Training Debug (Iteration 5742) ===
Q mean: -14.142395
Q std: 19.846876
Actor loss: 14.146376
Action reg: 0.003980
  l1.weight: grad_norm = 0.351582
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.333977
Total gradient norm: 1.102248
=== Actor Training Debug (Iteration 5743) ===
Q mean: -11.837603
Q std: 18.124420
Actor loss: 11.841582
Action reg: 0.003980
  l1.weight: grad_norm = 0.178307
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.112859
Total gradient norm: 0.339324
=== Actor Training Debug (Iteration 5744) ===
Q mean: -12.232525
Q std: 18.655994
Actor loss: 12.236497
Action reg: 0.003972
  l1.weight: grad_norm = 0.174300
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.136217
Total gradient norm: 0.382708
=== Actor Training Debug (Iteration 5745) ===
Q mean: -15.435646
Q std: 21.367706
Actor loss: 15.439621
Action reg: 0.003975
  l1.weight: grad_norm = 0.093363
  l1.bias: grad_norm = 0.000725
  l2.weight: grad_norm = 0.084798
Total gradient norm: 0.241766
=== Actor Training Debug (Iteration 5746) ===
Q mean: -13.648602
Q std: 20.060703
Actor loss: 13.652574
Action reg: 0.003972
  l1.weight: grad_norm = 0.113040
  l1.bias: grad_norm = 0.001277
  l2.weight: grad_norm = 0.086921
Total gradient norm: 0.256522
=== Actor Training Debug (Iteration 5747) ===
Q mean: -13.776075
Q std: 18.757179
Actor loss: 13.780056
Action reg: 0.003981
  l1.weight: grad_norm = 0.143735
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.137473
Total gradient norm: 0.400567
=== Actor Training Debug (Iteration 5748) ===
Q mean: -15.652995
Q std: 21.065069
Actor loss: 15.656975
Action reg: 0.003980
  l1.weight: grad_norm = 0.142182
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.106487
Total gradient norm: 0.321539
=== Actor Training Debug (Iteration 5749) ===
Q mean: -13.108576
Q std: 19.532553
Actor loss: 13.112550
Action reg: 0.003974
  l1.weight: grad_norm = 0.199830
  l1.bias: grad_norm = 0.000675
  l2.weight: grad_norm = 0.139316
Total gradient norm: 0.438964
=== Actor Training Debug (Iteration 5750) ===
Q mean: -13.295203
Q std: 19.424004
Actor loss: 13.299190
Action reg: 0.003986
  l1.weight: grad_norm = 0.059036
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.061479
Total gradient norm: 0.203599
=== Actor Training Debug (Iteration 5751) ===
Q mean: -13.522768
Q std: 20.272255
Actor loss: 13.526748
Action reg: 0.003980
  l1.weight: grad_norm = 0.078715
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.065274
Total gradient norm: 0.195903
=== Actor Training Debug (Iteration 5752) ===
Q mean: -13.900036
Q std: 20.117586
Actor loss: 13.903992
Action reg: 0.003956
  l1.weight: grad_norm = 0.317061
  l1.bias: grad_norm = 0.000627
  l2.weight: grad_norm = 0.233289
Total gradient norm: 0.760068
=== Actor Training Debug (Iteration 5753) ===
Q mean: -13.653974
Q std: 18.746981
Actor loss: 13.657948
Action reg: 0.003975
  l1.weight: grad_norm = 0.243964
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.181296
Total gradient norm: 0.461387
=== Actor Training Debug (Iteration 5754) ===
Q mean: -13.945896
Q std: 19.774216
Actor loss: 13.949880
Action reg: 0.003983
  l1.weight: grad_norm = 0.456427
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.281996
Total gradient norm: 0.987638
=== Actor Training Debug (Iteration 5755) ===
Q mean: -13.188520
Q std: 20.882803
Actor loss: 13.192501
Action reg: 0.003980
  l1.weight: grad_norm = 0.106455
  l1.bias: grad_norm = 0.000578
  l2.weight: grad_norm = 0.074826
Total gradient norm: 0.220807
=== Actor Training Debug (Iteration 5756) ===
Q mean: -13.525144
Q std: 19.381117
Actor loss: 13.529122
Action reg: 0.003979
  l1.weight: grad_norm = 0.094000
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.067356
Total gradient norm: 0.194837
=== Actor Training Debug (Iteration 5757) ===
Q mean: -13.531620
Q std: 19.859432
Actor loss: 13.535591
Action reg: 0.003971
  l1.weight: grad_norm = 0.177268
  l1.bias: grad_norm = 0.000987
  l2.weight: grad_norm = 0.167717
Total gradient norm: 0.651064
=== Actor Training Debug (Iteration 5758) ===
Q mean: -14.569573
Q std: 20.393677
Actor loss: 14.573558
Action reg: 0.003984
  l1.weight: grad_norm = 0.055894
  l1.bias: grad_norm = 0.001999
  l2.weight: grad_norm = 0.044887
Total gradient norm: 0.164749
=== Actor Training Debug (Iteration 5759) ===
Q mean: -14.202013
Q std: 20.272476
Actor loss: 14.205980
Action reg: 0.003968
  l1.weight: grad_norm = 0.294107
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.274098
Total gradient norm: 0.972121
=== Actor Training Debug (Iteration 5760) ===
Q mean: -12.960518
Q std: 19.588060
Actor loss: 12.964492
Action reg: 0.003974
  l1.weight: grad_norm = 0.144854
  l1.bias: grad_norm = 0.002065
  l2.weight: grad_norm = 0.094917
Total gradient norm: 0.304713
=== Actor Training Debug (Iteration 5761) ===
Q mean: -11.814939
Q std: 18.738747
Actor loss: 11.818903
Action reg: 0.003963
  l1.weight: grad_norm = 0.545897
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.435109
Total gradient norm: 1.418475
=== Actor Training Debug (Iteration 5762) ===
Q mean: -13.584764
Q std: 20.164049
Actor loss: 13.588744
Action reg: 0.003980
  l1.weight: grad_norm = 0.222235
  l1.bias: grad_norm = 0.001410
  l2.weight: grad_norm = 0.133516
Total gradient norm: 0.431817
=== Actor Training Debug (Iteration 5763) ===
Q mean: -14.374385
Q std: 20.197853
Actor loss: 14.378375
Action reg: 0.003991
  l1.weight: grad_norm = 0.198183
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.130089
Total gradient norm: 0.474485
=== Actor Training Debug (Iteration 5764) ===
Q mean: -13.162023
Q std: 19.476763
Actor loss: 13.165998
Action reg: 0.003975
  l1.weight: grad_norm = 0.280067
  l1.bias: grad_norm = 0.000516
  l2.weight: grad_norm = 0.207819
Total gradient norm: 0.576277
=== Actor Training Debug (Iteration 5765) ===
Q mean: -12.369041
Q std: 19.397102
Actor loss: 12.373013
Action reg: 0.003971
  l1.weight: grad_norm = 0.280599
  l1.bias: grad_norm = 0.001061
  l2.weight: grad_norm = 0.214068
Total gradient norm: 0.665079
=== Actor Training Debug (Iteration 5766) ===
Q mean: -12.051218
Q std: 18.553747
Actor loss: 12.055194
Action reg: 0.003976
  l1.weight: grad_norm = 0.210589
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.140395
Total gradient norm: 0.439931
=== Actor Training Debug (Iteration 5767) ===
Q mean: -14.390154
Q std: 20.969744
Actor loss: 14.394138
Action reg: 0.003985
  l1.weight: grad_norm = 0.246399
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.174930
Total gradient norm: 0.667798
=== Actor Training Debug (Iteration 5768) ===
Q mean: -14.748195
Q std: 21.318338
Actor loss: 14.752166
Action reg: 0.003971
  l1.weight: grad_norm = 0.251521
  l1.bias: grad_norm = 0.000828
  l2.weight: grad_norm = 0.162932
Total gradient norm: 0.498206
=== Actor Training Debug (Iteration 5769) ===
Q mean: -15.593611
Q std: 20.121820
Actor loss: 15.597568
Action reg: 0.003957
  l1.weight: grad_norm = 0.242795
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.191122
Total gradient norm: 0.645345
=== Actor Training Debug (Iteration 5770) ===
Q mean: -12.358088
Q std: 19.833237
Actor loss: 12.362060
Action reg: 0.003972
  l1.weight: grad_norm = 0.248215
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.192125
Total gradient norm: 0.524780
=== Actor Training Debug (Iteration 5771) ===
Q mean: -14.594865
Q std: 20.809444
Actor loss: 14.598836
Action reg: 0.003971
  l1.weight: grad_norm = 0.144411
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.113635
Total gradient norm: 0.361138
=== Actor Training Debug (Iteration 5772) ===
Q mean: -14.373117
Q std: 19.624056
Actor loss: 14.377091
Action reg: 0.003974
  l1.weight: grad_norm = 0.171928
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.128144
Total gradient norm: 0.370530
=== Actor Training Debug (Iteration 5773) ===
Q mean: -13.026073
Q std: 19.660948
Actor loss: 13.030044
Action reg: 0.003970
  l1.weight: grad_norm = 0.185706
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.157475
Total gradient norm: 0.511965
=== Actor Training Debug (Iteration 5774) ===
Q mean: -14.025688
Q std: 20.647875
Actor loss: 14.029663
Action reg: 0.003975
  l1.weight: grad_norm = 0.107242
  l1.bias: grad_norm = 0.000824
  l2.weight: grad_norm = 0.080525
Total gradient norm: 0.231532
=== Actor Training Debug (Iteration 5775) ===
Q mean: -13.142299
Q std: 18.601942
Actor loss: 13.146276
Action reg: 0.003978
  l1.weight: grad_norm = 0.077201
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.070177
Total gradient norm: 0.193733
=== Actor Training Debug (Iteration 5776) ===
Q mean: -14.476569
Q std: 20.917711
Actor loss: 14.480539
Action reg: 0.003970
  l1.weight: grad_norm = 0.265077
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.208345
Total gradient norm: 0.644947
=== Actor Training Debug (Iteration 5777) ===
Q mean: -14.259633
Q std: 20.530910
Actor loss: 14.263613
Action reg: 0.003979
  l1.weight: grad_norm = 0.126962
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.102829
Total gradient norm: 0.287645
=== Actor Training Debug (Iteration 5778) ===
Q mean: -11.751401
Q std: 19.111662
Actor loss: 11.755382
Action reg: 0.003981
  l1.weight: grad_norm = 0.094398
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.085613
Total gradient norm: 0.273368
=== Actor Training Debug (Iteration 5779) ===
Q mean: -12.123750
Q std: 17.789310
Actor loss: 12.127717
Action reg: 0.003967
  l1.weight: grad_norm = 0.199356
  l1.bias: grad_norm = 0.001229
  l2.weight: grad_norm = 0.137958
Total gradient norm: 0.484253
=== Actor Training Debug (Iteration 5780) ===
Q mean: -11.806702
Q std: 18.275995
Actor loss: 11.810672
Action reg: 0.003970
  l1.weight: grad_norm = 0.203044
  l1.bias: grad_norm = 0.000590
  l2.weight: grad_norm = 0.168913
Total gradient norm: 0.482173
=== Actor Training Debug (Iteration 5781) ===
Q mean: -11.468892
Q std: 18.417261
Actor loss: 11.472863
Action reg: 0.003971
  l1.weight: grad_norm = 0.129450
  l1.bias: grad_norm = 0.000823
  l2.weight: grad_norm = 0.110007
Total gradient norm: 0.336302
=== Actor Training Debug (Iteration 5782) ===
Q mean: -11.802814
Q std: 18.303169
Actor loss: 11.806791
Action reg: 0.003977
  l1.weight: grad_norm = 0.148075
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.113884
Total gradient norm: 0.344264
=== Actor Training Debug (Iteration 5783) ===
Q mean: -14.281722
Q std: 19.312479
Actor loss: 14.285704
Action reg: 0.003981
  l1.weight: grad_norm = 0.169490
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.122984
Total gradient norm: 0.417234
=== Actor Training Debug (Iteration 5784) ===
Q mean: -12.396145
Q std: 20.225761
Actor loss: 12.400124
Action reg: 0.003979
  l1.weight: grad_norm = 0.335858
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.270691
Total gradient norm: 0.845197
=== Actor Training Debug (Iteration 5785) ===
Q mean: -13.829384
Q std: 20.658194
Actor loss: 13.833351
Action reg: 0.003967
  l1.weight: grad_norm = 0.274609
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.194094
Total gradient norm: 0.615625
=== Actor Training Debug (Iteration 5786) ===
Q mean: -13.497730
Q std: 20.627884
Actor loss: 13.501711
Action reg: 0.003981
  l1.weight: grad_norm = 0.094002
  l1.bias: grad_norm = 0.001132
  l2.weight: grad_norm = 0.063651
Total gradient norm: 0.192683
=== Actor Training Debug (Iteration 5787) ===
Q mean: -14.115538
Q std: 20.992887
Actor loss: 14.119504
Action reg: 0.003966
  l1.weight: grad_norm = 0.181418
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.139428
Total gradient norm: 0.572585
=== Actor Training Debug (Iteration 5788) ===
Q mean: -10.874191
Q std: 17.297371
Actor loss: 10.878166
Action reg: 0.003975
  l1.weight: grad_norm = 0.873165
  l1.bias: grad_norm = 0.001105
  l2.weight: grad_norm = 0.632187
Total gradient norm: 2.428144
=== Actor Training Debug (Iteration 5789) ===
Q mean: -15.728407
Q std: 20.269310
Actor loss: 15.732384
Action reg: 0.003977
  l1.weight: grad_norm = 0.108232
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.087812
Total gradient norm: 0.258891
=== Actor Training Debug (Iteration 5790) ===
Q mean: -13.042447
Q std: 19.614979
Actor loss: 13.046420
Action reg: 0.003973
  l1.weight: grad_norm = 0.156336
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.135551
Total gradient norm: 0.526793
=== Actor Training Debug (Iteration 5791) ===
Q mean: -13.305279
Q std: 19.985176
Actor loss: 13.309246
Action reg: 0.003967
  l1.weight: grad_norm = 0.319711
  l1.bias: grad_norm = 0.000691
  l2.weight: grad_norm = 0.237022
Total gradient norm: 0.869589
=== Actor Training Debug (Iteration 5792) ===
Q mean: -13.387954
Q std: 20.964649
Actor loss: 13.391924
Action reg: 0.003970
  l1.weight: grad_norm = 0.110001
  l1.bias: grad_norm = 0.001274
  l2.weight: grad_norm = 0.069809
Total gradient norm: 0.192728
=== Actor Training Debug (Iteration 5793) ===
Q mean: -15.436508
Q std: 22.141245
Actor loss: 15.440487
Action reg: 0.003979
  l1.weight: grad_norm = 0.183335
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.144564
Total gradient norm: 0.382950
=== Actor Training Debug (Iteration 5794) ===
Q mean: -12.499481
Q std: 18.307043
Actor loss: 12.503454
Action reg: 0.003973
  l1.weight: grad_norm = 0.315498
  l1.bias: grad_norm = 0.000857
  l2.weight: grad_norm = 0.190186
Total gradient norm: 0.661820
=== Actor Training Debug (Iteration 5795) ===
Q mean: -14.705548
Q std: 20.558407
Actor loss: 14.709523
Action reg: 0.003975
  l1.weight: grad_norm = 0.117456
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.089769
Total gradient norm: 0.328972
=== Actor Training Debug (Iteration 5796) ===
Q mean: -12.873510
Q std: 18.818092
Actor loss: 12.877478
Action reg: 0.003967
  l1.weight: grad_norm = 0.328451
  l1.bias: grad_norm = 0.000768
  l2.weight: grad_norm = 0.266456
Total gradient norm: 0.772789
=== Actor Training Debug (Iteration 5797) ===
Q mean: -15.183047
Q std: 22.038534
Actor loss: 15.187026
Action reg: 0.003979
  l1.weight: grad_norm = 0.277568
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.220349
Total gradient norm: 0.639731
=== Actor Training Debug (Iteration 5798) ===
Q mean: -15.572569
Q std: 20.949625
Actor loss: 15.576554
Action reg: 0.003985
  l1.weight: grad_norm = 0.221227
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.170546
Total gradient norm: 0.449712
=== Actor Training Debug (Iteration 5799) ===
Q mean: -12.549202
Q std: 19.110195
Actor loss: 12.553176
Action reg: 0.003974
  l1.weight: grad_norm = 0.117504
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.093158
Total gradient norm: 0.295591
=== Actor Training Debug (Iteration 5800) ===
Q mean: -11.979343
Q std: 18.816631
Actor loss: 11.983321
Action reg: 0.003978
  l1.weight: grad_norm = 0.114037
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.080237
Total gradient norm: 0.228777
=== Actor Training Debug (Iteration 5801) ===
Q mean: -13.653189
Q std: 19.010941
Actor loss: 13.657158
Action reg: 0.003969
  l1.weight: grad_norm = 0.133183
  l1.bias: grad_norm = 0.000772
  l2.weight: grad_norm = 0.106367
Total gradient norm: 0.352515
=== Actor Training Debug (Iteration 5802) ===
Q mean: -14.640951
Q std: 20.742907
Actor loss: 14.644923
Action reg: 0.003972
  l1.weight: grad_norm = 0.139517
  l1.bias: grad_norm = 0.000802
  l2.weight: grad_norm = 0.101731
Total gradient norm: 0.323964
=== Actor Training Debug (Iteration 5803) ===
Q mean: -12.622190
Q std: 19.282690
Actor loss: 12.626161
Action reg: 0.003970
  l1.weight: grad_norm = 0.131133
  l1.bias: grad_norm = 0.002209
  l2.weight: grad_norm = 0.111704
Total gradient norm: 0.333527
=== Actor Training Debug (Iteration 5804) ===
Q mean: -12.903936
Q std: 19.266941
Actor loss: 12.907914
Action reg: 0.003978
  l1.weight: grad_norm = 0.305358
  l1.bias: grad_norm = 0.000792
  l2.weight: grad_norm = 0.237106
Total gradient norm: 0.718438
=== Actor Training Debug (Iteration 5805) ===
Q mean: -14.063625
Q std: 20.453331
Actor loss: 14.067610
Action reg: 0.003984
  l1.weight: grad_norm = 0.139246
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.122517
Total gradient norm: 0.418095
=== Actor Training Debug (Iteration 5806) ===
Q mean: -12.578218
Q std: 19.394411
Actor loss: 12.582190
Action reg: 0.003971
  l1.weight: grad_norm = 0.601599
  l1.bias: grad_norm = 0.000984
  l2.weight: grad_norm = 0.393472
Total gradient norm: 1.183788
=== Actor Training Debug (Iteration 5807) ===
Q mean: -12.916563
Q std: 19.298819
Actor loss: 12.920539
Action reg: 0.003976
  l1.weight: grad_norm = 0.249033
  l1.bias: grad_norm = 0.001310
  l2.weight: grad_norm = 0.244557
Total gradient norm: 0.693632
=== Actor Training Debug (Iteration 5808) ===
Q mean: -13.376041
Q std: 19.833160
Actor loss: 13.380002
Action reg: 0.003961
  l1.weight: grad_norm = 0.214524
  l1.bias: grad_norm = 0.001530
  l2.weight: grad_norm = 0.170594
Total gradient norm: 0.495439
=== Actor Training Debug (Iteration 5809) ===
Q mean: -14.337563
Q std: 20.712482
Actor loss: 14.341546
Action reg: 0.003984
  l1.weight: grad_norm = 0.092620
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.070707
Total gradient norm: 0.237483
=== Actor Training Debug (Iteration 5810) ===
Q mean: -14.327373
Q std: 20.543312
Actor loss: 14.331356
Action reg: 0.003984
  l1.weight: grad_norm = 0.124928
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.090530
Total gradient norm: 0.251660
=== Actor Training Debug (Iteration 5811) ===
Q mean: -14.214022
Q std: 19.568394
Actor loss: 14.218007
Action reg: 0.003985
  l1.weight: grad_norm = 0.056541
  l1.bias: grad_norm = 0.001790
  l2.weight: grad_norm = 0.046342
Total gradient norm: 0.147117
=== Actor Training Debug (Iteration 5812) ===
Q mean: -12.826878
Q std: 17.975149
Actor loss: 12.830853
Action reg: 0.003976
  l1.weight: grad_norm = 0.353975
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.244559
Total gradient norm: 0.983170
=== Actor Training Debug (Iteration 5813) ===
Q mean: -12.659454
Q std: 19.792809
Actor loss: 12.663440
Action reg: 0.003985
  l1.weight: grad_norm = 0.087284
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.058149
Total gradient norm: 0.176624
=== Actor Training Debug (Iteration 5814) ===
Q mean: -15.294750
Q std: 21.463135
Actor loss: 15.298733
Action reg: 0.003982
  l1.weight: grad_norm = 0.050174
  l1.bias: grad_norm = 0.000862
  l2.weight: grad_norm = 0.041384
Total gradient norm: 0.126024
=== Actor Training Debug (Iteration 5815) ===
Q mean: -16.252323
Q std: 21.359589
Actor loss: 16.256296
Action reg: 0.003973
  l1.weight: grad_norm = 0.133429
  l1.bias: grad_norm = 0.001323
  l2.weight: grad_norm = 0.090200
Total gradient norm: 0.296467
=== Actor Training Debug (Iteration 5816) ===
Q mean: -13.864540
Q std: 19.266172
Actor loss: 13.868497
Action reg: 0.003956
  l1.weight: grad_norm = 0.137437
  l1.bias: grad_norm = 0.005767
  l2.weight: grad_norm = 0.112476
Total gradient norm: 0.423942
=== Actor Training Debug (Iteration 5817) ===
Q mean: -13.971857
Q std: 19.942209
Actor loss: 13.975837
Action reg: 0.003979
  l1.weight: grad_norm = 0.127382
  l1.bias: grad_norm = 0.001277
  l2.weight: grad_norm = 0.106580
Total gradient norm: 0.284073
=== Actor Training Debug (Iteration 5818) ===
Q mean: -11.635365
Q std: 20.612148
Actor loss: 11.639327
Action reg: 0.003963
  l1.weight: grad_norm = 0.371904
  l1.bias: grad_norm = 0.002972
  l2.weight: grad_norm = 0.234253
Total gradient norm: 0.826312
=== Actor Training Debug (Iteration 5819) ===
Q mean: -12.276814
Q std: 19.943680
Actor loss: 12.280791
Action reg: 0.003978
  l1.weight: grad_norm = 0.171105
  l1.bias: grad_norm = 0.001474
  l2.weight: grad_norm = 0.144341
Total gradient norm: 0.440836
=== Actor Training Debug (Iteration 5820) ===
Q mean: -13.672995
Q std: 20.688845
Actor loss: 13.676969
Action reg: 0.003974
  l1.weight: grad_norm = 0.213022
  l1.bias: grad_norm = 0.000716
  l2.weight: grad_norm = 0.182386
Total gradient norm: 0.484452
=== Actor Training Debug (Iteration 5821) ===
Q mean: -13.358213
Q std: 19.161413
Actor loss: 13.362197
Action reg: 0.003983
  l1.weight: grad_norm = 0.378288
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.261856
Total gradient norm: 0.906414
=== Actor Training Debug (Iteration 5822) ===
Q mean: -12.572678
Q std: 18.985527
Actor loss: 12.576644
Action reg: 0.003966
  l1.weight: grad_norm = 0.132435
  l1.bias: grad_norm = 0.001494
  l2.weight: grad_norm = 0.106089
Total gradient norm: 0.337732
=== Actor Training Debug (Iteration 5823) ===
Q mean: -11.190901
Q std: 18.525810
Actor loss: 11.194881
Action reg: 0.003981
  l1.weight: grad_norm = 0.269534
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.204764
Total gradient norm: 0.659561
=== Actor Training Debug (Iteration 5824) ===
Q mean: -15.100838
Q std: 19.874949
Actor loss: 15.104829
Action reg: 0.003991
  l1.weight: grad_norm = 0.047861
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.037937
Total gradient norm: 0.113578
=== Actor Training Debug (Iteration 5825) ===
Q mean: -12.615143
Q std: 18.749903
Actor loss: 12.619123
Action reg: 0.003981
  l1.weight: grad_norm = 0.102450
  l1.bias: grad_norm = 0.001422
  l2.weight: grad_norm = 0.088950
Total gradient norm: 0.226095
=== Actor Training Debug (Iteration 5826) ===
Q mean: -13.980521
Q std: 20.209597
Actor loss: 13.984505
Action reg: 0.003984
  l1.weight: grad_norm = 0.059022
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.046076
Total gradient norm: 0.155547
=== Actor Training Debug (Iteration 5827) ===
Q mean: -12.537280
Q std: 20.077257
Actor loss: 12.541246
Action reg: 0.003967
  l1.weight: grad_norm = 0.223612
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.180653
Total gradient norm: 0.522613
=== Actor Training Debug (Iteration 5828) ===
Q mean: -15.356852
Q std: 20.741053
Actor loss: 15.360832
Action reg: 0.003981
  l1.weight: grad_norm = 0.142233
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.102983
Total gradient norm: 0.329251
=== Actor Training Debug (Iteration 5829) ===
Q mean: -12.639238
Q std: 19.779362
Actor loss: 12.643215
Action reg: 0.003977
  l1.weight: grad_norm = 0.187454
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.131085
Total gradient norm: 0.392324
=== Actor Training Debug (Iteration 5830) ===
Q mean: -15.111422
Q std: 21.713564
Actor loss: 15.115396
Action reg: 0.003975
  l1.weight: grad_norm = 0.138355
  l1.bias: grad_norm = 0.001447
  l2.weight: grad_norm = 0.092168
Total gradient norm: 0.292862
=== Actor Training Debug (Iteration 5831) ===
Q mean: -13.768442
Q std: 20.582256
Actor loss: 13.772422
Action reg: 0.003979
  l1.weight: grad_norm = 0.166825
  l1.bias: grad_norm = 0.000813
  l2.weight: grad_norm = 0.120922
Total gradient norm: 0.448395
=== Actor Training Debug (Iteration 5832) ===
Q mean: -12.876125
Q std: 19.994205
Actor loss: 12.880099
Action reg: 0.003974
  l1.weight: grad_norm = 0.196144
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.157873
Total gradient norm: 0.534663
=== Actor Training Debug (Iteration 5833) ===
Q mean: -13.324781
Q std: 19.214575
Actor loss: 13.328753
Action reg: 0.003971
  l1.weight: grad_norm = 0.119544
  l1.bias: grad_norm = 0.000800
  l2.weight: grad_norm = 0.088762
Total gradient norm: 0.229724
=== Actor Training Debug (Iteration 5834) ===
Q mean: -16.459396
Q std: 20.183550
Actor loss: 16.463371
Action reg: 0.003975
  l1.weight: grad_norm = 0.220063
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.186349
Total gradient norm: 0.571811
=== Actor Training Debug (Iteration 5835) ===
Q mean: -12.845230
Q std: 19.160036
Actor loss: 12.849195
Action reg: 0.003966
  l1.weight: grad_norm = 0.236861
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.182131
Total gradient norm: 0.577666
=== Actor Training Debug (Iteration 5836) ===
Q mean: -13.423059
Q std: 20.247446
Actor loss: 13.427026
Action reg: 0.003967
  l1.weight: grad_norm = 0.235243
  l1.bias: grad_norm = 0.000524
  l2.weight: grad_norm = 0.228623
Total gradient norm: 0.797630
=== Actor Training Debug (Iteration 5837) ===
Q mean: -15.351631
Q std: 19.946329
Actor loss: 15.355605
Action reg: 0.003974
  l1.weight: grad_norm = 0.116799
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.091325
Total gradient norm: 0.264356
=== Actor Training Debug (Iteration 5838) ===
Q mean: -14.703813
Q std: 20.303629
Actor loss: 14.707780
Action reg: 0.003967
  l1.weight: grad_norm = 0.325943
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.259653
Total gradient norm: 0.919824
=== Actor Training Debug (Iteration 5839) ===
Q mean: -12.667696
Q std: 20.128391
Actor loss: 12.671687
Action reg: 0.003991
  l1.weight: grad_norm = 0.076422
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.051811
Total gradient norm: 0.160050
=== Actor Training Debug (Iteration 5840) ===
Q mean: -15.306597
Q std: 20.170639
Actor loss: 15.310581
Action reg: 0.003985
  l1.weight: grad_norm = 0.069990
  l1.bias: grad_norm = 0.000849
  l2.weight: grad_norm = 0.048857
Total gradient norm: 0.151714
=== Actor Training Debug (Iteration 5841) ===
Q mean: -14.989965
Q std: 20.555607
Actor loss: 14.993950
Action reg: 0.003985
  l1.weight: grad_norm = 0.199492
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.129364
Total gradient norm: 0.388316
=== Actor Training Debug (Iteration 5842) ===
Q mean: -13.188249
Q std: 19.912548
Actor loss: 13.192231
Action reg: 0.003982
  l1.weight: grad_norm = 0.100254
  l1.bias: grad_norm = 0.000791
  l2.weight: grad_norm = 0.088117
Total gradient norm: 0.232464
=== Actor Training Debug (Iteration 5843) ===
Q mean: -14.477730
Q std: 20.464344
Actor loss: 14.481696
Action reg: 0.003966
  l1.weight: grad_norm = 0.145624
  l1.bias: grad_norm = 0.000952
  l2.weight: grad_norm = 0.093245
Total gradient norm: 0.351267
=== Actor Training Debug (Iteration 5844) ===
Q mean: -14.219665
Q std: 20.617596
Actor loss: 14.223641
Action reg: 0.003977
  l1.weight: grad_norm = 0.273810
  l1.bias: grad_norm = 0.000565
  l2.weight: grad_norm = 0.189456
Total gradient norm: 0.536739
=== Actor Training Debug (Iteration 5845) ===
Q mean: -10.386106
Q std: 17.906191
Actor loss: 10.390082
Action reg: 0.003976
  l1.weight: grad_norm = 0.128747
  l1.bias: grad_norm = 0.000876
  l2.weight: grad_norm = 0.115947
Total gradient norm: 0.376095
=== Actor Training Debug (Iteration 5846) ===
Q mean: -13.003517
Q std: 19.680489
Actor loss: 13.007503
Action reg: 0.003986
  l1.weight: grad_norm = 0.159706
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.127703
Total gradient norm: 0.481535
=== Actor Training Debug (Iteration 5847) ===
Q mean: -11.499900
Q std: 18.824709
Actor loss: 11.503880
Action reg: 0.003980
  l1.weight: grad_norm = 0.165802
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.113732
Total gradient norm: 0.337276
=== Actor Training Debug (Iteration 5848) ===
Q mean: -15.431740
Q std: 20.568058
Actor loss: 15.435711
Action reg: 0.003971
  l1.weight: grad_norm = 0.192139
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.147027
Total gradient norm: 0.382339
=== Actor Training Debug (Iteration 5849) ===
Q mean: -15.369464
Q std: 21.162851
Actor loss: 15.373437
Action reg: 0.003973
  l1.weight: grad_norm = 0.154570
  l1.bias: grad_norm = 0.001251
  l2.weight: grad_norm = 0.116672
Total gradient norm: 0.339822
=== Actor Training Debug (Iteration 5850) ===
Q mean: -14.443125
Q std: 19.916420
Actor loss: 14.447107
Action reg: 0.003982
  l1.weight: grad_norm = 0.128151
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.100529
Total gradient norm: 0.287541
=== Actor Training Debug (Iteration 5851) ===
Q mean: -13.741281
Q std: 19.832315
Actor loss: 13.745259
Action reg: 0.003979
  l1.weight: grad_norm = 0.127486
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.088147
Total gradient norm: 0.282545
=== Actor Training Debug (Iteration 5852) ===
Q mean: -12.919443
Q std: 20.077538
Actor loss: 12.923420
Action reg: 0.003977
  l1.weight: grad_norm = 0.191600
  l1.bias: grad_norm = 0.000721
  l2.weight: grad_norm = 0.131437
Total gradient norm: 0.435539
=== Actor Training Debug (Iteration 5853) ===
Q mean: -14.077204
Q std: 21.211710
Actor loss: 14.081175
Action reg: 0.003971
  l1.weight: grad_norm = 0.212830
  l1.bias: grad_norm = 0.000699
  l2.weight: grad_norm = 0.143494
Total gradient norm: 0.427324
=== Actor Training Debug (Iteration 5854) ===
Q mean: -11.463655
Q std: 19.599194
Actor loss: 11.467635
Action reg: 0.003979
  l1.weight: grad_norm = 0.174514
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.125328
Total gradient norm: 0.403239
=== Actor Training Debug (Iteration 5855) ===
Q mean: -13.553864
Q std: 19.664608
Actor loss: 13.557836
Action reg: 0.003972
  l1.weight: grad_norm = 0.094264
  l1.bias: grad_norm = 0.000747
  l2.weight: grad_norm = 0.088714
Total gradient norm: 0.278446
=== Actor Training Debug (Iteration 5856) ===
Q mean: -13.600275
Q std: 20.021198
Actor loss: 13.604248
Action reg: 0.003973
  l1.weight: grad_norm = 0.164291
  l1.bias: grad_norm = 0.000678
  l2.weight: grad_norm = 0.104459
Total gradient norm: 0.323819
=== Actor Training Debug (Iteration 5857) ===
Q mean: -15.081923
Q std: 21.323299
Actor loss: 15.085906
Action reg: 0.003983
  l1.weight: grad_norm = 0.107985
  l1.bias: grad_norm = 0.001182
  l2.weight: grad_norm = 0.092823
Total gradient norm: 0.325816
=== Actor Training Debug (Iteration 5858) ===
Q mean: -14.706503
Q std: 21.149202
Actor loss: 14.710479
Action reg: 0.003975
  l1.weight: grad_norm = 0.155740
  l1.bias: grad_norm = 0.000648
  l2.weight: grad_norm = 0.111194
Total gradient norm: 0.386317
=== Actor Training Debug (Iteration 5859) ===
Q mean: -12.646884
Q std: 19.620308
Actor loss: 12.650863
Action reg: 0.003978
  l1.weight: grad_norm = 0.221565
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.142388
Total gradient norm: 0.470927
=== Actor Training Debug (Iteration 5860) ===
Q mean: -11.742243
Q std: 19.916843
Actor loss: 11.746209
Action reg: 0.003966
  l1.weight: grad_norm = 0.174894
  l1.bias: grad_norm = 0.002664
  l2.weight: grad_norm = 0.129060
Total gradient norm: 0.388402
=== Actor Training Debug (Iteration 5861) ===
Q mean: -13.515652
Q std: 20.571518
Actor loss: 13.519632
Action reg: 0.003980
  l1.weight: grad_norm = 0.153808
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.113416
Total gradient norm: 0.334304
=== Actor Training Debug (Iteration 5862) ===
Q mean: -13.216814
Q std: 19.372536
Actor loss: 13.220797
Action reg: 0.003982
  l1.weight: grad_norm = 0.177029
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.141670
Total gradient norm: 0.409729
=== Actor Training Debug (Iteration 5863) ===
Q mean: -12.885132
Q std: 18.924026
Actor loss: 12.889112
Action reg: 0.003980
  l1.weight: grad_norm = 0.134438
  l1.bias: grad_norm = 0.000670
  l2.weight: grad_norm = 0.101238
Total gradient norm: 0.329998
=== Actor Training Debug (Iteration 5864) ===
Q mean: -11.335649
Q std: 17.833937
Actor loss: 11.339613
Action reg: 0.003964
  l1.weight: grad_norm = 0.103434
  l1.bias: grad_norm = 0.001093
  l2.weight: grad_norm = 0.077958
Total gradient norm: 0.282135
=== Actor Training Debug (Iteration 5865) ===
Q mean: -13.334080
Q std: 19.855747
Actor loss: 13.338058
Action reg: 0.003978
  l1.weight: grad_norm = 0.141513
  l1.bias: grad_norm = 0.000610
  l2.weight: grad_norm = 0.108433
Total gradient norm: 0.292610
=== Actor Training Debug (Iteration 5866) ===
Q mean: -13.567943
Q std: 19.723431
Actor loss: 13.571920
Action reg: 0.003978
  l1.weight: grad_norm = 0.298541
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.202822
Total gradient norm: 0.707597
=== Actor Training Debug (Iteration 5867) ===
Q mean: -15.215484
Q std: 20.866796
Actor loss: 15.219463
Action reg: 0.003980
  l1.weight: grad_norm = 0.281837
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.220717
Total gradient norm: 0.693174
=== Actor Training Debug (Iteration 5868) ===
Q mean: -13.674727
Q std: 21.139748
Actor loss: 13.678711
Action reg: 0.003983
  l1.weight: grad_norm = 0.121749
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.101460
Total gradient norm: 0.330431
=== Actor Training Debug (Iteration 5869) ===
Q mean: -13.465010
Q std: 19.427773
Actor loss: 13.468990
Action reg: 0.003981
  l1.weight: grad_norm = 0.049633
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.041190
Total gradient norm: 0.118034
=== Actor Training Debug (Iteration 5870) ===
Q mean: -12.677258
Q std: 19.322725
Actor loss: 12.681232
Action reg: 0.003974
  l1.weight: grad_norm = 0.133357
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.111747
Total gradient norm: 0.347567
=== Actor Training Debug (Iteration 5871) ===
Q mean: -13.896254
Q std: 20.732807
Actor loss: 13.900223
Action reg: 0.003969
  l1.weight: grad_norm = 0.219672
  l1.bias: grad_norm = 0.000571
  l2.weight: grad_norm = 0.176349
Total gradient norm: 0.570123
=== Actor Training Debug (Iteration 5872) ===
Q mean: -13.990734
Q std: 19.870794
Actor loss: 13.994704
Action reg: 0.003970
  l1.weight: grad_norm = 0.283927
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.199967
Total gradient norm: 0.591662
=== Actor Training Debug (Iteration 5873) ===
Q mean: -14.651276
Q std: 20.637384
Actor loss: 14.655257
Action reg: 0.003981
  l1.weight: grad_norm = 0.430876
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.302743
Total gradient norm: 1.137636
=== Actor Training Debug (Iteration 5874) ===
Q mean: -14.019673
Q std: 20.883104
Actor loss: 14.023661
Action reg: 0.003988
  l1.weight: grad_norm = 0.396384
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.395868
Total gradient norm: 1.079081
=== Actor Training Debug (Iteration 5875) ===
Q mean: -13.514097
Q std: 19.120512
Actor loss: 13.518077
Action reg: 0.003980
  l1.weight: grad_norm = 0.117659
  l1.bias: grad_norm = 0.000950
  l2.weight: grad_norm = 0.078122
Total gradient norm: 0.221086
=== Actor Training Debug (Iteration 5876) ===
Q mean: -12.594809
Q std: 19.798832
Actor loss: 12.598780
Action reg: 0.003971
  l1.weight: grad_norm = 0.248730
  l1.bias: grad_norm = 0.000552
  l2.weight: grad_norm = 0.195904
Total gradient norm: 0.617275
=== Actor Training Debug (Iteration 5877) ===
Q mean: -14.018292
Q std: 20.455242
Actor loss: 14.022281
Action reg: 0.003988
  l1.weight: grad_norm = 0.179743
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.143131
Total gradient norm: 0.529341
=== Actor Training Debug (Iteration 5878) ===
Q mean: -11.384597
Q std: 17.976719
Actor loss: 11.388580
Action reg: 0.003984
  l1.weight: grad_norm = 0.164456
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.139365
Total gradient norm: 0.380356
=== Actor Training Debug (Iteration 5879) ===
Q mean: -11.269691
Q std: 19.648287
Actor loss: 11.273671
Action reg: 0.003980
  l1.weight: grad_norm = 0.131937
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.107034
Total gradient norm: 0.340422
=== Actor Training Debug (Iteration 5880) ===
Q mean: -14.491597
Q std: 19.977640
Actor loss: 14.495579
Action reg: 0.003982
  l1.weight: grad_norm = 0.067075
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.052935
Total gradient norm: 0.147309
=== Actor Training Debug (Iteration 5881) ===
Q mean: -14.739618
Q std: 19.689632
Actor loss: 14.743602
Action reg: 0.003983
  l1.weight: grad_norm = 0.036600
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.028143
Total gradient norm: 0.089449
=== Actor Training Debug (Iteration 5882) ===
Q mean: -14.758428
Q std: 20.644917
Actor loss: 14.762404
Action reg: 0.003977
  l1.weight: grad_norm = 0.319431
  l1.bias: grad_norm = 0.000987
  l2.weight: grad_norm = 0.281788
Total gradient norm: 0.725296
=== Actor Training Debug (Iteration 5883) ===
Q mean: -14.148193
Q std: 19.697374
Actor loss: 14.152179
Action reg: 0.003986
  l1.weight: grad_norm = 0.065442
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.051136
Total gradient norm: 0.155365
=== Actor Training Debug (Iteration 5884) ===
Q mean: -15.506578
Q std: 22.101030
Actor loss: 15.510559
Action reg: 0.003980
  l1.weight: grad_norm = 0.127456
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.095359
Total gradient norm: 0.274923
=== Actor Training Debug (Iteration 5885) ===
Q mean: -11.512878
Q std: 17.833164
Actor loss: 11.516843
Action reg: 0.003964
  l1.weight: grad_norm = 0.132290
  l1.bias: grad_norm = 0.001106
  l2.weight: grad_norm = 0.108866
Total gradient norm: 0.301629
=== Actor Training Debug (Iteration 5886) ===
Q mean: -11.398977
Q std: 18.133078
Actor loss: 11.402946
Action reg: 0.003968
  l1.weight: grad_norm = 0.191190
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.146581
Total gradient norm: 0.433897
=== Actor Training Debug (Iteration 5887) ===
Q mean: -14.079391
Q std: 20.727243
Actor loss: 14.083370
Action reg: 0.003980
  l1.weight: grad_norm = 0.106010
  l1.bias: grad_norm = 0.000908
  l2.weight: grad_norm = 0.080509
Total gradient norm: 0.240832
=== Actor Training Debug (Iteration 5888) ===
Q mean: -14.200155
Q std: 20.698454
Actor loss: 14.204131
Action reg: 0.003975
  l1.weight: grad_norm = 0.330600
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.263984
Total gradient norm: 0.703617
=== Actor Training Debug (Iteration 5889) ===
Q mean: -15.702932
Q std: 21.472311
Actor loss: 15.706902
Action reg: 0.003969
  l1.weight: grad_norm = 0.266820
  l1.bias: grad_norm = 0.002741
  l2.weight: grad_norm = 0.181482
Total gradient norm: 0.590264
=== Actor Training Debug (Iteration 5890) ===
Q mean: -13.222188
Q std: 20.208019
Actor loss: 13.226174
Action reg: 0.003986
  l1.weight: grad_norm = 0.081654
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.065754
Total gradient norm: 0.188950
=== Actor Training Debug (Iteration 5891) ===
Q mean: -14.888988
Q std: 19.526508
Actor loss: 14.892974
Action reg: 0.003986
  l1.weight: grad_norm = 0.256418
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.201959
Total gradient norm: 0.791325
=== Actor Training Debug (Iteration 5892) ===
Q mean: -12.247978
Q std: 19.059435
Actor loss: 12.251958
Action reg: 0.003980
  l1.weight: grad_norm = 0.166388
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.145552
Total gradient norm: 0.445023
=== Actor Training Debug (Iteration 5893) ===
Q mean: -15.820939
Q std: 20.839737
Actor loss: 15.824923
Action reg: 0.003984
  l1.weight: grad_norm = 0.073087
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.061628
Total gradient norm: 0.201687
=== Actor Training Debug (Iteration 5894) ===
Q mean: -13.218822
Q std: 19.842667
Actor loss: 13.222796
Action reg: 0.003974
  l1.weight: grad_norm = 0.229949
  l1.bias: grad_norm = 0.002398
  l2.weight: grad_norm = 0.188816
Total gradient norm: 0.557325
=== Actor Training Debug (Iteration 5895) ===
Q mean: -14.391825
Q std: 21.350241
Actor loss: 14.395802
Action reg: 0.003977
  l1.weight: grad_norm = 0.132456
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.111762
Total gradient norm: 0.344946
=== Actor Training Debug (Iteration 5896) ===
Q mean: -15.845860
Q std: 21.311937
Actor loss: 15.849848
Action reg: 0.003987
  l1.weight: grad_norm = 0.149296
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.125113
Total gradient norm: 0.497372
=== Actor Training Debug (Iteration 5897) ===
Q mean: -14.825776
Q std: 20.527216
Actor loss: 14.829757
Action reg: 0.003981
  l1.weight: grad_norm = 0.098967
  l1.bias: grad_norm = 0.003437
  l2.weight: grad_norm = 0.072657
Total gradient norm: 0.238093
=== Actor Training Debug (Iteration 5898) ===
Q mean: -11.639466
Q std: 19.001774
Actor loss: 11.643455
Action reg: 0.003988
  l1.weight: grad_norm = 0.023209
  l1.bias: grad_norm = 0.000810
  l2.weight: grad_norm = 0.019441
Total gradient norm: 0.058881
=== Actor Training Debug (Iteration 5899) ===
Q mean: -15.525073
Q std: 21.595411
Actor loss: 15.529047
Action reg: 0.003974
  l1.weight: grad_norm = 0.168690
  l1.bias: grad_norm = 0.002712
  l2.weight: grad_norm = 0.140187
Total gradient norm: 0.433270
=== Actor Training Debug (Iteration 5900) ===
Q mean: -12.652884
Q std: 19.180103
Actor loss: 12.656853
Action reg: 0.003969
  l1.weight: grad_norm = 0.197294
  l1.bias: grad_norm = 0.002774
  l2.weight: grad_norm = 0.163717
Total gradient norm: 0.489341
=== Actor Training Debug (Iteration 5901) ===
Q mean: -14.613915
Q std: 20.139967
Actor loss: 14.617893
Action reg: 0.003977
  l1.weight: grad_norm = 0.254152
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.182563
Total gradient norm: 0.528674
=== Actor Training Debug (Iteration 5902) ===
Q mean: -14.522470
Q std: 19.906971
Actor loss: 14.526447
Action reg: 0.003977
  l1.weight: grad_norm = 0.134543
  l1.bias: grad_norm = 0.001747
  l2.weight: grad_norm = 0.093235
Total gradient norm: 0.272414
=== Actor Training Debug (Iteration 5903) ===
Q mean: -13.515257
Q std: 20.330717
Actor loss: 13.519232
Action reg: 0.003975
  l1.weight: grad_norm = 0.236383
  l1.bias: grad_norm = 0.000955
  l2.weight: grad_norm = 0.171143
Total gradient norm: 0.530697
=== Actor Training Debug (Iteration 5904) ===
Q mean: -13.745871
Q std: 19.353168
Actor loss: 13.749841
Action reg: 0.003970
  l1.weight: grad_norm = 0.190561
  l1.bias: grad_norm = 0.000812
  l2.weight: grad_norm = 0.159517
Total gradient norm: 0.488313
=== Actor Training Debug (Iteration 5905) ===
Q mean: -14.377878
Q std: 20.801846
Actor loss: 14.381860
Action reg: 0.003982
  l1.weight: grad_norm = 0.138667
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.100089
Total gradient norm: 0.312489
=== Actor Training Debug (Iteration 5906) ===
Q mean: -13.652988
Q std: 19.860565
Actor loss: 13.656959
Action reg: 0.003970
  l1.weight: grad_norm = 0.297240
  l1.bias: grad_norm = 0.000535
  l2.weight: grad_norm = 0.240948
Total gradient norm: 0.817868
=== Actor Training Debug (Iteration 5907) ===
Q mean: -12.334516
Q std: 17.985960
Actor loss: 12.338499
Action reg: 0.003984
  l1.weight: grad_norm = 0.123919
  l1.bias: grad_norm = 0.000783
  l2.weight: grad_norm = 0.090004
Total gradient norm: 0.248100
=== Actor Training Debug (Iteration 5908) ===
Q mean: -12.493681
Q std: 18.924290
Actor loss: 12.497661
Action reg: 0.003980
  l1.weight: grad_norm = 0.129856
  l1.bias: grad_norm = 0.000697
  l2.weight: grad_norm = 0.098107
Total gradient norm: 0.302190
=== Actor Training Debug (Iteration 5909) ===
Q mean: -13.723363
Q std: 19.811695
Actor loss: 13.727335
Action reg: 0.003972
  l1.weight: grad_norm = 0.178511
  l1.bias: grad_norm = 0.001780
  l2.weight: grad_norm = 0.134532
Total gradient norm: 0.404318
=== Actor Training Debug (Iteration 5910) ===
Q mean: -14.349422
Q std: 20.724489
Actor loss: 14.353400
Action reg: 0.003978
  l1.weight: grad_norm = 0.135513
  l1.bias: grad_norm = 0.000640
  l2.weight: grad_norm = 0.105937
Total gradient norm: 0.318126
=== Actor Training Debug (Iteration 5911) ===
Q mean: -15.288199
Q std: 21.660465
Actor loss: 15.292176
Action reg: 0.003977
  l1.weight: grad_norm = 0.150970
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.111855
Total gradient norm: 0.314351
=== Actor Training Debug (Iteration 5912) ===
Q mean: -14.948247
Q std: 21.162941
Actor loss: 14.952233
Action reg: 0.003986
  l1.weight: grad_norm = 0.091782
  l1.bias: grad_norm = 0.000577
  l2.weight: grad_norm = 0.071267
Total gradient norm: 0.259020
=== Actor Training Debug (Iteration 5913) ===
Q mean: -15.421762
Q std: 21.257299
Actor loss: 15.425749
Action reg: 0.003986
  l1.weight: grad_norm = 0.043802
  l1.bias: grad_norm = 0.000701
  l2.weight: grad_norm = 0.032887
Total gradient norm: 0.106302
=== Actor Training Debug (Iteration 5914) ===
Q mean: -10.891177
Q std: 18.314222
Actor loss: 10.895162
Action reg: 0.003984
  l1.weight: grad_norm = 0.083531
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.057930
Total gradient norm: 0.205831
=== Actor Training Debug (Iteration 5915) ===
Q mean: -13.599757
Q std: 20.679279
Actor loss: 13.603731
Action reg: 0.003974
  l1.weight: grad_norm = 0.101364
  l1.bias: grad_norm = 0.000695
  l2.weight: grad_norm = 0.085413
Total gradient norm: 0.282488
=== Actor Training Debug (Iteration 5916) ===
Q mean: -12.662068
Q std: 18.522982
Actor loss: 12.666050
Action reg: 0.003982
  l1.weight: grad_norm = 0.200381
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.160362
Total gradient norm: 0.498339
=== Actor Training Debug (Iteration 5917) ===
Q mean: -13.227299
Q std: 19.472567
Actor loss: 13.231284
Action reg: 0.003986
  l1.weight: grad_norm = 0.083928
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.068099
Total gradient norm: 0.217646
=== Actor Training Debug (Iteration 5918) ===
Q mean: -14.916559
Q std: 18.603216
Actor loss: 14.920533
Action reg: 0.003974
  l1.weight: grad_norm = 0.070314
  l1.bias: grad_norm = 0.001191
  l2.weight: grad_norm = 0.057208
Total gradient norm: 0.193867
=== Actor Training Debug (Iteration 5919) ===
Q mean: -14.276239
Q std: 19.839973
Actor loss: 14.280209
Action reg: 0.003969
  l1.weight: grad_norm = 0.293678
  l1.bias: grad_norm = 0.000461
  l2.weight: grad_norm = 0.236414
Total gradient norm: 0.724533
=== Actor Training Debug (Iteration 5920) ===
Q mean: -14.010190
Q std: 19.995689
Actor loss: 14.014173
Action reg: 0.003982
  l1.weight: grad_norm = 0.152052
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.119357
Total gradient norm: 0.371588
=== Actor Training Debug (Iteration 5921) ===
Q mean: -12.730437
Q std: 20.018200
Actor loss: 12.734409
Action reg: 0.003972
  l1.weight: grad_norm = 0.255867
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.196185
Total gradient norm: 0.578696
=== Actor Training Debug (Iteration 5922) ===
Q mean: -12.223350
Q std: 19.498627
Actor loss: 12.227326
Action reg: 0.003976
  l1.weight: grad_norm = 0.160044
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.119448
Total gradient norm: 0.345918
=== Actor Training Debug (Iteration 5923) ===
Q mean: -13.630634
Q std: 20.109493
Actor loss: 13.634613
Action reg: 0.003979
  l1.weight: grad_norm = 0.132570
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.095175
Total gradient norm: 0.371150
=== Actor Training Debug (Iteration 5924) ===
Q mean: -13.462431
Q std: 17.926550
Actor loss: 13.466412
Action reg: 0.003981
  l1.weight: grad_norm = 0.156850
  l1.bias: grad_norm = 0.000837
  l2.weight: grad_norm = 0.117994
Total gradient norm: 0.347770
=== Actor Training Debug (Iteration 5925) ===
Q mean: -13.313175
Q std: 20.995350
Actor loss: 13.317163
Action reg: 0.003987
  l1.weight: grad_norm = 0.080926
  l1.bias: grad_norm = 0.000435
  l2.weight: grad_norm = 0.072357
Total gradient norm: 0.218030
=== Actor Training Debug (Iteration 5926) ===
Q mean: -12.315529
Q std: 18.959929
Actor loss: 12.319513
Action reg: 0.003985
  l1.weight: grad_norm = 0.194345
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.136597
Total gradient norm: 0.627582
=== Actor Training Debug (Iteration 5927) ===
Q mean: -11.262312
Q std: 18.928652
Actor loss: 11.266300
Action reg: 0.003988
  l1.weight: grad_norm = 0.127119
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.099442
Total gradient norm: 0.360434
=== Actor Training Debug (Iteration 5928) ===
Q mean: -12.548632
Q std: 19.885757
Actor loss: 12.552603
Action reg: 0.003971
  l1.weight: grad_norm = 0.172881
  l1.bias: grad_norm = 0.000682
  l2.weight: grad_norm = 0.142117
Total gradient norm: 0.434010
=== Actor Training Debug (Iteration 5929) ===
Q mean: -13.928747
Q std: 20.447264
Actor loss: 13.932714
Action reg: 0.003968
  l1.weight: grad_norm = 0.197013
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.146655
Total gradient norm: 0.506224
=== Actor Training Debug (Iteration 5930) ===
Q mean: -15.134089
Q std: 22.240019
Actor loss: 15.138068
Action reg: 0.003979
  l1.weight: grad_norm = 0.181494
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.140970
Total gradient norm: 0.456376
=== Actor Training Debug (Iteration 5931) ===
Q mean: -15.455847
Q std: 21.414928
Actor loss: 15.459819
Action reg: 0.003972
  l1.weight: grad_norm = 0.237697
  l1.bias: grad_norm = 0.000780
  l2.weight: grad_norm = 0.183713
Total gradient norm: 0.487633
=== Actor Training Debug (Iteration 5932) ===
Q mean: -13.993424
Q std: 21.569490
Actor loss: 13.997401
Action reg: 0.003976
  l1.weight: grad_norm = 0.144260
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.121095
Total gradient norm: 0.423323
=== Actor Training Debug (Iteration 5933) ===
Q mean: -14.155778
Q std: 19.866983
Actor loss: 14.159765
Action reg: 0.003988
  l1.weight: grad_norm = 0.268034
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.185112
Total gradient norm: 0.682305
=== Actor Training Debug (Iteration 5934) ===
Q mean: -16.357979
Q std: 21.050123
Actor loss: 16.361956
Action reg: 0.003977
  l1.weight: grad_norm = 0.124550
  l1.bias: grad_norm = 0.000651
  l2.weight: grad_norm = 0.081870
Total gradient norm: 0.254912
=== Actor Training Debug (Iteration 5935) ===
Q mean: -16.570086
Q std: 20.775333
Actor loss: 16.574070
Action reg: 0.003984
  l1.weight: grad_norm = 0.079545
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.050662
Total gradient norm: 0.146383
=== Actor Training Debug (Iteration 5936) ===
Q mean: -12.657606
Q std: 18.890013
Actor loss: 12.661581
Action reg: 0.003975
  l1.weight: grad_norm = 0.229460
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.178114
Total gradient norm: 0.539090
=== Actor Training Debug (Iteration 5937) ===
Q mean: -13.920952
Q std: 20.539131
Actor loss: 13.924920
Action reg: 0.003968
  l1.weight: grad_norm = 0.175370
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.128257
Total gradient norm: 0.400692
=== Actor Training Debug (Iteration 5938) ===
Q mean: -15.444270
Q std: 21.060450
Actor loss: 15.448243
Action reg: 0.003973
  l1.weight: grad_norm = 0.112476
  l1.bias: grad_norm = 0.001115
  l2.weight: grad_norm = 0.084131
Total gradient norm: 0.243479
=== Actor Training Debug (Iteration 5939) ===
Q mean: -17.255304
Q std: 21.739000
Actor loss: 17.259274
Action reg: 0.003970
  l1.weight: grad_norm = 0.239787
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.173069
Total gradient norm: 0.554555
=== Actor Training Debug (Iteration 5940) ===
Q mean: -13.846067
Q std: 20.779648
Actor loss: 13.850041
Action reg: 0.003974
  l1.weight: grad_norm = 0.176481
  l1.bias: grad_norm = 0.001388
  l2.weight: grad_norm = 0.147273
Total gradient norm: 0.438226
=== Actor Training Debug (Iteration 5941) ===
Q mean: -14.852305
Q std: 20.342600
Actor loss: 14.856279
Action reg: 0.003974
  l1.weight: grad_norm = 0.146481
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.115019
Total gradient norm: 0.339829
=== Actor Training Debug (Iteration 5942) ===
Q mean: -15.056025
Q std: 20.518837
Actor loss: 15.060008
Action reg: 0.003984
  l1.weight: grad_norm = 0.094984
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.068802
Total gradient norm: 0.250245
=== Actor Training Debug (Iteration 5943) ===
Q mean: -14.269093
Q std: 19.553635
Actor loss: 14.273059
Action reg: 0.003967
  l1.weight: grad_norm = 0.210871
  l1.bias: grad_norm = 0.001055
  l2.weight: grad_norm = 0.171516
Total gradient norm: 0.523305
=== Actor Training Debug (Iteration 5944) ===
Q mean: -12.488781
Q std: 18.857553
Actor loss: 12.492761
Action reg: 0.003980
  l1.weight: grad_norm = 0.138107
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.110808
Total gradient norm: 0.295200
=== Actor Training Debug (Iteration 5945) ===
Q mean: -13.114881
Q std: 19.361938
Actor loss: 13.118852
Action reg: 0.003972
  l1.weight: grad_norm = 0.336192
  l1.bias: grad_norm = 0.000984
  l2.weight: grad_norm = 0.279119
Total gradient norm: 0.828070
=== Actor Training Debug (Iteration 5946) ===
Q mean: -13.780993
Q std: 19.623583
Actor loss: 13.784979
Action reg: 0.003987
  l1.weight: grad_norm = 0.103528
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.060380
Total gradient norm: 0.200738
=== Actor Training Debug (Iteration 5947) ===
Q mean: -13.943430
Q std: 20.620237
Actor loss: 13.947402
Action reg: 0.003972
  l1.weight: grad_norm = 0.164151
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.125389
Total gradient norm: 0.349795
=== Actor Training Debug (Iteration 5948) ===
Q mean: -12.181842
Q std: 19.096985
Actor loss: 12.185819
Action reg: 0.003977
  l1.weight: grad_norm = 0.206279
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.176008
Total gradient norm: 0.493420
=== Actor Training Debug (Iteration 5949) ===
Q mean: -14.436876
Q std: 20.458679
Actor loss: 14.440861
Action reg: 0.003984
  l1.weight: grad_norm = 0.098612
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.088140
Total gradient norm: 0.279104
=== Actor Training Debug (Iteration 5950) ===
Q mean: -13.851851
Q std: 20.698107
Actor loss: 13.855826
Action reg: 0.003976
  l1.weight: grad_norm = 0.161425
  l1.bias: grad_norm = 0.000876
  l2.weight: grad_norm = 0.137084
Total gradient norm: 0.400702
=== Actor Training Debug (Iteration 5951) ===
Q mean: -12.301949
Q std: 19.133417
Actor loss: 12.305931
Action reg: 0.003983
  l1.weight: grad_norm = 0.118968
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.098505
Total gradient norm: 0.290439
=== Actor Training Debug (Iteration 5952) ===
Q mean: -15.036259
Q std: 21.938988
Actor loss: 15.040233
Action reg: 0.003974
  l1.weight: grad_norm = 0.152169
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.106249
Total gradient norm: 0.301798
=== Actor Training Debug (Iteration 5953) ===
Q mean: -13.726341
Q std: 20.182064
Actor loss: 13.730309
Action reg: 0.003968
  l1.weight: grad_norm = 0.189923
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.154384
Total gradient norm: 0.423410
=== Actor Training Debug (Iteration 5954) ===
Q mean: -13.770866
Q std: 19.896265
Actor loss: 13.774840
Action reg: 0.003974
  l1.weight: grad_norm = 0.116456
  l1.bias: grad_norm = 0.002338
  l2.weight: grad_norm = 0.099485
Total gradient norm: 0.335640
=== Actor Training Debug (Iteration 5955) ===
Q mean: -13.908913
Q std: 19.081537
Actor loss: 13.912886
Action reg: 0.003973
  l1.weight: grad_norm = 0.103917
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.087055
Total gradient norm: 0.300078
=== Actor Training Debug (Iteration 5956) ===
Q mean: -13.017852
Q std: 18.629427
Actor loss: 13.021833
Action reg: 0.003982
  l1.weight: grad_norm = 0.124020
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.102508
Total gradient norm: 0.286579
=== Actor Training Debug (Iteration 5957) ===
Q mean: -14.369506
Q std: 20.367834
Actor loss: 14.373483
Action reg: 0.003977
  l1.weight: grad_norm = 0.235247
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.199660
Total gradient norm: 0.645580
=== Actor Training Debug (Iteration 5958) ===
Q mean: -12.937733
Q std: 18.944944
Actor loss: 12.941710
Action reg: 0.003976
  l1.weight: grad_norm = 0.092921
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.076164
Total gradient norm: 0.225326
=== Actor Training Debug (Iteration 5959) ===
Q mean: -12.845588
Q std: 19.801212
Actor loss: 12.849549
Action reg: 0.003962
  l1.weight: grad_norm = 0.454330
  l1.bias: grad_norm = 0.001711
  l2.weight: grad_norm = 0.351106
Total gradient norm: 1.214353
=== Actor Training Debug (Iteration 5960) ===
Q mean: -13.764133
Q std: 19.756948
Actor loss: 13.768118
Action reg: 0.003984
  l1.weight: grad_norm = 0.210232
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.139831
Total gradient norm: 0.439221
=== Actor Training Debug (Iteration 5961) ===
Q mean: -13.684137
Q std: 19.736967
Actor loss: 13.688121
Action reg: 0.003983
  l1.weight: grad_norm = 0.042555
  l1.bias: grad_norm = 0.000704
  l2.weight: grad_norm = 0.034559
Total gradient norm: 0.102510
=== Actor Training Debug (Iteration 5962) ===
Q mean: -12.732519
Q std: 18.833607
Actor loss: 12.736496
Action reg: 0.003977
  l1.weight: grad_norm = 0.169971
  l1.bias: grad_norm = 0.002027
  l2.weight: grad_norm = 0.154632
Total gradient norm: 0.442418
=== Actor Training Debug (Iteration 5963) ===
Q mean: -15.114158
Q std: 20.652096
Actor loss: 15.118119
Action reg: 0.003962
  l1.weight: grad_norm = 0.278153
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.166253
Total gradient norm: 0.569768
=== Actor Training Debug (Iteration 5964) ===
Q mean: -15.720427
Q std: 19.678974
Actor loss: 15.724404
Action reg: 0.003978
  l1.weight: grad_norm = 0.026851
  l1.bias: grad_norm = 0.002580
  l2.weight: grad_norm = 0.024603
Total gradient norm: 0.086945
=== Actor Training Debug (Iteration 5965) ===
Q mean: -15.251739
Q std: 21.090462
Actor loss: 15.255712
Action reg: 0.003973
  l1.weight: grad_norm = 0.070708
  l1.bias: grad_norm = 0.000754
  l2.weight: grad_norm = 0.050738
Total gradient norm: 0.140479
=== Actor Training Debug (Iteration 5966) ===
Q mean: -13.183279
Q std: 19.200432
Actor loss: 13.187249
Action reg: 0.003970
  l1.weight: grad_norm = 0.080303
  l1.bias: grad_norm = 0.002415
  l2.weight: grad_norm = 0.074043
Total gradient norm: 0.207369
=== Actor Training Debug (Iteration 5967) ===
Q mean: -15.893285
Q std: 21.917973
Actor loss: 15.897273
Action reg: 0.003988
  l1.weight: grad_norm = 0.125781
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.099104
Total gradient norm: 0.295568
=== Actor Training Debug (Iteration 5968) ===
Q mean: -14.616829
Q std: 21.310198
Actor loss: 14.620808
Action reg: 0.003979
  l1.weight: grad_norm = 0.168067
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.113454
Total gradient norm: 0.371213
=== Actor Training Debug (Iteration 5969) ===
Q mean: -13.937927
Q std: 20.306854
Actor loss: 13.941902
Action reg: 0.003975
  l1.weight: grad_norm = 0.217395
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.162467
Total gradient norm: 0.478233
=== Actor Training Debug (Iteration 5970) ===
Q mean: -14.238134
Q std: 20.936518
Actor loss: 14.242119
Action reg: 0.003984
  l1.weight: grad_norm = 0.218290
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.178155
Total gradient norm: 0.649536
=== Actor Training Debug (Iteration 5971) ===
Q mean: -9.955938
Q std: 18.532675
Actor loss: 9.959918
Action reg: 0.003980
  l1.weight: grad_norm = 0.100475
  l1.bias: grad_norm = 0.000612
  l2.weight: grad_norm = 0.071749
Total gradient norm: 0.195625
=== Actor Training Debug (Iteration 5972) ===
Q mean: -15.066036
Q std: 19.816238
Actor loss: 15.069999
Action reg: 0.003963
  l1.weight: grad_norm = 0.261737
  l1.bias: grad_norm = 0.001126
  l2.weight: grad_norm = 0.212207
Total gradient norm: 0.583455
=== Actor Training Debug (Iteration 5973) ===
Q mean: -13.683642
Q std: 20.511623
Actor loss: 13.687602
Action reg: 0.003960
  l1.weight: grad_norm = 1.430459
  l1.bias: grad_norm = 0.001242
  l2.weight: grad_norm = 1.143760
Total gradient norm: 3.115246
=== Actor Training Debug (Iteration 5974) ===
Q mean: -14.208195
Q std: 21.598835
Actor loss: 14.212173
Action reg: 0.003978
  l1.weight: grad_norm = 0.207935
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.148151
Total gradient norm: 0.450661
=== Actor Training Debug (Iteration 5975) ===
Q mean: -14.981997
Q std: 21.074001
Actor loss: 14.985969
Action reg: 0.003972
  l1.weight: grad_norm = 0.172353
  l1.bias: grad_norm = 0.001936
  l2.weight: grad_norm = 0.126999
Total gradient norm: 0.359941
=== Actor Training Debug (Iteration 5976) ===
Q mean: -13.155622
Q std: 19.611376
Actor loss: 13.159598
Action reg: 0.003977
  l1.weight: grad_norm = 0.108156
  l1.bias: grad_norm = 0.000746
  l2.weight: grad_norm = 0.093840
Total gradient norm: 0.283903
=== Actor Training Debug (Iteration 5977) ===
Q mean: -11.230165
Q std: 18.924099
Actor loss: 11.234148
Action reg: 0.003982
  l1.weight: grad_norm = 0.056945
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.044317
Total gradient norm: 0.157209
=== Actor Training Debug (Iteration 5978) ===
Q mean: -14.133482
Q std: 19.988224
Actor loss: 14.137438
Action reg: 0.003955
  l1.weight: grad_norm = 0.148680
  l1.bias: grad_norm = 0.002804
  l2.weight: grad_norm = 0.125512
Total gradient norm: 0.394298
=== Actor Training Debug (Iteration 5979) ===
Q mean: -14.328850
Q std: 21.244871
Actor loss: 14.332826
Action reg: 0.003976
  l1.weight: grad_norm = 0.062003
  l1.bias: grad_norm = 0.000519
  l2.weight: grad_norm = 0.052231
Total gradient norm: 0.154705
=== Actor Training Debug (Iteration 5980) ===
Q mean: -13.815843
Q std: 19.541969
Actor loss: 13.819819
Action reg: 0.003977
  l1.weight: grad_norm = 0.262117
  l1.bias: grad_norm = 0.001612
  l2.weight: grad_norm = 0.203829
Total gradient norm: 0.725188
=== Actor Training Debug (Iteration 5981) ===
Q mean: -13.802748
Q std: 20.665237
Actor loss: 13.806733
Action reg: 0.003985
  l1.weight: grad_norm = 0.166239
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.130968
Total gradient norm: 0.420596
=== Actor Training Debug (Iteration 5982) ===
Q mean: -15.960346
Q std: 22.633022
Actor loss: 15.964323
Action reg: 0.003977
  l1.weight: grad_norm = 0.171781
  l1.bias: grad_norm = 0.000498
  l2.weight: grad_norm = 0.131214
Total gradient norm: 0.383032
=== Actor Training Debug (Iteration 5983) ===
Q mean: -13.480539
Q std: 19.622314
Actor loss: 13.484511
Action reg: 0.003972
  l1.weight: grad_norm = 0.112739
  l1.bias: grad_norm = 0.000793
  l2.weight: grad_norm = 0.107812
Total gradient norm: 0.405822
=== Actor Training Debug (Iteration 5984) ===
Q mean: -13.969158
Q std: 20.906935
Actor loss: 13.973133
Action reg: 0.003975
  l1.weight: grad_norm = 0.111367
  l1.bias: grad_norm = 0.001392
  l2.weight: grad_norm = 0.083616
Total gradient norm: 0.235692
=== Actor Training Debug (Iteration 5985) ===
Q mean: -13.825912
Q std: 19.627666
Actor loss: 13.829894
Action reg: 0.003982
  l1.weight: grad_norm = 0.101260
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.081762
Total gradient norm: 0.266052
=== Actor Training Debug (Iteration 5986) ===
Q mean: -13.058976
Q std: 20.282885
Actor loss: 13.062955
Action reg: 0.003979
  l1.weight: grad_norm = 0.223897
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.202574
Total gradient norm: 0.846085
=== Actor Training Debug (Iteration 5987) ===
Q mean: -14.246660
Q std: 21.418104
Actor loss: 14.250645
Action reg: 0.003984
  l1.weight: grad_norm = 0.077944
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.064111
Total gradient norm: 0.162743
=== Actor Training Debug (Iteration 5988) ===
Q mean: -12.408846
Q std: 18.712570
Actor loss: 12.412827
Action reg: 0.003981
  l1.weight: grad_norm = 0.176432
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.138567
Total gradient norm: 0.389622
=== Actor Training Debug (Iteration 5989) ===
Q mean: -13.179344
Q std: 20.165558
Actor loss: 13.183317
Action reg: 0.003973
  l1.weight: grad_norm = 0.193219
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.138931
Total gradient norm: 0.503514
=== Actor Training Debug (Iteration 5990) ===
Q mean: -14.686581
Q std: 20.495773
Actor loss: 14.690557
Action reg: 0.003977
  l1.weight: grad_norm = 0.262108
  l1.bias: grad_norm = 0.000891
  l2.weight: grad_norm = 0.248814
Total gradient norm: 0.745451
=== Actor Training Debug (Iteration 5991) ===
Q mean: -13.038176
Q std: 19.577452
Actor loss: 13.042148
Action reg: 0.003972
  l1.weight: grad_norm = 0.285211
  l1.bias: grad_norm = 0.000516
  l2.weight: grad_norm = 0.203425
Total gradient norm: 0.619530
=== Actor Training Debug (Iteration 5992) ===
Q mean: -11.535452
Q std: 19.173279
Actor loss: 11.539430
Action reg: 0.003978
  l1.weight: grad_norm = 0.207520
  l1.bias: grad_norm = 0.000579
  l2.weight: grad_norm = 0.190613
Total gradient norm: 0.675786
=== Actor Training Debug (Iteration 5993) ===
Q mean: -14.373603
Q std: 20.440660
Actor loss: 14.377591
Action reg: 0.003988
  l1.weight: grad_norm = 0.041939
  l1.bias: grad_norm = 0.000572
  l2.weight: grad_norm = 0.036536
Total gradient norm: 0.115048
=== Actor Training Debug (Iteration 5994) ===
Q mean: -11.774334
Q std: 18.281622
Actor loss: 11.778308
Action reg: 0.003974
  l1.weight: grad_norm = 0.124993
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.109271
Total gradient norm: 0.375071
=== Actor Training Debug (Iteration 5995) ===
Q mean: -14.865723
Q std: 21.380672
Actor loss: 14.869702
Action reg: 0.003980
  l1.weight: grad_norm = 0.131938
  l1.bias: grad_norm = 0.000599
  l2.weight: grad_norm = 0.111020
Total gradient norm: 0.382672
=== Actor Training Debug (Iteration 5996) ===
Q mean: -15.238217
Q std: 20.052174
Actor loss: 15.242196
Action reg: 0.003979
  l1.weight: grad_norm = 0.065452
  l1.bias: grad_norm = 0.000927
  l2.weight: grad_norm = 0.045460
Total gradient norm: 0.147307
=== Actor Training Debug (Iteration 5997) ===
Q mean: -15.187117
Q std: 21.270430
Actor loss: 15.191088
Action reg: 0.003971
  l1.weight: grad_norm = 0.108493
  l1.bias: grad_norm = 0.001420
  l2.weight: grad_norm = 0.074272
Total gradient norm: 0.238592
=== Actor Training Debug (Iteration 5998) ===
Q mean: -13.545308
Q std: 19.447365
Actor loss: 13.549267
Action reg: 0.003959
  l1.weight: grad_norm = 0.325584
  l1.bias: grad_norm = 0.001006
  l2.weight: grad_norm = 0.247378
Total gradient norm: 0.699366
=== Actor Training Debug (Iteration 5999) ===
Q mean: -13.696096
Q std: 20.593637
Actor loss: 13.700076
Action reg: 0.003980
  l1.weight: grad_norm = 0.165937
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.132694
Total gradient norm: 0.452827
=== Actor Training Debug (Iteration 6000) ===
Q mean: -12.220943
Q std: 19.614595
Actor loss: 12.224919
Action reg: 0.003976
  l1.weight: grad_norm = 0.202266
  l1.bias: grad_norm = 0.000693
  l2.weight: grad_norm = 0.139399
Total gradient norm: 0.417821
Step 11000: Critic Loss: 0.9108, Actor Loss: 12.2249, Q Value: -12.2209
  Average reward: -317.862 | Average length: 100.0
Evaluation at episode 110: -317.862
=== Actor Training Debug (Iteration 6001) ===
Q mean: -15.919100
Q std: 21.764177
Actor loss: 15.923086
Action reg: 0.003986
  l1.weight: grad_norm = 0.078644
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.062749
Total gradient norm: 0.196630
=== Actor Training Debug (Iteration 6002) ===
Q mean: -14.053160
Q std: 20.859531
Actor loss: 14.057139
Action reg: 0.003980
  l1.weight: grad_norm = 0.078752
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.054954
Total gradient norm: 0.167494
=== Actor Training Debug (Iteration 6003) ===
Q mean: -12.438421
Q std: 18.532181
Actor loss: 12.442401
Action reg: 0.003980
  l1.weight: grad_norm = 0.262462
  l1.bias: grad_norm = 0.000717
  l2.weight: grad_norm = 0.211543
Total gradient norm: 0.583187
=== Actor Training Debug (Iteration 6004) ===
Q mean: -12.032742
Q std: 19.274858
Actor loss: 12.036723
Action reg: 0.003982
  l1.weight: grad_norm = 0.089344
  l1.bias: grad_norm = 0.000921
  l2.weight: grad_norm = 0.083943
Total gradient norm: 0.285216
=== Actor Training Debug (Iteration 6005) ===
Q mean: -13.234577
Q std: 18.288050
Actor loss: 13.238552
Action reg: 0.003975
  l1.weight: grad_norm = 0.077948
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.066365
Total gradient norm: 0.193029
=== Actor Training Debug (Iteration 6006) ===
Q mean: -16.924707
Q std: 23.089087
Actor loss: 16.928680
Action reg: 0.003973
  l1.weight: grad_norm = 0.215058
  l1.bias: grad_norm = 0.000686
  l2.weight: grad_norm = 0.165672
Total gradient norm: 0.497686
=== Actor Training Debug (Iteration 6007) ===
Q mean: -15.158715
Q std: 20.413435
Actor loss: 15.162700
Action reg: 0.003985
  l1.weight: grad_norm = 0.278183
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.182124
Total gradient norm: 0.686813
=== Actor Training Debug (Iteration 6008) ===
Q mean: -13.845084
Q std: 19.143139
Actor loss: 13.849065
Action reg: 0.003981
  l1.weight: grad_norm = 0.107834
  l1.bias: grad_norm = 0.000516
  l2.weight: grad_norm = 0.085854
Total gradient norm: 0.261314
=== Actor Training Debug (Iteration 6009) ===
Q mean: -14.421467
Q std: 21.269026
Actor loss: 14.425448
Action reg: 0.003982
  l1.weight: grad_norm = 0.132596
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.102867
Total gradient norm: 0.359280
=== Actor Training Debug (Iteration 6010) ===
Q mean: -12.639221
Q std: 20.223764
Actor loss: 12.643203
Action reg: 0.003982
  l1.weight: grad_norm = 0.183319
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.153461
Total gradient norm: 0.511333
=== Actor Training Debug (Iteration 6011) ===
Q mean: -12.351725
Q std: 19.438255
Actor loss: 12.355709
Action reg: 0.003984
  l1.weight: grad_norm = 0.113210
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.085423
Total gradient norm: 0.216385
=== Actor Training Debug (Iteration 6012) ===
Q mean: -14.582708
Q std: 19.731943
Actor loss: 14.586689
Action reg: 0.003980
  l1.weight: grad_norm = 0.081432
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.063954
Total gradient norm: 0.180522
=== Actor Training Debug (Iteration 6013) ===
Q mean: -13.013302
Q std: 20.382071
Actor loss: 13.017288
Action reg: 0.003986
  l1.weight: grad_norm = 0.099043
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.076927
Total gradient norm: 0.221310
=== Actor Training Debug (Iteration 6014) ===
Q mean: -17.330648
Q std: 22.828470
Actor loss: 17.334618
Action reg: 0.003969
  l1.weight: grad_norm = 0.163269
  l1.bias: grad_norm = 0.000906
  l2.weight: grad_norm = 0.143413
Total gradient norm: 0.472549
=== Actor Training Debug (Iteration 6015) ===
Q mean: -14.269228
Q std: 20.737806
Actor loss: 14.273198
Action reg: 0.003970
  l1.weight: grad_norm = 0.184902
  l1.bias: grad_norm = 0.000926
  l2.weight: grad_norm = 0.130280
Total gradient norm: 0.373027
=== Actor Training Debug (Iteration 6016) ===
Q mean: -13.979329
Q std: 19.192112
Actor loss: 13.983302
Action reg: 0.003973
  l1.weight: grad_norm = 0.303263
  l1.bias: grad_norm = 0.000761
  l2.weight: grad_norm = 0.211346
Total gradient norm: 0.622873
=== Actor Training Debug (Iteration 6017) ===
Q mean: -14.859285
Q std: 21.576250
Actor loss: 14.863258
Action reg: 0.003973
  l1.weight: grad_norm = 0.062207
  l1.bias: grad_norm = 0.000884
  l2.weight: grad_norm = 0.051752
Total gradient norm: 0.140430
=== Actor Training Debug (Iteration 6018) ===
Q mean: -14.162558
Q std: 20.443705
Actor loss: 14.166528
Action reg: 0.003970
  l1.weight: grad_norm = 0.079357
  l1.bias: grad_norm = 0.001135
  l2.weight: grad_norm = 0.063364
Total gradient norm: 0.203566
=== Actor Training Debug (Iteration 6019) ===
Q mean: -17.184233
Q std: 21.026608
Actor loss: 17.188208
Action reg: 0.003976
  l1.weight: grad_norm = 0.074363
  l1.bias: grad_norm = 0.000908
  l2.weight: grad_norm = 0.058315
Total gradient norm: 0.174348
=== Actor Training Debug (Iteration 6020) ===
Q mean: -12.837522
Q std: 19.555994
Actor loss: 12.841477
Action reg: 0.003956
  l1.weight: grad_norm = 0.390623
  l1.bias: grad_norm = 0.001905
  l2.weight: grad_norm = 0.259589
Total gradient norm: 0.770508
=== Actor Training Debug (Iteration 6021) ===
Q mean: -14.316917
Q std: 21.109907
Actor loss: 14.320890
Action reg: 0.003973
  l1.weight: grad_norm = 0.159326
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.122839
Total gradient norm: 0.334923
=== Actor Training Debug (Iteration 6022) ===
Q mean: -12.336946
Q std: 19.108292
Actor loss: 12.340912
Action reg: 0.003965
  l1.weight: grad_norm = 0.187299
  l1.bias: grad_norm = 0.000561
  l2.weight: grad_norm = 0.131579
Total gradient norm: 0.459671
=== Actor Training Debug (Iteration 6023) ===
Q mean: -15.255535
Q std: 21.088781
Actor loss: 15.259511
Action reg: 0.003976
  l1.weight: grad_norm = 0.104294
  l1.bias: grad_norm = 0.000584
  l2.weight: grad_norm = 0.094362
Total gradient norm: 0.278549
=== Actor Training Debug (Iteration 6024) ===
Q mean: -13.215691
Q std: 19.780914
Actor loss: 13.219662
Action reg: 0.003971
  l1.weight: grad_norm = 0.274313
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.191187
Total gradient norm: 0.550941
=== Actor Training Debug (Iteration 6025) ===
Q mean: -11.993430
Q std: 18.189661
Actor loss: 11.997402
Action reg: 0.003972
  l1.weight: grad_norm = 0.161609
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.126189
Total gradient norm: 0.403404
=== Actor Training Debug (Iteration 6026) ===
Q mean: -13.476315
Q std: 19.327564
Actor loss: 13.480292
Action reg: 0.003978
  l1.weight: grad_norm = 0.087339
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.063499
Total gradient norm: 0.186659
=== Actor Training Debug (Iteration 6027) ===
Q mean: -11.615606
Q std: 19.476913
Actor loss: 11.619580
Action reg: 0.003974
  l1.weight: grad_norm = 0.080613
  l1.bias: grad_norm = 0.000649
  l2.weight: grad_norm = 0.064191
Total gradient norm: 0.199051
=== Actor Training Debug (Iteration 6028) ===
Q mean: -13.607618
Q std: 19.348465
Actor loss: 13.611589
Action reg: 0.003971
  l1.weight: grad_norm = 0.181657
  l1.bias: grad_norm = 0.000589
  l2.weight: grad_norm = 0.141448
Total gradient norm: 0.413940
=== Actor Training Debug (Iteration 6029) ===
Q mean: -14.241131
Q std: 20.526325
Actor loss: 14.245100
Action reg: 0.003969
  l1.weight: grad_norm = 0.326402
  l1.bias: grad_norm = 0.000844
  l2.weight: grad_norm = 0.241272
Total gradient norm: 0.623495
=== Actor Training Debug (Iteration 6030) ===
Q mean: -12.970184
Q std: 20.122143
Actor loss: 12.974159
Action reg: 0.003975
  l1.weight: grad_norm = 0.209210
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.190573
Total gradient norm: 0.616736
=== Actor Training Debug (Iteration 6031) ===
Q mean: -12.343642
Q std: 19.696035
Actor loss: 12.347624
Action reg: 0.003981
  l1.weight: grad_norm = 0.114932
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.105365
Total gradient norm: 0.283494
=== Actor Training Debug (Iteration 6032) ===
Q mean: -11.095285
Q std: 17.387054
Actor loss: 11.099250
Action reg: 0.003964
  l1.weight: grad_norm = 0.083996
  l1.bias: grad_norm = 0.001366
  l2.weight: grad_norm = 0.065325
Total gradient norm: 0.218548
=== Actor Training Debug (Iteration 6033) ===
Q mean: -14.746964
Q std: 19.490913
Actor loss: 14.750945
Action reg: 0.003980
  l1.weight: grad_norm = 0.114493
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.086475
Total gradient norm: 0.250945
=== Actor Training Debug (Iteration 6034) ===
Q mean: -13.534298
Q std: 19.564432
Actor loss: 13.538279
Action reg: 0.003980
  l1.weight: grad_norm = 0.086990
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.077112
Total gradient norm: 0.221448
=== Actor Training Debug (Iteration 6035) ===
Q mean: -12.874092
Q std: 19.776114
Actor loss: 12.878076
Action reg: 0.003983
  l1.weight: grad_norm = 0.118836
  l1.bias: grad_norm = 0.001034
  l2.weight: grad_norm = 0.099292
Total gradient norm: 0.261470
=== Actor Training Debug (Iteration 6036) ===
Q mean: -13.841927
Q std: 20.644411
Actor loss: 13.845896
Action reg: 0.003969
  l1.weight: grad_norm = 0.185511
  l1.bias: grad_norm = 0.002065
  l2.weight: grad_norm = 0.145934
Total gradient norm: 0.410965
=== Actor Training Debug (Iteration 6037) ===
Q mean: -14.253580
Q std: 20.192091
Actor loss: 14.257556
Action reg: 0.003975
  l1.weight: grad_norm = 0.099173
  l1.bias: grad_norm = 0.001320
  l2.weight: grad_norm = 0.070225
Total gradient norm: 0.225734
=== Actor Training Debug (Iteration 6038) ===
Q mean: -13.813753
Q std: 20.056282
Actor loss: 13.817737
Action reg: 0.003983
  l1.weight: grad_norm = 0.154321
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.148649
Total gradient norm: 0.439396
=== Actor Training Debug (Iteration 6039) ===
Q mean: -14.449400
Q std: 20.741943
Actor loss: 14.453381
Action reg: 0.003981
  l1.weight: grad_norm = 0.156049
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.125124
Total gradient norm: 0.348364
=== Actor Training Debug (Iteration 6040) ===
Q mean: -13.479124
Q std: 19.256321
Actor loss: 13.483095
Action reg: 0.003971
  l1.weight: grad_norm = 0.106667
  l1.bias: grad_norm = 0.001555
  l2.weight: grad_norm = 0.076490
Total gradient norm: 0.241934
=== Actor Training Debug (Iteration 6041) ===
Q mean: -12.313478
Q std: 19.658691
Actor loss: 12.317453
Action reg: 0.003975
  l1.weight: grad_norm = 0.086653
  l1.bias: grad_norm = 0.002116
  l2.weight: grad_norm = 0.069281
Total gradient norm: 0.231091
=== Actor Training Debug (Iteration 6042) ===
Q mean: -12.354189
Q std: 20.007854
Actor loss: 12.358163
Action reg: 0.003974
  l1.weight: grad_norm = 0.194126
  l1.bias: grad_norm = 0.001004
  l2.weight: grad_norm = 0.135275
Total gradient norm: 0.440865
=== Actor Training Debug (Iteration 6043) ===
Q mean: -12.354565
Q std: 18.632515
Actor loss: 12.358549
Action reg: 0.003984
  l1.weight: grad_norm = 0.151440
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.128311
Total gradient norm: 0.400509
=== Actor Training Debug (Iteration 6044) ===
Q mean: -13.970341
Q std: 19.060280
Actor loss: 13.974318
Action reg: 0.003977
  l1.weight: grad_norm = 0.101097
  l1.bias: grad_norm = 0.001057
  l2.weight: grad_norm = 0.070754
Total gradient norm: 0.207291
=== Actor Training Debug (Iteration 6045) ===
Q mean: -14.223555
Q std: 20.752209
Actor loss: 14.227532
Action reg: 0.003978
  l1.weight: grad_norm = 0.080938
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.053626
Total gradient norm: 0.170938
=== Actor Training Debug (Iteration 6046) ===
Q mean: -15.295101
Q std: 21.421970
Actor loss: 15.299077
Action reg: 0.003976
  l1.weight: grad_norm = 0.101784
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.083251
Total gradient norm: 0.268682
=== Actor Training Debug (Iteration 6047) ===
Q mean: -15.143346
Q std: 20.021671
Actor loss: 15.147327
Action reg: 0.003981
  l1.weight: grad_norm = 0.154204
  l1.bias: grad_norm = 0.000631
  l2.weight: grad_norm = 0.132966
Total gradient norm: 0.448816
=== Actor Training Debug (Iteration 6048) ===
Q mean: -11.524082
Q std: 18.546156
Actor loss: 11.528065
Action reg: 0.003982
  l1.weight: grad_norm = 0.092356
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.071713
Total gradient norm: 0.243034
=== Actor Training Debug (Iteration 6049) ===
Q mean: -12.836565
Q std: 19.466211
Actor loss: 12.840546
Action reg: 0.003981
  l1.weight: grad_norm = 0.158140
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.118559
Total gradient norm: 0.342020
=== Actor Training Debug (Iteration 6050) ===
Q mean: -15.729840
Q std: 21.205053
Actor loss: 15.733814
Action reg: 0.003974
  l1.weight: grad_norm = 0.169722
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.131120
Total gradient norm: 0.413914
=== Actor Training Debug (Iteration 6051) ===
Q mean: -12.212172
Q std: 17.949621
Actor loss: 12.216146
Action reg: 0.003975
  l1.weight: grad_norm = 0.109717
  l1.bias: grad_norm = 0.000802
  l2.weight: grad_norm = 0.071764
Total gradient norm: 0.212825
=== Actor Training Debug (Iteration 6052) ===
Q mean: -14.546134
Q std: 20.215281
Actor loss: 14.550093
Action reg: 0.003959
  l1.weight: grad_norm = 0.184283
  l1.bias: grad_norm = 0.001249
  l2.weight: grad_norm = 0.146564
Total gradient norm: 0.428221
=== Actor Training Debug (Iteration 6053) ===
Q mean: -10.256809
Q std: 17.411983
Actor loss: 10.260772
Action reg: 0.003963
  l1.weight: grad_norm = 0.301417
  l1.bias: grad_norm = 0.000780
  l2.weight: grad_norm = 0.200167
Total gradient norm: 0.602410
=== Actor Training Debug (Iteration 6054) ===
Q mean: -11.457385
Q std: 18.114450
Actor loss: 11.461339
Action reg: 0.003954
  l1.weight: grad_norm = 0.155812
  l1.bias: grad_norm = 0.001638
  l2.weight: grad_norm = 0.115417
Total gradient norm: 0.376038
=== Actor Training Debug (Iteration 6055) ===
Q mean: -12.138363
Q std: 19.797453
Actor loss: 12.142342
Action reg: 0.003978
  l1.weight: grad_norm = 0.128574
  l1.bias: grad_norm = 0.000817
  l2.weight: grad_norm = 0.105290
Total gradient norm: 0.321345
=== Actor Training Debug (Iteration 6056) ===
Q mean: -14.955181
Q std: 21.266363
Actor loss: 14.959157
Action reg: 0.003976
  l1.weight: grad_norm = 0.084760
  l1.bias: grad_norm = 0.001224
  l2.weight: grad_norm = 0.065914
Total gradient norm: 0.207540
=== Actor Training Debug (Iteration 6057) ===
Q mean: -15.463880
Q std: 21.529902
Actor loss: 15.467858
Action reg: 0.003979
  l1.weight: grad_norm = 0.128047
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.103474
Total gradient norm: 0.365949
=== Actor Training Debug (Iteration 6058) ===
Q mean: -14.125994
Q std: 20.172289
Actor loss: 14.129967
Action reg: 0.003973
  l1.weight: grad_norm = 0.112322
  l1.bias: grad_norm = 0.001044
  l2.weight: grad_norm = 0.077505
Total gradient norm: 0.246707
=== Actor Training Debug (Iteration 6059) ===
Q mean: -16.237463
Q std: 22.027462
Actor loss: 16.241434
Action reg: 0.003971
  l1.weight: grad_norm = 0.223938
  l1.bias: grad_norm = 0.000867
  l2.weight: grad_norm = 0.162876
Total gradient norm: 0.572763
=== Actor Training Debug (Iteration 6060) ===
Q mean: -13.209780
Q std: 20.569065
Actor loss: 13.213755
Action reg: 0.003975
  l1.weight: grad_norm = 0.248964
  l1.bias: grad_norm = 0.000556
  l2.weight: grad_norm = 0.197923
Total gradient norm: 0.580426
=== Actor Training Debug (Iteration 6061) ===
Q mean: -13.231724
Q std: 19.855286
Actor loss: 13.235695
Action reg: 0.003971
  l1.weight: grad_norm = 0.147428
  l1.bias: grad_norm = 0.000771
  l2.weight: grad_norm = 0.105206
Total gradient norm: 0.308258
=== Actor Training Debug (Iteration 6062) ===
Q mean: -14.201139
Q std: 20.837326
Actor loss: 14.205119
Action reg: 0.003980
  l1.weight: grad_norm = 0.127822
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.094415
Total gradient norm: 0.276801
=== Actor Training Debug (Iteration 6063) ===
Q mean: -13.255099
Q std: 20.609875
Actor loss: 13.259081
Action reg: 0.003981
  l1.weight: grad_norm = 0.100790
  l1.bias: grad_norm = 0.001496
  l2.weight: grad_norm = 0.079218
Total gradient norm: 0.233485
=== Actor Training Debug (Iteration 6064) ===
Q mean: -13.173780
Q std: 20.304432
Actor loss: 13.177761
Action reg: 0.003980
  l1.weight: grad_norm = 0.115596
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.092713
Total gradient norm: 0.245543
=== Actor Training Debug (Iteration 6065) ===
Q mean: -12.345503
Q std: 18.669590
Actor loss: 12.349475
Action reg: 0.003972
  l1.weight: grad_norm = 0.188937
  l1.bias: grad_norm = 0.001397
  l2.weight: grad_norm = 0.123123
Total gradient norm: 0.426432
=== Actor Training Debug (Iteration 6066) ===
Q mean: -14.422981
Q std: 20.864594
Actor loss: 14.426947
Action reg: 0.003966
  l1.weight: grad_norm = 0.186736
  l1.bias: grad_norm = 0.001547
  l2.weight: grad_norm = 0.140331
Total gradient norm: 0.400445
=== Actor Training Debug (Iteration 6067) ===
Q mean: -12.321777
Q std: 19.550283
Actor loss: 12.325747
Action reg: 0.003969
  l1.weight: grad_norm = 0.114759
  l1.bias: grad_norm = 0.001981
  l2.weight: grad_norm = 0.079311
Total gradient norm: 0.246108
=== Actor Training Debug (Iteration 6068) ===
Q mean: -13.331682
Q std: 20.143698
Actor loss: 13.335660
Action reg: 0.003978
  l1.weight: grad_norm = 0.095283
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.060271
Total gradient norm: 0.163720
=== Actor Training Debug (Iteration 6069) ===
Q mean: -12.104090
Q std: 18.204067
Actor loss: 12.108066
Action reg: 0.003976
  l1.weight: grad_norm = 0.249106
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.177295
Total gradient norm: 0.499352
=== Actor Training Debug (Iteration 6070) ===
Q mean: -14.103123
Q std: 20.540911
Actor loss: 14.107100
Action reg: 0.003978
  l1.weight: grad_norm = 0.317453
  l1.bias: grad_norm = 0.000862
  l2.weight: grad_norm = 0.245770
Total gradient norm: 0.798375
=== Actor Training Debug (Iteration 6071) ===
Q mean: -14.134472
Q std: 20.654444
Actor loss: 14.138461
Action reg: 0.003989
  l1.weight: grad_norm = 0.083979
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.060894
Total gradient norm: 0.166928
=== Actor Training Debug (Iteration 6072) ===
Q mean: -12.663214
Q std: 19.361143
Actor loss: 12.667184
Action reg: 0.003970
  l1.weight: grad_norm = 0.141286
  l1.bias: grad_norm = 0.001866
  l2.weight: grad_norm = 0.113271
Total gradient norm: 0.328760
=== Actor Training Debug (Iteration 6073) ===
Q mean: -15.344626
Q std: 20.122389
Actor loss: 15.348604
Action reg: 0.003977
  l1.weight: grad_norm = 0.144561
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.132600
Total gradient norm: 0.434629
=== Actor Training Debug (Iteration 6074) ===
Q mean: -10.043829
Q std: 16.965406
Actor loss: 10.047809
Action reg: 0.003980
  l1.weight: grad_norm = 0.117631
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.095587
Total gradient norm: 0.282831
=== Actor Training Debug (Iteration 6075) ===
Q mean: -12.648079
Q std: 19.002407
Actor loss: 12.652063
Action reg: 0.003984
  l1.weight: grad_norm = 0.139944
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.100798
Total gradient norm: 0.281765
=== Actor Training Debug (Iteration 6076) ===
Q mean: -14.006002
Q std: 19.392906
Actor loss: 14.009975
Action reg: 0.003973
  l1.weight: grad_norm = 0.084917
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.059402
Total gradient norm: 0.198331
=== Actor Training Debug (Iteration 6077) ===
Q mean: -13.702320
Q std: 20.954422
Actor loss: 13.706298
Action reg: 0.003978
  l1.weight: grad_norm = 0.063565
  l1.bias: grad_norm = 0.001160
  l2.weight: grad_norm = 0.050323
Total gradient norm: 0.146810
=== Actor Training Debug (Iteration 6078) ===
Q mean: -11.741064
Q std: 18.030966
Actor loss: 11.745045
Action reg: 0.003981
  l1.weight: grad_norm = 0.111427
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.077827
Total gradient norm: 0.263919
=== Actor Training Debug (Iteration 6079) ===
Q mean: -15.013941
Q std: 20.477585
Actor loss: 15.017900
Action reg: 0.003959
  l1.weight: grad_norm = 0.147074
  l1.bias: grad_norm = 0.002540
  l2.weight: grad_norm = 0.111218
Total gradient norm: 0.335102
=== Actor Training Debug (Iteration 6080) ===
Q mean: -13.318995
Q std: 19.948019
Actor loss: 13.322959
Action reg: 0.003963
  l1.weight: grad_norm = 0.457794
  l1.bias: grad_norm = 0.000518
  l2.weight: grad_norm = 0.301844
Total gradient norm: 0.957274
=== Actor Training Debug (Iteration 6081) ===
Q mean: -14.023413
Q std: 20.979095
Actor loss: 14.027385
Action reg: 0.003972
  l1.weight: grad_norm = 0.261339
  l1.bias: grad_norm = 0.001853
  l2.weight: grad_norm = 0.179848
Total gradient norm: 0.494999
=== Actor Training Debug (Iteration 6082) ===
Q mean: -12.989439
Q std: 20.194250
Actor loss: 12.993419
Action reg: 0.003980
  l1.weight: grad_norm = 0.124630
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.097166
Total gradient norm: 0.282548
=== Actor Training Debug (Iteration 6083) ===
Q mean: -13.909555
Q std: 20.365358
Actor loss: 13.913510
Action reg: 0.003955
  l1.weight: grad_norm = 0.244202
  l1.bias: grad_norm = 0.002037
  l2.weight: grad_norm = 0.167397
Total gradient norm: 0.530117
=== Actor Training Debug (Iteration 6084) ===
Q mean: -15.254567
Q std: 20.916468
Actor loss: 15.258533
Action reg: 0.003966
  l1.weight: grad_norm = 0.103990
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.084335
Total gradient norm: 0.291745
=== Actor Training Debug (Iteration 6085) ===
Q mean: -14.919619
Q std: 20.974825
Actor loss: 14.923594
Action reg: 0.003975
  l1.weight: grad_norm = 0.126726
  l1.bias: grad_norm = 0.001333
  l2.weight: grad_norm = 0.093782
Total gradient norm: 0.277813
=== Actor Training Debug (Iteration 6086) ===
Q mean: -15.332371
Q std: 21.133228
Actor loss: 15.336349
Action reg: 0.003978
  l1.weight: grad_norm = 0.157557
  l1.bias: grad_norm = 0.001254
  l2.weight: grad_norm = 0.119790
Total gradient norm: 0.411968
=== Actor Training Debug (Iteration 6087) ===
Q mean: -12.775776
Q std: 18.225260
Actor loss: 12.779748
Action reg: 0.003972
  l1.weight: grad_norm = 0.084162
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.067638
Total gradient norm: 0.216735
=== Actor Training Debug (Iteration 6088) ===
Q mean: -14.196543
Q std: 20.585705
Actor loss: 14.200519
Action reg: 0.003976
  l1.weight: grad_norm = 0.158870
  l1.bias: grad_norm = 0.001694
  l2.weight: grad_norm = 0.145188
Total gradient norm: 0.425347
=== Actor Training Debug (Iteration 6089) ===
Q mean: -14.073490
Q std: 21.204420
Actor loss: 14.077462
Action reg: 0.003972
  l1.weight: grad_norm = 0.118057
  l1.bias: grad_norm = 0.001302
  l2.weight: grad_norm = 0.106047
Total gradient norm: 0.411308
=== Actor Training Debug (Iteration 6090) ===
Q mean: -13.313169
Q std: 19.326126
Actor loss: 13.317142
Action reg: 0.003973
  l1.weight: grad_norm = 1.113711
  l1.bias: grad_norm = 0.001107
  l2.weight: grad_norm = 0.715902
Total gradient norm: 2.101847
=== Actor Training Debug (Iteration 6091) ===
Q mean: -13.594525
Q std: 19.603918
Actor loss: 13.598497
Action reg: 0.003972
  l1.weight: grad_norm = 0.166237
  l1.bias: grad_norm = 0.000741
  l2.weight: grad_norm = 0.118295
Total gradient norm: 0.362798
=== Actor Training Debug (Iteration 6092) ===
Q mean: -13.501282
Q std: 18.245810
Actor loss: 13.505260
Action reg: 0.003979
  l1.weight: grad_norm = 0.186367
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.133628
Total gradient norm: 0.434401
=== Actor Training Debug (Iteration 6093) ===
Q mean: -13.727171
Q std: 19.547920
Actor loss: 13.731138
Action reg: 0.003967
  l1.weight: grad_norm = 0.148274
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.120864
Total gradient norm: 0.418031
=== Actor Training Debug (Iteration 6094) ===
Q mean: -13.748852
Q std: 21.332209
Actor loss: 13.752830
Action reg: 0.003977
  l1.weight: grad_norm = 0.137108
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.095131
Total gradient norm: 0.296001
=== Actor Training Debug (Iteration 6095) ===
Q mean: -14.247374
Q std: 20.193100
Actor loss: 14.251332
Action reg: 0.003959
  l1.weight: grad_norm = 0.339070
  l1.bias: grad_norm = 0.004024
  l2.weight: grad_norm = 0.244416
Total gradient norm: 0.944412
=== Actor Training Debug (Iteration 6096) ===
Q mean: -13.735809
Q std: 20.333321
Actor loss: 13.739787
Action reg: 0.003978
  l1.weight: grad_norm = 0.100600
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.083615
Total gradient norm: 0.264586
=== Actor Training Debug (Iteration 6097) ===
Q mean: -15.306890
Q std: 21.741024
Actor loss: 15.310858
Action reg: 0.003967
  l1.weight: grad_norm = 0.103760
  l1.bias: grad_norm = 0.000802
  l2.weight: grad_norm = 0.078500
Total gradient norm: 0.247759
=== Actor Training Debug (Iteration 6098) ===
Q mean: -13.652917
Q std: 18.914539
Actor loss: 13.656889
Action reg: 0.003972
  l1.weight: grad_norm = 0.131612
  l1.bias: grad_norm = 0.001667
  l2.weight: grad_norm = 0.099880
Total gradient norm: 0.281366
=== Actor Training Debug (Iteration 6099) ===
Q mean: -13.420187
Q std: 19.510269
Actor loss: 13.424170
Action reg: 0.003982
  l1.weight: grad_norm = 0.195153
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.158473
Total gradient norm: 0.423411
=== Actor Training Debug (Iteration 6100) ===
Q mean: -11.900305
Q std: 20.404778
Actor loss: 11.904269
Action reg: 0.003965
  l1.weight: grad_norm = 0.147741
  l1.bias: grad_norm = 0.000776
  l2.weight: grad_norm = 0.124313
Total gradient norm: 0.370544
Episode 111: Steps=100, Reward=-273.491, Buffer_size=11100
=== Actor Training Debug (Iteration 6101) ===
Q mean: -12.309588
Q std: 19.109760
Actor loss: 12.313575
Action reg: 0.003986
  l1.weight: grad_norm = 0.116108
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.093033
Total gradient norm: 0.267498
=== Actor Training Debug (Iteration 6102) ===
Q mean: -12.514214
Q std: 19.092312
Actor loss: 12.518198
Action reg: 0.003984
  l1.weight: grad_norm = 0.068818
  l1.bias: grad_norm = 0.000758
  l2.weight: grad_norm = 0.050673
Total gradient norm: 0.169083
=== Actor Training Debug (Iteration 6103) ===
Q mean: -11.873838
Q std: 19.304245
Actor loss: 11.877799
Action reg: 0.003960
  l1.weight: grad_norm = 0.098622
  l1.bias: grad_norm = 0.001211
  l2.weight: grad_norm = 0.084504
Total gradient norm: 0.286752
=== Actor Training Debug (Iteration 6104) ===
Q mean: -13.044614
Q std: 19.939625
Actor loss: 13.048591
Action reg: 0.003977
  l1.weight: grad_norm = 0.117117
  l1.bias: grad_norm = 0.000812
  l2.weight: grad_norm = 0.092307
Total gradient norm: 0.306192
=== Actor Training Debug (Iteration 6105) ===
Q mean: -13.836689
Q std: 21.074823
Actor loss: 13.840662
Action reg: 0.003973
  l1.weight: grad_norm = 0.097958
  l1.bias: grad_norm = 0.000938
  l2.weight: grad_norm = 0.070920
Total gradient norm: 0.244554
=== Actor Training Debug (Iteration 6106) ===
Q mean: -15.799766
Q std: 21.305334
Actor loss: 15.803736
Action reg: 0.003971
  l1.weight: grad_norm = 0.115673
  l1.bias: grad_norm = 0.001546
  l2.weight: grad_norm = 0.091011
Total gradient norm: 0.253615
=== Actor Training Debug (Iteration 6107) ===
Q mean: -13.034200
Q std: 19.039717
Actor loss: 13.038184
Action reg: 0.003984
  l1.weight: grad_norm = 0.202115
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.152174
Total gradient norm: 0.397078
=== Actor Training Debug (Iteration 6108) ===
Q mean: -15.615482
Q std: 20.897968
Actor loss: 15.619454
Action reg: 0.003972
  l1.weight: grad_norm = 0.232441
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.186766
Total gradient norm: 0.613792
=== Actor Training Debug (Iteration 6109) ===
Q mean: -14.408963
Q std: 20.591835
Actor loss: 14.412932
Action reg: 0.003969
  l1.weight: grad_norm = 0.213070
  l1.bias: grad_norm = 0.000971
  l2.weight: grad_norm = 0.143170
Total gradient norm: 0.478680
=== Actor Training Debug (Iteration 6110) ===
Q mean: -12.401034
Q std: 19.203781
Actor loss: 12.405000
Action reg: 0.003966
  l1.weight: grad_norm = 0.165629
  l1.bias: grad_norm = 0.001995
  l2.weight: grad_norm = 0.120554
Total gradient norm: 0.377788
=== Actor Training Debug (Iteration 6111) ===
Q mean: -13.885588
Q std: 20.588179
Actor loss: 13.889573
Action reg: 0.003986
  l1.weight: grad_norm = 0.156387
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.131057
Total gradient norm: 0.407773
=== Actor Training Debug (Iteration 6112) ===
Q mean: -16.523792
Q std: 22.398161
Actor loss: 16.527758
Action reg: 0.003966
  l1.weight: grad_norm = 0.272918
  l1.bias: grad_norm = 0.002205
  l2.weight: grad_norm = 0.181559
Total gradient norm: 0.494853
=== Actor Training Debug (Iteration 6113) ===
Q mean: -12.959082
Q std: 18.826694
Actor loss: 12.963035
Action reg: 0.003953
  l1.weight: grad_norm = 0.179125
  l1.bias: grad_norm = 0.001050
  l2.weight: grad_norm = 0.121805
Total gradient norm: 0.374483
=== Actor Training Debug (Iteration 6114) ===
Q mean: -14.908487
Q std: 21.034061
Actor loss: 14.912458
Action reg: 0.003971
  l1.weight: grad_norm = 0.089695
  l1.bias: grad_norm = 0.000661
  l2.weight: grad_norm = 0.062423
Total gradient norm: 0.179297
=== Actor Training Debug (Iteration 6115) ===
Q mean: -13.280072
Q std: 19.488104
Actor loss: 13.284057
Action reg: 0.003985
  l1.weight: grad_norm = 0.126140
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.112974
Total gradient norm: 0.285672
=== Actor Training Debug (Iteration 6116) ===
Q mean: -16.904385
Q std: 21.780079
Actor loss: 16.908369
Action reg: 0.003984
  l1.weight: grad_norm = 0.134148
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.102466
Total gradient norm: 0.298917
=== Actor Training Debug (Iteration 6117) ===
Q mean: -12.859278
Q std: 19.925697
Actor loss: 12.863260
Action reg: 0.003983
  l1.weight: grad_norm = 0.116566
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.088201
Total gradient norm: 0.266684
=== Actor Training Debug (Iteration 6118) ===
Q mean: -13.633492
Q std: 18.556211
Actor loss: 13.637466
Action reg: 0.003975
  l1.weight: grad_norm = 0.183920
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.137731
Total gradient norm: 0.419517
=== Actor Training Debug (Iteration 6119) ===
Q mean: -14.516003
Q std: 19.565010
Actor loss: 14.519976
Action reg: 0.003973
  l1.weight: grad_norm = 0.205865
  l1.bias: grad_norm = 0.001066
  l2.weight: grad_norm = 0.144882
Total gradient norm: 0.389306
=== Actor Training Debug (Iteration 6120) ===
Q mean: -13.480433
Q std: 20.823643
Actor loss: 13.484397
Action reg: 0.003965
  l1.weight: grad_norm = 0.157567
  l1.bias: grad_norm = 0.001321
  l2.weight: grad_norm = 0.122696
Total gradient norm: 0.365844
=== Actor Training Debug (Iteration 6121) ===
Q mean: -12.710093
Q std: 19.244093
Actor loss: 12.714068
Action reg: 0.003975
  l1.weight: grad_norm = 0.280663
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.206351
Total gradient norm: 0.635110
=== Actor Training Debug (Iteration 6122) ===
Q mean: -13.189873
Q std: 19.642406
Actor loss: 13.193830
Action reg: 0.003958
  l1.weight: grad_norm = 0.093040
  l1.bias: grad_norm = 0.002280
  l2.weight: grad_norm = 0.082889
Total gradient norm: 0.231964
=== Actor Training Debug (Iteration 6123) ===
Q mean: -13.732051
Q std: 19.253847
Actor loss: 13.736034
Action reg: 0.003984
  l1.weight: grad_norm = 0.115547
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.101159
Total gradient norm: 0.253799
=== Actor Training Debug (Iteration 6124) ===
Q mean: -16.102798
Q std: 21.025339
Actor loss: 16.106766
Action reg: 0.003967
  l1.weight: grad_norm = 0.107261
  l1.bias: grad_norm = 0.001541
  l2.weight: grad_norm = 0.083876
Total gradient norm: 0.257206
=== Actor Training Debug (Iteration 6125) ===
Q mean: -12.714594
Q std: 18.951492
Actor loss: 12.718557
Action reg: 0.003964
  l1.weight: grad_norm = 0.167282
  l1.bias: grad_norm = 0.001308
  l2.weight: grad_norm = 0.135497
Total gradient norm: 0.368107
=== Actor Training Debug (Iteration 6126) ===
Q mean: -13.140560
Q std: 19.372217
Actor loss: 13.144526
Action reg: 0.003966
  l1.weight: grad_norm = 0.182484
  l1.bias: grad_norm = 0.001445
  l2.weight: grad_norm = 0.110529
Total gradient norm: 0.349671
=== Actor Training Debug (Iteration 6127) ===
Q mean: -12.422976
Q std: 18.835287
Actor loss: 12.426964
Action reg: 0.003987
  l1.weight: grad_norm = 0.042200
  l1.bias: grad_norm = 0.001537
  l2.weight: grad_norm = 0.029402
Total gradient norm: 0.091996
=== Actor Training Debug (Iteration 6128) ===
Q mean: -12.295172
Q std: 18.645912
Actor loss: 12.299158
Action reg: 0.003986
  l1.weight: grad_norm = 0.142345
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.094807
Total gradient norm: 0.346784
=== Actor Training Debug (Iteration 6129) ===
Q mean: -13.606596
Q std: 20.964369
Actor loss: 13.610578
Action reg: 0.003982
  l1.weight: grad_norm = 0.123134
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.088606
Total gradient norm: 0.277906
=== Actor Training Debug (Iteration 6130) ===
Q mean: -13.368881
Q std: 20.366936
Actor loss: 13.372853
Action reg: 0.003972
  l1.weight: grad_norm = 0.070510
  l1.bias: grad_norm = 0.001238
  l2.weight: grad_norm = 0.055480
Total gradient norm: 0.163775
=== Actor Training Debug (Iteration 6131) ===
Q mean: -13.257727
Q std: 20.088930
Actor loss: 13.261702
Action reg: 0.003975
  l1.weight: grad_norm = 0.122405
  l1.bias: grad_norm = 0.000912
  l2.weight: grad_norm = 0.086047
Total gradient norm: 0.262626
=== Actor Training Debug (Iteration 6132) ===
Q mean: -13.460807
Q std: 18.816170
Actor loss: 13.464787
Action reg: 0.003980
  l1.weight: grad_norm = 0.117360
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.091479
Total gradient norm: 0.302817
=== Actor Training Debug (Iteration 6133) ===
Q mean: -13.411801
Q std: 19.799881
Actor loss: 13.415779
Action reg: 0.003978
  l1.weight: grad_norm = 0.026366
  l1.bias: grad_norm = 0.001684
  l2.weight: grad_norm = 0.022929
Total gradient norm: 0.102375
=== Actor Training Debug (Iteration 6134) ===
Q mean: -15.399446
Q std: 20.903618
Actor loss: 15.403434
Action reg: 0.003989
  l1.weight: grad_norm = 0.169045
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.133374
Total gradient norm: 0.423574
=== Actor Training Debug (Iteration 6135) ===
Q mean: -14.309968
Q std: 20.126640
Actor loss: 14.313936
Action reg: 0.003969
  l1.weight: grad_norm = 0.221165
  l1.bias: grad_norm = 0.002782
  l2.weight: grad_norm = 0.164369
Total gradient norm: 0.558419
=== Actor Training Debug (Iteration 6136) ===
Q mean: -15.093656
Q std: 19.954201
Actor loss: 15.097611
Action reg: 0.003956
  l1.weight: grad_norm = 0.190529
  l1.bias: grad_norm = 0.001552
  l2.weight: grad_norm = 0.160157
Total gradient norm: 0.473533
=== Actor Training Debug (Iteration 6137) ===
Q mean: -13.835623
Q std: 20.271780
Actor loss: 13.839596
Action reg: 0.003973
  l1.weight: grad_norm = 0.067033
  l1.bias: grad_norm = 0.002198
  l2.weight: grad_norm = 0.058339
Total gradient norm: 0.179464
=== Actor Training Debug (Iteration 6138) ===
Q mean: -14.841240
Q std: 21.450560
Actor loss: 14.845211
Action reg: 0.003971
  l1.weight: grad_norm = 0.120039
  l1.bias: grad_norm = 0.003369
  l2.weight: grad_norm = 0.098892
Total gradient norm: 0.336627
=== Actor Training Debug (Iteration 6139) ===
Q mean: -16.009186
Q std: 20.471636
Actor loss: 16.013161
Action reg: 0.003976
  l1.weight: grad_norm = 0.217780
  l1.bias: grad_norm = 0.000676
  l2.weight: grad_norm = 0.156137
Total gradient norm: 0.499080
=== Actor Training Debug (Iteration 6140) ===
Q mean: -14.083665
Q std: 20.699783
Actor loss: 14.087628
Action reg: 0.003964
  l1.weight: grad_norm = 0.172959
  l1.bias: grad_norm = 0.001226
  l2.weight: grad_norm = 0.130658
Total gradient norm: 0.394447
=== Actor Training Debug (Iteration 6141) ===
Q mean: -14.437559
Q std: 20.239607
Actor loss: 14.441541
Action reg: 0.003982
  l1.weight: grad_norm = 0.110044
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.084291
Total gradient norm: 0.245658
=== Actor Training Debug (Iteration 6142) ===
Q mean: -11.809093
Q std: 19.703756
Actor loss: 11.813064
Action reg: 0.003970
  l1.weight: grad_norm = 0.165800
  l1.bias: grad_norm = 0.001307
  l2.weight: grad_norm = 0.127144
Total gradient norm: 0.338593
=== Actor Training Debug (Iteration 6143) ===
Q mean: -13.769707
Q std: 20.039314
Actor loss: 13.773676
Action reg: 0.003969
  l1.weight: grad_norm = 0.194075
  l1.bias: grad_norm = 0.003507
  l2.weight: grad_norm = 0.140423
Total gradient norm: 0.444611
=== Actor Training Debug (Iteration 6144) ===
Q mean: -14.295204
Q std: 20.060490
Actor loss: 14.299167
Action reg: 0.003962
  l1.weight: grad_norm = 0.100043
  l1.bias: grad_norm = 0.001785
  l2.weight: grad_norm = 0.071978
Total gradient norm: 0.237538
=== Actor Training Debug (Iteration 6145) ===
Q mean: -12.816303
Q std: 19.725170
Actor loss: 12.820257
Action reg: 0.003954
  l1.weight: grad_norm = 0.261169
  l1.bias: grad_norm = 0.002732
  l2.weight: grad_norm = 0.178404
Total gradient norm: 0.567071
=== Actor Training Debug (Iteration 6146) ===
Q mean: -15.135223
Q std: 21.266232
Actor loss: 15.139211
Action reg: 0.003988
  l1.weight: grad_norm = 0.118331
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.092327
Total gradient norm: 0.291106
=== Actor Training Debug (Iteration 6147) ===
Q mean: -13.425618
Q std: 20.752504
Actor loss: 13.429590
Action reg: 0.003972
  l1.weight: grad_norm = 0.157153
  l1.bias: grad_norm = 0.000874
  l2.weight: grad_norm = 0.111511
Total gradient norm: 0.305280
=== Actor Training Debug (Iteration 6148) ===
Q mean: -12.466742
Q std: 19.074457
Actor loss: 12.470708
Action reg: 0.003967
  l1.weight: grad_norm = 0.142024
  l1.bias: grad_norm = 0.000908
  l2.weight: grad_norm = 0.116613
Total gradient norm: 0.362016
=== Actor Training Debug (Iteration 6149) ===
Q mean: -12.816395
Q std: 20.016150
Actor loss: 12.820377
Action reg: 0.003983
  l1.weight: grad_norm = 0.052790
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.040996
Total gradient norm: 0.131747
=== Actor Training Debug (Iteration 6150) ===
Q mean: -12.745008
Q std: 19.521940
Actor loss: 12.748991
Action reg: 0.003982
  l1.weight: grad_norm = 0.086748
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.082846
Total gradient norm: 0.239263
=== Actor Training Debug (Iteration 6151) ===
Q mean: -12.788389
Q std: 20.238165
Actor loss: 12.792356
Action reg: 0.003966
  l1.weight: grad_norm = 0.046214
  l1.bias: grad_norm = 0.002163
  l2.weight: grad_norm = 0.037289
Total gradient norm: 0.133803
=== Actor Training Debug (Iteration 6152) ===
Q mean: -14.401812
Q std: 21.749411
Actor loss: 14.405746
Action reg: 0.003935
  l1.weight: grad_norm = 0.078184
  l1.bias: grad_norm = 0.003607
  l2.weight: grad_norm = 0.072187
Total gradient norm: 0.294583
=== Actor Training Debug (Iteration 6153) ===
Q mean: -13.871061
Q std: 19.409554
Actor loss: 13.875034
Action reg: 0.003973
  l1.weight: grad_norm = 0.195099
  l1.bias: grad_norm = 0.000793
  l2.weight: grad_norm = 0.142164
Total gradient norm: 0.433585
=== Actor Training Debug (Iteration 6154) ===
Q mean: -15.883375
Q std: 22.048878
Actor loss: 15.887336
Action reg: 0.003961
  l1.weight: grad_norm = 0.093349
  l1.bias: grad_norm = 0.002137
  l2.weight: grad_norm = 0.085229
Total gradient norm: 0.278870
=== Actor Training Debug (Iteration 6155) ===
Q mean: -14.914260
Q std: 20.529480
Actor loss: 14.918231
Action reg: 0.003971
  l1.weight: grad_norm = 0.187925
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.142067
Total gradient norm: 0.427210
=== Actor Training Debug (Iteration 6156) ===
Q mean: -12.829990
Q std: 20.054470
Actor loss: 12.833955
Action reg: 0.003965
  l1.weight: grad_norm = 0.059280
  l1.bias: grad_norm = 0.003564
  l2.weight: grad_norm = 0.047586
Total gradient norm: 0.190981
=== Actor Training Debug (Iteration 6157) ===
Q mean: -12.597877
Q std: 18.374666
Actor loss: 12.601839
Action reg: 0.003963
  l1.weight: grad_norm = 0.215331
  l1.bias: grad_norm = 0.000943
  l2.weight: grad_norm = 0.171235
Total gradient norm: 0.563300
=== Actor Training Debug (Iteration 6158) ===
Q mean: -11.606351
Q std: 18.196102
Actor loss: 11.610310
Action reg: 0.003959
  l1.weight: grad_norm = 0.120642
  l1.bias: grad_norm = 0.003955
  l2.weight: grad_norm = 0.115300
Total gradient norm: 0.421255
=== Actor Training Debug (Iteration 6159) ===
Q mean: -13.078421
Q std: 19.656651
Actor loss: 13.082378
Action reg: 0.003958
  l1.weight: grad_norm = 0.149114
  l1.bias: grad_norm = 0.004547
  l2.weight: grad_norm = 0.111087
Total gradient norm: 0.335916
=== Actor Training Debug (Iteration 6160) ===
Q mean: -12.793615
Q std: 20.289127
Actor loss: 12.797595
Action reg: 0.003980
  l1.weight: grad_norm = 0.079536
  l1.bias: grad_norm = 0.001730
  l2.weight: grad_norm = 0.065459
Total gradient norm: 0.264535
=== Actor Training Debug (Iteration 6161) ===
Q mean: -13.410590
Q std: 20.470301
Actor loss: 13.414567
Action reg: 0.003977
  l1.weight: grad_norm = 0.063811
  l1.bias: grad_norm = 0.001211
  l2.weight: grad_norm = 0.041796
Total gradient norm: 0.127324
=== Actor Training Debug (Iteration 6162) ===
Q mean: -12.864121
Q std: 18.730625
Actor loss: 12.868093
Action reg: 0.003972
  l1.weight: grad_norm = 0.104613
  l1.bias: grad_norm = 0.001448
  l2.weight: grad_norm = 0.074127
Total gradient norm: 0.263930
=== Actor Training Debug (Iteration 6163) ===
Q mean: -14.757524
Q std: 21.036863
Actor loss: 14.761502
Action reg: 0.003978
  l1.weight: grad_norm = 0.230249
  l1.bias: grad_norm = 0.000741
  l2.weight: grad_norm = 0.161414
Total gradient norm: 0.434713
=== Actor Training Debug (Iteration 6164) ===
Q mean: -14.532442
Q std: 20.627914
Actor loss: 14.536397
Action reg: 0.003955
  l1.weight: grad_norm = 0.193553
  l1.bias: grad_norm = 0.001222
  l2.weight: grad_norm = 0.149428
Total gradient norm: 0.426012
=== Actor Training Debug (Iteration 6165) ===
Q mean: -13.991240
Q std: 19.707975
Actor loss: 13.995220
Action reg: 0.003981
  l1.weight: grad_norm = 0.123332
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.101559
Total gradient norm: 0.304911
=== Actor Training Debug (Iteration 6166) ===
Q mean: -13.970905
Q std: 20.752148
Actor loss: 13.974886
Action reg: 0.003980
  l1.weight: grad_norm = 0.163927
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.127129
Total gradient norm: 0.368609
=== Actor Training Debug (Iteration 6167) ===
Q mean: -12.289804
Q std: 20.524811
Actor loss: 12.293771
Action reg: 0.003967
  l1.weight: grad_norm = 0.105160
  l1.bias: grad_norm = 0.000827
  l2.weight: grad_norm = 0.079369
Total gradient norm: 0.226655
=== Actor Training Debug (Iteration 6168) ===
Q mean: -15.205786
Q std: 20.539185
Actor loss: 15.209760
Action reg: 0.003974
  l1.weight: grad_norm = 0.086978
  l1.bias: grad_norm = 0.000832
  l2.weight: grad_norm = 0.055233
Total gradient norm: 0.173553
=== Actor Training Debug (Iteration 6169) ===
Q mean: -15.314724
Q std: 22.266537
Actor loss: 15.318698
Action reg: 0.003974
  l1.weight: grad_norm = 0.078892
  l1.bias: grad_norm = 0.001191
  l2.weight: grad_norm = 0.061462
Total gradient norm: 0.171242
=== Actor Training Debug (Iteration 6170) ===
Q mean: -14.027492
Q std: 21.327921
Actor loss: 14.031471
Action reg: 0.003979
  l1.weight: grad_norm = 0.112013
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.086166
Total gradient norm: 0.279158
=== Actor Training Debug (Iteration 6171) ===
Q mean: -13.697641
Q std: 21.333464
Actor loss: 13.701617
Action reg: 0.003975
  l1.weight: grad_norm = 0.194041
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.142028
Total gradient norm: 0.439235
=== Actor Training Debug (Iteration 6172) ===
Q mean: -13.715298
Q std: 19.661898
Actor loss: 13.719276
Action reg: 0.003979
  l1.weight: grad_norm = 0.142626
  l1.bias: grad_norm = 0.000598
  l2.weight: grad_norm = 0.119863
Total gradient norm: 0.357115
=== Actor Training Debug (Iteration 6173) ===
Q mean: -15.051983
Q std: 20.721607
Actor loss: 15.055960
Action reg: 0.003977
  l1.weight: grad_norm = 0.210006
  l1.bias: grad_norm = 0.000616
  l2.weight: grad_norm = 0.155292
Total gradient norm: 0.429303
=== Actor Training Debug (Iteration 6174) ===
Q mean: -14.141272
Q std: 20.163481
Actor loss: 14.145240
Action reg: 0.003968
  l1.weight: grad_norm = 0.290168
  l1.bias: grad_norm = 0.001426
  l2.weight: grad_norm = 0.264283
Total gradient norm: 0.716590
=== Actor Training Debug (Iteration 6175) ===
Q mean: -13.503109
Q std: 19.559666
Actor loss: 13.507090
Action reg: 0.003981
  l1.weight: grad_norm = 0.153821
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.102428
Total gradient norm: 0.298643
=== Actor Training Debug (Iteration 6176) ===
Q mean: -13.390727
Q std: 20.456175
Actor loss: 13.394701
Action reg: 0.003973
  l1.weight: grad_norm = 0.074982
  l1.bias: grad_norm = 0.001488
  l2.weight: grad_norm = 0.056117
Total gradient norm: 0.187066
=== Actor Training Debug (Iteration 6177) ===
Q mean: -14.953520
Q std: 20.708952
Actor loss: 14.957500
Action reg: 0.003980
  l1.weight: grad_norm = 0.062486
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.048750
Total gradient norm: 0.143635
=== Actor Training Debug (Iteration 6178) ===
Q mean: -14.416157
Q std: 20.966707
Actor loss: 14.420136
Action reg: 0.003979
  l1.weight: grad_norm = 0.163733
  l1.bias: grad_norm = 0.000948
  l2.weight: grad_norm = 0.120155
Total gradient norm: 0.386084
=== Actor Training Debug (Iteration 6179) ===
Q mean: -16.802040
Q std: 21.421667
Actor loss: 16.806030
Action reg: 0.003990
  l1.weight: grad_norm = 0.148971
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.091996
Total gradient norm: 0.274341
=== Actor Training Debug (Iteration 6180) ===
Q mean: -12.932876
Q std: 19.787102
Actor loss: 12.936848
Action reg: 0.003972
  l1.weight: grad_norm = 0.092472
  l1.bias: grad_norm = 0.000689
  l2.weight: grad_norm = 0.073774
Total gradient norm: 0.223043
=== Actor Training Debug (Iteration 6181) ===
Q mean: -14.089781
Q std: 21.132027
Actor loss: 14.093766
Action reg: 0.003986
  l1.weight: grad_norm = 0.067740
  l1.bias: grad_norm = 0.001656
  l2.weight: grad_norm = 0.046138
Total gradient norm: 0.138825
=== Actor Training Debug (Iteration 6182) ===
Q mean: -13.434284
Q std: 19.353043
Actor loss: 13.438251
Action reg: 0.003966
  l1.weight: grad_norm = 0.198402
  l1.bias: grad_norm = 0.000946
  l2.weight: grad_norm = 0.129904
Total gradient norm: 0.393254
=== Actor Training Debug (Iteration 6183) ===
Q mean: -15.557168
Q std: 20.554827
Actor loss: 15.561146
Action reg: 0.003978
  l1.weight: grad_norm = 0.146459
  l1.bias: grad_norm = 0.000653
  l2.weight: grad_norm = 0.098107
Total gradient norm: 0.284304
=== Actor Training Debug (Iteration 6184) ===
Q mean: -12.480066
Q std: 18.417732
Actor loss: 12.484026
Action reg: 0.003960
  l1.weight: grad_norm = 0.132451
  l1.bias: grad_norm = 0.003414
  l2.weight: grad_norm = 0.107951
Total gradient norm: 0.325805
=== Actor Training Debug (Iteration 6185) ===
Q mean: -14.555320
Q std: 20.303484
Actor loss: 14.559299
Action reg: 0.003979
  l1.weight: grad_norm = 0.053862
  l1.bias: grad_norm = 0.003443
  l2.weight: grad_norm = 0.039806
Total gradient norm: 0.120476
=== Actor Training Debug (Iteration 6186) ===
Q mean: -15.565049
Q std: 22.013382
Actor loss: 15.569028
Action reg: 0.003978
  l1.weight: grad_norm = 0.087285
  l1.bias: grad_norm = 0.000870
  l2.weight: grad_norm = 0.058156
Total gradient norm: 0.156246
=== Actor Training Debug (Iteration 6187) ===
Q mean: -14.636234
Q std: 20.647528
Actor loss: 14.640199
Action reg: 0.003964
  l1.weight: grad_norm = 0.169287
  l1.bias: grad_norm = 0.000830
  l2.weight: grad_norm = 0.133092
Total gradient norm: 0.393829
=== Actor Training Debug (Iteration 6188) ===
Q mean: -14.062071
Q std: 20.371832
Actor loss: 14.066052
Action reg: 0.003982
  l1.weight: grad_norm = 0.219075
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.149553
Total gradient norm: 0.407937
=== Actor Training Debug (Iteration 6189) ===
Q mean: -14.096786
Q std: 20.366426
Actor loss: 14.100769
Action reg: 0.003984
  l1.weight: grad_norm = 0.096138
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.067732
Total gradient norm: 0.222789
=== Actor Training Debug (Iteration 6190) ===
Q mean: -14.840431
Q std: 20.990833
Actor loss: 14.844413
Action reg: 0.003982
  l1.weight: grad_norm = 0.087614
  l1.bias: grad_norm = 0.000839
  l2.weight: grad_norm = 0.063257
Total gradient norm: 0.176059
=== Actor Training Debug (Iteration 6191) ===
Q mean: -15.285563
Q std: 19.724035
Actor loss: 15.289549
Action reg: 0.003987
  l1.weight: grad_norm = 0.067263
  l1.bias: grad_norm = 0.000675
  l2.weight: grad_norm = 0.054790
Total gradient norm: 0.145594
=== Actor Training Debug (Iteration 6192) ===
Q mean: -13.618839
Q std: 19.005552
Actor loss: 13.622812
Action reg: 0.003973
  l1.weight: grad_norm = 0.070502
  l1.bias: grad_norm = 0.001247
  l2.weight: grad_norm = 0.057450
Total gradient norm: 0.181892
=== Actor Training Debug (Iteration 6193) ===
Q mean: -13.949545
Q std: 20.951138
Actor loss: 13.953525
Action reg: 0.003979
  l1.weight: grad_norm = 0.085773
  l1.bias: grad_norm = 0.000930
  l2.weight: grad_norm = 0.062549
Total gradient norm: 0.212426
=== Actor Training Debug (Iteration 6194) ===
Q mean: -14.689723
Q std: 20.414322
Actor loss: 14.693687
Action reg: 0.003965
  l1.weight: grad_norm = 0.164367
  l1.bias: grad_norm = 0.002005
  l2.weight: grad_norm = 0.126536
Total gradient norm: 0.353659
=== Actor Training Debug (Iteration 6195) ===
Q mean: -13.085577
Q std: 19.621361
Actor loss: 13.089554
Action reg: 0.003976
  l1.weight: grad_norm = 0.108089
  l1.bias: grad_norm = 0.001339
  l2.weight: grad_norm = 0.078944
Total gradient norm: 0.222200
=== Actor Training Debug (Iteration 6196) ===
Q mean: -13.382006
Q std: 20.008041
Actor loss: 13.385981
Action reg: 0.003975
  l1.weight: grad_norm = 0.199469
  l1.bias: grad_norm = 0.000733
  l2.weight: grad_norm = 0.154068
Total gradient norm: 0.485940
=== Actor Training Debug (Iteration 6197) ===
Q mean: -14.366722
Q std: 20.365366
Actor loss: 14.370689
Action reg: 0.003967
  l1.weight: grad_norm = 0.230920
  l1.bias: grad_norm = 0.001611
  l2.weight: grad_norm = 0.165037
Total gradient norm: 0.518981
=== Actor Training Debug (Iteration 6198) ===
Q mean: -13.205645
Q std: 19.420832
Actor loss: 13.209622
Action reg: 0.003978
  l1.weight: grad_norm = 0.126195
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.091375
Total gradient norm: 0.281017
=== Actor Training Debug (Iteration 6199) ===
Q mean: -13.181274
Q std: 20.937332
Actor loss: 13.185250
Action reg: 0.003976
  l1.weight: grad_norm = 0.166420
  l1.bias: grad_norm = 0.000507
  l2.weight: grad_norm = 0.146453
Total gradient norm: 0.429420
=== Actor Training Debug (Iteration 6200) ===
Q mean: -13.940569
Q std: 20.482815
Actor loss: 13.944541
Action reg: 0.003972
  l1.weight: grad_norm = 0.093208
  l1.bias: grad_norm = 0.000691
  l2.weight: grad_norm = 0.071010
Total gradient norm: 0.198169
=== Actor Training Debug (Iteration 6201) ===
Q mean: -13.925670
Q std: 20.993109
Actor loss: 13.929651
Action reg: 0.003982
  l1.weight: grad_norm = 0.097874
  l1.bias: grad_norm = 0.000687
  l2.weight: grad_norm = 0.086071
Total gradient norm: 0.245319
=== Actor Training Debug (Iteration 6202) ===
Q mean: -14.582250
Q std: 19.525776
Actor loss: 14.586225
Action reg: 0.003975
  l1.weight: grad_norm = 0.284541
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.251951
Total gradient norm: 0.676628
=== Actor Training Debug (Iteration 6203) ===
Q mean: -13.641279
Q std: 20.864195
Actor loss: 13.645247
Action reg: 0.003968
  l1.weight: grad_norm = 0.358019
  l1.bias: grad_norm = 0.000591
  l2.weight: grad_norm = 0.294691
Total gradient norm: 0.964146
=== Actor Training Debug (Iteration 6204) ===
Q mean: -14.998826
Q std: 21.401680
Actor loss: 15.002802
Action reg: 0.003976
  l1.weight: grad_norm = 0.126955
  l1.bias: grad_norm = 0.001261
  l2.weight: grad_norm = 0.089594
Total gradient norm: 0.317243
=== Actor Training Debug (Iteration 6205) ===
Q mean: -14.231234
Q std: 20.970543
Actor loss: 14.235199
Action reg: 0.003965
  l1.weight: grad_norm = 0.185233
  l1.bias: grad_norm = 0.000746
  l2.weight: grad_norm = 0.153038
Total gradient norm: 0.474182
=== Actor Training Debug (Iteration 6206) ===
Q mean: -12.827196
Q std: 19.549301
Actor loss: 12.831168
Action reg: 0.003972
  l1.weight: grad_norm = 0.119289
  l1.bias: grad_norm = 0.000849
  l2.weight: grad_norm = 0.088215
Total gradient norm: 0.250933
=== Actor Training Debug (Iteration 6207) ===
Q mean: -13.273571
Q std: 20.651089
Actor loss: 13.277548
Action reg: 0.003976
  l1.weight: grad_norm = 0.110710
  l1.bias: grad_norm = 0.000665
  l2.weight: grad_norm = 0.090081
Total gradient norm: 0.227184
=== Actor Training Debug (Iteration 6208) ===
Q mean: -13.613975
Q std: 19.773382
Actor loss: 13.617945
Action reg: 0.003970
  l1.weight: grad_norm = 0.183656
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.149011
Total gradient norm: 0.487593
=== Actor Training Debug (Iteration 6209) ===
Q mean: -11.350040
Q std: 17.483480
Actor loss: 11.354021
Action reg: 0.003981
  l1.weight: grad_norm = 0.106367
  l1.bias: grad_norm = 0.003323
  l2.weight: grad_norm = 0.075824
Total gradient norm: 0.242150
=== Actor Training Debug (Iteration 6210) ===
Q mean: -16.187788
Q std: 21.663177
Actor loss: 16.191765
Action reg: 0.003977
  l1.weight: grad_norm = 0.103530
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.083641
Total gradient norm: 0.287852
=== Actor Training Debug (Iteration 6211) ===
Q mean: -14.785656
Q std: 21.384197
Actor loss: 14.789635
Action reg: 0.003979
  l1.weight: grad_norm = 0.124326
  l1.bias: grad_norm = 0.000837
  l2.weight: grad_norm = 0.098063
Total gradient norm: 0.270623
=== Actor Training Debug (Iteration 6212) ===
Q mean: -15.972645
Q std: 20.890753
Actor loss: 15.976622
Action reg: 0.003977
  l1.weight: grad_norm = 0.165377
  l1.bias: grad_norm = 0.001154
  l2.weight: grad_norm = 0.137202
Total gradient norm: 0.438611
=== Actor Training Debug (Iteration 6213) ===
Q mean: -16.088223
Q std: 20.688396
Actor loss: 16.092209
Action reg: 0.003987
  l1.weight: grad_norm = 0.157019
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.110819
Total gradient norm: 0.314722
=== Actor Training Debug (Iteration 6214) ===
Q mean: -13.857176
Q std: 19.551579
Actor loss: 13.861151
Action reg: 0.003975
  l1.weight: grad_norm = 0.130904
  l1.bias: grad_norm = 0.000818
  l2.weight: grad_norm = 0.111057
Total gradient norm: 0.289928
=== Actor Training Debug (Iteration 6215) ===
Q mean: -13.375381
Q std: 20.907057
Actor loss: 13.379356
Action reg: 0.003975
  l1.weight: grad_norm = 0.186452
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.150413
Total gradient norm: 0.422641
=== Actor Training Debug (Iteration 6216) ===
Q mean: -13.427546
Q std: 20.465528
Actor loss: 13.431514
Action reg: 0.003968
  l1.weight: grad_norm = 0.216093
  l1.bias: grad_norm = 0.001491
  l2.weight: grad_norm = 0.178998
Total gradient norm: 0.550108
=== Actor Training Debug (Iteration 6217) ===
Q mean: -13.043638
Q std: 20.741987
Actor loss: 13.047624
Action reg: 0.003985
  l1.weight: grad_norm = 0.163828
  l1.bias: grad_norm = 0.001279
  l2.weight: grad_norm = 0.152550
Total gradient norm: 0.469003
=== Actor Training Debug (Iteration 6218) ===
Q mean: -14.825987
Q std: 19.530657
Actor loss: 14.829965
Action reg: 0.003978
  l1.weight: grad_norm = 0.397692
  l1.bias: grad_norm = 0.000895
  l2.weight: grad_norm = 0.367880
Total gradient norm: 1.200864
=== Actor Training Debug (Iteration 6219) ===
Q mean: -13.349699
Q std: 20.215233
Actor loss: 13.353675
Action reg: 0.003976
  l1.weight: grad_norm = 0.168064
  l1.bias: grad_norm = 0.000630
  l2.weight: grad_norm = 0.124367
Total gradient norm: 0.361417
=== Actor Training Debug (Iteration 6220) ===
Q mean: -13.513231
Q std: 19.609865
Actor loss: 13.517218
Action reg: 0.003987
  l1.weight: grad_norm = 0.113921
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.079430
Total gradient norm: 0.233926
=== Actor Training Debug (Iteration 6221) ===
Q mean: -15.070648
Q std: 21.483274
Actor loss: 15.074622
Action reg: 0.003974
  l1.weight: grad_norm = 0.150416
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.122766
Total gradient norm: 0.422522
=== Actor Training Debug (Iteration 6222) ===
Q mean: -12.698326
Q std: 19.748327
Actor loss: 12.702298
Action reg: 0.003972
  l1.weight: grad_norm = 0.160562
  l1.bias: grad_norm = 0.001429
  l2.weight: grad_norm = 0.131409
Total gradient norm: 0.384358
=== Actor Training Debug (Iteration 6223) ===
Q mean: -15.698931
Q std: 21.989197
Actor loss: 15.702906
Action reg: 0.003975
  l1.weight: grad_norm = 0.236950
  l1.bias: grad_norm = 0.000773
  l2.weight: grad_norm = 0.180028
Total gradient norm: 0.609587
=== Actor Training Debug (Iteration 6224) ===
Q mean: -16.421652
Q std: 21.965454
Actor loss: 16.425644
Action reg: 0.003993
  l1.weight: grad_norm = 0.052744
  l1.bias: grad_norm = 0.000860
  l2.weight: grad_norm = 0.049126
Total gradient norm: 0.156024
=== Actor Training Debug (Iteration 6225) ===
Q mean: -14.679882
Q std: 21.015921
Actor loss: 14.683861
Action reg: 0.003978
  l1.weight: grad_norm = 0.114213
  l1.bias: grad_norm = 0.000644
  l2.weight: grad_norm = 0.107010
Total gradient norm: 0.357314
=== Actor Training Debug (Iteration 6226) ===
Q mean: -16.334885
Q std: 21.787426
Actor loss: 16.338852
Action reg: 0.003968
  l1.weight: grad_norm = 0.146413
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.117609
Total gradient norm: 0.324332
=== Actor Training Debug (Iteration 6227) ===
Q mean: -14.445015
Q std: 20.609417
Actor loss: 14.448989
Action reg: 0.003974
  l1.weight: grad_norm = 0.117553
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.093543
Total gradient norm: 0.268086
=== Actor Training Debug (Iteration 6228) ===
Q mean: -14.506478
Q std: 20.291922
Actor loss: 14.510448
Action reg: 0.003970
  l1.weight: grad_norm = 0.134624
  l1.bias: grad_norm = 0.000604
  l2.weight: grad_norm = 0.098947
Total gradient norm: 0.266577
=== Actor Training Debug (Iteration 6229) ===
Q mean: -14.451570
Q std: 21.678707
Actor loss: 14.455546
Action reg: 0.003977
  l1.weight: grad_norm = 0.061442
  l1.bias: grad_norm = 0.001047
  l2.weight: grad_norm = 0.046742
Total gradient norm: 0.138277
=== Actor Training Debug (Iteration 6230) ===
Q mean: -17.259569
Q std: 22.867619
Actor loss: 17.263556
Action reg: 0.003987
  l1.weight: grad_norm = 0.195268
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.151756
Total gradient norm: 0.475959
=== Actor Training Debug (Iteration 6231) ===
Q mean: -11.803091
Q std: 19.206059
Actor loss: 11.807064
Action reg: 0.003973
  l1.weight: grad_norm = 0.217511
  l1.bias: grad_norm = 0.000672
  l2.weight: grad_norm = 0.161404
Total gradient norm: 0.427913
=== Actor Training Debug (Iteration 6232) ===
Q mean: -14.031965
Q std: 20.296810
Actor loss: 14.035943
Action reg: 0.003978
  l1.weight: grad_norm = 0.109726
  l1.bias: grad_norm = 0.000668
  l2.weight: grad_norm = 0.093908
Total gradient norm: 0.261179
=== Actor Training Debug (Iteration 6233) ===
Q mean: -11.330715
Q std: 18.329187
Actor loss: 11.334687
Action reg: 0.003972
  l1.weight: grad_norm = 0.051180
  l1.bias: grad_norm = 0.000996
  l2.weight: grad_norm = 0.037538
Total gradient norm: 0.109608
=== Actor Training Debug (Iteration 6234) ===
Q mean: -14.118347
Q std: 21.857874
Actor loss: 14.122323
Action reg: 0.003976
  l1.weight: grad_norm = 0.296437
  l1.bias: grad_norm = 0.000675
  l2.weight: grad_norm = 0.265866
Total gradient norm: 0.880508
=== Actor Training Debug (Iteration 6235) ===
Q mean: -13.514770
Q std: 20.730425
Actor loss: 13.518750
Action reg: 0.003980
  l1.weight: grad_norm = 0.126385
  l1.bias: grad_norm = 0.000872
  l2.weight: grad_norm = 0.097408
Total gradient norm: 0.294384
=== Actor Training Debug (Iteration 6236) ===
Q mean: -12.095890
Q std: 19.071373
Actor loss: 12.099869
Action reg: 0.003979
  l1.weight: grad_norm = 0.114245
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.085859
Total gradient norm: 0.283714
=== Actor Training Debug (Iteration 6237) ===
Q mean: -14.692186
Q std: 20.243454
Actor loss: 14.696172
Action reg: 0.003985
  l1.weight: grad_norm = 0.092760
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.079596
Total gradient norm: 0.244197
=== Actor Training Debug (Iteration 6238) ===
Q mean: -13.051050
Q std: 19.026598
Actor loss: 13.055025
Action reg: 0.003975
  l1.weight: grad_norm = 0.196285
  l1.bias: grad_norm = 0.000660
  l2.weight: grad_norm = 0.124220
Total gradient norm: 0.402995
=== Actor Training Debug (Iteration 6239) ===
Q mean: -14.733889
Q std: 21.458801
Actor loss: 14.737874
Action reg: 0.003985
  l1.weight: grad_norm = 0.071007
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.053313
Total gradient norm: 0.179049
=== Actor Training Debug (Iteration 6240) ===
Q mean: -12.434425
Q std: 19.185575
Actor loss: 12.438414
Action reg: 0.003988
  l1.weight: grad_norm = 0.155687
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.120161
Total gradient norm: 0.372939
=== Actor Training Debug (Iteration 6241) ===
Q mean: -15.454810
Q std: 21.633385
Actor loss: 15.458782
Action reg: 0.003972
  l1.weight: grad_norm = 0.100538
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.088367
Total gradient norm: 0.253865
=== Actor Training Debug (Iteration 6242) ===
Q mean: -15.223048
Q std: 21.465099
Actor loss: 15.227035
Action reg: 0.003986
  l1.weight: grad_norm = 0.093038
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.064267
Total gradient norm: 0.197618
=== Actor Training Debug (Iteration 6243) ===
Q mean: -16.749060
Q std: 21.452663
Actor loss: 16.753038
Action reg: 0.003978
  l1.weight: grad_norm = 0.079657
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.070866
Total gradient norm: 0.232594
=== Actor Training Debug (Iteration 6244) ===
Q mean: -13.491119
Q std: 20.165323
Actor loss: 13.495088
Action reg: 0.003968
  l1.weight: grad_norm = 0.121134
  l1.bias: grad_norm = 0.000922
  l2.weight: grad_norm = 0.088194
Total gradient norm: 0.264778
=== Actor Training Debug (Iteration 6245) ===
Q mean: -13.791491
Q std: 20.082376
Actor loss: 13.795465
Action reg: 0.003974
  l1.weight: grad_norm = 0.156471
  l1.bias: grad_norm = 0.000683
  l2.weight: grad_norm = 0.108533
Total gradient norm: 0.344198
=== Actor Training Debug (Iteration 6246) ===
Q mean: -13.725861
Q std: 21.155813
Actor loss: 13.729841
Action reg: 0.003981
  l1.weight: grad_norm = 0.202897
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.141127
Total gradient norm: 0.452941
=== Actor Training Debug (Iteration 6247) ===
Q mean: -10.663671
Q std: 15.384860
Actor loss: 10.667642
Action reg: 0.003970
  l1.weight: grad_norm = 0.091940
  l1.bias: grad_norm = 0.000601
  l2.weight: grad_norm = 0.072615
Total gradient norm: 0.220565
=== Actor Training Debug (Iteration 6248) ===
Q mean: -14.883199
Q std: 20.676903
Actor loss: 14.887172
Action reg: 0.003973
  l1.weight: grad_norm = 0.156352
  l1.bias: grad_norm = 0.000585
  l2.weight: grad_norm = 0.111546
Total gradient norm: 0.320590
=== Actor Training Debug (Iteration 6249) ===
Q mean: -13.826337
Q std: 20.507561
Actor loss: 13.830309
Action reg: 0.003972
  l1.weight: grad_norm = 0.194861
  l1.bias: grad_norm = 0.002634
  l2.weight: grad_norm = 0.163815
Total gradient norm: 0.462266
=== Actor Training Debug (Iteration 6250) ===
Q mean: -14.277919
Q std: 20.187160
Actor loss: 14.281900
Action reg: 0.003981
  l1.weight: grad_norm = 0.141353
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.113321
Total gradient norm: 0.348725
=== Actor Training Debug (Iteration 6251) ===
Q mean: -13.277989
Q std: 18.959690
Actor loss: 13.281961
Action reg: 0.003972
  l1.weight: grad_norm = 0.086980
  l1.bias: grad_norm = 0.000668
  l2.weight: grad_norm = 0.061412
Total gradient norm: 0.191663
=== Actor Training Debug (Iteration 6252) ===
Q mean: -13.116863
Q std: 19.248924
Actor loss: 13.120840
Action reg: 0.003977
  l1.weight: grad_norm = 0.151075
  l1.bias: grad_norm = 0.001476
  l2.weight: grad_norm = 0.100928
Total gradient norm: 0.294595
=== Actor Training Debug (Iteration 6253) ===
Q mean: -14.788983
Q std: 21.193192
Actor loss: 14.792968
Action reg: 0.003984
  l1.weight: grad_norm = 0.238113
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.173166
Total gradient norm: 0.562812
=== Actor Training Debug (Iteration 6254) ===
Q mean: -12.808749
Q std: 20.800798
Actor loss: 12.812727
Action reg: 0.003978
  l1.weight: grad_norm = 0.086149
  l1.bias: grad_norm = 0.001826
  l2.weight: grad_norm = 0.067267
Total gradient norm: 0.206274
=== Actor Training Debug (Iteration 6255) ===
Q mean: -13.712704
Q std: 19.270893
Actor loss: 13.716683
Action reg: 0.003980
  l1.weight: grad_norm = 0.061247
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.046552
Total gradient norm: 0.137515
=== Actor Training Debug (Iteration 6256) ===
Q mean: -12.844524
Q std: 20.525547
Actor loss: 12.848505
Action reg: 0.003981
  l1.weight: grad_norm = 0.183426
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.145010
Total gradient norm: 0.430402
=== Actor Training Debug (Iteration 6257) ===
Q mean: -13.566053
Q std: 20.629847
Actor loss: 13.570038
Action reg: 0.003984
  l1.weight: grad_norm = 0.279102
  l1.bias: grad_norm = 0.001909
  l2.weight: grad_norm = 0.192372
Total gradient norm: 0.729325
=== Actor Training Debug (Iteration 6258) ===
Q mean: -13.939388
Q std: 21.134171
Actor loss: 13.943375
Action reg: 0.003986
  l1.weight: grad_norm = 0.099358
  l1.bias: grad_norm = 0.001641
  l2.weight: grad_norm = 0.065360
Total gradient norm: 0.218758
=== Actor Training Debug (Iteration 6259) ===
Q mean: -13.287795
Q std: 19.564472
Actor loss: 13.291778
Action reg: 0.003982
  l1.weight: grad_norm = 0.077847
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.059790
Total gradient norm: 0.191479
=== Actor Training Debug (Iteration 6260) ===
Q mean: -15.809835
Q std: 21.330946
Actor loss: 15.813819
Action reg: 0.003983
  l1.weight: grad_norm = 0.251941
  l1.bias: grad_norm = 0.000893
  l2.weight: grad_norm = 0.160283
Total gradient norm: 0.469557
=== Actor Training Debug (Iteration 6261) ===
Q mean: -12.736322
Q std: 19.991886
Actor loss: 12.740298
Action reg: 0.003976
  l1.weight: grad_norm = 0.197476
  l1.bias: grad_norm = 0.001456
  l2.weight: grad_norm = 0.147879
Total gradient norm: 0.423829
=== Actor Training Debug (Iteration 6262) ===
Q mean: -15.911350
Q std: 21.157215
Actor loss: 15.915342
Action reg: 0.003992
  l1.weight: grad_norm = 0.033559
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.024758
Total gradient norm: 0.081397
=== Actor Training Debug (Iteration 6263) ===
Q mean: -14.323236
Q std: 19.307781
Actor loss: 14.327209
Action reg: 0.003974
  l1.weight: grad_norm = 0.196986
  l1.bias: grad_norm = 0.000980
  l2.weight: grad_norm = 0.171685
Total gradient norm: 0.479711
=== Actor Training Debug (Iteration 6264) ===
Q mean: -13.663397
Q std: 20.766592
Actor loss: 13.667377
Action reg: 0.003979
  l1.weight: grad_norm = 0.170882
  l1.bias: grad_norm = 0.000703
  l2.weight: grad_norm = 0.119919
Total gradient norm: 0.350243
=== Actor Training Debug (Iteration 6265) ===
Q mean: -12.430949
Q std: 20.111088
Actor loss: 12.434928
Action reg: 0.003978
  l1.weight: grad_norm = 0.148974
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.119479
Total gradient norm: 0.301111
=== Actor Training Debug (Iteration 6266) ===
Q mean: -11.056972
Q std: 18.803364
Actor loss: 11.060950
Action reg: 0.003979
  l1.weight: grad_norm = 0.070012
  l1.bias: grad_norm = 0.000787
  l2.weight: grad_norm = 0.058628
Total gradient norm: 0.184243
=== Actor Training Debug (Iteration 6267) ===
Q mean: -13.385733
Q std: 20.929482
Actor loss: 13.389716
Action reg: 0.003984
  l1.weight: grad_norm = 0.205145
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.162399
Total gradient norm: 0.507283
=== Actor Training Debug (Iteration 6268) ===
Q mean: -14.240721
Q std: 20.155087
Actor loss: 14.244698
Action reg: 0.003977
  l1.weight: grad_norm = 0.106853
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.081151
Total gradient norm: 0.250255
=== Actor Training Debug (Iteration 6269) ===
Q mean: -13.073328
Q std: 20.579226
Actor loss: 13.077297
Action reg: 0.003969
  l1.weight: grad_norm = 0.174442
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.109284
Total gradient norm: 0.313180
=== Actor Training Debug (Iteration 6270) ===
Q mean: -12.938297
Q std: 20.216545
Actor loss: 12.942285
Action reg: 0.003988
  l1.weight: grad_norm = 0.161276
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.138380
Total gradient norm: 0.466460
=== Actor Training Debug (Iteration 6271) ===
Q mean: -14.399443
Q std: 21.357597
Actor loss: 14.403416
Action reg: 0.003973
  l1.weight: grad_norm = 0.203902
  l1.bias: grad_norm = 0.000818
  l2.weight: grad_norm = 0.177328
Total gradient norm: 0.556552
=== Actor Training Debug (Iteration 6272) ===
Q mean: -14.061201
Q std: 19.841265
Actor loss: 14.065171
Action reg: 0.003970
  l1.weight: grad_norm = 0.061352
  l1.bias: grad_norm = 0.000819
  l2.weight: grad_norm = 0.052338
Total gradient norm: 0.160730
=== Actor Training Debug (Iteration 6273) ===
Q mean: -14.694727
Q std: 20.887430
Actor loss: 14.698709
Action reg: 0.003983
  l1.weight: grad_norm = 0.174126
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.139288
Total gradient norm: 0.448123
=== Actor Training Debug (Iteration 6274) ===
Q mean: -14.632671
Q std: 20.122795
Actor loss: 14.636654
Action reg: 0.003983
  l1.weight: grad_norm = 0.087974
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.059897
Total gradient norm: 0.196711
=== Actor Training Debug (Iteration 6275) ===
Q mean: -13.475609
Q std: 20.002138
Actor loss: 13.479585
Action reg: 0.003976
  l1.weight: grad_norm = 0.158241
  l1.bias: grad_norm = 0.000711
  l2.weight: grad_norm = 0.129393
Total gradient norm: 0.398055
=== Actor Training Debug (Iteration 6276) ===
Q mean: -13.727446
Q std: 19.891653
Actor loss: 13.731427
Action reg: 0.003982
  l1.weight: grad_norm = 0.094838
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.070215
Total gradient norm: 0.216366
=== Actor Training Debug (Iteration 6277) ===
Q mean: -12.269566
Q std: 18.905403
Actor loss: 12.273537
Action reg: 0.003971
  l1.weight: grad_norm = 0.218561
  l1.bias: grad_norm = 0.001411
  l2.weight: grad_norm = 0.148380
Total gradient norm: 0.502820
=== Actor Training Debug (Iteration 6278) ===
Q mean: -12.892740
Q std: 19.763935
Actor loss: 12.896722
Action reg: 0.003982
  l1.weight: grad_norm = 0.226873
  l1.bias: grad_norm = 0.000895
  l2.weight: grad_norm = 0.209270
Total gradient norm: 0.694489
=== Actor Training Debug (Iteration 6279) ===
Q mean: -12.922345
Q std: 20.895266
Actor loss: 12.926317
Action reg: 0.003972
  l1.weight: grad_norm = 0.138517
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.138234
Total gradient norm: 0.424510
=== Actor Training Debug (Iteration 6280) ===
Q mean: -12.049448
Q std: 19.689396
Actor loss: 12.053425
Action reg: 0.003977
  l1.weight: grad_norm = 0.149171
  l1.bias: grad_norm = 0.000847
  l2.weight: grad_norm = 0.129674
Total gradient norm: 0.360341
=== Actor Training Debug (Iteration 6281) ===
Q mean: -12.200365
Q std: 20.504845
Actor loss: 12.204345
Action reg: 0.003980
  l1.weight: grad_norm = 0.209851
  l1.bias: grad_norm = 0.000957
  l2.weight: grad_norm = 0.228728
Total gradient norm: 0.706354
=== Actor Training Debug (Iteration 6282) ===
Q mean: -13.240387
Q std: 19.503595
Actor loss: 13.244358
Action reg: 0.003971
  l1.weight: grad_norm = 0.104714
  l1.bias: grad_norm = 0.001401
  l2.weight: grad_norm = 0.088788
Total gradient norm: 0.254399
=== Actor Training Debug (Iteration 6283) ===
Q mean: -16.855707
Q std: 22.820377
Actor loss: 16.859694
Action reg: 0.003986
  l1.weight: grad_norm = 0.153298
  l1.bias: grad_norm = 0.001082
  l2.weight: grad_norm = 0.136851
Total gradient norm: 0.411076
=== Actor Training Debug (Iteration 6284) ===
Q mean: -14.864943
Q std: 21.681721
Actor loss: 14.868932
Action reg: 0.003989
  l1.weight: grad_norm = 0.141659
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.113256
Total gradient norm: 0.316689
=== Actor Training Debug (Iteration 6285) ===
Q mean: -14.533213
Q std: 21.612179
Actor loss: 14.537189
Action reg: 0.003977
  l1.weight: grad_norm = 0.245437
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.186848
Total gradient norm: 0.560529
=== Actor Training Debug (Iteration 6286) ===
Q mean: -16.570528
Q std: 21.895548
Actor loss: 16.574512
Action reg: 0.003984
  l1.weight: grad_norm = 0.055833
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.041046
Total gradient norm: 0.127516
=== Actor Training Debug (Iteration 6287) ===
Q mean: -16.727638
Q std: 22.720659
Actor loss: 16.731615
Action reg: 0.003976
  l1.weight: grad_norm = 0.143610
  l1.bias: grad_norm = 0.000557
  l2.weight: grad_norm = 0.093635
Total gradient norm: 0.266872
=== Actor Training Debug (Iteration 6288) ===
Q mean: -11.680532
Q std: 18.434912
Actor loss: 11.684507
Action reg: 0.003975
  l1.weight: grad_norm = 0.246739
  l1.bias: grad_norm = 0.002233
  l2.weight: grad_norm = 0.219383
Total gradient norm: 0.676888
=== Actor Training Debug (Iteration 6289) ===
Q mean: -14.435921
Q std: 22.079821
Actor loss: 14.439908
Action reg: 0.003987
  l1.weight: grad_norm = 0.061978
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.042191
Total gradient norm: 0.130242
=== Actor Training Debug (Iteration 6290) ===
Q mean: -14.454743
Q std: 20.629787
Actor loss: 14.458719
Action reg: 0.003976
  l1.weight: grad_norm = 0.195249
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.159992
Total gradient norm: 0.495873
=== Actor Training Debug (Iteration 6291) ===
Q mean: -12.400513
Q std: 19.282526
Actor loss: 12.404485
Action reg: 0.003973
  l1.weight: grad_norm = 0.049175
  l1.bias: grad_norm = 0.000521
  l2.weight: grad_norm = 0.038102
Total gradient norm: 0.119013
=== Actor Training Debug (Iteration 6292) ===
Q mean: -16.291241
Q std: 21.678953
Actor loss: 16.295219
Action reg: 0.003978
  l1.weight: grad_norm = 0.077241
  l1.bias: grad_norm = 0.000584
  l2.weight: grad_norm = 0.064862
Total gradient norm: 0.257352
=== Actor Training Debug (Iteration 6293) ===
Q mean: -14.503617
Q std: 20.810213
Actor loss: 14.507600
Action reg: 0.003983
  l1.weight: grad_norm = 0.116658
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.089959
Total gradient norm: 0.258878
=== Actor Training Debug (Iteration 6294) ===
Q mean: -14.264600
Q std: 20.500126
Actor loss: 14.268587
Action reg: 0.003987
  l1.weight: grad_norm = 0.101727
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.073145
Total gradient norm: 0.204110
=== Actor Training Debug (Iteration 6295) ===
Q mean: -14.743711
Q std: 21.751955
Actor loss: 14.747690
Action reg: 0.003979
  l1.weight: grad_norm = 0.127924
  l1.bias: grad_norm = 0.000866
  l2.weight: grad_norm = 0.087490
Total gradient norm: 0.267315
=== Actor Training Debug (Iteration 6296) ===
Q mean: -14.156229
Q std: 20.506603
Actor loss: 14.160213
Action reg: 0.003984
  l1.weight: grad_norm = 0.487583
  l1.bias: grad_norm = 0.000725
  l2.weight: grad_norm = 0.441061
Total gradient norm: 1.189938
=== Actor Training Debug (Iteration 6297) ===
Q mean: -13.914728
Q std: 18.936354
Actor loss: 13.918717
Action reg: 0.003989
  l1.weight: grad_norm = 0.060852
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.048063
Total gradient norm: 0.163067
=== Actor Training Debug (Iteration 6298) ===
Q mean: -16.117527
Q std: 21.280989
Actor loss: 16.121517
Action reg: 0.003989
  l1.weight: grad_norm = 0.135921
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.104907
Total gradient norm: 0.279502
=== Actor Training Debug (Iteration 6299) ===
Q mean: -12.924410
Q std: 21.178806
Actor loss: 12.928399
Action reg: 0.003989
  l1.weight: grad_norm = 0.145275
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.101583
Total gradient norm: 0.314232
=== Actor Training Debug (Iteration 6300) ===
Q mean: -13.621924
Q std: 20.171265
Actor loss: 13.625915
Action reg: 0.003990
  l1.weight: grad_norm = 0.102765
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.084921
Total gradient norm: 0.272975
=== Actor Training Debug (Iteration 6301) ===
Q mean: -12.423712
Q std: 19.276413
Actor loss: 12.427671
Action reg: 0.003960
  l1.weight: grad_norm = 0.306860
  l1.bias: grad_norm = 0.000617
  l2.weight: grad_norm = 0.213219
Total gradient norm: 0.600337
=== Actor Training Debug (Iteration 6302) ===
Q mean: -13.803535
Q std: 19.279734
Actor loss: 13.807518
Action reg: 0.003984
  l1.weight: grad_norm = 0.074270
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.071203
Total gradient norm: 0.231783
=== Actor Training Debug (Iteration 6303) ===
Q mean: -16.740158
Q std: 23.071674
Actor loss: 16.744148
Action reg: 0.003990
  l1.weight: grad_norm = 0.113126
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.088920
Total gradient norm: 0.290362
=== Actor Training Debug (Iteration 6304) ===
Q mean: -13.322124
Q std: 20.270020
Actor loss: 13.326105
Action reg: 0.003982
  l1.weight: grad_norm = 0.221105
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.155294
Total gradient norm: 0.482808
=== Actor Training Debug (Iteration 6305) ===
Q mean: -12.825239
Q std: 19.689796
Actor loss: 12.829208
Action reg: 0.003969
  l1.weight: grad_norm = 0.247296
  l1.bias: grad_norm = 0.001254
  l2.weight: grad_norm = 0.176067
Total gradient norm: 0.479809
=== Actor Training Debug (Iteration 6306) ===
Q mean: -12.817947
Q std: 20.232264
Actor loss: 12.821928
Action reg: 0.003981
  l1.weight: grad_norm = 0.134397
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.109911
Total gradient norm: 0.490010
=== Actor Training Debug (Iteration 6307) ===
Q mean: -12.272688
Q std: 19.865900
Actor loss: 12.276656
Action reg: 0.003968
  l1.weight: grad_norm = 0.182943
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.129944
Total gradient norm: 0.412818
=== Actor Training Debug (Iteration 6308) ===
Q mean: -13.565887
Q std: 19.139332
Actor loss: 13.569868
Action reg: 0.003981
  l1.weight: grad_norm = 0.198606
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.153382
Total gradient norm: 0.598929
=== Actor Training Debug (Iteration 6309) ===
Q mean: -15.836420
Q std: 21.736298
Actor loss: 15.840399
Action reg: 0.003979
  l1.weight: grad_norm = 0.166095
  l1.bias: grad_norm = 0.000432
  l2.weight: grad_norm = 0.132598
Total gradient norm: 0.414308
=== Actor Training Debug (Iteration 6310) ===
Q mean: -13.804602
Q std: 21.343233
Actor loss: 13.808578
Action reg: 0.003976
  l1.weight: grad_norm = 0.205528
  l1.bias: grad_norm = 0.001137
  l2.weight: grad_norm = 0.147694
Total gradient norm: 0.390216
=== Actor Training Debug (Iteration 6311) ===
Q mean: -15.600807
Q std: 20.792746
Actor loss: 15.604782
Action reg: 0.003975
  l1.weight: grad_norm = 0.157759
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.120908
Total gradient norm: 0.352855
=== Actor Training Debug (Iteration 6312) ===
Q mean: -14.612886
Q std: 20.461147
Actor loss: 14.616870
Action reg: 0.003984
  l1.weight: grad_norm = 0.193025
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.154144
Total gradient norm: 0.512187
=== Actor Training Debug (Iteration 6313) ===
Q mean: -15.487172
Q std: 20.599930
Actor loss: 15.491154
Action reg: 0.003982
  l1.weight: grad_norm = 0.250448
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.185099
Total gradient norm: 0.612763
=== Actor Training Debug (Iteration 6314) ===
Q mean: -13.120161
Q std: 19.333954
Actor loss: 13.124135
Action reg: 0.003974
  l1.weight: grad_norm = 0.283772
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.203794
Total gradient norm: 0.804137
=== Actor Training Debug (Iteration 6315) ===
Q mean: -14.870348
Q std: 21.023201
Actor loss: 14.874326
Action reg: 0.003978
  l1.weight: grad_norm = 0.153263
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.136976
Total gradient norm: 0.487603
=== Actor Training Debug (Iteration 6316) ===
Q mean: -11.647285
Q std: 19.309597
Actor loss: 11.651271
Action reg: 0.003985
  l1.weight: grad_norm = 0.161274
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.121545
Total gradient norm: 0.321138
=== Actor Training Debug (Iteration 6317) ===
Q mean: -14.478375
Q std: 21.824228
Actor loss: 14.482355
Action reg: 0.003980
  l1.weight: grad_norm = 0.203233
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.150242
Total gradient norm: 0.453683
=== Actor Training Debug (Iteration 6318) ===
Q mean: -13.448540
Q std: 19.377047
Actor loss: 13.452521
Action reg: 0.003982
  l1.weight: grad_norm = 0.141026
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.125067
Total gradient norm: 0.399689
=== Actor Training Debug (Iteration 6319) ===
Q mean: -15.136914
Q std: 21.615145
Actor loss: 15.140905
Action reg: 0.003991
  l1.weight: grad_norm = 0.090479
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.064931
Total gradient norm: 0.208782
=== Actor Training Debug (Iteration 6320) ===
Q mean: -13.079450
Q std: 18.392788
Actor loss: 13.083439
Action reg: 0.003989
  l1.weight: grad_norm = 0.083434
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.060290
Total gradient norm: 0.175343
=== Actor Training Debug (Iteration 6321) ===
Q mean: -14.596897
Q std: 20.553244
Actor loss: 14.600875
Action reg: 0.003978
  l1.weight: grad_norm = 0.064916
  l1.bias: grad_norm = 0.000662
  l2.weight: grad_norm = 0.052678
Total gradient norm: 0.158324
=== Actor Training Debug (Iteration 6322) ===
Q mean: -16.036726
Q std: 22.466394
Actor loss: 16.040714
Action reg: 0.003988
  l1.weight: grad_norm = 0.070795
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.071971
Total gradient norm: 0.219282
=== Actor Training Debug (Iteration 6323) ===
Q mean: -16.380247
Q std: 21.951977
Actor loss: 16.384232
Action reg: 0.003984
  l1.weight: grad_norm = 0.067129
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.053068
Total gradient norm: 0.152203
=== Actor Training Debug (Iteration 6324) ===
Q mean: -14.697163
Q std: 21.801777
Actor loss: 14.701137
Action reg: 0.003974
  l1.weight: grad_norm = 0.192150
  l1.bias: grad_norm = 0.000739
  l2.weight: grad_norm = 0.164233
Total gradient norm: 0.461642
=== Actor Training Debug (Iteration 6325) ===
Q mean: -14.819860
Q std: 20.255260
Actor loss: 14.823847
Action reg: 0.003986
  l1.weight: grad_norm = 0.110692
  l1.bias: grad_norm = 0.000939
  l2.weight: grad_norm = 0.072070
Total gradient norm: 0.220536
=== Actor Training Debug (Iteration 6326) ===
Q mean: -14.087582
Q std: 20.692131
Actor loss: 14.091566
Action reg: 0.003985
  l1.weight: grad_norm = 0.139585
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.110430
Total gradient norm: 0.308003
=== Actor Training Debug (Iteration 6327) ===
Q mean: -14.557252
Q std: 20.196236
Actor loss: 14.561236
Action reg: 0.003984
  l1.weight: grad_norm = 0.218925
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.160291
Total gradient norm: 0.550958
=== Actor Training Debug (Iteration 6328) ===
Q mean: -15.211113
Q std: 19.983391
Actor loss: 15.215095
Action reg: 0.003982
  l1.weight: grad_norm = 0.295742
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.197467
Total gradient norm: 0.526850
=== Actor Training Debug (Iteration 6329) ===
Q mean: -14.537313
Q std: 20.256887
Actor loss: 14.541299
Action reg: 0.003985
  l1.weight: grad_norm = 0.130225
  l1.bias: grad_norm = 0.000624
  l2.weight: grad_norm = 0.096797
Total gradient norm: 0.345766
=== Actor Training Debug (Iteration 6330) ===
Q mean: -14.319014
Q std: 21.147911
Actor loss: 14.322993
Action reg: 0.003980
  l1.weight: grad_norm = 0.262388
  l1.bias: grad_norm = 0.000449
  l2.weight: grad_norm = 0.167488
Total gradient norm: 0.444224
=== Actor Training Debug (Iteration 6331) ===
Q mean: -13.296501
Q std: 19.373945
Actor loss: 13.300486
Action reg: 0.003984
  l1.weight: grad_norm = 0.118513
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.101382
Total gradient norm: 0.396340
=== Actor Training Debug (Iteration 6332) ===
Q mean: -12.537640
Q std: 19.657745
Actor loss: 12.541623
Action reg: 0.003984
  l1.weight: grad_norm = 0.100701
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.075376
Total gradient norm: 0.211872
=== Actor Training Debug (Iteration 6333) ===
Q mean: -12.719940
Q std: 19.879192
Actor loss: 12.723920
Action reg: 0.003980
  l1.weight: grad_norm = 0.174336
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.136388
Total gradient norm: 0.395421
=== Actor Training Debug (Iteration 6334) ===
Q mean: -13.525543
Q std: 20.296247
Actor loss: 13.529524
Action reg: 0.003980
  l1.weight: grad_norm = 0.198960
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.151462
Total gradient norm: 0.461200
=== Actor Training Debug (Iteration 6335) ===
Q mean: -14.601002
Q std: 20.063131
Actor loss: 14.604972
Action reg: 0.003970
  l1.weight: grad_norm = 0.194415
  l1.bias: grad_norm = 0.000559
  l2.weight: grad_norm = 0.151433
Total gradient norm: 0.448185
=== Actor Training Debug (Iteration 6336) ===
Q mean: -14.060844
Q std: 20.320227
Actor loss: 14.064828
Action reg: 0.003984
  l1.weight: grad_norm = 0.047024
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.032692
Total gradient norm: 0.106406
=== Actor Training Debug (Iteration 6337) ===
Q mean: -13.780867
Q std: 19.513788
Actor loss: 13.784852
Action reg: 0.003986
  l1.weight: grad_norm = 0.127273
  l1.bias: grad_norm = 0.000538
  l2.weight: grad_norm = 0.092868
Total gradient norm: 0.271547
=== Actor Training Debug (Iteration 6338) ===
Q mean: -13.385427
Q std: 20.165623
Actor loss: 13.389402
Action reg: 0.003975
  l1.weight: grad_norm = 0.161066
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.136801
Total gradient norm: 0.400042
=== Actor Training Debug (Iteration 6339) ===
Q mean: -13.553379
Q std: 20.082520
Actor loss: 13.557362
Action reg: 0.003982
  l1.weight: grad_norm = 0.093057
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.058140
Total gradient norm: 0.173389
=== Actor Training Debug (Iteration 6340) ===
Q mean: -14.423772
Q std: 20.557144
Actor loss: 14.427746
Action reg: 0.003974
  l1.weight: grad_norm = 0.226563
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.174240
Total gradient norm: 0.470053
=== Actor Training Debug (Iteration 6341) ===
Q mean: -12.750335
Q std: 18.964678
Actor loss: 12.754303
Action reg: 0.003968
  l1.weight: grad_norm = 0.119788
  l1.bias: grad_norm = 0.001084
  l2.weight: grad_norm = 0.095184
Total gradient norm: 0.292807
=== Actor Training Debug (Iteration 6342) ===
Q mean: -13.509645
Q std: 19.977541
Actor loss: 13.513626
Action reg: 0.003980
  l1.weight: grad_norm = 0.092429
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.072279
Total gradient norm: 0.236354
=== Actor Training Debug (Iteration 6343) ===
Q mean: -13.771563
Q std: 20.122276
Actor loss: 13.775543
Action reg: 0.003981
  l1.weight: grad_norm = 0.070265
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.053690
Total gradient norm: 0.149683
=== Actor Training Debug (Iteration 6344) ===
Q mean: -14.894984
Q std: 20.037043
Actor loss: 14.898954
Action reg: 0.003970
  l1.weight: grad_norm = 0.131167
  l1.bias: grad_norm = 0.000753
  l2.weight: grad_norm = 0.106743
Total gradient norm: 0.325148
=== Actor Training Debug (Iteration 6345) ===
Q mean: -14.369102
Q std: 20.993769
Actor loss: 14.373085
Action reg: 0.003984
  l1.weight: grad_norm = 0.138834
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.105689
Total gradient norm: 0.275786
=== Actor Training Debug (Iteration 6346) ===
Q mean: -14.089685
Q std: 21.530380
Actor loss: 14.093668
Action reg: 0.003983
  l1.weight: grad_norm = 0.057633
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.047414
Total gradient norm: 0.141235
=== Actor Training Debug (Iteration 6347) ===
Q mean: -15.327358
Q std: 20.749615
Actor loss: 15.331335
Action reg: 0.003977
  l1.weight: grad_norm = 0.050059
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.040048
Total gradient norm: 0.122657
=== Actor Training Debug (Iteration 6348) ===
Q mean: -15.384991
Q std: 21.444817
Actor loss: 15.388967
Action reg: 0.003975
  l1.weight: grad_norm = 0.125754
  l1.bias: grad_norm = 0.001243
  l2.weight: grad_norm = 0.090217
Total gradient norm: 0.290806
=== Actor Training Debug (Iteration 6349) ===
Q mean: -15.832908
Q std: 22.175573
Actor loss: 15.836882
Action reg: 0.003974
  l1.weight: grad_norm = 0.102047
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.070050
Total gradient norm: 0.188647
=== Actor Training Debug (Iteration 6350) ===
Q mean: -14.034203
Q std: 20.599924
Actor loss: 14.038187
Action reg: 0.003984
  l1.weight: grad_norm = 0.041420
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.039132
Total gradient norm: 0.135828
=== Actor Training Debug (Iteration 6351) ===
Q mean: -14.915060
Q std: 21.237879
Actor loss: 14.919031
Action reg: 0.003971
  l1.weight: grad_norm = 0.323273
  l1.bias: grad_norm = 0.002935
  l2.weight: grad_norm = 0.262991
Total gradient norm: 0.679846
=== Actor Training Debug (Iteration 6352) ===
Q mean: -12.657932
Q std: 18.940092
Actor loss: 12.661910
Action reg: 0.003978
  l1.weight: grad_norm = 0.353591
  l1.bias: grad_norm = 0.000991
  l2.weight: grad_norm = 0.258528
Total gradient norm: 0.675573
=== Actor Training Debug (Iteration 6353) ===
Q mean: -12.615029
Q std: 19.117884
Actor loss: 12.619019
Action reg: 0.003989
  l1.weight: grad_norm = 0.182977
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.130342
Total gradient norm: 0.374864
=== Actor Training Debug (Iteration 6354) ===
Q mean: -13.929273
Q std: 19.824287
Actor loss: 13.933245
Action reg: 0.003972
  l1.weight: grad_norm = 0.154552
  l1.bias: grad_norm = 0.002150
  l2.weight: grad_norm = 0.122821
Total gradient norm: 0.347456
=== Actor Training Debug (Iteration 6355) ===
Q mean: -11.160486
Q std: 17.936020
Actor loss: 11.164457
Action reg: 0.003972
  l1.weight: grad_norm = 0.153612
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.119970
Total gradient norm: 0.383385
=== Actor Training Debug (Iteration 6356) ===
Q mean: -14.738359
Q std: 21.977331
Actor loss: 14.742340
Action reg: 0.003980
  l1.weight: grad_norm = 0.114110
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.082709
Total gradient norm: 0.244920
=== Actor Training Debug (Iteration 6357) ===
Q mean: -15.393524
Q std: 21.587025
Actor loss: 15.397499
Action reg: 0.003975
  l1.weight: grad_norm = 0.114104
  l1.bias: grad_norm = 0.000941
  l2.weight: grad_norm = 0.090954
Total gradient norm: 0.270822
=== Actor Training Debug (Iteration 6358) ===
Q mean: -14.453550
Q std: 20.445189
Actor loss: 14.457536
Action reg: 0.003985
  l1.weight: grad_norm = 0.094722
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.071003
Total gradient norm: 0.194879
=== Actor Training Debug (Iteration 6359) ===
Q mean: -13.324779
Q std: 19.867453
Actor loss: 13.328758
Action reg: 0.003980
  l1.weight: grad_norm = 0.165758
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.135899
Total gradient norm: 0.393001
=== Actor Training Debug (Iteration 6360) ===
Q mean: -14.640413
Q std: 20.662483
Actor loss: 14.644398
Action reg: 0.003984
  l1.weight: grad_norm = 0.144363
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.100308
Total gradient norm: 0.290910
=== Actor Training Debug (Iteration 6361) ===
Q mean: -13.650838
Q std: 20.242926
Actor loss: 13.654815
Action reg: 0.003976
  l1.weight: grad_norm = 0.127659
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.084419
Total gradient norm: 0.248063
=== Actor Training Debug (Iteration 6362) ===
Q mean: -14.457976
Q std: 19.523838
Actor loss: 14.461955
Action reg: 0.003979
  l1.weight: grad_norm = 0.102344
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.076383
Total gradient norm: 0.239652
=== Actor Training Debug (Iteration 6363) ===
Q mean: -16.775360
Q std: 22.347307
Actor loss: 16.779335
Action reg: 0.003974
  l1.weight: grad_norm = 0.091159
  l1.bias: grad_norm = 0.000845
  l2.weight: grad_norm = 0.060706
Total gradient norm: 0.178550
=== Actor Training Debug (Iteration 6364) ===
Q mean: -14.051273
Q std: 20.438583
Actor loss: 14.055251
Action reg: 0.003978
  l1.weight: grad_norm = 0.087738
  l1.bias: grad_norm = 0.001031
  l2.weight: grad_norm = 0.053441
Total gradient norm: 0.158953
=== Actor Training Debug (Iteration 6365) ===
Q mean: -15.536806
Q std: 21.804247
Actor loss: 15.540787
Action reg: 0.003981
  l1.weight: grad_norm = 0.070169
  l1.bias: grad_norm = 0.000534
  l2.weight: grad_norm = 0.062614
Total gradient norm: 0.181397
=== Actor Training Debug (Iteration 6366) ===
Q mean: -13.302855
Q std: 20.477177
Actor loss: 13.306827
Action reg: 0.003972
  l1.weight: grad_norm = 0.082467
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.068622
Total gradient norm: 0.198064
=== Actor Training Debug (Iteration 6367) ===
Q mean: -15.607444
Q std: 21.638439
Actor loss: 15.611412
Action reg: 0.003969
  l1.weight: grad_norm = 0.109123
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.086413
Total gradient norm: 0.245483
=== Actor Training Debug (Iteration 6368) ===
Q mean: -14.829983
Q std: 20.298500
Actor loss: 14.833954
Action reg: 0.003971
  l1.weight: grad_norm = 0.178539
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.164143
Total gradient norm: 0.541717
=== Actor Training Debug (Iteration 6369) ===
Q mean: -12.428007
Q std: 19.617477
Actor loss: 12.431979
Action reg: 0.003972
  l1.weight: grad_norm = 0.319712
  l1.bias: grad_norm = 0.000599
  l2.weight: grad_norm = 0.266019
Total gradient norm: 0.757822
=== Actor Training Debug (Iteration 6370) ===
Q mean: -13.802211
Q std: 20.199820
Actor loss: 13.806201
Action reg: 0.003990
  l1.weight: grad_norm = 0.176598
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.167790
Total gradient norm: 0.499020
=== Actor Training Debug (Iteration 6371) ===
Q mean: -14.295349
Q std: 22.069805
Actor loss: 14.299318
Action reg: 0.003969
  l1.weight: grad_norm = 0.090610
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.066767
Total gradient norm: 0.210348
=== Actor Training Debug (Iteration 6372) ===
Q mean: -15.393494
Q std: 21.106516
Actor loss: 15.397461
Action reg: 0.003967
  l1.weight: grad_norm = 0.338580
  l1.bias: grad_norm = 0.001119
  l2.weight: grad_norm = 0.234593
Total gradient norm: 0.669775
=== Actor Training Debug (Iteration 6373) ===
Q mean: -14.736986
Q std: 21.219589
Actor loss: 14.740953
Action reg: 0.003968
  l1.weight: grad_norm = 0.084377
  l1.bias: grad_norm = 0.000822
  l2.weight: grad_norm = 0.066039
Total gradient norm: 0.194560
=== Actor Training Debug (Iteration 6374) ===
Q mean: -13.229351
Q std: 20.651499
Actor loss: 13.233339
Action reg: 0.003988
  l1.weight: grad_norm = 0.075763
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.059277
Total gradient norm: 0.161396
=== Actor Training Debug (Iteration 6375) ===
Q mean: -15.361260
Q std: 21.876989
Actor loss: 15.365239
Action reg: 0.003979
  l1.weight: grad_norm = 0.066194
  l1.bias: grad_norm = 0.000793
  l2.weight: grad_norm = 0.048671
Total gradient norm: 0.140081
=== Actor Training Debug (Iteration 6376) ===
Q mean: -14.012693
Q std: 20.952044
Actor loss: 14.016670
Action reg: 0.003977
  l1.weight: grad_norm = 0.121778
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.100448
Total gradient norm: 0.313139
=== Actor Training Debug (Iteration 6377) ===
Q mean: -13.841061
Q std: 20.875278
Actor loss: 13.845040
Action reg: 0.003980
  l1.weight: grad_norm = 0.121637
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.093401
Total gradient norm: 0.280632
=== Actor Training Debug (Iteration 6378) ===
Q mean: -13.892879
Q std: 19.759357
Actor loss: 13.896869
Action reg: 0.003989
  l1.weight: grad_norm = 0.209339
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.190969
Total gradient norm: 0.527342
=== Actor Training Debug (Iteration 6379) ===
Q mean: -12.511311
Q std: 19.643595
Actor loss: 12.515290
Action reg: 0.003980
  l1.weight: grad_norm = 0.078259
  l1.bias: grad_norm = 0.000687
  l2.weight: grad_norm = 0.055363
Total gradient norm: 0.183806
=== Actor Training Debug (Iteration 6380) ===
Q mean: -17.311878
Q std: 23.052986
Actor loss: 17.315855
Action reg: 0.003977
  l1.weight: grad_norm = 0.083428
  l1.bias: grad_norm = 0.001643
  l2.weight: grad_norm = 0.066108
Total gradient norm: 0.210644
=== Actor Training Debug (Iteration 6381) ===
Q mean: -12.622519
Q std: 19.516748
Actor loss: 12.626497
Action reg: 0.003977
  l1.weight: grad_norm = 0.087057
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.068861
Total gradient norm: 0.201958
=== Actor Training Debug (Iteration 6382) ===
Q mean: -12.450598
Q std: 18.736612
Actor loss: 12.454583
Action reg: 0.003986
  l1.weight: grad_norm = 0.102357
  l1.bias: grad_norm = 0.000594
  l2.weight: grad_norm = 0.077781
Total gradient norm: 0.279167
=== Actor Training Debug (Iteration 6383) ===
Q mean: -11.240307
Q std: 19.363401
Actor loss: 11.244291
Action reg: 0.003985
  l1.weight: grad_norm = 0.037364
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.032864
Total gradient norm: 0.102083
=== Actor Training Debug (Iteration 6384) ===
Q mean: -13.665443
Q std: 20.954645
Actor loss: 13.669436
Action reg: 0.003993
  l1.weight: grad_norm = 0.050020
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.033828
Total gradient norm: 0.099735
=== Actor Training Debug (Iteration 6385) ===
Q mean: -17.144211
Q std: 22.846565
Actor loss: 17.148199
Action reg: 0.003988
  l1.weight: grad_norm = 0.087519
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.070504
Total gradient norm: 0.251147
=== Actor Training Debug (Iteration 6386) ===
Q mean: -15.810657
Q std: 22.244974
Actor loss: 15.814643
Action reg: 0.003987
  l1.weight: grad_norm = 0.213987
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.191390
Total gradient norm: 0.487754
=== Actor Training Debug (Iteration 6387) ===
Q mean: -14.757188
Q std: 21.672913
Actor loss: 14.761172
Action reg: 0.003984
  l1.weight: grad_norm = 0.216365
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.134187
Total gradient norm: 0.436409
=== Actor Training Debug (Iteration 6388) ===
Q mean: -13.499215
Q std: 20.590754
Actor loss: 13.503203
Action reg: 0.003988
  l1.weight: grad_norm = 0.131682
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.108551
Total gradient norm: 0.340997
=== Actor Training Debug (Iteration 6389) ===
Q mean: -11.827998
Q std: 18.213570
Actor loss: 11.831963
Action reg: 0.003965
  l1.weight: grad_norm = 0.208876
  l1.bias: grad_norm = 0.000607
  l2.weight: grad_norm = 0.155970
Total gradient norm: 0.525061
=== Actor Training Debug (Iteration 6390) ===
Q mean: -15.139189
Q std: 21.224924
Actor loss: 15.143168
Action reg: 0.003980
  l1.weight: grad_norm = 0.069455
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.053482
Total gradient norm: 0.189979
=== Actor Training Debug (Iteration 6391) ===
Q mean: -14.769817
Q std: 21.592604
Actor loss: 14.773794
Action reg: 0.003977
  l1.weight: grad_norm = 0.151244
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.114288
Total gradient norm: 0.347872
=== Actor Training Debug (Iteration 6392) ===
Q mean: -15.002674
Q std: 20.416161
Actor loss: 15.006649
Action reg: 0.003975
  l1.weight: grad_norm = 0.080855
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.053655
Total gradient norm: 0.173590
=== Actor Training Debug (Iteration 6393) ===
Q mean: -14.860904
Q std: 22.220610
Actor loss: 14.864878
Action reg: 0.003974
  l1.weight: grad_norm = 0.188199
  l1.bias: grad_norm = 0.000584
  l2.weight: grad_norm = 0.136482
Total gradient norm: 0.418400
=== Actor Training Debug (Iteration 6394) ===
Q mean: -13.146746
Q std: 20.278099
Actor loss: 13.150733
Action reg: 0.003987
  l1.weight: grad_norm = 0.096799
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.088837
Total gradient norm: 0.254890
=== Actor Training Debug (Iteration 6395) ===
Q mean: -11.377975
Q std: 18.078426
Actor loss: 11.381957
Action reg: 0.003982
  l1.weight: grad_norm = 0.134752
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.101222
Total gradient norm: 0.283892
=== Actor Training Debug (Iteration 6396) ===
Q mean: -12.114256
Q std: 18.935301
Actor loss: 12.118229
Action reg: 0.003973
  l1.weight: grad_norm = 0.336880
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.256953
Total gradient norm: 0.722656
=== Actor Training Debug (Iteration 6397) ===
Q mean: -13.648772
Q std: 19.472906
Actor loss: 13.652750
Action reg: 0.003978
  l1.weight: grad_norm = 0.116536
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.074869
Total gradient norm: 0.213407
=== Actor Training Debug (Iteration 6398) ===
Q mean: -16.051502
Q std: 21.724566
Actor loss: 16.055485
Action reg: 0.003982
  l1.weight: grad_norm = 0.062449
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.041167
Total gradient norm: 0.117778
=== Actor Training Debug (Iteration 6399) ===
Q mean: -12.697865
Q std: 18.973454
Actor loss: 12.701849
Action reg: 0.003985
  l1.weight: grad_norm = 0.166560
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.133911
Total gradient norm: 0.384243
=== Actor Training Debug (Iteration 6400) ===
Q mean: -11.747488
Q std: 17.717674
Actor loss: 11.751467
Action reg: 0.003979
  l1.weight: grad_norm = 0.159050
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.154320
Total gradient norm: 0.463403
=== Actor Training Debug (Iteration 6401) ===
Q mean: -12.518414
Q std: 18.808891
Actor loss: 12.522386
Action reg: 0.003972
  l1.weight: grad_norm = 0.132802
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.094196
Total gradient norm: 0.294153
=== Actor Training Debug (Iteration 6402) ===
Q mean: -13.775921
Q std: 20.701895
Actor loss: 13.779895
Action reg: 0.003974
  l1.weight: grad_norm = 0.249786
  l1.bias: grad_norm = 0.001070
  l2.weight: grad_norm = 0.168428
Total gradient norm: 0.555066
=== Actor Training Debug (Iteration 6403) ===
Q mean: -13.670732
Q std: 19.939077
Actor loss: 13.674706
Action reg: 0.003974
  l1.weight: grad_norm = 0.227373
  l1.bias: grad_norm = 0.001424
  l2.weight: grad_norm = 0.144120
Total gradient norm: 0.433132
=== Actor Training Debug (Iteration 6404) ===
Q mean: -14.189049
Q std: 20.262024
Actor loss: 14.193024
Action reg: 0.003974
  l1.weight: grad_norm = 0.199297
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.127404
Total gradient norm: 0.365012
=== Actor Training Debug (Iteration 6405) ===
Q mean: -13.761386
Q std: 21.482092
Actor loss: 13.765359
Action reg: 0.003973
  l1.weight: grad_norm = 0.268401
  l1.bias: grad_norm = 0.000888
  l2.weight: grad_norm = 0.193635
Total gradient norm: 0.577788
=== Actor Training Debug (Iteration 6406) ===
Q mean: -11.677134
Q std: 19.221338
Actor loss: 11.681109
Action reg: 0.003976
  l1.weight: grad_norm = 0.138116
  l1.bias: grad_norm = 0.001204
  l2.weight: grad_norm = 0.099926
Total gradient norm: 0.313901
=== Actor Training Debug (Iteration 6407) ===
Q mean: -13.914379
Q std: 20.735987
Actor loss: 13.918363
Action reg: 0.003984
  l1.weight: grad_norm = 0.135205
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.113602
Total gradient norm: 0.327172
=== Actor Training Debug (Iteration 6408) ===
Q mean: -15.700941
Q std: 22.196018
Actor loss: 15.704915
Action reg: 0.003974
  l1.weight: grad_norm = 0.111420
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.080479
Total gradient norm: 0.201197
=== Actor Training Debug (Iteration 6409) ===
Q mean: -12.856573
Q std: 19.549387
Actor loss: 12.860559
Action reg: 0.003985
  l1.weight: grad_norm = 0.066731
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.063042
Total gradient norm: 0.176882
=== Actor Training Debug (Iteration 6410) ===
Q mean: -14.678977
Q std: 21.296309
Actor loss: 14.682944
Action reg: 0.003967
  l1.weight: grad_norm = 0.244884
  l1.bias: grad_norm = 0.000844
  l2.weight: grad_norm = 0.184487
Total gradient norm: 0.544554
=== Actor Training Debug (Iteration 6411) ===
Q mean: -15.441392
Q std: 20.359741
Actor loss: 15.445380
Action reg: 0.003989
  l1.weight: grad_norm = 0.084496
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.069911
Total gradient norm: 0.243980
=== Actor Training Debug (Iteration 6412) ===
Q mean: -15.541718
Q std: 22.251209
Actor loss: 15.545693
Action reg: 0.003975
  l1.weight: grad_norm = 0.228047
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.156637
Total gradient norm: 0.483680
=== Actor Training Debug (Iteration 6413) ===
Q mean: -14.223242
Q std: 20.693834
Actor loss: 14.227220
Action reg: 0.003978
  l1.weight: grad_norm = 0.146545
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.120490
Total gradient norm: 0.366718
=== Actor Training Debug (Iteration 6414) ===
Q mean: -13.566691
Q std: 20.803928
Actor loss: 13.570669
Action reg: 0.003978
  l1.weight: grad_norm = 0.092830
  l1.bias: grad_norm = 0.000543
  l2.weight: grad_norm = 0.069855
Total gradient norm: 0.236549
=== Actor Training Debug (Iteration 6415) ===
Q mean: -13.231628
Q std: 20.356855
Actor loss: 13.235610
Action reg: 0.003981
  l1.weight: grad_norm = 0.174447
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.129468
Total gradient norm: 0.421984
=== Actor Training Debug (Iteration 6416) ===
Q mean: -12.010386
Q std: 19.811907
Actor loss: 12.014366
Action reg: 0.003980
  l1.weight: grad_norm = 0.192631
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.145884
Total gradient norm: 0.446105
=== Actor Training Debug (Iteration 6417) ===
Q mean: -12.845199
Q std: 19.677402
Actor loss: 12.849173
Action reg: 0.003974
  l1.weight: grad_norm = 0.166614
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.129470
Total gradient norm: 0.392849
=== Actor Training Debug (Iteration 6418) ===
Q mean: -14.504180
Q std: 20.659094
Actor loss: 14.508169
Action reg: 0.003989
  l1.weight: grad_norm = 0.052728
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.046636
Total gradient norm: 0.134397
=== Actor Training Debug (Iteration 6419) ===
Q mean: -14.657742
Q std: 18.969643
Actor loss: 14.661717
Action reg: 0.003976
  l1.weight: grad_norm = 0.098714
  l1.bias: grad_norm = 0.000755
  l2.weight: grad_norm = 0.068720
Total gradient norm: 0.211987
=== Actor Training Debug (Iteration 6420) ===
Q mean: -13.695661
Q std: 20.068916
Actor loss: 13.699636
Action reg: 0.003976
  l1.weight: grad_norm = 0.150428
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.123348
Total gradient norm: 0.362545
=== Actor Training Debug (Iteration 6421) ===
Q mean: -13.303891
Q std: 20.282949
Actor loss: 13.307861
Action reg: 0.003970
  l1.weight: grad_norm = 0.200065
  l1.bias: grad_norm = 0.001461
  l2.weight: grad_norm = 0.159165
Total gradient norm: 0.485781
=== Actor Training Debug (Iteration 6422) ===
Q mean: -14.056597
Q std: 22.428284
Actor loss: 14.060582
Action reg: 0.003985
  l1.weight: grad_norm = 0.138936
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.105467
Total gradient norm: 0.326948
=== Actor Training Debug (Iteration 6423) ===
Q mean: -14.940115
Q std: 21.219603
Actor loss: 14.944089
Action reg: 0.003974
  l1.weight: grad_norm = 0.191953
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.129317
Total gradient norm: 0.347608
=== Actor Training Debug (Iteration 6424) ===
Q mean: -13.098053
Q std: 19.534491
Actor loss: 13.102045
Action reg: 0.003992
  l1.weight: grad_norm = 0.084781
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.057789
Total gradient norm: 0.168622
=== Actor Training Debug (Iteration 6425) ===
Q mean: -12.306313
Q std: 18.791819
Actor loss: 12.310294
Action reg: 0.003982
  l1.weight: grad_norm = 0.113375
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.109308
Total gradient norm: 0.374246
=== Actor Training Debug (Iteration 6426) ===
Q mean: -13.464752
Q std: 19.167727
Actor loss: 13.468731
Action reg: 0.003979
  l1.weight: grad_norm = 0.209438
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.156989
Total gradient norm: 0.430073
=== Actor Training Debug (Iteration 6427) ===
Q mean: -15.361835
Q std: 21.790028
Actor loss: 15.365806
Action reg: 0.003971
  l1.weight: grad_norm = 0.092486
  l1.bias: grad_norm = 0.001140
  l2.weight: grad_norm = 0.074715
Total gradient norm: 0.264605
=== Actor Training Debug (Iteration 6428) ===
Q mean: -13.551441
Q std: 21.793465
Actor loss: 13.555411
Action reg: 0.003970
  l1.weight: grad_norm = 0.326715
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.250489
Total gradient norm: 0.695666
=== Actor Training Debug (Iteration 6429) ===
Q mean: -15.453323
Q std: 22.134386
Actor loss: 15.457308
Action reg: 0.003984
  l1.weight: grad_norm = 0.174256
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.115227
Total gradient norm: 0.367245
=== Actor Training Debug (Iteration 6430) ===
Q mean: -15.533168
Q std: 21.980871
Actor loss: 15.537156
Action reg: 0.003989
  l1.weight: grad_norm = 0.142012
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.114614
Total gradient norm: 0.335662
=== Actor Training Debug (Iteration 6431) ===
Q mean: -12.219460
Q std: 18.206297
Actor loss: 12.223439
Action reg: 0.003980
  l1.weight: grad_norm = 0.077109
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.053437
Total gradient norm: 0.159790
=== Actor Training Debug (Iteration 6432) ===
Q mean: -12.724987
Q std: 19.563557
Actor loss: 12.728973
Action reg: 0.003986
  l1.weight: grad_norm = 0.088996
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.069652
Total gradient norm: 0.199384
=== Actor Training Debug (Iteration 6433) ===
Q mean: -13.445987
Q std: 19.791748
Actor loss: 13.449961
Action reg: 0.003974
  l1.weight: grad_norm = 0.241516
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.174851
Total gradient norm: 0.489439
=== Actor Training Debug (Iteration 6434) ===
Q mean: -14.274849
Q std: 20.428267
Actor loss: 14.278821
Action reg: 0.003972
  l1.weight: grad_norm = 0.193202
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.163042
Total gradient norm: 0.485572
=== Actor Training Debug (Iteration 6435) ===
Q mean: -16.905014
Q std: 22.377226
Actor loss: 16.908991
Action reg: 0.003977
  l1.weight: grad_norm = 0.091686
  l1.bias: grad_norm = 0.000720
  l2.weight: grad_norm = 0.068740
Total gradient norm: 0.203110
=== Actor Training Debug (Iteration 6436) ===
Q mean: -14.640224
Q std: 20.520182
Actor loss: 14.644214
Action reg: 0.003989
  l1.weight: grad_norm = 0.027771
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.019773
Total gradient norm: 0.064299
=== Actor Training Debug (Iteration 6437) ===
Q mean: -13.762857
Q std: 20.660500
Actor loss: 13.766829
Action reg: 0.003971
  l1.weight: grad_norm = 0.106092
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.078130
Total gradient norm: 0.240781
=== Actor Training Debug (Iteration 6438) ===
Q mean: -12.073308
Q std: 18.882092
Actor loss: 12.077292
Action reg: 0.003985
  l1.weight: grad_norm = 0.175012
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.137129
Total gradient norm: 0.361564
=== Actor Training Debug (Iteration 6439) ===
Q mean: -15.737459
Q std: 23.085220
Actor loss: 15.741436
Action reg: 0.003977
  l1.weight: grad_norm = 0.157283
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.127349
Total gradient norm: 0.392836
=== Actor Training Debug (Iteration 6440) ===
Q mean: -12.064769
Q std: 19.339108
Actor loss: 12.068744
Action reg: 0.003975
  l1.weight: grad_norm = 0.242699
  l1.bias: grad_norm = 0.000667
  l2.weight: grad_norm = 0.167447
Total gradient norm: 0.530945
=== Actor Training Debug (Iteration 6441) ===
Q mean: -14.591049
Q std: 20.990997
Actor loss: 14.595029
Action reg: 0.003980
  l1.weight: grad_norm = 0.318751
  l1.bias: grad_norm = 0.000405
  l2.weight: grad_norm = 0.211699
Total gradient norm: 0.669123
=== Actor Training Debug (Iteration 6442) ===
Q mean: -14.423169
Q std: 21.443527
Actor loss: 14.427149
Action reg: 0.003980
  l1.weight: grad_norm = 0.190592
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.140403
Total gradient norm: 0.427629
=== Actor Training Debug (Iteration 6443) ===
Q mean: -13.711759
Q std: 20.826197
Actor loss: 13.715744
Action reg: 0.003986
  l1.weight: grad_norm = 0.110024
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.078625
Total gradient norm: 0.259053
=== Actor Training Debug (Iteration 6444) ===
Q mean: -15.125191
Q std: 20.836212
Actor loss: 15.129175
Action reg: 0.003984
  l1.weight: grad_norm = 0.140466
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.097531
Total gradient norm: 0.274579
=== Actor Training Debug (Iteration 6445) ===
Q mean: -11.433304
Q std: 18.506790
Actor loss: 11.437284
Action reg: 0.003981
  l1.weight: grad_norm = 0.149607
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.103106
Total gradient norm: 0.292135
=== Actor Training Debug (Iteration 6446) ===
Q mean: -12.943983
Q std: 18.441122
Actor loss: 12.947965
Action reg: 0.003981
  l1.weight: grad_norm = 0.145408
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.118464
Total gradient norm: 0.361909
=== Actor Training Debug (Iteration 6447) ===
Q mean: -12.835560
Q std: 19.734917
Actor loss: 12.839535
Action reg: 0.003975
  l1.weight: grad_norm = 0.142945
  l1.bias: grad_norm = 0.000791
  l2.weight: grad_norm = 0.113832
Total gradient norm: 0.294662
=== Actor Training Debug (Iteration 6448) ===
Q mean: -14.479624
Q std: 20.545198
Actor loss: 14.483604
Action reg: 0.003981
  l1.weight: grad_norm = 0.059190
  l1.bias: grad_norm = 0.001193
  l2.weight: grad_norm = 0.045488
Total gradient norm: 0.138613
=== Actor Training Debug (Iteration 6449) ===
Q mean: -12.483141
Q std: 19.356268
Actor loss: 12.487120
Action reg: 0.003979
  l1.weight: grad_norm = 0.114423
  l1.bias: grad_norm = 0.000612
  l2.weight: grad_norm = 0.087856
Total gradient norm: 0.243233
=== Actor Training Debug (Iteration 6450) ===
Q mean: -12.062284
Q std: 19.490334
Actor loss: 12.066257
Action reg: 0.003973
  l1.weight: grad_norm = 0.253483
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.160135
Total gradient norm: 0.440947
=== Actor Training Debug (Iteration 6451) ===
Q mean: -15.282463
Q std: 20.780439
Actor loss: 15.286448
Action reg: 0.003985
  l1.weight: grad_norm = 0.083941
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.067470
Total gradient norm: 0.199344
=== Actor Training Debug (Iteration 6452) ===
Q mean: -13.099778
Q std: 20.224211
Actor loss: 13.103758
Action reg: 0.003979
  l1.weight: grad_norm = 0.124276
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.080603
Total gradient norm: 0.253725
=== Actor Training Debug (Iteration 6453) ===
Q mean: -13.473097
Q std: 20.639271
Actor loss: 13.477070
Action reg: 0.003973
  l1.weight: grad_norm = 0.105529
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.072120
Total gradient norm: 0.192359
=== Actor Training Debug (Iteration 6454) ===
Q mean: -15.314400
Q std: 21.102806
Actor loss: 15.318371
Action reg: 0.003971
  l1.weight: grad_norm = 0.127925
  l1.bias: grad_norm = 0.000509
  l2.weight: grad_norm = 0.101793
Total gradient norm: 0.318103
=== Actor Training Debug (Iteration 6455) ===
Q mean: -11.816537
Q std: 19.122883
Actor loss: 11.820509
Action reg: 0.003972
  l1.weight: grad_norm = 0.220018
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.148819
Total gradient norm: 0.514366
=== Actor Training Debug (Iteration 6456) ===
Q mean: -15.356344
Q std: 21.400162
Actor loss: 15.360328
Action reg: 0.003984
  l1.weight: grad_norm = 0.334335
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.253560
Total gradient norm: 0.807346
=== Actor Training Debug (Iteration 6457) ===
Q mean: -17.012516
Q std: 21.522043
Actor loss: 17.016502
Action reg: 0.003987
  l1.weight: grad_norm = 0.060944
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.043477
Total gradient norm: 0.135113
=== Actor Training Debug (Iteration 6458) ===
Q mean: -16.565594
Q std: 22.126858
Actor loss: 16.569580
Action reg: 0.003986
  l1.weight: grad_norm = 0.080642
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.052949
Total gradient norm: 0.148690
=== Actor Training Debug (Iteration 6459) ===
Q mean: -13.151857
Q std: 20.512768
Actor loss: 13.155834
Action reg: 0.003977
  l1.weight: grad_norm = 0.288744
  l1.bias: grad_norm = 0.001743
  l2.weight: grad_norm = 0.199220
Total gradient norm: 0.547623
=== Actor Training Debug (Iteration 6460) ===
Q mean: -12.087763
Q std: 19.029613
Actor loss: 12.091736
Action reg: 0.003973
  l1.weight: grad_norm = 0.112994
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.078558
Total gradient norm: 0.242169
=== Actor Training Debug (Iteration 6461) ===
Q mean: -13.096004
Q std: 20.159718
Actor loss: 13.099980
Action reg: 0.003977
  l1.weight: grad_norm = 0.158108
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.118378
Total gradient norm: 0.344768
=== Actor Training Debug (Iteration 6462) ===
Q mean: -14.810169
Q std: 19.519840
Actor loss: 14.814139
Action reg: 0.003970
  l1.weight: grad_norm = 0.180801
  l1.bias: grad_norm = 0.000528
  l2.weight: grad_norm = 0.159056
Total gradient norm: 0.466072
=== Actor Training Debug (Iteration 6463) ===
Q mean: -13.566015
Q std: 19.973091
Actor loss: 13.569981
Action reg: 0.003965
  l1.weight: grad_norm = 0.323304
  l1.bias: grad_norm = 0.000795
  l2.weight: grad_norm = 0.242517
Total gradient norm: 0.926915
=== Actor Training Debug (Iteration 6464) ===
Q mean: -14.439468
Q std: 19.973579
Actor loss: 14.443438
Action reg: 0.003969
  l1.weight: grad_norm = 0.108759
  l1.bias: grad_norm = 0.000822
  l2.weight: grad_norm = 0.089885
Total gradient norm: 0.262891
=== Actor Training Debug (Iteration 6465) ===
Q mean: -13.278980
Q std: 20.422314
Actor loss: 13.282965
Action reg: 0.003984
  l1.weight: grad_norm = 0.241590
  l1.bias: grad_norm = 0.000517
  l2.weight: grad_norm = 0.198114
Total gradient norm: 0.648683
=== Actor Training Debug (Iteration 6466) ===
Q mean: -12.059784
Q std: 17.569937
Actor loss: 12.063766
Action reg: 0.003982
  l1.weight: grad_norm = 0.176309
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.159457
Total gradient norm: 0.535745
=== Actor Training Debug (Iteration 6467) ===
Q mean: -12.223459
Q std: 19.172014
Actor loss: 12.227424
Action reg: 0.003964
  l1.weight: grad_norm = 0.326138
  l1.bias: grad_norm = 0.001089
  l2.weight: grad_norm = 0.192127
Total gradient norm: 0.570080
=== Actor Training Debug (Iteration 6468) ===
Q mean: -13.206163
Q std: 19.782581
Actor loss: 13.210146
Action reg: 0.003982
  l1.weight: grad_norm = 0.210699
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.139885
Total gradient norm: 0.389511
=== Actor Training Debug (Iteration 6469) ===
Q mean: -12.392253
Q std: 21.087395
Actor loss: 12.396227
Action reg: 0.003974
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.097335
Total gradient norm: 0.297577
=== Actor Training Debug (Iteration 6470) ===
Q mean: -13.654827
Q std: 21.137653
Actor loss: 13.658803
Action reg: 0.003975
  l1.weight: grad_norm = 0.152593
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.112014
Total gradient norm: 0.321824
=== Actor Training Debug (Iteration 6471) ===
Q mean: -12.103595
Q std: 19.604334
Actor loss: 12.107572
Action reg: 0.003977
  l1.weight: grad_norm = 0.211546
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.164408
Total gradient norm: 0.533768
=== Actor Training Debug (Iteration 6472) ===
Q mean: -16.292267
Q std: 21.622143
Actor loss: 16.296234
Action reg: 0.003968
  l1.weight: grad_norm = 0.393118
  l1.bias: grad_norm = 0.000780
  l2.weight: grad_norm = 0.299295
Total gradient norm: 1.053441
=== Actor Training Debug (Iteration 6473) ===
Q mean: -14.530167
Q std: 20.777952
Actor loss: 14.534134
Action reg: 0.003967
  l1.weight: grad_norm = 0.276810
  l1.bias: grad_norm = 0.001250
  l2.weight: grad_norm = 0.166996
Total gradient norm: 0.491183
=== Actor Training Debug (Iteration 6474) ===
Q mean: -14.955392
Q std: 21.078360
Actor loss: 14.959373
Action reg: 0.003981
  l1.weight: grad_norm = 0.136762
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.111288
Total gradient norm: 0.284900
=== Actor Training Debug (Iteration 6475) ===
Q mean: -11.958800
Q std: 18.549028
Actor loss: 11.962783
Action reg: 0.003982
  l1.weight: grad_norm = 0.199038
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.151122
Total gradient norm: 0.437429
=== Actor Training Debug (Iteration 6476) ===
Q mean: -13.036886
Q std: 19.546207
Actor loss: 13.040870
Action reg: 0.003984
  l1.weight: grad_norm = 0.077552
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.062828
Total gradient norm: 0.183467
=== Actor Training Debug (Iteration 6477) ===
Q mean: -16.202654
Q std: 20.902756
Actor loss: 16.206625
Action reg: 0.003972
  l1.weight: grad_norm = 0.172776
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.129033
Total gradient norm: 0.387041
=== Actor Training Debug (Iteration 6478) ===
Q mean: -12.042684
Q std: 19.266386
Actor loss: 12.046660
Action reg: 0.003976
  l1.weight: grad_norm = 0.170078
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.129099
Total gradient norm: 0.406198
=== Actor Training Debug (Iteration 6479) ===
Q mean: -11.867035
Q std: 19.148319
Actor loss: 11.871017
Action reg: 0.003981
  l1.weight: grad_norm = 0.426699
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.341194
Total gradient norm: 0.818032
=== Actor Training Debug (Iteration 6480) ===
Q mean: -12.255875
Q std: 19.126600
Actor loss: 12.259848
Action reg: 0.003973
  l1.weight: grad_norm = 0.209793
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.149420
Total gradient norm: 0.467465
=== Actor Training Debug (Iteration 6481) ===
Q mean: -16.742884
Q std: 21.579189
Actor loss: 16.746872
Action reg: 0.003988
  l1.weight: grad_norm = 0.111407
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.088871
Total gradient norm: 0.298158
=== Actor Training Debug (Iteration 6482) ===
Q mean: -14.391333
Q std: 20.902065
Actor loss: 14.395310
Action reg: 0.003978
  l1.weight: grad_norm = 0.127487
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.097212
Total gradient norm: 0.278684
=== Actor Training Debug (Iteration 6483) ===
Q mean: -13.701091
Q std: 21.413774
Actor loss: 13.705081
Action reg: 0.003990
  l1.weight: grad_norm = 0.082085
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.057373
Total gradient norm: 0.184296
=== Actor Training Debug (Iteration 6484) ===
Q mean: -13.015846
Q std: 19.989752
Actor loss: 13.019823
Action reg: 0.003976
  l1.weight: grad_norm = 0.121274
  l1.bias: grad_norm = 0.000781
  l2.weight: grad_norm = 0.100578
Total gradient norm: 0.273098
=== Actor Training Debug (Iteration 6485) ===
Q mean: -14.480284
Q std: 20.587921
Actor loss: 14.484261
Action reg: 0.003977
  l1.weight: grad_norm = 0.189547
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.143210
Total gradient norm: 0.474429
=== Actor Training Debug (Iteration 6486) ===
Q mean: -12.735781
Q std: 19.320192
Actor loss: 12.739750
Action reg: 0.003970
  l1.weight: grad_norm = 0.191558
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.135056
Total gradient norm: 0.393921
=== Actor Training Debug (Iteration 6487) ===
Q mean: -11.492008
Q std: 19.607172
Actor loss: 11.495981
Action reg: 0.003973
  l1.weight: grad_norm = 0.303456
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.193101
Total gradient norm: 0.592737
=== Actor Training Debug (Iteration 6488) ===
Q mean: -13.554170
Q std: 19.781649
Actor loss: 13.558150
Action reg: 0.003980
  l1.weight: grad_norm = 0.132681
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.107507
Total gradient norm: 0.342738
=== Actor Training Debug (Iteration 6489) ===
Q mean: -13.942112
Q std: 21.111214
Actor loss: 13.946086
Action reg: 0.003974
  l1.weight: grad_norm = 0.105764
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.079495
Total gradient norm: 0.260679
=== Actor Training Debug (Iteration 6490) ===
Q mean: -16.006493
Q std: 21.100824
Actor loss: 16.010473
Action reg: 0.003980
  l1.weight: grad_norm = 0.106031
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.069981
Total gradient norm: 0.229611
=== Actor Training Debug (Iteration 6491) ===
Q mean: -13.048170
Q std: 20.023214
Actor loss: 13.052143
Action reg: 0.003973
  l1.weight: grad_norm = 0.250504
  l1.bias: grad_norm = 0.000782
  l2.weight: grad_norm = 0.197123
Total gradient norm: 0.634004
=== Actor Training Debug (Iteration 6492) ===
Q mean: -14.045295
Q std: 20.476744
Actor loss: 14.049270
Action reg: 0.003975
  l1.weight: grad_norm = 0.109074
  l1.bias: grad_norm = 0.001271
  l2.weight: grad_norm = 0.090301
Total gradient norm: 0.263765
=== Actor Training Debug (Iteration 6493) ===
Q mean: -12.685719
Q std: 19.365231
Actor loss: 12.689706
Action reg: 0.003986
  l1.weight: grad_norm = 0.043226
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.031577
Total gradient norm: 0.094988
=== Actor Training Debug (Iteration 6494) ===
Q mean: -14.430819
Q std: 20.076477
Actor loss: 14.434789
Action reg: 0.003970
  l1.weight: grad_norm = 0.151417
  l1.bias: grad_norm = 0.002682
  l2.weight: grad_norm = 0.114097
Total gradient norm: 0.351091
=== Actor Training Debug (Iteration 6495) ===
Q mean: -15.795367
Q std: 21.325340
Actor loss: 15.799340
Action reg: 0.003973
  l1.weight: grad_norm = 0.159409
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.124505
Total gradient norm: 0.333025
=== Actor Training Debug (Iteration 6496) ===
Q mean: -14.323952
Q std: 20.048679
Actor loss: 14.327932
Action reg: 0.003981
  l1.weight: grad_norm = 0.138281
  l1.bias: grad_norm = 0.000899
  l2.weight: grad_norm = 0.098490
Total gradient norm: 0.309437
=== Actor Training Debug (Iteration 6497) ===
Q mean: -13.225432
Q std: 19.343967
Actor loss: 13.229405
Action reg: 0.003973
  l1.weight: grad_norm = 0.125243
  l1.bias: grad_norm = 0.001596
  l2.weight: grad_norm = 0.101270
Total gradient norm: 0.297060
=== Actor Training Debug (Iteration 6498) ===
Q mean: -13.920635
Q std: 20.441998
Actor loss: 13.924603
Action reg: 0.003967
  l1.weight: grad_norm = 0.147704
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.109682
Total gradient norm: 0.306820
=== Actor Training Debug (Iteration 6499) ===
Q mean: -13.689123
Q std: 19.463976
Actor loss: 13.693102
Action reg: 0.003978
  l1.weight: grad_norm = 0.124887
  l1.bias: grad_norm = 0.001145
  l2.weight: grad_norm = 0.089647
Total gradient norm: 0.243428
=== Actor Training Debug (Iteration 6500) ===
Q mean: -15.707564
Q std: 21.332314
Actor loss: 15.711543
Action reg: 0.003979
  l1.weight: grad_norm = 0.196429
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.157222
Total gradient norm: 0.492653
  Average reward: -316.004 | Average length: 100.0
Evaluation at episode 115: -316.004
=== Actor Training Debug (Iteration 6501) ===
Q mean: -13.196747
Q std: 20.514231
Actor loss: 13.200728
Action reg: 0.003981
  l1.weight: grad_norm = 0.185688
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.132421
Total gradient norm: 0.372037
=== Actor Training Debug (Iteration 6502) ===
Q mean: -14.730527
Q std: 21.312540
Actor loss: 14.734509
Action reg: 0.003982
  l1.weight: grad_norm = 0.083371
  l1.bias: grad_norm = 0.000608
  l2.weight: grad_norm = 0.074097
Total gradient norm: 0.208415
=== Actor Training Debug (Iteration 6503) ===
Q mean: -14.381093
Q std: 19.932682
Actor loss: 14.385073
Action reg: 0.003979
  l1.weight: grad_norm = 0.298418
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.204691
Total gradient norm: 0.645762
=== Actor Training Debug (Iteration 6504) ===
Q mean: -14.106405
Q std: 20.546511
Actor loss: 14.110386
Action reg: 0.003980
  l1.weight: grad_norm = 0.188551
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.135581
Total gradient norm: 0.505048
=== Actor Training Debug (Iteration 6505) ===
Q mean: -14.704962
Q std: 20.479698
Actor loss: 14.708948
Action reg: 0.003986
  l1.weight: grad_norm = 0.119616
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.074687
Total gradient norm: 0.202367
=== Actor Training Debug (Iteration 6506) ===
Q mean: -13.077079
Q std: 20.217575
Actor loss: 13.081057
Action reg: 0.003978
  l1.weight: grad_norm = 0.101298
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.076498
Total gradient norm: 0.225416
=== Actor Training Debug (Iteration 6507) ===
Q mean: -14.713610
Q std: 21.560938
Actor loss: 14.717600
Action reg: 0.003990
  l1.weight: grad_norm = 0.036708
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.024081
Total gradient norm: 0.069550
=== Actor Training Debug (Iteration 6508) ===
Q mean: -13.951740
Q std: 21.011269
Actor loss: 13.955721
Action reg: 0.003981
  l1.weight: grad_norm = 0.081850
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.055261
Total gradient norm: 0.151595
=== Actor Training Debug (Iteration 6509) ===
Q mean: -15.285976
Q std: 21.788858
Actor loss: 15.289948
Action reg: 0.003971
  l1.weight: grad_norm = 0.227030
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.202449
Total gradient norm: 0.631028
=== Actor Training Debug (Iteration 6510) ===
Q mean: -14.283000
Q std: 20.715403
Actor loss: 14.286983
Action reg: 0.003983
  l1.weight: grad_norm = 0.198476
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.137574
Total gradient norm: 0.438699
=== Actor Training Debug (Iteration 6511) ===
Q mean: -16.127363
Q std: 21.960546
Actor loss: 16.131338
Action reg: 0.003976
  l1.weight: grad_norm = 0.141190
  l1.bias: grad_norm = 0.000567
  l2.weight: grad_norm = 0.130817
Total gradient norm: 0.390410
=== Actor Training Debug (Iteration 6512) ===
Q mean: -12.902901
Q std: 20.184343
Actor loss: 12.906883
Action reg: 0.003983
  l1.weight: grad_norm = 0.135190
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.110520
Total gradient norm: 0.363820
=== Actor Training Debug (Iteration 6513) ===
Q mean: -16.047375
Q std: 22.213005
Actor loss: 16.051352
Action reg: 0.003977
  l1.weight: grad_norm = 0.188425
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.148394
Total gradient norm: 0.442817
=== Actor Training Debug (Iteration 6514) ===
Q mean: -12.382607
Q std: 20.037048
Actor loss: 12.386586
Action reg: 0.003979
  l1.weight: grad_norm = 0.219697
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.165823
Total gradient norm: 0.539178
=== Actor Training Debug (Iteration 6515) ===
Q mean: -15.838816
Q std: 22.076288
Actor loss: 15.842800
Action reg: 0.003985
  l1.weight: grad_norm = 0.463725
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.306937
Total gradient norm: 0.875258
=== Actor Training Debug (Iteration 6516) ===
Q mean: -14.221340
Q std: 20.881516
Actor loss: 14.225308
Action reg: 0.003968
  l1.weight: grad_norm = 0.176288
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.126386
Total gradient norm: 0.357460
=== Actor Training Debug (Iteration 6517) ===
Q mean: -15.991577
Q std: 21.593725
Actor loss: 15.995562
Action reg: 0.003984
  l1.weight: grad_norm = 0.080251
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.062928
Total gradient norm: 0.198361
=== Actor Training Debug (Iteration 6518) ===
Q mean: -11.986408
Q std: 19.174440
Actor loss: 11.990397
Action reg: 0.003989
  l1.weight: grad_norm = 0.099545
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.069959
Total gradient norm: 0.180085
=== Actor Training Debug (Iteration 6519) ===
Q mean: -13.314566
Q std: 19.886547
Actor loss: 13.318546
Action reg: 0.003980
  l1.weight: grad_norm = 0.146693
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.119093
Total gradient norm: 0.435514
=== Actor Training Debug (Iteration 6520) ===
Q mean: -13.844699
Q std: 20.390030
Actor loss: 13.848680
Action reg: 0.003982
  l1.weight: grad_norm = 0.171543
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.112130
Total gradient norm: 0.298212
=== Actor Training Debug (Iteration 6521) ===
Q mean: -14.314032
Q std: 20.576448
Actor loss: 14.318020
Action reg: 0.003988
  l1.weight: grad_norm = 0.079554
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.057144
Total gradient norm: 0.164438
=== Actor Training Debug (Iteration 6522) ===
Q mean: -12.586014
Q std: 20.672192
Actor loss: 12.589987
Action reg: 0.003973
  l1.weight: grad_norm = 0.291183
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 0.239509
Total gradient norm: 0.737095
=== Actor Training Debug (Iteration 6523) ===
Q mean: -14.677545
Q std: 21.026594
Actor loss: 14.681519
Action reg: 0.003974
  l1.weight: grad_norm = 0.181840
  l1.bias: grad_norm = 0.000884
  l2.weight: grad_norm = 0.121982
Total gradient norm: 0.375143
=== Actor Training Debug (Iteration 6524) ===
Q mean: -13.960739
Q std: 20.612673
Actor loss: 13.964713
Action reg: 0.003974
  l1.weight: grad_norm = 0.424579
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.355761
Total gradient norm: 1.268140
=== Actor Training Debug (Iteration 6525) ===
Q mean: -15.784362
Q std: 22.392878
Actor loss: 15.788337
Action reg: 0.003975
  l1.weight: grad_norm = 0.192887
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.133176
Total gradient norm: 0.389149
=== Actor Training Debug (Iteration 6526) ===
Q mean: -15.375744
Q std: 21.143055
Actor loss: 15.379719
Action reg: 0.003975
  l1.weight: grad_norm = 0.282025
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.171787
Total gradient norm: 0.492338
=== Actor Training Debug (Iteration 6527) ===
Q mean: -14.787090
Q std: 20.944979
Actor loss: 14.791077
Action reg: 0.003986
  l1.weight: grad_norm = 0.111485
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.081424
Total gradient norm: 0.257571
=== Actor Training Debug (Iteration 6528) ===
Q mean: -11.910042
Q std: 18.775110
Actor loss: 11.914019
Action reg: 0.003977
  l1.weight: grad_norm = 0.127482
  l1.bias: grad_norm = 0.000551
  l2.weight: grad_norm = 0.096425
Total gradient norm: 0.286150
=== Actor Training Debug (Iteration 6529) ===
Q mean: -13.797663
Q std: 20.391935
Actor loss: 13.801637
Action reg: 0.003974
  l1.weight: grad_norm = 0.202394
  l1.bias: grad_norm = 0.000862
  l2.weight: grad_norm = 0.158094
Total gradient norm: 0.508618
=== Actor Training Debug (Iteration 6530) ===
Q mean: -15.849817
Q std: 21.216631
Actor loss: 15.853801
Action reg: 0.003983
  l1.weight: grad_norm = 0.112620
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.094462
Total gradient norm: 0.296533
=== Actor Training Debug (Iteration 6531) ===
Q mean: -13.789131
Q std: 20.719458
Actor loss: 13.793107
Action reg: 0.003976
  l1.weight: grad_norm = 0.140553
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.142413
Total gradient norm: 0.420874
=== Actor Training Debug (Iteration 6532) ===
Q mean: -13.659910
Q std: 20.386124
Actor loss: 13.663882
Action reg: 0.003972
  l1.weight: grad_norm = 0.246261
  l1.bias: grad_norm = 0.000536
  l2.weight: grad_norm = 0.180773
Total gradient norm: 0.506634
=== Actor Training Debug (Iteration 6533) ===
Q mean: -14.432788
Q std: 21.044744
Actor loss: 14.436760
Action reg: 0.003972
  l1.weight: grad_norm = 0.088139
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.064491
Total gradient norm: 0.182855
=== Actor Training Debug (Iteration 6534) ===
Q mean: -14.058358
Q std: 20.196440
Actor loss: 14.062335
Action reg: 0.003977
  l1.weight: grad_norm = 0.187888
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.127803
Total gradient norm: 0.364076
=== Actor Training Debug (Iteration 6535) ===
Q mean: -13.096081
Q std: 19.528736
Actor loss: 13.100046
Action reg: 0.003965
  l1.weight: grad_norm = 0.276520
  l1.bias: grad_norm = 0.000526
  l2.weight: grad_norm = 0.198512
Total gradient norm: 0.584836
=== Actor Training Debug (Iteration 6536) ===
Q mean: -12.310067
Q std: 19.110418
Actor loss: 12.314038
Action reg: 0.003971
  l1.weight: grad_norm = 0.143508
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.095924
Total gradient norm: 0.294722
=== Actor Training Debug (Iteration 6537) ===
Q mean: -16.336981
Q std: 21.367689
Actor loss: 16.340954
Action reg: 0.003974
  l1.weight: grad_norm = 0.103086
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.082105
Total gradient norm: 0.235383
=== Actor Training Debug (Iteration 6538) ===
Q mean: -15.935200
Q std: 21.108376
Actor loss: 15.939178
Action reg: 0.003978
  l1.weight: grad_norm = 0.156105
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.135690
Total gradient norm: 0.346358
=== Actor Training Debug (Iteration 6539) ===
Q mean: -14.140611
Q std: 19.680119
Actor loss: 14.144585
Action reg: 0.003974
  l1.weight: grad_norm = 0.154122
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.116088
Total gradient norm: 0.338099
=== Actor Training Debug (Iteration 6540) ===
Q mean: -15.060268
Q std: 20.298647
Actor loss: 15.064256
Action reg: 0.003987
  l1.weight: grad_norm = 0.144516
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.101670
Total gradient norm: 0.338485
=== Actor Training Debug (Iteration 6541) ===
Q mean: -12.554749
Q std: 20.178865
Actor loss: 12.558725
Action reg: 0.003976
  l1.weight: grad_norm = 0.284432
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.211594
Total gradient norm: 0.603303
=== Actor Training Debug (Iteration 6542) ===
Q mean: -14.216383
Q std: 20.879107
Actor loss: 14.220362
Action reg: 0.003979
  l1.weight: grad_norm = 0.200309
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.153557
Total gradient norm: 0.434927
=== Actor Training Debug (Iteration 6543) ===
Q mean: -12.072217
Q std: 18.904114
Actor loss: 12.076188
Action reg: 0.003971
  l1.weight: grad_norm = 0.176041
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.145293
Total gradient norm: 0.456724
=== Actor Training Debug (Iteration 6544) ===
Q mean: -13.826891
Q std: 20.790304
Actor loss: 13.830869
Action reg: 0.003977
  l1.weight: grad_norm = 0.180505
  l1.bias: grad_norm = 0.000989
  l2.weight: grad_norm = 0.139279
Total gradient norm: 0.426171
=== Actor Training Debug (Iteration 6545) ===
Q mean: -13.311777
Q std: 20.512081
Actor loss: 13.315757
Action reg: 0.003980
  l1.weight: grad_norm = 0.120913
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.084012
Total gradient norm: 0.242638
=== Actor Training Debug (Iteration 6546) ===
Q mean: -14.814130
Q std: 22.292341
Actor loss: 14.818104
Action reg: 0.003974
  l1.weight: grad_norm = 0.316226
  l1.bias: grad_norm = 0.000615
  l2.weight: grad_norm = 0.296978
Total gradient norm: 0.868734
=== Actor Training Debug (Iteration 6547) ===
Q mean: -13.572665
Q std: 20.792934
Actor loss: 13.576634
Action reg: 0.003969
  l1.weight: grad_norm = 0.207475
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.176366
Total gradient norm: 0.530232
=== Actor Training Debug (Iteration 6548) ===
Q mean: -12.548435
Q std: 19.154156
Actor loss: 12.552423
Action reg: 0.003988
  l1.weight: grad_norm = 0.103990
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.080205
Total gradient norm: 0.215031
=== Actor Training Debug (Iteration 6549) ===
Q mean: -12.544538
Q std: 19.149836
Actor loss: 12.548522
Action reg: 0.003984
  l1.weight: grad_norm = 0.077078
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.058092
Total gradient norm: 0.157158
=== Actor Training Debug (Iteration 6550) ===
Q mean: -13.035631
Q std: 19.097242
Actor loss: 13.039614
Action reg: 0.003982
  l1.weight: grad_norm = 0.243902
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.182367
Total gradient norm: 0.568342
=== Actor Training Debug (Iteration 6551) ===
Q mean: -13.534422
Q std: 20.924654
Actor loss: 13.538385
Action reg: 0.003964
  l1.weight: grad_norm = 0.245700
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.183255
Total gradient norm: 0.519500
=== Actor Training Debug (Iteration 6552) ===
Q mean: -13.284175
Q std: 19.761581
Actor loss: 13.288157
Action reg: 0.003982
  l1.weight: grad_norm = 0.166676
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.123184
Total gradient norm: 0.340506
=== Actor Training Debug (Iteration 6553) ===
Q mean: -14.699177
Q std: 21.314720
Actor loss: 14.703158
Action reg: 0.003981
  l1.weight: grad_norm = 0.108138
  l1.bias: grad_norm = 0.001422
  l2.weight: grad_norm = 0.087699
Total gradient norm: 0.278643
=== Actor Training Debug (Iteration 6554) ===
Q mean: -14.971916
Q std: 22.918587
Actor loss: 14.975892
Action reg: 0.003976
  l1.weight: grad_norm = 0.196340
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.166441
Total gradient norm: 0.487724
=== Actor Training Debug (Iteration 6555) ===
Q mean: -14.560572
Q std: 20.990206
Actor loss: 14.564557
Action reg: 0.003986
  l1.weight: grad_norm = 0.132564
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.112788
Total gradient norm: 0.323520
=== Actor Training Debug (Iteration 6556) ===
Q mean: -14.762653
Q std: 21.527018
Actor loss: 14.766631
Action reg: 0.003978
  l1.weight: grad_norm = 0.186853
  l1.bias: grad_norm = 0.000804
  l2.weight: grad_norm = 0.140142
Total gradient norm: 0.472789
=== Actor Training Debug (Iteration 6557) ===
Q mean: -11.859875
Q std: 19.104122
Actor loss: 11.863853
Action reg: 0.003979
  l1.weight: grad_norm = 0.117160
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.084114
Total gradient norm: 0.240864
=== Actor Training Debug (Iteration 6558) ===
Q mean: -14.126755
Q std: 21.035345
Actor loss: 14.130736
Action reg: 0.003981
  l1.weight: grad_norm = 0.103951
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.071270
Total gradient norm: 0.217611
=== Actor Training Debug (Iteration 6559) ===
Q mean: -12.334290
Q std: 19.648457
Actor loss: 12.338270
Action reg: 0.003980
  l1.weight: grad_norm = 0.091150
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.060637
Total gradient norm: 0.179416
=== Actor Training Debug (Iteration 6560) ===
Q mean: -15.289143
Q std: 21.060955
Actor loss: 15.293120
Action reg: 0.003978
  l1.weight: grad_norm = 0.104208
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.083121
Total gradient norm: 0.248491
=== Actor Training Debug (Iteration 6561) ===
Q mean: -14.959038
Q std: 21.037615
Actor loss: 14.962998
Action reg: 0.003961
  l1.weight: grad_norm = 0.388845
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.337220
Total gradient norm: 0.969161
=== Actor Training Debug (Iteration 6562) ===
Q mean: -15.647448
Q std: 21.391418
Actor loss: 15.651428
Action reg: 0.003981
  l1.weight: grad_norm = 0.088001
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.064608
Total gradient norm: 0.185365
=== Actor Training Debug (Iteration 6563) ===
Q mean: -12.652721
Q std: 19.686724
Actor loss: 12.656702
Action reg: 0.003981
  l1.weight: grad_norm = 0.102485
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.074260
Total gradient norm: 0.229087
=== Actor Training Debug (Iteration 6564) ===
Q mean: -15.254861
Q std: 21.833893
Actor loss: 15.258847
Action reg: 0.003986
  l1.weight: grad_norm = 0.141288
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.112402
Total gradient norm: 0.307135
=== Actor Training Debug (Iteration 6565) ===
Q mean: -14.206701
Q std: 20.621986
Actor loss: 14.210668
Action reg: 0.003966
  l1.weight: grad_norm = 0.453754
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.339980
Total gradient norm: 1.117880
=== Actor Training Debug (Iteration 6566) ===
Q mean: -12.959751
Q std: 20.053337
Actor loss: 12.963733
Action reg: 0.003982
  l1.weight: grad_norm = 0.291968
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.212880
Total gradient norm: 0.574643
=== Actor Training Debug (Iteration 6567) ===
Q mean: -13.165043
Q std: 20.233418
Actor loss: 13.169024
Action reg: 0.003981
  l1.weight: grad_norm = 0.113465
  l1.bias: grad_norm = 0.000588
  l2.weight: grad_norm = 0.088051
Total gradient norm: 0.251050
=== Actor Training Debug (Iteration 6568) ===
Q mean: -15.144587
Q std: 20.869823
Actor loss: 15.148553
Action reg: 0.003967
  l1.weight: grad_norm = 0.162248
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.139672
Total gradient norm: 0.401237
=== Actor Training Debug (Iteration 6569) ===
Q mean: -13.717617
Q std: 20.654068
Actor loss: 13.721592
Action reg: 0.003975
  l1.weight: grad_norm = 0.110553
  l1.bias: grad_norm = 0.000962
  l2.weight: grad_norm = 0.078705
Total gradient norm: 0.240451
=== Actor Training Debug (Iteration 6570) ===
Q mean: -15.675980
Q std: 21.656361
Actor loss: 15.679970
Action reg: 0.003990
  l1.weight: grad_norm = 0.138459
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.090034
Total gradient norm: 0.279728
=== Actor Training Debug (Iteration 6571) ===
Q mean: -13.983680
Q std: 20.982025
Actor loss: 13.987642
Action reg: 0.003963
  l1.weight: grad_norm = 0.290717
  l1.bias: grad_norm = 0.000847
  l2.weight: grad_norm = 0.197209
Total gradient norm: 0.540029
=== Actor Training Debug (Iteration 6572) ===
Q mean: -11.962448
Q std: 18.855721
Actor loss: 11.966413
Action reg: 0.003966
  l1.weight: grad_norm = 0.112305
  l1.bias: grad_norm = 0.000627
  l2.weight: grad_norm = 0.089659
Total gradient norm: 0.294923
=== Actor Training Debug (Iteration 6573) ===
Q mean: -14.549332
Q std: 19.549469
Actor loss: 14.553321
Action reg: 0.003989
  l1.weight: grad_norm = 0.171278
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.111641
Total gradient norm: 0.331257
=== Actor Training Debug (Iteration 6574) ===
Q mean: -11.317152
Q std: 19.291971
Actor loss: 11.321133
Action reg: 0.003980
  l1.weight: grad_norm = 0.142068
  l1.bias: grad_norm = 0.000585
  l2.weight: grad_norm = 0.116696
Total gradient norm: 0.378782
=== Actor Training Debug (Iteration 6575) ===
Q mean: -12.817121
Q std: 20.228085
Actor loss: 12.821100
Action reg: 0.003979
  l1.weight: grad_norm = 0.154750
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.121688
Total gradient norm: 0.309601
=== Actor Training Debug (Iteration 6576) ===
Q mean: -14.225504
Q std: 20.149872
Actor loss: 14.229488
Action reg: 0.003984
  l1.weight: grad_norm = 0.131535
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.087704
Total gradient norm: 0.246981
=== Actor Training Debug (Iteration 6577) ===
Q mean: -12.655548
Q std: 18.832220
Actor loss: 12.659526
Action reg: 0.003978
  l1.weight: grad_norm = 0.351609
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.331586
Total gradient norm: 0.853702
=== Actor Training Debug (Iteration 6578) ===
Q mean: -13.944084
Q std: 20.437225
Actor loss: 13.948070
Action reg: 0.003985
  l1.weight: grad_norm = 0.063574
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.050943
Total gradient norm: 0.156710
=== Actor Training Debug (Iteration 6579) ===
Q mean: -13.319181
Q std: 20.152189
Actor loss: 13.323166
Action reg: 0.003984
  l1.weight: grad_norm = 0.131937
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.111895
Total gradient norm: 0.339049
=== Actor Training Debug (Iteration 6580) ===
Q mean: -13.975471
Q std: 21.561932
Actor loss: 13.979446
Action reg: 0.003975
  l1.weight: grad_norm = 0.124175
  l1.bias: grad_norm = 0.000698
  l2.weight: grad_norm = 0.090380
Total gradient norm: 0.264925
=== Actor Training Debug (Iteration 6581) ===
Q mean: -16.427355
Q std: 21.952648
Actor loss: 16.431330
Action reg: 0.003976
  l1.weight: grad_norm = 0.306894
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.220503
Total gradient norm: 0.557539
=== Actor Training Debug (Iteration 6582) ===
Q mean: -14.994476
Q std: 20.901768
Actor loss: 14.998458
Action reg: 0.003982
  l1.weight: grad_norm = 0.157295
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.122961
Total gradient norm: 0.375521
=== Actor Training Debug (Iteration 6583) ===
Q mean: -13.876965
Q std: 19.826509
Actor loss: 13.880940
Action reg: 0.003975
  l1.weight: grad_norm = 0.445192
  l1.bias: grad_norm = 0.000546
  l2.weight: grad_norm = 0.313888
Total gradient norm: 0.946979
=== Actor Training Debug (Iteration 6584) ===
Q mean: -15.056803
Q std: 21.703760
Actor loss: 15.060786
Action reg: 0.003984
  l1.weight: grad_norm = 0.054928
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.046401
Total gradient norm: 0.134397
=== Actor Training Debug (Iteration 6585) ===
Q mean: -13.810266
Q std: 21.154835
Actor loss: 13.814253
Action reg: 0.003986
  l1.weight: grad_norm = 0.078075
  l1.bias: grad_norm = 0.000810
  l2.weight: grad_norm = 0.065057
Total gradient norm: 0.222868
=== Actor Training Debug (Iteration 6586) ===
Q mean: -14.925300
Q std: 21.056944
Actor loss: 14.929279
Action reg: 0.003980
  l1.weight: grad_norm = 0.134801
  l1.bias: grad_norm = 0.000747
  l2.weight: grad_norm = 0.090606
Total gradient norm: 0.285712
=== Actor Training Debug (Iteration 6587) ===
Q mean: -16.715635
Q std: 21.912167
Actor loss: 16.719618
Action reg: 0.003983
  l1.weight: grad_norm = 0.084538
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.055744
Total gradient norm: 0.179856
=== Actor Training Debug (Iteration 6588) ===
Q mean: -11.759525
Q std: 18.906919
Actor loss: 11.763503
Action reg: 0.003977
  l1.weight: grad_norm = 0.084005
  l1.bias: grad_norm = 0.000517
  l2.weight: grad_norm = 0.058800
Total gradient norm: 0.171895
=== Actor Training Debug (Iteration 6589) ===
Q mean: -13.267513
Q std: 19.409111
Actor loss: 13.271496
Action reg: 0.003983
  l1.weight: grad_norm = 0.150019
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.104041
Total gradient norm: 0.291223
=== Actor Training Debug (Iteration 6590) ===
Q mean: -13.169548
Q std: 19.793221
Actor loss: 13.173516
Action reg: 0.003968
  l1.weight: grad_norm = 0.186305
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.142809
Total gradient norm: 0.434720
=== Actor Training Debug (Iteration 6591) ===
Q mean: -16.296188
Q std: 21.118589
Actor loss: 16.300158
Action reg: 0.003970
  l1.weight: grad_norm = 0.150783
  l1.bias: grad_norm = 0.001537
  l2.weight: grad_norm = 0.120673
Total gradient norm: 0.341185
=== Actor Training Debug (Iteration 6592) ===
Q mean: -14.409317
Q std: 20.400606
Actor loss: 14.413280
Action reg: 0.003963
  l1.weight: grad_norm = 0.267479
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.184771
Total gradient norm: 0.654125
=== Actor Training Debug (Iteration 6593) ===
Q mean: -14.534582
Q std: 21.447586
Actor loss: 14.538558
Action reg: 0.003976
  l1.weight: grad_norm = 0.507527
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.309673
Total gradient norm: 0.897362
=== Actor Training Debug (Iteration 6594) ===
Q mean: -16.222187
Q std: 21.937901
Actor loss: 16.226168
Action reg: 0.003981
  l1.weight: grad_norm = 0.317058
  l1.bias: grad_norm = 0.000640
  l2.weight: grad_norm = 0.238937
Total gradient norm: 0.632909
=== Actor Training Debug (Iteration 6595) ===
Q mean: -12.639744
Q std: 19.935236
Actor loss: 12.643714
Action reg: 0.003970
  l1.weight: grad_norm = 0.108030
  l1.bias: grad_norm = 0.000701
  l2.weight: grad_norm = 0.083781
Total gradient norm: 0.280853
=== Actor Training Debug (Iteration 6596) ===
Q mean: -13.001739
Q std: 19.865065
Actor loss: 13.005701
Action reg: 0.003963
  l1.weight: grad_norm = 0.195865
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.149917
Total gradient norm: 0.453757
=== Actor Training Debug (Iteration 6597) ===
Q mean: -14.111487
Q std: 20.934666
Actor loss: 14.115460
Action reg: 0.003973
  l1.weight: grad_norm = 0.342728
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.292119
Total gradient norm: 0.832554
=== Actor Training Debug (Iteration 6598) ===
Q mean: -12.572918
Q std: 20.454800
Actor loss: 12.576898
Action reg: 0.003980
  l1.weight: grad_norm = 0.142106
  l1.bias: grad_norm = 0.000565
  l2.weight: grad_norm = 0.096229
Total gradient norm: 0.256863
=== Actor Training Debug (Iteration 6599) ===
Q mean: -14.681668
Q std: 22.163132
Actor loss: 14.685642
Action reg: 0.003974
  l1.weight: grad_norm = 0.115006
  l1.bias: grad_norm = 0.000671
  l2.weight: grad_norm = 0.072477
Total gradient norm: 0.200231
=== Actor Training Debug (Iteration 6600) ===
Q mean: -15.881367
Q std: 22.466997
Actor loss: 15.885344
Action reg: 0.003977
  l1.weight: grad_norm = 0.205031
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.138359
Total gradient norm: 0.443419
=== Actor Training Debug (Iteration 6601) ===
Q mean: -12.419485
Q std: 18.147453
Actor loss: 12.423459
Action reg: 0.003974
  l1.weight: grad_norm = 0.183024
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.127265
Total gradient norm: 0.388984
=== Actor Training Debug (Iteration 6602) ===
Q mean: -13.325231
Q std: 20.267750
Actor loss: 13.329213
Action reg: 0.003983
  l1.weight: grad_norm = 0.148838
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.112615
Total gradient norm: 0.354771
=== Actor Training Debug (Iteration 6603) ===
Q mean: -13.690401
Q std: 20.936779
Actor loss: 13.694373
Action reg: 0.003972
  l1.weight: grad_norm = 0.340890
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.221133
Total gradient norm: 0.653132
=== Actor Training Debug (Iteration 6604) ===
Q mean: -15.067141
Q std: 21.645311
Actor loss: 15.071126
Action reg: 0.003985
  l1.weight: grad_norm = 0.112170
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.097999
Total gradient norm: 0.256805
=== Actor Training Debug (Iteration 6605) ===
Q mean: -13.171597
Q std: 21.106647
Actor loss: 13.175571
Action reg: 0.003974
  l1.weight: grad_norm = 0.176245
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.136632
Total gradient norm: 0.392387
=== Actor Training Debug (Iteration 6606) ===
Q mean: -14.297521
Q std: 21.786932
Actor loss: 14.301494
Action reg: 0.003973
  l1.weight: grad_norm = 0.122891
  l1.bias: grad_norm = 0.001246
  l2.weight: grad_norm = 0.101067
Total gradient norm: 0.297975
=== Actor Training Debug (Iteration 6607) ===
Q mean: -11.577794
Q std: 19.505098
Actor loss: 11.581781
Action reg: 0.003987
  l1.weight: grad_norm = 0.067717
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.058017
Total gradient norm: 0.157120
=== Actor Training Debug (Iteration 6608) ===
Q mean: -13.769838
Q std: 19.478796
Actor loss: 13.773808
Action reg: 0.003969
  l1.weight: grad_norm = 0.138102
  l1.bias: grad_norm = 0.001175
  l2.weight: grad_norm = 0.096481
Total gradient norm: 0.306769
=== Actor Training Debug (Iteration 6609) ===
Q mean: -13.993870
Q std: 20.606968
Actor loss: 13.997838
Action reg: 0.003969
  l1.weight: grad_norm = 0.207403
  l1.bias: grad_norm = 0.001089
  l2.weight: grad_norm = 0.133424
Total gradient norm: 0.432275
=== Actor Training Debug (Iteration 6610) ===
Q mean: -12.910849
Q std: 20.290478
Actor loss: 12.914822
Action reg: 0.003973
  l1.weight: grad_norm = 0.099047
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.070329
Total gradient norm: 0.233060
=== Actor Training Debug (Iteration 6611) ===
Q mean: -13.896232
Q std: 20.980150
Actor loss: 13.900195
Action reg: 0.003963
  l1.weight: grad_norm = 0.239868
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.182631
Total gradient norm: 0.606238
=== Actor Training Debug (Iteration 6612) ===
Q mean: -15.656517
Q std: 22.623373
Actor loss: 15.660496
Action reg: 0.003979
  l1.weight: grad_norm = 0.026971
  l1.bias: grad_norm = 0.000871
  l2.weight: grad_norm = 0.022996
Total gradient norm: 0.068455
=== Actor Training Debug (Iteration 6613) ===
Q mean: -12.611286
Q std: 18.820305
Actor loss: 12.615256
Action reg: 0.003970
  l1.weight: grad_norm = 0.176017
  l1.bias: grad_norm = 0.001453
  l2.weight: grad_norm = 0.141437
Total gradient norm: 0.382405
=== Actor Training Debug (Iteration 6614) ===
Q mean: -13.643074
Q std: 19.827593
Actor loss: 13.647054
Action reg: 0.003980
  l1.weight: grad_norm = 0.111120
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.082220
Total gradient norm: 0.249855
=== Actor Training Debug (Iteration 6615) ===
Q mean: -13.216303
Q std: 19.961931
Actor loss: 13.220284
Action reg: 0.003980
  l1.weight: grad_norm = 0.074060
  l1.bias: grad_norm = 0.000519
  l2.weight: grad_norm = 0.049187
Total gradient norm: 0.142007
=== Actor Training Debug (Iteration 6616) ===
Q mean: -14.943665
Q std: 21.615192
Actor loss: 14.947639
Action reg: 0.003974
  l1.weight: grad_norm = 0.116674
  l1.bias: grad_norm = 0.000578
  l2.weight: grad_norm = 0.072713
Total gradient norm: 0.214860
=== Actor Training Debug (Iteration 6617) ===
Q mean: -14.596346
Q std: 20.398483
Actor loss: 14.600329
Action reg: 0.003983
  l1.weight: grad_norm = 0.135653
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.126172
Total gradient norm: 0.366428
=== Actor Training Debug (Iteration 6618) ===
Q mean: -14.339121
Q std: 18.952433
Actor loss: 14.343095
Action reg: 0.003974
  l1.weight: grad_norm = 0.270744
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.194338
Total gradient norm: 0.584772
=== Actor Training Debug (Iteration 6619) ===
Q mean: -14.494284
Q std: 20.919943
Actor loss: 14.498267
Action reg: 0.003984
  l1.weight: grad_norm = 0.149511
  l1.bias: grad_norm = 0.001987
  l2.weight: grad_norm = 0.100285
Total gradient norm: 0.289445
=== Actor Training Debug (Iteration 6620) ===
Q mean: -12.806368
Q std: 20.202518
Actor loss: 12.810344
Action reg: 0.003976
  l1.weight: grad_norm = 0.111556
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.087129
Total gradient norm: 0.271091
=== Actor Training Debug (Iteration 6621) ===
Q mean: -14.944889
Q std: 20.811722
Actor loss: 14.948852
Action reg: 0.003963
  l1.weight: grad_norm = 0.306965
  l1.bias: grad_norm = 0.001836
  l2.weight: grad_norm = 0.197964
Total gradient norm: 0.586148
=== Actor Training Debug (Iteration 6622) ===
Q mean: -15.465271
Q std: 22.090714
Actor loss: 15.469246
Action reg: 0.003975
  l1.weight: grad_norm = 0.319986
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.269587
Total gradient norm: 0.927170
=== Actor Training Debug (Iteration 6623) ===
Q mean: -14.724104
Q std: 21.368814
Actor loss: 14.728079
Action reg: 0.003975
  l1.weight: grad_norm = 0.247720
  l1.bias: grad_norm = 0.001658
  l2.weight: grad_norm = 0.202879
Total gradient norm: 0.572471
=== Actor Training Debug (Iteration 6624) ===
Q mean: -15.632327
Q std: 20.679333
Actor loss: 15.636315
Action reg: 0.003989
  l1.weight: grad_norm = 0.110422
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.071063
Total gradient norm: 0.197162
=== Actor Training Debug (Iteration 6625) ===
Q mean: -14.895922
Q std: 21.326374
Actor loss: 14.899894
Action reg: 0.003972
  l1.weight: grad_norm = 0.122592
  l1.bias: grad_norm = 0.000668
  l2.weight: grad_norm = 0.089640
Total gradient norm: 0.246197
=== Actor Training Debug (Iteration 6626) ===
Q mean: -12.872955
Q std: 19.517982
Actor loss: 12.876928
Action reg: 0.003973
  l1.weight: grad_norm = 0.119668
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.099742
Total gradient norm: 0.278959
=== Actor Training Debug (Iteration 6627) ===
Q mean: -14.794553
Q std: 21.852720
Actor loss: 14.798529
Action reg: 0.003976
  l1.weight: grad_norm = 0.197482
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.177527
Total gradient norm: 0.501868
=== Actor Training Debug (Iteration 6628) ===
Q mean: -12.306065
Q std: 20.115152
Actor loss: 12.310028
Action reg: 0.003963
  l1.weight: grad_norm = 0.247499
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.170508
Total gradient norm: 0.474753
=== Actor Training Debug (Iteration 6629) ===
Q mean: -12.781042
Q std: 19.986702
Actor loss: 12.785016
Action reg: 0.003974
  l1.weight: grad_norm = 0.134754
  l1.bias: grad_norm = 0.000701
  l2.weight: grad_norm = 0.105815
Total gradient norm: 0.315408
=== Actor Training Debug (Iteration 6630) ===
Q mean: -13.292326
Q std: 20.158300
Actor loss: 13.296315
Action reg: 0.003990
  l1.weight: grad_norm = 0.056281
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.044026
Total gradient norm: 0.124644
=== Actor Training Debug (Iteration 6631) ===
Q mean: -16.304466
Q std: 23.147223
Actor loss: 16.308455
Action reg: 0.003988
  l1.weight: grad_norm = 0.161769
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.121854
Total gradient norm: 0.400555
=== Actor Training Debug (Iteration 6632) ===
Q mean: -13.585666
Q std: 21.349522
Actor loss: 13.589620
Action reg: 0.003954
  l1.weight: grad_norm = 0.137752
  l1.bias: grad_norm = 0.000887
  l2.weight: grad_norm = 0.111097
Total gradient norm: 0.334518
=== Actor Training Debug (Iteration 6633) ===
Q mean: -14.588081
Q std: 20.563234
Actor loss: 14.592062
Action reg: 0.003981
  l1.weight: grad_norm = 0.219605
  l1.bias: grad_norm = 0.000528
  l2.weight: grad_norm = 0.179162
Total gradient norm: 0.547097
=== Actor Training Debug (Iteration 6634) ===
Q mean: -14.276751
Q std: 20.677174
Actor loss: 14.280709
Action reg: 0.003958
  l1.weight: grad_norm = 0.133539
  l1.bias: grad_norm = 0.000754
  l2.weight: grad_norm = 0.096884
Total gradient norm: 0.284536
=== Actor Training Debug (Iteration 6635) ===
Q mean: -13.410777
Q std: 19.637419
Actor loss: 13.414758
Action reg: 0.003981
  l1.weight: grad_norm = 0.168134
  l1.bias: grad_norm = 0.000966
  l2.weight: grad_norm = 0.136248
Total gradient norm: 0.362979
=== Actor Training Debug (Iteration 6636) ===
Q mean: -13.719201
Q std: 19.960299
Actor loss: 13.723183
Action reg: 0.003981
  l1.weight: grad_norm = 0.097556
  l1.bias: grad_norm = 0.000887
  l2.weight: grad_norm = 0.067570
Total gradient norm: 0.205172
=== Actor Training Debug (Iteration 6637) ===
Q mean: -10.958656
Q std: 18.695923
Actor loss: 10.962635
Action reg: 0.003979
  l1.weight: grad_norm = 0.129065
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.085274
Total gradient norm: 0.241097
=== Actor Training Debug (Iteration 6638) ===
Q mean: -15.473270
Q std: 21.258701
Actor loss: 15.477253
Action reg: 0.003983
  l1.weight: grad_norm = 0.090281
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.053280
Total gradient norm: 0.161988
=== Actor Training Debug (Iteration 6639) ===
Q mean: -14.178968
Q std: 21.292116
Actor loss: 14.182946
Action reg: 0.003978
  l1.weight: grad_norm = 0.095809
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.071147
Total gradient norm: 0.233261
=== Actor Training Debug (Iteration 6640) ===
Q mean: -14.946754
Q std: 21.433077
Actor loss: 14.950744
Action reg: 0.003990
  l1.weight: grad_norm = 0.133282
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.107761
Total gradient norm: 0.305179
=== Actor Training Debug (Iteration 6641) ===
Q mean: -12.437766
Q std: 20.108171
Actor loss: 12.441753
Action reg: 0.003987
  l1.weight: grad_norm = 0.336812
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.228049
Total gradient norm: 0.752528
=== Actor Training Debug (Iteration 6642) ===
Q mean: -12.861891
Q std: 19.545315
Actor loss: 12.865856
Action reg: 0.003965
  l1.weight: grad_norm = 0.280939
  l1.bias: grad_norm = 0.000611
  l2.weight: grad_norm = 0.227379
Total gradient norm: 0.686043
=== Actor Training Debug (Iteration 6643) ===
Q mean: -16.363810
Q std: 22.573786
Actor loss: 16.367785
Action reg: 0.003974
  l1.weight: grad_norm = 0.193215
  l1.bias: grad_norm = 0.000933
  l2.weight: grad_norm = 0.136102
Total gradient norm: 0.403679
=== Actor Training Debug (Iteration 6644) ===
Q mean: -12.844032
Q std: 19.099165
Actor loss: 12.848004
Action reg: 0.003972
  l1.weight: grad_norm = 0.151259
  l1.bias: grad_norm = 0.000680
  l2.weight: grad_norm = 0.140670
Total gradient norm: 0.434007
=== Actor Training Debug (Iteration 6645) ===
Q mean: -14.564436
Q std: 21.927460
Actor loss: 14.568409
Action reg: 0.003973
  l1.weight: grad_norm = 0.148174
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.088227
Total gradient norm: 0.236525
=== Actor Training Debug (Iteration 6646) ===
Q mean: -14.698553
Q std: 22.635557
Actor loss: 14.702522
Action reg: 0.003969
  l1.weight: grad_norm = 0.357593
  l1.bias: grad_norm = 0.000808
  l2.weight: grad_norm = 0.269221
Total gradient norm: 0.779583
=== Actor Training Debug (Iteration 6647) ===
Q mean: -12.820450
Q std: 19.855371
Actor loss: 12.824406
Action reg: 0.003956
  l1.weight: grad_norm = 0.217540
  l1.bias: grad_norm = 0.001303
  l2.weight: grad_norm = 0.164698
Total gradient norm: 0.498534
=== Actor Training Debug (Iteration 6648) ===
Q mean: -13.315210
Q std: 20.317648
Actor loss: 13.319190
Action reg: 0.003980
  l1.weight: grad_norm = 0.052950
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.043372
Total gradient norm: 0.132163
=== Actor Training Debug (Iteration 6649) ===
Q mean: -13.012073
Q std: 20.830929
Actor loss: 13.016052
Action reg: 0.003980
  l1.weight: grad_norm = 0.076656
  l1.bias: grad_norm = 0.000449
  l2.weight: grad_norm = 0.058515
Total gradient norm: 0.151798
=== Actor Training Debug (Iteration 6650) ===
Q mean: -12.623701
Q std: 19.597923
Actor loss: 12.627684
Action reg: 0.003982
  l1.weight: grad_norm = 0.137399
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.118865
Total gradient norm: 0.372517
=== Actor Training Debug (Iteration 6651) ===
Q mean: -13.046148
Q std: 18.719385
Actor loss: 13.050123
Action reg: 0.003975
  l1.weight: grad_norm = 0.293058
  l1.bias: grad_norm = 0.000556
  l2.weight: grad_norm = 0.234375
Total gradient norm: 0.692867
=== Actor Training Debug (Iteration 6652) ===
Q mean: -13.523936
Q std: 21.058304
Actor loss: 13.527914
Action reg: 0.003978
  l1.weight: grad_norm = 0.063810
  l1.bias: grad_norm = 0.000625
  l2.weight: grad_norm = 0.055986
Total gradient norm: 0.161892
=== Actor Training Debug (Iteration 6653) ===
Q mean: -15.190793
Q std: 21.049212
Actor loss: 15.194759
Action reg: 0.003966
  l1.weight: grad_norm = 0.154045
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.111153
Total gradient norm: 0.359188
=== Actor Training Debug (Iteration 6654) ===
Q mean: -13.473709
Q std: 19.930674
Actor loss: 13.477686
Action reg: 0.003976
  l1.weight: grad_norm = 0.116332
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.084504
Total gradient norm: 0.252323
=== Actor Training Debug (Iteration 6655) ===
Q mean: -14.123714
Q std: 21.299351
Actor loss: 14.127687
Action reg: 0.003972
  l1.weight: grad_norm = 0.111957
  l1.bias: grad_norm = 0.000910
  l2.weight: grad_norm = 0.079755
Total gradient norm: 0.236952
=== Actor Training Debug (Iteration 6656) ===
Q mean: -12.488686
Q std: 20.379942
Actor loss: 12.492666
Action reg: 0.003981
  l1.weight: grad_norm = 0.217396
  l1.bias: grad_norm = 0.000612
  l2.weight: grad_norm = 0.172709
Total gradient norm: 0.508925
=== Actor Training Debug (Iteration 6657) ===
Q mean: -17.686146
Q std: 23.124813
Actor loss: 17.690111
Action reg: 0.003965
  l1.weight: grad_norm = 0.145267
  l1.bias: grad_norm = 0.000804
  l2.weight: grad_norm = 0.098710
Total gradient norm: 0.299517
=== Actor Training Debug (Iteration 6658) ===
Q mean: -12.673553
Q std: 20.276108
Actor loss: 12.677533
Action reg: 0.003980
  l1.weight: grad_norm = 0.155069
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.120156
Total gradient norm: 0.341210
=== Actor Training Debug (Iteration 6659) ===
Q mean: -14.384090
Q std: 21.261719
Actor loss: 14.388058
Action reg: 0.003967
  l1.weight: grad_norm = 0.165444
  l1.bias: grad_norm = 0.000633
  l2.weight: grad_norm = 0.154352
Total gradient norm: 0.491741
=== Actor Training Debug (Iteration 6660) ===
Q mean: -12.507814
Q std: 20.292290
Actor loss: 12.511775
Action reg: 0.003961
  l1.weight: grad_norm = 0.259129
  l1.bias: grad_norm = 0.000680
  l2.weight: grad_norm = 0.225859
Total gradient norm: 0.820137
=== Actor Training Debug (Iteration 6661) ===
Q mean: -12.695334
Q std: 19.064093
Actor loss: 12.699315
Action reg: 0.003980
  l1.weight: grad_norm = 0.415217
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.270895
Total gradient norm: 0.836177
=== Actor Training Debug (Iteration 6662) ===
Q mean: -13.309250
Q std: 19.550909
Actor loss: 13.313216
Action reg: 0.003967
  l1.weight: grad_norm = 0.154410
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.121853
Total gradient norm: 0.343775
=== Actor Training Debug (Iteration 6663) ===
Q mean: -12.484833
Q std: 18.613285
Actor loss: 12.488812
Action reg: 0.003980
  l1.weight: grad_norm = 0.133110
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.091121
Total gradient norm: 0.251532
=== Actor Training Debug (Iteration 6664) ===
Q mean: -12.767153
Q std: 19.479383
Actor loss: 12.771124
Action reg: 0.003971
  l1.weight: grad_norm = 0.225891
  l1.bias: grad_norm = 0.000719
  l2.weight: grad_norm = 0.175437
Total gradient norm: 0.489001
=== Actor Training Debug (Iteration 6665) ===
Q mean: -15.361399
Q std: 21.037603
Actor loss: 15.365379
Action reg: 0.003981
  l1.weight: grad_norm = 0.058161
  l1.bias: grad_norm = 0.000968
  l2.weight: grad_norm = 0.038976
Total gradient norm: 0.118265
=== Actor Training Debug (Iteration 6666) ===
Q mean: -13.965118
Q std: 21.148338
Actor loss: 13.969095
Action reg: 0.003977
  l1.weight: grad_norm = 0.198961
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.146671
Total gradient norm: 0.433151
=== Actor Training Debug (Iteration 6667) ===
Q mean: -13.827829
Q std: 20.106100
Actor loss: 13.831807
Action reg: 0.003978
  l1.weight: grad_norm = 0.166763
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.154406
Total gradient norm: 0.433137
=== Actor Training Debug (Iteration 6668) ===
Q mean: -13.819424
Q std: 20.330299
Actor loss: 13.823396
Action reg: 0.003972
  l1.weight: grad_norm = 0.143394
  l1.bias: grad_norm = 0.000751
  l2.weight: grad_norm = 0.130939
Total gradient norm: 0.434776
=== Actor Training Debug (Iteration 6669) ===
Q mean: -15.834852
Q std: 21.391533
Actor loss: 15.838830
Action reg: 0.003978
  l1.weight: grad_norm = 0.073735
  l1.bias: grad_norm = 0.000693
  l2.weight: grad_norm = 0.055070
Total gradient norm: 0.167975
=== Actor Training Debug (Iteration 6670) ===
Q mean: -14.998765
Q std: 21.990788
Actor loss: 15.002747
Action reg: 0.003982
  l1.weight: grad_norm = 0.070781
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.058172
Total gradient norm: 0.202497
=== Actor Training Debug (Iteration 6671) ===
Q mean: -13.386642
Q std: 20.368145
Actor loss: 13.390618
Action reg: 0.003976
  l1.weight: grad_norm = 0.148178
  l1.bias: grad_norm = 0.002172
  l2.weight: grad_norm = 0.116483
Total gradient norm: 0.397203
=== Actor Training Debug (Iteration 6672) ===
Q mean: -13.699831
Q std: 20.972649
Actor loss: 13.703812
Action reg: 0.003981
  l1.weight: grad_norm = 0.155176
  l1.bias: grad_norm = 0.000772
  l2.weight: grad_norm = 0.096690
Total gradient norm: 0.296912
=== Actor Training Debug (Iteration 6673) ===
Q mean: -13.770397
Q std: 19.839600
Actor loss: 13.774373
Action reg: 0.003976
  l1.weight: grad_norm = 0.218518
  l1.bias: grad_norm = 0.001500
  l2.weight: grad_norm = 0.140276
Total gradient norm: 0.560792
=== Actor Training Debug (Iteration 6674) ===
Q mean: -13.476460
Q std: 20.302912
Actor loss: 13.480429
Action reg: 0.003969
  l1.weight: grad_norm = 0.175061
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.136433
Total gradient norm: 0.394702
=== Actor Training Debug (Iteration 6675) ===
Q mean: -15.402265
Q std: 21.098764
Actor loss: 15.406234
Action reg: 0.003969
  l1.weight: grad_norm = 0.157638
  l1.bias: grad_norm = 0.002728
  l2.weight: grad_norm = 0.121608
Total gradient norm: 0.359095
=== Actor Training Debug (Iteration 6676) ===
Q mean: -12.128207
Q std: 19.291725
Actor loss: 12.132174
Action reg: 0.003966
  l1.weight: grad_norm = 0.143038
  l1.bias: grad_norm = 0.003535
  l2.weight: grad_norm = 0.135602
Total gradient norm: 0.417814
=== Actor Training Debug (Iteration 6677) ===
Q mean: -15.200413
Q std: 22.516478
Actor loss: 15.204376
Action reg: 0.003964
  l1.weight: grad_norm = 0.266104
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.212513
Total gradient norm: 0.608242
=== Actor Training Debug (Iteration 6678) ===
Q mean: -10.225428
Q std: 19.474426
Actor loss: 10.229382
Action reg: 0.003954
  l1.weight: grad_norm = 0.177567
  l1.bias: grad_norm = 0.001129
  l2.weight: grad_norm = 0.155798
Total gradient norm: 0.487846
=== Actor Training Debug (Iteration 6679) ===
Q mean: -11.757357
Q std: 19.088816
Actor loss: 11.761331
Action reg: 0.003974
  l1.weight: grad_norm = 0.274811
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.200443
Total gradient norm: 0.622152
=== Actor Training Debug (Iteration 6680) ===
Q mean: -13.551884
Q std: 20.559938
Actor loss: 13.555857
Action reg: 0.003973
  l1.weight: grad_norm = 0.108106
  l1.bias: grad_norm = 0.001620
  l2.weight: grad_norm = 0.086796
Total gradient norm: 0.257785
=== Actor Training Debug (Iteration 6681) ===
Q mean: -12.983133
Q std: 19.133625
Actor loss: 12.987095
Action reg: 0.003962
  l1.weight: grad_norm = 0.221337
  l1.bias: grad_norm = 0.001405
  l2.weight: grad_norm = 0.172732
Total gradient norm: 0.545025
=== Actor Training Debug (Iteration 6682) ===
Q mean: -14.005632
Q std: 20.348875
Actor loss: 14.009595
Action reg: 0.003963
  l1.weight: grad_norm = 0.198080
  l1.bias: grad_norm = 0.002118
  l2.weight: grad_norm = 0.128161
Total gradient norm: 0.385896
=== Actor Training Debug (Iteration 6683) ===
Q mean: -13.215116
Q std: 20.047979
Actor loss: 13.219080
Action reg: 0.003964
  l1.weight: grad_norm = 0.114651
  l1.bias: grad_norm = 0.001324
  l2.weight: grad_norm = 0.090138
Total gradient norm: 0.252769
=== Actor Training Debug (Iteration 6684) ===
Q mean: -11.051481
Q std: 18.545574
Actor loss: 11.055452
Action reg: 0.003971
  l1.weight: grad_norm = 0.214403
  l1.bias: grad_norm = 0.001293
  l2.weight: grad_norm = 0.178216
Total gradient norm: 0.491139
=== Actor Training Debug (Iteration 6685) ===
Q mean: -13.372396
Q std: 20.610300
Actor loss: 13.376371
Action reg: 0.003975
  l1.weight: grad_norm = 0.091351
  l1.bias: grad_norm = 0.001862
  l2.weight: grad_norm = 0.076875
Total gradient norm: 0.214767
=== Actor Training Debug (Iteration 6686) ===
Q mean: -13.246044
Q std: 19.414326
Actor loss: 13.250023
Action reg: 0.003978
  l1.weight: grad_norm = 0.103984
  l1.bias: grad_norm = 0.002116
  l2.weight: grad_norm = 0.083350
Total gradient norm: 0.227291
=== Actor Training Debug (Iteration 6687) ===
Q mean: -15.758421
Q std: 21.113594
Actor loss: 15.762394
Action reg: 0.003973
  l1.weight: grad_norm = 0.118201
  l1.bias: grad_norm = 0.001159
  l2.weight: grad_norm = 0.093831
Total gradient norm: 0.303069
=== Actor Training Debug (Iteration 6688) ===
Q mean: -10.080052
Q std: 18.607964
Actor loss: 10.084030
Action reg: 0.003978
  l1.weight: grad_norm = 0.311719
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.211709
Total gradient norm: 0.595548
=== Actor Training Debug (Iteration 6689) ===
Q mean: -11.648170
Q std: 20.102596
Actor loss: 11.652142
Action reg: 0.003972
  l1.weight: grad_norm = 0.118952
  l1.bias: grad_norm = 0.001435
  l2.weight: grad_norm = 0.081809
Total gradient norm: 0.297082
=== Actor Training Debug (Iteration 6690) ===
Q mean: -14.630571
Q std: 20.166832
Actor loss: 14.634548
Action reg: 0.003977
  l1.weight: grad_norm = 0.135908
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.106344
Total gradient norm: 0.332967
=== Actor Training Debug (Iteration 6691) ===
Q mean: -14.160727
Q std: 19.806021
Actor loss: 14.164707
Action reg: 0.003980
  l1.weight: grad_norm = 0.112901
  l1.bias: grad_norm = 0.000572
  l2.weight: grad_norm = 0.086814
Total gradient norm: 0.245733
=== Actor Training Debug (Iteration 6692) ===
Q mean: -13.622808
Q std: 20.145956
Actor loss: 13.626767
Action reg: 0.003959
  l1.weight: grad_norm = 0.193993
  l1.bias: grad_norm = 0.001743
  l2.weight: grad_norm = 0.150105
Total gradient norm: 0.471962
=== Actor Training Debug (Iteration 6693) ===
Q mean: -13.719839
Q std: 19.054504
Actor loss: 13.723820
Action reg: 0.003981
  l1.weight: grad_norm = 0.172428
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.144317
Total gradient norm: 0.406011
=== Actor Training Debug (Iteration 6694) ===
Q mean: -14.171699
Q std: 20.820517
Actor loss: 14.175674
Action reg: 0.003976
  l1.weight: grad_norm = 0.104153
  l1.bias: grad_norm = 0.000618
  l2.weight: grad_norm = 0.085752
Total gradient norm: 0.261033
=== Actor Training Debug (Iteration 6695) ===
Q mean: -13.427060
Q std: 20.978827
Actor loss: 13.431027
Action reg: 0.003968
  l1.weight: grad_norm = 0.143033
  l1.bias: grad_norm = 0.001945
  l2.weight: grad_norm = 0.124482
Total gradient norm: 0.375249
=== Actor Training Debug (Iteration 6696) ===
Q mean: -13.727334
Q std: 20.429243
Actor loss: 13.731298
Action reg: 0.003964
  l1.weight: grad_norm = 0.093575
  l1.bias: grad_norm = 0.001675
  l2.weight: grad_norm = 0.076736
Total gradient norm: 0.275100
=== Actor Training Debug (Iteration 6697) ===
Q mean: -14.629202
Q std: 20.370562
Actor loss: 14.633184
Action reg: 0.003982
  l1.weight: grad_norm = 0.106055
  l1.bias: grad_norm = 0.000710
  l2.weight: grad_norm = 0.073269
Total gradient norm: 0.215352
=== Actor Training Debug (Iteration 6698) ===
Q mean: -13.081838
Q std: 19.777172
Actor loss: 13.085809
Action reg: 0.003971
  l1.weight: grad_norm = 0.051768
  l1.bias: grad_norm = 0.002887
  l2.weight: grad_norm = 0.038960
Total gradient norm: 0.129904
=== Actor Training Debug (Iteration 6699) ===
Q mean: -13.132208
Q std: 18.631290
Actor loss: 13.136173
Action reg: 0.003965
  l1.weight: grad_norm = 0.183112
  l1.bias: grad_norm = 0.006221
  l2.weight: grad_norm = 0.144372
Total gradient norm: 0.413590
=== Actor Training Debug (Iteration 6700) ===
Q mean: -14.128580
Q std: 19.734407
Actor loss: 14.132563
Action reg: 0.003983
  l1.weight: grad_norm = 0.129379
  l1.bias: grad_norm = 0.001463
  l2.weight: grad_norm = 0.076290
Total gradient norm: 0.208817
=== Actor Training Debug (Iteration 6701) ===
Q mean: -10.929792
Q std: 17.952307
Actor loss: 10.933761
Action reg: 0.003968
  l1.weight: grad_norm = 0.271903
  l1.bias: grad_norm = 0.001698
  l2.weight: grad_norm = 0.222036
Total gradient norm: 0.656721
=== Actor Training Debug (Iteration 6702) ===
Q mean: -14.681193
Q std: 20.532640
Actor loss: 14.685144
Action reg: 0.003951
  l1.weight: grad_norm = 0.287716
  l1.bias: grad_norm = 0.000727
  l2.weight: grad_norm = 0.214901
Total gradient norm: 0.731241
=== Actor Training Debug (Iteration 6703) ===
Q mean: -15.576824
Q std: 22.529446
Actor loss: 15.580810
Action reg: 0.003985
  l1.weight: grad_norm = 0.126329
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.107964
Total gradient norm: 0.299697
=== Actor Training Debug (Iteration 6704) ===
Q mean: -16.596905
Q std: 23.272316
Actor loss: 16.600876
Action reg: 0.003972
  l1.weight: grad_norm = 0.118365
  l1.bias: grad_norm = 0.001208
  l2.weight: grad_norm = 0.110032
Total gradient norm: 0.396467
=== Actor Training Debug (Iteration 6705) ===
Q mean: -12.795523
Q std: 20.252686
Actor loss: 12.799491
Action reg: 0.003968
  l1.weight: grad_norm = 0.147613
  l1.bias: grad_norm = 0.001163
  l2.weight: grad_norm = 0.111930
Total gradient norm: 0.314312
=== Actor Training Debug (Iteration 6706) ===
Q mean: -13.927634
Q std: 20.496290
Actor loss: 13.931608
Action reg: 0.003974
  l1.weight: grad_norm = 0.141641
  l1.bias: grad_norm = 0.001010
  l2.weight: grad_norm = 0.111033
Total gradient norm: 0.339155
=== Actor Training Debug (Iteration 6707) ===
Q mean: -15.917477
Q std: 22.281466
Actor loss: 15.921447
Action reg: 0.003970
  l1.weight: grad_norm = 0.158612
  l1.bias: grad_norm = 0.000884
  l2.weight: grad_norm = 0.117550
Total gradient norm: 0.329192
=== Actor Training Debug (Iteration 6708) ===
Q mean: -13.139348
Q std: 18.477762
Actor loss: 13.143319
Action reg: 0.003971
  l1.weight: grad_norm = 0.195000
  l1.bias: grad_norm = 0.000510
  l2.weight: grad_norm = 0.141495
Total gradient norm: 0.442698
=== Actor Training Debug (Iteration 6709) ===
Q mean: -14.833327
Q std: 20.712410
Actor loss: 14.837301
Action reg: 0.003974
  l1.weight: grad_norm = 0.191707
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.157453
Total gradient norm: 0.528306
=== Actor Training Debug (Iteration 6710) ===
Q mean: -11.467135
Q std: 19.666872
Actor loss: 11.471098
Action reg: 0.003962
  l1.weight: grad_norm = 0.173901
  l1.bias: grad_norm = 0.003268
  l2.weight: grad_norm = 0.127650
Total gradient norm: 0.360354
=== Actor Training Debug (Iteration 6711) ===
Q mean: -15.525775
Q std: 21.496134
Actor loss: 15.529752
Action reg: 0.003977
  l1.weight: grad_norm = 0.199901
  l1.bias: grad_norm = 0.000856
  l2.weight: grad_norm = 0.134260
Total gradient norm: 0.369863
=== Actor Training Debug (Iteration 6712) ===
Q mean: -13.909937
Q std: 20.799154
Actor loss: 13.913888
Action reg: 0.003951
  l1.weight: grad_norm = 0.099594
  l1.bias: grad_norm = 0.002044
  l2.weight: grad_norm = 0.078926
Total gradient norm: 0.237426
=== Actor Training Debug (Iteration 6713) ===
Q mean: -12.647781
Q std: 19.309864
Actor loss: 12.651756
Action reg: 0.003975
  l1.weight: grad_norm = 0.139059
  l1.bias: grad_norm = 0.000684
  l2.weight: grad_norm = 0.109927
Total gradient norm: 0.339413
=== Actor Training Debug (Iteration 6714) ===
Q mean: -12.425755
Q std: 19.411709
Actor loss: 12.429715
Action reg: 0.003961
  l1.weight: grad_norm = 0.223488
  l1.bias: grad_norm = 0.002254
  l2.weight: grad_norm = 0.142974
Total gradient norm: 0.545783
=== Actor Training Debug (Iteration 6715) ===
Q mean: -15.736113
Q std: 21.700974
Actor loss: 15.740085
Action reg: 0.003972
  l1.weight: grad_norm = 0.209590
  l1.bias: grad_norm = 0.000982
  l2.weight: grad_norm = 0.151601
Total gradient norm: 0.424425
=== Actor Training Debug (Iteration 6716) ===
Q mean: -14.837972
Q std: 20.673412
Actor loss: 14.841932
Action reg: 0.003961
  l1.weight: grad_norm = 0.184846
  l1.bias: grad_norm = 0.002593
  l2.weight: grad_norm = 0.150641
Total gradient norm: 0.533068
=== Actor Training Debug (Iteration 6717) ===
Q mean: -16.284201
Q std: 22.740534
Actor loss: 16.288187
Action reg: 0.003987
  l1.weight: grad_norm = 0.141313
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.104763
Total gradient norm: 0.347148
=== Actor Training Debug (Iteration 6718) ===
Q mean: -14.370197
Q std: 21.202761
Actor loss: 14.374139
Action reg: 0.003941
  l1.weight: grad_norm = 0.145525
  l1.bias: grad_norm = 0.002794
  l2.weight: grad_norm = 0.129910
Total gradient norm: 0.410763
=== Actor Training Debug (Iteration 6719) ===
Q mean: -13.812440
Q std: 20.046030
Actor loss: 13.816400
Action reg: 0.003959
  l1.weight: grad_norm = 0.228969
  l1.bias: grad_norm = 0.005897
  l2.weight: grad_norm = 0.173138
Total gradient norm: 0.533694
=== Actor Training Debug (Iteration 6720) ===
Q mean: -13.373419
Q std: 21.766312
Actor loss: 13.377389
Action reg: 0.003970
  l1.weight: grad_norm = 0.148866
  l1.bias: grad_norm = 0.001882
  l2.weight: grad_norm = 0.107996
Total gradient norm: 0.339119
=== Actor Training Debug (Iteration 6721) ===
Q mean: -12.379892
Q std: 19.021025
Actor loss: 12.383835
Action reg: 0.003943
  l1.weight: grad_norm = 0.231886
  l1.bias: grad_norm = 0.002056
  l2.weight: grad_norm = 0.162652
Total gradient norm: 0.419083
=== Actor Training Debug (Iteration 6722) ===
Q mean: -15.554152
Q std: 21.928396
Actor loss: 15.558116
Action reg: 0.003965
  l1.weight: grad_norm = 0.332247
  l1.bias: grad_norm = 0.001024
  l2.weight: grad_norm = 0.210057
Total gradient norm: 0.568711
=== Actor Training Debug (Iteration 6723) ===
Q mean: -14.893761
Q std: 22.524834
Actor loss: 14.897724
Action reg: 0.003963
  l1.weight: grad_norm = 0.294885
  l1.bias: grad_norm = 0.002303
  l2.weight: grad_norm = 0.233388
Total gradient norm: 0.690218
=== Actor Training Debug (Iteration 6724) ===
Q mean: -11.923366
Q std: 19.070488
Actor loss: 11.927323
Action reg: 0.003958
  l1.weight: grad_norm = 0.259864
  l1.bias: grad_norm = 0.002117
  l2.weight: grad_norm = 0.189600
Total gradient norm: 0.538711
=== Actor Training Debug (Iteration 6725) ===
Q mean: -15.460333
Q std: 22.024511
Actor loss: 15.464307
Action reg: 0.003974
  l1.weight: grad_norm = 0.223790
  l1.bias: grad_norm = 0.002128
  l2.weight: grad_norm = 0.172800
Total gradient norm: 0.527486
=== Actor Training Debug (Iteration 6726) ===
Q mean: -15.136659
Q std: 21.582878
Actor loss: 15.140608
Action reg: 0.003950
  l1.weight: grad_norm = 0.268384
  l1.bias: grad_norm = 0.004446
  l2.weight: grad_norm = 0.209458
Total gradient norm: 0.604043
=== Actor Training Debug (Iteration 6727) ===
Q mean: -11.974339
Q std: 18.698317
Actor loss: 11.978303
Action reg: 0.003963
  l1.weight: grad_norm = 0.098468
  l1.bias: grad_norm = 0.002135
  l2.weight: grad_norm = 0.073103
Total gradient norm: 0.206892
=== Actor Training Debug (Iteration 6728) ===
Q mean: -13.190745
Q std: 19.993151
Actor loss: 13.194719
Action reg: 0.003974
  l1.weight: grad_norm = 0.084128
  l1.bias: grad_norm = 0.001772
  l2.weight: grad_norm = 0.058197
Total gradient norm: 0.166571
=== Actor Training Debug (Iteration 6729) ===
Q mean: -13.965157
Q std: 19.910900
Actor loss: 13.969124
Action reg: 0.003967
  l1.weight: grad_norm = 0.133279
  l1.bias: grad_norm = 0.001504
  l2.weight: grad_norm = 0.102907
Total gradient norm: 0.322388
=== Actor Training Debug (Iteration 6730) ===
Q mean: -12.339991
Q std: 18.792917
Actor loss: 12.343964
Action reg: 0.003973
  l1.weight: grad_norm = 0.238124
  l1.bias: grad_norm = 0.000858
  l2.weight: grad_norm = 0.179370
Total gradient norm: 0.676046
=== Actor Training Debug (Iteration 6731) ===
Q mean: -14.212703
Q std: 20.617008
Actor loss: 14.216659
Action reg: 0.003956
  l1.weight: grad_norm = 0.127753
  l1.bias: grad_norm = 0.001405
  l2.weight: grad_norm = 0.102542
Total gradient norm: 0.281584
=== Actor Training Debug (Iteration 6732) ===
Q mean: -13.685248
Q std: 20.812073
Actor loss: 13.689223
Action reg: 0.003975
  l1.weight: grad_norm = 0.156881
  l1.bias: grad_norm = 0.001408
  l2.weight: grad_norm = 0.101686
Total gradient norm: 0.314267
=== Actor Training Debug (Iteration 6733) ===
Q mean: -12.166283
Q std: 19.078690
Actor loss: 12.170249
Action reg: 0.003967
  l1.weight: grad_norm = 0.302663
  l1.bias: grad_norm = 0.002753
  l2.weight: grad_norm = 0.202530
Total gradient norm: 0.611186
=== Actor Training Debug (Iteration 6734) ===
Q mean: -13.335657
Q std: 19.374001
Actor loss: 13.339616
Action reg: 0.003959
  l1.weight: grad_norm = 0.187715
  l1.bias: grad_norm = 0.008317
  l2.weight: grad_norm = 0.145472
Total gradient norm: 0.479242
=== Actor Training Debug (Iteration 6735) ===
Q mean: -14.455854
Q std: 20.719921
Actor loss: 14.459810
Action reg: 0.003956
  l1.weight: grad_norm = 0.101392
  l1.bias: grad_norm = 0.002146
  l2.weight: grad_norm = 0.089545
Total gradient norm: 0.251209
=== Actor Training Debug (Iteration 6736) ===
Q mean: -14.205547
Q std: 20.790848
Actor loss: 14.209531
Action reg: 0.003983
  l1.weight: grad_norm = 0.259486
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.201712
Total gradient norm: 0.770107
=== Actor Training Debug (Iteration 6737) ===
Q mean: -14.085527
Q std: 19.374685
Actor loss: 14.089502
Action reg: 0.003975
  l1.weight: grad_norm = 0.213407
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.180079
Total gradient norm: 0.508171
=== Actor Training Debug (Iteration 6738) ===
Q mean: -13.683634
Q std: 20.450344
Actor loss: 13.687586
Action reg: 0.003952
  l1.weight: grad_norm = 0.100255
  l1.bias: grad_norm = 0.002264
  l2.weight: grad_norm = 0.075639
Total gradient norm: 0.245353
=== Actor Training Debug (Iteration 6739) ===
Q mean: -14.171092
Q std: 21.337584
Actor loss: 14.175052
Action reg: 0.003960
  l1.weight: grad_norm = 0.258671
  l1.bias: grad_norm = 0.001635
  l2.weight: grad_norm = 0.178824
Total gradient norm: 0.597962
=== Actor Training Debug (Iteration 6740) ===
Q mean: -12.257769
Q std: 18.745319
Actor loss: 12.261738
Action reg: 0.003969
  l1.weight: grad_norm = 0.186275
  l1.bias: grad_norm = 0.002004
  l2.weight: grad_norm = 0.149033
Total gradient norm: 0.468720
=== Actor Training Debug (Iteration 6741) ===
Q mean: -11.778255
Q std: 19.313698
Actor loss: 11.782210
Action reg: 0.003956
  l1.weight: grad_norm = 0.202056
  l1.bias: grad_norm = 0.002480
  l2.weight: grad_norm = 0.148262
Total gradient norm: 0.462613
=== Actor Training Debug (Iteration 6742) ===
Q mean: -14.665754
Q std: 20.464083
Actor loss: 14.669733
Action reg: 0.003979
  l1.weight: grad_norm = 0.097167
  l1.bias: grad_norm = 0.001538
  l2.weight: grad_norm = 0.070436
Total gradient norm: 0.211927
=== Actor Training Debug (Iteration 6743) ===
Q mean: -14.956302
Q std: 20.492392
Actor loss: 14.960258
Action reg: 0.003956
  l1.weight: grad_norm = 0.147813
  l1.bias: grad_norm = 0.001649
  l2.weight: grad_norm = 0.127764
Total gradient norm: 0.449750
=== Actor Training Debug (Iteration 6744) ===
Q mean: -15.231281
Q std: 22.476330
Actor loss: 15.235243
Action reg: 0.003961
  l1.weight: grad_norm = 0.327609
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.225749
Total gradient norm: 0.670784
=== Actor Training Debug (Iteration 6745) ===
Q mean: -12.818549
Q std: 19.451206
Actor loss: 12.822527
Action reg: 0.003978
  l1.weight: grad_norm = 0.233308
  l1.bias: grad_norm = 0.001859
  l2.weight: grad_norm = 0.177819
Total gradient norm: 0.679827
=== Actor Training Debug (Iteration 6746) ===
Q mean: -15.744043
Q std: 21.914984
Actor loss: 15.748001
Action reg: 0.003957
  l1.weight: grad_norm = 0.091553
  l1.bias: grad_norm = 0.003130
  l2.weight: grad_norm = 0.070297
Total gradient norm: 0.235510
=== Actor Training Debug (Iteration 6747) ===
Q mean: -15.836101
Q std: 22.499765
Actor loss: 15.840074
Action reg: 0.003973
  l1.weight: grad_norm = 0.061272
  l1.bias: grad_norm = 0.001746
  l2.weight: grad_norm = 0.043448
Total gradient norm: 0.127534
=== Actor Training Debug (Iteration 6748) ===
Q mean: -15.453577
Q std: 20.493551
Actor loss: 15.457548
Action reg: 0.003971
  l1.weight: grad_norm = 0.168169
  l1.bias: grad_norm = 0.000657
  l2.weight: grad_norm = 0.134544
Total gradient norm: 0.357389
=== Actor Training Debug (Iteration 6749) ===
Q mean: -12.710789
Q std: 19.234703
Actor loss: 12.714768
Action reg: 0.003979
  l1.weight: grad_norm = 0.089022
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.076704
Total gradient norm: 0.208178
=== Actor Training Debug (Iteration 6750) ===
Q mean: -14.900115
Q std: 21.299299
Actor loss: 14.904057
Action reg: 0.003942
  l1.weight: grad_norm = 0.137845
  l1.bias: grad_norm = 0.003523
  l2.weight: grad_norm = 0.113077
Total gradient norm: 0.363548
=== Actor Training Debug (Iteration 6751) ===
Q mean: -11.483821
Q std: 19.677530
Actor loss: 11.487777
Action reg: 0.003956
  l1.weight: grad_norm = 0.105870
  l1.bias: grad_norm = 0.002737
  l2.weight: grad_norm = 0.092385
Total gradient norm: 0.317465
=== Actor Training Debug (Iteration 6752) ===
Q mean: -14.119846
Q std: 20.508377
Actor loss: 14.123814
Action reg: 0.003967
  l1.weight: grad_norm = 0.148710
  l1.bias: grad_norm = 0.001162
  l2.weight: grad_norm = 0.111103
Total gradient norm: 0.307089
=== Actor Training Debug (Iteration 6753) ===
Q mean: -13.101999
Q std: 20.787907
Actor loss: 13.105974
Action reg: 0.003974
  l1.weight: grad_norm = 0.073572
  l1.bias: grad_norm = 0.002197
  l2.weight: grad_norm = 0.051935
Total gradient norm: 0.172641
=== Actor Training Debug (Iteration 6754) ===
Q mean: -14.004664
Q std: 22.006390
Actor loss: 14.008617
Action reg: 0.003953
  l1.weight: grad_norm = 0.250782
  l1.bias: grad_norm = 0.002412
  l2.weight: grad_norm = 0.196464
Total gradient norm: 0.561153
=== Actor Training Debug (Iteration 6755) ===
Q mean: -15.073743
Q std: 21.486006
Actor loss: 15.077730
Action reg: 0.003988
  l1.weight: grad_norm = 0.376288
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.252705
Total gradient norm: 0.698030
=== Actor Training Debug (Iteration 6756) ===
Q mean: -11.937211
Q std: 19.174252
Actor loss: 11.941166
Action reg: 0.003955
  l1.weight: grad_norm = 0.123802
  l1.bias: grad_norm = 0.003309
  l2.weight: grad_norm = 0.099095
Total gradient norm: 0.317349
=== Actor Training Debug (Iteration 6757) ===
Q mean: -13.170548
Q std: 20.383333
Actor loss: 13.174484
Action reg: 0.003936
  l1.weight: grad_norm = 0.224161
  l1.bias: grad_norm = 0.002094
  l2.weight: grad_norm = 0.158348
Total gradient norm: 0.484671
=== Actor Training Debug (Iteration 6758) ===
Q mean: -11.773726
Q std: 18.033413
Actor loss: 11.777695
Action reg: 0.003969
  l1.weight: grad_norm = 0.194027
  l1.bias: grad_norm = 0.001570
  l2.weight: grad_norm = 0.151776
Total gradient norm: 0.382322
=== Actor Training Debug (Iteration 6759) ===
Q mean: -15.001287
Q std: 20.694122
Actor loss: 15.005249
Action reg: 0.003962
  l1.weight: grad_norm = 0.229136
  l1.bias: grad_norm = 0.001738
  l2.weight: grad_norm = 0.160355
Total gradient norm: 0.462925
=== Actor Training Debug (Iteration 6760) ===
Q mean: -12.608858
Q std: 19.294340
Actor loss: 12.612826
Action reg: 0.003968
  l1.weight: grad_norm = 0.202689
  l1.bias: grad_norm = 0.003255
  l2.weight: grad_norm = 0.153913
Total gradient norm: 0.477409
=== Actor Training Debug (Iteration 6761) ===
Q mean: -13.452863
Q std: 21.728813
Actor loss: 13.456827
Action reg: 0.003964
  l1.weight: grad_norm = 0.188668
  l1.bias: grad_norm = 0.001658
  l2.weight: grad_norm = 0.151349
Total gradient norm: 0.444075
=== Actor Training Debug (Iteration 6762) ===
Q mean: -12.268569
Q std: 20.067350
Actor loss: 12.272535
Action reg: 0.003966
  l1.weight: grad_norm = 0.173911
  l1.bias: grad_norm = 0.001667
  l2.weight: grad_norm = 0.121390
Total gradient norm: 0.343286
=== Actor Training Debug (Iteration 6763) ===
Q mean: -14.803255
Q std: 21.817629
Actor loss: 14.807239
Action reg: 0.003983
  l1.weight: grad_norm = 0.232852
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.185982
Total gradient norm: 0.537848
=== Actor Training Debug (Iteration 6764) ===
Q mean: -13.331021
Q std: 20.632927
Actor loss: 13.335000
Action reg: 0.003979
  l1.weight: grad_norm = 0.144561
  l1.bias: grad_norm = 0.000484
  l2.weight: grad_norm = 0.104039
Total gradient norm: 0.302195
=== Actor Training Debug (Iteration 6765) ===
Q mean: -14.733343
Q std: 19.783056
Actor loss: 14.737323
Action reg: 0.003980
  l1.weight: grad_norm = 0.264510
  l1.bias: grad_norm = 0.001125
  l2.weight: grad_norm = 0.155946
Total gradient norm: 0.419372
=== Actor Training Debug (Iteration 6766) ===
Q mean: -14.107069
Q std: 19.773861
Actor loss: 14.111044
Action reg: 0.003975
  l1.weight: grad_norm = 0.206259
  l1.bias: grad_norm = 0.001056
  l2.weight: grad_norm = 0.151227
Total gradient norm: 0.425820
=== Actor Training Debug (Iteration 6767) ===
Q mean: -13.493750
Q std: 19.796103
Actor loss: 13.497723
Action reg: 0.003973
  l1.weight: grad_norm = 0.120998
  l1.bias: grad_norm = 0.000715
  l2.weight: grad_norm = 0.079719
Total gradient norm: 0.230671
=== Actor Training Debug (Iteration 6768) ===
Q mean: -12.210468
Q std: 18.509153
Actor loss: 12.214429
Action reg: 0.003961
  l1.weight: grad_norm = 0.181846
  l1.bias: grad_norm = 0.002103
  l2.weight: grad_norm = 0.151379
Total gradient norm: 0.518793
=== Actor Training Debug (Iteration 6769) ===
Q mean: -13.781256
Q std: 20.283709
Actor loss: 13.785201
Action reg: 0.003946
  l1.weight: grad_norm = 0.207244
  l1.bias: grad_norm = 0.002938
  l2.weight: grad_norm = 0.159328
Total gradient norm: 0.531856
=== Actor Training Debug (Iteration 6770) ===
Q mean: -15.036979
Q std: 21.446795
Actor loss: 15.040953
Action reg: 0.003973
  l1.weight: grad_norm = 0.253538
  l1.bias: grad_norm = 0.000629
  l2.weight: grad_norm = 0.206895
Total gradient norm: 0.621884
=== Actor Training Debug (Iteration 6771) ===
Q mean: -12.610323
Q std: 20.759621
Actor loss: 12.614289
Action reg: 0.003967
  l1.weight: grad_norm = 0.174793
  l1.bias: grad_norm = 0.000864
  l2.weight: grad_norm = 0.127833
Total gradient norm: 0.351214
=== Actor Training Debug (Iteration 6772) ===
Q mean: -14.795172
Q std: 21.594687
Actor loss: 14.799129
Action reg: 0.003958
  l1.weight: grad_norm = 0.154936
  l1.bias: grad_norm = 0.001261
  l2.weight: grad_norm = 0.135680
Total gradient norm: 0.381970
=== Actor Training Debug (Iteration 6773) ===
Q mean: -15.039877
Q std: 22.113634
Actor loss: 15.043835
Action reg: 0.003958
  l1.weight: grad_norm = 0.198954
  l1.bias: grad_norm = 0.001435
  l2.weight: grad_norm = 0.147916
Total gradient norm: 0.440678
=== Actor Training Debug (Iteration 6774) ===
Q mean: -13.008594
Q std: 20.141605
Actor loss: 13.012547
Action reg: 0.003953
  l1.weight: grad_norm = 0.122472
  l1.bias: grad_norm = 0.002245
  l2.weight: grad_norm = 0.106355
Total gradient norm: 0.333086
=== Actor Training Debug (Iteration 6775) ===
Q mean: -13.752572
Q std: 19.806011
Actor loss: 13.756548
Action reg: 0.003976
  l1.weight: grad_norm = 0.224369
  l1.bias: grad_norm = 0.002137
  l2.weight: grad_norm = 0.172156
Total gradient norm: 0.487299
=== Actor Training Debug (Iteration 6776) ===
Q mean: -12.455858
Q std: 19.387444
Actor loss: 12.459808
Action reg: 0.003950
  l1.weight: grad_norm = 0.212170
  l1.bias: grad_norm = 0.001530
  l2.weight: grad_norm = 0.159590
Total gradient norm: 0.442178
=== Actor Training Debug (Iteration 6777) ===
Q mean: -14.494716
Q std: 20.969440
Actor loss: 14.498694
Action reg: 0.003978
  l1.weight: grad_norm = 0.167421
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.139728
Total gradient norm: 0.420941
=== Actor Training Debug (Iteration 6778) ===
Q mean: -13.278976
Q std: 19.540018
Actor loss: 13.282924
Action reg: 0.003947
  l1.weight: grad_norm = 0.141491
  l1.bias: grad_norm = 0.001873
  l2.weight: grad_norm = 0.094397
Total gradient norm: 0.286742
=== Actor Training Debug (Iteration 6779) ===
Q mean: -13.477158
Q std: 19.981497
Actor loss: 13.481116
Action reg: 0.003959
  l1.weight: grad_norm = 0.103466
  l1.bias: grad_norm = 0.005097
  l2.weight: grad_norm = 0.092994
Total gradient norm: 0.334413
=== Actor Training Debug (Iteration 6780) ===
Q mean: -14.785388
Q std: 21.156624
Actor loss: 14.789356
Action reg: 0.003968
  l1.weight: grad_norm = 0.269827
  l1.bias: grad_norm = 0.000560
  l2.weight: grad_norm = 0.214073
Total gradient norm: 0.547926
=== Actor Training Debug (Iteration 6781) ===
Q mean: -12.474906
Q std: 20.002523
Actor loss: 12.478865
Action reg: 0.003958
  l1.weight: grad_norm = 0.186374
  l1.bias: grad_norm = 0.002374
  l2.weight: grad_norm = 0.144786
Total gradient norm: 0.397815
=== Actor Training Debug (Iteration 6782) ===
Q mean: -13.852921
Q std: 21.314186
Actor loss: 13.856879
Action reg: 0.003958
  l1.weight: grad_norm = 0.216932
  l1.bias: grad_norm = 0.004093
  l2.weight: grad_norm = 0.172293
Total gradient norm: 0.552838
=== Actor Training Debug (Iteration 6783) ===
Q mean: -15.522589
Q std: 22.580770
Actor loss: 15.526560
Action reg: 0.003971
  l1.weight: grad_norm = 0.210499
  l1.bias: grad_norm = 0.001288
  l2.weight: grad_norm = 0.138278
Total gradient norm: 0.424004
=== Actor Training Debug (Iteration 6784) ===
Q mean: -13.103791
Q std: 19.513224
Actor loss: 13.107751
Action reg: 0.003959
  l1.weight: grad_norm = 0.144326
  l1.bias: grad_norm = 0.001450
  l2.weight: grad_norm = 0.105037
Total gradient norm: 0.303004
=== Actor Training Debug (Iteration 6785) ===
Q mean: -14.515882
Q std: 20.629566
Actor loss: 14.519853
Action reg: 0.003971
  l1.weight: grad_norm = 0.187067
  l1.bias: grad_norm = 0.002826
  l2.weight: grad_norm = 0.127194
Total gradient norm: 0.365243
=== Actor Training Debug (Iteration 6786) ===
Q mean: -12.959384
Q std: 21.075005
Actor loss: 12.963349
Action reg: 0.003966
  l1.weight: grad_norm = 0.205080
  l1.bias: grad_norm = 0.001522
  l2.weight: grad_norm = 0.165550
Total gradient norm: 0.539830
=== Actor Training Debug (Iteration 6787) ===
Q mean: -13.171648
Q std: 19.764084
Actor loss: 13.175618
Action reg: 0.003970
  l1.weight: grad_norm = 0.148986
  l1.bias: grad_norm = 0.002581
  l2.weight: grad_norm = 0.119843
Total gradient norm: 0.389771
=== Actor Training Debug (Iteration 6788) ===
Q mean: -10.740746
Q std: 18.496151
Actor loss: 10.744711
Action reg: 0.003964
  l1.weight: grad_norm = 0.115864
  l1.bias: grad_norm = 0.002608
  l2.weight: grad_norm = 0.090899
Total gradient norm: 0.286585
=== Actor Training Debug (Iteration 6789) ===
Q mean: -13.124112
Q std: 19.875809
Actor loss: 13.128087
Action reg: 0.003975
  l1.weight: grad_norm = 0.177810
  l1.bias: grad_norm = 0.000775
  l2.weight: grad_norm = 0.148669
Total gradient norm: 0.487153
=== Actor Training Debug (Iteration 6790) ===
Q mean: -14.837545
Q std: 20.962521
Actor loss: 14.841516
Action reg: 0.003970
  l1.weight: grad_norm = 0.393319
  l1.bias: grad_norm = 0.001544
  l2.weight: grad_norm = 0.334544
Total gradient norm: 0.971898
=== Actor Training Debug (Iteration 6791) ===
Q mean: -15.256809
Q std: 22.338940
Actor loss: 15.260763
Action reg: 0.003954
  l1.weight: grad_norm = 0.207553
  l1.bias: grad_norm = 0.003585
  l2.weight: grad_norm = 0.143894
Total gradient norm: 0.475144
=== Actor Training Debug (Iteration 6792) ===
Q mean: -11.516493
Q std: 19.432306
Actor loss: 11.520461
Action reg: 0.003969
  l1.weight: grad_norm = 0.162675
  l1.bias: grad_norm = 0.002205
  l2.weight: grad_norm = 0.133009
Total gradient norm: 0.388830
=== Actor Training Debug (Iteration 6793) ===
Q mean: -15.870188
Q std: 21.887991
Actor loss: 15.874157
Action reg: 0.003969
  l1.weight: grad_norm = 0.071721
  l1.bias: grad_norm = 0.002544
  l2.weight: grad_norm = 0.063042
Total gradient norm: 0.218360
=== Actor Training Debug (Iteration 6794) ===
Q mean: -13.252948
Q std: 21.506050
Actor loss: 13.256916
Action reg: 0.003968
  l1.weight: grad_norm = 0.160433
  l1.bias: grad_norm = 0.002377
  l2.weight: grad_norm = 0.123516
Total gradient norm: 0.342993
=== Actor Training Debug (Iteration 6795) ===
Q mean: -13.668191
Q std: 19.978683
Actor loss: 13.672173
Action reg: 0.003981
  l1.weight: grad_norm = 0.120042
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.092426
Total gradient norm: 0.263728
=== Actor Training Debug (Iteration 6796) ===
Q mean: -12.384126
Q std: 19.144762
Actor loss: 12.388075
Action reg: 0.003949
  l1.weight: grad_norm = 0.227472
  l1.bias: grad_norm = 0.001253
  l2.weight: grad_norm = 0.161848
Total gradient norm: 0.457058
=== Actor Training Debug (Iteration 6797) ===
Q mean: -13.930929
Q std: 20.517147
Actor loss: 13.934910
Action reg: 0.003981
  l1.weight: grad_norm = 0.095810
  l1.bias: grad_norm = 0.001017
  l2.weight: grad_norm = 0.091273
Total gradient norm: 0.295226
=== Actor Training Debug (Iteration 6798) ===
Q mean: -13.634813
Q std: 20.725317
Actor loss: 13.638775
Action reg: 0.003962
  l1.weight: grad_norm = 0.206735
  l1.bias: grad_norm = 0.001108
  l2.weight: grad_norm = 0.172136
Total gradient norm: 0.472790
=== Actor Training Debug (Iteration 6799) ===
Q mean: -13.981237
Q std: 20.539076
Actor loss: 13.985213
Action reg: 0.003976
  l1.weight: grad_norm = 0.199420
  l1.bias: grad_norm = 0.001978
  l2.weight: grad_norm = 0.150132
Total gradient norm: 0.587151
=== Actor Training Debug (Iteration 6800) ===
Q mean: -13.601079
Q std: 20.162941
Actor loss: 13.605033
Action reg: 0.003954
  l1.weight: grad_norm = 0.105218
  l1.bias: grad_norm = 0.002534
  l2.weight: grad_norm = 0.084477
Total gradient norm: 0.282041
=== Actor Training Debug (Iteration 6801) ===
Q mean: -13.343693
Q std: 21.005676
Actor loss: 13.347666
Action reg: 0.003973
  l1.weight: grad_norm = 0.117785
  l1.bias: grad_norm = 0.000934
  l2.weight: grad_norm = 0.082618
Total gradient norm: 0.226661
=== Actor Training Debug (Iteration 6802) ===
Q mean: -11.430372
Q std: 18.442965
Actor loss: 11.434343
Action reg: 0.003971
  l1.weight: grad_norm = 0.142717
  l1.bias: grad_norm = 0.001385
  l2.weight: grad_norm = 0.122156
Total gradient norm: 0.336580
=== Actor Training Debug (Iteration 6803) ===
Q mean: -13.848852
Q std: 21.102116
Actor loss: 13.852817
Action reg: 0.003965
  l1.weight: grad_norm = 0.274743
  l1.bias: grad_norm = 0.000780
  l2.weight: grad_norm = 0.187588
Total gradient norm: 0.488484
=== Actor Training Debug (Iteration 6804) ===
Q mean: -13.516885
Q std: 19.969593
Actor loss: 13.520845
Action reg: 0.003961
  l1.weight: grad_norm = 0.224945
  l1.bias: grad_norm = 0.002099
  l2.weight: grad_norm = 0.176443
Total gradient norm: 0.478137
=== Actor Training Debug (Iteration 6805) ===
Q mean: -11.031503
Q std: 18.423164
Actor loss: 11.035466
Action reg: 0.003964
  l1.weight: grad_norm = 0.255434
  l1.bias: grad_norm = 0.001074
  l2.weight: grad_norm = 0.189919
Total gradient norm: 0.542587
=== Actor Training Debug (Iteration 6806) ===
Q mean: -14.396204
Q std: 20.304348
Actor loss: 14.400181
Action reg: 0.003977
  l1.weight: grad_norm = 0.197857
  l1.bias: grad_norm = 0.000707
  l2.weight: grad_norm = 0.145540
Total gradient norm: 0.414288
=== Actor Training Debug (Iteration 6807) ===
Q mean: -13.960581
Q std: 20.871061
Actor loss: 13.964547
Action reg: 0.003967
  l1.weight: grad_norm = 0.197148
  l1.bias: grad_norm = 0.001923
  l2.weight: grad_norm = 0.144256
Total gradient norm: 0.402316
=== Actor Training Debug (Iteration 6808) ===
Q mean: -13.000529
Q std: 20.117289
Actor loss: 13.004498
Action reg: 0.003968
  l1.weight: grad_norm = 0.163134
  l1.bias: grad_norm = 0.001525
  l2.weight: grad_norm = 0.106830
Total gradient norm: 0.300148
=== Actor Training Debug (Iteration 6809) ===
Q mean: -10.820505
Q std: 16.890518
Actor loss: 10.824453
Action reg: 0.003948
  l1.weight: grad_norm = 0.147052
  l1.bias: grad_norm = 0.003457
  l2.weight: grad_norm = 0.121929
Total gradient norm: 0.375063
=== Actor Training Debug (Iteration 6810) ===
Q mean: -15.244462
Q std: 20.833401
Actor loss: 15.248435
Action reg: 0.003973
  l1.weight: grad_norm = 0.101673
  l1.bias: grad_norm = 0.001451
  l2.weight: grad_norm = 0.075434
Total gradient norm: 0.235140
=== Actor Training Debug (Iteration 6811) ===
Q mean: -14.063369
Q std: 20.728260
Actor loss: 14.067342
Action reg: 0.003973
  l1.weight: grad_norm = 0.235905
  l1.bias: grad_norm = 0.002235
  l2.weight: grad_norm = 0.185044
Total gradient norm: 0.566826
=== Actor Training Debug (Iteration 6812) ===
Q mean: -13.372228
Q std: 20.288889
Actor loss: 13.376194
Action reg: 0.003966
  l1.weight: grad_norm = 0.096864
  l1.bias: grad_norm = 0.003843
  l2.weight: grad_norm = 0.070517
Total gradient norm: 0.258810
=== Actor Training Debug (Iteration 6813) ===
Q mean: -12.376572
Q std: 18.856754
Actor loss: 12.380547
Action reg: 0.003975
  l1.weight: grad_norm = 0.102885
  l1.bias: grad_norm = 0.001282
  l2.weight: grad_norm = 0.085384
Total gradient norm: 0.285121
=== Actor Training Debug (Iteration 6814) ===
Q mean: -12.194658
Q std: 20.880299
Actor loss: 12.198608
Action reg: 0.003950
  l1.weight: grad_norm = 0.416210
  l1.bias: grad_norm = 0.002788
  l2.weight: grad_norm = 0.317619
Total gradient norm: 0.949941
=== Actor Training Debug (Iteration 6815) ===
Q mean: -11.455481
Q std: 18.890663
Actor loss: 11.459433
Action reg: 0.003952
  l1.weight: grad_norm = 0.255065
  l1.bias: grad_norm = 0.003936
  l2.weight: grad_norm = 0.206381
Total gradient norm: 0.587650
=== Actor Training Debug (Iteration 6816) ===
Q mean: -13.572962
Q std: 19.920094
Actor loss: 13.576927
Action reg: 0.003965
  l1.weight: grad_norm = 0.194805
  l1.bias: grad_norm = 0.001008
  l2.weight: grad_norm = 0.142051
Total gradient norm: 0.444094
=== Actor Training Debug (Iteration 6817) ===
Q mean: -14.973537
Q std: 21.629799
Actor loss: 14.977509
Action reg: 0.003972
  l1.weight: grad_norm = 0.240592
  l1.bias: grad_norm = 0.001059
  l2.weight: grad_norm = 0.147700
Total gradient norm: 0.436291
=== Actor Training Debug (Iteration 6818) ===
Q mean: -13.010452
Q std: 20.226128
Actor loss: 13.014412
Action reg: 0.003960
  l1.weight: grad_norm = 0.152817
  l1.bias: grad_norm = 0.004372
  l2.weight: grad_norm = 0.133736
Total gradient norm: 0.464859
=== Actor Training Debug (Iteration 6819) ===
Q mean: -13.555658
Q std: 18.705542
Actor loss: 13.559622
Action reg: 0.003963
  l1.weight: grad_norm = 0.259557
  l1.bias: grad_norm = 0.003137
  l2.weight: grad_norm = 0.229636
Total gradient norm: 0.736693
=== Actor Training Debug (Iteration 6820) ===
Q mean: -15.485464
Q std: 21.608080
Actor loss: 15.489440
Action reg: 0.003976
  l1.weight: grad_norm = 0.145118
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.122418
Total gradient norm: 0.353315
=== Actor Training Debug (Iteration 6821) ===
Q mean: -14.994083
Q std: 21.820381
Actor loss: 14.998040
Action reg: 0.003957
  l1.weight: grad_norm = 0.192186
  l1.bias: grad_norm = 0.001057
  l2.weight: grad_norm = 0.148662
Total gradient norm: 0.384466
=== Actor Training Debug (Iteration 6822) ===
Q mean: -15.963134
Q std: 21.801140
Actor loss: 15.967098
Action reg: 0.003964
  l1.weight: grad_norm = 0.177387
  l1.bias: grad_norm = 0.003323
  l2.weight: grad_norm = 0.112441
Total gradient norm: 0.345738
=== Actor Training Debug (Iteration 6823) ===
Q mean: -11.937952
Q std: 18.878353
Actor loss: 11.941919
Action reg: 0.003967
  l1.weight: grad_norm = 0.141975
  l1.bias: grad_norm = 0.000845
  l2.weight: grad_norm = 0.116970
Total gradient norm: 0.343398
=== Actor Training Debug (Iteration 6824) ===
Q mean: -15.391439
Q std: 22.546387
Actor loss: 15.395409
Action reg: 0.003969
  l1.weight: grad_norm = 0.188290
  l1.bias: grad_norm = 0.000934
  l2.weight: grad_norm = 0.142939
Total gradient norm: 0.409729
=== Actor Training Debug (Iteration 6825) ===
Q mean: -17.043785
Q std: 23.372189
Actor loss: 17.047749
Action reg: 0.003963
  l1.weight: grad_norm = 0.241510
  l1.bias: grad_norm = 0.001163
  l2.weight: grad_norm = 0.181321
Total gradient norm: 0.491136
=== Actor Training Debug (Iteration 6826) ===
Q mean: -14.049387
Q std: 20.777203
Actor loss: 14.053361
Action reg: 0.003973
  l1.weight: grad_norm = 0.078768
  l1.bias: grad_norm = 0.001216
  l2.weight: grad_norm = 0.055867
Total gradient norm: 0.176920
=== Actor Training Debug (Iteration 6827) ===
Q mean: -14.210074
Q std: 20.880360
Actor loss: 14.214029
Action reg: 0.003954
  l1.weight: grad_norm = 0.098473
  l1.bias: grad_norm = 0.003068
  l2.weight: grad_norm = 0.071359
Total gradient norm: 0.221318
=== Actor Training Debug (Iteration 6828) ===
Q mean: -14.735490
Q std: 21.005533
Actor loss: 14.739466
Action reg: 0.003976
  l1.weight: grad_norm = 0.165778
  l1.bias: grad_norm = 0.001137
  l2.weight: grad_norm = 0.116944
Total gradient norm: 0.315042
=== Actor Training Debug (Iteration 6829) ===
Q mean: -12.615591
Q std: 19.681091
Actor loss: 12.619559
Action reg: 0.003968
  l1.weight: grad_norm = 0.175260
  l1.bias: grad_norm = 0.002601
  l2.weight: grad_norm = 0.129793
Total gradient norm: 0.397592
=== Actor Training Debug (Iteration 6830) ===
Q mean: -14.781209
Q std: 20.986961
Actor loss: 14.785197
Action reg: 0.003988
  l1.weight: grad_norm = 0.088940
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.076176
Total gradient norm: 0.250790
=== Actor Training Debug (Iteration 6831) ===
Q mean: -15.006285
Q std: 21.441349
Actor loss: 15.010256
Action reg: 0.003971
  l1.weight: grad_norm = 0.118352
  l1.bias: grad_norm = 0.002555
  l2.weight: grad_norm = 0.083376
Total gradient norm: 0.286709
=== Actor Training Debug (Iteration 6832) ===
Q mean: -15.501663
Q std: 21.748581
Actor loss: 15.505627
Action reg: 0.003963
  l1.weight: grad_norm = 0.286952
  l1.bias: grad_norm = 0.001467
  l2.weight: grad_norm = 0.204854
Total gradient norm: 0.554740
=== Actor Training Debug (Iteration 6833) ===
Q mean: -12.758029
Q std: 18.972164
Actor loss: 12.762019
Action reg: 0.003990
  l1.weight: grad_norm = 0.160921
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.093135
Total gradient norm: 0.266013
=== Actor Training Debug (Iteration 6834) ===
Q mean: -13.067564
Q std: 20.317774
Actor loss: 13.071540
Action reg: 0.003976
  l1.weight: grad_norm = 0.197247
  l1.bias: grad_norm = 0.001653
  l2.weight: grad_norm = 0.142074
Total gradient norm: 0.421581
=== Actor Training Debug (Iteration 6835) ===
Q mean: -16.647011
Q std: 22.126183
Actor loss: 16.650991
Action reg: 0.003981
  l1.weight: grad_norm = 0.188926
  l1.bias: grad_norm = 0.001178
  l2.weight: grad_norm = 0.141931
Total gradient norm: 0.418504
=== Actor Training Debug (Iteration 6836) ===
Q mean: -16.147541
Q std: 21.922123
Actor loss: 16.151524
Action reg: 0.003982
  l1.weight: grad_norm = 0.229061
  l1.bias: grad_norm = 0.000518
  l2.weight: grad_norm = 0.181432
Total gradient norm: 0.658316
=== Actor Training Debug (Iteration 6837) ===
Q mean: -12.840061
Q std: 20.193583
Actor loss: 12.844038
Action reg: 0.003977
  l1.weight: grad_norm = 0.063647
  l1.bias: grad_norm = 0.000908
  l2.weight: grad_norm = 0.055592
Total gradient norm: 0.162826
=== Actor Training Debug (Iteration 6838) ===
Q mean: -13.134207
Q std: 19.072701
Actor loss: 13.138161
Action reg: 0.003954
  l1.weight: grad_norm = 0.130454
  l1.bias: grad_norm = 0.001665
  l2.weight: grad_norm = 0.105242
Total gradient norm: 0.278930
=== Actor Training Debug (Iteration 6839) ===
Q mean: -11.971635
Q std: 19.061405
Actor loss: 11.975608
Action reg: 0.003973
  l1.weight: grad_norm = 0.076452
  l1.bias: grad_norm = 0.001668
  l2.weight: grad_norm = 0.065666
Total gradient norm: 0.178782
=== Actor Training Debug (Iteration 6840) ===
Q mean: -15.187807
Q std: 22.033564
Actor loss: 15.191775
Action reg: 0.003968
  l1.weight: grad_norm = 0.158166
  l1.bias: grad_norm = 0.001379
  l2.weight: grad_norm = 0.117031
Total gradient norm: 0.393242
=== Actor Training Debug (Iteration 6841) ===
Q mean: -14.482863
Q std: 20.376007
Actor loss: 14.486829
Action reg: 0.003965
  l1.weight: grad_norm = 0.142715
  l1.bias: grad_norm = 0.003717
  l2.weight: grad_norm = 0.114723
Total gradient norm: 0.376703
=== Actor Training Debug (Iteration 6842) ===
Q mean: -11.524901
Q std: 18.841522
Actor loss: 11.528859
Action reg: 0.003958
  l1.weight: grad_norm = 0.114863
  l1.bias: grad_norm = 0.001868
  l2.weight: grad_norm = 0.090030
Total gradient norm: 0.269481
=== Actor Training Debug (Iteration 6843) ===
Q mean: -13.328864
Q std: 20.887524
Actor loss: 13.332803
Action reg: 0.003939
  l1.weight: grad_norm = 0.297749
  l1.bias: grad_norm = 0.002249
  l2.weight: grad_norm = 0.224554
Total gradient norm: 0.642442
=== Actor Training Debug (Iteration 6844) ===
Q mean: -14.931272
Q std: 21.831333
Actor loss: 14.935247
Action reg: 0.003976
  l1.weight: grad_norm = 0.123431
  l1.bias: grad_norm = 0.001141
  l2.weight: grad_norm = 0.102074
Total gradient norm: 0.334370
=== Actor Training Debug (Iteration 6845) ===
Q mean: -13.722179
Q std: 20.255995
Actor loss: 13.726155
Action reg: 0.003976
  l1.weight: grad_norm = 0.215475
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.172107
Total gradient norm: 0.526485
=== Actor Training Debug (Iteration 6846) ===
Q mean: -14.393076
Q std: 20.765612
Actor loss: 14.397049
Action reg: 0.003973
  l1.weight: grad_norm = 0.150098
  l1.bias: grad_norm = 0.000764
  l2.weight: grad_norm = 0.134106
Total gradient norm: 0.409252
=== Actor Training Debug (Iteration 6847) ===
Q mean: -14.001657
Q std: 20.768894
Actor loss: 14.005637
Action reg: 0.003980
  l1.weight: grad_norm = 0.053207
  l1.bias: grad_norm = 0.000550
  l2.weight: grad_norm = 0.042388
Total gradient norm: 0.132994
=== Actor Training Debug (Iteration 6848) ===
Q mean: -14.628829
Q std: 21.874477
Actor loss: 14.632796
Action reg: 0.003967
  l1.weight: grad_norm = 0.215639
  l1.bias: grad_norm = 0.001806
  l2.weight: grad_norm = 0.132372
Total gradient norm: 0.342085
=== Actor Training Debug (Iteration 6849) ===
Q mean: -14.036417
Q std: 20.604780
Actor loss: 14.040398
Action reg: 0.003981
  l1.weight: grad_norm = 0.227489
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.155308
Total gradient norm: 0.436064
=== Actor Training Debug (Iteration 6850) ===
Q mean: -12.479806
Q std: 21.376125
Actor loss: 12.483762
Action reg: 0.003956
  l1.weight: grad_norm = 0.342431
  l1.bias: grad_norm = 0.001608
  l2.weight: grad_norm = 0.276292
Total gradient norm: 0.827562
=== Actor Training Debug (Iteration 6851) ===
Q mean: -16.811705
Q std: 21.936895
Actor loss: 16.815678
Action reg: 0.003973
  l1.weight: grad_norm = 0.218016
  l1.bias: grad_norm = 0.001985
  l2.weight: grad_norm = 0.163477
Total gradient norm: 0.497290
=== Actor Training Debug (Iteration 6852) ===
Q mean: -15.007282
Q std: 20.511745
Actor loss: 15.011257
Action reg: 0.003975
  l1.weight: grad_norm = 0.454211
  l1.bias: grad_norm = 0.001796
  l2.weight: grad_norm = 0.361233
Total gradient norm: 1.187729
=== Actor Training Debug (Iteration 6853) ===
Q mean: -13.628984
Q std: 20.320164
Actor loss: 13.632947
Action reg: 0.003963
  l1.weight: grad_norm = 0.173355
  l1.bias: grad_norm = 0.001556
  l2.weight: grad_norm = 0.127400
Total gradient norm: 0.351218
=== Actor Training Debug (Iteration 6854) ===
Q mean: -12.326240
Q std: 19.243862
Actor loss: 12.330221
Action reg: 0.003982
  l1.weight: grad_norm = 0.136902
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.085855
Total gradient norm: 0.242670
=== Actor Training Debug (Iteration 6855) ===
Q mean: -12.049074
Q std: 19.001360
Actor loss: 12.053056
Action reg: 0.003981
  l1.weight: grad_norm = 0.124518
  l1.bias: grad_norm = 0.002681
  l2.weight: grad_norm = 0.103722
Total gradient norm: 0.336752
=== Actor Training Debug (Iteration 6856) ===
Q mean: -13.884675
Q std: 20.357317
Actor loss: 13.888651
Action reg: 0.003976
  l1.weight: grad_norm = 0.239088
  l1.bias: grad_norm = 0.000769
  l2.weight: grad_norm = 0.179664
Total gradient norm: 0.552745
=== Actor Training Debug (Iteration 6857) ===
Q mean: -12.325916
Q std: 21.117702
Actor loss: 12.329868
Action reg: 0.003952
  l1.weight: grad_norm = 0.190682
  l1.bias: grad_norm = 0.003804
  l2.weight: grad_norm = 0.139465
Total gradient norm: 0.426380
=== Actor Training Debug (Iteration 6858) ===
Q mean: -16.237652
Q std: 21.416485
Actor loss: 16.241621
Action reg: 0.003969
  l1.weight: grad_norm = 0.236575
  l1.bias: grad_norm = 0.000989
  l2.weight: grad_norm = 0.143687
Total gradient norm: 0.382734
=== Actor Training Debug (Iteration 6859) ===
Q mean: -14.973377
Q std: 21.135550
Actor loss: 14.977349
Action reg: 0.003972
  l1.weight: grad_norm = 0.096917
  l1.bias: grad_norm = 0.001562
  l2.weight: grad_norm = 0.071642
Total gradient norm: 0.200042
=== Actor Training Debug (Iteration 6860) ===
Q mean: -13.809162
Q std: 19.538828
Actor loss: 13.813143
Action reg: 0.003981
  l1.weight: grad_norm = 0.155393
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.114999
Total gradient norm: 0.318357
=== Actor Training Debug (Iteration 6861) ===
Q mean: -14.137044
Q std: 20.773197
Actor loss: 14.141005
Action reg: 0.003961
  l1.weight: grad_norm = 0.141772
  l1.bias: grad_norm = 0.003938
  l2.weight: grad_norm = 0.103578
Total gradient norm: 0.324918
=== Actor Training Debug (Iteration 6862) ===
Q mean: -13.779062
Q std: 20.228010
Actor loss: 13.783026
Action reg: 0.003963
  l1.weight: grad_norm = 0.197325
  l1.bias: grad_norm = 0.001189
  l2.weight: grad_norm = 0.155169
Total gradient norm: 0.471501
=== Actor Training Debug (Iteration 6863) ===
Q mean: -13.643478
Q std: 19.016182
Actor loss: 13.647452
Action reg: 0.003974
  l1.weight: grad_norm = 0.078407
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.058465
Total gradient norm: 0.175055
=== Actor Training Debug (Iteration 6864) ===
Q mean: -15.211189
Q std: 20.008562
Actor loss: 15.215172
Action reg: 0.003983
  l1.weight: grad_norm = 0.104940
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.084482
Total gradient norm: 0.213572
=== Actor Training Debug (Iteration 6865) ===
Q mean: -15.765903
Q std: 22.132648
Actor loss: 15.769881
Action reg: 0.003978
  l1.weight: grad_norm = 0.204597
  l1.bias: grad_norm = 0.001516
  l2.weight: grad_norm = 0.166552
Total gradient norm: 0.518568
=== Actor Training Debug (Iteration 6866) ===
Q mean: -15.320354
Q std: 21.652248
Actor loss: 15.324327
Action reg: 0.003973
  l1.weight: grad_norm = 0.279785
  l1.bias: grad_norm = 0.001563
  l2.weight: grad_norm = 0.217669
Total gradient norm: 0.607718
=== Actor Training Debug (Iteration 6867) ===
Q mean: -15.048723
Q std: 21.584003
Actor loss: 15.052690
Action reg: 0.003966
  l1.weight: grad_norm = 0.222018
  l1.bias: grad_norm = 0.002306
  l2.weight: grad_norm = 0.169455
Total gradient norm: 0.447177
=== Actor Training Debug (Iteration 6868) ===
Q mean: -13.489229
Q std: 20.279747
Actor loss: 13.493207
Action reg: 0.003977
  l1.weight: grad_norm = 0.134676
  l1.bias: grad_norm = 0.001121
  l2.weight: grad_norm = 0.105637
Total gradient norm: 0.310726
=== Actor Training Debug (Iteration 6869) ===
Q mean: -14.609055
Q std: 20.399418
Actor loss: 14.613006
Action reg: 0.003951
  l1.weight: grad_norm = 0.286856
  l1.bias: grad_norm = 0.003519
  l2.weight: grad_norm = 0.211887
Total gradient norm: 0.612698
=== Actor Training Debug (Iteration 6870) ===
Q mean: -13.218669
Q std: 19.649376
Actor loss: 13.222631
Action reg: 0.003961
  l1.weight: grad_norm = 0.482450
  l1.bias: grad_norm = 0.002058
  l2.weight: grad_norm = 0.359421
Total gradient norm: 1.136017
=== Actor Training Debug (Iteration 6871) ===
Q mean: -14.800882
Q std: 21.397434
Actor loss: 14.804857
Action reg: 0.003975
  l1.weight: grad_norm = 0.197233
  l1.bias: grad_norm = 0.000775
  l2.weight: grad_norm = 0.176349
Total gradient norm: 0.441357
=== Actor Training Debug (Iteration 6872) ===
Q mean: -14.748593
Q std: 21.442196
Actor loss: 14.752559
Action reg: 0.003965
  l1.weight: grad_norm = 0.167541
  l1.bias: grad_norm = 0.001619
  l2.weight: grad_norm = 0.148454
Total gradient norm: 0.420356
=== Actor Training Debug (Iteration 6873) ===
Q mean: -12.632779
Q std: 19.429785
Actor loss: 12.636744
Action reg: 0.003965
  l1.weight: grad_norm = 0.220564
  l1.bias: grad_norm = 0.005293
  l2.weight: grad_norm = 0.162289
Total gradient norm: 0.475686
=== Actor Training Debug (Iteration 6874) ===
Q mean: -13.523367
Q std: 20.642347
Actor loss: 13.527338
Action reg: 0.003971
  l1.weight: grad_norm = 0.347971
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.268095
Total gradient norm: 0.766109
=== Actor Training Debug (Iteration 6875) ===
Q mean: -12.849785
Q std: 19.928202
Actor loss: 12.853747
Action reg: 0.003963
  l1.weight: grad_norm = 0.142219
  l1.bias: grad_norm = 0.003019
  l2.weight: grad_norm = 0.098375
Total gradient norm: 0.280449
=== Actor Training Debug (Iteration 6876) ===
Q mean: -15.295960
Q std: 21.447102
Actor loss: 15.299943
Action reg: 0.003983
  l1.weight: grad_norm = 0.206897
  l1.bias: grad_norm = 0.001272
  l2.weight: grad_norm = 0.147975
Total gradient norm: 0.435922
=== Actor Training Debug (Iteration 6877) ===
Q mean: -13.399192
Q std: 20.205763
Actor loss: 13.403173
Action reg: 0.003982
  l1.weight: grad_norm = 0.108136
  l1.bias: grad_norm = 0.000461
  l2.weight: grad_norm = 0.093354
Total gradient norm: 0.243778
=== Actor Training Debug (Iteration 6878) ===
Q mean: -13.698561
Q std: 20.890539
Actor loss: 13.702531
Action reg: 0.003970
  l1.weight: grad_norm = 0.151151
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.117346
Total gradient norm: 0.337020
=== Actor Training Debug (Iteration 6879) ===
Q mean: -14.276404
Q std: 21.554893
Actor loss: 14.280360
Action reg: 0.003956
  l1.weight: grad_norm = 0.227039
  l1.bias: grad_norm = 0.001061
  l2.weight: grad_norm = 0.162660
Total gradient norm: 0.467244
=== Actor Training Debug (Iteration 6880) ===
Q mean: -15.919785
Q std: 22.540989
Actor loss: 15.923760
Action reg: 0.003974
  l1.weight: grad_norm = 0.292172
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.222289
Total gradient norm: 0.662608
=== Actor Training Debug (Iteration 6881) ===
Q mean: -12.281077
Q std: 19.931313
Actor loss: 12.285055
Action reg: 0.003978
  l1.weight: grad_norm = 0.218486
  l1.bias: grad_norm = 0.000938
  l2.weight: grad_norm = 0.141787
Total gradient norm: 0.446591
=== Actor Training Debug (Iteration 6882) ===
Q mean: -12.085950
Q std: 19.103781
Actor loss: 12.089920
Action reg: 0.003970
  l1.weight: grad_norm = 0.205170
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.143909
Total gradient norm: 0.416047
=== Actor Training Debug (Iteration 6883) ===
Q mean: -11.858450
Q std: 19.001534
Actor loss: 11.862403
Action reg: 0.003953
  l1.weight: grad_norm = 0.191308
  l1.bias: grad_norm = 0.001598
  l2.weight: grad_norm = 0.169149
Total gradient norm: 0.559460
=== Actor Training Debug (Iteration 6884) ===
Q mean: -12.388372
Q std: 19.251099
Actor loss: 12.392319
Action reg: 0.003946
  l1.weight: grad_norm = 0.222713
  l1.bias: grad_norm = 0.000629
  l2.weight: grad_norm = 0.173844
Total gradient norm: 0.477614
=== Actor Training Debug (Iteration 6885) ===
Q mean: -13.614283
Q std: 20.431730
Actor loss: 13.618251
Action reg: 0.003969
  l1.weight: grad_norm = 0.215546
  l1.bias: grad_norm = 0.002506
  l2.weight: grad_norm = 0.189168
Total gradient norm: 0.511077
=== Actor Training Debug (Iteration 6886) ===
Q mean: -17.054285
Q std: 23.803715
Actor loss: 17.058256
Action reg: 0.003971
  l1.weight: grad_norm = 0.405799
  l1.bias: grad_norm = 0.001178
  l2.weight: grad_norm = 0.291619
Total gradient norm: 0.896976
=== Actor Training Debug (Iteration 6887) ===
Q mean: -12.587202
Q std: 20.865679
Actor loss: 12.591152
Action reg: 0.003950
  l1.weight: grad_norm = 0.178877
  l1.bias: grad_norm = 0.001922
  l2.weight: grad_norm = 0.132417
Total gradient norm: 0.369784
=== Actor Training Debug (Iteration 6888) ===
Q mean: -12.395386
Q std: 21.226927
Actor loss: 12.399365
Action reg: 0.003979
  l1.weight: grad_norm = 0.265684
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.213479
Total gradient norm: 0.788794
=== Actor Training Debug (Iteration 6889) ===
Q mean: -13.452471
Q std: 19.436745
Actor loss: 13.456444
Action reg: 0.003973
  l1.weight: grad_norm = 0.239975
  l1.bias: grad_norm = 0.002678
  l2.weight: grad_norm = 0.156434
Total gradient norm: 0.425486
=== Actor Training Debug (Iteration 6890) ===
Q mean: -14.230600
Q std: 21.697182
Actor loss: 14.234560
Action reg: 0.003960
  l1.weight: grad_norm = 0.220064
  l1.bias: grad_norm = 0.002430
  l2.weight: grad_norm = 0.159764
Total gradient norm: 0.444640
=== Actor Training Debug (Iteration 6891) ===
Q mean: -14.065319
Q std: 20.014040
Actor loss: 14.069287
Action reg: 0.003968
  l1.weight: grad_norm = 0.171740
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.123990
Total gradient norm: 0.363069
=== Actor Training Debug (Iteration 6892) ===
Q mean: -14.001733
Q std: 20.868361
Actor loss: 14.005701
Action reg: 0.003968
  l1.weight: grad_norm = 0.120938
  l1.bias: grad_norm = 0.001818
  l2.weight: grad_norm = 0.103942
Total gradient norm: 0.266138
=== Actor Training Debug (Iteration 6893) ===
Q mean: -11.953503
Q std: 20.205673
Actor loss: 11.957458
Action reg: 0.003956
  l1.weight: grad_norm = 0.224584
  l1.bias: grad_norm = 0.001996
  l2.weight: grad_norm = 0.158196
Total gradient norm: 0.419102
=== Actor Training Debug (Iteration 6894) ===
Q mean: -11.517794
Q std: 19.715694
Actor loss: 11.521760
Action reg: 0.003967
  l1.weight: grad_norm = 0.139623
  l1.bias: grad_norm = 0.002142
  l2.weight: grad_norm = 0.100288
Total gradient norm: 0.277469
=== Actor Training Debug (Iteration 6895) ===
Q mean: -12.289164
Q std: 20.421917
Actor loss: 12.293137
Action reg: 0.003973
  l1.weight: grad_norm = 0.173750
  l1.bias: grad_norm = 0.001209
  l2.weight: grad_norm = 0.129701
Total gradient norm: 0.347030
=== Actor Training Debug (Iteration 6896) ===
Q mean: -13.822920
Q std: 20.139326
Actor loss: 13.826880
Action reg: 0.003959
  l1.weight: grad_norm = 0.224868
  l1.bias: grad_norm = 0.001911
  l2.weight: grad_norm = 0.168018
Total gradient norm: 0.472178
=== Actor Training Debug (Iteration 6897) ===
Q mean: -14.439446
Q std: 20.679554
Actor loss: 14.443424
Action reg: 0.003978
  l1.weight: grad_norm = 0.215987
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.163576
Total gradient norm: 0.429556
=== Actor Training Debug (Iteration 6898) ===
Q mean: -14.293882
Q std: 19.764956
Actor loss: 14.297856
Action reg: 0.003974
  l1.weight: grad_norm = 0.307332
  l1.bias: grad_norm = 0.001012
  l2.weight: grad_norm = 0.264534
Total gradient norm: 0.725270
=== Actor Training Debug (Iteration 6899) ===
Q mean: -14.026294
Q std: 20.819006
Actor loss: 14.030261
Action reg: 0.003968
  l1.weight: grad_norm = 0.065432
  l1.bias: grad_norm = 0.000754
  l2.weight: grad_norm = 0.059981
Total gradient norm: 0.165217
=== Actor Training Debug (Iteration 6900) ===
Q mean: -14.301401
Q std: 21.302568
Actor loss: 14.305362
Action reg: 0.003961
  l1.weight: grad_norm = 0.174557
  l1.bias: grad_norm = 0.004463
  l2.weight: grad_norm = 0.141264
Total gradient norm: 0.481930
=== Actor Training Debug (Iteration 6901) ===
Q mean: -12.921943
Q std: 19.473261
Actor loss: 12.925905
Action reg: 0.003962
  l1.weight: grad_norm = 0.141271
  l1.bias: grad_norm = 0.000693
  l2.weight: grad_norm = 0.122921
Total gradient norm: 0.310111
=== Actor Training Debug (Iteration 6902) ===
Q mean: -14.356941
Q std: 21.460133
Actor loss: 14.360904
Action reg: 0.003962
  l1.weight: grad_norm = 0.227173
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.178417
Total gradient norm: 0.487874
=== Actor Training Debug (Iteration 6903) ===
Q mean: -15.513186
Q std: 21.477213
Actor loss: 15.517146
Action reg: 0.003960
  l1.weight: grad_norm = 0.207001
  l1.bias: grad_norm = 0.001489
  l2.weight: grad_norm = 0.165458
Total gradient norm: 0.515359
=== Actor Training Debug (Iteration 6904) ===
Q mean: -13.897745
Q std: 21.163548
Actor loss: 13.901731
Action reg: 0.003985
  l1.weight: grad_norm = 0.106808
  l1.bias: grad_norm = 0.001282
  l2.weight: grad_norm = 0.079572
Total gradient norm: 0.215996
=== Actor Training Debug (Iteration 6905) ===
Q mean: -11.581936
Q std: 18.035929
Actor loss: 11.585909
Action reg: 0.003973
  l1.weight: grad_norm = 0.209073
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.175241
Total gradient norm: 0.466693
=== Actor Training Debug (Iteration 6906) ===
Q mean: -16.744080
Q std: 23.300739
Actor loss: 16.748056
Action reg: 0.003977
  l1.weight: grad_norm = 0.159669
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.102584
Total gradient norm: 0.279945
=== Actor Training Debug (Iteration 6907) ===
Q mean: -16.623306
Q std: 23.028971
Actor loss: 16.627268
Action reg: 0.003961
  l1.weight: grad_norm = 0.138872
  l1.bias: grad_norm = 0.000609
  l2.weight: grad_norm = 0.103577
Total gradient norm: 0.301215
=== Actor Training Debug (Iteration 6908) ===
Q mean: -13.067310
Q std: 19.910393
Actor loss: 13.071265
Action reg: 0.003955
  l1.weight: grad_norm = 0.180814
  l1.bias: grad_norm = 0.001728
  l2.weight: grad_norm = 0.129490
Total gradient norm: 0.390009
=== Actor Training Debug (Iteration 6909) ===
Q mean: -14.036465
Q std: 21.489136
Actor loss: 14.040444
Action reg: 0.003980
  l1.weight: grad_norm = 0.086683
  l1.bias: grad_norm = 0.001063
  l2.weight: grad_norm = 0.058267
Total gradient norm: 0.188386
=== Actor Training Debug (Iteration 6910) ===
Q mean: -13.656202
Q std: 20.694983
Actor loss: 13.660158
Action reg: 0.003956
  l1.weight: grad_norm = 0.146060
  l1.bias: grad_norm = 0.001037
  l2.weight: grad_norm = 0.108978
Total gradient norm: 0.309813
=== Actor Training Debug (Iteration 6911) ===
Q mean: -11.873861
Q std: 20.365604
Actor loss: 11.877826
Action reg: 0.003964
  l1.weight: grad_norm = 0.093316
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.087489
Total gradient norm: 0.245432
=== Actor Training Debug (Iteration 6912) ===
Q mean: -13.815084
Q std: 21.119848
Actor loss: 13.819039
Action reg: 0.003955
  l1.weight: grad_norm = 0.302952
  l1.bias: grad_norm = 0.002290
  l2.weight: grad_norm = 0.242990
Total gradient norm: 0.728077
=== Actor Training Debug (Iteration 6913) ===
Q mean: -15.699707
Q std: 21.834286
Actor loss: 15.703682
Action reg: 0.003975
  l1.weight: grad_norm = 0.116916
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.093781
Total gradient norm: 0.252359
=== Actor Training Debug (Iteration 6914) ===
Q mean: -13.270359
Q std: 20.611393
Actor loss: 13.274333
Action reg: 0.003974
  l1.weight: grad_norm = 0.137960
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.117718
Total gradient norm: 0.417382
=== Actor Training Debug (Iteration 6915) ===
Q mean: -13.382828
Q std: 19.800219
Actor loss: 13.386783
Action reg: 0.003955
  l1.weight: grad_norm = 0.106870
  l1.bias: grad_norm = 0.002854
  l2.weight: grad_norm = 0.094811
Total gradient norm: 0.271153
=== Actor Training Debug (Iteration 6916) ===
Q mean: -14.405249
Q std: 21.861687
Actor loss: 14.409224
Action reg: 0.003975
  l1.weight: grad_norm = 0.232511
  l1.bias: grad_norm = 0.001083
  l2.weight: grad_norm = 0.202655
Total gradient norm: 0.611308
=== Actor Training Debug (Iteration 6917) ===
Q mean: -13.613678
Q std: 20.983974
Actor loss: 13.617626
Action reg: 0.003948
  l1.weight: grad_norm = 0.075331
  l1.bias: grad_norm = 0.001263
  l2.weight: grad_norm = 0.054001
Total gradient norm: 0.166914
=== Actor Training Debug (Iteration 6918) ===
Q mean: -10.894608
Q std: 18.439331
Actor loss: 10.898571
Action reg: 0.003963
  l1.weight: grad_norm = 0.127417
  l1.bias: grad_norm = 0.003181
  l2.weight: grad_norm = 0.100402
Total gradient norm: 0.344520
=== Actor Training Debug (Iteration 6919) ===
Q mean: -10.891120
Q std: 18.116838
Actor loss: 10.895087
Action reg: 0.003967
  l1.weight: grad_norm = 0.086467
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.067568
Total gradient norm: 0.195089
=== Actor Training Debug (Iteration 6920) ===
Q mean: -14.084240
Q std: 20.499466
Actor loss: 14.088199
Action reg: 0.003959
  l1.weight: grad_norm = 0.126277
  l1.bias: grad_norm = 0.001960
  l2.weight: grad_norm = 0.098071
Total gradient norm: 0.296685
=== Actor Training Debug (Iteration 6921) ===
Q mean: -12.898922
Q std: 18.986301
Actor loss: 12.902901
Action reg: 0.003978
  l1.weight: grad_norm = 0.198345
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.186173
Total gradient norm: 0.675639
=== Actor Training Debug (Iteration 6922) ===
Q mean: -13.357855
Q std: 20.906950
Actor loss: 13.361809
Action reg: 0.003954
  l1.weight: grad_norm = 0.224848
  l1.bias: grad_norm = 0.002251
  l2.weight: grad_norm = 0.170257
Total gradient norm: 0.492423
=== Actor Training Debug (Iteration 6923) ===
Q mean: -13.307270
Q std: 19.290977
Actor loss: 13.311249
Action reg: 0.003979
  l1.weight: grad_norm = 0.242209
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.169786
Total gradient norm: 0.457293
=== Actor Training Debug (Iteration 6924) ===
Q mean: -12.792553
Q std: 19.500776
Actor loss: 12.796505
Action reg: 0.003952
  l1.weight: grad_norm = 0.159467
  l1.bias: grad_norm = 0.005800
  l2.weight: grad_norm = 0.120464
Total gradient norm: 0.395530
=== Actor Training Debug (Iteration 6925) ===
Q mean: -11.754910
Q std: 18.923492
Actor loss: 11.758882
Action reg: 0.003972
  l1.weight: grad_norm = 0.138646
  l1.bias: grad_norm = 0.001624
  l2.weight: grad_norm = 0.112407
Total gradient norm: 0.323594
=== Actor Training Debug (Iteration 6926) ===
Q mean: -14.504269
Q std: 20.584002
Actor loss: 14.508247
Action reg: 0.003979
  l1.weight: grad_norm = 0.143213
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.112064
Total gradient norm: 0.317412
=== Actor Training Debug (Iteration 6927) ===
Q mean: -13.966130
Q std: 19.417717
Actor loss: 13.970105
Action reg: 0.003975
  l1.weight: grad_norm = 0.208998
  l1.bias: grad_norm = 0.001339
  l2.weight: grad_norm = 0.182568
Total gradient norm: 0.516493
=== Actor Training Debug (Iteration 6928) ===
Q mean: -15.622368
Q std: 21.883121
Actor loss: 15.626333
Action reg: 0.003965
  l1.weight: grad_norm = 0.260405
  l1.bias: grad_norm = 0.001894
  l2.weight: grad_norm = 0.185706
Total gradient norm: 0.502664
=== Actor Training Debug (Iteration 6929) ===
Q mean: -16.183084
Q std: 22.065807
Actor loss: 16.187061
Action reg: 0.003976
  l1.weight: grad_norm = 0.162782
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.116897
Total gradient norm: 0.347679
=== Actor Training Debug (Iteration 6930) ===
Q mean: -14.363276
Q std: 20.030293
Actor loss: 14.367242
Action reg: 0.003965
  l1.weight: grad_norm = 0.258506
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.202183
Total gradient norm: 0.484021
=== Actor Training Debug (Iteration 6931) ===
Q mean: -15.490278
Q std: 21.710484
Actor loss: 15.494244
Action reg: 0.003965
  l1.weight: grad_norm = 0.285491
  l1.bias: grad_norm = 0.002001
  l2.weight: grad_norm = 0.233396
Total gradient norm: 0.905769
=== Actor Training Debug (Iteration 6932) ===
Q mean: -15.581417
Q std: 21.585592
Actor loss: 15.585399
Action reg: 0.003982
  l1.weight: grad_norm = 0.044633
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.038628
Total gradient norm: 0.114708
=== Actor Training Debug (Iteration 6933) ===
Q mean: -12.460380
Q std: 19.185549
Actor loss: 12.464332
Action reg: 0.003952
  l1.weight: grad_norm = 0.201860
  l1.bias: grad_norm = 0.001865
  l2.weight: grad_norm = 0.153644
Total gradient norm: 0.490600
=== Actor Training Debug (Iteration 6934) ===
Q mean: -14.281040
Q std: 20.002754
Actor loss: 14.285027
Action reg: 0.003986
  l1.weight: grad_norm = 0.108000
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.087630
Total gradient norm: 0.234369
=== Actor Training Debug (Iteration 6935) ===
Q mean: -13.463142
Q std: 20.319256
Actor loss: 13.467117
Action reg: 0.003975
  l1.weight: grad_norm = 0.154012
  l1.bias: grad_norm = 0.001317
  l2.weight: grad_norm = 0.107852
Total gradient norm: 0.346446
=== Actor Training Debug (Iteration 6936) ===
Q mean: -14.006508
Q std: 20.026102
Actor loss: 14.010477
Action reg: 0.003969
  l1.weight: grad_norm = 0.198294
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.147573
Total gradient norm: 0.457364
=== Actor Training Debug (Iteration 6937) ===
Q mean: -15.543410
Q std: 22.064930
Actor loss: 15.547368
Action reg: 0.003957
  l1.weight: grad_norm = 0.167162
  l1.bias: grad_norm = 0.000993
  l2.weight: grad_norm = 0.135302
Total gradient norm: 0.384440
=== Actor Training Debug (Iteration 6938) ===
Q mean: -14.567296
Q std: 22.075195
Actor loss: 14.571268
Action reg: 0.003972
  l1.weight: grad_norm = 0.114382
  l1.bias: grad_norm = 0.001243
  l2.weight: grad_norm = 0.092126
Total gradient norm: 0.298847
=== Actor Training Debug (Iteration 6939) ===
Q mean: -11.574158
Q std: 19.277515
Actor loss: 11.578111
Action reg: 0.003953
  l1.weight: grad_norm = 0.253231
  l1.bias: grad_norm = 0.002513
  l2.weight: grad_norm = 0.202501
Total gradient norm: 0.545200
=== Actor Training Debug (Iteration 6940) ===
Q mean: -13.700791
Q std: 20.039324
Actor loss: 13.704761
Action reg: 0.003969
  l1.weight: grad_norm = 0.117556
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.079350
Total gradient norm: 0.232261
=== Actor Training Debug (Iteration 6941) ===
Q mean: -14.349556
Q std: 20.972963
Actor loss: 14.353509
Action reg: 0.003953
  l1.weight: grad_norm = 0.224326
  l1.bias: grad_norm = 0.001820
  l2.weight: grad_norm = 0.176990
Total gradient norm: 0.546001
=== Actor Training Debug (Iteration 6942) ===
Q mean: -13.577168
Q std: 20.542677
Actor loss: 13.581133
Action reg: 0.003966
  l1.weight: grad_norm = 0.288495
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.230637
Total gradient norm: 0.685950
=== Actor Training Debug (Iteration 6943) ===
Q mean: -14.009360
Q std: 20.818630
Actor loss: 14.013324
Action reg: 0.003963
  l1.weight: grad_norm = 0.255823
  l1.bias: grad_norm = 0.001744
  l2.weight: grad_norm = 0.172938
Total gradient norm: 0.500894
=== Actor Training Debug (Iteration 6944) ===
Q mean: -13.899164
Q std: 20.786777
Actor loss: 13.903143
Action reg: 0.003979
  l1.weight: grad_norm = 0.213092
  l1.bias: grad_norm = 0.000972
  l2.weight: grad_norm = 0.172992
Total gradient norm: 0.526948
=== Actor Training Debug (Iteration 6945) ===
Q mean: -12.982631
Q std: 20.902863
Actor loss: 12.986606
Action reg: 0.003975
  l1.weight: grad_norm = 0.123043
  l1.bias: grad_norm = 0.001030
  l2.weight: grad_norm = 0.081788
Total gradient norm: 0.239412
=== Actor Training Debug (Iteration 6946) ===
Q mean: -13.937201
Q std: 19.371561
Actor loss: 13.941174
Action reg: 0.003972
  l1.weight: grad_norm = 0.271153
  l1.bias: grad_norm = 0.000939
  l2.weight: grad_norm = 0.204469
Total gradient norm: 0.686737
=== Actor Training Debug (Iteration 6947) ===
Q mean: -11.787162
Q std: 18.682192
Actor loss: 11.791111
Action reg: 0.003949
  l1.weight: grad_norm = 0.212562
  l1.bias: grad_norm = 0.001433
  l2.weight: grad_norm = 0.167051
Total gradient norm: 0.456061
=== Actor Training Debug (Iteration 6948) ===
Q mean: -13.272662
Q std: 21.345444
Actor loss: 13.276625
Action reg: 0.003963
  l1.weight: grad_norm = 0.258560
  l1.bias: grad_norm = 0.002160
  l2.weight: grad_norm = 0.201019
Total gradient norm: 0.557997
=== Actor Training Debug (Iteration 6949) ===
Q mean: -13.041383
Q std: 20.043333
Actor loss: 13.045339
Action reg: 0.003956
  l1.weight: grad_norm = 0.240644
  l1.bias: grad_norm = 0.000814
  l2.weight: grad_norm = 0.189351
Total gradient norm: 0.536103
=== Actor Training Debug (Iteration 6950) ===
Q mean: -11.890556
Q std: 17.753128
Actor loss: 11.894510
Action reg: 0.003954
  l1.weight: grad_norm = 0.337104
  l1.bias: grad_norm = 0.001730
  l2.weight: grad_norm = 0.281500
Total gradient norm: 0.899981
=== Actor Training Debug (Iteration 6951) ===
Q mean: -13.040657
Q std: 18.935684
Actor loss: 13.044619
Action reg: 0.003961
  l1.weight: grad_norm = 0.257455
  l1.bias: grad_norm = 0.001155
  l2.weight: grad_norm = 0.186116
Total gradient norm: 0.596455
=== Actor Training Debug (Iteration 6952) ===
Q mean: -12.893642
Q std: 19.578001
Actor loss: 12.897593
Action reg: 0.003950
  l1.weight: grad_norm = 0.519087
  l1.bias: grad_norm = 0.001321
  l2.weight: grad_norm = 0.460613
Total gradient norm: 1.220647
=== Actor Training Debug (Iteration 6953) ===
Q mean: -14.937977
Q std: 22.815140
Actor loss: 14.941961
Action reg: 0.003985
  l1.weight: grad_norm = 0.128357
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.108225
Total gradient norm: 0.311620
=== Actor Training Debug (Iteration 6954) ===
Q mean: -13.769995
Q std: 20.622364
Actor loss: 13.773954
Action reg: 0.003959
  l1.weight: grad_norm = 0.191518
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.140259
Total gradient norm: 0.371111
=== Actor Training Debug (Iteration 6955) ===
Q mean: -13.932323
Q std: 20.152626
Actor loss: 13.936302
Action reg: 0.003979
  l1.weight: grad_norm = 0.171600
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.128334
Total gradient norm: 0.358552
=== Actor Training Debug (Iteration 6956) ===
Q mean: -14.951150
Q std: 21.002968
Actor loss: 14.955128
Action reg: 0.003978
  l1.weight: grad_norm = 0.105876
  l1.bias: grad_norm = 0.002974
  l2.weight: grad_norm = 0.091781
Total gradient norm: 0.275465
=== Actor Training Debug (Iteration 6957) ===
Q mean: -16.549885
Q std: 22.761284
Actor loss: 16.553867
Action reg: 0.003982
  l1.weight: grad_norm = 0.135399
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.102798
Total gradient norm: 0.286959
=== Actor Training Debug (Iteration 6958) ===
Q mean: -14.940585
Q std: 21.261129
Actor loss: 14.944564
Action reg: 0.003978
  l1.weight: grad_norm = 0.140255
  l1.bias: grad_norm = 0.002048
  l2.weight: grad_norm = 0.103000
Total gradient norm: 0.302661
=== Actor Training Debug (Iteration 6959) ===
Q mean: -13.751837
Q std: 21.403299
Actor loss: 13.755816
Action reg: 0.003980
  l1.weight: grad_norm = 0.113160
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.090599
Total gradient norm: 0.275927
=== Actor Training Debug (Iteration 6960) ===
Q mean: -11.289415
Q std: 18.306383
Actor loss: 11.293378
Action reg: 0.003962
  l1.weight: grad_norm = 0.123246
  l1.bias: grad_norm = 0.002701
  l2.weight: grad_norm = 0.098553
Total gradient norm: 0.292385
=== Actor Training Debug (Iteration 6961) ===
Q mean: -12.800094
Q std: 19.453032
Actor loss: 12.804051
Action reg: 0.003958
  l1.weight: grad_norm = 0.230427
  l1.bias: grad_norm = 0.002606
  l2.weight: grad_norm = 0.176132
Total gradient norm: 0.500248
=== Actor Training Debug (Iteration 6962) ===
Q mean: -12.833165
Q std: 20.748411
Actor loss: 12.837147
Action reg: 0.003981
  l1.weight: grad_norm = 0.098590
  l1.bias: grad_norm = 0.002387
  l2.weight: grad_norm = 0.068389
Total gradient norm: 0.194385
=== Actor Training Debug (Iteration 6963) ===
Q mean: -13.003830
Q std: 18.981279
Actor loss: 13.007785
Action reg: 0.003955
  l1.weight: grad_norm = 0.131775
  l1.bias: grad_norm = 0.001410
  l2.weight: grad_norm = 0.101059
Total gradient norm: 0.296703
=== Actor Training Debug (Iteration 6964) ===
Q mean: -14.580860
Q std: 20.131943
Actor loss: 14.584802
Action reg: 0.003942
  l1.weight: grad_norm = 0.362115
  l1.bias: grad_norm = 0.000510
  l2.weight: grad_norm = 0.266977
Total gradient norm: 0.789817
=== Actor Training Debug (Iteration 6965) ===
Q mean: -15.770985
Q std: 22.347366
Actor loss: 15.774944
Action reg: 0.003959
  l1.weight: grad_norm = 0.299894
  l1.bias: grad_norm = 0.000562
  l2.weight: grad_norm = 0.278397
Total gradient norm: 0.846276
=== Actor Training Debug (Iteration 6966) ===
Q mean: -12.431643
Q std: 17.775043
Actor loss: 12.435615
Action reg: 0.003972
  l1.weight: grad_norm = 0.210618
  l1.bias: grad_norm = 0.000778
  l2.weight: grad_norm = 0.156214
Total gradient norm: 0.442917
=== Actor Training Debug (Iteration 6967) ===
Q mean: -13.424023
Q std: 19.672211
Actor loss: 13.427988
Action reg: 0.003965
  l1.weight: grad_norm = 0.500440
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.348381
Total gradient norm: 0.984439
=== Actor Training Debug (Iteration 6968) ===
Q mean: -14.279593
Q std: 20.991213
Actor loss: 14.283573
Action reg: 0.003981
  l1.weight: grad_norm = 0.129353
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.087711
Total gradient norm: 0.255046
=== Actor Training Debug (Iteration 6969) ===
Q mean: -14.817911
Q std: 21.250393
Actor loss: 14.821899
Action reg: 0.003988
  l1.weight: grad_norm = 0.166598
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.115505
Total gradient norm: 0.312110
=== Actor Training Debug (Iteration 6970) ===
Q mean: -12.666151
Q std: 20.517612
Actor loss: 12.670096
Action reg: 0.003945
  l1.weight: grad_norm = 0.169239
  l1.bias: grad_norm = 0.002363
  l2.weight: grad_norm = 0.131156
Total gradient norm: 0.366771
=== Actor Training Debug (Iteration 6971) ===
Q mean: -14.788752
Q std: 21.476011
Actor loss: 14.792711
Action reg: 0.003960
  l1.weight: grad_norm = 0.199586
  l1.bias: grad_norm = 0.001917
  l2.weight: grad_norm = 0.160789
Total gradient norm: 0.506350
=== Actor Training Debug (Iteration 6972) ===
Q mean: -13.361773
Q std: 21.350559
Actor loss: 13.365735
Action reg: 0.003963
  l1.weight: grad_norm = 0.135940
  l1.bias: grad_norm = 0.000607
  l2.weight: grad_norm = 0.097723
Total gradient norm: 0.274370
=== Actor Training Debug (Iteration 6973) ===
Q mean: -14.920126
Q std: 20.643656
Actor loss: 14.924089
Action reg: 0.003964
  l1.weight: grad_norm = 0.133016
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.099713
Total gradient norm: 0.324888
=== Actor Training Debug (Iteration 6974) ===
Q mean: -13.323874
Q std: 20.885229
Actor loss: 13.327845
Action reg: 0.003971
  l1.weight: grad_norm = 0.223949
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.199918
Total gradient norm: 0.635860
=== Actor Training Debug (Iteration 6975) ===
Q mean: -13.600302
Q std: 20.546741
Actor loss: 13.604264
Action reg: 0.003962
  l1.weight: grad_norm = 0.183140
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.156697
Total gradient norm: 0.537244
=== Actor Training Debug (Iteration 6976) ===
Q mean: -12.905733
Q std: 20.722219
Actor loss: 12.909709
Action reg: 0.003976
  l1.weight: grad_norm = 0.179413
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.127156
Total gradient norm: 0.419428
=== Actor Training Debug (Iteration 6977) ===
Q mean: -14.275642
Q std: 20.843887
Actor loss: 14.279620
Action reg: 0.003978
  l1.weight: grad_norm = 0.166678
  l1.bias: grad_norm = 0.000967
  l2.weight: grad_norm = 0.107437
Total gradient norm: 0.336809
=== Actor Training Debug (Iteration 6978) ===
Q mean: -13.298756
Q std: 18.830877
Actor loss: 13.302723
Action reg: 0.003967
  l1.weight: grad_norm = 0.139837
  l1.bias: grad_norm = 0.001607
  l2.weight: grad_norm = 0.126365
Total gradient norm: 0.391821
=== Actor Training Debug (Iteration 6979) ===
Q mean: -11.823762
Q std: 19.735842
Actor loss: 11.827727
Action reg: 0.003965
  l1.weight: grad_norm = 0.265085
  l1.bias: grad_norm = 0.000588
  l2.weight: grad_norm = 0.212160
Total gradient norm: 0.579364
=== Actor Training Debug (Iteration 6980) ===
Q mean: -14.009726
Q std: 21.097303
Actor loss: 14.013702
Action reg: 0.003977
  l1.weight: grad_norm = 0.111480
  l1.bias: grad_norm = 0.001013
  l2.weight: grad_norm = 0.086876
Total gradient norm: 0.252588
=== Actor Training Debug (Iteration 6981) ===
Q mean: -12.415366
Q std: 20.948641
Actor loss: 12.419342
Action reg: 0.003976
  l1.weight: grad_norm = 0.195577
  l1.bias: grad_norm = 0.001750
  l2.weight: grad_norm = 0.166063
Total gradient norm: 0.412742
=== Actor Training Debug (Iteration 6982) ===
Q mean: -11.544912
Q std: 19.409275
Actor loss: 11.548888
Action reg: 0.003975
  l1.weight: grad_norm = 0.272447
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.178492
Total gradient norm: 0.502362
=== Actor Training Debug (Iteration 6983) ===
Q mean: -15.886587
Q std: 21.685837
Actor loss: 15.890543
Action reg: 0.003955
  l1.weight: grad_norm = 0.234342
  l1.bias: grad_norm = 0.002694
  l2.weight: grad_norm = 0.153726
Total gradient norm: 0.431861
=== Actor Training Debug (Iteration 6984) ===
Q mean: -16.134853
Q std: 22.095997
Actor loss: 16.138838
Action reg: 0.003985
  l1.weight: grad_norm = 0.136544
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.118555
Total gradient norm: 0.381958
=== Actor Training Debug (Iteration 6985) ===
Q mean: -11.306127
Q std: 18.234777
Actor loss: 11.310097
Action reg: 0.003970
  l1.weight: grad_norm = 0.222959
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.157705
Total gradient norm: 0.436083
=== Actor Training Debug (Iteration 6986) ===
Q mean: -12.316401
Q std: 20.156326
Actor loss: 12.320363
Action reg: 0.003963
  l1.weight: grad_norm = 0.132673
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.099427
Total gradient norm: 0.271119
=== Actor Training Debug (Iteration 6987) ===
Q mean: -14.107130
Q std: 20.487545
Actor loss: 14.111106
Action reg: 0.003976
  l1.weight: grad_norm = 0.235764
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.188484
Total gradient norm: 0.580478
=== Actor Training Debug (Iteration 6988) ===
Q mean: -16.175903
Q std: 22.678648
Actor loss: 16.179869
Action reg: 0.003966
  l1.weight: grad_norm = 0.200222
  l1.bias: grad_norm = 0.000900
  l2.weight: grad_norm = 0.125956
Total gradient norm: 0.362365
=== Actor Training Debug (Iteration 6989) ===
Q mean: -16.813591
Q std: 22.013979
Actor loss: 16.817566
Action reg: 0.003975
  l1.weight: grad_norm = 0.239629
  l1.bias: grad_norm = 0.002225
  l2.weight: grad_norm = 0.146156
Total gradient norm: 0.453065
=== Actor Training Debug (Iteration 6990) ===
Q mean: -16.279896
Q std: 22.449337
Actor loss: 16.283871
Action reg: 0.003975
  l1.weight: grad_norm = 0.163280
  l1.bias: grad_norm = 0.000667
  l2.weight: grad_norm = 0.110805
Total gradient norm: 0.321209
=== Actor Training Debug (Iteration 6991) ===
Q mean: -15.113120
Q std: 20.820601
Actor loss: 15.117085
Action reg: 0.003965
  l1.weight: grad_norm = 0.096882
  l1.bias: grad_norm = 0.000661
  l2.weight: grad_norm = 0.076528
Total gradient norm: 0.216961
=== Actor Training Debug (Iteration 6992) ===
Q mean: -12.317307
Q std: 19.658157
Actor loss: 12.321278
Action reg: 0.003971
  l1.weight: grad_norm = 0.608274
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.451805
Total gradient norm: 1.190237
=== Actor Training Debug (Iteration 6993) ===
Q mean: -15.108633
Q std: 21.568470
Actor loss: 15.112617
Action reg: 0.003984
  l1.weight: grad_norm = 0.146163
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.118606
Total gradient norm: 0.317336
=== Actor Training Debug (Iteration 6994) ===
Q mean: -14.654623
Q std: 22.609341
Actor loss: 14.658573
Action reg: 0.003950
  l1.weight: grad_norm = 0.250405
  l1.bias: grad_norm = 0.000414
  l2.weight: grad_norm = 0.191505
Total gradient norm: 0.577094
=== Actor Training Debug (Iteration 6995) ===
Q mean: -13.313901
Q std: 19.513210
Actor loss: 13.317871
Action reg: 0.003970
  l1.weight: grad_norm = 0.219002
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.149180
Total gradient norm: 0.452716
=== Actor Training Debug (Iteration 6996) ===
Q mean: -13.518279
Q std: 20.648180
Actor loss: 13.522255
Action reg: 0.003976
  l1.weight: grad_norm = 0.163008
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.097282
Total gradient norm: 0.279828
=== Actor Training Debug (Iteration 6997) ===
Q mean: -12.650532
Q std: 20.218815
Actor loss: 12.654506
Action reg: 0.003974
  l1.weight: grad_norm = 0.261290
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.238066
Total gradient norm: 0.679489
=== Actor Training Debug (Iteration 6998) ===
Q mean: -13.084667
Q std: 20.157717
Actor loss: 13.088628
Action reg: 0.003961
  l1.weight: grad_norm = 0.233370
  l1.bias: grad_norm = 0.004912
  l2.weight: grad_norm = 0.184299
Total gradient norm: 0.614223
=== Actor Training Debug (Iteration 6999) ===
Q mean: -14.988261
Q std: 21.971161
Actor loss: 14.992226
Action reg: 0.003964
  l1.weight: grad_norm = 0.078343
  l1.bias: grad_norm = 0.001137
  l2.weight: grad_norm = 0.064059
Total gradient norm: 0.221435
=== Actor Training Debug (Iteration 7000) ===
Q mean: -11.500067
Q std: 17.290874
Actor loss: 11.504038
Action reg: 0.003971
  l1.weight: grad_norm = 0.257750
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.210091
Total gradient norm: 0.622002
Step 12000: Critic Loss: 1.8094, Actor Loss: 11.5040, Q Value: -11.5001
  Average reward: -322.175 | Average length: 100.0
Evaluation at episode 120: -322.175
=== Actor Training Debug (Iteration 7001) ===
Q mean: -15.355705
Q std: 22.985556
Actor loss: 15.359666
Action reg: 0.003960
  l1.weight: grad_norm = 0.152566
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.111287
Total gradient norm: 0.319968
=== Actor Training Debug (Iteration 7002) ===
Q mean: -14.028284
Q std: 20.748795
Actor loss: 14.032250
Action reg: 0.003966
  l1.weight: grad_norm = 0.233718
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.179743
Total gradient norm: 0.520791
=== Actor Training Debug (Iteration 7003) ===
Q mean: -15.338520
Q std: 21.304134
Actor loss: 15.342454
Action reg: 0.003934
  l1.weight: grad_norm = 0.255605
  l1.bias: grad_norm = 0.000698
  l2.weight: grad_norm = 0.171102
Total gradient norm: 0.458696
=== Actor Training Debug (Iteration 7004) ===
Q mean: -12.214703
Q std: 18.558167
Actor loss: 12.218679
Action reg: 0.003977
  l1.weight: grad_norm = 0.112694
  l1.bias: grad_norm = 0.000956
  l2.weight: grad_norm = 0.089407
Total gradient norm: 0.297347
=== Actor Training Debug (Iteration 7005) ===
Q mean: -14.303791
Q std: 22.017912
Actor loss: 14.307752
Action reg: 0.003960
  l1.weight: grad_norm = 0.206863
  l1.bias: grad_norm = 0.000999
  l2.weight: grad_norm = 0.143868
Total gradient norm: 0.403647
=== Actor Training Debug (Iteration 7006) ===
Q mean: -14.119090
Q std: 20.710117
Actor loss: 14.123079
Action reg: 0.003989
  l1.weight: grad_norm = 0.120837
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.088365
Total gradient norm: 0.249809
=== Actor Training Debug (Iteration 7007) ===
Q mean: -14.704229
Q std: 22.211510
Actor loss: 14.708181
Action reg: 0.003952
  l1.weight: grad_norm = 0.140532
  l1.bias: grad_norm = 0.000567
  l2.weight: grad_norm = 0.120892
Total gradient norm: 0.316320
=== Actor Training Debug (Iteration 7008) ===
Q mean: -11.976149
Q std: 20.597511
Actor loss: 11.980106
Action reg: 0.003958
  l1.weight: grad_norm = 0.197655
  l1.bias: grad_norm = 0.001240
  l2.weight: grad_norm = 0.149323
Total gradient norm: 0.458928
=== Actor Training Debug (Iteration 7009) ===
Q mean: -14.583538
Q std: 22.173840
Actor loss: 14.587486
Action reg: 0.003948
  l1.weight: grad_norm = 0.222233
  l1.bias: grad_norm = 0.000742
  l2.weight: grad_norm = 0.179359
Total gradient norm: 0.470728
=== Actor Training Debug (Iteration 7010) ===
Q mean: -12.243267
Q std: 19.729626
Actor loss: 12.247232
Action reg: 0.003965
  l1.weight: grad_norm = 0.272995
  l1.bias: grad_norm = 0.001034
  l2.weight: grad_norm = 0.205024
Total gradient norm: 0.546870
=== Actor Training Debug (Iteration 7011) ===
Q mean: -13.231137
Q std: 18.457220
Actor loss: 13.235103
Action reg: 0.003965
  l1.weight: grad_norm = 0.152839
  l1.bias: grad_norm = 0.002923
  l2.weight: grad_norm = 0.104012
Total gradient norm: 0.318275
=== Actor Training Debug (Iteration 7012) ===
Q mean: -11.054351
Q std: 18.088478
Actor loss: 11.058311
Action reg: 0.003961
  l1.weight: grad_norm = 0.302752
  l1.bias: grad_norm = 0.001761
  l2.weight: grad_norm = 0.228828
Total gradient norm: 0.650255
=== Actor Training Debug (Iteration 7013) ===
Q mean: -13.923721
Q std: 21.019287
Actor loss: 13.927688
Action reg: 0.003967
  l1.weight: grad_norm = 0.307134
  l1.bias: grad_norm = 0.000805
  l2.weight: grad_norm = 0.219291
Total gradient norm: 0.681743
=== Actor Training Debug (Iteration 7014) ===
Q mean: -13.599294
Q std: 20.419508
Actor loss: 13.603261
Action reg: 0.003967
  l1.weight: grad_norm = 0.494194
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.354887
Total gradient norm: 1.057634
=== Actor Training Debug (Iteration 7015) ===
Q mean: -13.849561
Q std: 20.390043
Actor loss: 13.853522
Action reg: 0.003961
  l1.weight: grad_norm = 0.176816
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.117359
Total gradient norm: 0.344820
=== Actor Training Debug (Iteration 7016) ===
Q mean: -15.604126
Q std: 22.666191
Actor loss: 15.608085
Action reg: 0.003959
  l1.weight: grad_norm = 0.196594
  l1.bias: grad_norm = 0.001135
  l2.weight: grad_norm = 0.128607
Total gradient norm: 0.358829
=== Actor Training Debug (Iteration 7017) ===
Q mean: -14.380297
Q std: 21.346830
Actor loss: 14.384269
Action reg: 0.003972
  l1.weight: grad_norm = 0.308267
  l1.bias: grad_norm = 0.000832
  l2.weight: grad_norm = 0.207740
Total gradient norm: 0.578924
=== Actor Training Debug (Iteration 7018) ===
Q mean: -13.882958
Q std: 20.809557
Actor loss: 13.886902
Action reg: 0.003943
  l1.weight: grad_norm = 0.510124
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.340974
Total gradient norm: 1.032657
=== Actor Training Debug (Iteration 7019) ===
Q mean: -13.651242
Q std: 20.836010
Actor loss: 13.655190
Action reg: 0.003948
  l1.weight: grad_norm = 0.187046
  l1.bias: grad_norm = 0.001458
  l2.weight: grad_norm = 0.129822
Total gradient norm: 0.355475
=== Actor Training Debug (Iteration 7020) ===
Q mean: -16.049179
Q std: 21.936119
Actor loss: 16.053152
Action reg: 0.003972
  l1.weight: grad_norm = 0.124834
  l1.bias: grad_norm = 0.000902
  l2.weight: grad_norm = 0.097436
Total gradient norm: 0.260311
=== Actor Training Debug (Iteration 7021) ===
Q mean: -13.799389
Q std: 20.992800
Actor loss: 13.803354
Action reg: 0.003965
  l1.weight: grad_norm = 0.192036
  l1.bias: grad_norm = 0.002547
  l2.weight: grad_norm = 0.134700
Total gradient norm: 0.389051
=== Actor Training Debug (Iteration 7022) ===
Q mean: -13.944328
Q std: 21.653822
Actor loss: 13.948305
Action reg: 0.003976
  l1.weight: grad_norm = 0.066584
  l1.bias: grad_norm = 0.000984
  l2.weight: grad_norm = 0.045560
Total gradient norm: 0.132722
=== Actor Training Debug (Iteration 7023) ===
Q mean: -14.057729
Q std: 20.952669
Actor loss: 14.061704
Action reg: 0.003975
  l1.weight: grad_norm = 0.364849
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.249391
Total gradient norm: 0.693072
=== Actor Training Debug (Iteration 7024) ===
Q mean: -14.386391
Q std: 20.714682
Actor loss: 14.390370
Action reg: 0.003980
  l1.weight: grad_norm = 0.133700
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.106257
Total gradient norm: 0.286485
=== Actor Training Debug (Iteration 7025) ===
Q mean: -11.670313
Q std: 18.424223
Actor loss: 11.674287
Action reg: 0.003974
  l1.weight: grad_norm = 0.170318
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.127432
Total gradient norm: 0.344288
=== Actor Training Debug (Iteration 7026) ===
Q mean: -14.958125
Q std: 22.902750
Actor loss: 14.962075
Action reg: 0.003950
  l1.weight: grad_norm = 0.235015
  l1.bias: grad_norm = 0.001233
  l2.weight: grad_norm = 0.159084
Total gradient norm: 0.698960
=== Actor Training Debug (Iteration 7027) ===
Q mean: -16.261742
Q std: 23.723562
Actor loss: 16.265705
Action reg: 0.003963
  l1.weight: grad_norm = 0.153000
  l1.bias: grad_norm = 0.002990
  l2.weight: grad_norm = 0.118196
Total gradient norm: 0.369032
=== Actor Training Debug (Iteration 7028) ===
Q mean: -14.134810
Q std: 20.202888
Actor loss: 14.138780
Action reg: 0.003969
  l1.weight: grad_norm = 0.133493
  l1.bias: grad_norm = 0.001391
  l2.weight: grad_norm = 0.094251
Total gradient norm: 0.269294
=== Actor Training Debug (Iteration 7029) ===
Q mean: -14.615134
Q std: 21.654623
Actor loss: 14.619095
Action reg: 0.003960
  l1.weight: grad_norm = 0.328136
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.272845
Total gradient norm: 0.723901
=== Actor Training Debug (Iteration 7030) ===
Q mean: -14.097184
Q std: 21.319244
Actor loss: 14.101152
Action reg: 0.003968
  l1.weight: grad_norm = 0.249426
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.175834
Total gradient norm: 0.464755
=== Actor Training Debug (Iteration 7031) ===
Q mean: -14.252176
Q std: 19.674774
Actor loss: 14.256149
Action reg: 0.003973
  l1.weight: grad_norm = 0.128743
  l1.bias: grad_norm = 0.000631
  l2.weight: grad_norm = 0.097940
Total gradient norm: 0.253332
=== Actor Training Debug (Iteration 7032) ===
Q mean: -15.851880
Q std: 21.867367
Actor loss: 15.855834
Action reg: 0.003954
  l1.weight: grad_norm = 0.207151
  l1.bias: grad_norm = 0.002054
  l2.weight: grad_norm = 0.157708
Total gradient norm: 0.454330
=== Actor Training Debug (Iteration 7033) ===
Q mean: -13.862804
Q std: 20.518402
Actor loss: 13.866772
Action reg: 0.003967
  l1.weight: grad_norm = 0.168177
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.152779
Total gradient norm: 0.473278
=== Actor Training Debug (Iteration 7034) ===
Q mean: -13.891271
Q std: 20.177940
Actor loss: 13.895244
Action reg: 0.003973
  l1.weight: grad_norm = 0.276506
  l1.bias: grad_norm = 0.000615
  l2.weight: grad_norm = 0.216189
Total gradient norm: 0.670117
=== Actor Training Debug (Iteration 7035) ===
Q mean: -13.179133
Q std: 19.311995
Actor loss: 13.183107
Action reg: 0.003974
  l1.weight: grad_norm = 0.193335
  l1.bias: grad_norm = 0.001496
  l2.weight: grad_norm = 0.133426
Total gradient norm: 0.342890
=== Actor Training Debug (Iteration 7036) ===
Q mean: -13.628620
Q std: 21.346241
Actor loss: 13.632600
Action reg: 0.003979
  l1.weight: grad_norm = 0.113681
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.071048
Total gradient norm: 0.217113
=== Actor Training Debug (Iteration 7037) ===
Q mean: -13.770350
Q std: 20.333439
Actor loss: 13.774305
Action reg: 0.003955
  l1.weight: grad_norm = 0.250057
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.171231
Total gradient norm: 0.471837
=== Actor Training Debug (Iteration 7038) ===
Q mean: -13.521059
Q std: 20.650223
Actor loss: 13.525026
Action reg: 0.003967
  l1.weight: grad_norm = 0.139205
  l1.bias: grad_norm = 0.000917
  l2.weight: grad_norm = 0.101411
Total gradient norm: 0.309818
=== Actor Training Debug (Iteration 7039) ===
Q mean: -12.419725
Q std: 19.558346
Actor loss: 12.423690
Action reg: 0.003965
  l1.weight: grad_norm = 0.191106
  l1.bias: grad_norm = 0.001130
  l2.weight: grad_norm = 0.149857
Total gradient norm: 0.398492
=== Actor Training Debug (Iteration 7040) ===
Q mean: -10.647636
Q std: 15.732143
Actor loss: 10.651595
Action reg: 0.003959
  l1.weight: grad_norm = 0.154565
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.109424
Total gradient norm: 0.299492
=== Actor Training Debug (Iteration 7041) ===
Q mean: -14.089758
Q std: 20.831533
Actor loss: 14.093740
Action reg: 0.003982
  l1.weight: grad_norm = 0.150325
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.100127
Total gradient norm: 0.265720
=== Actor Training Debug (Iteration 7042) ===
Q mean: -13.464777
Q std: 20.455700
Actor loss: 13.468749
Action reg: 0.003972
  l1.weight: grad_norm = 0.147072
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.099782
Total gradient norm: 0.277154
=== Actor Training Debug (Iteration 7043) ===
Q mean: -15.101200
Q std: 21.298592
Actor loss: 15.105154
Action reg: 0.003954
  l1.weight: grad_norm = 0.162474
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.126270
Total gradient norm: 0.336062
=== Actor Training Debug (Iteration 7044) ===
Q mean: -13.943214
Q std: 20.377150
Actor loss: 13.947189
Action reg: 0.003975
  l1.weight: grad_norm = 0.308818
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.239811
Total gradient norm: 0.739283
=== Actor Training Debug (Iteration 7045) ===
Q mean: -14.220311
Q std: 21.627649
Actor loss: 14.224263
Action reg: 0.003952
  l1.weight: grad_norm = 0.113201
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.085423
Total gradient norm: 0.234074
=== Actor Training Debug (Iteration 7046) ===
Q mean: -14.452765
Q std: 22.036835
Actor loss: 14.456729
Action reg: 0.003965
  l1.weight: grad_norm = 0.167690
  l1.bias: grad_norm = 0.000718
  l2.weight: grad_norm = 0.130640
Total gradient norm: 0.366577
=== Actor Training Debug (Iteration 7047) ===
Q mean: -15.034035
Q std: 22.029030
Actor loss: 15.037992
Action reg: 0.003958
  l1.weight: grad_norm = 0.135737
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.099104
Total gradient norm: 0.274211
=== Actor Training Debug (Iteration 7048) ===
Q mean: -14.931837
Q std: 21.318201
Actor loss: 14.935805
Action reg: 0.003968
  l1.weight: grad_norm = 0.108022
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.076469
Total gradient norm: 0.230693
=== Actor Training Debug (Iteration 7049) ===
Q mean: -14.134371
Q std: 20.932253
Actor loss: 14.138349
Action reg: 0.003978
  l1.weight: grad_norm = 0.144488
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.099469
Total gradient norm: 0.289198
=== Actor Training Debug (Iteration 7050) ===
Q mean: -13.794237
Q std: 21.303638
Actor loss: 13.798217
Action reg: 0.003980
  l1.weight: grad_norm = 0.245141
  l1.bias: grad_norm = 0.003310
  l2.weight: grad_norm = 0.178622
Total gradient norm: 0.551214
=== Actor Training Debug (Iteration 7051) ===
Q mean: -14.990011
Q std: 21.281458
Actor loss: 14.993969
Action reg: 0.003957
  l1.weight: grad_norm = 0.111939
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.086183
Total gradient norm: 0.250353
=== Actor Training Debug (Iteration 7052) ===
Q mean: -15.518770
Q std: 22.502365
Actor loss: 15.522738
Action reg: 0.003969
  l1.weight: grad_norm = 0.214194
  l1.bias: grad_norm = 0.000853
  l2.weight: grad_norm = 0.196181
Total gradient norm: 0.557830
=== Actor Training Debug (Iteration 7053) ===
Q mean: -14.682795
Q std: 21.076401
Actor loss: 14.686773
Action reg: 0.003978
  l1.weight: grad_norm = 0.184587
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.145005
Total gradient norm: 0.365590
=== Actor Training Debug (Iteration 7054) ===
Q mean: -11.506775
Q std: 19.546717
Actor loss: 11.510721
Action reg: 0.003947
  l1.weight: grad_norm = 0.247127
  l1.bias: grad_norm = 0.000546
  l2.weight: grad_norm = 0.196989
Total gradient norm: 0.552483
=== Actor Training Debug (Iteration 7055) ===
Q mean: -13.594318
Q std: 21.248535
Actor loss: 13.598280
Action reg: 0.003961
  l1.weight: grad_norm = 0.241312
  l1.bias: grad_norm = 0.000492
  l2.weight: grad_norm = 0.159864
Total gradient norm: 0.459463
=== Actor Training Debug (Iteration 7056) ===
Q mean: -11.948260
Q std: 19.804066
Actor loss: 11.952228
Action reg: 0.003968
  l1.weight: grad_norm = 0.240490
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.183361
Total gradient norm: 0.497495
=== Actor Training Debug (Iteration 7057) ===
Q mean: -14.015879
Q std: 19.767160
Actor loss: 14.019844
Action reg: 0.003965
  l1.weight: grad_norm = 0.225000
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.161731
Total gradient norm: 0.477077
=== Actor Training Debug (Iteration 7058) ===
Q mean: -13.432673
Q std: 21.903946
Actor loss: 13.436634
Action reg: 0.003961
  l1.weight: grad_norm = 0.055354
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.050376
Total gradient norm: 0.134630
=== Actor Training Debug (Iteration 7059) ===
Q mean: -14.589767
Q std: 20.910234
Actor loss: 14.593733
Action reg: 0.003967
  l1.weight: grad_norm = 0.266290
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.191187
Total gradient norm: 0.569337
=== Actor Training Debug (Iteration 7060) ===
Q mean: -14.673540
Q std: 23.358313
Actor loss: 14.677511
Action reg: 0.003971
  l1.weight: grad_norm = 0.182950
  l1.bias: grad_norm = 0.000909
  l2.weight: grad_norm = 0.155393
Total gradient norm: 0.481671
=== Actor Training Debug (Iteration 7061) ===
Q mean: -13.843212
Q std: 20.414724
Actor loss: 13.847188
Action reg: 0.003976
  l1.weight: grad_norm = 0.197867
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.145627
Total gradient norm: 0.427039
=== Actor Training Debug (Iteration 7062) ===
Q mean: -15.377443
Q std: 20.573044
Actor loss: 15.381382
Action reg: 0.003938
  l1.weight: grad_norm = 0.084090
  l1.bias: grad_norm = 0.002913
  l2.weight: grad_norm = 0.058401
Total gradient norm: 0.214821
=== Actor Training Debug (Iteration 7063) ===
Q mean: -14.664791
Q std: 21.348810
Actor loss: 14.668748
Action reg: 0.003957
  l1.weight: grad_norm = 0.217248
  l1.bias: grad_norm = 0.001951
  l2.weight: grad_norm = 0.149260
Total gradient norm: 0.452234
=== Actor Training Debug (Iteration 7064) ===
Q mean: -14.824478
Q std: 20.032175
Actor loss: 14.828439
Action reg: 0.003961
  l1.weight: grad_norm = 0.188842
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 0.140343
Total gradient norm: 0.518244
=== Actor Training Debug (Iteration 7065) ===
Q mean: -14.516798
Q std: 21.279524
Actor loss: 14.520770
Action reg: 0.003972
  l1.weight: grad_norm = 0.257067
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.218590
Total gradient norm: 0.577282
=== Actor Training Debug (Iteration 7066) ===
Q mean: -13.653769
Q std: 20.246504
Actor loss: 13.657741
Action reg: 0.003971
  l1.weight: grad_norm = 0.156041
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.119464
Total gradient norm: 0.322662
=== Actor Training Debug (Iteration 7067) ===
Q mean: -15.653793
Q std: 21.877384
Actor loss: 15.657773
Action reg: 0.003979
  l1.weight: grad_norm = 0.097514
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.072960
Total gradient norm: 0.198357
=== Actor Training Debug (Iteration 7068) ===
Q mean: -15.363731
Q std: 21.954771
Actor loss: 15.367700
Action reg: 0.003968
  l1.weight: grad_norm = 0.101644
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.075877
Total gradient norm: 0.200279
=== Actor Training Debug (Iteration 7069) ===
Q mean: -14.832413
Q std: 22.635975
Actor loss: 14.836373
Action reg: 0.003960
  l1.weight: grad_norm = 0.161235
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.112899
Total gradient norm: 0.346752
=== Actor Training Debug (Iteration 7070) ===
Q mean: -13.404789
Q std: 18.902191
Actor loss: 13.408762
Action reg: 0.003973
  l1.weight: grad_norm = 0.133208
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.092236
Total gradient norm: 0.240476
=== Actor Training Debug (Iteration 7071) ===
Q mean: -12.893491
Q std: 20.971533
Actor loss: 12.897477
Action reg: 0.003987
  l1.weight: grad_norm = 0.058044
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.046714
Total gradient norm: 0.127463
=== Actor Training Debug (Iteration 7072) ===
Q mean: -12.661545
Q std: 18.668367
Actor loss: 12.665493
Action reg: 0.003948
  l1.weight: grad_norm = 0.370277
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.261340
Total gradient norm: 0.707723
=== Actor Training Debug (Iteration 7073) ===
Q mean: -12.356426
Q std: 19.584810
Actor loss: 12.360381
Action reg: 0.003955
  l1.weight: grad_norm = 0.147685
  l1.bias: grad_norm = 0.002460
  l2.weight: grad_norm = 0.102330
Total gradient norm: 0.319429
=== Actor Training Debug (Iteration 7074) ===
Q mean: -13.654428
Q std: 20.608213
Actor loss: 13.658414
Action reg: 0.003985
  l1.weight: grad_norm = 0.103299
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.072987
Total gradient norm: 0.198028
=== Actor Training Debug (Iteration 7075) ===
Q mean: -13.374168
Q std: 21.048304
Actor loss: 13.378140
Action reg: 0.003972
  l1.weight: grad_norm = 0.211138
  l1.bias: grad_norm = 0.002514
  l2.weight: grad_norm = 0.150595
Total gradient norm: 0.395329
=== Actor Training Debug (Iteration 7076) ===
Q mean: -14.298637
Q std: 21.275326
Actor loss: 14.302599
Action reg: 0.003962
  l1.weight: grad_norm = 0.094331
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.072164
Total gradient norm: 0.185933
=== Actor Training Debug (Iteration 7077) ===
Q mean: -15.595129
Q std: 21.018383
Actor loss: 15.599111
Action reg: 0.003982
  l1.weight: grad_norm = 0.271229
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.186654
Total gradient norm: 0.547596
=== Actor Training Debug (Iteration 7078) ===
Q mean: -15.064198
Q std: 21.492010
Actor loss: 15.068159
Action reg: 0.003961
  l1.weight: grad_norm = 0.344357
  l1.bias: grad_norm = 0.001421
  l2.weight: grad_norm = 0.249618
Total gradient norm: 0.697389
=== Actor Training Debug (Iteration 7079) ===
Q mean: -15.642881
Q std: 21.711555
Actor loss: 15.646852
Action reg: 0.003971
  l1.weight: grad_norm = 0.769172
  l1.bias: grad_norm = 0.000585
  l2.weight: grad_norm = 0.599364
Total gradient norm: 1.998148
=== Actor Training Debug (Iteration 7080) ===
Q mean: -13.979988
Q std: 20.025297
Actor loss: 13.983935
Action reg: 0.003948
  l1.weight: grad_norm = 0.147531
  l1.bias: grad_norm = 0.000745
  l2.weight: grad_norm = 0.119155
Total gradient norm: 0.324378
=== Actor Training Debug (Iteration 7081) ===
Q mean: -13.927479
Q std: 21.598322
Actor loss: 13.931456
Action reg: 0.003977
  l1.weight: grad_norm = 0.160326
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.149017
Total gradient norm: 0.519832
=== Actor Training Debug (Iteration 7082) ===
Q mean: -15.237609
Q std: 21.483109
Actor loss: 15.241575
Action reg: 0.003966
  l1.weight: grad_norm = 0.200187
  l1.bias: grad_norm = 0.000600
  l2.weight: grad_norm = 0.124449
Total gradient norm: 0.338913
=== Actor Training Debug (Iteration 7083) ===
Q mean: -14.570741
Q std: 21.734697
Actor loss: 14.574705
Action reg: 0.003964
  l1.weight: grad_norm = 0.104172
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.076475
Total gradient norm: 0.218765
=== Actor Training Debug (Iteration 7084) ===
Q mean: -15.234818
Q std: 21.165289
Actor loss: 15.238796
Action reg: 0.003979
  l1.weight: grad_norm = 0.103182
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.076078
Total gradient norm: 0.208070
=== Actor Training Debug (Iteration 7085) ===
Q mean: -15.397367
Q std: 22.710789
Actor loss: 15.401342
Action reg: 0.003975
  l1.weight: grad_norm = 0.088333
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.069899
Total gradient norm: 0.228574
=== Actor Training Debug (Iteration 7086) ===
Q mean: -15.851614
Q std: 22.218575
Actor loss: 15.855581
Action reg: 0.003967
  l1.weight: grad_norm = 0.250483
  l1.bias: grad_norm = 0.000703
  l2.weight: grad_norm = 0.219430
Total gradient norm: 0.867606
=== Actor Training Debug (Iteration 7087) ===
Q mean: -13.561355
Q std: 18.949217
Actor loss: 13.565336
Action reg: 0.003981
  l1.weight: grad_norm = 0.235142
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.201746
Total gradient norm: 0.611139
=== Actor Training Debug (Iteration 7088) ===
Q mean: -13.583018
Q std: 18.594713
Actor loss: 13.586966
Action reg: 0.003948
  l1.weight: grad_norm = 0.138494
  l1.bias: grad_norm = 0.001929
  l2.weight: grad_norm = 0.092034
Total gradient norm: 0.258397
=== Actor Training Debug (Iteration 7089) ===
Q mean: -12.359462
Q std: 18.598915
Actor loss: 12.363404
Action reg: 0.003943
  l1.weight: grad_norm = 0.238303
  l1.bias: grad_norm = 0.000670
  l2.weight: grad_norm = 0.150167
Total gradient norm: 0.464842
=== Actor Training Debug (Iteration 7090) ===
Q mean: -13.298273
Q std: 19.289091
Actor loss: 13.302222
Action reg: 0.003949
  l1.weight: grad_norm = 0.438814
  l1.bias: grad_norm = 0.001456
  l2.weight: grad_norm = 0.374592
Total gradient norm: 1.190001
=== Actor Training Debug (Iteration 7091) ===
Q mean: -14.228625
Q std: 19.754444
Actor loss: 14.232579
Action reg: 0.003954
  l1.weight: grad_norm = 0.288565
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.203858
Total gradient norm: 0.566815
=== Actor Training Debug (Iteration 7092) ===
Q mean: -14.127259
Q std: 22.138910
Actor loss: 14.131214
Action reg: 0.003955
  l1.weight: grad_norm = 0.228411
  l1.bias: grad_norm = 0.000945
  l2.weight: grad_norm = 0.161535
Total gradient norm: 0.473804
=== Actor Training Debug (Iteration 7093) ===
Q mean: -12.467180
Q std: 19.791883
Actor loss: 12.471127
Action reg: 0.003946
  l1.weight: grad_norm = 0.215303
  l1.bias: grad_norm = 0.001031
  l2.weight: grad_norm = 0.138430
Total gradient norm: 0.422696
=== Actor Training Debug (Iteration 7094) ===
Q mean: -13.893951
Q std: 20.493887
Actor loss: 13.897918
Action reg: 0.003966
  l1.weight: grad_norm = 0.280962
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.202482
Total gradient norm: 0.686678
=== Actor Training Debug (Iteration 7095) ===
Q mean: -16.817085
Q std: 22.485271
Actor loss: 16.821043
Action reg: 0.003959
  l1.weight: grad_norm = 0.093481
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.073016
Total gradient norm: 0.224241
=== Actor Training Debug (Iteration 7096) ===
Q mean: -12.665583
Q std: 20.080151
Actor loss: 12.669550
Action reg: 0.003968
  l1.weight: grad_norm = 0.210389
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.145694
Total gradient norm: 0.362111
=== Actor Training Debug (Iteration 7097) ===
Q mean: -14.769755
Q std: 20.464508
Actor loss: 14.773701
Action reg: 0.003945
  l1.weight: grad_norm = 0.182366
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.132828
Total gradient norm: 0.381433
=== Actor Training Debug (Iteration 7098) ===
Q mean: -13.184496
Q std: 20.134960
Actor loss: 13.188465
Action reg: 0.003969
  l1.weight: grad_norm = 0.165645
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.103907
Total gradient norm: 0.320537
=== Actor Training Debug (Iteration 7099) ===
Q mean: -11.878165
Q std: 18.407049
Actor loss: 11.882136
Action reg: 0.003971
  l1.weight: grad_norm = 0.201835
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.165992
Total gradient norm: 0.529829
=== Actor Training Debug (Iteration 7100) ===
Q mean: -15.620543
Q std: 21.282701
Actor loss: 15.624497
Action reg: 0.003955
  l1.weight: grad_norm = 0.233548
  l1.bias: grad_norm = 0.001478
  l2.weight: grad_norm = 0.169209
Total gradient norm: 0.464461
Episode 121: Steps=100, Reward=-268.207, Buffer_size=12100
=== Actor Training Debug (Iteration 7101) ===
Q mean: -14.153130
Q std: 20.817558
Actor loss: 14.157101
Action reg: 0.003971
  l1.weight: grad_norm = 0.289690
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.218139
Total gradient norm: 0.672826
=== Actor Training Debug (Iteration 7102) ===
Q mean: -15.134424
Q std: 21.921787
Actor loss: 15.138392
Action reg: 0.003969
  l1.weight: grad_norm = 0.122573
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.088391
Total gradient norm: 0.262086
=== Actor Training Debug (Iteration 7103) ===
Q mean: -9.552727
Q std: 17.577223
Actor loss: 9.556705
Action reg: 0.003977
  l1.weight: grad_norm = 0.192077
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.135098
Total gradient norm: 0.354668
=== Actor Training Debug (Iteration 7104) ===
Q mean: -15.185041
Q std: 21.649012
Actor loss: 15.189013
Action reg: 0.003972
  l1.weight: grad_norm = 0.125236
  l1.bias: grad_norm = 0.003318
  l2.weight: grad_norm = 0.088162
Total gradient norm: 0.257445
=== Actor Training Debug (Iteration 7105) ===
Q mean: -13.386265
Q std: 19.681192
Actor loss: 13.390214
Action reg: 0.003949
  l1.weight: grad_norm = 0.265233
  l1.bias: grad_norm = 0.005089
  l2.weight: grad_norm = 0.203855
Total gradient norm: 0.664885
=== Actor Training Debug (Iteration 7106) ===
Q mean: -14.302884
Q std: 21.179689
Actor loss: 14.306863
Action reg: 0.003979
  l1.weight: grad_norm = 0.128143
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.101947
Total gradient norm: 0.286020
=== Actor Training Debug (Iteration 7107) ===
Q mean: -14.389870
Q std: 21.377441
Actor loss: 14.393837
Action reg: 0.003967
  l1.weight: grad_norm = 0.075346
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.057010
Total gradient norm: 0.152827
=== Actor Training Debug (Iteration 7108) ===
Q mean: -15.680527
Q std: 23.636225
Actor loss: 15.684500
Action reg: 0.003973
  l1.weight: grad_norm = 0.129669
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.096462
Total gradient norm: 0.288450
=== Actor Training Debug (Iteration 7109) ===
Q mean: -14.814669
Q std: 20.650866
Actor loss: 14.818643
Action reg: 0.003974
  l1.weight: grad_norm = 0.190817
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.157536
Total gradient norm: 0.427926
=== Actor Training Debug (Iteration 7110) ===
Q mean: -12.564693
Q std: 18.973572
Actor loss: 12.568677
Action reg: 0.003983
  l1.weight: grad_norm = 0.246652
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.155963
Total gradient norm: 0.416484
=== Actor Training Debug (Iteration 7111) ===
Q mean: -14.346684
Q std: 20.954252
Actor loss: 14.350655
Action reg: 0.003971
  l1.weight: grad_norm = 0.096362
  l1.bias: grad_norm = 0.002231
  l2.weight: grad_norm = 0.068897
Total gradient norm: 0.210583
=== Actor Training Debug (Iteration 7112) ===
Q mean: -16.395657
Q std: 21.824427
Actor loss: 16.399603
Action reg: 0.003946
  l1.weight: grad_norm = 0.410285
  l1.bias: grad_norm = 0.002606
  l2.weight: grad_norm = 0.339021
Total gradient norm: 0.897036
=== Actor Training Debug (Iteration 7113) ===
Q mean: -14.312994
Q std: 20.839483
Actor loss: 14.316955
Action reg: 0.003960
  l1.weight: grad_norm = 0.190900
  l1.bias: grad_norm = 0.001294
  l2.weight: grad_norm = 0.116955
Total gradient norm: 0.329405
=== Actor Training Debug (Iteration 7114) ===
Q mean: -13.858777
Q std: 21.221218
Actor loss: 13.862765
Action reg: 0.003988
  l1.weight: grad_norm = 0.074159
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.062474
Total gradient norm: 0.177867
=== Actor Training Debug (Iteration 7115) ===
Q mean: -16.878929
Q std: 23.448498
Actor loss: 16.882893
Action reg: 0.003963
  l1.weight: grad_norm = 0.266474
  l1.bias: grad_norm = 0.004030
  l2.weight: grad_norm = 0.209254
Total gradient norm: 0.550656
=== Actor Training Debug (Iteration 7116) ===
Q mean: -15.885668
Q std: 22.624054
Actor loss: 15.889633
Action reg: 0.003966
  l1.weight: grad_norm = 0.192143
  l1.bias: grad_norm = 0.001401
  l2.weight: grad_norm = 0.132519
Total gradient norm: 0.382193
=== Actor Training Debug (Iteration 7117) ===
Q mean: -13.199232
Q std: 19.582853
Actor loss: 13.203180
Action reg: 0.003948
  l1.weight: grad_norm = 0.164831
  l1.bias: grad_norm = 0.001724
  l2.weight: grad_norm = 0.112495
Total gradient norm: 0.322218
=== Actor Training Debug (Iteration 7118) ===
Q mean: -13.339730
Q std: 20.131420
Actor loss: 13.343679
Action reg: 0.003949
  l1.weight: grad_norm = 0.069432
  l1.bias: grad_norm = 0.005279
  l2.weight: grad_norm = 0.062199
Total gradient norm: 0.266743
=== Actor Training Debug (Iteration 7119) ===
Q mean: -12.604642
Q std: 19.183937
Actor loss: 12.608616
Action reg: 0.003974
  l1.weight: grad_norm = 0.165350
  l1.bias: grad_norm = 0.001986
  l2.weight: grad_norm = 0.141048
Total gradient norm: 0.330692
=== Actor Training Debug (Iteration 7120) ===
Q mean: -14.858329
Q std: 21.171495
Actor loss: 14.862296
Action reg: 0.003968
  l1.weight: grad_norm = 0.211690
  l1.bias: grad_norm = 0.001670
  l2.weight: grad_norm = 0.163126
Total gradient norm: 0.446052
=== Actor Training Debug (Iteration 7121) ===
Q mean: -13.072529
Q std: 20.320131
Actor loss: 13.076477
Action reg: 0.003948
  l1.weight: grad_norm = 0.149919
  l1.bias: grad_norm = 0.001605
  l2.weight: grad_norm = 0.144534
Total gradient norm: 0.348165
=== Actor Training Debug (Iteration 7122) ===
Q mean: -15.418259
Q std: 21.448460
Actor loss: 15.422236
Action reg: 0.003978
  l1.weight: grad_norm = 0.186110
  l1.bias: grad_norm = 0.000628
  l2.weight: grad_norm = 0.124781
Total gradient norm: 0.381619
=== Actor Training Debug (Iteration 7123) ===
Q mean: -15.725837
Q std: 20.788723
Actor loss: 15.729808
Action reg: 0.003971
  l1.weight: grad_norm = 0.140567
  l1.bias: grad_norm = 0.000656
  l2.weight: grad_norm = 0.138767
Total gradient norm: 0.388792
=== Actor Training Debug (Iteration 7124) ===
Q mean: -14.413895
Q std: 21.273872
Actor loss: 14.417854
Action reg: 0.003959
  l1.weight: grad_norm = 0.257412
  l1.bias: grad_norm = 0.003328
  l2.weight: grad_norm = 0.199019
Total gradient norm: 0.683005
=== Actor Training Debug (Iteration 7125) ===
Q mean: -11.455856
Q std: 18.755657
Actor loss: 11.459808
Action reg: 0.003952
  l1.weight: grad_norm = 0.163834
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.128420
Total gradient norm: 0.388071
=== Actor Training Debug (Iteration 7126) ===
Q mean: -15.778045
Q std: 22.971628
Actor loss: 15.781985
Action reg: 0.003940
  l1.weight: grad_norm = 0.183667
  l1.bias: grad_norm = 0.007275
  l2.weight: grad_norm = 0.129042
Total gradient norm: 0.443142
=== Actor Training Debug (Iteration 7127) ===
Q mean: -14.558603
Q std: 21.220837
Actor loss: 14.562554
Action reg: 0.003951
  l1.weight: grad_norm = 0.211832
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.136487
Total gradient norm: 0.375477
=== Actor Training Debug (Iteration 7128) ===
Q mean: -11.771081
Q std: 19.293495
Actor loss: 11.775048
Action reg: 0.003968
  l1.weight: grad_norm = 0.167198
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.134283
Total gradient norm: 0.362658
=== Actor Training Debug (Iteration 7129) ===
Q mean: -14.467434
Q std: 19.889021
Actor loss: 14.471368
Action reg: 0.003934
  l1.weight: grad_norm = 0.217553
  l1.bias: grad_norm = 0.000606
  l2.weight: grad_norm = 0.184446
Total gradient norm: 0.495354
=== Actor Training Debug (Iteration 7130) ===
Q mean: -14.601353
Q std: 20.524956
Actor loss: 14.605328
Action reg: 0.003975
  l1.weight: grad_norm = 0.254873
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.176481
Total gradient norm: 0.465555
=== Actor Training Debug (Iteration 7131) ===
Q mean: -14.206416
Q std: 21.790928
Actor loss: 14.210394
Action reg: 0.003978
  l1.weight: grad_norm = 0.365919
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.244806
Total gradient norm: 0.863711
=== Actor Training Debug (Iteration 7132) ===
Q mean: -13.225302
Q std: 19.581171
Actor loss: 13.229262
Action reg: 0.003960
  l1.weight: grad_norm = 0.312318
  l1.bias: grad_norm = 0.001881
  l2.weight: grad_norm = 0.293566
Total gradient norm: 0.744464
=== Actor Training Debug (Iteration 7133) ===
Q mean: -12.292946
Q std: 20.138161
Actor loss: 12.296886
Action reg: 0.003941
  l1.weight: grad_norm = 0.358229
  l1.bias: grad_norm = 0.003314
  l2.weight: grad_norm = 0.255465
Total gradient norm: 0.933481
=== Actor Training Debug (Iteration 7134) ===
Q mean: -13.358131
Q std: 20.315023
Actor loss: 13.362091
Action reg: 0.003960
  l1.weight: grad_norm = 0.209456
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.168452
Total gradient norm: 0.492390
=== Actor Training Debug (Iteration 7135) ===
Q mean: -15.073387
Q std: 21.548410
Actor loss: 15.077362
Action reg: 0.003975
  l1.weight: grad_norm = 0.107901
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.085225
Total gradient norm: 0.226228
=== Actor Training Debug (Iteration 7136) ===
Q mean: -15.065027
Q std: 22.444870
Actor loss: 15.068995
Action reg: 0.003967
  l1.weight: grad_norm = 0.101332
  l1.bias: grad_norm = 0.002033
  l2.weight: grad_norm = 0.085861
Total gradient norm: 0.269240
=== Actor Training Debug (Iteration 7137) ===
Q mean: -12.265733
Q std: 18.803534
Actor loss: 12.269706
Action reg: 0.003973
  l1.weight: grad_norm = 0.276144
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.205615
Total gradient norm: 0.569931
=== Actor Training Debug (Iteration 7138) ===
Q mean: -15.380964
Q std: 22.022919
Actor loss: 15.384900
Action reg: 0.003936
  l1.weight: grad_norm = 0.380020
  l1.bias: grad_norm = 0.000516
  l2.weight: grad_norm = 0.256940
Total gradient norm: 0.786869
=== Actor Training Debug (Iteration 7139) ===
Q mean: -15.257578
Q std: 22.880087
Actor loss: 15.261540
Action reg: 0.003962
  l1.weight: grad_norm = 0.194120
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.142250
Total gradient norm: 0.409923
=== Actor Training Debug (Iteration 7140) ===
Q mean: -13.344802
Q std: 20.409691
Actor loss: 13.348783
Action reg: 0.003981
  l1.weight: grad_norm = 0.300911
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.212448
Total gradient norm: 0.566036
=== Actor Training Debug (Iteration 7141) ===
Q mean: -14.648425
Q std: 20.164688
Actor loss: 14.652390
Action reg: 0.003965
  l1.weight: grad_norm = 0.273984
  l1.bias: grad_norm = 0.000820
  l2.weight: grad_norm = 0.203969
Total gradient norm: 0.631481
=== Actor Training Debug (Iteration 7142) ===
Q mean: -16.018436
Q std: 22.236803
Actor loss: 16.022402
Action reg: 0.003964
  l1.weight: grad_norm = 0.122410
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.091091
Total gradient norm: 0.270254
=== Actor Training Debug (Iteration 7143) ===
Q mean: -13.036873
Q std: 20.373161
Actor loss: 13.040847
Action reg: 0.003974
  l1.weight: grad_norm = 0.195810
  l1.bias: grad_norm = 0.001613
  l2.weight: grad_norm = 0.165531
Total gradient norm: 0.448285
=== Actor Training Debug (Iteration 7144) ===
Q mean: -13.260286
Q std: 21.052052
Actor loss: 13.264264
Action reg: 0.003978
  l1.weight: grad_norm = 0.205262
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.171613
Total gradient norm: 0.432839
=== Actor Training Debug (Iteration 7145) ===
Q mean: -13.221306
Q std: 19.818436
Actor loss: 13.225266
Action reg: 0.003960
  l1.weight: grad_norm = 0.157353
  l1.bias: grad_norm = 0.001386
  l2.weight: grad_norm = 0.107859
Total gradient norm: 0.328767
=== Actor Training Debug (Iteration 7146) ===
Q mean: -13.190017
Q std: 20.384970
Actor loss: 13.193989
Action reg: 0.003972
  l1.weight: grad_norm = 0.098411
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.064951
Total gradient norm: 0.189984
=== Actor Training Debug (Iteration 7147) ===
Q mean: -16.297705
Q std: 22.138054
Actor loss: 16.301666
Action reg: 0.003962
  l1.weight: grad_norm = 0.143824
  l1.bias: grad_norm = 0.001665
  l2.weight: grad_norm = 0.099602
Total gradient norm: 0.274063
=== Actor Training Debug (Iteration 7148) ===
Q mean: -13.140624
Q std: 20.368015
Actor loss: 13.144594
Action reg: 0.003970
  l1.weight: grad_norm = 0.071371
  l1.bias: grad_norm = 0.000787
  l2.weight: grad_norm = 0.059286
Total gradient norm: 0.160111
=== Actor Training Debug (Iteration 7149) ===
Q mean: -13.016515
Q std: 20.644512
Actor loss: 13.020477
Action reg: 0.003962
  l1.weight: grad_norm = 0.113063
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.083627
Total gradient norm: 0.237751
=== Actor Training Debug (Iteration 7150) ===
Q mean: -13.013205
Q std: 20.917624
Actor loss: 13.017157
Action reg: 0.003952
  l1.weight: grad_norm = 0.300030
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.265658
Total gradient norm: 0.803583
=== Actor Training Debug (Iteration 7151) ===
Q mean: -12.918899
Q std: 20.090742
Actor loss: 12.922847
Action reg: 0.003949
  l1.weight: grad_norm = 0.222438
  l1.bias: grad_norm = 0.001548
  l2.weight: grad_norm = 0.178190
Total gradient norm: 0.486251
=== Actor Training Debug (Iteration 7152) ===
Q mean: -14.320613
Q std: 20.203411
Actor loss: 14.324568
Action reg: 0.003955
  l1.weight: grad_norm = 0.113471
  l1.bias: grad_norm = 0.000652
  l2.weight: grad_norm = 0.085326
Total gradient norm: 0.218943
=== Actor Training Debug (Iteration 7153) ===
Q mean: -13.807232
Q std: 19.637547
Actor loss: 13.811203
Action reg: 0.003971
  l1.weight: grad_norm = 0.044771
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.033473
Total gradient norm: 0.100349
=== Actor Training Debug (Iteration 7154) ===
Q mean: -14.004152
Q std: 22.009724
Actor loss: 14.008108
Action reg: 0.003956
  l1.weight: grad_norm = 0.211316
  l1.bias: grad_norm = 0.001368
  l2.weight: grad_norm = 0.149815
Total gradient norm: 0.364405
=== Actor Training Debug (Iteration 7155) ===
Q mean: -13.072968
Q std: 20.705000
Actor loss: 13.076925
Action reg: 0.003958
  l1.weight: grad_norm = 0.203506
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.155753
Total gradient norm: 0.422130
=== Actor Training Debug (Iteration 7156) ===
Q mean: -14.003074
Q std: 19.716759
Actor loss: 14.007054
Action reg: 0.003980
  l1.weight: grad_norm = 0.135131
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.100767
Total gradient norm: 0.288436
=== Actor Training Debug (Iteration 7157) ===
Q mean: -16.414913
Q std: 22.331154
Actor loss: 16.418879
Action reg: 0.003966
  l1.weight: grad_norm = 0.133292
  l1.bias: grad_norm = 0.001450
  l2.weight: grad_norm = 0.092834
Total gradient norm: 0.253148
=== Actor Training Debug (Iteration 7158) ===
Q mean: -14.425730
Q std: 21.442047
Actor loss: 14.429684
Action reg: 0.003954
  l1.weight: grad_norm = 0.186480
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.126571
Total gradient norm: 0.387523
=== Actor Training Debug (Iteration 7159) ===
Q mean: -13.639345
Q std: 21.643841
Actor loss: 13.643291
Action reg: 0.003945
  l1.weight: grad_norm = 1.038619
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.748643
Total gradient norm: 2.222597
=== Actor Training Debug (Iteration 7160) ===
Q mean: -15.240531
Q std: 22.093790
Actor loss: 15.244500
Action reg: 0.003969
  l1.weight: grad_norm = 0.375657
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.281631
Total gradient norm: 0.796584
=== Actor Training Debug (Iteration 7161) ===
Q mean: -13.955849
Q std: 20.400700
Actor loss: 13.959817
Action reg: 0.003968
  l1.weight: grad_norm = 0.086873
  l1.bias: grad_norm = 0.001156
  l2.weight: grad_norm = 0.069871
Total gradient norm: 0.195603
=== Actor Training Debug (Iteration 7162) ===
Q mean: -12.758829
Q std: 19.317602
Actor loss: 12.762812
Action reg: 0.003983
  l1.weight: grad_norm = 0.189790
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.133632
Total gradient norm: 0.380697
=== Actor Training Debug (Iteration 7163) ===
Q mean: -12.813766
Q std: 19.512810
Actor loss: 12.817714
Action reg: 0.003947
  l1.weight: grad_norm = 0.260421
  l1.bias: grad_norm = 0.001550
  l2.weight: grad_norm = 0.178383
Total gradient norm: 0.516825
=== Actor Training Debug (Iteration 7164) ===
Q mean: -14.211259
Q std: 21.334513
Actor loss: 14.215234
Action reg: 0.003975
  l1.weight: grad_norm = 0.220826
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.130492
Total gradient norm: 0.377962
=== Actor Training Debug (Iteration 7165) ===
Q mean: -14.596239
Q std: 20.785131
Actor loss: 14.600202
Action reg: 0.003963
  l1.weight: grad_norm = 0.303389
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.234580
Total gradient norm: 0.819157
=== Actor Training Debug (Iteration 7166) ===
Q mean: -16.784176
Q std: 23.180847
Actor loss: 16.788113
Action reg: 0.003937
  l1.weight: grad_norm = 0.123632
  l1.bias: grad_norm = 0.001350
  l2.weight: grad_norm = 0.089978
Total gradient norm: 0.272077
=== Actor Training Debug (Iteration 7167) ===
Q mean: -15.989298
Q std: 22.948082
Actor loss: 15.993261
Action reg: 0.003963
  l1.weight: grad_norm = 0.128615
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.100538
Total gradient norm: 0.257054
=== Actor Training Debug (Iteration 7168) ===
Q mean: -11.976297
Q std: 18.737633
Actor loss: 11.980255
Action reg: 0.003958
  l1.weight: grad_norm = 0.145740
  l1.bias: grad_norm = 0.001406
  l2.weight: grad_norm = 0.110020
Total gradient norm: 0.318210
=== Actor Training Debug (Iteration 7169) ===
Q mean: -13.361204
Q std: 20.723545
Actor loss: 13.365170
Action reg: 0.003965
  l1.weight: grad_norm = 0.162105
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.119937
Total gradient norm: 0.306703
=== Actor Training Debug (Iteration 7170) ===
Q mean: -14.675125
Q std: 20.662249
Actor loss: 14.679091
Action reg: 0.003967
  l1.weight: grad_norm = 0.175707
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.144318
Total gradient norm: 0.364645
=== Actor Training Debug (Iteration 7171) ===
Q mean: -14.511726
Q std: 21.405493
Actor loss: 14.515694
Action reg: 0.003967
  l1.weight: grad_norm = 0.285508
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.237681
Total gradient norm: 0.613633
=== Actor Training Debug (Iteration 7172) ===
Q mean: -12.824787
Q std: 19.177084
Actor loss: 12.828724
Action reg: 0.003937
  l1.weight: grad_norm = 0.259050
  l1.bias: grad_norm = 0.001234
  l2.weight: grad_norm = 0.200617
Total gradient norm: 0.570119
=== Actor Training Debug (Iteration 7173) ===
Q mean: -13.666149
Q std: 21.419237
Actor loss: 13.670115
Action reg: 0.003966
  l1.weight: grad_norm = 0.091251
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.075635
Total gradient norm: 0.216991
=== Actor Training Debug (Iteration 7174) ===
Q mean: -14.012217
Q std: 21.154802
Actor loss: 14.016187
Action reg: 0.003970
  l1.weight: grad_norm = 0.234937
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.140026
Total gradient norm: 0.385186
=== Actor Training Debug (Iteration 7175) ===
Q mean: -13.328869
Q std: 20.544376
Actor loss: 13.332831
Action reg: 0.003963
  l1.weight: grad_norm = 0.145397
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.132594
Total gradient norm: 0.385062
=== Actor Training Debug (Iteration 7176) ===
Q mean: -13.998201
Q std: 20.910143
Actor loss: 14.002152
Action reg: 0.003951
  l1.weight: grad_norm = 0.220352
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.178299
Total gradient norm: 0.539278
=== Actor Training Debug (Iteration 7177) ===
Q mean: -14.016948
Q std: 20.874752
Actor loss: 14.020895
Action reg: 0.003947
  l1.weight: grad_norm = 0.129524
  l1.bias: grad_norm = 0.001914
  l2.weight: grad_norm = 0.095156
Total gradient norm: 0.293280
=== Actor Training Debug (Iteration 7178) ===
Q mean: -14.845100
Q std: 20.969713
Actor loss: 14.849062
Action reg: 0.003961
  l1.weight: grad_norm = 0.151613
  l1.bias: grad_norm = 0.000920
  l2.weight: grad_norm = 0.127492
Total gradient norm: 0.380616
=== Actor Training Debug (Iteration 7179) ===
Q mean: -15.008766
Q std: 20.964813
Actor loss: 15.012748
Action reg: 0.003982
  l1.weight: grad_norm = 0.169614
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.119650
Total gradient norm: 0.318929
=== Actor Training Debug (Iteration 7180) ===
Q mean: -12.826657
Q std: 19.595119
Actor loss: 12.830622
Action reg: 0.003965
  l1.weight: grad_norm = 0.189218
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.140102
Total gradient norm: 0.387507
=== Actor Training Debug (Iteration 7181) ===
Q mean: -18.314421
Q std: 24.034163
Actor loss: 18.318386
Action reg: 0.003965
  l1.weight: grad_norm = 0.180740
  l1.bias: grad_norm = 0.001296
  l2.weight: grad_norm = 0.143530
Total gradient norm: 0.465271
=== Actor Training Debug (Iteration 7182) ===
Q mean: -15.975399
Q std: 22.768814
Actor loss: 15.979365
Action reg: 0.003966
  l1.weight: grad_norm = 0.097748
  l1.bias: grad_norm = 0.000957
  l2.weight: grad_norm = 0.077041
Total gradient norm: 0.231428
=== Actor Training Debug (Iteration 7183) ===
Q mean: -14.596514
Q std: 20.327793
Actor loss: 14.600470
Action reg: 0.003956
  l1.weight: grad_norm = 0.256341
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.235240
Total gradient norm: 0.649305
=== Actor Training Debug (Iteration 7184) ===
Q mean: -13.136960
Q std: 19.868885
Actor loss: 13.140909
Action reg: 0.003949
  l1.weight: grad_norm = 0.158061
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.131261
Total gradient norm: 0.365173
=== Actor Training Debug (Iteration 7185) ===
Q mean: -14.592721
Q std: 21.710035
Actor loss: 14.596677
Action reg: 0.003956
  l1.weight: grad_norm = 0.459840
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.349073
Total gradient norm: 1.017783
=== Actor Training Debug (Iteration 7186) ===
Q mean: -12.369821
Q std: 19.818348
Actor loss: 12.373778
Action reg: 0.003958
  l1.weight: grad_norm = 0.131539
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.091796
Total gradient norm: 0.269792
=== Actor Training Debug (Iteration 7187) ===
Q mean: -15.251102
Q std: 22.428164
Actor loss: 15.255042
Action reg: 0.003939
  l1.weight: grad_norm = 0.179903
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.147887
Total gradient norm: 0.394568
=== Actor Training Debug (Iteration 7188) ===
Q mean: -13.371422
Q std: 20.663183
Actor loss: 13.375376
Action reg: 0.003954
  l1.weight: grad_norm = 0.224917
  l1.bias: grad_norm = 0.002540
  l2.weight: grad_norm = 0.172222
Total gradient norm: 0.476522
=== Actor Training Debug (Iteration 7189) ===
Q mean: -11.459404
Q std: 19.098072
Actor loss: 11.463322
Action reg: 0.003918
  l1.weight: grad_norm = 0.196330
  l1.bias: grad_norm = 0.002599
  l2.weight: grad_norm = 0.147728
Total gradient norm: 0.504615
=== Actor Training Debug (Iteration 7190) ===
Q mean: -13.695350
Q std: 20.600845
Actor loss: 13.699328
Action reg: 0.003979
  l1.weight: grad_norm = 0.289757
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.194176
Total gradient norm: 0.513322
=== Actor Training Debug (Iteration 7191) ===
Q mean: -12.404897
Q std: 19.804920
Actor loss: 12.408869
Action reg: 0.003972
  l1.weight: grad_norm = 0.269415
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.199540
Total gradient norm: 0.500491
=== Actor Training Debug (Iteration 7192) ===
Q mean: -13.809151
Q std: 20.710293
Actor loss: 13.813124
Action reg: 0.003973
  l1.weight: grad_norm = 0.123051
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.103700
Total gradient norm: 0.307775
=== Actor Training Debug (Iteration 7193) ===
Q mean: -15.834024
Q std: 21.539213
Actor loss: 15.837982
Action reg: 0.003957
  l1.weight: grad_norm = 0.224820
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.185062
Total gradient norm: 0.516318
=== Actor Training Debug (Iteration 7194) ===
Q mean: -13.692677
Q std: 20.465279
Actor loss: 13.696659
Action reg: 0.003982
  l1.weight: grad_norm = 0.097796
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.069227
Total gradient norm: 0.197127
=== Actor Training Debug (Iteration 7195) ===
Q mean: -14.933037
Q std: 22.747080
Actor loss: 14.937004
Action reg: 0.003968
  l1.weight: grad_norm = 0.146029
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.140502
Total gradient norm: 0.387762
=== Actor Training Debug (Iteration 7196) ===
Q mean: -15.407990
Q std: 22.175930
Actor loss: 15.411935
Action reg: 0.003945
  l1.weight: grad_norm = 0.098503
  l1.bias: grad_norm = 0.000681
  l2.weight: grad_norm = 0.067272
Total gradient norm: 0.182466
=== Actor Training Debug (Iteration 7197) ===
Q mean: -15.775957
Q std: 22.001549
Actor loss: 15.779928
Action reg: 0.003971
  l1.weight: grad_norm = 0.547658
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.354524
Total gradient norm: 0.964549
=== Actor Training Debug (Iteration 7198) ===
Q mean: -14.303926
Q std: 20.161888
Actor loss: 14.307890
Action reg: 0.003964
  l1.weight: grad_norm = 0.207583
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.152623
Total gradient norm: 0.469958
=== Actor Training Debug (Iteration 7199) ===
Q mean: -14.791711
Q std: 21.724483
Actor loss: 14.795666
Action reg: 0.003955
  l1.weight: grad_norm = 0.191846
  l1.bias: grad_norm = 0.001697
  l2.weight: grad_norm = 0.149982
Total gradient norm: 0.381689
=== Actor Training Debug (Iteration 7200) ===
Q mean: -15.647392
Q std: 21.495228
Actor loss: 15.651360
Action reg: 0.003968
  l1.weight: grad_norm = 0.147565
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.107061
Total gradient norm: 0.340563
=== Actor Training Debug (Iteration 7201) ===
Q mean: -12.904381
Q std: 19.920774
Actor loss: 12.908339
Action reg: 0.003958
  l1.weight: grad_norm = 0.216454
  l1.bias: grad_norm = 0.000498
  l2.weight: grad_norm = 0.169900
Total gradient norm: 0.446581
=== Actor Training Debug (Iteration 7202) ===
Q mean: -13.842182
Q std: 20.463249
Actor loss: 13.846155
Action reg: 0.003973
  l1.weight: grad_norm = 0.267455
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.174404
Total gradient norm: 0.466890
=== Actor Training Debug (Iteration 7203) ===
Q mean: -13.276527
Q std: 20.356958
Actor loss: 13.280499
Action reg: 0.003972
  l1.weight: grad_norm = 0.185910
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.155468
Total gradient norm: 0.493242
=== Actor Training Debug (Iteration 7204) ===
Q mean: -14.693115
Q std: 21.883987
Actor loss: 14.697074
Action reg: 0.003958
  l1.weight: grad_norm = 0.207019
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.150459
Total gradient norm: 0.450884
=== Actor Training Debug (Iteration 7205) ===
Q mean: -15.012175
Q std: 21.113617
Actor loss: 15.016147
Action reg: 0.003972
  l1.weight: grad_norm = 0.141888
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.098224
Total gradient norm: 0.284789
=== Actor Training Debug (Iteration 7206) ===
Q mean: -12.994710
Q std: 20.015812
Actor loss: 12.998666
Action reg: 0.003956
  l1.weight: grad_norm = 0.186142
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.147426
Total gradient norm: 0.374997
=== Actor Training Debug (Iteration 7207) ===
Q mean: -13.230087
Q std: 19.220842
Actor loss: 13.234042
Action reg: 0.003955
  l1.weight: grad_norm = 0.157348
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.130314
Total gradient norm: 0.369630
=== Actor Training Debug (Iteration 7208) ===
Q mean: -12.098318
Q std: 20.085497
Actor loss: 12.102275
Action reg: 0.003957
  l1.weight: grad_norm = 0.212174
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.155139
Total gradient norm: 0.458915
=== Actor Training Debug (Iteration 7209) ===
Q mean: -16.911823
Q std: 23.499708
Actor loss: 16.915791
Action reg: 0.003967
  l1.weight: grad_norm = 0.309768
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.262910
Total gradient norm: 0.819274
=== Actor Training Debug (Iteration 7210) ===
Q mean: -14.564575
Q std: 20.883663
Actor loss: 14.568537
Action reg: 0.003961
  l1.weight: grad_norm = 0.314646
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.293586
Total gradient norm: 0.995361
=== Actor Training Debug (Iteration 7211) ===
Q mean: -14.826594
Q std: 21.266289
Actor loss: 14.830566
Action reg: 0.003972
  l1.weight: grad_norm = 0.084301
  l1.bias: grad_norm = 0.000454
  l2.weight: grad_norm = 0.065696
Total gradient norm: 0.175023
=== Actor Training Debug (Iteration 7212) ===
Q mean: -11.284279
Q std: 19.308640
Actor loss: 11.288238
Action reg: 0.003958
  l1.weight: grad_norm = 0.117571
  l1.bias: grad_norm = 0.000994
  l2.weight: grad_norm = 0.091180
Total gradient norm: 0.280073
=== Actor Training Debug (Iteration 7213) ===
Q mean: -15.396612
Q std: 21.381365
Actor loss: 15.400595
Action reg: 0.003983
  l1.weight: grad_norm = 0.121758
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.098578
Total gradient norm: 0.273936
=== Actor Training Debug (Iteration 7214) ===
Q mean: -13.845257
Q std: 20.139708
Actor loss: 13.849216
Action reg: 0.003960
  l1.weight: grad_norm = 0.144015
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.110143
Total gradient norm: 0.290292
=== Actor Training Debug (Iteration 7215) ===
Q mean: -14.241095
Q std: 20.210320
Actor loss: 14.245061
Action reg: 0.003966
  l1.weight: grad_norm = 0.090202
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.065402
Total gradient norm: 0.182719
=== Actor Training Debug (Iteration 7216) ===
Q mean: -13.652228
Q std: 20.022636
Actor loss: 13.656202
Action reg: 0.003974
  l1.weight: grad_norm = 0.113619
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.090601
Total gradient norm: 0.254231
=== Actor Training Debug (Iteration 7217) ===
Q mean: -15.312402
Q std: 21.992596
Actor loss: 15.316370
Action reg: 0.003968
  l1.weight: grad_norm = 0.246698
  l1.bias: grad_norm = 0.001084
  l2.weight: grad_norm = 0.191122
Total gradient norm: 0.513065
=== Actor Training Debug (Iteration 7218) ===
Q mean: -15.941664
Q std: 21.916361
Actor loss: 15.945623
Action reg: 0.003960
  l1.weight: grad_norm = 0.135042
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.110581
Total gradient norm: 0.285397
=== Actor Training Debug (Iteration 7219) ===
Q mean: -13.048656
Q std: 19.721285
Actor loss: 13.052620
Action reg: 0.003964
  l1.weight: grad_norm = 0.294772
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.202923
Total gradient norm: 0.577294
=== Actor Training Debug (Iteration 7220) ===
Q mean: -14.026863
Q std: 21.203087
Actor loss: 14.030790
Action reg: 0.003927
  l1.weight: grad_norm = 0.179088
  l1.bias: grad_norm = 0.000589
  l2.weight: grad_norm = 0.126949
Total gradient norm: 0.402036
=== Actor Training Debug (Iteration 7221) ===
Q mean: -16.123478
Q std: 22.410580
Actor loss: 16.127441
Action reg: 0.003963
  l1.weight: grad_norm = 0.182066
  l1.bias: grad_norm = 0.000507
  l2.weight: grad_norm = 0.125918
Total gradient norm: 0.330719
=== Actor Training Debug (Iteration 7222) ===
Q mean: -14.459010
Q std: 20.588852
Actor loss: 14.462960
Action reg: 0.003950
  l1.weight: grad_norm = 0.204012
  l1.bias: grad_norm = 0.000814
  l2.weight: grad_norm = 0.157227
Total gradient norm: 0.476717
=== Actor Training Debug (Iteration 7223) ===
Q mean: -14.081532
Q std: 21.573408
Actor loss: 14.085488
Action reg: 0.003957
  l1.weight: grad_norm = 0.164451
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.122850
Total gradient norm: 0.358339
=== Actor Training Debug (Iteration 7224) ===
Q mean: -13.794891
Q std: 20.691261
Actor loss: 13.798848
Action reg: 0.003957
  l1.weight: grad_norm = 0.196065
  l1.bias: grad_norm = 0.001976
  l2.weight: grad_norm = 0.123846
Total gradient norm: 0.369550
=== Actor Training Debug (Iteration 7225) ===
Q mean: -13.210217
Q std: 20.818342
Actor loss: 13.214187
Action reg: 0.003969
  l1.weight: grad_norm = 0.236229
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.167042
Total gradient norm: 0.457691
=== Actor Training Debug (Iteration 7226) ===
Q mean: -13.680430
Q std: 21.535351
Actor loss: 13.684394
Action reg: 0.003964
  l1.weight: grad_norm = 0.207949
  l1.bias: grad_norm = 0.000842
  l2.weight: grad_norm = 0.144138
Total gradient norm: 0.443360
=== Actor Training Debug (Iteration 7227) ===
Q mean: -13.596149
Q std: 21.113304
Actor loss: 13.600117
Action reg: 0.003967
  l1.weight: grad_norm = 0.178597
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.132046
Total gradient norm: 0.374298
=== Actor Training Debug (Iteration 7228) ===
Q mean: -14.371830
Q std: 19.713421
Actor loss: 14.375804
Action reg: 0.003974
  l1.weight: grad_norm = 0.189309
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.142843
Total gradient norm: 0.415034
=== Actor Training Debug (Iteration 7229) ===
Q mean: -15.198306
Q std: 21.751345
Actor loss: 15.202266
Action reg: 0.003959
  l1.weight: grad_norm = 0.086477
  l1.bias: grad_norm = 0.001114
  l2.weight: grad_norm = 0.068071
Total gradient norm: 0.187765
=== Actor Training Debug (Iteration 7230) ===
Q mean: -11.916236
Q std: 18.060295
Actor loss: 11.920196
Action reg: 0.003960
  l1.weight: grad_norm = 0.313083
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.227558
Total gradient norm: 0.629115
=== Actor Training Debug (Iteration 7231) ===
Q mean: -13.815042
Q std: 20.870167
Actor loss: 13.818995
Action reg: 0.003954
  l1.weight: grad_norm = 0.211234
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.159192
Total gradient norm: 0.500165
=== Actor Training Debug (Iteration 7232) ===
Q mean: -13.947953
Q std: 19.914192
Actor loss: 13.951911
Action reg: 0.003957
  l1.weight: grad_norm = 0.373626
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.271102
Total gradient norm: 0.787366
=== Actor Training Debug (Iteration 7233) ===
Q mean: -12.380035
Q std: 20.222067
Actor loss: 12.383963
Action reg: 0.003927
  l1.weight: grad_norm = 0.120719
  l1.bias: grad_norm = 0.001458
  l2.weight: grad_norm = 0.088369
Total gradient norm: 0.234491
=== Actor Training Debug (Iteration 7234) ===
Q mean: -14.660778
Q std: 20.885973
Actor loss: 14.664753
Action reg: 0.003975
  l1.weight: grad_norm = 0.273121
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.229550
Total gradient norm: 0.626477
=== Actor Training Debug (Iteration 7235) ===
Q mean: -13.868459
Q std: 20.122643
Actor loss: 13.872434
Action reg: 0.003975
  l1.weight: grad_norm = 0.083174
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.068611
Total gradient norm: 0.219879
=== Actor Training Debug (Iteration 7236) ===
Q mean: -14.852732
Q std: 21.403202
Actor loss: 14.856702
Action reg: 0.003970
  l1.weight: grad_norm = 0.323034
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.246927
Total gradient norm: 0.591385
=== Actor Training Debug (Iteration 7237) ===
Q mean: -12.930573
Q std: 20.543924
Actor loss: 12.934537
Action reg: 0.003964
  l1.weight: grad_norm = 0.179088
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.128348
Total gradient norm: 0.394437
=== Actor Training Debug (Iteration 7238) ===
Q mean: -16.222923
Q std: 21.748314
Actor loss: 16.226883
Action reg: 0.003959
  l1.weight: grad_norm = 0.210627
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.146885
Total gradient norm: 0.464560
=== Actor Training Debug (Iteration 7239) ===
Q mean: -15.101847
Q std: 20.950418
Actor loss: 15.105827
Action reg: 0.003981
  l1.weight: grad_norm = 0.191459
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.123380
Total gradient norm: 0.328122
=== Actor Training Debug (Iteration 7240) ===
Q mean: -13.741415
Q std: 20.568941
Actor loss: 13.745393
Action reg: 0.003978
  l1.weight: grad_norm = 0.168680
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.128515
Total gradient norm: 0.355726
=== Actor Training Debug (Iteration 7241) ===
Q mean: -12.488047
Q std: 19.952187
Actor loss: 12.492013
Action reg: 0.003966
  l1.weight: grad_norm = 0.329197
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.234179
Total gradient norm: 0.664087
=== Actor Training Debug (Iteration 7242) ===
Q mean: -14.796128
Q std: 22.834074
Actor loss: 14.800097
Action reg: 0.003970
  l1.weight: grad_norm = 0.298213
  l1.bias: grad_norm = 0.001930
  l2.weight: grad_norm = 0.280190
Total gradient norm: 0.871800
=== Actor Training Debug (Iteration 7243) ===
Q mean: -12.467699
Q std: 21.538483
Actor loss: 12.471661
Action reg: 0.003961
  l1.weight: grad_norm = 0.153864
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.119725
Total gradient norm: 0.338084
=== Actor Training Debug (Iteration 7244) ===
Q mean: -15.950603
Q std: 22.652512
Actor loss: 15.954555
Action reg: 0.003951
  l1.weight: grad_norm = 0.160039
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.123137
Total gradient norm: 0.339137
=== Actor Training Debug (Iteration 7245) ===
Q mean: -11.880840
Q std: 19.183779
Actor loss: 11.884786
Action reg: 0.003945
  l1.weight: grad_norm = 0.163190
  l1.bias: grad_norm = 0.000610
  l2.weight: grad_norm = 0.121136
Total gradient norm: 0.311215
=== Actor Training Debug (Iteration 7246) ===
Q mean: -16.627787
Q std: 22.347668
Actor loss: 16.631721
Action reg: 0.003935
  l1.weight: grad_norm = 0.297978
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.215620
Total gradient norm: 0.743954
=== Actor Training Debug (Iteration 7247) ===
Q mean: -15.655039
Q std: 21.666859
Actor loss: 15.658987
Action reg: 0.003948
  l1.weight: grad_norm = 0.253790
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.195083
Total gradient norm: 0.609352
=== Actor Training Debug (Iteration 7248) ===
Q mean: -12.830669
Q std: 20.468704
Actor loss: 12.834612
Action reg: 0.003943
  l1.weight: grad_norm = 0.135598
  l1.bias: grad_norm = 0.000610
  l2.weight: grad_norm = 0.097241
Total gradient norm: 0.260414
=== Actor Training Debug (Iteration 7249) ===
Q mean: -12.714933
Q std: 20.893581
Actor loss: 12.718895
Action reg: 0.003962
  l1.weight: grad_norm = 0.210494
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.155535
Total gradient norm: 0.443564
=== Actor Training Debug (Iteration 7250) ===
Q mean: -15.126986
Q std: 22.124426
Actor loss: 15.130949
Action reg: 0.003964
  l1.weight: grad_norm = 0.193957
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.153913
Total gradient norm: 0.417410
=== Actor Training Debug (Iteration 7251) ===
Q mean: -15.680950
Q std: 21.485502
Actor loss: 15.684895
Action reg: 0.003945
  l1.weight: grad_norm = 0.165126
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.126665
Total gradient norm: 0.346312
=== Actor Training Debug (Iteration 7252) ===
Q mean: -13.613788
Q std: 19.555893
Actor loss: 13.617773
Action reg: 0.003986
  l1.weight: grad_norm = 0.069091
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.050363
Total gradient norm: 0.142874
=== Actor Training Debug (Iteration 7253) ===
Q mean: -13.009546
Q std: 20.418816
Actor loss: 13.013526
Action reg: 0.003980
  l1.weight: grad_norm = 0.061049
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.052083
Total gradient norm: 0.149479
=== Actor Training Debug (Iteration 7254) ===
Q mean: -15.275145
Q std: 22.196480
Actor loss: 15.279098
Action reg: 0.003953
  l1.weight: grad_norm = 0.173272
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.120497
Total gradient norm: 0.320484
=== Actor Training Debug (Iteration 7255) ===
Q mean: -13.834846
Q std: 20.711868
Actor loss: 13.838811
Action reg: 0.003964
  l1.weight: grad_norm = 0.233338
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.154774
Total gradient norm: 0.471146
=== Actor Training Debug (Iteration 7256) ===
Q mean: -13.429699
Q std: 20.537947
Actor loss: 13.433668
Action reg: 0.003969
  l1.weight: grad_norm = 0.159137
  l1.bias: grad_norm = 0.000589
  l2.weight: grad_norm = 0.123929
Total gradient norm: 0.391060
=== Actor Training Debug (Iteration 7257) ===
Q mean: -14.941095
Q std: 21.576492
Actor loss: 14.945070
Action reg: 0.003975
  l1.weight: grad_norm = 0.178436
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.127987
Total gradient norm: 0.314804
=== Actor Training Debug (Iteration 7258) ===
Q mean: -14.214720
Q std: 21.694294
Actor loss: 14.218697
Action reg: 0.003977
  l1.weight: grad_norm = 0.209504
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.159508
Total gradient norm: 0.449110
=== Actor Training Debug (Iteration 7259) ===
Q mean: -14.372585
Q std: 21.714430
Actor loss: 14.376554
Action reg: 0.003970
  l1.weight: grad_norm = 0.100638
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.099328
Total gradient norm: 0.287763
=== Actor Training Debug (Iteration 7260) ===
Q mean: -12.083705
Q std: 19.654198
Actor loss: 12.087667
Action reg: 0.003963
  l1.weight: grad_norm = 0.242872
  l1.bias: grad_norm = 0.001953
  l2.weight: grad_norm = 0.180679
Total gradient norm: 0.644392
=== Actor Training Debug (Iteration 7261) ===
Q mean: -15.294652
Q std: 22.455091
Actor loss: 15.298622
Action reg: 0.003970
  l1.weight: grad_norm = 0.169053
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.131524
Total gradient norm: 0.411107
=== Actor Training Debug (Iteration 7262) ===
Q mean: -16.972933
Q std: 22.557364
Actor loss: 16.976891
Action reg: 0.003957
  l1.weight: grad_norm = 0.170516
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.130408
Total gradient norm: 0.398285
=== Actor Training Debug (Iteration 7263) ===
Q mean: -14.437718
Q std: 19.522724
Actor loss: 14.441687
Action reg: 0.003968
  l1.weight: grad_norm = 0.081372
  l1.bias: grad_norm = 0.001406
  l2.weight: grad_norm = 0.058678
Total gradient norm: 0.165265
=== Actor Training Debug (Iteration 7264) ===
Q mean: -11.382915
Q std: 18.787073
Actor loss: 11.386866
Action reg: 0.003951
  l1.weight: grad_norm = 0.230509
  l1.bias: grad_norm = 0.001692
  l2.weight: grad_norm = 0.198687
Total gradient norm: 0.583986
=== Actor Training Debug (Iteration 7265) ===
Q mean: -14.435568
Q std: 21.436300
Actor loss: 14.439493
Action reg: 0.003925
  l1.weight: grad_norm = 0.206164
  l1.bias: grad_norm = 0.001624
  l2.weight: grad_norm = 0.149008
Total gradient norm: 0.424827
=== Actor Training Debug (Iteration 7266) ===
Q mean: -13.588516
Q std: 19.564659
Actor loss: 13.592458
Action reg: 0.003941
  l1.weight: grad_norm = 0.122139
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.091427
Total gradient norm: 0.258633
=== Actor Training Debug (Iteration 7267) ===
Q mean: -15.654459
Q std: 22.267906
Actor loss: 15.658427
Action reg: 0.003968
  l1.weight: grad_norm = 0.199532
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.137450
Total gradient norm: 0.343533
=== Actor Training Debug (Iteration 7268) ===
Q mean: -14.436789
Q std: 21.448759
Actor loss: 14.440767
Action reg: 0.003978
  l1.weight: grad_norm = 0.088403
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.069118
Total gradient norm: 0.224724
=== Actor Training Debug (Iteration 7269) ===
Q mean: -12.537180
Q std: 19.232258
Actor loss: 12.541149
Action reg: 0.003969
  l1.weight: grad_norm = 0.149177
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.100635
Total gradient norm: 0.299383
=== Actor Training Debug (Iteration 7270) ===
Q mean: -15.540427
Q std: 22.678095
Actor loss: 15.544399
Action reg: 0.003972
  l1.weight: grad_norm = 0.050362
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.040523
Total gradient norm: 0.108517
=== Actor Training Debug (Iteration 7271) ===
Q mean: -14.192757
Q std: 20.745916
Actor loss: 14.196726
Action reg: 0.003969
  l1.weight: grad_norm = 0.171807
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.143714
Total gradient norm: 0.391007
=== Actor Training Debug (Iteration 7272) ===
Q mean: -12.430974
Q std: 19.676455
Actor loss: 12.434943
Action reg: 0.003969
  l1.weight: grad_norm = 0.148892
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.105694
Total gradient norm: 0.298720
=== Actor Training Debug (Iteration 7273) ===
Q mean: -14.624171
Q std: 20.916624
Actor loss: 14.628142
Action reg: 0.003971
  l1.weight: grad_norm = 0.206917
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.151657
Total gradient norm: 0.415314
=== Actor Training Debug (Iteration 7274) ===
Q mean: -13.711239
Q std: 20.316189
Actor loss: 13.715193
Action reg: 0.003954
  l1.weight: grad_norm = 0.142117
  l1.bias: grad_norm = 0.000923
  l2.weight: grad_norm = 0.112931
Total gradient norm: 0.312570
=== Actor Training Debug (Iteration 7275) ===
Q mean: -14.399931
Q std: 20.274281
Actor loss: 14.403893
Action reg: 0.003962
  l1.weight: grad_norm = 0.180313
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.129718
Total gradient norm: 0.391560
=== Actor Training Debug (Iteration 7276) ===
Q mean: -13.320814
Q std: 19.347717
Actor loss: 13.324760
Action reg: 0.003946
  l1.weight: grad_norm = 0.150549
  l1.bias: grad_norm = 0.002932
  l2.weight: grad_norm = 0.114904
Total gradient norm: 0.325259
=== Actor Training Debug (Iteration 7277) ===
Q mean: -12.179843
Q std: 19.249716
Actor loss: 12.183783
Action reg: 0.003939
  l1.weight: grad_norm = 0.136753
  l1.bias: grad_norm = 0.001718
  l2.weight: grad_norm = 0.102093
Total gradient norm: 0.280321
=== Actor Training Debug (Iteration 7278) ===
Q mean: -15.782761
Q std: 21.433300
Actor loss: 15.786716
Action reg: 0.003956
  l1.weight: grad_norm = 0.126331
  l1.bias: grad_norm = 0.002748
  l2.weight: grad_norm = 0.094638
Total gradient norm: 0.282018
=== Actor Training Debug (Iteration 7279) ===
Q mean: -15.030648
Q std: 21.719215
Actor loss: 15.034607
Action reg: 0.003959
  l1.weight: grad_norm = 0.155122
  l1.bias: grad_norm = 0.002779
  l2.weight: grad_norm = 0.100515
Total gradient norm: 0.313520
=== Actor Training Debug (Iteration 7280) ===
Q mean: -12.682732
Q std: 21.054573
Actor loss: 12.686692
Action reg: 0.003961
  l1.weight: grad_norm = 0.163376
  l1.bias: grad_norm = 0.000567
  l2.weight: grad_norm = 0.127806
Total gradient norm: 0.356466
=== Actor Training Debug (Iteration 7281) ===
Q mean: -15.095620
Q std: 21.227491
Actor loss: 15.099584
Action reg: 0.003963
  l1.weight: grad_norm = 0.265593
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.200432
Total gradient norm: 0.639554
=== Actor Training Debug (Iteration 7282) ===
Q mean: -13.796178
Q std: 20.131844
Actor loss: 13.800154
Action reg: 0.003976
  l1.weight: grad_norm = 0.290196
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.244778
Total gradient norm: 0.634699
=== Actor Training Debug (Iteration 7283) ===
Q mean: -14.031706
Q std: 20.369942
Actor loss: 14.035663
Action reg: 0.003956
  l1.weight: grad_norm = 0.138527
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.100807
Total gradient norm: 0.277874
=== Actor Training Debug (Iteration 7284) ===
Q mean: -15.067363
Q std: 21.463659
Actor loss: 15.071320
Action reg: 0.003957
  l1.weight: grad_norm = 0.258863
  l1.bias: grad_norm = 0.001064
  l2.weight: grad_norm = 0.199657
Total gradient norm: 0.575924
=== Actor Training Debug (Iteration 7285) ===
Q mean: -14.761889
Q std: 20.981291
Actor loss: 14.765847
Action reg: 0.003959
  l1.weight: grad_norm = 0.249420
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.176140
Total gradient norm: 0.529478
=== Actor Training Debug (Iteration 7286) ===
Q mean: -15.714350
Q std: 22.060684
Actor loss: 15.718321
Action reg: 0.003971
  l1.weight: grad_norm = 0.198694
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.137079
Total gradient norm: 0.392592
=== Actor Training Debug (Iteration 7287) ===
Q mean: -13.677483
Q std: 21.620155
Actor loss: 13.681431
Action reg: 0.003948
  l1.weight: grad_norm = 0.263752
  l1.bias: grad_norm = 0.000778
  l2.weight: grad_norm = 0.225886
Total gradient norm: 0.692567
=== Actor Training Debug (Iteration 7288) ===
Q mean: -13.511917
Q std: 21.756945
Actor loss: 13.515877
Action reg: 0.003959
  l1.weight: grad_norm = 0.213312
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.181699
Total gradient norm: 0.536193
=== Actor Training Debug (Iteration 7289) ===
Q mean: -13.849136
Q std: 20.703686
Actor loss: 13.853091
Action reg: 0.003955
  l1.weight: grad_norm = 0.116344
  l1.bias: grad_norm = 0.001253
  l2.weight: grad_norm = 0.099738
Total gradient norm: 0.301806
=== Actor Training Debug (Iteration 7290) ===
Q mean: -13.786303
Q std: 21.144676
Actor loss: 13.790237
Action reg: 0.003935
  l1.weight: grad_norm = 0.175645
  l1.bias: grad_norm = 0.001743
  l2.weight: grad_norm = 0.137842
Total gradient norm: 0.371837
=== Actor Training Debug (Iteration 7291) ===
Q mean: -14.590629
Q std: 21.563440
Actor loss: 14.594584
Action reg: 0.003956
  l1.weight: grad_norm = 0.190430
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.163805
Total gradient norm: 0.465574
=== Actor Training Debug (Iteration 7292) ===
Q mean: -13.294692
Q std: 18.552937
Actor loss: 13.298656
Action reg: 0.003964
  l1.weight: grad_norm = 0.200694
  l1.bias: grad_norm = 0.001462
  l2.weight: grad_norm = 0.140798
Total gradient norm: 0.427191
=== Actor Training Debug (Iteration 7293) ===
Q mean: -17.252598
Q std: 22.446178
Actor loss: 17.256552
Action reg: 0.003953
  l1.weight: grad_norm = 0.232945
  l1.bias: grad_norm = 0.001519
  l2.weight: grad_norm = 0.187557
Total gradient norm: 0.555602
=== Actor Training Debug (Iteration 7294) ===
Q mean: -15.813446
Q std: 22.627834
Actor loss: 15.817423
Action reg: 0.003977
  l1.weight: grad_norm = 0.116470
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.080382
Total gradient norm: 0.221260
=== Actor Training Debug (Iteration 7295) ===
Q mean: -13.116049
Q std: 20.289970
Actor loss: 13.120006
Action reg: 0.003957
  l1.weight: grad_norm = 0.150188
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.117591
Total gradient norm: 0.365822
=== Actor Training Debug (Iteration 7296) ===
Q mean: -13.291617
Q std: 19.189873
Actor loss: 13.295577
Action reg: 0.003960
  l1.weight: grad_norm = 0.184758
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.129620
Total gradient norm: 0.380990
=== Actor Training Debug (Iteration 7297) ===
Q mean: -14.299969
Q std: 20.154202
Actor loss: 14.303929
Action reg: 0.003960
  l1.weight: grad_norm = 0.288010
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.200706
Total gradient norm: 0.514992
=== Actor Training Debug (Iteration 7298) ===
Q mean: -13.441021
Q std: 19.118799
Actor loss: 13.445004
Action reg: 0.003982
  l1.weight: grad_norm = 0.228121
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.169096
Total gradient norm: 0.579612
=== Actor Training Debug (Iteration 7299) ===
Q mean: -12.635702
Q std: 19.887104
Actor loss: 12.639688
Action reg: 0.003986
  l1.weight: grad_norm = 0.134172
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.113473
Total gradient norm: 0.291206
=== Actor Training Debug (Iteration 7300) ===
Q mean: -14.125475
Q std: 19.725119
Actor loss: 14.129450
Action reg: 0.003975
  l1.weight: grad_norm = 0.233657
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.169734
Total gradient norm: 0.458706
=== Actor Training Debug (Iteration 7301) ===
Q mean: -16.566414
Q std: 22.198256
Actor loss: 16.570377
Action reg: 0.003964
  l1.weight: grad_norm = 0.082057
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.065959
Total gradient norm: 0.190527
=== Actor Training Debug (Iteration 7302) ===
Q mean: -14.471625
Q std: 22.060703
Actor loss: 14.475604
Action reg: 0.003979
  l1.weight: grad_norm = 0.116019
  l1.bias: grad_norm = 0.001085
  l2.weight: grad_norm = 0.089689
Total gradient norm: 0.269699
=== Actor Training Debug (Iteration 7303) ===
Q mean: -11.789161
Q std: 18.919434
Actor loss: 11.793105
Action reg: 0.003944
  l1.weight: grad_norm = 0.263080
  l1.bias: grad_norm = 0.000787
  l2.weight: grad_norm = 0.193180
Total gradient norm: 0.569084
=== Actor Training Debug (Iteration 7304) ===
Q mean: -15.677349
Q std: 21.135595
Actor loss: 15.681314
Action reg: 0.003965
  l1.weight: grad_norm = 0.163359
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.132991
Total gradient norm: 0.373260
=== Actor Training Debug (Iteration 7305) ===
Q mean: -13.274727
Q std: 19.759655
Actor loss: 13.278686
Action reg: 0.003959
  l1.weight: grad_norm = 0.072535
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.057458
Total gradient norm: 0.168225
=== Actor Training Debug (Iteration 7306) ===
Q mean: -13.166647
Q std: 19.893034
Actor loss: 13.170598
Action reg: 0.003951
  l1.weight: grad_norm = 0.328034
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.245556
Total gradient norm: 0.672296
=== Actor Training Debug (Iteration 7307) ===
Q mean: -13.010027
Q std: 19.031389
Actor loss: 13.013983
Action reg: 0.003956
  l1.weight: grad_norm = 0.212741
  l1.bias: grad_norm = 0.000635
  l2.weight: grad_norm = 0.137908
Total gradient norm: 0.406400
=== Actor Training Debug (Iteration 7308) ===
Q mean: -13.974134
Q std: 21.432106
Actor loss: 13.978069
Action reg: 0.003935
  l1.weight: grad_norm = 0.308091
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.275738
Total gradient norm: 0.796121
=== Actor Training Debug (Iteration 7309) ===
Q mean: -14.993969
Q std: 20.544317
Actor loss: 14.997947
Action reg: 0.003978
  l1.weight: grad_norm = 0.100491
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.079627
Total gradient norm: 0.246175
=== Actor Training Debug (Iteration 7310) ===
Q mean: -12.532444
Q std: 20.246534
Actor loss: 12.536416
Action reg: 0.003972
  l1.weight: grad_norm = 0.210839
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.158316
Total gradient norm: 0.408293
=== Actor Training Debug (Iteration 7311) ===
Q mean: -13.885270
Q std: 20.917812
Actor loss: 13.889222
Action reg: 0.003952
  l1.weight: grad_norm = 0.144514
  l1.bias: grad_norm = 0.001603
  l2.weight: grad_norm = 0.109207
Total gradient norm: 0.288339
=== Actor Training Debug (Iteration 7312) ===
Q mean: -11.958220
Q std: 19.553165
Actor loss: 11.962179
Action reg: 0.003959
  l1.weight: grad_norm = 0.203681
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.145739
Total gradient norm: 0.419514
=== Actor Training Debug (Iteration 7313) ===
Q mean: -17.171438
Q std: 21.257442
Actor loss: 17.175381
Action reg: 0.003942
  l1.weight: grad_norm = 0.119996
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.083951
Total gradient norm: 0.282605
=== Actor Training Debug (Iteration 7314) ===
Q mean: -15.038359
Q std: 20.682901
Actor loss: 15.042315
Action reg: 0.003956
  l1.weight: grad_norm = 0.282436
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.200923
Total gradient norm: 0.556756
=== Actor Training Debug (Iteration 7315) ===
Q mean: -16.180298
Q std: 21.823402
Actor loss: 16.184267
Action reg: 0.003969
  l1.weight: grad_norm = 0.159912
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.113261
Total gradient norm: 0.297748
=== Actor Training Debug (Iteration 7316) ===
Q mean: -12.722873
Q std: 21.024349
Actor loss: 12.726852
Action reg: 0.003979
  l1.weight: grad_norm = 0.146239
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.093875
Total gradient norm: 0.317841
=== Actor Training Debug (Iteration 7317) ===
Q mean: -15.867172
Q std: 22.752733
Actor loss: 15.871152
Action reg: 0.003980
  l1.weight: grad_norm = 0.174456
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.132533
Total gradient norm: 0.364552
=== Actor Training Debug (Iteration 7318) ===
Q mean: -12.667557
Q std: 18.578611
Actor loss: 12.671518
Action reg: 0.003961
  l1.weight: grad_norm = 0.147011
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.109153
Total gradient norm: 0.307752
=== Actor Training Debug (Iteration 7319) ===
Q mean: -15.007542
Q std: 20.352180
Actor loss: 15.011517
Action reg: 0.003975
  l1.weight: grad_norm = 0.244087
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.176018
Total gradient norm: 0.524063
=== Actor Training Debug (Iteration 7320) ===
Q mean: -11.843536
Q std: 18.819139
Actor loss: 11.847502
Action reg: 0.003965
  l1.weight: grad_norm = 0.183938
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.145540
Total gradient norm: 0.385223
=== Actor Training Debug (Iteration 7321) ===
Q mean: -12.267498
Q std: 20.426580
Actor loss: 12.271451
Action reg: 0.003953
  l1.weight: grad_norm = 0.188407
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.130979
Total gradient norm: 0.397447
=== Actor Training Debug (Iteration 7322) ===
Q mean: -11.629753
Q std: 19.507792
Actor loss: 11.633725
Action reg: 0.003972
  l1.weight: grad_norm = 0.242367
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.180841
Total gradient norm: 0.477743
=== Actor Training Debug (Iteration 7323) ===
Q mean: -12.752919
Q std: 19.826847
Actor loss: 12.756894
Action reg: 0.003975
  l1.weight: grad_norm = 0.265923
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.219180
Total gradient norm: 0.531498
=== Actor Training Debug (Iteration 7324) ===
Q mean: -14.332751
Q std: 20.373642
Actor loss: 14.336709
Action reg: 0.003958
  l1.weight: grad_norm = 0.211735
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.170084
Total gradient norm: 0.519095
=== Actor Training Debug (Iteration 7325) ===
Q mean: -13.507387
Q std: 19.166210
Actor loss: 13.511350
Action reg: 0.003962
  l1.weight: grad_norm = 0.233906
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.177854
Total gradient norm: 0.479616
=== Actor Training Debug (Iteration 7326) ===
Q mean: -13.830408
Q std: 20.238319
Actor loss: 13.834373
Action reg: 0.003964
  l1.weight: grad_norm = 0.265155
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.197410
Total gradient norm: 0.569940
=== Actor Training Debug (Iteration 7327) ===
Q mean: -12.737179
Q std: 20.831841
Actor loss: 12.741152
Action reg: 0.003973
  l1.weight: grad_norm = 0.151775
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.101681
Total gradient norm: 0.293479
=== Actor Training Debug (Iteration 7328) ===
Q mean: -13.981907
Q std: 21.273806
Actor loss: 13.985874
Action reg: 0.003967
  l1.weight: grad_norm = 0.389677
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.381448
Total gradient norm: 1.215163
=== Actor Training Debug (Iteration 7329) ===
Q mean: -14.060514
Q std: 20.827526
Actor loss: 14.064466
Action reg: 0.003952
  l1.weight: grad_norm = 0.174374
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.140311
Total gradient norm: 0.433820
=== Actor Training Debug (Iteration 7330) ===
Q mean: -13.376040
Q std: 20.457020
Actor loss: 13.379996
Action reg: 0.003957
  l1.weight: grad_norm = 0.116046
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.094585
Total gradient norm: 0.304387
=== Actor Training Debug (Iteration 7331) ===
Q mean: -12.629988
Q std: 20.560463
Actor loss: 12.633949
Action reg: 0.003961
  l1.weight: grad_norm = 0.107112
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.092666
Total gradient norm: 0.272249
=== Actor Training Debug (Iteration 7332) ===
Q mean: -13.529507
Q std: 20.251411
Actor loss: 13.533486
Action reg: 0.003980
  l1.weight: grad_norm = 0.116577
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.082398
Total gradient norm: 0.267735
=== Actor Training Debug (Iteration 7333) ===
Q mean: -13.118329
Q std: 20.091072
Actor loss: 13.122293
Action reg: 0.003964
  l1.weight: grad_norm = 0.411866
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.370400
Total gradient norm: 1.254059
=== Actor Training Debug (Iteration 7334) ===
Q mean: -14.006029
Q std: 20.302111
Actor loss: 14.010007
Action reg: 0.003978
  l1.weight: grad_norm = 0.236054
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.159774
Total gradient norm: 0.432677
=== Actor Training Debug (Iteration 7335) ===
Q mean: -13.439375
Q std: 21.047892
Actor loss: 13.443341
Action reg: 0.003966
  l1.weight: grad_norm = 0.225124
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.155727
Total gradient norm: 0.449818
=== Actor Training Debug (Iteration 7336) ===
Q mean: -14.886721
Q std: 22.195709
Actor loss: 14.890675
Action reg: 0.003954
  l1.weight: grad_norm = 0.283955
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.190864
Total gradient norm: 0.577450
=== Actor Training Debug (Iteration 7337) ===
Q mean: -15.732641
Q std: 21.970224
Actor loss: 15.736596
Action reg: 0.003955
  l1.weight: grad_norm = 0.121723
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.094467
Total gradient norm: 0.254495
=== Actor Training Debug (Iteration 7338) ===
Q mean: -14.876087
Q std: 21.786245
Actor loss: 14.880050
Action reg: 0.003962
  l1.weight: grad_norm = 0.162918
  l1.bias: grad_norm = 0.003046
  l2.weight: grad_norm = 0.118201
Total gradient norm: 0.367238
=== Actor Training Debug (Iteration 7339) ===
Q mean: -13.554834
Q std: 19.207159
Actor loss: 13.558805
Action reg: 0.003971
  l1.weight: grad_norm = 0.126906
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.083840
Total gradient norm: 0.234231
=== Actor Training Debug (Iteration 7340) ===
Q mean: -14.677565
Q std: 23.135664
Actor loss: 14.681512
Action reg: 0.003947
  l1.weight: grad_norm = 0.183064
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.140140
Total gradient norm: 0.377489
=== Actor Training Debug (Iteration 7341) ===
Q mean: -13.978828
Q std: 21.224659
Actor loss: 13.982800
Action reg: 0.003971
  l1.weight: grad_norm = 0.674827
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.458792
Total gradient norm: 1.498822
=== Actor Training Debug (Iteration 7342) ===
Q mean: -14.934347
Q std: 21.423820
Actor loss: 14.938326
Action reg: 0.003979
  l1.weight: grad_norm = 0.159432
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.119882
Total gradient norm: 0.333595
=== Actor Training Debug (Iteration 7343) ===
Q mean: -12.520647
Q std: 19.600994
Actor loss: 12.524626
Action reg: 0.003979
  l1.weight: grad_norm = 0.067918
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.048927
Total gradient norm: 0.142560
=== Actor Training Debug (Iteration 7344) ===
Q mean: -13.391110
Q std: 21.817307
Actor loss: 13.395080
Action reg: 0.003969
  l1.weight: grad_norm = 0.314515
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.214223
Total gradient norm: 0.604497
=== Actor Training Debug (Iteration 7345) ===
Q mean: -15.745228
Q std: 22.291519
Actor loss: 15.749178
Action reg: 0.003951
  l1.weight: grad_norm = 0.187132
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.139867
Total gradient norm: 0.402824
=== Actor Training Debug (Iteration 7346) ===
Q mean: -12.828621
Q std: 19.809282
Actor loss: 12.832576
Action reg: 0.003955
  l1.weight: grad_norm = 0.267058
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.213750
Total gradient norm: 0.648971
=== Actor Training Debug (Iteration 7347) ===
Q mean: -17.102539
Q std: 22.694410
Actor loss: 17.106516
Action reg: 0.003977
  l1.weight: grad_norm = 0.695854
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.544485
Total gradient norm: 1.526849
=== Actor Training Debug (Iteration 7348) ===
Q mean: -13.059822
Q std: 19.046261
Actor loss: 13.063787
Action reg: 0.003964
  l1.weight: grad_norm = 0.380192
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.315165
Total gradient norm: 0.936315
=== Actor Training Debug (Iteration 7349) ===
Q mean: -16.163614
Q std: 22.971212
Actor loss: 16.167578
Action reg: 0.003964
  l1.weight: grad_norm = 0.186325
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.149854
Total gradient norm: 0.407810
=== Actor Training Debug (Iteration 7350) ===
Q mean: -13.625114
Q std: 21.329105
Actor loss: 13.629077
Action reg: 0.003962
  l1.weight: grad_norm = 0.203149
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.138400
Total gradient norm: 0.373982
=== Actor Training Debug (Iteration 7351) ===
Q mean: -13.569694
Q std: 20.884254
Actor loss: 13.573668
Action reg: 0.003974
  l1.weight: grad_norm = 0.207652
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.167636
Total gradient norm: 0.468604
=== Actor Training Debug (Iteration 7352) ===
Q mean: -13.606781
Q std: 19.306650
Actor loss: 13.610746
Action reg: 0.003965
  l1.weight: grad_norm = 0.212252
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.159457
Total gradient norm: 0.458585
=== Actor Training Debug (Iteration 7353) ===
Q mean: -17.410219
Q std: 22.991863
Actor loss: 17.414186
Action reg: 0.003968
  l1.weight: grad_norm = 0.143186
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.097832
Total gradient norm: 0.273617
=== Actor Training Debug (Iteration 7354) ===
Q mean: -12.806511
Q std: 21.023787
Actor loss: 12.810479
Action reg: 0.003968
  l1.weight: grad_norm = 0.296158
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.186373
Total gradient norm: 0.537171
=== Actor Training Debug (Iteration 7355) ===
Q mean: -14.221394
Q std: 21.734062
Actor loss: 14.225373
Action reg: 0.003980
  l1.weight: grad_norm = 0.053356
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.045645
Total gradient norm: 0.144468
=== Actor Training Debug (Iteration 7356) ===
Q mean: -15.378030
Q std: 22.380825
Actor loss: 15.381998
Action reg: 0.003968
  l1.weight: grad_norm = 0.153649
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.117313
Total gradient norm: 0.304368
=== Actor Training Debug (Iteration 7357) ===
Q mean: -14.192131
Q std: 20.951799
Actor loss: 14.196080
Action reg: 0.003949
  l1.weight: grad_norm = 0.354362
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.260320
Total gradient norm: 0.810147
=== Actor Training Debug (Iteration 7358) ===
Q mean: -11.728210
Q std: 19.253750
Actor loss: 11.732176
Action reg: 0.003965
  l1.weight: grad_norm = 0.243603
  l1.bias: grad_norm = 0.001389
  l2.weight: grad_norm = 0.211719
Total gradient norm: 0.655492
=== Actor Training Debug (Iteration 7359) ===
Q mean: -14.692755
Q std: 21.994806
Actor loss: 14.696697
Action reg: 0.003943
  l1.weight: grad_norm = 0.192106
  l1.bias: grad_norm = 0.000647
  l2.weight: grad_norm = 0.131512
Total gradient norm: 0.374912
=== Actor Training Debug (Iteration 7360) ===
Q mean: -15.637354
Q std: 21.500484
Actor loss: 15.641312
Action reg: 0.003958
  l1.weight: grad_norm = 0.236238
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.153230
Total gradient norm: 0.425268
=== Actor Training Debug (Iteration 7361) ===
Q mean: -13.774326
Q std: 21.681648
Actor loss: 13.778291
Action reg: 0.003964
  l1.weight: grad_norm = 0.182894
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.127406
Total gradient norm: 0.358945
=== Actor Training Debug (Iteration 7362) ===
Q mean: -13.858326
Q std: 20.372427
Actor loss: 13.862287
Action reg: 0.003961
  l1.weight: grad_norm = 0.143262
  l1.bias: grad_norm = 0.001328
  l2.weight: grad_norm = 0.106183
Total gradient norm: 0.308571
=== Actor Training Debug (Iteration 7363) ===
Q mean: -12.094431
Q std: 19.670881
Actor loss: 12.098391
Action reg: 0.003959
  l1.weight: grad_norm = 0.227726
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.183828
Total gradient norm: 0.502789
=== Actor Training Debug (Iteration 7364) ===
Q mean: -13.485006
Q std: 21.009462
Actor loss: 13.488979
Action reg: 0.003973
  l1.weight: grad_norm = 0.289485
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.217721
Total gradient norm: 0.692661
=== Actor Training Debug (Iteration 7365) ===
Q mean: -14.336967
Q std: 21.220387
Actor loss: 14.340935
Action reg: 0.003967
  l1.weight: grad_norm = 0.151149
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.111683
Total gradient norm: 0.313271
=== Actor Training Debug (Iteration 7366) ===
Q mean: -14.887756
Q std: 21.226719
Actor loss: 14.891717
Action reg: 0.003961
  l1.weight: grad_norm = 0.160499
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.129254
Total gradient norm: 0.359851
=== Actor Training Debug (Iteration 7367) ===
Q mean: -13.792747
Q std: 20.050394
Actor loss: 13.796706
Action reg: 0.003959
  l1.weight: grad_norm = 0.099644
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.069987
Total gradient norm: 0.197108
=== Actor Training Debug (Iteration 7368) ===
Q mean: -13.221226
Q std: 20.558641
Actor loss: 13.225188
Action reg: 0.003963
  l1.weight: grad_norm = 0.112834
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.089174
Total gradient norm: 0.239838
=== Actor Training Debug (Iteration 7369) ===
Q mean: -13.995959
Q std: 20.180552
Actor loss: 13.999945
Action reg: 0.003985
  l1.weight: grad_norm = 0.208058
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.156132
Total gradient norm: 0.472109
=== Actor Training Debug (Iteration 7370) ===
Q mean: -11.942557
Q std: 19.974762
Actor loss: 11.946489
Action reg: 0.003932
  l1.weight: grad_norm = 0.156127
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.108126
Total gradient norm: 0.286274
=== Actor Training Debug (Iteration 7371) ===
Q mean: -14.331998
Q std: 20.489288
Actor loss: 14.335973
Action reg: 0.003975
  l1.weight: grad_norm = 0.141282
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.121446
Total gradient norm: 0.338230
=== Actor Training Debug (Iteration 7372) ===
Q mean: -17.253901
Q std: 23.565903
Actor loss: 17.257885
Action reg: 0.003984
  l1.weight: grad_norm = 0.172453
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.146577
Total gradient norm: 0.521355
=== Actor Training Debug (Iteration 7373) ===
Q mean: -13.882130
Q std: 21.285856
Actor loss: 13.886086
Action reg: 0.003957
  l1.weight: grad_norm = 0.129081
  l1.bias: grad_norm = 0.001509
  l2.weight: grad_norm = 0.104705
Total gradient norm: 0.313543
=== Actor Training Debug (Iteration 7374) ===
Q mean: -13.621636
Q std: 20.278187
Actor loss: 13.625620
Action reg: 0.003983
  l1.weight: grad_norm = 0.269389
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.197699
Total gradient norm: 0.565169
=== Actor Training Debug (Iteration 7375) ===
Q mean: -14.500864
Q std: 21.299816
Actor loss: 14.504809
Action reg: 0.003945
  l1.weight: grad_norm = 0.302658
  l1.bias: grad_norm = 0.000561
  l2.weight: grad_norm = 0.231684
Total gradient norm: 0.627170
=== Actor Training Debug (Iteration 7376) ===
Q mean: -14.410555
Q std: 19.595097
Actor loss: 14.414524
Action reg: 0.003969
  l1.weight: grad_norm = 0.279658
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.193198
Total gradient norm: 0.563371
=== Actor Training Debug (Iteration 7377) ===
Q mean: -15.340051
Q std: 21.866985
Actor loss: 15.343988
Action reg: 0.003938
  l1.weight: grad_norm = 0.272709
  l1.bias: grad_norm = 0.001079
  l2.weight: grad_norm = 0.187891
Total gradient norm: 0.518819
=== Actor Training Debug (Iteration 7378) ===
Q mean: -13.920035
Q std: 20.080683
Actor loss: 13.923994
Action reg: 0.003958
  l1.weight: grad_norm = 0.350527
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.258506
Total gradient norm: 0.877489
=== Actor Training Debug (Iteration 7379) ===
Q mean: -14.097456
Q std: 21.874180
Actor loss: 14.101425
Action reg: 0.003969
  l1.weight: grad_norm = 0.217886
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.155612
Total gradient norm: 0.426203
=== Actor Training Debug (Iteration 7380) ===
Q mean: -14.080240
Q std: 22.218657
Actor loss: 14.084200
Action reg: 0.003960
  l1.weight: grad_norm = 0.208724
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.157068
Total gradient norm: 0.464184
=== Actor Training Debug (Iteration 7381) ===
Q mean: -14.214174
Q std: 20.753922
Actor loss: 14.218149
Action reg: 0.003975
  l1.weight: grad_norm = 0.197167
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.137970
Total gradient norm: 0.403090
=== Actor Training Debug (Iteration 7382) ===
Q mean: -14.383829
Q std: 20.494658
Actor loss: 14.387798
Action reg: 0.003969
  l1.weight: grad_norm = 0.149804
  l1.bias: grad_norm = 0.000956
  l2.weight: grad_norm = 0.115021
Total gradient norm: 0.346037
=== Actor Training Debug (Iteration 7383) ===
Q mean: -15.514882
Q std: 21.514532
Actor loss: 15.518856
Action reg: 0.003974
  l1.weight: grad_norm = 0.392641
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.257322
Total gradient norm: 0.773349
=== Actor Training Debug (Iteration 7384) ===
Q mean: -13.750013
Q std: 21.084484
Actor loss: 13.753968
Action reg: 0.003955
  l1.weight: grad_norm = 0.348133
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.232393
Total gradient norm: 0.654686
=== Actor Training Debug (Iteration 7385) ===
Q mean: -14.095363
Q std: 20.629148
Actor loss: 14.099312
Action reg: 0.003949
  l1.weight: grad_norm = 0.227687
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.218221
Total gradient norm: 0.724009
=== Actor Training Debug (Iteration 7386) ===
Q mean: -11.409603
Q std: 20.141380
Actor loss: 11.413550
Action reg: 0.003947
  l1.weight: grad_norm = 0.330959
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.245451
Total gradient norm: 0.647974
=== Actor Training Debug (Iteration 7387) ===
Q mean: -14.306862
Q std: 21.971487
Actor loss: 14.310844
Action reg: 0.003983
  l1.weight: grad_norm = 0.242359
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.191469
Total gradient norm: 0.535461
=== Actor Training Debug (Iteration 7388) ===
Q mean: -17.446865
Q std: 22.717810
Actor loss: 17.450830
Action reg: 0.003965
  l1.weight: grad_norm = 0.192089
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.130314
Total gradient norm: 0.357673
=== Actor Training Debug (Iteration 7389) ===
Q mean: -13.270394
Q std: 20.807362
Actor loss: 13.274338
Action reg: 0.003944
  l1.weight: grad_norm = 0.570787
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.333128
Total gradient norm: 0.920594
=== Actor Training Debug (Iteration 7390) ===
Q mean: -13.167540
Q std: 21.309679
Actor loss: 13.171496
Action reg: 0.003956
  l1.weight: grad_norm = 0.255223
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.197006
Total gradient norm: 0.525795
=== Actor Training Debug (Iteration 7391) ===
Q mean: -13.876381
Q std: 21.147459
Actor loss: 13.880326
Action reg: 0.003945
  l1.weight: grad_norm = 0.170587
  l1.bias: grad_norm = 0.000692
  l2.weight: grad_norm = 0.115408
Total gradient norm: 0.338027
=== Actor Training Debug (Iteration 7392) ===
Q mean: -14.433191
Q std: 21.781654
Actor loss: 14.437159
Action reg: 0.003967
  l1.weight: grad_norm = 0.316361
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.251078
Total gradient norm: 0.668722
=== Actor Training Debug (Iteration 7393) ===
Q mean: -13.804935
Q std: 20.932791
Actor loss: 13.808904
Action reg: 0.003970
  l1.weight: grad_norm = 0.214573
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.170207
Total gradient norm: 0.463600
=== Actor Training Debug (Iteration 7394) ===
Q mean: -12.551933
Q std: 19.606829
Actor loss: 12.555910
Action reg: 0.003977
  l1.weight: grad_norm = 0.181862
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.137543
Total gradient norm: 0.369356
=== Actor Training Debug (Iteration 7395) ===
Q mean: -12.588542
Q std: 19.626347
Actor loss: 12.592523
Action reg: 0.003980
  l1.weight: grad_norm = 0.126895
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.094925
Total gradient norm: 0.235936
=== Actor Training Debug (Iteration 7396) ===
Q mean: -17.226206
Q std: 23.324972
Actor loss: 17.230164
Action reg: 0.003958
  l1.weight: grad_norm = 0.259450
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.225281
Total gradient norm: 0.686205
=== Actor Training Debug (Iteration 7397) ===
Q mean: -14.901045
Q std: 22.073563
Actor loss: 14.904993
Action reg: 0.003948
  l1.weight: grad_norm = 0.350930
  l1.bias: grad_norm = 0.000618
  l2.weight: grad_norm = 0.235727
Total gradient norm: 0.631065
=== Actor Training Debug (Iteration 7398) ===
Q mean: -13.965199
Q std: 21.970297
Actor loss: 13.969138
Action reg: 0.003940
  l1.weight: grad_norm = 0.220745
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.196263
Total gradient norm: 0.567687
=== Actor Training Debug (Iteration 7399) ===
Q mean: -13.983032
Q std: 20.111511
Actor loss: 13.986995
Action reg: 0.003962
  l1.weight: grad_norm = 0.251126
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.176501
Total gradient norm: 0.527296
=== Actor Training Debug (Iteration 7400) ===
Q mean: -14.112061
Q std: 21.444618
Actor loss: 14.116042
Action reg: 0.003981
  l1.weight: grad_norm = 0.063376
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.045198
Total gradient norm: 0.119216
=== Actor Training Debug (Iteration 7401) ===
Q mean: -13.829574
Q std: 21.166622
Actor loss: 13.833545
Action reg: 0.003972
  l1.weight: grad_norm = 0.176098
  l1.bias: grad_norm = 0.000505
  l2.weight: grad_norm = 0.131776
Total gradient norm: 0.370309
=== Actor Training Debug (Iteration 7402) ===
Q mean: -14.892101
Q std: 22.074377
Actor loss: 14.896038
Action reg: 0.003937
  l1.weight: grad_norm = 0.216167
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.173803
Total gradient norm: 0.531588
=== Actor Training Debug (Iteration 7403) ===
Q mean: -17.268591
Q std: 22.778879
Actor loss: 17.272518
Action reg: 0.003928
  l1.weight: grad_norm = 0.139660
  l1.bias: grad_norm = 0.000449
  l2.weight: grad_norm = 0.102890
Total gradient norm: 0.302601
=== Actor Training Debug (Iteration 7404) ===
Q mean: -14.064173
Q std: 20.685448
Actor loss: 14.068151
Action reg: 0.003979
  l1.weight: grad_norm = 0.177727
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.122597
Total gradient norm: 0.377794
=== Actor Training Debug (Iteration 7405) ===
Q mean: -14.348924
=== Actor Training Debug (Iteration 7438) ===
Q mean: -14.232956
Q std: 20.185261
Actor loss: 14.236913
Action reg: 0.003957
  l1.weight: grad_norm = 0.152791
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.124455
Total gradient norm: 0.382913
=== Actor Training Debug (Iteration 7439) ===
Q mean: -13.782977
Q std: 20.304003
Actor loss: 13.786947
Action reg: 0.003970
  l1.weight: grad_norm = 0.117759
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.089995
Total gradient norm: 0.232951
=== Actor Training Debug (Iteration 7440) ===
Q mean: -15.454111
Q std: 22.666786
Actor loss: 15.458088
Action reg: 0.003977
  l1.weight: grad_norm = 0.081391
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.071348
Total gradient norm: 0.189710
=== Actor Training Debug (Iteration 7441) ===
Q mean: -14.985813
Q std: 21.261408
Actor loss: 14.989766
Action reg: 0.003953
  l1.weight: grad_norm = 0.184456
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.141644
Total gradient norm: 0.395106
=== Actor Training Debug (Iteration 7442) ===
Q mean: -15.383938
Q std: 21.248726
Actor loss: 15.387908
Action reg: 0.003970
  l1.weight: grad_norm = 0.138466
  l1.bias: grad_norm = 0.000662
  l2.weight: grad_norm = 0.104016
Total gradient norm: 0.308746
=== Actor Training Debug (Iteration 7443) ===
Q mean: -13.750631
Q std: 20.393747
Actor loss: 13.754603
Action reg: 0.003972
  l1.weight: grad_norm = 0.244319
  l1.bias: grad_norm = 0.001212
  l2.weight: grad_norm = 0.177868
Total gradient norm: 0.480251
=== Actor Training Debug (Iteration 7444) ===
Q mean: -13.621666
Q std: 20.932346
Actor loss: 13.625617
Action reg: 0.003951
  l1.weight: grad_norm = 0.202639
  l1.bias: grad_norm = 0.000476
  l2.weight: grad_norm = 0.146059
Total gradient norm: 0.373862
=== Actor Training Debug (Iteration 7445) ===
Q mean: -15.052524
Q std: 21.565481
Actor loss: 15.056509
Action reg: 0.003986
  l1.weight: grad_norm = 0.128470
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.092959
Total gradient norm: 0.245432
=== Actor Training Debug (Iteration 7446) ===
Q mean: -13.283457
Q std: 21.522915
Actor loss: 13.287415
Action reg: 0.003958
  l1.weight: grad_norm = 0.224300
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.193812
Total gradient norm: 0.488256
=== Actor Training Debug (Iteration 7447) ===
Q mean: -13.846148
Q std: 20.627102
Actor loss: 13.850130
Action reg: 0.003981
  l1.weight: grad_norm = 0.194521
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.139250
Total gradient norm: 0.422037
=== Actor Training Debug (Iteration 7448) ===
Q mean: -14.921518
Q std: 22.352633
Actor loss: 14.925474
Action reg: 0.003955
  l1.weight: grad_norm = 0.187950
  l1.bias: grad_norm = 0.001497
  l2.weight: grad_norm = 0.166227
Total gradient norm: 0.507955
=== Actor Training Debug (Iteration 7449) ===
Q mean: -13.571096
Q std: 19.658512
Actor loss: 13.575046
Action reg: 0.003949
  l1.weight: grad_norm = 0.148627
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.100070
Total gradient norm: 0.271956
=== Actor Training Debug (Iteration 7450) ===
Q mean: -14.049667
Q std: 19.739187
Actor loss: 14.053639
Action reg: 0.003972
  l1.weight: grad_norm = 0.264827
  l1.bias: grad_norm = 0.001636
  l2.weight: grad_norm = 0.159357
Total gradient norm: 0.448433
=== Actor Training Debug (Iteration 7451) ===
Q mean: -13.000760
Q std: 19.638094
Actor loss: 13.004719
Action reg: 0.003958
  l1.weight: grad_norm = 0.158940
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.120653
Total gradient norm: 0.382349
=== Actor Training Debug (Iteration 7452) ===
Q mean: -13.676704
Q std: 20.221262
Actor loss: 13.680660
Action reg: 0.003956
  l1.weight: grad_norm = 0.229210
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.139532
Total gradient norm: 0.362282
=== Actor Training Debug (Iteration 7453) ===
Q mean: -13.720978
Q std: 20.120972
Actor loss: 13.724930
Action reg: 0.003952
  l1.weight: grad_norm = 0.112823
  l1.bias: grad_norm = 0.000978
  l2.weight: grad_norm = 0.078184
Total gradient norm: 0.222070
=== Actor Training Debug (Iteration 7454) ===
Q mean: -16.239506
Q std: 21.026756
Actor loss: 16.243483
Action reg: 0.003976
  l1.weight: grad_norm = 0.197210
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.134764
Total gradient norm: 0.501373
=== Actor Training Debug (Iteration 7455) ===
Q mean: -14.689796
Q std: 21.689850
Actor loss: 14.693763
Action reg: 0.003966
  l1.weight: grad_norm = 0.180268
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.155420
Total gradient norm: 0.475001
=== Actor Training Debug (Iteration 7456) ===
Q mean: -16.260609
Q std: 23.395573
Actor loss: 16.264580
Action reg: 0.003971
  l1.weight: grad_norm = 0.314590
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.255645
Total gradient norm: 0.797262
=== Actor Training Debug (Iteration 7457) ===
Q mean: -13.790791
Q std: 19.519684
Actor loss: 13.794741
Action reg: 0.003950
  l1.weight: grad_norm = 0.162272
  l1.bias: grad_norm = 0.000516
  l2.weight: grad_norm = 0.115481
Total gradient norm: 0.317506
=== Actor Training Debug (Iteration 7458) ===
Q mean: -15.501905
Q std: 21.464399
Actor loss: 15.505868
Action reg: 0.003962
  l1.weight: grad_norm = 0.286452
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.185070
Total gradient norm: 0.498360
=== Actor Training Debug (Iteration 7459) ===
Q mean: -13.946098
Q std: 21.506346
Actor loss: 13.950050
Action reg: 0.003952
  l1.weight: grad_norm = 0.411257
  l1.bias: grad_norm = 0.000743
  l2.weight: grad_norm = 0.296421
Total gradient norm: 0.802377
=== Actor Training Debug (Iteration 7460) ===
Q mean: -16.205654
Q std: 22.937357
Actor loss: 16.209623
Action reg: 0.003970
  l1.weight: grad_norm = 0.216613
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.171482
Total gradient norm: 0.437260
=== Actor Training Debug (Iteration 7461) ===
Q mean: -14.563608
Q std: 22.241369
Actor loss: 14.567561
Action reg: 0.003953
  l1.weight: grad_norm = 0.284120
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.214523
Total gradient norm: 0.621692
=== Actor Training Debug (Iteration 7462) ===
Q mean: -14.080793
Q std: 20.353687
Actor loss: 14.084744
Action reg: 0.003951
  l1.weight: grad_norm = 0.222469
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.167073
Total gradient norm: 0.416350
=== Actor Training Debug (Iteration 7463) ===
Q mean: -13.665626
Q std: 21.446028
Actor loss: 13.669610
Action reg: 0.003985
  l1.weight: grad_norm = 0.233558
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.180836
Total gradient norm: 0.522336
=== Actor Training Debug (Iteration 7464) ===
Q mean: -16.125700
Q std: 21.565830
Actor loss: 16.129669
Action reg: 0.003969
  l1.weight: grad_norm = 0.097078
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.079229
Total gradient norm: 0.200679
=== Actor Training Debug (Iteration 7465) ===
Q mean: -14.864760
Q std: 21.754337
Actor loss: 14.868725
Action reg: 0.003964
  l1.weight: grad_norm = 0.309990
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.217597
Total gradient norm: 0.571052
=== Actor Training Debug (Iteration 7466) ===
Q mean: -14.315802
Q std: 20.313824
Actor loss: 14.319776
Action reg: 0.003974
  l1.weight: grad_norm = 0.245634
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.174736
Total gradient norm: 0.507291
=== Actor Training Debug (Iteration 7467) ===
Q mean: -17.732677
Q std: 24.325212
Actor loss: 17.736645
Action reg: 0.003967
  l1.weight: grad_norm = 0.279894
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.235326
Total gradient norm: 0.686303
=== Actor Training Debug (Iteration 7468) ===
Q mean: -15.566256
Q std: 20.974154
Actor loss: 15.570233
Action reg: 0.003978
  l1.weight: grad_norm = 0.219019
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.153698
Total gradient norm: 0.410099
=== Actor Training Debug (Iteration 7469) ===
Q mean: -13.815231
Q std: 20.406729
Actor loss: 13.819189
Action reg: 0.003958
  l1.weight: grad_norm = 0.248606
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.170380
Total gradient norm: 0.411323
=== Actor Training Debug (Iteration 7470) ===
Q mean: -15.014083
Q std: 21.066050
Actor loss: 15.018053
Action reg: 0.003971
  l1.weight: grad_norm = 0.321154
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.243685
Total gradient norm: 0.690371
=== Actor Training Debug (Iteration 7471) ===
Q mean: -15.495142
Q std: 22.177452
Actor loss: 15.499091
Action reg: 0.003949
  l1.weight: grad_norm = 0.237682
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.196787
Total gradient norm: 0.559129
=== Actor Training Debug (Iteration 7472) ===
Q mean: -14.184566
Q std: 21.415161
Actor loss: 14.188544
Action reg: 0.003978
  l1.weight: grad_norm = 0.235955
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.190365
Total gradient norm: 0.603007
=== Actor Training Debug (Iteration 7473) ===
Q mean: -13.223095
Q std: 20.319044
Actor loss: 13.227073
Action reg: 0.003977
  l1.weight: grad_norm = 0.204053
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.144124
Total gradient norm: 0.420664
=== Actor Training Debug (Iteration 7474) ===
Q mean: -15.279746
Q std: 20.680986
Actor loss: 15.283719
Action reg: 0.003973
  l1.weight: grad_norm = 0.202513
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.150544
Total gradient norm: 0.433341
=== Actor Training Debug (Iteration 7475) ===
Q mean: -13.772079
Q std: 19.820816
Actor loss: 13.776028
Action reg: 0.003948
  l1.weight: grad_norm = 0.252379
  l1.bias: grad_norm = 0.000604
  l2.weight: grad_norm = 0.202255
Total gradient norm: 0.554192
=== Actor Training Debug (Iteration 7476) ===
Q mean: -15.377125
Q std: 21.082626
Actor loss: 15.381068
Action reg: 0.003944
  l1.weight: grad_norm = 0.138958
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.091527
Total gradient norm: 0.277856
=== Actor Training Debug (Iteration 7477) ===
Q mean: -13.457931
Q std: 20.740648
Actor loss: 13.461893
Action reg: 0.003963
  l1.weight: grad_norm = 0.139094
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.089433
Total gradient norm: 0.258423
=== Actor Training Debug (Iteration 7478) ===
Q mean: -14.997845
Q std: 21.009047
Actor loss: 15.001794
Action reg: 0.003949
  l1.weight: grad_norm = 0.326749
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.226994
Total gradient norm: 0.613268
=== Actor Training Debug (Iteration 7479) ===
Q mean: -13.132952
Q std: 19.106348
Actor loss: 13.136908
Action reg: 0.003956
  l1.weight: grad_norm = 0.126005
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.088220
Total gradient norm: 0.231473
=== Actor Training Debug (Iteration 7480) ===
Q mean: -13.522618
Q std: 19.740486
Actor loss: 13.526575
Action reg: 0.003957
  l1.weight: grad_norm = 0.076489
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.073571
Total gradient norm: 0.208847
=== Actor Training Debug (Iteration 7481) ===
Q mean: -11.621054
Q std: 20.247736
Actor loss: 11.625012
Action reg: 0.003959
  l1.weight: grad_norm = 0.099184
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.093297
Total gradient norm: 0.321346
=== Actor Training Debug (Iteration 7482) ===
Q mean: -15.512616
Q std: 21.023436
Actor loss: 15.516591
Action reg: 0.003975
  l1.weight: grad_norm = 0.157070
  l1.bias: grad_norm = 0.000594
  l2.weight: grad_norm = 0.106987
Total gradient norm: 0.295600
=== Actor Training Debug (Iteration 7483) ===
Q mean: -15.501477
Q std: 21.335867
Actor loss: 15.505441
Action reg: 0.003963
  l1.weight: grad_norm = 0.240922
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.183017
Total gradient norm: 0.490127
=== Actor Training Debug (Iteration 7484) ===
Q mean: -14.652725
Q std: 21.117496
Actor loss: 14.656680
Action reg: 0.003955
  l1.weight: grad_norm = 0.323757
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.227494
Total gradient norm: 0.636090
=== Actor Training Debug (Iteration 7485) ===
Q mean: -14.061430
Q std: 18.882502
Actor loss: 14.065392
Action reg: 0.003961
  l1.weight: grad_norm = 0.131674
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.096100
Total gradient norm: 0.268466
=== Actor Training Debug (Iteration 7486) ===
Q mean: -13.824994
Q std: 21.994598
Actor loss: 13.828961
Action reg: 0.003967
  l1.weight: grad_norm = 0.166651
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.146933
Total gradient norm: 0.470286
=== Actor Training Debug (Iteration 7487) ===
Q mean: -14.972786
Q std: 20.510534
Actor loss: 14.976727
Action reg: 0.003941
  l1.weight: grad_norm = 0.071598
  l1.bias: grad_norm = 0.000757
  l2.weight: grad_norm = 0.050143
Total gradient norm: 0.152240
=== Actor Training Debug (Iteration 7488) ===
Q mean: -14.369155
Q std: 21.519117
Actor loss: 14.373114
Action reg: 0.003958
  l1.weight: grad_norm = 0.278852
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.214491
Total gradient norm: 0.669888
=== Actor Training Debug (Iteration 7489) ===
Q mean: -14.191380
Q std: 21.027880
Actor loss: 14.195344
Action reg: 0.003965
  l1.weight: grad_norm = 0.197402
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.151661
Total gradient norm: 0.374677
=== Actor Training Debug (Iteration 7490) ===
Q mean: -13.695665
Q std: 20.203213
Actor loss: 13.699611
Action reg: 0.003946
  l1.weight: grad_norm = 0.355673
  l1.bias: grad_norm = 0.000834
  l2.weight: grad_norm = 0.261928
Total gradient norm: 0.762258
=== Actor Training Debug (Iteration 7491) ===
Q mean: -13.567522
Q std: 20.338284
Actor loss: 13.571494
Action reg: 0.003972
  l1.weight: grad_norm = 0.345098
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.242671
Total gradient norm: 0.614546
=== Actor Training Debug (Iteration 7492) ===
Q mean: -15.323495
Q std: 21.388123
Actor loss: 15.327451
Action reg: 0.003956
  l1.weight: grad_norm = 0.160025
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.125985
Total gradient norm: 0.354074
=== Actor Training Debug (Iteration 7493) ===
Q mean: -14.336551
Q std: 20.779497
Actor loss: 14.340515
Action reg: 0.003964
  l1.weight: grad_norm = 0.709959
  l1.bias: grad_norm = 0.000936
  l2.weight: grad_norm = 0.500181
Total gradient norm: 1.690528
=== Actor Training Debug (Iteration 7494) ===
Q mean: -14.712663
Q std: 21.757944
Actor loss: 14.716650
Action reg: 0.003987
  l1.weight: grad_norm = 0.149905
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.111460
Total gradient norm: 0.307698
=== Actor Training Debug (Iteration 7495) ===
Q mean: -16.998180
Q std: 22.517414
Actor loss: 17.002153
Action reg: 0.003973
  l1.weight: grad_norm = 0.184914
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.132150
Total gradient norm: 0.394475
=== Actor Training Debug (Iteration 7496) ===
Q mean: -14.631100
Q std: 20.782951
Actor loss: 14.635057
Action reg: 0.003958
  l1.weight: grad_norm = 0.366894
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.284785
Total gradient norm: 0.745035
=== Actor Training Debug (Iteration 7497) ===
Q mean: -13.972681
Q std: 19.603500
Actor loss: 13.976619
Action reg: 0.003938
  l1.weight: grad_norm = 0.225402
  l1.bias: grad_norm = 0.001918
  l2.weight: grad_norm = 0.173594
Total gradient norm: 0.504094
=== Actor Training Debug (Iteration 7498) ===
Q mean: -15.365649
Q std: 21.338001
Actor loss: 15.369607
Action reg: 0.003958
  l1.weight: grad_norm = 0.334245
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.251456
Total gradient norm: 0.747136
=== Actor Training Debug (Iteration 7499) ===
Q mean: -14.771038
Q std: 22.435501
Actor loss: 14.774991
Action reg: 0.003953
  l1.weight: grad_norm = 0.124069
  l1.bias: grad_norm = 0.000405
  l2.weight: grad_norm = 0.101378
Total gradient norm: 0.265544
=== Actor Training Debug (Iteration 7500) ===
Q mean: -14.866438
Q std: 21.665056
Actor loss: 14.870416
Action reg: 0.003978
  l1.weight: grad_norm = 0.148520
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.112316
Total gradient norm: 0.295367
  Average reward: -317.654 | Average length: 100.0
Evaluation at episode 125: -317.654
=== Actor Training Debug (Iteration 7501) ===
Q mean: -14.672447
Q std: 22.287672
Actor loss: 14.676393
Action reg: 0.003945
  l1.weight: grad_norm = 0.341833
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.254007
Total gradient norm: 0.704216
=== Actor Training Debug (Iteration 7502) ===
Q mean: -13.418958
Q std: 19.468719
Actor loss: 13.422895
Action reg: 0.003938
  l1.weight: grad_norm = 0.133860
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.113706
Total gradient norm: 0.304922
=== Actor Training Debug (Iteration 7503) ===
Q mean: -15.130251
Q std: 21.229700
Actor loss: 15.134217
Action reg: 0.003966
  l1.weight: grad_norm = 0.109627
  l1.bias: grad_norm = 0.002037
  l2.weight: grad_norm = 0.074272
Total gradient norm: 0.217284
=== Actor Training Debug (Iteration 7504) ===
Q mean: -15.448605
Q std: 22.624029
Actor loss: 15.452573
Action reg: 0.003968
  l1.weight: grad_norm = 0.162858
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.151194
Total gradient norm: 0.455570
=== Actor Training Debug (Iteration 7505) ===
Q mean: -17.036720
Q std: 22.852434
Actor loss: 17.040655
Action reg: 0.003935
  l1.weight: grad_norm = 0.250711
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.204553
Total gradient norm: 0.742067
=== Actor Training Debug (Iteration 7506) ===
Q mean: -16.084179
Q std: 22.153822
Actor loss: 16.088148
Action reg: 0.003968
  l1.weight: grad_norm = 0.260757
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.192076
Total gradient norm: 0.621250
=== Actor Training Debug (Iteration 7507) ===
Q mean: -12.781097
Q std: 19.879305
Actor loss: 12.785063
Action reg: 0.003966
  l1.weight: grad_norm = 0.286319
  l1.bias: grad_norm = 0.000931
  l2.weight: grad_norm = 0.220459
Total gradient norm: 0.612350
=== Actor Training Debug (Iteration 7508) ===
Q mean: -13.301451
Q std: 19.917488
Actor loss: 13.305403
Action reg: 0.003952
  l1.weight: grad_norm = 0.221208
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.173256
Total gradient norm: 0.500924
=== Actor Training Debug (Iteration 7509) ===
Q mean: -15.249349
Q std: 22.333138
Actor loss: 15.253331
Action reg: 0.003983
  l1.weight: grad_norm = 0.037750
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.027885
Total gradient norm: 0.086553
=== Actor Training Debug (Iteration 7510) ===
Q mean: -14.464417
Q std: 21.424709
Actor loss: 14.468372
Action reg: 0.003955
  l1.weight: grad_norm = 0.065396
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.050843
Total gradient norm: 0.139912
=== Actor Training Debug (Iteration 7511) ===
Q mean: -14.006733
Q std: 21.282694
Actor loss: 14.010697
Action reg: 0.003965
  l1.weight: grad_norm = 0.220602
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.168259
Total gradient norm: 0.478119
=== Actor Training Debug (Iteration 7512) ===
Q mean: -13.216025
Q std: 19.692318
Actor loss: 13.219978
Action reg: 0.003953
  l1.weight: grad_norm = 0.187488
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.123523
Total gradient norm: 0.371875
=== Actor Training Debug (Iteration 7513) ===
Q mean: -12.787788
Q std: 19.250713
Actor loss: 12.791753
Action reg: 0.003964
  l1.weight: grad_norm = 0.164424
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.139427
Total gradient norm: 0.410460
=== Actor Training Debug (Iteration 7514) ===
Q mean: -14.155060
Q std: 22.148882
Actor loss: 14.159012
Action reg: 0.003952
  l1.weight: grad_norm = 0.203194
  l1.bias: grad_norm = 0.000626
  l2.weight: grad_norm = 0.160550
Total gradient norm: 0.436514
=== Actor Training Debug (Iteration 7515) ===
Q mean: -13.131538
Q std: 19.589918
Actor loss: 13.135518
Action reg: 0.003979
  l1.weight: grad_norm = 0.254740
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.168562
Total gradient norm: 0.472195
=== Actor Training Debug (Iteration 7516) ===
Q mean: -13.429502
Q std: 21.536541
Actor loss: 13.433473
Action reg: 0.003971
  l1.weight: grad_norm = 0.377940
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.274954
Total gradient norm: 0.717070
=== Actor Training Debug (Iteration 7517) ===
Q mean: -12.961879
Q std: 19.659351
Actor loss: 12.965825
Action reg: 0.003946
  l1.weight: grad_norm = 0.204229
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.155866
Total gradient norm: 0.419912
=== Actor Training Debug (Iteration 7518) ===
Q mean: -13.429173
Q std: 20.560703
Actor loss: 13.433144
Action reg: 0.003971
  l1.weight: grad_norm = 0.227716
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.148709
Total gradient norm: 0.409306
=== Actor Training Debug (Iteration 7519) ===
Q mean: -11.928031
Q std: 18.576023
Actor loss: 11.932005
Action reg: 0.003974
  l1.weight: grad_norm = 0.170337
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.120932
Total gradient norm: 0.330265
=== Actor Training Debug (Iteration 7520) ===
Q mean: -16.038492
Q std: 23.091326
Actor loss: 16.042448
Action reg: 0.003956
  l1.weight: grad_norm = 0.178111
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.145644
Total gradient norm: 0.441408
=== Actor Training Debug (Iteration 7521) ===
Q mean: -12.826815
Q std: 20.317915
Actor loss: 12.830785
Action reg: 0.003970
  l1.weight: grad_norm = 0.179383
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.156217
Total gradient norm: 0.463028
=== Actor Training Debug (Iteration 7522) ===
Q mean: -15.432512
Q std: 22.965094
Actor loss: 15.436486
Action reg: 0.003974
  l1.weight: grad_norm = 0.175096
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.115553
Total gradient norm: 0.313095
=== Actor Training Debug (Iteration 7523) ===
Q mean: -13.717991
Q std: 19.588736
Actor loss: 13.721929
Action reg: 0.003938
  l1.weight: grad_norm = 0.108996
  l1.bias: grad_norm = 0.000556
  l2.weight: grad_norm = 0.090543
Total gradient norm: 0.235105
=== Actor Training Debug (Iteration 7524) ===
Q mean: -15.800430
Q std: 22.050362
Actor loss: 15.804409
Action reg: 0.003979
  l1.weight: grad_norm = 0.155563
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.120358
Total gradient norm: 0.344492
=== Actor Training Debug (Iteration 7525) ===
Q mean: -13.529480
Q std: 19.416519
Actor loss: 13.533438
Action reg: 0.003958
  l1.weight: grad_norm = 0.176031
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.122776
Total gradient norm: 0.335637
=== Actor Training Debug (Iteration 7526) ===
Q mean: -15.384382
Q std: 22.534439
Actor loss: 15.388330
Action reg: 0.003948
  l1.weight: grad_norm = 0.489874
  l1.bias: grad_norm = 0.001163
  l2.weight: grad_norm = 0.351885
Total gradient norm: 0.940540
=== Actor Training Debug (Iteration 7527) ===
Q mean: -13.628811
Q std: 21.121372
Actor loss: 13.632777
Action reg: 0.003967
  l1.weight: grad_norm = 0.265499
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.169442
Total gradient norm: 0.449219
=== Actor Training Debug (Iteration 7528) ===
Q mean: -13.441662
Q std: 20.715742
Actor loss: 13.445613
Action reg: 0.003951
  l1.weight: grad_norm = 0.245892
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.164829
Total gradient norm: 0.403422
=== Actor Training Debug (Iteration 7529) ===
Q mean: -14.321584
Q std: 21.125992
Actor loss: 14.325562
Action reg: 0.003978
  l1.weight: grad_norm = 0.251703
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.168639
Total gradient norm: 0.511466
=== Actor Training Debug (Iteration 7530) ===
Q mean: -14.351310
Q std: 21.652542
Actor loss: 14.355294
Action reg: 0.003984
  l1.weight: grad_norm = 0.200140
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.175797
Total gradient norm: 0.572748
=== Actor Training Debug (Iteration 7531) ===
Q mean: -11.269035
Q std: 18.491732
Actor loss: 11.272998
Action reg: 0.003962
  l1.weight: grad_norm = 0.270382
  l1.bias: grad_norm = 0.001530
  l2.weight: grad_norm = 0.222853
Total gradient norm: 0.693048
=== Actor Training Debug (Iteration 7532) ===
Q mean: -15.804219
Q std: 22.488873
Actor loss: 15.808196
Action reg: 0.003977
  l1.weight: grad_norm = 0.086715
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.057173
Total gradient norm: 0.162430
=== Actor Training Debug (Iteration 7533) ===
Q mean: -14.038031
Q std: 20.387476
Actor loss: 14.041981
Action reg: 0.003950
  l1.weight: grad_norm = 0.116717
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.092137
Total gradient norm: 0.252236
=== Actor Training Debug (Iteration 7534) ===
Q mean: -15.389232
Q std: 22.881790
Actor loss: 15.393216
Action reg: 0.003984
  l1.weight: grad_norm = 0.083902
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.067434
Total gradient norm: 0.180005
=== Actor Training Debug (Iteration 7535) ===
Q mean: -13.018715
Q std: 19.133125
Actor loss: 13.022686
Action reg: 0.003971
  l1.weight: grad_norm = 0.210734
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.136919
Total gradient norm: 0.502821
=== Actor Training Debug (Iteration 7536) ===
Q mean: -13.653393
Q std: 20.945244
Actor loss: 13.657363
Action reg: 0.003970
  l1.weight: grad_norm = 0.182606
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.118350
Total gradient norm: 0.354152
=== Actor Training Debug (Iteration 7537) ===
Q mean: -15.613617
Q std: 22.490967
Actor loss: 15.617590
Action reg: 0.003973
  l1.weight: grad_norm = 0.371141
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.252996
Total gradient norm: 0.714355
=== Actor Training Debug (Iteration 7538) ===
Q mean: -12.322867
Q std: 19.814093
Actor loss: 12.326832
Action reg: 0.003965
  l1.weight: grad_norm = 0.129070
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.097524
Total gradient norm: 0.287592
=== Actor Training Debug (Iteration 7539) ===
Q mean: -13.422264
Q std: 19.154779
Actor loss: 13.426241
Action reg: 0.003977
  l1.weight: grad_norm = 0.135420
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.092390
Total gradient norm: 0.280364
=== Actor Training Debug (Iteration 7540) ===
Q mean: -15.701507
Q std: 21.432915
Actor loss: 15.705478
Action reg: 0.003972
  l1.weight: grad_norm = 0.128229
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.114723
Total gradient norm: 0.301133
=== Actor Training Debug (Iteration 7541) ===
Q mean: -13.666201
Q std: 19.776903
Actor loss: 13.670151
Action reg: 0.003950
  l1.weight: grad_norm = 0.159786
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.114391
Total gradient norm: 0.377027
=== Actor Training Debug (Iteration 7542) ===
Q mean: -12.325438
Q std: 18.597788
Actor loss: 12.329398
Action reg: 0.003961
  l1.weight: grad_norm = 0.186338
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.144736
Total gradient norm: 0.400962
=== Actor Training Debug (Iteration 7543) ===
Q mean: -14.434449
Q std: 21.025902
Actor loss: 14.438395
Action reg: 0.003946
  l1.weight: grad_norm = 0.169508
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.136094
Total gradient norm: 0.342742
=== Actor Training Debug (Iteration 7544) ===
Q mean: -15.185236
Q std: 20.894287
Actor loss: 15.189214
Action reg: 0.003978
  l1.weight: grad_norm = 0.077883
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.055516
Total gradient norm: 0.161188
=== Actor Training Debug (Iteration 7545) ===
Q mean: -13.737115
Q std: 21.807789
Actor loss: 13.741074
Action reg: 0.003959
  l1.weight: grad_norm = 0.198338
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.148424
Total gradient norm: 0.412675
=== Actor Training Debug (Iteration 7546) ===
Q mean: -13.862965
Q std: 19.190861
Actor loss: 13.866920
Action reg: 0.003955
  l1.weight: grad_norm = 0.123874
  l1.bias: grad_norm = 0.000575
  l2.weight: grad_norm = 0.091332
Total gradient norm: 0.283124
=== Actor Training Debug (Iteration 7547) ===
Q mean: -14.400688
Q std: 22.798521
Actor loss: 14.404646
Action reg: 0.003958
  l1.weight: grad_norm = 0.189448
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.135721
Total gradient norm: 0.395603
=== Actor Training Debug (Iteration 7548) ===
Q mean: -13.844964
Q std: 20.231754
Actor loss: 13.848921
Action reg: 0.003957
  l1.weight: grad_norm = 0.248609
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.179465
Total gradient norm: 0.510017
=== Actor Training Debug (Iteration 7549) ===
Q mean: -13.410192
Q std: 21.729284
Actor loss: 13.414152
Action reg: 0.003959
  l1.weight: grad_norm = 0.260583
  l1.bias: grad_norm = 0.001895
  l2.weight: grad_norm = 0.193927
Total gradient norm: 0.584269
=== Actor Training Debug (Iteration 7550) ===
Q mean: -14.608883
Q std: 21.029751
Actor loss: 14.612837
Action reg: 0.003954
  l1.weight: grad_norm = 0.106414
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.085747
Total gradient norm: 0.223695
=== Actor Training Debug (Iteration 7551) ===
Q mean: -14.007427
Q std: 21.306013
Actor loss: 14.011407
Action reg: 0.003980
  l1.weight: grad_norm = 0.092519
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.065949
Total gradient norm: 0.192469
=== Actor Training Debug (Iteration 7552) ===
Q mean: -14.428143
Q std: 20.906776
Actor loss: 14.432118
Action reg: 0.003976
  l1.weight: grad_norm = 0.230238
  l1.bias: grad_norm = 0.000923
  l2.weight: grad_norm = 0.163527
Total gradient norm: 0.475148
=== Actor Training Debug (Iteration 7553) ===
Q mean: -13.971248
Q std: 21.738670
Actor loss: 13.975203
Action reg: 0.003955
  l1.weight: grad_norm = 0.205313
  l1.bias: grad_norm = 0.001449
  l2.weight: grad_norm = 0.145970
Total gradient norm: 0.398569
=== Actor Training Debug (Iteration 7554) ===
Q mean: -16.009809
Q std: 22.426386
Actor loss: 16.013788
Action reg: 0.003978
  l1.weight: grad_norm = 0.167582
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.131999
Total gradient norm: 0.360217
=== Actor Training Debug (Iteration 7555) ===
Q mean: -11.936235
Q std: 19.744514
Actor loss: 11.940191
Action reg: 0.003956
  l1.weight: grad_norm = 0.366246
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.275484
Total gradient norm: 0.773230
=== Actor Training Debug (Iteration 7556) ===
Q mean: -11.227304
Q std: 18.225290
Actor loss: 11.231290
Action reg: 0.003985
  l1.weight: grad_norm = 0.108256
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.093911
Total gradient norm: 0.254532
=== Actor Training Debug (Iteration 7557) ===
Q mean: -13.195423
Q std: 20.272833
Actor loss: 13.199383
Action reg: 0.003960
  l1.weight: grad_norm = 0.226702
  l1.bias: grad_norm = 0.000746
  l2.weight: grad_norm = 0.169043
Total gradient norm: 0.617808
=== Actor Training Debug (Iteration 7558) ===
Q mean: -14.509460
Q std: 20.442720
Actor loss: 14.513437
Action reg: 0.003977
  l1.weight: grad_norm = 0.056954
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.040727
Total gradient norm: 0.115010
=== Actor Training Debug (Iteration 7559) ===
Q mean: -12.017151
Q std: 19.096405
Actor loss: 12.021115
Action reg: 0.003965
  l1.weight: grad_norm = 0.166838
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.116665
Total gradient norm: 0.324526
=== Actor Training Debug (Iteration 7560) ===
Q mean: -17.127983
Q std: 23.746096
Actor loss: 17.131950
Action reg: 0.003968
  l1.weight: grad_norm = 0.265584
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.202219
Total gradient norm: 0.588407
=== Actor Training Debug (Iteration 7561) ===
Q mean: -14.600179
Q std: 21.540546
Actor loss: 14.604142
Action reg: 0.003963
  l1.weight: grad_norm = 0.142281
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.099619
Total gradient norm: 0.272901
=== Actor Training Debug (Iteration 7562) ===
Q mean: -12.973250
Q std: 19.509268
Actor loss: 12.977217
Action reg: 0.003967
  l1.weight: grad_norm = 0.604964
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.415089
Total gradient norm: 1.500865
=== Actor Training Debug (Iteration 7563) ===
Q mean: -13.603316
Q std: 20.917803
Actor loss: 13.607278
Action reg: 0.003961
  l1.weight: grad_norm = 0.182059
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.149671
Total gradient norm: 0.385395
=== Actor Training Debug (Iteration 7564) ===
Q mean: -13.764805
Q std: 21.326664
Actor loss: 13.768768
Action reg: 0.003964
  l1.weight: grad_norm = 0.219713
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.185873
Total gradient norm: 0.521131
=== Actor Training Debug (Iteration 7565) ===
Q mean: -12.325745
Q std: 19.292431
Actor loss: 12.329715
Action reg: 0.003970
  l1.weight: grad_norm = 0.136571
  l1.bias: grad_norm = 0.001643
  l2.weight: grad_norm = 0.101159
Total gradient norm: 0.285789
=== Actor Training Debug (Iteration 7566) ===
Q mean: -14.211733
Q std: 20.299091
Actor loss: 14.215690
Action reg: 0.003957
  l1.weight: grad_norm = 0.311155
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.267064
Total gradient norm: 0.721207
=== Actor Training Debug (Iteration 7567) ===
Q mean: -17.085960
Q std: 22.363150
Actor loss: 17.089926
Action reg: 0.003966
  l1.weight: grad_norm = 0.559556
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.327776
Total gradient norm: 0.944052
=== Actor Training Debug (Iteration 7568) ===
Q mean: -12.275780
Q std: 18.534637
Actor loss: 12.279743
Action reg: 0.003963
  l1.weight: grad_norm = 0.142372
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.111478
Total gradient norm: 0.365436
=== Actor Training Debug (Iteration 7569) ===
Q mean: -15.678563
Q std: 22.131990
Actor loss: 15.682512
Action reg: 0.003949
  l1.weight: grad_norm = 0.215582
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.157299
Total gradient norm: 0.421441
=== Actor Training Debug (Iteration 7570) ===
Q mean: -15.633096
Q std: 21.373213
Actor loss: 15.637080
Action reg: 0.003984
  l1.weight: grad_norm = 0.148909
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.114031
Total gradient norm: 0.372523
=== Actor Training Debug (Iteration 7571) ===
Q mean: -18.263901
Q std: 24.269594
Actor loss: 18.267868
Action reg: 0.003967
  l1.weight: grad_norm = 0.207215
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.136397
Total gradient norm: 0.387849
=== Actor Training Debug (Iteration 7572) ===
Q mean: -15.930438
Q std: 22.642725
Actor loss: 15.934394
Action reg: 0.003955
  l1.weight: grad_norm = 0.245925
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.173938
Total gradient norm: 0.530323
=== Actor Training Debug (Iteration 7573) ===
Q mean: -14.548739
Q std: 21.876146
Actor loss: 14.552705
Action reg: 0.003965
  l1.weight: grad_norm = 0.203571
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.193988
Total gradient norm: 0.575165
=== Actor Training Debug (Iteration 7574) ===
Q mean: -12.733362
Q std: 19.254759
Actor loss: 12.737312
Action reg: 0.003950
  l1.weight: grad_norm = 0.254220
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.186531
Total gradient norm: 0.551440
=== Actor Training Debug (Iteration 7575) ===
Q mean: -13.232211
Q std: 19.285709
Actor loss: 13.236187
Action reg: 0.003975
  l1.weight: grad_norm = 0.246342
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.181180
Total gradient norm: 0.464087
=== Actor Training Debug (Iteration 7576) ===
Q mean: -15.428290
Q std: 22.256411
Actor loss: 15.432253
Action reg: 0.003963
  l1.weight: grad_norm = 0.138182
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.103879
Total gradient norm: 0.293017
=== Actor Training Debug (Iteration 7577) ===
Q mean: -16.353899
Q std: 22.697563
Actor loss: 16.357832
Action reg: 0.003933
  l1.weight: grad_norm = 0.418897
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.253280
Total gradient norm: 0.685730
=== Actor Training Debug (Iteration 7578) ===
Q mean: -14.465549
Q std: 20.771193
Actor loss: 14.469518
Action reg: 0.003969
  l1.weight: grad_norm = 0.446081
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.287754
Total gradient norm: 0.854299
=== Actor Training Debug (Iteration 7579) ===
Q mean: -14.392962
Q std: 20.457561
Actor loss: 14.396927
Action reg: 0.003965
  l1.weight: grad_norm = 0.266839
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.199980
Total gradient norm: 0.511131
=== Actor Training Debug (Iteration 7580) ===
Q mean: -14.146682
Q std: 20.715668
Actor loss: 14.150648
Action reg: 0.003967
  l1.weight: grad_norm = 0.182547
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.147204
Total gradient norm: 0.434254
=== Actor Training Debug (Iteration 7581) ===
Q mean: -13.405871
Q std: 21.392500
Actor loss: 13.409838
Action reg: 0.003967
  l1.weight: grad_norm = 0.552925
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.363505
Total gradient norm: 1.001293
=== Actor Training Debug (Iteration 7582) ===
Q mean: -12.716236
Q std: 19.946787
Actor loss: 12.720201
Action reg: 0.003966
  l1.weight: grad_norm = 0.220594
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.175434
Total gradient norm: 0.502647
=== Actor Training Debug (Iteration 7583) ===
Q mean: -14.817225
Q std: 21.270296
Actor loss: 14.821194
Action reg: 0.003969
  l1.weight: grad_norm = 0.108442
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.072607
Total gradient norm: 0.279270
=== Actor Training Debug (Iteration 7584) ===
Q mean: -15.092762
Q std: 20.200882
Actor loss: 15.096728
Action reg: 0.003966
  l1.weight: grad_norm = 0.232098
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.165576
Total gradient norm: 0.455795
=== Actor Training Debug (Iteration 7585) ===
Q mean: -16.649246
Q std: 23.261307
Actor loss: 16.653221
Action reg: 0.003974
  l1.weight: grad_norm = 0.198334
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.140239
Total gradient norm: 0.362432
=== Actor Training Debug (Iteration 7586) ===
Q mean: -13.312595
Q std: 19.581091
Actor loss: 13.316560
Action reg: 0.003964
  l1.weight: grad_norm = 0.212716
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.139533
Total gradient norm: 0.434500
=== Actor Training Debug (Iteration 7587) ===
Q mean: -12.726873
Q std: 20.025635
Actor loss: 12.730821
Action reg: 0.003948
  l1.weight: grad_norm = 0.192887
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.149948
Total gradient norm: 0.395577
=== Actor Training Debug (Iteration 7588) ===
Q mean: -17.018354
Q std: 23.581537
Actor loss: 17.022318
Action reg: 0.003963
  l1.weight: grad_norm = 0.223318
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.152673
Total gradient norm: 0.407777
=== Actor Training Debug (Iteration 7589) ===
Q mean: -13.526752
Q std: 19.332281
Actor loss: 13.530695
Action reg: 0.003943
  l1.weight: grad_norm = 0.138165
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.108702
Total gradient norm: 0.308204
=== Actor Training Debug (Iteration 7590) ===
Q mean: -14.755792
Q std: 20.877645
Actor loss: 14.759745
Action reg: 0.003953
  l1.weight: grad_norm = 0.229490
  l1.bias: grad_norm = 0.000729
  l2.weight: grad_norm = 0.150609
Total gradient norm: 0.401694
=== Actor Training Debug (Iteration 7591) ===
Q mean: -13.667204
Q std: 20.973797
Actor loss: 13.671183
Action reg: 0.003979
  l1.weight: grad_norm = 0.143625
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.127169
Total gradient norm: 0.325352
=== Actor Training Debug (Iteration 7592) ===
Q mean: -14.287439
Q std: 20.983253
Actor loss: 14.291375
Action reg: 0.003936
  l1.weight: grad_norm = 0.188292
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.169150
Total gradient norm: 0.442250
=== Actor Training Debug (Iteration 7593) ===
Q mean: -14.992234
Q std: 21.355732
Actor loss: 14.996206
Action reg: 0.003972
  l1.weight: grad_norm = 0.154093
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.108530
Total gradient norm: 0.298425
=== Actor Training Debug (Iteration 7594) ===
Q mean: -16.777822
Q std: 21.882380
Actor loss: 16.781788
Action reg: 0.003966
  l1.weight: grad_norm = 0.316627
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.277721
Total gradient norm: 0.829211
=== Actor Training Debug (Iteration 7595) ===
Q mean: -13.136522
Q std: 18.827414
Actor loss: 13.140487
Action reg: 0.003965
  l1.weight: grad_norm = 0.240808
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.155265
Total gradient norm: 0.426898
=== Actor Training Debug (Iteration 7596) ===
Q mean: -15.383890
Q std: 21.951506
Actor loss: 15.387856
Action reg: 0.003965
  l1.weight: grad_norm = 0.308211
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.254622
Total gradient norm: 0.701374
=== Actor Training Debug (Iteration 7597) ===
Q mean: -13.831552
Q std: 21.405437
Actor loss: 13.835503
Action reg: 0.003951
  l1.weight: grad_norm = 0.159155
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.123968
Total gradient norm: 0.334198
=== Actor Training Debug (Iteration 7598) ===
Q mean: -12.409658
Q std: 21.998106
Actor loss: 12.413625
Action reg: 0.003967
  l1.weight: grad_norm = 0.102684
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.079975
Total gradient norm: 0.245288
=== Actor Training Debug (Iteration 7599) ===
Q mean: -13.786322
Q std: 20.224688
Actor loss: 13.790281
Action reg: 0.003960
  l1.weight: grad_norm = 0.385603
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.321658
Total gradient norm: 0.975873
=== Actor Training Debug (Iteration 7600) ===
Q mean: -15.393744
Q std: 21.863091
Actor loss: 15.397718
Action reg: 0.003975
  l1.weight: grad_norm = 0.670826
  l1.bias: grad_norm = 0.000478
  l2.weight: grad_norm = 0.450606
Total gradient norm: 1.381590
=== Actor Training Debug (Iteration 7601) ===
Q mean: -11.736938
Q std: 19.360872
Actor loss: 11.740860
Action reg: 0.003922
  l1.weight: grad_norm = 0.262083
  l1.bias: grad_norm = 0.001736
  l2.weight: grad_norm = 0.205175
Total gradient norm: 0.543075
=== Actor Training Debug (Iteration 7602) ===
Q mean: -15.105269
Q std: 21.087671
Actor loss: 15.109215
Action reg: 0.003945
  l1.weight: grad_norm = 0.283243
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.213903
Total gradient norm: 0.665476
=== Actor Training Debug (Iteration 7603) ===
Q mean: -11.031798
Q std: 16.929359
Actor loss: 11.035755
Action reg: 0.003957
  l1.weight: grad_norm = 0.375458
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.239892
Total gradient norm: 0.633158
=== Actor Training Debug (Iteration 7604) ===
Q mean: -14.972021
Q std: 21.589981
Actor loss: 14.976001
Action reg: 0.003980
  l1.weight: grad_norm = 0.088022
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.074255
Total gradient norm: 0.208059
=== Actor Training Debug (Iteration 7605) ===
Q mean: -14.871058
Q std: 21.063553
Actor loss: 14.875029
Action reg: 0.003971
  l1.weight: grad_norm = 0.133317
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.096134
Total gradient norm: 0.249926
=== Actor Training Debug (Iteration 7606) ===
Q mean: -12.450327
Q std: 19.643476
Actor loss: 12.454266
Action reg: 0.003938
  l1.weight: grad_norm = 0.220581
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.163323
Total gradient norm: 0.427840
=== Actor Training Debug (Iteration 7607) ===
Q mean: -13.976758
Q std: 21.164032
Actor loss: 13.980719
Action reg: 0.003961
  l1.weight: grad_norm = 0.211645
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.156256
Total gradient norm: 0.435125
=== Actor Training Debug (Iteration 7608) ===
Q mean: -12.376718
Q std: 19.000982
Actor loss: 12.380646
Action reg: 0.003928
  l1.weight: grad_norm = 0.273618
  l1.bias: grad_norm = 0.001363
  l2.weight: grad_norm = 0.177657
Total gradient norm: 0.495947
=== Actor Training Debug (Iteration 7609) ===
Q mean: -13.777240
Q std: 22.512320
Actor loss: 13.781185
Action reg: 0.003946
  l1.weight: grad_norm = 0.195514
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.150546
Total gradient norm: 0.375459
=== Actor Training Debug (Iteration 7610) ===
Q mean: -14.287643
Q std: 19.822052
Actor loss: 14.291591
Action reg: 0.003948
  l1.weight: grad_norm = 0.275473
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.183789
Total gradient norm: 0.541265
=== Actor Training Debug (Iteration 7611) ===
Q mean: -15.520799
Q std: 20.837997
Actor loss: 15.524755
Action reg: 0.003956
  l1.weight: grad_norm = 0.191506
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.144862
Total gradient norm: 0.416649
=== Actor Training Debug (Iteration 7612) ===
Q mean: -13.301920
Q std: 20.141890
Actor loss: 13.305882
Action reg: 0.003963
  l1.weight: grad_norm = 0.166671
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.125353
Total gradient norm: 0.352242
=== Actor Training Debug (Iteration 7613) ===
Q mean: -14.391101
Q std: 21.872274
Actor loss: 14.395077
Action reg: 0.003975
  l1.weight: grad_norm = 0.387159
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.284841
Total gradient norm: 0.843913
=== Actor Training Debug (Iteration 7614) ===
Q mean: -14.317529
Q std: 21.678528
Actor loss: 14.321474
Action reg: 0.003945
  l1.weight: grad_norm = 0.315746
  l1.bias: grad_norm = 0.002133
  l2.weight: grad_norm = 0.209067
Total gradient norm: 0.574016
=== Actor Training Debug (Iteration 7615) ===
Q mean: -15.171761
Q std: 22.673286
Actor loss: 15.175705
Action reg: 0.003945
  l1.weight: grad_norm = 0.091373
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.075606
Total gradient norm: 0.256693
=== Actor Training Debug (Iteration 7616) ===
Q mean: -15.196041
Q std: 22.208298
Actor loss: 15.200007
Action reg: 0.003966
  l1.weight: grad_norm = 0.091748
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.072856
Total gradient norm: 0.209453
=== Actor Training Debug (Iteration 7617) ===
Q mean: -13.951221
Q std: 20.585531
Actor loss: 13.955198
Action reg: 0.003976
  l1.weight: grad_norm = 0.127128
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.107365
Total gradient norm: 0.297249
=== Actor Training Debug (Iteration 7618) ===
Q mean: -14.087402
Q std: 20.711388
Actor loss: 14.091378
Action reg: 0.003976
  l1.weight: grad_norm = 0.132909
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.086643
Total gradient norm: 0.220776
=== Actor Training Debug (Iteration 7619) ===
Q mean: -12.441966
Q std: 19.264294
Actor loss: 12.445930
Action reg: 0.003965
  l1.weight: grad_norm = 0.167967
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.129318
Total gradient norm: 0.332091
=== Actor Training Debug (Iteration 7620) ===
Q mean: -12.981405
Q std: 20.085920
Actor loss: 12.985374
Action reg: 0.003970
  l1.weight: grad_norm = 0.124772
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.097043
Total gradient norm: 0.291941
=== Actor Training Debug (Iteration 7621) ===
Q mean: -13.828082
Q std: 20.460505
Actor loss: 13.832055
Action reg: 0.003973
  l1.weight: grad_norm = 0.164799
  l1.bias: grad_norm = 0.000544
  l2.weight: grad_norm = 0.144345
Total gradient norm: 0.395678
=== Actor Training Debug (Iteration 7622) ===
Q mean: -14.423929
Q std: 20.027592
Actor loss: 14.427886
Action reg: 0.003957
  l1.weight: grad_norm = 0.287944
  l1.bias: grad_norm = 0.000671
  l2.weight: grad_norm = 0.201221
Total gradient norm: 0.619081
=== Actor Training Debug (Iteration 7623) ===
Q mean: -13.970764
Q std: 21.379673
Actor loss: 13.974741
Action reg: 0.003976
  l1.weight: grad_norm = 0.161225
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.129020
Total gradient norm: 0.322808
=== Actor Training Debug (Iteration 7624) ===
Q mean: -14.704723
Q std: 21.241871
Actor loss: 14.708703
Action reg: 0.003980
  l1.weight: grad_norm = 0.254679
  l1.bias: grad_norm = 0.000907
  l2.weight: grad_norm = 0.208144
Total gradient norm: 0.501961
=== Actor Training Debug (Iteration 7625) ===
Q mean: -13.536926
Q std: 22.165953
Actor loss: 13.540899
Action reg: 0.003973
  l1.weight: grad_norm = 0.161917
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.107361
Total gradient norm: 0.278886
=== Actor Training Debug (Iteration 7626) ===
Q mean: -13.321048
Q std: 18.892324
Actor loss: 13.325003
Action reg: 0.003955
  l1.weight: grad_norm = 0.144624
  l1.bias: grad_norm = 0.000738
  l2.weight: grad_norm = 0.112734
Total gradient norm: 0.354136
=== Actor Training Debug (Iteration 7627) ===
Q mean: -12.676112
Q std: 19.028282
Actor loss: 12.680045
Action reg: 0.003933
  l1.weight: grad_norm = 0.293856
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.217060
Total gradient norm: 0.676914
=== Actor Training Debug (Iteration 7628) ===
Q mean: -13.445272
Q std: 20.476202
Actor loss: 13.449231
Action reg: 0.003959
  l1.weight: grad_norm = 0.127531
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.093084
Total gradient norm: 0.280366
=== Actor Training Debug (Iteration 7629) ===
Q mean: -14.419912
Q std: 21.918514
Actor loss: 14.423864
Action reg: 0.003952
  l1.weight: grad_norm = 0.114351
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.077436
Total gradient norm: 0.206227
=== Actor Training Debug (Iteration 7630) ===
Q mean: -13.717218
Q std: 19.682930
Actor loss: 13.721179
Action reg: 0.003961
  l1.weight: grad_norm = 0.263759
  l1.bias: grad_norm = 0.000846
  l2.weight: grad_norm = 0.175133
Total gradient norm: 0.467808
=== Actor Training Debug (Iteration 7631) ===
Q mean: -16.383295
Q std: 21.353916
Actor loss: 16.387281
Action reg: 0.003986
  l1.weight: grad_norm = 0.256373
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.151027
Total gradient norm: 0.400980
=== Actor Training Debug (Iteration 7632) ===
Q mean: -14.675916
Q std: 20.831615
Actor loss: 14.679852
Action reg: 0.003936
  l1.weight: grad_norm = 0.147370
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.153878
Total gradient norm: 0.438655
=== Actor Training Debug (Iteration 7633) ===
Q mean: -13.551148
Q std: 20.981592
Actor loss: 13.555124
Action reg: 0.003976
  l1.weight: grad_norm = 0.209676
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.152979
Total gradient norm: 0.518070
=== Actor Training Debug (Iteration 7634) ===
Q mean: -14.686958
Q std: 22.494387
Actor loss: 14.690919
Action reg: 0.003961
  l1.weight: grad_norm = 0.257495
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.186732
Total gradient norm: 0.488313
=== Actor Training Debug (Iteration 7635) ===
Q mean: -15.579081
Q std: 20.574247
Actor loss: 15.583053
Action reg: 0.003972
  l1.weight: grad_norm = 0.160641
  l1.bias: grad_norm = 0.000481
  l2.weight: grad_norm = 0.131184
Total gradient norm: 0.352246
=== Actor Training Debug (Iteration 7636) ===
Q mean: -15.241217
Q std: 22.076311
Actor loss: 15.245183
Action reg: 0.003966
  l1.weight: grad_norm = 0.179064
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.131590
Total gradient norm: 0.348832
=== Actor Training Debug (Iteration 7637) ===
Q mean: -16.841179
Q std: 23.244621
Actor loss: 16.845156
Action reg: 0.003977
  l1.weight: grad_norm = 0.082337
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.068370
Total gradient norm: 0.215541
=== Actor Training Debug (Iteration 7638) ===
Q mean: -13.567587
Q std: 20.767660
Actor loss: 13.571553
Action reg: 0.003966
  l1.weight: grad_norm = 0.319628
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.216912
Total gradient norm: 0.651898
=== Actor Training Debug (Iteration 7639) ===
Q mean: -13.531175
Q std: 20.682966
Actor loss: 13.535115
Action reg: 0.003941
  l1.weight: grad_norm = 0.293850
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.251786
Total gradient norm: 0.827626
=== Actor Training Debug (Iteration 7640) ===
Q mean: -14.182804
Q std: 21.775537
Actor loss: 14.186767
Action reg: 0.003963
  l1.weight: grad_norm = 0.197375
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.141490
Total gradient norm: 0.390139
=== Actor Training Debug (Iteration 7641) ===
Q mean: -16.251724
Q std: 21.008259
Actor loss: 16.255697
Action reg: 0.003973
  l1.weight: grad_norm = 0.204197
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.165354
Total gradient norm: 0.461375
=== Actor Training Debug (Iteration 7642) ===
Q mean: -15.132940
Q std: 20.360271
Actor loss: 15.136910
Action reg: 0.003970
  l1.weight: grad_norm = 0.105256
  l1.bias: grad_norm = 0.000553
  l2.weight: grad_norm = 0.075550
Total gradient norm: 0.218926
=== Actor Training Debug (Iteration 7643) ===
Q mean: -16.737471
Q std: 22.038656
Actor loss: 16.741430
Action reg: 0.003959
  l1.weight: grad_norm = 0.180889
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.131391
Total gradient norm: 0.337921
=== Actor Training Debug (Iteration 7644) ===
Q mean: -11.862906
Q std: 19.756826
Actor loss: 11.866857
Action reg: 0.003950
  l1.weight: grad_norm = 0.555750
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.371539
Total gradient norm: 0.910125
=== Actor Training Debug (Iteration 7645) ===
Q mean: -13.352816
Q std: 20.512299
Actor loss: 13.356791
Action reg: 0.003976
  l1.weight: grad_norm = 0.115771
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.095191
Total gradient norm: 0.259497
=== Actor Training Debug (Iteration 7646) ===
Q mean: -14.394518
Q std: 21.930948
Actor loss: 14.398493
Action reg: 0.003975
  l1.weight: grad_norm = 0.259373
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.180727
Total gradient norm: 0.499575
=== Actor Training Debug (Iteration 7647) ===
Q mean: -15.027412
Q std: 22.245687
Actor loss: 15.031374
Action reg: 0.003961
  l1.weight: grad_norm = 0.190795
  l1.bias: grad_norm = 0.000585
  l2.weight: grad_norm = 0.138792
Total gradient norm: 0.386518
=== Actor Training Debug (Iteration 7648) ===
Q mean: -16.217823
Q std: 21.819569
Actor loss: 16.221794
Action reg: 0.003972
  l1.weight: grad_norm = 0.169787
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.122409
Total gradient norm: 0.327506
=== Actor Training Debug (Iteration 7649) ===
Q mean: -12.102160
Q std: 20.647482
Actor loss: 12.106129
Action reg: 0.003969
  l1.weight: grad_norm = 0.140092
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.106812
Total gradient norm: 0.268232
=== Actor Training Debug (Iteration 7650) ===
Q mean: -16.754089
Q std: 24.541254
Actor loss: 16.758053
Action reg: 0.003963
  l1.weight: grad_norm = 0.087330
  l1.bias: grad_norm = 0.000632
  l2.weight: grad_norm = 0.066209
Total gradient norm: 0.172065
=== Actor Training Debug (Iteration 7651) ===
Q mean: -14.550247
Q std: 21.014194
Actor loss: 14.554235
Action reg: 0.003989
  l1.weight: grad_norm = 0.064047
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.050733
Total gradient norm: 0.148625
=== Actor Training Debug (Iteration 7652) ===
Q mean: -13.261771
Q std: 19.923338
Actor loss: 13.265750
Action reg: 0.003978
  l1.weight: grad_norm = 0.302089
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.228450
Total gradient norm: 0.595647
=== Actor Training Debug (Iteration 7653) ===
Q mean: -13.982865
Q std: 20.542959
Actor loss: 13.986822
Action reg: 0.003956
  l1.weight: grad_norm = 0.166742
  l1.bias: grad_norm = 0.000405
  l2.weight: grad_norm = 0.130598
Total gradient norm: 0.394740
=== Actor Training Debug (Iteration 7654) ===
Q mean: -12.003456
Q std: 18.669937
Actor loss: 12.007429
Action reg: 0.003973
  l1.weight: grad_norm = 0.347787
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.265378
Total gradient norm: 0.848810
=== Actor Training Debug (Iteration 7655) ===
Q mean: -11.061182
Q std: 18.297646
Actor loss: 11.065148
Action reg: 0.003966
  l1.weight: grad_norm = 0.186411
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.135979
Total gradient norm: 0.410517
=== Actor Training Debug (Iteration 7656) ===
Q mean: -13.784896
Q std: 20.657896
Actor loss: 13.788847
Action reg: 0.003951
  l1.weight: grad_norm = 0.257947
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.187990
Total gradient norm: 0.496106
=== Actor Training Debug (Iteration 7657) ===
Q mean: -16.447153
Q std: 22.506138
Actor loss: 16.451120
Action reg: 0.003967
  l1.weight: grad_norm = 0.274139
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.167533
Total gradient norm: 0.481349
=== Actor Training Debug (Iteration 7658) ===
Q mean: -13.359017
Q std: 21.550411
Actor loss: 13.362986
Action reg: 0.003968
  l1.weight: grad_norm = 0.300507
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.217496
Total gradient norm: 0.579908
=== Actor Training Debug (Iteration 7659) ===
Q mean: -12.635139
Q std: 18.767897
Actor loss: 12.639121
Action reg: 0.003981
  l1.weight: grad_norm = 0.199829
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.115331
Total gradient norm: 0.321159
=== Actor Training Debug (Iteration 7660) ===
Q mean: -15.632673
Q std: 23.147102
Actor loss: 15.636607
Action reg: 0.003934
  l1.weight: grad_norm = 0.424909
  l1.bias: grad_norm = 0.001240
  l2.weight: grad_norm = 0.319994
Total gradient norm: 1.003573
=== Actor Training Debug (Iteration 7661) ===
Q mean: -15.036705
Q std: 22.270451
Actor loss: 15.040689
Action reg: 0.003983
  l1.weight: grad_norm = 0.170950
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.119755
Total gradient norm: 0.319833
=== Actor Training Debug (Iteration 7662) ===
Q mean: -12.993363
Q std: 20.050234
Actor loss: 12.997334
Action reg: 0.003970
  l1.weight: grad_norm = 0.199752
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.145482
Total gradient norm: 0.428536
=== Actor Training Debug (Iteration 7663) ===
Q mean: -14.969351
Q std: 23.078852
Actor loss: 14.973327
Action reg: 0.003976
  l1.weight: grad_norm = 0.172973
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.129407
Total gradient norm: 0.379542
=== Actor Training Debug (Iteration 7664) ===
Q mean: -15.924812
Q std: 22.630566
Actor loss: 15.928779
Action reg: 0.003967
  l1.weight: grad_norm = 0.154735
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.126338
Total gradient norm: 0.348046
=== Actor Training Debug (Iteration 7665) ===
Q mean: -12.984056
Q std: 20.284485
Actor loss: 12.987994
Action reg: 0.003939
  l1.weight: grad_norm = 0.165593
  l1.bias: grad_norm = 0.000642
  l2.weight: grad_norm = 0.139875
Total gradient norm: 0.408664
=== Actor Training Debug (Iteration 7666) ===
Q mean: -12.406021
Q std: 18.525913
Actor loss: 12.409984
Action reg: 0.003963
  l1.weight: grad_norm = 0.294065
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.203087
Total gradient norm: 0.588164
=== Actor Training Debug (Iteration 7667) ===
Q mean: -14.412626
Q std: 18.634954
Actor loss: 14.416603
Action reg: 0.003977
  l1.weight: grad_norm = 0.200200
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.137041
Total gradient norm: 0.389836
=== Actor Training Debug (Iteration 7668) ===
Q mean: -13.765306
Q std: 19.810720
Actor loss: 13.769281
Action reg: 0.003975
  l1.weight: grad_norm = 0.224101
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.151628
Total gradient norm: 0.401150
=== Actor Training Debug (Iteration 7669) ===
Q mean: -13.909272
Q std: 21.713148
Actor loss: 13.913256
Action reg: 0.003984
  l1.weight: grad_norm = 0.234244
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.181480
Total gradient norm: 0.521256
=== Actor Training Debug (Iteration 7670) ===
Q mean: -14.784032
Q std: 21.014734
Actor loss: 14.787986
Action reg: 0.003953
  l1.weight: grad_norm = 0.336068
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.304460
Total gradient norm: 1.168394
=== Actor Training Debug (Iteration 7671) ===
Q mean: -12.054887
Q std: 19.311956
Actor loss: 12.058860
Action reg: 0.003973
  l1.weight: grad_norm = 0.213259
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.143019
Total gradient norm: 0.398834
=== Actor Training Debug (Iteration 7672) ===
Q mean: -14.033534
Q std: 20.967226
Actor loss: 14.037498
Action reg: 0.003963
  l1.weight: grad_norm = 0.296060
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.221037
Total gradient norm: 0.609511
=== Actor Training Debug (Iteration 7673) ===
Q mean: -17.162403
Q std: 22.988169
Actor loss: 17.166368
Action reg: 0.003966
  l1.weight: grad_norm = 0.103890
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.082856
Total gradient norm: 0.243937
=== Actor Training Debug (Iteration 7674) ===
Q mean: -16.806732
Q std: 20.919754
Actor loss: 16.810711
Action reg: 0.003978
  l1.weight: grad_norm = 0.219056
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.150840
Total gradient norm: 0.401381
=== Actor Training Debug (Iteration 7675) ===
Q mean: -12.879081
Q std: 20.287102
Actor loss: 12.883032
Action reg: 0.003951
  l1.weight: grad_norm = 0.206865
  l1.bias: grad_norm = 0.000566
  l2.weight: grad_norm = 0.179010
Total gradient norm: 0.489314
=== Actor Training Debug (Iteration 7676) ===
Q mean: -15.114616
Q std: 21.548191
Actor loss: 15.118588
Action reg: 0.003972
  l1.weight: grad_norm = 0.252845
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.207733
Total gradient norm: 0.650913
=== Actor Training Debug (Iteration 7677) ===
Q mean: -13.890741
Q std: 20.783773
Actor loss: 13.894705
Action reg: 0.003964
  l1.weight: grad_norm = 0.149451
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.112553
Total gradient norm: 0.323444
=== Actor Training Debug (Iteration 7678) ===
Q mean: -14.317827
Q std: 20.912498
Actor loss: 14.321787
Action reg: 0.003959
  l1.weight: grad_norm = 0.240027
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.168864
Total gradient norm: 0.392071
=== Actor Training Debug (Iteration 7679) ===
Q mean: -13.799692
Q std: 21.476551
Actor loss: 13.803639
Action reg: 0.003947
  l1.weight: grad_norm = 0.363474
  l1.bias: grad_norm = 0.000543
  l2.weight: grad_norm = 0.235496
Total gradient norm: 0.631625
=== Actor Training Debug (Iteration 7680) ===
Q mean: -15.718338
Q std: 21.795282
Actor loss: 15.722306
Action reg: 0.003968
  l1.weight: grad_norm = 0.057448
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.047739
Total gradient norm: 0.141255
=== Actor Training Debug (Iteration 7681) ===
Q mean: -14.075523
Q std: 19.949036
Actor loss: 14.079479
Action reg: 0.003956
  l1.weight: grad_norm = 0.141025
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.137511
Total gradient norm: 0.434627
=== Actor Training Debug (Iteration 7682) ===
Q mean: -15.012487
Q std: 22.688768
Actor loss: 15.016458
Action reg: 0.003971
  l1.weight: grad_norm = 0.116919
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.093738
Total gradient norm: 0.274773
=== Actor Training Debug (Iteration 7683) ===
Q mean: -14.533927
Q std: 21.562567
Actor loss: 14.537896
Action reg: 0.003969
  l1.weight: grad_norm = 0.204627
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.165351
Total gradient norm: 0.435697
=== Actor Training Debug (Iteration 7684) ===
Q mean: -13.511374
Q std: 20.391743
Actor loss: 13.515326
Action reg: 0.003953
  l1.weight: grad_norm = 0.245756
  l1.bias: grad_norm = 0.001805
  l2.weight: grad_norm = 0.171121
Total gradient norm: 0.645308
=== Actor Training Debug (Iteration 7685) ===
Q mean: -13.156859
Q std: 19.452171
Actor loss: 13.160828
Action reg: 0.003969
  l1.weight: grad_norm = 0.157962
  l1.bias: grad_norm = 0.001680
  l2.weight: grad_norm = 0.134884
Total gradient norm: 0.394676
=== Actor Training Debug (Iteration 7686) ===
Q mean: -15.082031
Q std: 20.733425
Actor loss: 15.086008
Action reg: 0.003977
  l1.weight: grad_norm = 0.135783
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.106119
Total gradient norm: 0.307451
=== Actor Training Debug (Iteration 7687) ===
Q mean: -14.025800
Q std: 20.254248
Actor loss: 14.029754
Action reg: 0.003954
  l1.weight: grad_norm = 0.248448
  l1.bias: grad_norm = 0.000613
  l2.weight: grad_norm = 0.193391
Total gradient norm: 0.533376
=== Actor Training Debug (Iteration 7688) ===
Q mean: -15.835009
Q std: 21.675550
Actor loss: 15.838978
Action reg: 0.003969
  l1.weight: grad_norm = 0.423838
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.309354
Total gradient norm: 0.825347
=== Actor Training Debug (Iteration 7689) ===
Q mean: -14.326620
Q std: 23.622715
Actor loss: 14.330585
Action reg: 0.003964
  l1.weight: grad_norm = 0.215648
  l1.bias: grad_norm = 0.000454
  l2.weight: grad_norm = 0.132393
Total gradient norm: 0.372428
=== Actor Training Debug (Iteration 7690) ===
Q mean: -12.986873
Q std: 21.022017
Actor loss: 12.990821
Action reg: 0.003948
  l1.weight: grad_norm = 0.243136
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.167260
Total gradient norm: 0.441075
=== Actor Training Debug (Iteration 7691) ===
Q mean: -15.496085
Q std: 21.929478
Actor loss: 15.500068
Action reg: 0.003982
  l1.weight: grad_norm = 0.214690
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.136327
Total gradient norm: 0.352969
=== Actor Training Debug (Iteration 7692) ===
Q mean: -14.159155
Q std: 21.679609
Actor loss: 14.163115
Action reg: 0.003959
  l1.weight: grad_norm = 0.203759
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.162577
Total gradient norm: 0.476345
=== Actor Training Debug (Iteration 7693) ===
Q mean: -16.428101
Q std: 21.418356
Actor loss: 16.432055
Action reg: 0.003953
  l1.weight: grad_norm = 0.440233
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.346133
Total gradient norm: 0.953668
=== Actor Training Debug (Iteration 7694) ===
Q mean: -18.016226
Q std: 23.508696
Actor loss: 18.020159
Action reg: 0.003933
  l1.weight: grad_norm = 0.399371
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.267946
Total gradient norm: 0.795836
=== Actor Training Debug (Iteration 7695) ===
Q mean: -15.319053
Q std: 21.542631
Actor loss: 15.322997
Action reg: 0.003945
  l1.weight: grad_norm = 0.182582
  l1.bias: grad_norm = 0.000526
  l2.weight: grad_norm = 0.162966
Total gradient norm: 0.500258
=== Actor Training Debug (Iteration 7696) ===
Q mean: -14.730438
Q std: 22.018219
Actor loss: 14.734422
Action reg: 0.003984
  l1.weight: grad_norm = 0.185987
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.142046
Total gradient norm: 0.353273
=== Actor Training Debug (Iteration 7697) ===
Q mean: -15.665848
Q std: 21.771517
Actor loss: 15.669821
Action reg: 0.003973
  l1.weight: grad_norm = 0.164060
  l1.bias: grad_norm = 0.000728
  l2.weight: grad_norm = 0.126953
Total gradient norm: 0.335633
=== Actor Training Debug (Iteration 7698) ===
Q mean: -14.480725
Q std: 21.438875
Actor loss: 14.484653
Action reg: 0.003928
  l1.weight: grad_norm = 0.286906
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.260163
Total gradient norm: 0.958826
=== Actor Training Debug (Iteration 7699) ===
Q mean: -14.904426
Q std: 20.575388
Actor loss: 14.908397
Action reg: 0.003971
  l1.weight: grad_norm = 0.232217
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.179486
Total gradient norm: 0.495425
=== Actor Training Debug (Iteration 7700) ===
Q mean: -12.956481
Q std: 20.590937
Actor loss: 12.960455
Action reg: 0.003974
  l1.weight: grad_norm = 0.186431
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.137193
Total gradient norm: 0.365858
=== Actor Training Debug (Iteration 7701) ===
Q mean: -12.251497
Q std: 20.515923
Actor loss: 12.255469
Action reg: 0.003972
  l1.weight: grad_norm = 0.155248
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.128340
Total gradient norm: 0.394514
=== Actor Training Debug (Iteration 7702) ===
Q mean: -14.702011
Q std: 21.028065
Actor loss: 14.705972
Action reg: 0.003961
  l1.weight: grad_norm = 0.229435
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.157383
Total gradient norm: 0.446886
=== Actor Training Debug (Iteration 7703) ===
Q mean: -14.463531
Q std: 19.636528
Actor loss: 14.467461
Action reg: 0.003929
  l1.weight: grad_norm = 0.238553
  l1.bias: grad_norm = 0.000653
  l2.weight: grad_norm = 0.194667
Total gradient norm: 0.673137
=== Actor Training Debug (Iteration 7704) ===
Q mean: -11.779124
Q std: 17.890244
Actor loss: 11.783102
Action reg: 0.003978
  l1.weight: grad_norm = 0.142063
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.116236
Total gradient norm: 0.330229
=== Actor Training Debug (Iteration 7705) ===
Q mean: -12.764302
Q std: 20.257427
Actor loss: 12.768263
Action reg: 0.003961
  l1.weight: grad_norm = 0.252466
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.185386
Total gradient norm: 0.508091
=== Actor Training Debug (Iteration 7706) ===
Q mean: -15.821457
Q std: 21.858013
Actor loss: 15.825384
Action reg: 0.003928
  l1.weight: grad_norm = 0.258699
  l1.bias: grad_norm = 0.000691
  l2.weight: grad_norm = 0.209798
Total gradient norm: 0.629005
=== Actor Training Debug (Iteration 7707) ===
Q mean: -14.614565
Q std: 20.846828
Actor loss: 14.618505
Action reg: 0.003940
  l1.weight: grad_norm = 0.584694
  l1.bias: grad_norm = 0.000656
  l2.weight: grad_norm = 0.402101
Total gradient norm: 1.029912
=== Actor Training Debug (Iteration 7708) ===
Q mean: -15.412506
Q std: 22.773209
Actor loss: 15.416460
Action reg: 0.003954
  l1.weight: grad_norm = 0.221893
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.135326
Total gradient norm: 0.360860
=== Actor Training Debug (Iteration 7709) ===
Q mean: -15.275007
Q std: 21.551802
Actor loss: 15.278967
Action reg: 0.003960
  l1.weight: grad_norm = 0.179706
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.158771
Total gradient norm: 0.468531
=== Actor Training Debug (Iteration 7710) ===
Q mean: -13.429598
Q std: 20.074060
Actor loss: 13.433545
Action reg: 0.003947
  l1.weight: grad_norm = 0.522347
  l1.bias: grad_norm = 0.000683
  l2.weight: grad_norm = 0.390018
Total gradient norm: 1.357556
=== Actor Training Debug (Iteration 7711) ===
Q mean: -13.972710
Q std: 21.036900
Actor loss: 13.976665
Action reg: 0.003956
  l1.weight: grad_norm = 0.189929
  l1.bias: grad_norm = 0.000629
  l2.weight: grad_norm = 0.181019
Total gradient norm: 0.560398
=== Actor Training Debug (Iteration 7712) ===
Q mean: -15.524349
Q std: 22.080538
Actor loss: 15.528303
Action reg: 0.003954
  l1.weight: grad_norm = 0.252933
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.196121
Total gradient norm: 0.635582
=== Actor Training Debug (Iteration 7713) ===
Q mean: -14.628916
Q std: 21.335505
Actor loss: 14.632874
Action reg: 0.003958
  l1.weight: grad_norm = 0.127658
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.096020
Total gradient norm: 0.307391
=== Actor Training Debug (Iteration 7714) ===
Q mean: -18.329151
Q std: 23.714766
Actor loss: 18.333109
Action reg: 0.003958
  l1.weight: grad_norm = 0.281201
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.207963
Total gradient norm: 0.752948
=== Actor Training Debug (Iteration 7715) ===
Q mean: -15.924067
Q std: 22.253094
Actor loss: 15.928026
Action reg: 0.003959
  l1.weight: grad_norm = 0.267721
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.206774
Total gradient norm: 0.550597
=== Actor Training Debug (Iteration 7716) ===
Q mean: -12.980469
Q std: 19.135040
Actor loss: 12.984419
Action reg: 0.003950
  l1.weight: grad_norm = 0.406288
  l1.bias: grad_norm = 0.000671
  l2.weight: grad_norm = 0.319115
Total gradient norm: 0.951081
=== Actor Training Debug (Iteration 7717) ===
Q mean: -12.476377
Q std: 19.240070
Actor loss: 12.480339
Action reg: 0.003961
  l1.weight: grad_norm = 0.151616
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.110463
Total gradient norm: 0.352332
=== Actor Training Debug (Iteration 7718) ===
Q mean: -13.011003
Q std: 20.364416
Actor loss: 13.014955
Action reg: 0.003951
  l1.weight: grad_norm = 0.204174
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.178094
Total gradient norm: 0.576781
=== Actor Training Debug (Iteration 7719) ===
Q mean: -15.239666
Q std: 21.556868
Actor loss: 15.243638
Action reg: 0.003972
  l1.weight: grad_norm = 0.247334
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.189210
Total gradient norm: 0.700397
=== Actor Training Debug (Iteration 7720) ===
Q mean: -13.430961
Q std: 21.116655
Actor loss: 13.434924
Action reg: 0.003963
  l1.weight: grad_norm = 0.327969
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.238349
Total gradient norm: 0.660529
=== Actor Training Debug (Iteration 7721) ===
Q mean: -14.304813
Q std: 19.844843
Actor loss: 14.308763
Action reg: 0.003949
  l1.weight: grad_norm = 0.215213
  l1.bias: grad_norm = 0.000575
  l2.weight: grad_norm = 0.164600
Total gradient norm: 0.459441
=== Actor Training Debug (Iteration 7722) ===
Q mean: -18.481022
Q std: 23.753277
Actor loss: 18.484978
Action reg: 0.003955
  l1.weight: grad_norm = 0.194900
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.158296
Total gradient norm: 0.468860
=== Actor Training Debug (Iteration 7723) ===
Q mean: -15.829644
Q std: 21.936895
Actor loss: 15.833589
Action reg: 0.003944
  l1.weight: grad_norm = 0.109588
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.093349
Total gradient norm: 0.259730
=== Actor Training Debug (Iteration 7724) ===
Q mean: -13.592255
Q std: 21.358948
Actor loss: 13.596207
Action reg: 0.003952
  l1.weight: grad_norm = 0.196010
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.136032
Total gradient norm: 0.361744
=== Actor Training Debug (Iteration 7725) ===
Q mean: -12.424279
Q std: 19.995653
Actor loss: 12.428250
Action reg: 0.003971
  l1.weight: grad_norm = 0.209613
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.158869
Total gradient norm: 0.435208
=== Actor Training Debug (Iteration 7726) ===
Q mean: -15.548573
Q std: 23.781464
Actor loss: 15.552539
Action reg: 0.003966
  l1.weight: grad_norm = 0.300052
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.234792
Total gradient norm: 0.612309
=== Actor Training Debug (Iteration 7727) ===
Q mean: -15.007720
Q std: 22.497702
Actor loss: 15.011679
Action reg: 0.003959
  l1.weight: grad_norm = 0.277434
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.215463
Total gradient norm: 0.657596
=== Actor Training Debug (Iteration 7728) ===
Q mean: -13.965261
Q std: 21.651608
Actor loss: 13.969200
Action reg: 0.003940
  l1.weight: grad_norm = 0.203427
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.149167
Total gradient norm: 0.410526
=== Actor Training Debug (Iteration 7729) ===
Q mean: -12.842699
Q std: 19.307203
Actor loss: 12.846666
Action reg: 0.003967
  l1.weight: grad_norm = 0.191571
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.144138
Total gradient norm: 0.355051
=== Actor Training Debug (Iteration 7730) ===
Q mean: -17.276844
Q std: 23.740593
Actor loss: 17.280828
Action reg: 0.003985
  l1.weight: grad_norm = 0.179442
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.149075
Total gradient norm: 0.396471
=== Actor Training Debug (Iteration 7731) ===
Q mean: -14.708497
Q std: 20.875002
Actor loss: 14.712452
Action reg: 0.003955
  l1.weight: grad_norm = 0.232485
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.156260
Total gradient norm: 0.428506
=== Actor Training Debug (Iteration 7732) ===
Q mean: -16.298321
Q std: 22.151695
Actor loss: 16.302282
Action reg: 0.003961
  l1.weight: grad_norm = 0.195848
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.152281
Total gradient norm: 0.438287
=== Actor Training Debug (Iteration 7733) ===
Q mean: -16.301069
Q std: 21.744024
Actor loss: 16.305042
Action reg: 0.003973
  l1.weight: grad_norm = 0.166099
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.117328
Total gradient norm: 0.380390
=== Actor Training Debug (Iteration 7734) ===
Q mean: -13.122225
Q std: 19.321543
Actor loss: 13.126197
Action reg: 0.003972
  l1.weight: grad_norm = 0.412519
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.344206
Total gradient norm: 1.010063
=== Actor Training Debug (Iteration 7735) ===
Q mean: -15.394474
Q std: 22.848408
Actor loss: 15.398423
Action reg: 0.003949
  l1.weight: grad_norm = 0.249573
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.188814
Total gradient norm: 0.497491
=== Actor Training Debug (Iteration 7736) ===
Q mean: -12.393229
Q std: 20.548210
Actor loss: 12.397193
Action reg: 0.003965
  l1.weight: grad_norm = 0.164262
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.120275
Total gradient norm: 0.317054
=== Actor Training Debug (Iteration 7737) ===
Q mean: -14.735422
Q std: 21.478540
Actor loss: 14.739400
Action reg: 0.003977
  l1.weight: grad_norm = 0.397726
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.245200
Total gradient norm: 0.678584
=== Actor Training Debug (Iteration 7738) ===
Q mean: -14.321285
Q std: 20.826994
Actor loss: 14.325236
Action reg: 0.003951
  l1.weight: grad_norm = 0.202966
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.161002
Total gradient norm: 0.523920
=== Actor Training Debug (Iteration 7739) ===
Q mean: -15.861591
Q std: 22.125299
Actor loss: 15.865520
Action reg: 0.003928
  l1.weight: grad_norm = 0.162370
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.141890
Total gradient norm: 0.347663
=== Actor Training Debug (Iteration 7740) ===
Q mean: -15.935924
Q std: 22.231441
Actor loss: 15.939887
Action reg: 0.003963
  l1.weight: grad_norm = 0.175818
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.129677
Total gradient norm: 0.350723
=== Actor Training Debug (Iteration 7741) ===
Q mean: -12.908718
Q std: 18.718689
Actor loss: 12.912678
Action reg: 0.003960
  l1.weight: grad_norm = 0.508777
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.441791
Total gradient norm: 1.260036
=== Actor Training Debug (Iteration 7742) ===
Q mean: -13.964438
Q std: 21.149063
Actor loss: 13.968390
Action reg: 0.003952
  l1.weight: grad_norm = 0.182853
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.166264
Total gradient norm: 0.439875
=== Actor Training Debug (Iteration 7743) ===
Q mean: -14.960438
Q std: 20.238340
Actor loss: 14.964395
Action reg: 0.003957
  l1.weight: grad_norm = 0.293946
  l1.bias: grad_norm = 0.000534
  l2.weight: grad_norm = 0.259267
Total gradient norm: 0.755606
=== Actor Training Debug (Iteration 7744) ===
Q mean: -16.087933
Q std: 21.779308
Actor loss: 16.091900
Action reg: 0.003966
  l1.weight: grad_norm = 0.198375
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.157741
Total gradient norm: 0.401758
=== Actor Training Debug (Iteration 7745) ===
Q mean: -15.512296
Q std: 23.193193
Actor loss: 15.516262
Action reg: 0.003966
  l1.weight: grad_norm = 0.161274
  l1.bias: grad_norm = 0.001378
  l2.weight: grad_norm = 0.113826
Total gradient norm: 0.344063
=== Actor Training Debug (Iteration 7746) ===
Q mean: -14.033016
Q std: 21.203487
Actor loss: 14.036988
Action reg: 0.003972
  l1.weight: grad_norm = 0.119500
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.091704
Total gradient norm: 0.252176
=== Actor Training Debug (Iteration 7747) ===
Q mean: -13.216978
Q std: 19.309591
Actor loss: 13.220922
Action reg: 0.003943
  l1.weight: grad_norm = 0.246382
  l1.bias: grad_norm = 0.000563
  l2.weight: grad_norm = 0.168897
Total gradient norm: 0.456062
=== Actor Training Debug (Iteration 7748) ===
Q mean: -13.765873
Q std: 19.911959
Actor loss: 13.769846
Action reg: 0.003973
  l1.weight: grad_norm = 0.265914
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.194627
Total gradient norm: 0.519516
=== Actor Training Debug (Iteration 7749) ===
Q mean: -10.761595
Q std: 17.988556
Actor loss: 10.765546
Action reg: 0.003951
  l1.weight: grad_norm = 0.159742
  l1.bias: grad_norm = 0.000860
  l2.weight: grad_norm = 0.118522
Total gradient norm: 0.376074
=== Actor Training Debug (Iteration 7750) ===
Q mean: -14.805745
Q std: 21.073586
Actor loss: 14.809691
Action reg: 0.003946
  l1.weight: grad_norm = 0.202972
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.155495
Total gradient norm: 0.444183
=== Actor Training Debug (Iteration 7751) ===
Q mean: -15.744084
Q std: 21.324295
Actor loss: 15.748058
Action reg: 0.003974
  l1.weight: grad_norm = 0.233609
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.178565
Total gradient norm: 0.641440
=== Actor Training Debug (Iteration 7752) ===
Q mean: -14.656857
Q std: 21.954374
Actor loss: 14.660812
Action reg: 0.003955
  l1.weight: grad_norm = 0.309676
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.272649
Total gradient norm: 0.753912
=== Actor Training Debug (Iteration 7753) ===
Q mean: -13.626200
Q std: 18.836443
Actor loss: 13.630139
Action reg: 0.003940
  l1.weight: grad_norm = 0.312341
  l1.bias: grad_norm = 0.000636
  l2.weight: grad_norm = 0.226986
Total gradient norm: 0.718420
=== Actor Training Debug (Iteration 7754) ===
Q mean: -14.642604
Q std: 21.590086
Actor loss: 14.646568
Action reg: 0.003965
  l1.weight: grad_norm = 0.194197
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.157671
Total gradient norm: 0.428517
=== Actor Training Debug (Iteration 7755) ===
Q mean: -15.112040
Q std: 22.256208
Actor loss: 15.116014
Action reg: 0.003974
  l1.weight: grad_norm = 0.128374
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.106604
Total gradient norm: 0.293019
=== Actor Training Debug (Iteration 7756) ===
Q mean: -14.152685
Q std: 21.044065
Actor loss: 14.156640
Action reg: 0.003955
  l1.weight: grad_norm = 0.261792
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.206278
Total gradient norm: 0.560463
=== Actor Training Debug (Iteration 7757) ===
Q mean: -15.308992
Q std: 22.616301
Actor loss: 15.312948
Action reg: 0.003956
  l1.weight: grad_norm = 0.191744
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.160856
Total gradient norm: 0.560676
=== Actor Training Debug (Iteration 7758) ===
Q mean: -12.547229
Q std: 19.552839
Actor loss: 12.551178
Action reg: 0.003949
  l1.weight: grad_norm = 0.238238
  l1.bias: grad_norm = 0.000721
  l2.weight: grad_norm = 0.198860
Total gradient norm: 0.561808
=== Actor Training Debug (Iteration 7759) ===
Q mean: -14.048702
Q std: 21.226656
Actor loss: 14.052667
Action reg: 0.003965
  l1.weight: grad_norm = 0.133567
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.095526
Total gradient norm: 0.270963
=== Actor Training Debug (Iteration 7760) ===
Q mean: -16.061869
Q std: 22.207184
Actor loss: 16.065836
Action reg: 0.003967
  l1.weight: grad_norm = 0.155144
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.108216
Total gradient norm: 0.315436
=== Actor Training Debug (Iteration 7761) ===
Q mean: -13.024991
Q std: 20.239527
Actor loss: 13.028966
Action reg: 0.003975
  l1.weight: grad_norm = 0.267267
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.219093
Total gradient norm: 0.544585
=== Actor Training Debug (Iteration 7762) ===
Q mean: -14.104630
Q std: 19.139929
Actor loss: 14.108613
Action reg: 0.003984
  l1.weight: grad_norm = 0.135539
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.107094
Total gradient norm: 0.304782
=== Actor Training Debug (Iteration 7763) ===
Q mean: -12.059669
Q std: 19.444530
Actor loss: 12.063626
Action reg: 0.003957
  l1.weight: grad_norm = 0.316297
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.233316
Total gradient norm: 0.619307
=== Actor Training Debug (Iteration 7764) ===
Q mean: -13.674412
Q std: 20.691704
Actor loss: 13.678381
Action reg: 0.003969
  l1.weight: grad_norm = 0.192994
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.155150
Total gradient norm: 0.476014
=== Actor Training Debug (Iteration 7765) ===
Q mean: -14.524423
Q std: 20.277075
Actor loss: 14.528384
Action reg: 0.003961
  l1.weight: grad_norm = 0.299856
  l1.bias: grad_norm = 0.000414
  l2.weight: grad_norm = 0.242672
Total gradient norm: 0.607346
=== Actor Training Debug (Iteration 7766) ===
Q mean: -14.499849
Q std: 21.068911
Actor loss: 14.503805
Action reg: 0.003956
  l1.weight: grad_norm = 0.194771
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.133754
Total gradient norm: 0.378317
=== Actor Training Debug (Iteration 7767) ===
Q mean: -13.109102
Q std: 19.248240
Actor loss: 13.113045
Action reg: 0.003943
  l1.weight: grad_norm = 0.158530
  l1.bias: grad_norm = 0.000633
  l2.weight: grad_norm = 0.130310
Total gradient norm: 0.384594
=== Actor Training Debug (Iteration 7768) ===
Q mean: -13.188904
Q std: 20.250713
Actor loss: 13.192841
Action reg: 0.003937
  l1.weight: grad_norm = 0.231225
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.182211
Total gradient norm: 0.505091
=== Actor Training Debug (Iteration 7769) ===
Q mean: -14.117077
Q std: 20.564503
Actor loss: 14.121035
Action reg: 0.003958
  l1.weight: grad_norm = 0.119610
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.099576
Total gradient norm: 0.279327
=== Actor Training Debug (Iteration 7770) ===
Q mean: -14.783094
Q std: 20.323673
Actor loss: 14.787041
Action reg: 0.003946
  l1.weight: grad_norm = 0.331158
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.256177
Total gradient norm: 0.729184
=== Actor Training Debug (Iteration 7771) ===
Q mean: -14.380363
Q std: 20.528152
Actor loss: 14.384306
Action reg: 0.003942
  l1.weight: grad_norm = 0.227831
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.182412
Total gradient norm: 0.536420
=== Actor Training Debug (Iteration 7772) ===
Q mean: -15.904440
Q std: 21.215422
Actor loss: 15.908401
Action reg: 0.003961
  l1.weight: grad_norm = 0.165147
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.126231
Total gradient norm: 0.336484
=== Actor Training Debug (Iteration 7773) ===
Q mean: -16.716686
Q std: 22.720816
Actor loss: 16.720640
Action reg: 0.003954
  l1.weight: grad_norm = 0.254641
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.159093
Total gradient norm: 0.442541
=== Actor Training Debug (Iteration 7774) ===
Q mean: -13.757250
Q std: 19.929512
Actor loss: 13.761230
Action reg: 0.003979
  l1.weight: grad_norm = 0.150072
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.120260
Total gradient norm: 0.312381
=== Actor Training Debug (Iteration 7775) ===
Q mean: -14.743213
Q std: 20.724890
Actor loss: 14.747158
Action reg: 0.003945
  l1.weight: grad_norm = 0.299929
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.257727
Total gradient norm: 0.726613
=== Actor Training Debug (Iteration 7776) ===
Q mean: -12.797715
Q std: 20.267759
Actor loss: 12.801691
Action reg: 0.003975
  l1.weight: grad_norm = 0.299889
  l1.bias: grad_norm = 0.001135
  l2.weight: grad_norm = 0.260582
Total gradient norm: 0.701361
=== Actor Training Debug (Iteration 7777) ===
Q mean: -12.225741
Q std: 18.080242
Actor loss: 12.229703
Action reg: 0.003961
  l1.weight: grad_norm = 0.314983
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.213597
Total gradient norm: 0.558168
=== Actor Training Debug (Iteration 7778) ===
Q mean: -12.614254
Q std: 18.480206
Actor loss: 12.618230
Action reg: 0.003976
  l1.weight: grad_norm = 0.207689
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.160917
Total gradient norm: 0.448935
=== Actor Training Debug (Iteration 7779) ===
Q mean: -14.561362
Q std: 22.422525
Actor loss: 14.565331
Action reg: 0.003968
  l1.weight: grad_norm = 0.124419
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.100950
Total gradient norm: 0.339137
=== Actor Training Debug (Iteration 7780) ===
Q mean: -15.710337
Q std: 21.486984
Actor loss: 15.714282
Action reg: 0.003945
  l1.weight: grad_norm = 0.149199
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.109421
Total gradient norm: 0.287548
=== Actor Training Debug (Iteration 7781) ===
Q mean: -14.510440
Q std: 22.839943
Actor loss: 14.514407
Action reg: 0.003967
  l1.weight: grad_norm = 0.259996
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.201706
Total gradient norm: 0.601438
=== Actor Training Debug (Iteration 7782) ===
Q mean: -14.438253
Q std: 20.739887
Actor loss: 14.442191
Action reg: 0.003938
  l1.weight: grad_norm = 0.331492
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.239045
Total gradient norm: 0.634663
=== Actor Training Debug (Iteration 7783) ===
Q mean: -13.401359
Q std: 20.220381
Actor loss: 13.405315
Action reg: 0.003956
  l1.weight: grad_norm = 0.308581
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.252244
Total gradient norm: 0.707082
=== Actor Training Debug (Iteration 7784) ===
Q mean: -11.861681
Q std: 20.211708
Actor loss: 11.865638
Action reg: 0.003957
  l1.weight: grad_norm = 0.232491
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.200212
Total gradient norm: 0.592446
=== Actor Training Debug (Iteration 7785) ===
Q mean: -13.047787
Q std: 21.261927
Actor loss: 13.051759
Action reg: 0.003972
  l1.weight: grad_norm = 0.558878
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.405703
Total gradient norm: 1.060299
=== Actor Training Debug (Iteration 7786) ===
Q mean: -14.892921
Q std: 20.593992
Actor loss: 14.896895
Action reg: 0.003973
  l1.weight: grad_norm = 0.131945
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.105376
Total gradient norm: 0.297376
=== Actor Training Debug (Iteration 7787) ===
Q mean: -15.481499
Q std: 22.226875
Actor loss: 15.485454
Action reg: 0.003955
  l1.weight: grad_norm = 0.344961
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.237433
Total gradient norm: 0.762056
=== Actor Training Debug (Iteration 7788) ===
Q mean: -14.225506
Q std: 21.683764
Actor loss: 14.229464
Action reg: 0.003958
  l1.weight: grad_norm = 0.260385
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.202114
Total gradient norm: 0.579580
=== Actor Training Debug (Iteration 7789) ===
Q mean: -14.200321
Q std: 19.930269
Actor loss: 14.204273
Action reg: 0.003952
  l1.weight: grad_norm = 0.451945
  l1.bias: grad_norm = 0.000651
  l2.weight: grad_norm = 0.300641
Total gradient norm: 0.805956
=== Actor Training Debug (Iteration 7790) ===
Q mean: -13.702183
Q std: 21.484886
Actor loss: 13.706139
Action reg: 0.003956
  l1.weight: grad_norm = 0.388599
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.254603
Total gradient norm: 0.660271
=== Actor Training Debug (Iteration 7791) ===
Q mean: -14.037380
Q std: 21.187389
Actor loss: 14.041345
Action reg: 0.003964
  l1.weight: grad_norm = 0.290834
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.196397
Total gradient norm: 0.589300
=== Actor Training Debug (Iteration 7792) ===
Q mean: -14.723837
Q std: 21.515875
Actor loss: 14.727795
Action reg: 0.003957
  l1.weight: grad_norm = 0.361183
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.251097
Total gradient norm: 0.775794
=== Actor Training Debug (Iteration 7793) ===
Q mean: -13.616430
Q std: 20.460361
Actor loss: 13.620379
Action reg: 0.003950
  l1.weight: grad_norm = 0.135297
  l1.bias: grad_norm = 0.000638
  l2.weight: grad_norm = 0.106083
Total gradient norm: 0.282080
=== Actor Training Debug (Iteration 7794) ===
Q mean: -16.332180
Q std: 23.235588
Actor loss: 16.336155
Action reg: 0.003974
  l1.weight: grad_norm = 0.149597
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.121145
Total gradient norm: 0.413717
=== Actor Training Debug (Iteration 7795) ===
Q mean: -14.438637
Q std: 21.125998
Actor loss: 14.442617
Action reg: 0.003981
  l1.weight: grad_norm = 0.151153
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.110685
Total gradient norm: 0.287995
=== Actor Training Debug (Iteration 7796) ===
Q mean: -13.711359
Q std: 20.722120
Actor loss: 13.715322
Action reg: 0.003964
  l1.weight: grad_norm = 0.181845
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.141806
Total gradient norm: 0.402689
=== Actor Training Debug (Iteration 7797) ===
Q mean: -14.605017
Q std: 21.048573
Actor loss: 14.608988
Action reg: 0.003971
  l1.weight: grad_norm = 0.184305
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.147499
Total gradient norm: 0.400790
=== Actor Training Debug (Iteration 7798) ===
Q mean: -14.663386
Q std: 20.380539
Actor loss: 14.667334
Action reg: 0.003947
  l1.weight: grad_norm = 0.191562
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.162879
Total gradient norm: 0.433851
=== Actor Training Debug (Iteration 7799) ===
Q mean: -14.977768
Q std: 21.007261
Actor loss: 14.981739
Action reg: 0.003971
  l1.weight: grad_norm = 0.389125
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.360644
Total gradient norm: 1.184859
=== Actor Training Debug (Iteration 7800) ===
Q mean: -15.192404
Q std: 21.628222
Actor loss: 15.196375
Action reg: 0.003971
  l1.weight: grad_norm = 0.319167
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.214566
Total gradient norm: 0.652863
=== Actor Training Debug (Iteration 7801) ===
Q mean: -12.378458
Q std: 20.299484
Actor loss: 12.382408
Action reg: 0.003950
  l1.weight: grad_norm = 0.292931
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.247842
Total gradient norm: 0.670089
=== Actor Training Debug (Iteration 7802) ===
Q mean: -14.927906
Q std: 22.468950
Actor loss: 14.931856
Action reg: 0.003950
  l1.weight: grad_norm = 0.138505
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.113272
Total gradient norm: 0.295624
=== Actor Training Debug (Iteration 7803) ===
Q mean: -14.675703
Q std: 22.821161
Actor loss: 14.679649
Action reg: 0.003947
  l1.weight: grad_norm = 0.138605
  l1.bias: grad_norm = 0.001704
  l2.weight: grad_norm = 0.106346
Total gradient norm: 0.285557
=== Actor Training Debug (Iteration 7804) ===
Q mean: -15.078354
Q std: 20.119452
Actor loss: 15.082334
Action reg: 0.003980
  l1.weight: grad_norm = 0.210172
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.146440
Total gradient norm: 0.412606
=== Actor Training Debug (Iteration 7805) ===
Q mean: -15.186610
Q std: 21.799803
Actor loss: 15.190543
Action reg: 0.003933
  l1.weight: grad_norm = 0.160940
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.114186
Total gradient norm: 0.298478
=== Actor Training Debug (Iteration 7806) ===
Q mean: -12.652794
Q std: 19.503679
Actor loss: 12.656759
Action reg: 0.003965
  l1.weight: grad_norm = 0.077835
  l1.bias: grad_norm = 0.002363
  l2.weight: grad_norm = 0.063925
Total gradient norm: 0.191252
=== Actor Training Debug (Iteration 7807) ===
Q mean: -13.935472
Q std: 20.721022
Actor loss: 13.939410
Action reg: 0.003938
  l1.weight: grad_norm = 0.341462
  l1.bias: grad_norm = 0.003872
  l2.weight: grad_norm = 0.342611
Total gradient norm: 1.034666
=== Actor Training Debug (Iteration 7808) ===
Q mean: -12.389155
Q std: 18.410267
Actor loss: 12.393104
Action reg: 0.003948
  l1.weight: grad_norm = 0.215722
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.175087
Total gradient norm: 0.480091
=== Actor Training Debug (Iteration 7809) ===
Q mean: -12.859842
Q std: 19.692896
Actor loss: 12.863791
Action reg: 0.003949
  l1.weight: grad_norm = 0.178504
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.139967
Total gradient norm: 0.426375
=== Actor Training Debug (Iteration 7810) ===
Q mean: -14.373804
Q std: 21.087996
Actor loss: 14.377760
Action reg: 0.003956
  l1.weight: grad_norm = 0.297249
  l1.bias: grad_norm = 0.002322
  l2.weight: grad_norm = 0.204472
Total gradient norm: 0.537005
=== Actor Training Debug (Iteration 7811) ===
Q mean: -15.510875
Q std: 20.949900
Actor loss: 15.514841
Action reg: 0.003966
  l1.weight: grad_norm = 0.397281
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.285618
Total gradient norm: 0.783184
=== Actor Training Debug (Iteration 7812) ===
Q mean: -14.349228
Q std: 21.092699
Actor loss: 14.353195
Action reg: 0.003967
  l1.weight: grad_norm = 0.133158
  l1.bias: grad_norm = 0.001947
  l2.weight: grad_norm = 0.098622
Total gradient norm: 0.294942
=== Actor Training Debug (Iteration 7813) ===
Q mean: -15.369281
Q std: 22.578800
Actor loss: 15.373231
Action reg: 0.003950
  l1.weight: grad_norm = 0.180541
  l1.bias: grad_norm = 0.001081
  l2.weight: grad_norm = 0.133082
Total gradient norm: 0.384457
=== Actor Training Debug (Iteration 7814) ===
Q mean: -15.656742
Q std: 21.015347
Actor loss: 15.660709
Action reg: 0.003968
  l1.weight: grad_norm = 0.206260
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.161478
Total gradient norm: 0.447035
=== Actor Training Debug (Iteration 7815) ===
Q mean: -13.227190
Q std: 20.283630
Actor loss: 13.231151
Action reg: 0.003961
  l1.weight: grad_norm = 0.203935
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.149178
Total gradient norm: 0.481053
=== Actor Training Debug (Iteration 7816) ===
Q mean: -12.501381
Q std: 17.743120
Actor loss: 12.505336
Action reg: 0.003955
  l1.weight: grad_norm = 0.153742
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.122358
Total gradient norm: 0.334534
=== Actor Training Debug (Iteration 7817) ===
Q mean: -14.297751
Q std: 20.329287
Actor loss: 14.301735
Action reg: 0.003984
  l1.weight: grad_norm = 0.169185
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.113775
Total gradient norm: 0.316541
=== Actor Training Debug (Iteration 7818) ===
Q mean: -16.076399
Q std: 22.604933
Actor loss: 16.080366
Action reg: 0.003967
  l1.weight: grad_norm = 0.249754
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.185300
Total gradient norm: 0.530146
=== Actor Training Debug (Iteration 7819) ===
Q mean: -13.450412
Q std: 21.281761
Actor loss: 13.454371
Action reg: 0.003959
  l1.weight: grad_norm = 0.333683
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.275711
Total gradient norm: 0.771007
=== Actor Training Debug (Iteration 7820) ===
Q mean: -15.497475
Q std: 20.708323
Actor loss: 15.501431
Action reg: 0.003956
  l1.weight: grad_norm = 0.342437
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.218789
Total gradient norm: 0.578673
=== Actor Training Debug (Iteration 7821) ===
Q mean: -13.397973
Q std: 20.215321
Actor loss: 13.401940
Action reg: 0.003967
  l1.weight: grad_norm = 0.207507
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.161513
Total gradient norm: 0.427592
=== Actor Training Debug (Iteration 7822) ===
Q mean: -13.045625
Q std: 20.056520
Actor loss: 13.049568
Action reg: 0.003944
  l1.weight: grad_norm = 0.212136
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.154933
Total gradient norm: 0.423739
=== Actor Training Debug (Iteration 7823) ===
Q mean: -12.700701
Q std: 18.540764
Actor loss: 12.704651
Action reg: 0.003951
  l1.weight: grad_norm = 0.178887
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.142301
Total gradient norm: 0.409691
=== Actor Training Debug (Iteration 7824) ===
Q mean: -14.868093
Q std: 21.064352
Actor loss: 14.872056
Action reg: 0.003962
  l1.weight: grad_norm = 0.473403
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.311110
Total gradient norm: 0.829297
=== Actor Training Debug (Iteration 7825) ===
Q mean: -13.594788
Q std: 19.941607
Actor loss: 13.598735
Action reg: 0.003947
  l1.weight: grad_norm = 0.171326
  l1.bias: grad_norm = 0.001949
  l2.weight: grad_norm = 0.113719
Total gradient norm: 0.337596
=== Actor Training Debug (Iteration 7826) ===
Q mean: -13.124209
Q std: 19.629210
Actor loss: 13.128179
Action reg: 0.003969
  l1.weight: grad_norm = 0.206538
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.155088
Total gradient norm: 0.498303
=== Actor Training Debug (Iteration 7827) ===
Q mean: -15.628250
Q std: 21.774460
Actor loss: 15.632199
Action reg: 0.003949
  l1.weight: grad_norm = 0.143012
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.108911
Total gradient norm: 0.360142
=== Actor Training Debug (Iteration 7828) ===
Q mean: -14.483246
Q std: 20.845892
Actor loss: 14.487211
Action reg: 0.003965
  l1.weight: grad_norm = 0.304898
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.201030
Total gradient norm: 0.522190
=== Actor Training Debug (Iteration 7829) ===
Q mean: -14.364859
Q std: 20.957970
Actor loss: 14.368803
Action reg: 0.003944
  l1.weight: grad_norm = 0.368712
  l1.bias: grad_norm = 0.000799
  l2.weight: grad_norm = 0.284914
Total gradient norm: 0.792508
=== Actor Training Debug (Iteration 7830) ===
Q mean: -14.755490
Q std: 20.868769
Actor loss: 14.759461
Action reg: 0.003971
  l1.weight: grad_norm = 0.184193
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.145860
Total gradient norm: 0.392021
=== Actor Training Debug (Iteration 7831) ===
Q mean: -14.381317
Q std: 20.812132
Actor loss: 14.385281
Action reg: 0.003963
  l1.weight: grad_norm = 0.303044
  l1.bias: grad_norm = 0.000760
  l2.weight: grad_norm = 0.209672
Total gradient norm: 0.608846
=== Actor Training Debug (Iteration 7832) ===
Q mean: -13.731968
Q std: 20.163771
Actor loss: 13.735924
Action reg: 0.003956
  l1.weight: grad_norm = 0.234824
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.194186
Total gradient norm: 0.551007
=== Actor Training Debug (Iteration 7833) ===
Q mean: -13.539045
Q std: 20.265400
Actor loss: 13.542987
Action reg: 0.003941
  l1.weight: grad_norm = 0.407208
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.280997
Total gradient norm: 0.765527
=== Actor Training Debug (Iteration 7834) ===
Q mean: -14.056236
Q std: 20.968376
Actor loss: 14.060196
Action reg: 0.003960
  l1.weight: grad_norm = 0.194160
  l1.bias: grad_norm = 0.001047
  l2.weight: grad_norm = 0.160764
Total gradient norm: 0.401821
=== Actor Training Debug (Iteration 7835) ===
Q mean: -12.670792
Q std: 19.797670
Actor loss: 12.674765
Action reg: 0.003973
  l1.weight: grad_norm = 0.238232
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.160976
Total gradient norm: 0.477167
=== Actor Training Debug (Iteration 7836) ===
Q mean: -13.112542
Q std: 20.373072
Actor loss: 13.116525
Action reg: 0.003982
  l1.weight: grad_norm = 0.498410
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.301066
Total gradient norm: 0.933700
=== Actor Training Debug (Iteration 7837) ===
Q mean: -13.975912
Q std: 21.815565
Actor loss: 13.979877
Action reg: 0.003964
  l1.weight: grad_norm = 0.221694
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.193339
Total gradient norm: 0.528612
=== Actor Training Debug (Iteration 7838) ===
Q mean: -15.786010
Q std: 20.234890
Actor loss: 15.789979
Action reg: 0.003969
  l1.weight: grad_norm = 0.305261
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.212903
Total gradient norm: 0.584057
=== Actor Training Debug (Iteration 7839) ===
Q mean: -13.570058
Q std: 19.917252
Actor loss: 13.574011
Action reg: 0.003953
  l1.weight: grad_norm = 0.182921
  l1.bias: grad_norm = 0.002397
  l2.weight: grad_norm = 0.142290
Total gradient norm: 0.412071
=== Actor Training Debug (Iteration 7840) ===
Q mean: -14.200953
Q std: 20.314182
Actor loss: 14.204926
Action reg: 0.003973
  l1.weight: grad_norm = 0.209881
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.145710
Total gradient norm: 0.433407
=== Actor Training Debug (Iteration 7841) ===
Q mean: -14.883179
Q std: 20.870380
Actor loss: 14.887129
Action reg: 0.003950
  l1.weight: grad_norm = 0.364913
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.313003
Total gradient norm: 0.887876
=== Actor Training Debug (Iteration 7842) ===
Q mean: -14.419510
Q std: 20.406607
Actor loss: 14.423457
Action reg: 0.003947
  l1.weight: grad_norm = 0.292434
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.195488
Total gradient norm: 0.530177
=== Actor Training Debug (Iteration 7843) ===
Q mean: -14.078534
Q std: 21.329268
Actor loss: 14.082488
Action reg: 0.003954
  l1.weight: grad_norm = 0.203745
  l1.bias: grad_norm = 0.001203
  l2.weight: grad_norm = 0.174676
Total gradient norm: 0.407836
=== Actor Training Debug (Iteration 7844) ===
Q mean: -14.526686
Q std: 22.334881
Actor loss: 14.530633
Action reg: 0.003947
  l1.weight: grad_norm = 0.496639
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.415046
Total gradient norm: 1.239894
=== Actor Training Debug (Iteration 7845) ===
Q mean: -17.042665
Q std: 22.131836
Actor loss: 17.046608
Action reg: 0.003943
  l1.weight: grad_norm = 0.206810
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.156768
Total gradient norm: 0.405672
=== Actor Training Debug (Iteration 7846) ===
Q mean: -14.410285
Q std: 21.108236
Actor loss: 14.414231
Action reg: 0.003947
  l1.weight: grad_norm = 0.189652
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.132161
Total gradient norm: 0.413673
=== Actor Training Debug (Iteration 7847) ===
Q mean: -15.791982
Q std: 20.130507
Actor loss: 15.795958
Action reg: 0.003976
  l1.weight: grad_norm = 0.279574
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.210231
Total gradient norm: 0.554490
=== Actor Training Debug (Iteration 7848) ===
Q mean: -13.814440
Q std: 19.937134
Actor loss: 13.818384
Action reg: 0.003945
  l1.weight: grad_norm = 0.343984
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.249938
Total gradient norm: 0.764197
=== Actor Training Debug (Iteration 7849) ===
Q mean: -14.652534
Q std: 20.211691
Actor loss: 14.656500
Action reg: 0.003966
  l1.weight: grad_norm = 0.106033
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.079517
Total gradient norm: 0.215244
=== Actor Training Debug (Iteration 7850) ===
Q mean: -14.138441
Q std: 20.188040
Actor loss: 14.142411
Action reg: 0.003970
  l1.weight: grad_norm = 0.146902
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.113177
Total gradient norm: 0.300122
=== Actor Training Debug (Iteration 7851) ===
Q mean: -15.616037
Q std: 22.567390
Actor loss: 15.620000
Action reg: 0.003962
  l1.weight: grad_norm = 0.222578
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.175572
Total gradient norm: 0.450940
=== Actor Training Debug (Iteration 7852) ===
Q mean: -16.323799
Q std: 23.414190
Actor loss: 16.327772
Action reg: 0.003973
  l1.weight: grad_norm = 0.156996
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.121371
Total gradient norm: 0.328349
=== Actor Training Debug (Iteration 7853) ===
Q mean: -13.964057
Q std: 20.755909
Actor loss: 13.968029
Action reg: 0.003972
  l1.weight: grad_norm = 0.802618
  l1.bias: grad_norm = 0.000595
  l2.weight: grad_norm = 0.738455
Total gradient norm: 2.283316
=== Actor Training Debug (Iteration 7854) ===
Q mean: -15.545454
Q std: 21.772146
Actor loss: 15.549413
Action reg: 0.003959
  l1.weight: grad_norm = 0.231985
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.198525
Total gradient norm: 0.555182
=== Actor Training Debug (Iteration 7855) ===
Q mean: -11.486828
Q std: 18.492874
Actor loss: 11.490799
Action reg: 0.003971
  l1.weight: grad_norm = 0.151290
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.116391
Total gradient norm: 0.315960
=== Actor Training Debug (Iteration 7856) ===
Q mean: -13.354469
Q std: 20.270557
Actor loss: 13.358435
Action reg: 0.003966
  l1.weight: grad_norm = 0.206218
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.142436
Total gradient norm: 0.419945
=== Actor Training Debug (Iteration 7857) ===
Q mean: -15.717397
Q std: 22.474979
Actor loss: 15.721356
Action reg: 0.003959
  l1.weight: grad_norm = 0.270170
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.246710
Total gradient norm: 0.796314
=== Actor Training Debug (Iteration 7858) ===
Q mean: -14.409946
Q std: 21.553747
Actor loss: 14.413897
Action reg: 0.003950
  l1.weight: grad_norm = 0.220619
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.181769
Total gradient norm: 0.531098
=== Actor Training Debug (Iteration 7859) ===
Q mean: -14.928650
Q std: 21.201653
Actor loss: 14.932600
Action reg: 0.003950
  l1.weight: grad_norm = 0.214641
  l1.bias: grad_norm = 0.000931
  l2.weight: grad_norm = 0.178859
Total gradient norm: 0.582418
=== Actor Training Debug (Iteration 7860) ===
Q mean: -15.341124
Q std: 21.501295
Actor loss: 15.345081
Action reg: 0.003958
  l1.weight: grad_norm = 0.181148
  l1.bias: grad_norm = 0.001654
  l2.weight: grad_norm = 0.144418
Total gradient norm: 0.397597
=== Actor Training Debug (Iteration 7861) ===
Q mean: -11.963219
Q std: 17.813219
Actor loss: 11.967167
Action reg: 0.003948
  l1.weight: grad_norm = 0.279822
  l1.bias: grad_norm = 0.000476
  l2.weight: grad_norm = 0.208632
Total gradient norm: 0.529954
=== Actor Training Debug (Iteration 7862) ===
Q mean: -14.009305
Q std: 19.892941
Actor loss: 14.013260
Action reg: 0.003955
  l1.weight: grad_norm = 0.189105
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.158650
Total gradient norm: 0.430078
=== Actor Training Debug (Iteration 7863) ===
Q mean: -13.847415
Q std: 20.867849
Actor loss: 13.851378
Action reg: 0.003963
  l1.weight: grad_norm = 0.259812
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.196399
Total gradient norm: 0.556967
=== Actor Training Debug (Iteration 7864) ===
Q mean: -14.202923
Q std: 21.269211
Actor loss: 14.206861
Action reg: 0.003939
  l1.weight: grad_norm = 0.194098
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.163962
Total gradient norm: 0.572607
=== Actor Training Debug (Iteration 7865) ===
Q mean: -13.872737
Q std: 20.473061
Actor loss: 13.876704
Action reg: 0.003967
  l1.weight: grad_norm = 0.182046
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.135367
Total gradient norm: 0.371218
=== Actor Training Debug (Iteration 7866) ===
Q mean: -13.364422
Q std: 20.023678
Actor loss: 13.368392
Action reg: 0.003970
  l1.weight: grad_norm = 0.254631
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.206832
Total gradient norm: 0.522244
=== Actor Training Debug (Iteration 7867) ===
Q mean: -14.143665
Q std: 20.504396
Actor loss: 14.147615
Action reg: 0.003950
  l1.weight: grad_norm = 0.061618
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.049557
Total gradient norm: 0.129981
=== Actor Training Debug (Iteration 7868) ===
Q mean: -15.042188
Q std: 21.424955
Actor loss: 15.046137
Action reg: 0.003950
  l1.weight: grad_norm = 0.197358
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.139223
Total gradient norm: 0.384808
=== Actor Training Debug (Iteration 7869) ===
Q mean: -15.075176
Q std: 22.114378
Actor loss: 15.079133
Action reg: 0.003957
  l1.weight: grad_norm = 0.158580
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.134971
Total gradient norm: 0.377744
=== Actor Training Debug (Iteration 7870) ===
Q mean: -14.170435
Q std: 20.365669
Actor loss: 14.174414
Action reg: 0.003978
  l1.weight: grad_norm = 0.260592
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.232147
Total gradient norm: 0.609264
=== Actor Training Debug (Iteration 7871) ===
Q mean: -13.310903
Q std: 21.008072
Actor loss: 13.314837
Action reg: 0.003935
  l1.weight: grad_norm = 0.206387
  l1.bias: grad_norm = 0.001042
  l2.weight: grad_norm = 0.163408
Total gradient norm: 0.502527
=== Actor Training Debug (Iteration 7872) ===
Q mean: -14.476022
Q std: 22.434313
Actor loss: 14.479955
Action reg: 0.003933
  l1.weight: grad_norm = 0.262665
  l1.bias: grad_norm = 0.000840
  l2.weight: grad_norm = 0.181409
Total gradient norm: 0.507802
=== Actor Training Debug (Iteration 7873) ===
Q mean: -13.200553
Q std: 20.041117
Actor loss: 13.204527
Action reg: 0.003974
  l1.weight: grad_norm = 0.271981
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.204388
Total gradient norm: 0.500797
=== Actor Training Debug (Iteration 7874) ===
Q mean: -13.832834
Q std: 20.671757
Actor loss: 13.836790
Action reg: 0.003956
  l1.weight: grad_norm = 0.262005
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.212357
Total gradient norm: 0.634250
=== Actor Training Debug (Iteration 7875) ===
Q mean: -13.646427
Q std: 19.790306
Actor loss: 13.650394
Action reg: 0.003967
  l1.weight: grad_norm = 0.107629
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.089504
Total gradient norm: 0.245478
=== Actor Training Debug (Iteration 7876) ===
Q mean: -13.994230
Q std: 21.273726
Actor loss: 13.998207
Action reg: 0.003977
  l1.weight: grad_norm = 0.150971
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.107252
Total gradient norm: 0.287389
=== Actor Training Debug (Iteration 7877) ===
Q mean: -13.623758
Q std: 21.303785
Actor loss: 13.627711
Action reg: 0.003953
  l1.weight: grad_norm = 0.174274
  l1.bias: grad_norm = 0.001655
  l2.weight: grad_norm = 0.138233
Total gradient norm: 0.428470
=== Actor Training Debug (Iteration 7878) ===
Q mean: -13.481895
Q std: 19.832125
Actor loss: 13.485854
Action reg: 0.003958
  l1.weight: grad_norm = 0.251613
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.166267
Total gradient norm: 0.520614
=== Actor Training Debug (Iteration 7879) ===
Q mean: -13.519353
Q std: 19.789026
Actor loss: 13.523321
Action reg: 0.003969
  l1.weight: grad_norm = 0.191555
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.141498
Total gradient norm: 0.389540
=== Actor Training Debug (Iteration 7880) ===
Q mean: -12.710207
Q std: 19.858498
Actor loss: 12.714167
Action reg: 0.003960
  l1.weight: grad_norm = 0.245908
  l1.bias: grad_norm = 0.000945
  l2.weight: grad_norm = 0.206736
Total gradient norm: 0.612629
=== Actor Training Debug (Iteration 7881) ===
Q mean: -13.110798
Q std: 21.401461
Actor loss: 13.114778
Action reg: 0.003980
  l1.weight: grad_norm = 0.163925
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.126546
Total gradient norm: 0.348782
=== Actor Training Debug (Iteration 7882) ===
Q mean: -13.004688
Q std: 18.873550
Actor loss: 13.008649
Action reg: 0.003961
  l1.weight: grad_norm = 0.163770
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.117384
Total gradient norm: 0.305616
=== Actor Training Debug (Iteration 7883) ===
Q mean: -13.793406
Q std: 19.567410
Actor loss: 13.797370
Action reg: 0.003964
  l1.weight: grad_norm = 0.409526
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.236328
Total gradient norm: 0.654718
=== Actor Training Debug (Iteration 7884) ===
Q mean: -15.776970
Q std: 22.030701
Actor loss: 15.780946
Action reg: 0.003976
  l1.weight: grad_norm = 0.326141
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.262604
Total gradient norm: 0.770616
=== Actor Training Debug (Iteration 7885) ===
Q mean: -13.994806
Q std: 21.672277
Actor loss: 13.998779
Action reg: 0.003973
  l1.weight: grad_norm = 0.068986
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.058171
Total gradient norm: 0.152458
=== Actor Training Debug (Iteration 7886) ===
Q mean: -13.026800
Q std: 20.211115
Actor loss: 13.030748
Action reg: 0.003948
  l1.weight: grad_norm = 0.164064
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.125234
Total gradient norm: 0.296455
=== Actor Training Debug (Iteration 7887) ===
Q mean: -14.470010
Q std: 20.663933
Actor loss: 14.473943
Action reg: 0.003933
  l1.weight: grad_norm = 0.188677
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.156435
Total gradient norm: 0.428149
=== Actor Training Debug (Iteration 7888) ===
Q mean: -14.669840
Q std: 21.258984
Actor loss: 14.673793
Action reg: 0.003953
  l1.weight: grad_norm = 0.282103
  l1.bias: grad_norm = 0.000594
  l2.weight: grad_norm = 0.257160
Total gradient norm: 0.874351
=== Actor Training Debug (Iteration 7889) ===
Q mean: -16.610876
Q std: 22.960634
Actor loss: 16.614847
Action reg: 0.003970
  l1.weight: grad_norm = 0.182977
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.135928
Total gradient norm: 0.432270
=== Actor Training Debug (Iteration 7890) ===
Q mean: -14.538929
Q std: 20.739685
Actor loss: 14.542871
Action reg: 0.003943
  l1.weight: grad_norm = 0.089182
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.071181
Total gradient norm: 0.202015
=== Actor Training Debug (Iteration 7891) ===
Q mean: -14.589742
Q std: 20.705969
Actor loss: 14.593694
Action reg: 0.003952
  l1.weight: grad_norm = 0.241159
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.194486
Total gradient norm: 0.578145
=== Actor Training Debug (Iteration 7892) ===
Q mean: -12.317495
Q std: 19.095722
Actor loss: 12.321456
Action reg: 0.003961
  l1.weight: grad_norm = 0.164705
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.121959
Total gradient norm: 0.353138
=== Actor Training Debug (Iteration 7893) ===
Q mean: -15.492054
Q std: 21.616955
Actor loss: 15.496005
Action reg: 0.003951
  l1.weight: grad_norm = 0.208239
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.155528
Total gradient norm: 0.384977
=== Actor Training Debug (Iteration 7894) ===
Q mean: -16.007631
Q std: 22.559031
Actor loss: 16.011597
Action reg: 0.003965
  l1.weight: grad_norm = 0.112428
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.068588
Total gradient norm: 0.193075
=== Actor Training Debug (Iteration 7895) ===
Q mean: -13.783751
Q std: 20.136721
Actor loss: 13.787720
Action reg: 0.003969
  l1.weight: grad_norm = 0.446055
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.393066
Total gradient norm: 1.528139
=== Actor Training Debug (Iteration 7896) ===
Q mean: -16.116367
Q std: 20.604834
Actor loss: 16.120331
Action reg: 0.003963
  l1.weight: grad_norm = 0.372548
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.275116
Total gradient norm: 0.774649
=== Actor Training Debug (Iteration 7897) ===
Q mean: -12.640555
Q std: 19.261724
Actor loss: 12.644500
Action reg: 0.003944
  l1.weight: grad_norm = 0.230272
  l1.bias: grad_norm = 0.000505
  l2.weight: grad_norm = 0.166531
Total gradient norm: 0.470654
=== Actor Training Debug (Iteration 7898) ===
Q mean: -15.865626
Q std: 20.940411
Actor loss: 15.869584
Action reg: 0.003958
  l1.weight: grad_norm = 0.377892
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.306683
Total gradient norm: 0.910381
=== Actor Training Debug (Iteration 7899) ===
Q mean: -15.310688
Q std: 21.347240
Actor loss: 15.314651
Action reg: 0.003963
  l1.weight: grad_norm = 0.440241
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.310646
Total gradient norm: 0.924066
=== Actor Training Debug (Iteration 7900) ===
Q mean: -14.727459
Q std: 21.905638
Actor loss: 14.731427
Action reg: 0.003969
  l1.weight: grad_norm = 0.438971
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.266102
Total gradient norm: 0.658882
=== Actor Training Debug (Iteration 7901) ===
Q mean: -14.532452
Q std: 21.351524
Actor loss: 14.536405
Action reg: 0.003953
  l1.weight: grad_norm = 0.193030
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.153592
Total gradient norm: 0.417979
=== Actor Training Debug (Iteration 7902) ===
Q mean: -15.425985
Q std: 20.996531
Actor loss: 15.429962
Action reg: 0.003977
  l1.weight: grad_norm = 0.164614
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.147694
Total gradient norm: 0.423839
=== Actor Training Debug (Iteration 7903) ===
Q mean: -14.179618
Q std: 20.614286
Actor loss: 14.183572
Action reg: 0.003954
  l1.weight: grad_norm = 0.323398
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.233234
Total gradient norm: 0.645967
=== Actor Training Debug (Iteration 7904) ===
Q mean: -13.593748
Q std: 20.713671
Actor loss: 13.597706
Action reg: 0.003958
  l1.weight: grad_norm = 0.415406
  l1.bias: grad_norm = 0.002128
  l2.weight: grad_norm = 0.282514
Total gradient norm: 0.796124
=== Actor Training Debug (Iteration 7905) ===
Q mean: -13.768646
Q std: 19.997612
Actor loss: 13.772598
Action reg: 0.003952
  l1.weight: grad_norm = 0.295428
  l1.bias: grad_norm = 0.000617
  l2.weight: grad_norm = 0.196787
Total gradient norm: 0.556943
=== Actor Training Debug (Iteration 7906) ===
Q mean: -14.234304
Q std: 22.400711
Actor loss: 14.238256
Action reg: 0.003951
  l1.weight: grad_norm = 0.167199
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.130878
Total gradient norm: 0.441047
=== Actor Training Debug (Iteration 7907) ===
Q mean: -17.077972
Q std: 23.906507
Actor loss: 17.081940
Action reg: 0.003967
  l1.weight: grad_norm = 0.660615
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.431441
Total gradient norm: 1.145624
=== Actor Training Debug (Iteration 7908) ===
Q mean: -17.467276
Q std: 23.282034
Actor loss: 17.471241
Action reg: 0.003965
  l1.weight: grad_norm = 0.176459
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.152435
Total gradient norm: 0.415938
=== Actor Training Debug (Iteration 7909) ===
Q mean: -15.229408
Q std: 21.891327
Actor loss: 15.233354
Action reg: 0.003946
  l1.weight: grad_norm = 0.156383
  l1.bias: grad_norm = 0.000633
  l2.weight: grad_norm = 0.122962
Total gradient norm: 0.315811
=== Actor Training Debug (Iteration 7910) ===
Q mean: -15.687369
Q std: 21.014866
Actor loss: 15.691326
Action reg: 0.003957
  l1.weight: grad_norm = 0.178103
  l1.bias: grad_norm = 0.001955
  l2.weight: grad_norm = 0.132220
Total gradient norm: 0.381948
=== Actor Training Debug (Iteration 7911) ===
Q mean: -14.944134
Q std: 21.691525
Actor loss: 14.948119
Action reg: 0.003985
  l1.weight: grad_norm = 0.230071
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.166876
Total gradient norm: 0.585027
=== Actor Training Debug (Iteration 7912) ===
Q mean: -14.418468
Q std: 20.786295
Actor loss: 14.422439
Action reg: 0.003970
  l1.weight: grad_norm = 0.206257
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.153178
Total gradient norm: 0.401189
=== Actor Training Debug (Iteration 7913) ===
Q mean: -14.049500
Q std: 20.082850
Actor loss: 14.053469
Action reg: 0.003969
  l1.weight: grad_norm = 0.124815
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.103483
Total gradient norm: 0.286407
=== Actor Training Debug (Iteration 7914) ===
Q mean: -12.962383
Q std: 20.271425
Actor loss: 12.966353
Action reg: 0.003970
  l1.weight: grad_norm = 0.118903
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.085394
Total gradient norm: 0.246114
=== Actor Training Debug (Iteration 7915) ===
Q mean: -13.919775
Q std: 20.896639
Actor loss: 13.923749
Action reg: 0.003974
  l1.weight: grad_norm = 0.221456
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.172524
Total gradient norm: 0.478215
=== Actor Training Debug (Iteration 7916) ===
Q mean: -14.056309
Q std: 20.947363
Actor loss: 14.060260
Action reg: 0.003951
  l1.weight: grad_norm = 0.372290
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.310400
Total gradient norm: 0.978055
=== Actor Training Debug (Iteration 7917) ===
Q mean: -14.688840
Q std: 20.681471
Actor loss: 14.692771
Action reg: 0.003931
  l1.weight: grad_norm = 0.391362
  l1.bias: grad_norm = 0.000635
  l2.weight: grad_norm = 0.268737
Total gradient norm: 0.760830
=== Actor Training Debug (Iteration 7918) ===
Q mean: -12.892134
Q std: 21.083139
Actor loss: 12.896083
Action reg: 0.003950
  l1.weight: grad_norm = 0.125032
  l1.bias: grad_norm = 0.001872
  l2.weight: grad_norm = 0.107444
Total gradient norm: 0.340020
=== Actor Training Debug (Iteration 7919) ===
Q mean: -14.241825
Q std: 20.456730
Actor loss: 14.245783
Action reg: 0.003958
  l1.weight: grad_norm = 1.004288
  l1.bias: grad_norm = 0.000811
  l2.weight: grad_norm = 0.782336
Total gradient norm: 2.341073
=== Actor Training Debug (Iteration 7920) ===
Q mean: -16.922480
Q std: 22.771267
Actor loss: 16.926449
Action reg: 0.003968
  l1.weight: grad_norm = 0.267642
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.181340
Total gradient norm: 0.555118
=== Actor Training Debug (Iteration 7921) ===
Q mean: -14.086721
Q std: 20.592154
Actor loss: 14.090690
Action reg: 0.003968
  l1.weight: grad_norm = 0.174469
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.155166
Total gradient norm: 0.411622
=== Actor Training Debug (Iteration 7922) ===
Q mean: -13.156190
Q std: 20.585896
Actor loss: 13.160166
Action reg: 0.003976
  l1.weight: grad_norm = 0.077194
  l1.bias: grad_norm = 0.001519
  l2.weight: grad_norm = 0.062091
Total gradient norm: 0.188348
=== Actor Training Debug (Iteration 7923) ===
Q mean: -14.992493
Q std: 21.897165
Actor loss: 14.996439
Action reg: 0.003946
  l1.weight: grad_norm = 0.340341
  l1.bias: grad_norm = 0.000494
  l2.weight: grad_norm = 0.230084
Total gradient norm: 0.625881
=== Actor Training Debug (Iteration 7924) ===
Q mean: -15.608578
Q std: 21.771441
Actor loss: 15.612550
Action reg: 0.003972
  l1.weight: grad_norm = 0.149821
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.116605
Total gradient norm: 0.317195
=== Actor Training Debug (Iteration 7925) ===
Q mean: -14.072763
Q std: 20.224411
Actor loss: 14.076717
Action reg: 0.003954
  l1.weight: grad_norm = 0.262323
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.181022
Total gradient norm: 0.456023
=== Actor Training Debug (Iteration 7926) ===
Q mean: -13.978136
Q std: 20.063900
Actor loss: 13.982121
Action reg: 0.003984
  l1.weight: grad_norm = 0.118746
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.090716
Total gradient norm: 0.235063
=== Actor Training Debug (Iteration 7927) ===
Q mean: -17.516197
Q std: 22.339800
Actor loss: 17.520170
Action reg: 0.003974
  l1.weight: grad_norm = 0.111733
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.087490
Total gradient norm: 0.250475
=== Actor Training Debug (Iteration 7928) ===
Q mean: -14.083006
Q std: 20.442724
Actor loss: 14.086951
Action reg: 0.003945
  l1.weight: grad_norm = 0.336926
  l1.bias: grad_norm = 0.000644
  l2.weight: grad_norm = 0.210774
Total gradient norm: 0.601040
=== Actor Training Debug (Iteration 7929) ===
Q mean: -14.129012
Q std: 20.995829
Actor loss: 14.132969
Action reg: 0.003956
  l1.weight: grad_norm = 0.273922
  l1.bias: grad_norm = 0.001740
  l2.weight: grad_norm = 0.209852
Total gradient norm: 0.594994
=== Actor Training Debug (Iteration 7930) ===
Q mean: -15.925041
Q std: 22.336676
Actor loss: 15.929003
Action reg: 0.003962
  l1.weight: grad_norm = 0.161561
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.126454
Total gradient norm: 0.324986
=== Actor Training Debug (Iteration 7931) ===
Q mean: -15.481548
Q std: 21.227930
Actor loss: 15.485517
Action reg: 0.003968
  l1.weight: grad_norm = 0.140554
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.110715
Total gradient norm: 0.277211
=== Actor Training Debug (Iteration 7932) ===
Q mean: -14.727108
Q std: 20.949438
Actor loss: 14.731076
Action reg: 0.003968
  l1.weight: grad_norm = 0.297169
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.234429
Total gradient norm: 0.698270
=== Actor Training Debug (Iteration 7933) ===
Q mean: -13.788792
Q std: 21.008251
Actor loss: 13.792768
Action reg: 0.003977
  l1.weight: grad_norm = 0.218478
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.148145
Total gradient norm: 0.365325
=== Actor Training Debug (Iteration 7934) ===
Q mean: -13.130885
Q std: 18.775610
Actor loss: 13.134857
Action reg: 0.003972
  l1.weight: grad_norm = 0.149726
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.121710
Total gradient norm: 0.361922
=== Actor Training Debug (Iteration 7935) ===
Q mean: -14.702785
Q std: 21.998053
Actor loss: 14.706714
Action reg: 0.003928
  l1.weight: grad_norm = 0.219127
  l1.bias: grad_norm = 0.000825
  l2.weight: grad_norm = 0.164030
Total gradient norm: 0.452290
=== Actor Training Debug (Iteration 7936) ===
Q mean: -13.567419
Q std: 21.840248
Actor loss: 13.571378
Action reg: 0.003959
  l1.weight: grad_norm = 0.239624
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.195600
Total gradient norm: 0.488658
=== Actor Training Debug (Iteration 7937) ===
Q mean: -16.532618
Q std: 22.929012
Actor loss: 16.536587
Action reg: 0.003968
  l1.weight: grad_norm = 0.124912
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.092479
Total gradient norm: 0.254907
=== Actor Training Debug (Iteration 7938) ===
Q mean: -16.960728
Q std: 22.787611
Actor loss: 16.964699
Action reg: 0.003972
  l1.weight: grad_norm = 0.127118
  l1.bias: grad_norm = 0.001473
  l2.weight: grad_norm = 0.096310
Total gradient norm: 0.285326
=== Actor Training Debug (Iteration 7939) ===
Q mean: -16.690525
Q std: 21.729647
Actor loss: 16.694477
Action reg: 0.003952
  l1.weight: grad_norm = 0.380389
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.280460
Total gradient norm: 0.809072
=== Actor Training Debug (Iteration 7940) ===
Q mean: -12.611603
Q std: 19.074301
Actor loss: 12.615566
Action reg: 0.003964
  l1.weight: grad_norm = 0.383857
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.245129
Total gradient norm: 0.688415
=== Actor Training Debug (Iteration 7941) ===
Q mean: -15.286766
Q std: 21.832291
Actor loss: 15.290696
Action reg: 0.003930
  l1.weight: grad_norm = 0.412267
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.279978
Total gradient norm: 0.828996
=== Actor Training Debug (Iteration 7942) ===
Q mean: -12.415278
Q std: 18.643044
Actor loss: 12.419249
Action reg: 0.003970
  l1.weight: grad_norm = 0.233489
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.156996
Total gradient norm: 0.452659
=== Actor Training Debug (Iteration 7943) ===
Q mean: -14.985561
Q std: 21.268803
Actor loss: 14.989506
Action reg: 0.003945
  l1.weight: grad_norm = 0.157770
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.100647
Total gradient norm: 0.263537
=== Actor Training Debug (Iteration 7944) ===
Q mean: -14.559577
Q std: 21.407433
Actor loss: 14.563516
Action reg: 0.003939
  l1.weight: grad_norm = 0.203663
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.143856
Total gradient norm: 0.389374
=== Actor Training Debug (Iteration 7945) ===
Q mean: -14.593889
Q std: 21.631464
Actor loss: 14.597817
Action reg: 0.003928
  l1.weight: grad_norm = 0.283146
  l1.bias: grad_norm = 0.000724
  l2.weight: grad_norm = 0.245360
Total gradient norm: 0.728097
=== Actor Training Debug (Iteration 7946) ===
Q mean: -14.336916
Q std: 21.518013
Actor loss: 14.340876
Action reg: 0.003960
  l1.weight: grad_norm = 0.266570
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.207140
Total gradient norm: 0.541773
=== Actor Training Debug (Iteration 7947) ===
Q mean: -15.950417
Q std: 23.110041
Actor loss: 15.954376
Action reg: 0.003959
  l1.weight: grad_norm = 0.229008
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.153664
Total gradient norm: 0.412207
=== Actor Training Debug (Iteration 7948) ===
Q mean: -13.424177
Q std: 21.115067
Actor loss: 13.428132
Action reg: 0.003955
  l1.weight: grad_norm = 0.121793
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.098082
Total gradient norm: 0.292235
=== Actor Training Debug (Iteration 7949) ===
Q mean: -14.220612
Q std: 22.117306
Actor loss: 14.224568
Action reg: 0.003957
  l1.weight: grad_norm = 0.196501
  l1.bias: grad_norm = 0.001278
  l2.weight: grad_norm = 0.132472
Total gradient norm: 0.344985
=== Actor Training Debug (Iteration 7950) ===
Q mean: -14.927007
Q std: 21.814438
Actor loss: 14.930963
Action reg: 0.003956
  l1.weight: grad_norm = 0.172118
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.132556
Total gradient norm: 0.361154
=== Actor Training Debug (Iteration 7951) ===
Q mean: -15.265769
Q std: 22.388163
Actor loss: 15.269711
Action reg: 0.003942
  l1.weight: grad_norm = 0.335062
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.211720
Total gradient norm: 0.616739
=== Actor Training Debug (Iteration 7952) ===
Q mean: -14.745399
Q std: 19.834768
Actor loss: 14.749335
Action reg: 0.003936
  l1.weight: grad_norm = 0.205356
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.145718
Total gradient norm: 0.419232
=== Actor Training Debug (Iteration 7953) ===
Q mean: -16.523102
Q std: 22.207855
Actor loss: 16.527075
Action reg: 0.003973
  l1.weight: grad_norm = 0.175332
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.131652
Total gradient norm: 0.349136
=== Actor Training Debug (Iteration 7954) ===
Q mean: -16.761992
Q std: 22.478971
Actor loss: 16.765915
Action reg: 0.003923
  l1.weight: grad_norm = 0.183552
  l1.bias: grad_norm = 0.003411
  l2.weight: grad_norm = 0.138270
Total gradient norm: 0.405176
=== Actor Training Debug (Iteration 7955) ===
Q mean: -13.349426
Q std: 19.759060
Actor loss: 13.353372
Action reg: 0.003946
  l1.weight: grad_norm = 0.270094
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.174877
Total gradient norm: 0.503748
=== Actor Training Debug (Iteration 7956) ===
Q mean: -14.262026
Q std: 21.366697
Actor loss: 14.265974
Action reg: 0.003948
  l1.weight: grad_norm = 0.151696
  l1.bias: grad_norm = 0.001839
  l2.weight: grad_norm = 0.113803
Total gradient norm: 0.357910
=== Actor Training Debug (Iteration 7957) ===
Q mean: -14.217970
Q std: 21.180843
Actor loss: 14.221886
Action reg: 0.003915
  l1.weight: grad_norm = 0.269530
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.185955
Total gradient norm: 0.494883
=== Actor Training Debug (Iteration 7958) ===
Q mean: -13.687721
Q std: 19.162498
Actor loss: 13.691698
Action reg: 0.003977
  l1.weight: grad_norm = 0.232949
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.160900
Total gradient norm: 0.414876
=== Actor Training Debug (Iteration 7959) ===
Q mean: -14.018397
Q std: 20.982048
Actor loss: 14.022373
Action reg: 0.003976
  l1.weight: grad_norm = 0.156077
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.127979
Total gradient norm: 0.350843
=== Actor Training Debug (Iteration 7960) ===
Q mean: -13.860395
Q std: 19.336071
Actor loss: 13.864358
Action reg: 0.003963
  l1.weight: grad_norm = 0.213672
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.174054
Total gradient norm: 0.456002
=== Actor Training Debug (Iteration 7961) ===
Q mean: -14.256229
Q std: 20.770458
Actor loss: 14.260160
Action reg: 0.003931
  l1.weight: grad_norm = 0.128459
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.091564
Total gradient norm: 0.237789
=== Actor Training Debug (Iteration 7962) ===
Q mean: -12.771946
Q std: 19.501078
Actor loss: 12.775899
Action reg: 0.003953
  l1.weight: grad_norm = 0.165608
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.112849
Total gradient norm: 0.332261
=== Actor Training Debug (Iteration 7963) ===
Q mean: -16.240913
Q std: 22.803101
Actor loss: 16.244864
Action reg: 0.003951
  l1.weight: grad_norm = 0.142111
  l1.bias: grad_norm = 0.000677
  l2.weight: grad_norm = 0.118961
Total gradient norm: 0.310999
=== Actor Training Debug (Iteration 7964) ===
Q mean: -14.573818
Q std: 21.475014
Actor loss: 14.577775
Action reg: 0.003957
  l1.weight: grad_norm = 0.268828
  l1.bias: grad_norm = 0.001771
  l2.weight: grad_norm = 0.185970
Total gradient norm: 0.529151
=== Actor Training Debug (Iteration 7965) ===
Q mean: -15.986740
Q std: 22.577461
Actor loss: 15.990703
Action reg: 0.003963
  l1.weight: grad_norm = 0.173449
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.126596
Total gradient norm: 0.329876
=== Actor Training Debug (Iteration 7966) ===
Q mean: -16.211151
Q std: 22.657495
Actor loss: 16.215126
Action reg: 0.003974
  l1.weight: grad_norm = 0.284510
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.267923
Total gradient norm: 0.878415
=== Actor Training Debug (Iteration 7967) ===
Q mean: -14.383135
Q std: 20.484175
Actor loss: 14.387065
Action reg: 0.003930
  l1.weight: grad_norm = 0.190335
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.145177
Total gradient norm: 0.398134
=== Actor Training Debug (Iteration 7968) ===
Q mean: -13.943611
Q std: 20.511290
Actor loss: 13.947554
Action reg: 0.003942
  l1.weight: grad_norm = 0.192900
  l1.bias: grad_norm = 0.000540
  l2.weight: grad_norm = 0.140500
Total gradient norm: 0.421498
=== Actor Training Debug (Iteration 7969) ===
Q mean: -12.978143
Q std: 20.139463
Actor loss: 12.982107
Action reg: 0.003964
  l1.weight: grad_norm = 0.379771
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.294409
Total gradient norm: 0.925679
=== Actor Training Debug (Iteration 7970) ===
Q mean: -15.324859
Q std: 21.345743
Actor loss: 15.328807
Action reg: 0.003948
  l1.weight: grad_norm = 0.318241
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.218877
Total gradient norm: 0.578183
=== Actor Training Debug (Iteration 7971) ===
Q mean: -15.009655
Q std: 22.338200
Actor loss: 15.013599
Action reg: 0.003945
  l1.weight: grad_norm = 0.249423
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.163978
Total gradient norm: 0.436998
=== Actor Training Debug (Iteration 7972) ===
Q mean: -15.441774
Q std: 22.273209
Actor loss: 15.445734
Action reg: 0.003960
  l1.weight: grad_norm = 0.296768
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.262763
Total gradient norm: 0.737917
=== Actor Training Debug (Iteration 7973) ===
Q mean: -15.872465
Q std: 22.520416
Actor loss: 15.876435
Action reg: 0.003970
  l1.weight: grad_norm = 0.220841
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.171660
Total gradient norm: 0.452128
=== Actor Training Debug (Iteration 7974) ===
Q mean: -13.975734
Q std: 21.532707
Actor loss: 13.979714
Action reg: 0.003981
  l1.weight: grad_norm = 0.220262
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.150301
Total gradient norm: 0.429146
=== Actor Training Debug (Iteration 7975) ===
Q mean: -14.576606
Q std: 21.731256
Actor loss: 14.580571
Action reg: 0.003966
  l1.weight: grad_norm = 0.397240
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.309270
Total gradient norm: 1.149033
=== Actor Training Debug (Iteration 7976) ===
Q mean: -15.765964
Q std: 21.570486
Actor loss: 15.769948
Action reg: 0.003985
  l1.weight: grad_norm = 0.078576
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.051837
Total gradient norm: 0.157044
=== Actor Training Debug (Iteration 7977) ===
Q mean: -15.049404
Q std: 19.629097
Actor loss: 15.053353
Action reg: 0.003949
  l1.weight: grad_norm = 0.198082
  l1.bias: grad_norm = 0.000548
  l2.weight: grad_norm = 0.146321
Total gradient norm: 0.438413
=== Actor Training Debug (Iteration 7978) ===
Q mean: -14.804520
Q std: 20.193115
Actor loss: 14.808473
Action reg: 0.003953
  l1.weight: grad_norm = 0.316414
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.234864
Total gradient norm: 0.666950
=== Actor Training Debug (Iteration 7979) ===
Q mean: -14.625931
Q std: 21.102085
Actor loss: 14.629911
Action reg: 0.003980
  l1.weight: grad_norm = 0.306869
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.220507
Total gradient norm: 0.645682
=== Actor Training Debug (Iteration 7980) ===
Q mean: -13.422731
Q std: 19.583391
Actor loss: 13.426702
Action reg: 0.003971
  l1.weight: grad_norm = 0.239582
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.164103
Total gradient norm: 0.445024
=== Actor Training Debug (Iteration 7981) ===
Q mean: -13.933725
Q std: 20.284218
Actor loss: 13.937678
Action reg: 0.003953
  l1.weight: grad_norm = 0.479574
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.312042
Total gradient norm: 0.906659
=== Actor Training Debug (Iteration 7982) ===
Q mean: -16.745438
Q std: 22.100185
Actor loss: 16.749382
Action reg: 0.003945
  l1.weight: grad_norm = 0.155120
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.127504
Total gradient norm: 0.395085
=== Actor Training Debug (Iteration 7983) ===
Q mean: -15.044445
Q std: 21.391573
Actor loss: 15.048387
Action reg: 0.003942
  l1.weight: grad_norm = 0.283780
  l1.bias: grad_norm = 0.000655
  l2.weight: grad_norm = 0.191307
Total gradient norm: 0.567844
=== Actor Training Debug (Iteration 7984) ===
Q mean: -14.300875
Q std: 22.188349
Actor loss: 14.304817
Action reg: 0.003942
  l1.weight: grad_norm = 0.122641
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.091838
Total gradient norm: 0.274487
=== Actor Training Debug (Iteration 7985) ===
Q mean: -14.814949
Q std: 20.305071
Actor loss: 14.818890
Action reg: 0.003940
  l1.weight: grad_norm = 0.194774
  l1.bias: grad_norm = 0.000994
  l2.weight: grad_norm = 0.190661
Total gradient norm: 0.545514
=== Actor Training Debug (Iteration 7986) ===
Q mean: -15.943388
Q std: 20.906027
Actor loss: 15.947351
Action reg: 0.003963
  l1.weight: grad_norm = 0.273690
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.219825
Total gradient norm: 0.599876
=== Actor Training Debug (Iteration 7987) ===
Q mean: -13.715210
Q std: 20.255835
Actor loss: 13.719155
Action reg: 0.003946
  l1.weight: grad_norm = 0.181633
  l1.bias: grad_norm = 0.000809
  l2.weight: grad_norm = 0.145145
Total gradient norm: 0.416373
=== Actor Training Debug (Iteration 7988) ===
Q mean: -13.072845
Q std: 19.745268
Actor loss: 13.076819
Action reg: 0.003974
  l1.weight: grad_norm = 0.231687
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.202310
Total gradient norm: 0.538889
=== Actor Training Debug (Iteration 7989) ===
Q mean: -13.118755
Q std: 20.561762
Actor loss: 13.122707
Action reg: 0.003952
  l1.weight: grad_norm = 0.150037
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.109674
Total gradient norm: 0.295944
=== Actor Training Debug (Iteration 7990) ===
Q mean: -13.607477
Q std: 19.940845
Actor loss: 13.611446
Action reg: 0.003970
  l1.weight: grad_norm = 0.168127
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.120449
Total gradient norm: 0.360991
=== Actor Training Debug (Iteration 7991) ===
Q mean: -12.797070
Q std: 20.958242
Actor loss: 12.800995
Action reg: 0.003925
  l1.weight: grad_norm = 0.500714
  l1.bias: grad_norm = 0.002248
  l2.weight: grad_norm = 0.356802
Total gradient norm: 1.126644
=== Actor Training Debug (Iteration 7992) ===
Q mean: -13.818231
Q std: 20.471605
Actor loss: 13.822179
Action reg: 0.003949
  l1.weight: grad_norm = 0.346656
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.245771
Total gradient norm: 0.645720
=== Actor Training Debug (Iteration 7993) ===
Q mean: -10.835499
Q std: 17.513947
Actor loss: 10.839469
Action reg: 0.003970
  l1.weight: grad_norm = 0.155901
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.140317
Total gradient norm: 0.470271
=== Actor Training Debug (Iteration 7994) ===
Q mean: -14.468004
Q std: 21.051346
Actor loss: 14.471971
Action reg: 0.003966
  l1.weight: grad_norm = 0.112140
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.077271
Total gradient norm: 0.249533
=== Actor Training Debug (Iteration 7995) ===
Q mean: -12.956090
Q std: 20.604900
Actor loss: 12.960056
Action reg: 0.003966
  l1.weight: grad_norm = 0.265735
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.178064
Total gradient norm: 0.440630
=== Actor Training Debug (Iteration 7996) ===
Q mean: -14.029482
Q std: 21.042023
Actor loss: 14.033430
Action reg: 0.003948
  l1.weight: grad_norm = 0.257353
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.173638
Total gradient norm: 0.483268
=== Actor Training Debug (Iteration 7997) ===
Q mean: -15.671108
Q std: 21.417278
Actor loss: 15.675085
Action reg: 0.003977
  l1.weight: grad_norm = 0.109249
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.071236
Total gradient norm: 0.200029
=== Actor Training Debug (Iteration 7998) ===
Q mean: -15.600003
Q std: 21.799984
Actor loss: 15.603971
Action reg: 0.003967
  l1.weight: grad_norm = 0.371794
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.245583
Total gradient norm: 0.606462
=== Actor Training Debug (Iteration 7999) ===
Q mean: -16.037514
Q std: 22.278341
Actor loss: 16.041468
Action reg: 0.003954
  l1.weight: grad_norm = 0.273041
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.208078
Total gradient norm: 0.599141
=== Actor Training Debug (Iteration 8000) ===
Q mean: -13.342052
Q std: 20.173059
Actor loss: 13.346009
Action reg: 0.003957
  l1.weight: grad_norm = 0.306517
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.248914
Total gradient norm: 0.710715
Step 13000: Critic Loss: 1.2497, Actor Loss: 13.3460, Q Value: -13.3421
  Average reward: -318.632 | Average length: 100.0
Evaluation at episode 130: -318.632
=== Actor Training Debug (Iteration 8001) ===
Q mean: -14.204551
Q std: 19.513056
Actor loss: 14.208519
Action reg: 0.003968
  l1.weight: grad_norm = 0.201448
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.137487
Total gradient norm: 0.361552
=== Actor Training Debug (Iteration 8002) ===
Q mean: -13.329044
Q std: 20.055534
Actor loss: 13.332994
Action reg: 0.003950
  l1.weight: grad_norm = 0.301291
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.233274
Total gradient norm: 0.610546
=== Actor Training Debug (Iteration 8003) ===
Q mean: -13.313423
Q std: 19.295706
Actor loss: 13.317392
Action reg: 0.003970
  l1.weight: grad_norm = 0.439375
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.315094
Total gradient norm: 0.956413
=== Actor Training Debug (Iteration 8004) ===
Q mean: -12.255576
Q std: 18.053017
Actor loss: 12.259515
Action reg: 0.003939
  l1.weight: grad_norm = 0.204201
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.138190
Total gradient norm: 0.409358
=== Actor Training Debug (Iteration 8005) ===
Q mean: -14.197438
Q std: 21.272957
Actor loss: 14.201378
Action reg: 0.003939
  l1.weight: grad_norm = 0.284370
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.233704
Total gradient norm: 0.675734
=== Actor Training Debug (Iteration 8006) ===
Q mean: -15.052988
Q std: 21.695921
Actor loss: 15.056965
Action reg: 0.003977
  l1.weight: grad_norm = 0.133410
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.103169
Total gradient norm: 0.289494
=== Actor Training Debug (Iteration 8007) ===
Q mean: -14.843802
Q std: 21.061684
Actor loss: 14.847763
Action reg: 0.003961
  l1.weight: grad_norm = 0.440482
  l1.bias: grad_norm = 0.000609
  l2.weight: grad_norm = 0.242718
Total gradient norm: 0.670927
=== Actor Training Debug (Iteration 8008) ===
Q mean: -15.346820
Q std: 21.481071
Actor loss: 15.350801
Action reg: 0.003981
  l1.weight: grad_norm = 0.181570
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.155313
Total gradient norm: 0.441917
=== Actor Training Debug (Iteration 8009) ===
Q mean: -14.526587
Q std: 18.687916
Actor loss: 14.530554
Action reg: 0.003967
  l1.weight: grad_norm = 0.219123
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.132643
Total gradient norm: 0.342195
=== Actor Training Debug (Iteration 8010) ===
Q mean: -14.957430
Q std: 22.516338
Actor loss: 14.961404
Action reg: 0.003974
  l1.weight: grad_norm = 0.190252
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.132157
Total gradient norm: 0.384941
=== Actor Training Debug (Iteration 8011) ===
Q mean: -13.256140
Q std: 20.617624
Actor loss: 13.260105
Action reg: 0.003966
  l1.weight: grad_norm = 0.182473
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.135833
Total gradient norm: 0.375344
=== Actor Training Debug (Iteration 8012) ===
Q mean: -13.859514
Q std: 21.529339
Actor loss: 13.863487
Action reg: 0.003973
  l1.weight: grad_norm = 0.316474
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.200878
Total gradient norm: 0.558266
=== Actor Training Debug (Iteration 8013) ===
Q mean: -15.266991
Q std: 20.760580
Actor loss: 15.270956
Action reg: 0.003965
  l1.weight: grad_norm = 0.216353
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.155139
Total gradient norm: 0.386707
=== Actor Training Debug (Iteration 8014) ===
Q mean: -15.134405
Q std: 20.253725
Actor loss: 15.138361
Action reg: 0.003956
  l1.weight: grad_norm = 0.192429
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.130630
Total gradient norm: 0.422079
=== Actor Training Debug (Iteration 8015) ===
Q mean: -14.156488
Q std: 20.928236
Actor loss: 14.160438
Action reg: 0.003949
  l1.weight: grad_norm = 0.317443
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.272538
Total gradient norm: 0.746827
=== Actor Training Debug (Iteration 8016) ===
Q mean: -15.588759
Q std: 21.642208
Actor loss: 15.592732
Action reg: 0.003973
  l1.weight: grad_norm = 0.143864
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.102982
Total gradient norm: 0.299080
=== Actor Training Debug (Iteration 8017) ===
Q mean: -13.424197
Q std: 21.060009
Actor loss: 13.428172
Action reg: 0.003975
  l1.weight: grad_norm = 0.314322
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.204617
Total gradient norm: 0.547510
=== Actor Training Debug (Iteration 8018) ===
Q mean: -16.261126
Q std: 22.034218
Actor loss: 16.265087
Action reg: 0.003962
  l1.weight: grad_norm = 0.208045
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.136428
Total gradient norm: 0.357602
=== Actor Training Debug (Iteration 8019) ===
Q mean: -15.328905
Q std: 20.342728
Actor loss: 15.332877
Action reg: 0.003972
  l1.weight: grad_norm = 0.257275
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.178985
Total gradient norm: 0.531041
=== Actor Training Debug (Iteration 8020) ===
Q mean: -14.299377
Q std: 20.203781
Actor loss: 14.303319
Action reg: 0.003941
  l1.weight: grad_norm = 0.192118
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.126231
Total gradient norm: 0.361329
=== Actor Training Debug (Iteration 8021) ===
Q mean: -12.438496
Q std: 18.884964
Actor loss: 12.442455
Action reg: 0.003960
  l1.weight: grad_norm = 0.230334
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.182012
Total gradient norm: 0.615731
=== Actor Training Debug (Iteration 8022) ===
Q mean: -12.889137
Q std: 19.618452
Actor loss: 12.893079
Action reg: 0.003941
  l1.weight: grad_norm = 0.210124
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.167832
Total gradient norm: 0.420445
=== Actor Training Debug (Iteration 8023) ===
Q mean: -13.901155
Q std: 19.226625
Actor loss: 13.905094
Action reg: 0.003939
  l1.weight: grad_norm = 0.117016
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.074980
Total gradient norm: 0.214839
=== Actor Training Debug (Iteration 8024) ===
Q mean: -12.208977
Q std: 18.015463
Actor loss: 12.212924
Action reg: 0.003947
  l1.weight: grad_norm = 0.408316
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.287785
Total gradient norm: 0.745356
=== Actor Training Debug (Iteration 8025) ===
Q mean: -14.094885
Q std: 19.887814
Actor loss: 14.098824
Action reg: 0.003939
  l1.weight: grad_norm = 0.262339
  l1.bias: grad_norm = 0.001836
  l2.weight: grad_norm = 0.196892
Total gradient norm: 0.612569
=== Actor Training Debug (Iteration 8026) ===
Q mean: -14.613268
Q std: 21.334587
Actor loss: 14.617219
Action reg: 0.003951
  l1.weight: grad_norm = 0.162776
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.122210
Total gradient norm: 0.338967
=== Actor Training Debug (Iteration 8027) ===
Q mean: -13.309553
Q std: 19.413244
Actor loss: 13.313506
Action reg: 0.003953
  l1.weight: grad_norm = 0.211161
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.153970
Total gradient norm: 0.489264
=== Actor Training Debug (Iteration 8028) ===
Q mean: -16.427509
Q std: 23.115929
Actor loss: 16.431475
Action reg: 0.003965
  l1.weight: grad_norm = 0.159892
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.105197
Total gradient norm: 0.291261
=== Actor Training Debug (Iteration 8029) ===
Q mean: -12.968752
Q std: 20.690727
Actor loss: 12.972712
Action reg: 0.003959
  l1.weight: grad_norm = 0.188848
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.169003
Total gradient norm: 0.473661
=== Actor Training Debug (Iteration 8030) ===
Q mean: -13.992977
Q std: 19.940243
Actor loss: 13.996952
Action reg: 0.003975
  l1.weight: grad_norm = 0.213066
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.178650
Total gradient norm: 0.526948
=== Actor Training Debug (Iteration 8031) ===
Q mean: -16.050396
Q std: 22.166231
Actor loss: 16.054344
Action reg: 0.003949
  l1.weight: grad_norm = 0.296201
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.230206
Total gradient norm: 0.605888
=== Actor Training Debug (Iteration 8032) ===
Q mean: -12.730318
Q std: 19.979008
Actor loss: 12.734264
Action reg: 0.003946
  l1.weight: grad_norm = 0.261590
  l1.bias: grad_norm = 0.000478
  l2.weight: grad_norm = 0.194252
Total gradient norm: 0.544231
=== Actor Training Debug (Iteration 8033) ===
Q mean: -15.910051
Q std: 22.656517
Actor loss: 15.914014
Action reg: 0.003963
  l1.weight: grad_norm = 0.155537
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.139790
Total gradient norm: 0.350212
=== Actor Training Debug (Iteration 8034) ===
Q mean: -12.514174
Q std: 18.969818
Actor loss: 12.518147
Action reg: 0.003974
  l1.weight: grad_norm = 0.381503
  l1.bias: grad_norm = 0.001759
  l2.weight: grad_norm = 0.283363
Total gradient norm: 0.721463
=== Actor Training Debug (Iteration 8035) ===
Q mean: -15.369998
Q std: 21.414541
Actor loss: 15.373953
Action reg: 0.003955
  l1.weight: grad_norm = 0.353893
  l1.bias: grad_norm = 0.000589
  l2.weight: grad_norm = 0.227010
Total gradient norm: 0.579701
=== Actor Training Debug (Iteration 8036) ===
Q mean: -12.385185
Q std: 18.387186
Actor loss: 12.389154
Action reg: 0.003969
  l1.weight: grad_norm = 0.236157
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.164882
Total gradient norm: 0.473777
=== Actor Training Debug (Iteration 8037) ===
Q mean: -14.289721
Q std: 21.213667
Actor loss: 14.293694
Action reg: 0.003974
  l1.weight: grad_norm = 0.372714
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.296116
Total gradient norm: 0.681062
=== Actor Training Debug (Iteration 8038) ===
Q mean: -15.334860
Q std: 21.347912
Actor loss: 15.338803
Action reg: 0.003943
  l1.weight: grad_norm = 0.392294
  l1.bias: grad_norm = 0.001573
  l2.weight: grad_norm = 0.289985
Total gradient norm: 0.986916
=== Actor Training Debug (Iteration 8039) ===
Q mean: -12.638792
Q std: 18.147383
Actor loss: 12.642731
Action reg: 0.003939
  l1.weight: grad_norm = 0.225697
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.161159
Total gradient norm: 0.456234
=== Actor Training Debug (Iteration 8040) ===
Q mean: -15.916916
Q std: 22.661146
Actor loss: 15.920884
Action reg: 0.003968
  l1.weight: grad_norm = 0.228154
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.155632
Total gradient norm: 0.442769
=== Actor Training Debug (Iteration 8041) ===
Q mean: -14.125646
Q std: 19.892586
Actor loss: 14.129607
Action reg: 0.003961
  l1.weight: grad_norm = 0.236995
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.189343
Total gradient norm: 0.541673
=== Actor Training Debug (Iteration 8042) ===
Q mean: -14.158388
Q std: 19.520739
Actor loss: 14.162333
Action reg: 0.003946
  l1.weight: grad_norm = 0.165680
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.131134
Total gradient norm: 0.339930
=== Actor Training Debug (Iteration 8043) ===
Q mean: -13.037173
Q std: 20.382212
Actor loss: 13.041155
Action reg: 0.003982
  l1.weight: grad_norm = 0.178824
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.132719
Total gradient norm: 0.353059
=== Actor Training Debug (Iteration 8044) ===
Q mean: -14.374424
Q std: 21.081861
Actor loss: 14.378386
Action reg: 0.003962
  l1.weight: grad_norm = 0.240063
  l1.bias: grad_norm = 0.001931
  l2.weight: grad_norm = 0.198343
Total gradient norm: 0.540899
=== Actor Training Debug (Iteration 8045) ===
Q mean: -13.500691
Q std: 19.912779
Actor loss: 13.504646
Action reg: 0.003954
  l1.weight: grad_norm = 0.246897
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.221951
Total gradient norm: 0.622481
=== Actor Training Debug (Iteration 8046) ===
Q mean: -12.258923
Q std: 20.184057
Actor loss: 12.262875
Action reg: 0.003952
  l1.weight: grad_norm = 0.283219
  l1.bias: grad_norm = 0.001489
  l2.weight: grad_norm = 0.211616
Total gradient norm: 0.546829
=== Actor Training Debug (Iteration 8047) ===
Q mean: -13.083239
Q std: 19.757988
Actor loss: 13.087181
Action reg: 0.003943
  l1.weight: grad_norm = 0.340456
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.283689
Total gradient norm: 0.806002
=== Actor Training Debug (Iteration 8048) ===
Q mean: -14.020813
Q std: 20.651764
Actor loss: 14.024776
Action reg: 0.003963
  l1.weight: grad_norm = 0.128546
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.116636
Total gradient norm: 0.279814
=== Actor Training Debug (Iteration 8049) ===
Q mean: -11.134951
Q std: 19.055948
Actor loss: 11.138914
Action reg: 0.003963
  l1.weight: grad_norm = 0.169400
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.180395
Total gradient norm: 0.477186
=== Actor Training Debug (Iteration 8050) ===
Q mean: -15.164527
Q std: 20.695305
Actor loss: 15.168486
Action reg: 0.003959
  l1.weight: grad_norm = 0.273643
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.189743
Total gradient norm: 0.499884
=== Actor Training Debug (Iteration 8051) ===
Q mean: -14.762686
Q std: 20.437044
Actor loss: 14.766632
Action reg: 0.003947
  l1.weight: grad_norm = 0.176946
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.114494
Total gradient norm: 0.324522
=== Actor Training Debug (Iteration 8052) ===
Q mean: -17.364410
Q std: 21.664932
Actor loss: 17.368366
Action reg: 0.003956
  l1.weight: grad_norm = 0.465115
  l1.bias: grad_norm = 0.000643
  l2.weight: grad_norm = 0.329298
Total gradient norm: 0.910607
=== Actor Training Debug (Iteration 8053) ===
Q mean: -16.476074
Q std: 22.874424
Actor loss: 16.480013
Action reg: 0.003938
  l1.weight: grad_norm = 0.189304
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 0.135438
Total gradient norm: 0.407641
=== Actor Training Debug (Iteration 8054) ===
Q mean: -15.360251
Q std: 21.174892
Actor loss: 15.364194
Action reg: 0.003943
  l1.weight: grad_norm = 0.092750
  l1.bias: grad_norm = 0.001474
  l2.weight: grad_norm = 0.080132
Total gradient norm: 0.220809
=== Actor Training Debug (Iteration 8055) ===
Q mean: -13.660761
Q std: 20.298937
Actor loss: 13.664706
Action reg: 0.003946
  l1.weight: grad_norm = 0.332166
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.253929
Total gradient norm: 0.639411
=== Actor Training Debug (Iteration 8056) ===
Q mean: -15.633625
Q std: 20.691099
Actor loss: 15.637605
Action reg: 0.003980
  l1.weight: grad_norm = 0.196966
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.153361
Total gradient norm: 0.460010
=== Actor Training Debug (Iteration 8057) ===
Q mean: -15.513563
Q std: 21.969862
Actor loss: 15.517503
Action reg: 0.003940
  l1.weight: grad_norm = 0.318136
  l1.bias: grad_norm = 0.001862
  l2.weight: grad_norm = 0.241617
Total gradient norm: 0.773528
=== Actor Training Debug (Iteration 8058) ===
Q mean: -14.711147
Q std: 20.721863
Actor loss: 14.715094
Action reg: 0.003946
  l1.weight: grad_norm = 0.424250
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.283470
Total gradient norm: 0.732356
=== Actor Training Debug (Iteration 8059) ===
Q mean: -14.553557
Q std: 21.647310
Actor loss: 14.557528
Action reg: 0.003971
  l1.weight: grad_norm = 0.229009
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.150948
Total gradient norm: 0.412720
=== Actor Training Debug (Iteration 8060) ===
Q mean: -14.728632
Q std: 21.461256
Actor loss: 14.732553
Action reg: 0.003920
  l1.weight: grad_norm = 0.445365
  l1.bias: grad_norm = 0.001405
  l2.weight: grad_norm = 0.355287
Total gradient norm: 0.950737
=== Actor Training Debug (Iteration 8061) ===
Q mean: -13.844631
Q std: 20.258453
Actor loss: 13.848604
Action reg: 0.003973
  l1.weight: grad_norm = 0.123035
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.086303
Total gradient norm: 0.240025
=== Actor Training Debug (Iteration 8062) ===
Q mean: -14.494720
Q std: 21.076944
Actor loss: 14.498675
Action reg: 0.003956
  l1.weight: grad_norm = 0.206693
  l1.bias: grad_norm = 0.001446
  l2.weight: grad_norm = 0.151062
Total gradient norm: 0.442635
=== Actor Training Debug (Iteration 8063) ===
Q mean: -11.649962
Q std: 18.758001
Actor loss: 11.653919
Action reg: 0.003956
  l1.weight: grad_norm = 0.130624
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.107464
Total gradient norm: 0.304307
=== Actor Training Debug (Iteration 8064) ===
Q mean: -15.262098
Q std: 21.701931
Actor loss: 15.266037
Action reg: 0.003938
  l1.weight: grad_norm = 0.249033
  l1.bias: grad_norm = 0.000582
  l2.weight: grad_norm = 0.175631
Total gradient norm: 0.482155
=== Actor Training Debug (Iteration 8065) ===
Q mean: -13.826672
Q std: 20.457869
Actor loss: 13.830613
Action reg: 0.003942
  l1.weight: grad_norm = 0.133160
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.091052
Total gradient norm: 0.234842
=== Actor Training Debug (Iteration 8066) ===
Q mean: -17.649668
Q std: 22.000578
Actor loss: 17.653633
Action reg: 0.003965
  l1.weight: grad_norm = 0.164894
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.106350
Total gradient norm: 0.287950
=== Actor Training Debug (Iteration 8067) ===
Q mean: -12.146749
Q std: 18.521133
Actor loss: 12.150717
Action reg: 0.003967
  l1.weight: grad_norm = 0.112811
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.085497
Total gradient norm: 0.223695
=== Actor Training Debug (Iteration 8068) ===
Q mean: -17.503658
Q std: 22.726219
Actor loss: 17.507624
Action reg: 0.003965
  l1.weight: grad_norm = 0.172800
  l1.bias: grad_norm = 0.001571
  l2.weight: grad_norm = 0.122701
Total gradient norm: 0.318027
=== Actor Training Debug (Iteration 8069) ===
Q mean: -15.787727
Q std: 22.683918
Actor loss: 15.791690
Action reg: 0.003962
  l1.weight: grad_norm = 0.188100
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.128629
Total gradient norm: 0.345465
=== Actor Training Debug (Iteration 8070) ===
Q mean: -14.550695
Q std: 21.773300
Actor loss: 14.554642
Action reg: 0.003946
  l1.weight: grad_norm = 0.218546
  l1.bias: grad_norm = 0.001477
  l2.weight: grad_norm = 0.208434
Total gradient norm: 0.526677
=== Actor Training Debug (Iteration 8071) ===
Q mean: -14.306652
Q std: 20.413692
Actor loss: 14.310563
Action reg: 0.003911
  l1.weight: grad_norm = 0.141625
  l1.bias: grad_norm = 0.001805
  l2.weight: grad_norm = 0.113418
Total gradient norm: 0.325168
=== Actor Training Debug (Iteration 8072) ===
Q mean: -14.040094
Q std: 20.703875
Actor loss: 14.044039
Action reg: 0.003944
  l1.weight: grad_norm = 0.180899
  l1.bias: grad_norm = 0.001253
  l2.weight: grad_norm = 0.120054
Total gradient norm: 0.331577
=== Actor Training Debug (Iteration 8073) ===
Q mean: -13.703953
Q std: 20.090178
Actor loss: 13.707926
Action reg: 0.003973
  l1.weight: grad_norm = 0.195936
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.160718
Total gradient norm: 0.419723
=== Actor Training Debug (Iteration 8074) ===
Q mean: -15.099008
Q std: 20.815485
Actor loss: 15.102969
Action reg: 0.003962
  l1.weight: grad_norm = 0.124470
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.093464
Total gradient norm: 0.266734
=== Actor Training Debug (Iteration 8075) ===
Q mean: -16.051743
Q std: 22.990204
Actor loss: 16.055700
Action reg: 0.003958
  l1.weight: grad_norm = 0.209886
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.173571
Total gradient norm: 0.484759
=== Actor Training Debug (Iteration 8076) ===
Q mean: -14.250961
Q std: 20.770599
Actor loss: 14.254898
Action reg: 0.003936
  l1.weight: grad_norm = 0.246561
  l1.bias: grad_norm = 0.000643
  l2.weight: grad_norm = 0.170126
Total gradient norm: 0.469909
=== Actor Training Debug (Iteration 8077) ===
Q mean: -15.932852
Q std: 21.734722
Actor loss: 15.936796
Action reg: 0.003944
  l1.weight: grad_norm = 0.155057
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.124606
Total gradient norm: 0.338667
=== Actor Training Debug (Iteration 8078) ===
Q mean: -18.236097
Q std: 23.789120
Actor loss: 18.240047
Action reg: 0.003950
  l1.weight: grad_norm = 0.286100
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.181315
Total gradient norm: 0.500578
=== Actor Training Debug (Iteration 8079) ===
Q mean: -13.281393
Q std: 21.831099
Actor loss: 13.285335
Action reg: 0.003942
  l1.weight: grad_norm = 0.143188
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.115298
Total gradient norm: 0.296475
=== Actor Training Debug (Iteration 8080) ===
Q mean: -14.570505
Q std: 20.385153
Actor loss: 14.574480
Action reg: 0.003975
  l1.weight: grad_norm = 0.442485
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.297228
Total gradient norm: 0.895513
=== Actor Training Debug (Iteration 8081) ===
Q mean: -16.539379
Q std: 22.304420
Actor loss: 16.543341
Action reg: 0.003961
  l1.weight: grad_norm = 0.433754
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.297513
Total gradient norm: 0.896126
=== Actor Training Debug (Iteration 8082) ===
Q mean: -14.333793
Q std: 21.926710
Actor loss: 14.337744
Action reg: 0.003951
  l1.weight: grad_norm = 0.248138
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.204552
Total gradient norm: 0.594851
=== Actor Training Debug (Iteration 8083) ===
Q mean: -15.244995
Q std: 20.784443
Actor loss: 15.248961
Action reg: 0.003966
  l1.weight: grad_norm = 0.336289
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.296943
Total gradient norm: 0.826850
=== Actor Training Debug (Iteration 8084) ===
Q mean: -14.954000
Q std: 20.752743
Actor loss: 14.957972
Action reg: 0.003972
  l1.weight: grad_norm = 0.177451
  l1.bias: grad_norm = 0.000476
  l2.weight: grad_norm = 0.146972
Total gradient norm: 0.480478
=== Actor Training Debug (Iteration 8085) ===
Q mean: -16.022316
Q std: 23.006634
Actor loss: 16.026270
Action reg: 0.003954
  l1.weight: grad_norm = 0.164249
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.137186
Total gradient norm: 0.378160
=== Actor Training Debug (Iteration 8086) ===
Q mean: -14.234638
Q std: 20.546392
Actor loss: 14.238585
Action reg: 0.003947
  l1.weight: grad_norm = 0.141872
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.116870
Total gradient norm: 0.369348
=== Actor Training Debug (Iteration 8087) ===
Q mean: -14.476270
Q std: 21.447025
Actor loss: 14.480213
Action reg: 0.003943
  l1.weight: grad_norm = 0.297104
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.212158
Total gradient norm: 0.546456
=== Actor Training Debug (Iteration 8088) ===
Q mean: -13.532747
Q std: 21.892960
Actor loss: 13.536716
Action reg: 0.003969
  l1.weight: grad_norm = 0.495768
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.407112
Total gradient norm: 1.003228
=== Actor Training Debug (Iteration 8089) ===
Q mean: -15.691139
Q std: 20.574589
Actor loss: 15.695081
Action reg: 0.003942
  l1.weight: grad_norm = 0.124652
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.087524
Total gradient norm: 0.225277
=== Actor Training Debug (Iteration 8090) ===
Q mean: -16.043327
Q std: 22.033466
Actor loss: 16.047295
Action reg: 0.003967
  l1.weight: grad_norm = 0.204448
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.166966
Total gradient norm: 0.543759
=== Actor Training Debug (Iteration 8091) ===
Q mean: -14.583719
Q std: 21.895697
Actor loss: 14.587690
Action reg: 0.003972
  l1.weight: grad_norm = 0.303167
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.224937
Total gradient norm: 0.657065
=== Actor Training Debug (Iteration 8092) ===
Q mean: -13.417173
Q std: 19.504608
Actor loss: 13.421148
Action reg: 0.003975
  l1.weight: grad_norm = 0.129718
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.086902
Total gradient norm: 0.221877
=== Actor Training Debug (Iteration 8093) ===
Q mean: -14.417122
Q std: 20.450558
Actor loss: 14.421085
Action reg: 0.003964
  l1.weight: grad_norm = 0.503637
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.321227
Total gradient norm: 0.855878
=== Actor Training Debug (Iteration 8094) ===
Q mean: -17.877132
Q std: 23.448967
Actor loss: 17.881079
Action reg: 0.003946
  l1.weight: grad_norm = 0.194113
  l1.bias: grad_norm = 0.000643
  l2.weight: grad_norm = 0.120227
Total gradient norm: 0.321739
=== Actor Training Debug (Iteration 8095) ===
Q mean: -15.764118
Q std: 22.627184
Actor loss: 15.768084
Action reg: 0.003966
  l1.weight: grad_norm = 0.199448
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.164013
Total gradient norm: 0.505867
=== Actor Training Debug (Iteration 8096) ===
Q mean: -17.222462
Q std: 22.595055
Actor loss: 17.226418
Action reg: 0.003956
  l1.weight: grad_norm = 0.335665
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.251508
Total gradient norm: 0.726739
=== Actor Training Debug (Iteration 8097) ===
Q mean: -13.520150
Q std: 18.141573
Actor loss: 13.524107
Action reg: 0.003957
  l1.weight: grad_norm = 0.166671
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.124346
Total gradient norm: 0.313901
=== Actor Training Debug (Iteration 8098) ===
Q mean: -14.586864
Q std: 19.840693
Actor loss: 14.590834
Action reg: 0.003970
  l1.weight: grad_norm = 0.224249
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.152679
Total gradient norm: 0.385679
=== Actor Training Debug (Iteration 8099) ===
Q mean: -14.111112
Q std: 21.775448
Actor loss: 14.115072
Action reg: 0.003961
  l1.weight: grad_norm = 0.307985
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.209839
Total gradient norm: 0.671821
=== Actor Training Debug (Iteration 8100) ===
Q mean: -13.682590
Q std: 20.315193
Actor loss: 13.686563
Action reg: 0.003973
  l1.weight: grad_norm = 0.092884
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.063764
Total gradient norm: 0.189382
Episode 131: Steps=100, Reward=-267.931, Buffer_size=13100
=== Actor Training Debug (Iteration 8101) ===
Q mean: -16.590399
Q std: 22.132984
Actor loss: 16.594355
Action reg: 0.003956
  l1.weight: grad_norm = 0.180912
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.143920
Total gradient norm: 0.406195
=== Actor Training Debug (Iteration 8102) ===
Q mean: -14.738060
Q std: 21.664122
Actor loss: 14.742023
Action reg: 0.003964
  l1.weight: grad_norm = 0.139942
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.102091
Total gradient norm: 0.279602
=== Actor Training Debug (Iteration 8103) ===
Q mean: -16.213200
Q std: 22.615330
Actor loss: 16.217154
Action reg: 0.003954
  l1.weight: grad_norm = 0.407979
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.257984
Total gradient norm: 0.684703
=== Actor Training Debug (Iteration 8104) ===
Q mean: -13.979217
Q std: 20.569345
Actor loss: 13.983183
Action reg: 0.003966
  l1.weight: grad_norm = 0.237777
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.172666
Total gradient norm: 0.477933
=== Actor Training Debug (Iteration 8105) ===
Q mean: -14.551117
Q std: 21.469669
Actor loss: 14.555062
Action reg: 0.003945
  l1.weight: grad_norm = 0.231658
  l1.bias: grad_norm = 0.002293
  l2.weight: grad_norm = 0.178237
Total gradient norm: 0.459188
=== Actor Training Debug (Iteration 8106) ===
Q mean: -14.185838
Q std: 20.220797
Actor loss: 14.189788
Action reg: 0.003950
  l1.weight: grad_norm = 0.282041
  l1.bias: grad_norm = 0.001870
  l2.weight: grad_norm = 0.187625
Total gradient norm: 0.547095
=== Actor Training Debug (Iteration 8107) ===
Q mean: -13.768709
Q std: 19.481804
Actor loss: 13.772655
Action reg: 0.003945
  l1.weight: grad_norm = 0.238044
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.178089
Total gradient norm: 0.623892
=== Actor Training Debug (Iteration 8108) ===
Q mean: -15.624468
Q std: 22.557991
Actor loss: 15.628419
Action reg: 0.003951
  l1.weight: grad_norm = 0.225268
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.189744
Total gradient norm: 0.494524
=== Actor Training Debug (Iteration 8109) ===
Q mean: -14.606462
Q std: 20.711676
Actor loss: 14.610420
Action reg: 0.003957
  l1.weight: grad_norm = 0.308999
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.212874
Total gradient norm: 0.585575
=== Actor Training Debug (Iteration 8110) ===
Q mean: -13.546654
Q std: 19.623312
Actor loss: 13.550595
Action reg: 0.003942
  l1.weight: grad_norm = 0.240273
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.171160
Total gradient norm: 0.497931
=== Actor Training Debug (Iteration 8111) ===
Q mean: -14.598917
Q std: 21.812233
Actor loss: 14.602886
Action reg: 0.003969
  l1.weight: grad_norm = 0.391873
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.293425
Total gradient norm: 0.772749
=== Actor Training Debug (Iteration 8112) ===
Q mean: -14.364862
Q std: 20.269938
Actor loss: 14.368802
Action reg: 0.003939
  l1.weight: grad_norm = 0.202281
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.161480
Total gradient norm: 0.441407
=== Actor Training Debug (Iteration 8113) ===
Q mean: -16.546387
Q std: 21.696922
Actor loss: 16.550348
Action reg: 0.003962
  l1.weight: grad_norm = 0.284096
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.222343
Total gradient norm: 0.623095
=== Actor Training Debug (Iteration 8114) ===
Q mean: -12.674824
Q std: 18.763954
Actor loss: 12.678770
Action reg: 0.003946
  l1.weight: grad_norm = 0.110702
  l1.bias: grad_norm = 0.001935
  l2.weight: grad_norm = 0.083347
Total gradient norm: 0.235051
=== Actor Training Debug (Iteration 8115) ===
Q mean: -15.457578
Q std: 23.671078
Actor loss: 15.461535
Action reg: 0.003958
  l1.weight: grad_norm = 0.263869
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.202111
Total gradient norm: 0.581961
=== Actor Training Debug (Iteration 8116) ===
Q mean: -15.391602
Q std: 21.409929
Actor loss: 15.395572
Action reg: 0.003970
  l1.weight: grad_norm = 0.388256
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.302058
Total gradient norm: 0.807435
=== Actor Training Debug (Iteration 8117) ===
Q mean: -11.557944
Q std: 18.758915
Actor loss: 11.561918
Action reg: 0.003974
  l1.weight: grad_norm = 0.234613
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.189341
Total gradient norm: 0.572686
=== Actor Training Debug (Iteration 8118) ===
Q mean: -12.396996
Q std: 18.423286
Actor loss: 12.400956
Action reg: 0.003961
  l1.weight: grad_norm = 0.203357
  l1.bias: grad_norm = 0.001843
  l2.weight: grad_norm = 0.154816
Total gradient norm: 0.424442
=== Actor Training Debug (Iteration 8119) ===
Q mean: -14.699825
Q std: 22.533804
Actor loss: 14.703780
Action reg: 0.003955
  l1.weight: grad_norm = 0.234043
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.173899
Total gradient norm: 0.506111
=== Actor Training Debug (Iteration 8120) ===
Q mean: -12.993956
Q std: 18.785303
Actor loss: 12.997913
Action reg: 0.003958
  l1.weight: grad_norm = 0.344424
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.223046
Total gradient norm: 0.611983
=== Actor Training Debug (Iteration 8121) ===
Q mean: -14.030115
Q std: 21.065035
Actor loss: 14.034075
Action reg: 0.003960
  l1.weight: grad_norm = 0.186827
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.147558
Total gradient norm: 0.417922
=== Actor Training Debug (Iteration 8122) ===
Q mean: -12.530345
Q std: 19.429052
Actor loss: 12.534305
Action reg: 0.003959
  l1.weight: grad_norm = 0.411229
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.300040
Total gradient norm: 0.752198
=== Actor Training Debug (Iteration 8123) ===
Q mean: -12.272811
Q std: 18.498772
Actor loss: 12.276767
Action reg: 0.003955
  l1.weight: grad_norm = 0.311607
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.240498
Total gradient norm: 0.618554
=== Actor Training Debug (Iteration 8124) ===
Q mean: -14.981321
Q std: 21.120348
Actor loss: 14.985301
Action reg: 0.003979
  l1.weight: grad_norm = 0.247909
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.200494
Total gradient norm: 0.521167
=== Actor Training Debug (Iteration 8125) ===
Q mean: -12.235596
Q std: 17.775944
Actor loss: 12.239567
Action reg: 0.003971
  l1.weight: grad_norm = 0.263971
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.201583
Total gradient norm: 0.518980
=== Actor Training Debug (Iteration 8126) ===
Q mean: -16.624685
Q std: 21.897383
Actor loss: 16.628651
Action reg: 0.003966
  l1.weight: grad_norm = 0.209742
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.164550
Total gradient norm: 0.422627
=== Actor Training Debug (Iteration 8127) ===
Q mean: -15.872004
Q std: 22.279926
Actor loss: 15.875980
Action reg: 0.003977
  l1.weight: grad_norm = 0.188068
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.149823
Total gradient norm: 0.371495
=== Actor Training Debug (Iteration 8128) ===
Q mean: -12.781921
Q std: 19.561476
Actor loss: 12.785872
Action reg: 0.003951
  l1.weight: grad_norm = 0.233973
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.209992
Total gradient norm: 0.588881
=== Actor Training Debug (Iteration 8129) ===
Q mean: -13.430419
Q std: 20.040642
Actor loss: 13.434378
Action reg: 0.003958
  l1.weight: grad_norm = 0.139248
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.101104
Total gradient norm: 0.276614
=== Actor Training Debug (Iteration 8130) ===
Q mean: -15.643360
Q std: 20.817692
Actor loss: 15.647308
Action reg: 0.003949
  l1.weight: grad_norm = 0.218941
  l1.bias: grad_norm = 0.000529
  l2.weight: grad_norm = 0.141175
Total gradient norm: 0.450774
=== Actor Training Debug (Iteration 8131) ===
Q mean: -14.227435
Q std: 21.154217
Actor loss: 14.231396
Action reg: 0.003961
  l1.weight: grad_norm = 0.127152
  l1.bias: grad_norm = 0.000654
  l2.weight: grad_norm = 0.093896
Total gradient norm: 0.230444
=== Actor Training Debug (Iteration 8132) ===
Q mean: -13.998635
Q std: 20.100637
Actor loss: 14.002618
Action reg: 0.003982
  l1.weight: grad_norm = 0.209509
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.161209
Total gradient norm: 0.441278
=== Actor Training Debug (Iteration 8133) ===
Q mean: -13.600330
Q std: 20.037313
Actor loss: 13.604306
Action reg: 0.003976
  l1.weight: grad_norm = 0.147108
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.109002
Total gradient norm: 0.337924
=== Actor Training Debug (Iteration 8134) ===
Q mean: -12.381241
Q std: 18.939224
Actor loss: 12.385192
Action reg: 0.003951
  l1.weight: grad_norm = 0.248128
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.181349
Total gradient norm: 0.502712
=== Actor Training Debug (Iteration 8135) ===
Q mean: -13.044079
Q std: 20.761768
Actor loss: 13.048030
Action reg: 0.003951
  l1.weight: grad_norm = 0.307682
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.202347
Total gradient norm: 0.581653
=== Actor Training Debug (Iteration 8136) ===
Q mean: -11.495928
Q std: 19.574644
Actor loss: 11.499884
Action reg: 0.003956
  l1.weight: grad_norm = 0.343449
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.262953
Total gradient norm: 0.677171
=== Actor Training Debug (Iteration 8137) ===
Q mean: -15.411567
Q std: 21.087751
Actor loss: 15.415536
Action reg: 0.003969
  l1.weight: grad_norm = 0.255447
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.195172
Total gradient norm: 0.627433
=== Actor Training Debug (Iteration 8138) ===
Q mean: -14.401651
Q std: 21.025612
Actor loss: 14.405590
Action reg: 0.003938
  l1.weight: grad_norm = 0.334551
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.248561
Total gradient norm: 0.666581
=== Actor Training Debug (Iteration 8139) ===
Q mean: -14.550488
Q std: 21.240931
Actor loss: 14.554453
Action reg: 0.003966
  l1.weight: grad_norm = 0.155242
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.127076
Total gradient norm: 0.384764
=== Actor Training Debug (Iteration 8140) ===
Q mean: -14.659060
Q std: 21.532640
Actor loss: 14.663021
Action reg: 0.003962
  l1.weight: grad_norm = 0.192531
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.138514
Total gradient norm: 0.395703
=== Actor Training Debug (Iteration 8141) ===
Q mean: -13.343100
Q std: 19.674603
Actor loss: 13.347022
Action reg: 0.003922
  l1.weight: grad_norm = 0.444600
  l1.bias: grad_norm = 0.001455
  l2.weight: grad_norm = 0.308221
Total gradient norm: 0.892385
=== Actor Training Debug (Iteration 8142) ===
Q mean: -14.273411
Q std: 20.018410
Actor loss: 14.277388
Action reg: 0.003977
  l1.weight: grad_norm = 0.200060
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.144338
Total gradient norm: 0.407692
=== Actor Training Debug (Iteration 8143) ===
Q mean: -13.454163
Q std: 21.133478
Actor loss: 13.458116
Action reg: 0.003953
  l1.weight: grad_norm = 0.317467
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.220305
Total gradient norm: 0.555856
=== Actor Training Debug (Iteration 8144) ===
Q mean: -13.697369
Q std: 19.848011
Actor loss: 13.701336
Action reg: 0.003967
  l1.weight: grad_norm = 0.192802
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.150810
Total gradient norm: 0.444185
=== Actor Training Debug (Iteration 8145) ===
Q mean: -14.867444
Q std: 22.410019
Actor loss: 14.871424
Action reg: 0.003980
  l1.weight: grad_norm = 0.168196
  l1.bias: grad_norm = 0.001944
  l2.weight: grad_norm = 0.110287
Total gradient norm: 0.285220
=== Actor Training Debug (Iteration 8146) ===
Q mean: -13.490808
Q std: 20.337811
Actor loss: 13.494754
Action reg: 0.003945
  l1.weight: grad_norm = 0.413393
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.300103
Total gradient norm: 0.866289
=== Actor Training Debug (Iteration 8147) ===
Q mean: -12.486757
Q std: 19.059940
Actor loss: 12.490708
Action reg: 0.003951
  l1.weight: grad_norm = 0.220448
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.174242
Total gradient norm: 0.454129
=== Actor Training Debug (Iteration 8148) ===
Q mean: -15.929212
Q std: 21.337799
Actor loss: 15.933187
Action reg: 0.003975
  l1.weight: grad_norm = 0.261654
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.184102
Total gradient norm: 0.493296
=== Actor Training Debug (Iteration 8149) ===
Q mean: -15.009602
Q std: 21.353592
Actor loss: 15.013560
Action reg: 0.003959
  l1.weight: grad_norm = 0.166469
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.122687
Total gradient norm: 0.331645
=== Actor Training Debug (Iteration 8150) ===
Q mean: -12.595488
Q std: 18.759640
Actor loss: 12.599465
Action reg: 0.003978
  l1.weight: grad_norm = 0.321123
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.259128
Total gradient norm: 0.754873
=== Actor Training Debug (Iteration 8151) ===
Q mean: -14.088584
Q std: 20.242147
Actor loss: 14.092566
Action reg: 0.003981
  l1.weight: grad_norm = 0.091696
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.075959
Total gradient norm: 0.239572
=== Actor Training Debug (Iteration 8152) ===
Q mean: -14.026089
Q std: 19.593992
Actor loss: 14.030058
Action reg: 0.003970
  l1.weight: grad_norm = 0.240493
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.182401
Total gradient norm: 0.472655
=== Actor Training Debug (Iteration 8153) ===
Q mean: -16.898756
Q std: 22.718651
Actor loss: 16.902716
Action reg: 0.003960
  l1.weight: grad_norm = 0.660302
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.496676
Total gradient norm: 1.489169
=== Actor Training Debug (Iteration 8154) ===
Q mean: -14.451856
Q std: 21.140484
Actor loss: 14.455809
Action reg: 0.003953
  l1.weight: grad_norm = 0.328630
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.228364
Total gradient norm: 0.596190
=== Actor Training Debug (Iteration 8155) ===
Q mean: -14.499956
Q std: 21.933523
Actor loss: 14.503901
Action reg: 0.003945
  l1.weight: grad_norm = 0.145336
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.110451
Total gradient norm: 0.348877
=== Actor Training Debug (Iteration 8156) ===
Q mean: -12.867159
Q std: 20.111841
Actor loss: 12.871138
Action reg: 0.003978
  l1.weight: grad_norm = 0.269445
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.201810
Total gradient norm: 0.563116
=== Actor Training Debug (Iteration 8157) ===
Q mean: -13.693243
Q std: 19.874712
Actor loss: 13.697204
Action reg: 0.003961
  l1.weight: grad_norm = 0.249741
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.184937
Total gradient norm: 0.487381
=== Actor Training Debug (Iteration 8158) ===
Q mean: -15.705534
Q std: 20.825474
Actor loss: 15.709485
Action reg: 0.003951
  l1.weight: grad_norm = 0.289964
  l1.bias: grad_norm = 0.002690
  l2.weight: grad_norm = 0.196343
Total gradient norm: 0.538598
=== Actor Training Debug (Iteration 8159) ===
Q mean: -14.778440
Q std: 21.744415
Actor loss: 14.782420
Action reg: 0.003981
  l1.weight: grad_norm = 0.146575
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.115397
Total gradient norm: 0.340804
=== Actor Training Debug (Iteration 8160) ===
Q mean: -13.774415
Q std: 21.023376
Actor loss: 13.778399
Action reg: 0.003984
  l1.weight: grad_norm = 0.186694
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.140995
Total gradient norm: 0.399319
=== Actor Training Debug (Iteration 8161) ===
Q mean: -13.729557
Q std: 21.487219
Actor loss: 13.733519
Action reg: 0.003962
  l1.weight: grad_norm = 0.373164
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.277068
Total gradient norm: 0.736198
=== Actor Training Debug (Iteration 8162) ===
Q mean: -15.519695
Q std: 22.791212
Actor loss: 15.523626
Action reg: 0.003931
  l1.weight: grad_norm = 0.276517
  l1.bias: grad_norm = 0.000454
  l2.weight: grad_norm = 0.225340
Total gradient norm: 0.671644
=== Actor Training Debug (Iteration 8163) ===
Q mean: -13.562326
Q std: 20.485697
Actor loss: 13.566299
Action reg: 0.003973
  l1.weight: grad_norm = 0.223074
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.173747
Total gradient norm: 0.465489
=== Actor Training Debug (Iteration 8164) ===
Q mean: -11.382829
Q std: 18.314402
Actor loss: 11.386798
Action reg: 0.003969
  l1.weight: grad_norm = 0.174545
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.131701
Total gradient norm: 0.366384
=== Actor Training Debug (Iteration 8165) ===
Q mean: -17.001368
Q std: 23.015604
Actor loss: 17.005327
Action reg: 0.003960
  l1.weight: grad_norm = 0.404911
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.263117
Total gradient norm: 0.778803
=== Actor Training Debug (Iteration 8166) ===
Q mean: -15.437677
Q std: 20.949461
Actor loss: 15.441631
Action reg: 0.003954
  l1.weight: grad_norm = 0.295726
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.224401
Total gradient norm: 0.636948
=== Actor Training Debug (Iteration 8167) ===
Q mean: -14.594415
Q std: 21.207230
Actor loss: 14.598385
Action reg: 0.003970
  l1.weight: grad_norm = 0.198260
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.168030
Total gradient norm: 0.478965
=== Actor Training Debug (Iteration 8168) ===
Q mean: -12.803731
Q std: 19.230404
Actor loss: 12.807675
Action reg: 0.003944
  l1.weight: grad_norm = 0.207976
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.160939
Total gradient norm: 0.489077
=== Actor Training Debug (Iteration 8169) ===
Q mean: -14.139544
Q std: 20.393536
Actor loss: 14.143515
Action reg: 0.003972
  l1.weight: grad_norm = 0.162488
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.130656
Total gradient norm: 0.382178
=== Actor Training Debug (Iteration 8170) ===
Q mean: -15.168139
Q std: 22.244635
Actor loss: 15.172110
Action reg: 0.003970
  l1.weight: grad_norm = 0.213573
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.167889
Total gradient norm: 0.443295
=== Actor Training Debug (Iteration 8171) ===
Q mean: -16.447208
Q std: 22.869150
Actor loss: 16.451187
Action reg: 0.003979
  l1.weight: grad_norm = 0.241615
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.220172
Total gradient norm: 0.704995
=== Actor Training Debug (Iteration 8172) ===
Q mean: -14.166218
Q std: 21.107063
Actor loss: 14.170161
Action reg: 0.003944
  l1.weight: grad_norm = 0.203428
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.160685
Total gradient norm: 0.397173
=== Actor Training Debug (Iteration 8173) ===
Q mean: -15.967402
Q std: 22.699562
Actor loss: 15.971380
Action reg: 0.003978
  l1.weight: grad_norm = 0.180458
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.147961
Total gradient norm: 0.407009
=== Actor Training Debug (Iteration 8174) ===
Q mean: -13.976791
Q std: 19.572983
Actor loss: 13.980742
Action reg: 0.003951
  l1.weight: grad_norm = 0.864016
  l1.bias: grad_norm = 0.000939
  l2.weight: grad_norm = 0.681001
Total gradient norm: 2.032418
=== Actor Training Debug (Iteration 8175) ===
Q mean: -12.473680
Q std: 19.035069
Actor loss: 12.477630
Action reg: 0.003949
  l1.weight: grad_norm = 0.323494
  l1.bias: grad_norm = 0.002524
  l2.weight: grad_norm = 0.235815
Total gradient norm: 0.686431
=== Actor Training Debug (Iteration 8176) ===
Q mean: -13.565430
Q std: 21.384323
Actor loss: 13.569407
Action reg: 0.003977
  l1.weight: grad_norm = 0.199934
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.166363
Total gradient norm: 0.461336
=== Actor Training Debug (Iteration 8177) ===
Q mean: -18.994589
Q std: 24.640476
Actor loss: 18.998554
Action reg: 0.003966
  l1.weight: grad_norm = 0.121386
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.087981
Total gradient norm: 0.233740
=== Actor Training Debug (Iteration 8178) ===
Q mean: -17.104832
Q std: 22.460766
Actor loss: 17.108791
Action reg: 0.003960
  l1.weight: grad_norm = 0.154672
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.126075
Total gradient norm: 0.355218
=== Actor Training Debug (Iteration 8179) ===
Q mean: -13.954575
Q std: 20.286001
Actor loss: 13.958529
Action reg: 0.003955
  l1.weight: grad_norm = 0.225646
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.211922
Total gradient norm: 0.650046
=== Actor Training Debug (Iteration 8180) ===
Q mean: -14.321815
Q std: 20.178072
Actor loss: 14.325783
Action reg: 0.003967
  l1.weight: grad_norm = 0.108730
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.073362
Total gradient norm: 0.195554
=== Actor Training Debug (Iteration 8181) ===
Q mean: -13.663088
Q std: 20.088873
Actor loss: 13.667059
Action reg: 0.003971
  l1.weight: grad_norm = 0.337830
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.240091
Total gradient norm: 0.684907
=== Actor Training Debug (Iteration 8182) ===
Q mean: -17.204369
Q std: 23.083050
Actor loss: 17.208342
Action reg: 0.003973
  l1.weight: grad_norm = 0.132119
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.106963
Total gradient norm: 0.307493
=== Actor Training Debug (Iteration 8183) ===
Q mean: -14.673817
Q std: 19.173870
Actor loss: 14.677795
Action reg: 0.003979
  l1.weight: grad_norm = 0.384308
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.241983
Total gradient norm: 0.662719
=== Actor Training Debug (Iteration 8184) ===
Q mean: -14.919323
Q std: 21.303431
Actor loss: 14.923265
Action reg: 0.003942
  l1.weight: grad_norm = 0.537803
  l1.bias: grad_norm = 0.000690
  l2.weight: grad_norm = 0.341887
Total gradient norm: 0.916085
=== Actor Training Debug (Iteration 8185) ===
Q mean: -14.065808
Q std: 21.184515
Actor loss: 14.069756
Action reg: 0.003948
  l1.weight: grad_norm = 0.286005
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.197102
Total gradient norm: 0.563318
=== Actor Training Debug (Iteration 8186) ===
Q mean: -16.204956
Q std: 21.517284
Actor loss: 16.208920
Action reg: 0.003964
  l1.weight: grad_norm = 0.227871
  l1.bias: grad_norm = 0.000690
  l2.weight: grad_norm = 0.177632
Total gradient norm: 0.649078
=== Actor Training Debug (Iteration 8187) ===
Q mean: -15.355659
Q std: 21.725830
Actor loss: 15.359622
Action reg: 0.003962
  l1.weight: grad_norm = 0.150614
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.114561
Total gradient norm: 0.333116
=== Actor Training Debug (Iteration 8188) ===
Q mean: -15.287796
Q std: 22.729872
Actor loss: 15.291738
Action reg: 0.003942
  l1.weight: grad_norm = 0.318751
  l1.bias: grad_norm = 0.000821
  l2.weight: grad_norm = 0.218519
Total gradient norm: 0.667887
=== Actor Training Debug (Iteration 8189) ===
Q mean: -14.886412
Q std: 20.128950
Actor loss: 14.890387
Action reg: 0.003975
  l1.weight: grad_norm = 0.144908
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.101788
Total gradient norm: 0.264617
=== Actor Training Debug (Iteration 8190) ===
Q mean: -13.943121
Q std: 20.300098
Actor loss: 13.947072
Action reg: 0.003952
  l1.weight: grad_norm = 0.074611
  l1.bias: grad_norm = 0.001024
  l2.weight: grad_norm = 0.059135
Total gradient norm: 0.162710
=== Actor Training Debug (Iteration 8191) ===
Q mean: -14.496686
Q std: 20.795986
Actor loss: 14.500660
Action reg: 0.003974
  l1.weight: grad_norm = 0.332493
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.279315
Total gradient norm: 0.889484
=== Actor Training Debug (Iteration 8192) ===
Q mean: -14.413292
Q std: 21.663319
Actor loss: 14.417232
Action reg: 0.003940
  l1.weight: grad_norm = 0.332930
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.238634
Total gradient norm: 0.695799
=== Actor Training Debug (Iteration 8193) ===
Q mean: -13.408913
Q std: 19.634754
Actor loss: 13.412878
Action reg: 0.003966
  l1.weight: grad_norm = 0.277670
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.205242
Total gradient norm: 0.543223
=== Actor Training Debug (Iteration 8194) ===
Q mean: -13.698101
Q std: 21.826527
Actor loss: 13.702040
Action reg: 0.003938
  l1.weight: grad_norm = 0.207033
  l1.bias: grad_norm = 0.000624
  l2.weight: grad_norm = 0.161858
Total gradient norm: 0.470903
=== Actor Training Debug (Iteration 8195) ===
Q mean: -11.633991
Q std: 18.220055
Actor loss: 11.637954
Action reg: 0.003963
  l1.weight: grad_norm = 0.988193
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.835828
Total gradient norm: 2.804910
=== Actor Training Debug (Iteration 8196) ===
Q mean: -16.514513
Q std: 23.224031
Actor loss: 16.518475
Action reg: 0.003962
  l1.weight: grad_norm = 0.459634
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.328070
Total gradient norm: 1.079204
=== Actor Training Debug (Iteration 8197) ===
Q mean: -13.240329
Q std: 19.413671
Actor loss: 13.244281
Action reg: 0.003952
  l1.weight: grad_norm = 0.082559
  l1.bias: grad_norm = 0.000616
  l2.weight: grad_norm = 0.067320
Total gradient norm: 0.173560
=== Actor Training Debug (Iteration 8198) ===
Q mean: -15.439694
Q std: 22.352446
Actor loss: 15.443639
Action reg: 0.003944
  l1.weight: grad_norm = 0.329964
  l1.bias: grad_norm = 0.000552
  l2.weight: grad_norm = 0.255479
Total gradient norm: 0.689388
=== Actor Training Debug (Iteration 8199) ===
Q mean: -14.792448
Q std: 22.843021
Actor loss: 14.796410
Action reg: 0.003961
  l1.weight: grad_norm = 0.242841
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.180939
Total gradient norm: 0.472842
=== Actor Training Debug (Iteration 8200) ===
Q mean: -16.298931
Q std: 22.200165
Actor loss: 16.302876
Action reg: 0.003945
  l1.weight: grad_norm = 0.252684
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.245739
Total gradient norm: 0.753731
=== Actor Training Debug (Iteration 8201) ===
Q mean: -14.899972
Q std: 21.712225
Actor loss: 14.903944
Action reg: 0.003973
  l1.weight: grad_norm = 0.243380
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.165696
Total gradient norm: 0.508949
=== Actor Training Debug (Iteration 8202) ===
Q mean: -17.351734
Q std: 22.902128
Actor loss: 17.355707
Action reg: 0.003973
  l1.weight: grad_norm = 0.230959
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.181248
Total gradient norm: 0.531070
=== Actor Training Debug (Iteration 8203) ===
Q mean: -13.498218
Q std: 19.495626
Actor loss: 13.502159
Action reg: 0.003942
  l1.weight: grad_norm = 0.193585
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.141188
Total gradient norm: 0.352405
=== Actor Training Debug (Iteration 8204) ===
Q mean: -15.361237
Q std: 21.008949
Actor loss: 15.365203
Action reg: 0.003966
  l1.weight: grad_norm = 0.167210
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.121282
Total gradient norm: 0.331122
=== Actor Training Debug (Iteration 8205) ===
Q mean: -15.390828
Q std: 19.087366
Actor loss: 15.394799
Action reg: 0.003971
  l1.weight: grad_norm = 0.178235
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.143001
Total gradient norm: 0.359826
=== Actor Training Debug (Iteration 8206) ===
Q mean: -14.774123
Q std: 21.494591
Actor loss: 14.778099
Action reg: 0.003976
  l1.weight: grad_norm = 0.415842
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.289574
Total gradient norm: 0.703000
=== Actor Training Debug (Iteration 8207) ===
Q mean: -14.491186
Q std: 21.227701
Actor loss: 14.495116
Action reg: 0.003931
  l1.weight: grad_norm = 0.349029
  l1.bias: grad_norm = 0.000628
  l2.weight: grad_norm = 0.241618
Total gradient norm: 0.636555
=== Actor Training Debug (Iteration 8208) ===
Q mean: -13.870502
Q std: 20.643518
Actor loss: 13.874480
Action reg: 0.003978
  l1.weight: grad_norm = 0.162792
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.114047
Total gradient norm: 0.358923
=== Actor Training Debug (Iteration 8209) ===
Q mean: -14.385137
Q std: 21.513117
Actor loss: 14.389105
Action reg: 0.003968
  l1.weight: grad_norm = 0.136081
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.110462
Total gradient norm: 0.276484
=== Actor Training Debug (Iteration 8210) ===
Q mean: -16.043255
Q std: 22.096292
Actor loss: 16.047213
Action reg: 0.003957
  l1.weight: grad_norm = 0.230719
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.159778
Total gradient norm: 0.407126
=== Actor Training Debug (Iteration 8211) ===
Q mean: -15.723099
Q std: 21.001719
Actor loss: 15.727065
Action reg: 0.003966
  l1.weight: grad_norm = 0.118274
  l1.bias: grad_norm = 0.002100
  l2.weight: grad_norm = 0.096040
Total gradient norm: 0.260122
=== Actor Training Debug (Iteration 8212) ===
Q mean: -14.678707
Q std: 21.055428
Actor loss: 14.682667
Action reg: 0.003960
  l1.weight: grad_norm = 0.404575
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.296402
Total gradient norm: 0.874746
=== Actor Training Debug (Iteration 8213) ===
Q mean: -14.841236
Q std: 19.804707
Actor loss: 14.845191
Action reg: 0.003954
  l1.weight: grad_norm = 0.216033
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.176185
Total gradient norm: 0.505211
=== Actor Training Debug (Iteration 8214) ===
Q mean: -14.063890
Q std: 19.406967
Actor loss: 14.067860
Action reg: 0.003969
  l1.weight: grad_norm = 0.271604
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.184898
Total gradient norm: 0.476434
=== Actor Training Debug (Iteration 8215) ===
Q mean: -13.921480
Q std: 20.897114
Actor loss: 13.925432
Action reg: 0.003952
  l1.weight: grad_norm = 0.318029
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.205506
Total gradient norm: 0.565606
=== Actor Training Debug (Iteration 8216) ===
Q mean: -15.536005
Q std: 22.248951
Actor loss: 15.539961
Action reg: 0.003956
  l1.weight: grad_norm = 0.345106
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.289089
Total gradient norm: 0.903159
=== Actor Training Debug (Iteration 8217) ===
Q mean: -15.069555
Q std: 20.784698
Actor loss: 15.073473
Action reg: 0.003917
  l1.weight: grad_norm = 0.209411
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.170088
Total gradient norm: 0.505762
=== Actor Training Debug (Iteration 8218) ===
Q mean: -14.701481
Q std: 21.228346
Actor loss: 14.705458
Action reg: 0.003976
  l1.weight: grad_norm = 0.202126
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.141634
Total gradient norm: 0.423829
=== Actor Training Debug (Iteration 8219) ===
Q mean: -12.730536
Q std: 19.717722
Actor loss: 12.734490
Action reg: 0.003955
  l1.weight: grad_norm = 0.198923
  l1.bias: grad_norm = 0.000617
  l2.weight: grad_norm = 0.152631
Total gradient norm: 0.467524
=== Actor Training Debug (Iteration 8220) ===
Q mean: -16.710400
Q std: 23.498636
Actor loss: 16.714354
Action reg: 0.003954
  l1.weight: grad_norm = 0.287592
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.213811
Total gradient norm: 0.580100
=== Actor Training Debug (Iteration 8221) ===
Q mean: -13.527696
Q std: 20.716200
Actor loss: 13.531630
Action reg: 0.003934
  l1.weight: grad_norm = 0.260091
  l1.bias: grad_norm = 0.001706
  l2.weight: grad_norm = 0.204039
Total gradient norm: 0.633845
=== Actor Training Debug (Iteration 8222) ===
Q mean: -15.539055
Q std: 21.087709
Actor loss: 15.542998
Action reg: 0.003944
  l1.weight: grad_norm = 0.093951
  l1.bias: grad_norm = 0.000535
  l2.weight: grad_norm = 0.073002
Total gradient norm: 0.221085
=== Actor Training Debug (Iteration 8223) ===
Q mean: -14.998045
Q std: 21.299017
Actor loss: 15.001997
Action reg: 0.003952
  l1.weight: grad_norm = 0.178930
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.128326
Total gradient norm: 0.324786
=== Actor Training Debug (Iteration 8224) ===
Q mean: -14.569889
Q std: 20.971521
Actor loss: 14.573859
Action reg: 0.003970
  l1.weight: grad_norm = 0.144755
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.106935
Total gradient norm: 0.278775
=== Actor Training Debug (Iteration 8225) ===
Q mean: -14.125650
Q std: 20.160294
Actor loss: 14.129597
Action reg: 0.003947
  l1.weight: grad_norm = 0.285762
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.198355
Total gradient norm: 0.547904
=== Actor Training Debug (Iteration 8226) ===
Q mean: -15.304209
Q std: 20.859682
Actor loss: 15.308166
Action reg: 0.003957
  l1.weight: grad_norm = 0.265185
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.175445
Total gradient norm: 0.437797
=== Actor Training Debug (Iteration 8227) ===
Q mean: -14.278542
Q std: 21.904699
Actor loss: 14.282491
Action reg: 0.003949
  l1.weight: grad_norm = 0.402663
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.312683
Total gradient norm: 0.816216
=== Actor Training Debug (Iteration 8228) ===
Q mean: -15.330364
Q std: 21.113857
Actor loss: 15.334319
Action reg: 0.003955
  l1.weight: grad_norm = 0.311999
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.226816
Total gradient norm: 0.558609
=== Actor Training Debug (Iteration 8229) ===
Q mean: -15.220619
Q std: 20.906698
Actor loss: 15.224569
Action reg: 0.003950
  l1.weight: grad_norm = 0.090650
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.072683
Total gradient norm: 0.218690
=== Actor Training Debug (Iteration 8230) ===
Q mean: -14.151073
Q std: 20.229774
Actor loss: 14.155046
Action reg: 0.003972
  l1.weight: grad_norm = 0.246887
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.192099
Total gradient norm: 0.582366
=== Actor Training Debug (Iteration 8231) ===
Q mean: -13.988219
Q std: 20.622734
Actor loss: 13.992185
Action reg: 0.003966
  l1.weight: grad_norm = 0.455062
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.389720
Total gradient norm: 1.107968
=== Actor Training Debug (Iteration 8232) ===
Q mean: -16.445284
Q std: 21.258593
Actor loss: 16.449251
Action reg: 0.003968
  l1.weight: grad_norm = 0.126591
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.085850
Total gradient norm: 0.266257
=== Actor Training Debug (Iteration 8233) ===
Q mean: -15.451244
Q std: 21.239960
Actor loss: 15.455215
Action reg: 0.003970
  l1.weight: grad_norm = 0.193408
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.130800
Total gradient norm: 0.343409
=== Actor Training Debug (Iteration 8234) ===
Q mean: -15.427355
Q std: 21.337500
Actor loss: 15.431328
Action reg: 0.003973
  l1.weight: grad_norm = 0.182845
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.151188
Total gradient norm: 0.439320
=== Actor Training Debug (Iteration 8235) ===
Q mean: -18.261600
Q std: 23.323900
Actor loss: 18.265564
Action reg: 0.003964
  l1.weight: grad_norm = 0.261452
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.167974
Total gradient norm: 0.428150
=== Actor Training Debug (Iteration 8236) ===
Q mean: -13.072674
Q std: 21.297634
Actor loss: 13.076633
Action reg: 0.003960
  l1.weight: grad_norm = 0.503115
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.331390
Total gradient norm: 0.830708
=== Actor Training Debug (Iteration 8237) ===
Q mean: -17.759085
Q std: 23.728371
Actor loss: 17.763042
Action reg: 0.003958
  l1.weight: grad_norm = 0.205606
  l1.bias: grad_norm = 0.002034
  l2.weight: grad_norm = 0.128471
Total gradient norm: 0.345741
=== Actor Training Debug (Iteration 8238) ===
Q mean: -13.512058
Q std: 19.970018
Actor loss: 13.516040
Action reg: 0.003981
  l1.weight: grad_norm = 0.177588
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.120665
Total gradient norm: 0.362011
=== Actor Training Debug (Iteration 8239) ===
Q mean: -14.879539
Q std: 21.466146
Actor loss: 14.883495
Action reg: 0.003956
  l1.weight: grad_norm = 0.083597
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.062272
Total gradient norm: 0.167771
=== Actor Training Debug (Iteration 8240) ===
Q mean: -12.959641
Q std: 18.895466
Actor loss: 12.963619
Action reg: 0.003979
  l1.weight: grad_norm = 0.301466
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.263113
Total gradient norm: 0.704016
=== Actor Training Debug (Iteration 8241) ===
Q mean: -13.240637
Q std: 17.842016
Actor loss: 13.244578
Action reg: 0.003942
  l1.weight: grad_norm = 0.231031
  l1.bias: grad_norm = 0.001794
  l2.weight: grad_norm = 0.192121
Total gradient norm: 0.522548
=== Actor Training Debug (Iteration 8242) ===
Q mean: -12.996552
Q std: 20.809135
Actor loss: 13.000501
Action reg: 0.003949
  l1.weight: grad_norm = 0.303494
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.234502
Total gradient norm: 0.734675
=== Actor Training Debug (Iteration 8243) ===
Q mean: -15.604761
Q std: 21.084324
Actor loss: 15.608704
Action reg: 0.003942
  l1.weight: grad_norm = 0.378635
  l1.bias: grad_norm = 0.000666
  l2.weight: grad_norm = 0.285908
Total gradient norm: 0.769877
=== Actor Training Debug (Iteration 8244) ===
Q mean: -14.930335
Q std: 22.204779
Actor loss: 14.934306
Action reg: 0.003971
  l1.weight: grad_norm = 1.080054
  l1.bias: grad_norm = 0.000693
  l2.weight: grad_norm = 0.943944
Total gradient norm: 2.925358
=== Actor Training Debug (Iteration 8245) ===
Q mean: -14.478651
Q std: 21.053694
Actor loss: 14.482614
Action reg: 0.003963
  l1.weight: grad_norm = 0.271362
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.227772
Total gradient norm: 0.633683
=== Actor Training Debug (Iteration 8246) ===
Q mean: -13.996128
Q std: 21.326756
Actor loss: 14.000080
Action reg: 0.003952
  l1.weight: grad_norm = 0.400867
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.315503
Total gradient norm: 0.881310
=== Actor Training Debug (Iteration 8247) ===
Q mean: -16.328447
Q std: 22.978418
Actor loss: 16.332417
Action reg: 0.003969
  l1.weight: grad_norm = 0.195972
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.153446
Total gradient norm: 0.381661
=== Actor Training Debug (Iteration 8248) ===
Q mean: -13.766958
Q std: 19.752907
Actor loss: 13.770898
Action reg: 0.003940
  l1.weight: grad_norm = 0.270483
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.191342
Total gradient norm: 0.654213
=== Actor Training Debug (Iteration 8249) ===
Q mean: -17.104639
Q std: 21.947504
Actor loss: 17.108589
Action reg: 0.003951
  l1.weight: grad_norm = 0.511044
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.315723
Total gradient norm: 0.845962
=== Actor Training Debug (Iteration 8250) ===
Q mean: -13.798534
Q std: 19.688225
Actor loss: 13.802506
Action reg: 0.003972
  l1.weight: grad_norm = 0.191664
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.142661
Total gradient norm: 0.408808
=== Actor Training Debug (Iteration 8251) ===
Q mean: -12.315606
Q std: 20.572392
Actor loss: 12.319551
Action reg: 0.003945
  l1.weight: grad_norm = 0.317892
  l1.bias: grad_norm = 0.000557
  l2.weight: grad_norm = 0.243852
Total gradient norm: 0.637744
=== Actor Training Debug (Iteration 8252) ===
Q mean: -13.382348
Q std: 21.547125
Actor loss: 13.386305
Action reg: 0.003957
  l1.weight: grad_norm = 0.261750
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.163912
Total gradient norm: 0.482919
=== Actor Training Debug (Iteration 8253) ===
Q mean: -14.956306
Q std: 20.887911
Actor loss: 14.960250
Action reg: 0.003943
  l1.weight: grad_norm = 0.213178
  l1.bias: grad_norm = 0.000852
  l2.weight: grad_norm = 0.148085
Total gradient norm: 0.426017
=== Actor Training Debug (Iteration 8254) ===
Q mean: -15.012365
Q std: 21.375702
Actor loss: 15.016336
Action reg: 0.003971
  l1.weight: grad_norm = 0.172008
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.148800
Total gradient norm: 0.414596
=== Actor Training Debug (Iteration 8255) ===
Q mean: -14.958931
Q std: 21.514486
Actor loss: 14.962897
Action reg: 0.003966
  l1.weight: grad_norm = 0.125970
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.117178
Total gradient norm: 0.324398
=== Actor Training Debug (Iteration 8256) ===
Q mean: -13.403574
Q std: 21.817362
Actor loss: 13.407537
Action reg: 0.003962
  l1.weight: grad_norm = 0.209513
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.160195
Total gradient norm: 0.470463
=== Actor Training Debug (Iteration 8257) ===
Q mean: -14.777564
Q std: 20.574606
Actor loss: 14.781519
Action reg: 0.003955
  l1.weight: grad_norm = 0.238222
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.170910
Total gradient norm: 0.476110
=== Actor Training Debug (Iteration 8258) ===
Q mean: -14.158913
Q std: 20.879845
Actor loss: 14.162872
Action reg: 0.003960
  l1.weight: grad_norm = 0.448735
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.272625
Total gradient norm: 0.830975
=== Actor Training Debug (Iteration 8259) ===
Q mean: -13.006413
Q std: 20.227562
Actor loss: 13.010377
Action reg: 0.003964
  l1.weight: grad_norm = 0.554442
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.344041
Total gradient norm: 0.948332
=== Actor Training Debug (Iteration 8260) ===
Q mean: -10.689131
Q std: 17.447969
Actor loss: 10.693092
Action reg: 0.003962
  l1.weight: grad_norm = 0.183045
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.146068
Total gradient norm: 0.419805
=== Actor Training Debug (Iteration 8261) ===
Q mean: -14.784643
Q std: 21.165485
Actor loss: 14.788560
Action reg: 0.003917
  l1.weight: grad_norm = 0.274520
  l1.bias: grad_norm = 0.000649
  l2.weight: grad_norm = 0.195037
Total gradient norm: 0.482344
=== Actor Training Debug (Iteration 8262) ===
Q mean: -16.629457
Q std: 22.386240
Actor loss: 16.633415
Action reg: 0.003958
  l1.weight: grad_norm = 0.290410
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.195190
Total gradient norm: 0.555464
=== Actor Training Debug (Iteration 8263) ===
Q mean: -14.620842
Q std: 21.006243
Actor loss: 14.624817
Action reg: 0.003975
  l1.weight: grad_norm = 0.243182
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.179053
Total gradient norm: 0.498823
=== Actor Training Debug (Iteration 8264) ===
Q mean: -12.889133
Q std: 20.299501
Actor loss: 12.893060
Action reg: 0.003926
  l1.weight: grad_norm = 0.225608
  l1.bias: grad_norm = 0.003886
  l2.weight: grad_norm = 0.201380
Total gradient norm: 0.566801
=== Actor Training Debug (Iteration 8265) ===
Q mean: -14.832933
Q std: 21.024603
Actor loss: 14.836849
Action reg: 0.003916
  l1.weight: grad_norm = 0.424590
  l1.bias: grad_norm = 0.001725
  l2.weight: grad_norm = 0.291819
Total gradient norm: 1.009866
=== Actor Training Debug (Iteration 8266) ===
Q mean: -12.106430
Q std: 19.625664
Actor loss: 12.110384
Action reg: 0.003954
  l1.weight: grad_norm = 0.375652
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.315949
Total gradient norm: 0.959833
=== Actor Training Debug (Iteration 8267) ===
Q mean: -15.764381
Q std: 22.702045
Actor loss: 15.768335
Action reg: 0.003954
  l1.weight: grad_norm = 0.170815
  l1.bias: grad_norm = 0.000604
  l2.weight: grad_norm = 0.117992
Total gradient norm: 0.355439
=== Actor Training Debug (Iteration 8268) ===
Q mean: -16.939871
Q std: 23.193758
Actor loss: 16.943832
Action reg: 0.003961
  l1.weight: grad_norm = 0.256848
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.204656
Total gradient norm: 0.583846
=== Actor Training Debug (Iteration 8269) ===
Q mean: -14.476164
Q std: 21.296455
Actor loss: 14.480123
Action reg: 0.003959
  l1.weight: grad_norm = 0.249126
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.179141
Total gradient norm: 0.495037
=== Actor Training Debug (Iteration 8270) ===
Q mean: -13.431446
Q std: 20.282391
Actor loss: 13.435407
Action reg: 0.003960
  l1.weight: grad_norm = 0.369982
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.282494
Total gradient norm: 0.694101
=== Actor Training Debug (Iteration 8271) ===
Q mean: -13.898350
Q std: 20.721291
Actor loss: 13.902323
Action reg: 0.003973
  l1.weight: grad_norm = 0.261343
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.179470
Total gradient norm: 0.491805
=== Actor Training Debug (Iteration 8272) ===
Q mean: -15.420875
Q std: 20.777740
Actor loss: 15.424860
Action reg: 0.003986
  l1.weight: grad_norm = 0.189921
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.145936
Total gradient norm: 0.404650
=== Actor Training Debug (Iteration 8273) ===
Q mean: -15.765070
Q std: 21.864614
Actor loss: 15.769038
Action reg: 0.003968
  l1.weight: grad_norm = 0.156265
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.125791
Total gradient norm: 0.346600
=== Actor Training Debug (Iteration 8274) ===
Q mean: -15.053677
Q std: 21.296392
Actor loss: 15.057610
Action reg: 0.003933
  l1.weight: grad_norm = 0.251019
  l1.bias: grad_norm = 0.000567
  l2.weight: grad_norm = 0.176544
Total gradient norm: 0.492524
=== Actor Training Debug (Iteration 8275) ===
Q mean: -14.392729
Q std: 21.498024
Actor loss: 14.396683
Action reg: 0.003954
  l1.weight: grad_norm = 0.174826
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.149189
Total gradient norm: 0.458172
=== Actor Training Debug (Iteration 8276) ===
Q mean: -17.620008
Q std: 23.860737
Actor loss: 17.623980
Action reg: 0.003971
  l1.weight: grad_norm = 0.447030
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.281729
Total gradient norm: 0.875567
=== Actor Training Debug (Iteration 8277) ===
Q mean: -13.758579
Q std: 20.821461
Actor loss: 13.762529
Action reg: 0.003950
  l1.weight: grad_norm = 0.374847
  l1.bias: grad_norm = 0.001555
  l2.weight: grad_norm = 0.305682
Total gradient norm: 0.887468
=== Actor Training Debug (Iteration 8278) ===
Q mean: -15.119452
Q std: 22.299116
Actor loss: 15.123410
Action reg: 0.003959
  l1.weight: grad_norm = 0.258972
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.177292
Total gradient norm: 0.468356
=== Actor Training Debug (Iteration 8279) ===
Q mean: -16.566826
Q std: 21.439003
Actor loss: 16.570793
Action reg: 0.003967
  l1.weight: grad_norm = 0.220550
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.166802
Total gradient norm: 0.420845
=== Actor Training Debug (Iteration 8280) ===
Q mean: -15.520510
Q std: 21.742203
Actor loss: 15.524460
Action reg: 0.003951
  l1.weight: grad_norm = 0.508479
  l1.bias: grad_norm = 0.000795
  l2.weight: grad_norm = 0.335183
Total gradient norm: 0.940005
=== Actor Training Debug (Iteration 8281) ===
Q mean: -12.268167
Q std: 19.476606
Actor loss: 12.272120
Action reg: 0.003953
  l1.weight: grad_norm = 0.181167
  l1.bias: grad_norm = 0.000780
  l2.weight: grad_norm = 0.140504
Total gradient norm: 0.361697
=== Actor Training Debug (Iteration 8282) ===
Q mean: -14.496693
Q std: 21.266775
Actor loss: 14.500650
Action reg: 0.003958
  l1.weight: grad_norm = 0.271789
  l1.bias: grad_norm = 0.003565
  l2.weight: grad_norm = 0.205471
Total gradient norm: 0.633289
=== Actor Training Debug (Iteration 8283) ===
Q mean: -14.950005
Q std: 22.071997
Actor loss: 14.953976
Action reg: 0.003971
  l1.weight: grad_norm = 0.194477
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.138479
Total gradient norm: 0.440358
=== Actor Training Debug (Iteration 8284) ===
Q mean: -17.045954
Q std: 23.580708
Actor loss: 17.049892
Action reg: 0.003939
  l1.weight: grad_norm = 0.272132
  l1.bias: grad_norm = 0.001526
  l2.weight: grad_norm = 0.215647
Total gradient norm: 0.565950
=== Actor Training Debug (Iteration 8285) ===
Q mean: -16.040249
Q std: 22.220905
Actor loss: 16.044212
Action reg: 0.003963
  l1.weight: grad_norm = 0.323267
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.209281
Total gradient norm: 0.536862
=== Actor Training Debug (Iteration 8286) ===
Q mean: -14.879933
Q std: 22.000280
Actor loss: 14.883898
Action reg: 0.003965
  l1.weight: grad_norm = 0.302207
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.203969
Total gradient norm: 0.621065
=== Actor Training Debug (Iteration 8287) ===
Q mean: -15.087924
Q std: 21.705723
Actor loss: 15.091890
Action reg: 0.003966
  l1.weight: grad_norm = 0.174267
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.143814
Total gradient norm: 0.385271
=== Actor Training Debug (Iteration 8288) ===
Q mean: -12.428026
Q std: 19.994944
Actor loss: 12.431965
Action reg: 0.003939
  l1.weight: grad_norm = 0.261834
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.172853
Total gradient norm: 0.449124
=== Actor Training Debug (Iteration 8289) ===
Q mean: -14.177455
Q std: 20.655136
Actor loss: 14.181410
Action reg: 0.003955
  l1.weight: grad_norm = 0.230160
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.156127
Total gradient norm: 0.417869
=== Actor Training Debug (Iteration 8290) ===
Q mean: -13.974680
Q std: 21.103878
Actor loss: 13.978648
Action reg: 0.003968
  l1.weight: grad_norm = 0.150225
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.116448
Total gradient norm: 0.415432
=== Actor Training Debug (Iteration 8291) ===
Q mean: -15.417118
Q std: 21.053236
Actor loss: 15.421082
Action reg: 0.003963
  l1.weight: grad_norm = 0.204856
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.154320
Total gradient norm: 0.391413
=== Actor Training Debug (Iteration 8292) ===
Q mean: -14.358170
Q std: 21.324841
Actor loss: 14.362119
Action reg: 0.003949
  l1.weight: grad_norm = 0.202655
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.142295
Total gradient norm: 0.400203
=== Actor Training Debug (Iteration 8293) ===
Q mean: -13.368116
Q std: 18.845776
Actor loss: 13.372079
Action reg: 0.003962
  l1.weight: grad_norm = 0.355686
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.240831
Total gradient norm: 0.701214
=== Actor Training Debug (Iteration 8294) ===
Q mean: -13.521133
Q std: 21.795883
Actor loss: 13.525094
Action reg: 0.003960
  l1.weight: grad_norm = 0.414208
  l1.bias: grad_norm = 0.001442
  l2.weight: grad_norm = 0.281786
Total gradient norm: 0.762108
=== Actor Training Debug (Iteration 8295) ===
Q mean: -14.444334
Q std: 21.310226
Actor loss: 14.448303
Action reg: 0.003970
  l1.weight: grad_norm = 0.264669
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.187856
Total gradient norm: 0.630005
=== Actor Training Debug (Iteration 8296) ===
Q mean: -13.354222
Q std: 19.703255
Actor loss: 13.358185
Action reg: 0.003962
  l1.weight: grad_norm = 0.175688
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.111495
Total gradient norm: 0.329952
=== Actor Training Debug (Iteration 8297) ===
Q mean: -14.990005
Q std: 21.182280
Actor loss: 14.993962
Action reg: 0.003957
  l1.weight: grad_norm = 0.154816
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.109248
Total gradient norm: 0.291360
=== Actor Training Debug (Iteration 8298) ===
Q mean: -14.672441
Q std: 21.711740
Actor loss: 14.676394
Action reg: 0.003954
  l1.weight: grad_norm = 0.216883
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.152077
Total gradient norm: 0.426099
=== Actor Training Debug (Iteration 8299) ===
Q mean: -14.110559
Q std: 21.283806
Actor loss: 14.114529
Action reg: 0.003969
  l1.weight: grad_norm = 0.236376
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.199881
Total gradient norm: 0.533195
=== Actor Training Debug (Iteration 8300) ===
Q mean: -14.130135
Q std: 19.971752
Actor loss: 14.134104
Action reg: 0.003969
  l1.weight: grad_norm = 0.415299
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.320133
Total gradient norm: 0.784768
=== Actor Training Debug (Iteration 8301) ===
Q mean: -15.519917
Q std: 21.408199
Actor loss: 15.523885
Action reg: 0.003967
  l1.weight: grad_norm = 0.168144
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.138758
Total gradient norm: 0.365787
=== Actor Training Debug (Iteration 8302) ===
Q mean: -15.122663
Q std: 21.221924
Actor loss: 15.126614
Action reg: 0.003950
  l1.weight: grad_norm = 0.231533
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.167488
Total gradient norm: 0.460922
=== Actor Training Debug (Iteration 8303) ===
Q mean: -15.562020
Q std: 21.145239
Actor loss: 15.565991
Action reg: 0.003971
  l1.weight: grad_norm = 0.291215
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.189603
Total gradient norm: 0.534377
=== Actor Training Debug (Iteration 8304) ===
Q mean: -11.919737
Q std: 20.169304
Actor loss: 11.923686
Action reg: 0.003949
  l1.weight: grad_norm = 0.213110
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.173625
Total gradient norm: 0.475989
=== Actor Training Debug (Iteration 8305) ===
Q mean: -15.757335
Q std: 21.665003
Actor loss: 15.761293
Action reg: 0.003959
  l1.weight: grad_norm = 0.232619
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.171214
Total gradient norm: 0.451392
=== Actor Training Debug (Iteration 8306) ===
Q mean: -14.538567
Q std: 22.293421
Actor loss: 14.542484
Action reg: 0.003918
  l1.weight: grad_norm = 0.285337
  l1.bias: grad_norm = 0.002463
  l2.weight: grad_norm = 0.214853
Total gradient norm: 0.569481
=== Actor Training Debug (Iteration 8307) ===
Q mean: -14.227221
Q std: 19.856178
Actor loss: 14.231185
Action reg: 0.003964
  l1.weight: grad_norm = 0.227453
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.208673
Total gradient norm: 0.619316
=== Actor Training Debug (Iteration 8308) ===
Q mean: -14.785536
Q std: 20.917072
Actor loss: 14.789484
Action reg: 0.003949
  l1.weight: grad_norm = 0.268498
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.190962
Total gradient norm: 0.527906
=== Actor Training Debug (Iteration 8309) ===
Q mean: -15.690886
Q std: 22.993948
Actor loss: 15.694861
Action reg: 0.003976
  l1.weight: grad_norm = 0.244782
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.182277
Total gradient norm: 0.544554
=== Actor Training Debug (Iteration 8310) ===
Q mean: -13.878244
Q std: 20.446692
Actor loss: 13.882215
Action reg: 0.003971
  l1.weight: grad_norm = 0.093797
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.077075
Total gradient norm: 0.212786
=== Actor Training Debug (Iteration 8311) ===
Q mean: -13.366678
Q std: 20.834177
Actor loss: 13.370639
Action reg: 0.003961
  l1.weight: grad_norm = 0.191516
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.148704
Total gradient norm: 0.422229
=== Actor Training Debug (Iteration 8312) ===
Q mean: -15.162704
Q std: 21.085857
Actor loss: 15.166676
Action reg: 0.003972
  l1.weight: grad_norm = 0.313550
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.223512
Total gradient norm: 0.650202
=== Actor Training Debug (Iteration 8313) ===
Q mean: -13.218575
Q std: 19.377508
Actor loss: 13.222551
Action reg: 0.003977
  l1.weight: grad_norm = 0.285429
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.225017
Total gradient norm: 0.564514
=== Actor Training Debug (Iteration 8314) ===
Q mean: -15.590764
Q std: 22.175234
Actor loss: 15.594726
Action reg: 0.003961
  l1.weight: grad_norm = 0.338110
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.253422
Total gradient norm: 0.643382
=== Actor Training Debug (Iteration 8315) ===
Q mean: -13.802814
Q std: 20.882048
Actor loss: 13.806754
Action reg: 0.003940
  l1.weight: grad_norm = 0.324233
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.239027
Total gradient norm: 0.698719
=== Actor Training Debug (Iteration 8316) ===
Q mean: -15.188909
Q std: 20.894567
Actor loss: 15.192854
Action reg: 0.003946
  l1.weight: grad_norm = 0.237455
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.174956
Total gradient norm: 0.469238
=== Actor Training Debug (Iteration 8317) ===
Q mean: -15.123291
Q std: 22.752140
Actor loss: 15.127247
Action reg: 0.003956
  l1.weight: grad_norm = 0.255424
  l1.bias: grad_norm = 0.002171
  l2.weight: grad_norm = 0.166675
Total gradient norm: 0.455273
=== Actor Training Debug (Iteration 8318) ===
Q mean: -13.666857
Q std: 20.009161
Actor loss: 13.670796
Action reg: 0.003940
  l1.weight: grad_norm = 0.121686
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.107542
Total gradient norm: 0.302319
=== Actor Training Debug (Iteration 8319) ===
Q mean: -15.854863
Q std: 22.674334
Actor loss: 15.858809
Action reg: 0.003946
  l1.weight: grad_norm = 0.231865
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.173936
Total gradient norm: 0.504545
=== Actor Training Debug (Iteration 8320) ===
Q mean: -13.745125
Q std: 19.023949
Actor loss: 13.749082
Action reg: 0.003957
  l1.weight: grad_norm = 0.204768
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.156782
Total gradient norm: 0.428921
=== Actor Training Debug (Iteration 8321) ===
Q mean: -13.317894
Q std: 20.360409
Actor loss: 13.321851
Action reg: 0.003957
  l1.weight: grad_norm = 0.296274
  l1.bias: grad_norm = 0.001953
  l2.weight: grad_norm = 0.222664
Total gradient norm: 0.653030
=== Actor Training Debug (Iteration 8322) ===
Q mean: -13.902473
Q std: 19.783314
Actor loss: 13.906414
Action reg: 0.003941
  l1.weight: grad_norm = 0.166313
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.128679
Total gradient norm: 0.349012
=== Actor Training Debug (Iteration 8323) ===
Q mean: -14.429446
Q std: 21.997627
Actor loss: 14.433383
Action reg: 0.003937
  l1.weight: grad_norm = 0.196481
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.178891
Total gradient norm: 0.519648
=== Actor Training Debug (Iteration 8324) ===
Q mean: -14.763341
Q std: 21.207821
Actor loss: 14.767298
Action reg: 0.003957
  l1.weight: grad_norm = 0.086237
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.073902
Total gradient norm: 0.217887
=== Actor Training Debug (Iteration 8325) ===
Q mean: -12.618409
Q std: 20.126827
Actor loss: 12.622361
Action reg: 0.003952
  l1.weight: grad_norm = 0.160787
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.129951
Total gradient norm: 0.373045
=== Actor Training Debug (Iteration 8326) ===
Q mean: -15.746346
Q std: 22.008133
Actor loss: 15.750286
Action reg: 0.003941
  l1.weight: grad_norm = 0.293752
  l1.bias: grad_norm = 0.000738
  l2.weight: grad_norm = 0.200065
Total gradient norm: 0.507977
=== Actor Training Debug (Iteration 8327) ===
Q mean: -15.961992
Q std: 22.214823
Actor loss: 15.965945
Action reg: 0.003953
  l1.weight: grad_norm = 0.176825
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.137142
Total gradient norm: 0.352363
=== Actor Training Debug (Iteration 8328) ===
Q mean: -14.804993
Q std: 19.264496
Actor loss: 14.808965
Action reg: 0.003972
  l1.weight: grad_norm = 0.406786
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.243022
Total gradient norm: 0.702374
=== Actor Training Debug (Iteration 8329) ===
Q mean: -13.493898
Q std: 20.252802
Actor loss: 13.497853
Action reg: 0.003955
  l1.weight: grad_norm = 0.476859
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.324029
Total gradient norm: 0.875432
=== Actor Training Debug (Iteration 8330) ===
Q mean: -13.321919
Q std: 18.446537
Actor loss: 13.325879
Action reg: 0.003960
  l1.weight: grad_norm = 0.302724
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.221381
Total gradient norm: 0.561945
=== Actor Training Debug (Iteration 8331) ===
Q mean: -13.887958
Q std: 20.155363
Actor loss: 13.891894
Action reg: 0.003937
  l1.weight: grad_norm = 0.173854
  l1.bias: grad_norm = 0.000617
  l2.weight: grad_norm = 0.132008
Total gradient norm: 0.396351
=== Actor Training Debug (Iteration 8332) ===
Q mean: -14.019505
Q std: 21.441904
Actor loss: 14.023452
Action reg: 0.003948
  l1.weight: grad_norm = 0.521844
  l1.bias: grad_norm = 0.000535
  l2.weight: grad_norm = 0.424251
Total gradient norm: 1.178418
=== Actor Training Debug (Iteration 8333) ===
Q mean: -13.687072
Q std: 20.854008
Actor loss: 13.691028
Action reg: 0.003955
  l1.weight: grad_norm = 0.233743
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.184864
Total gradient norm: 0.505739
=== Actor Training Debug (Iteration 8334) ===
Q mean: -12.822392
Q std: 20.459641
Actor loss: 12.826339
Action reg: 0.003947
  l1.weight: grad_norm = 0.222689
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.161636
Total gradient norm: 0.394629
=== Actor Training Debug (Iteration 8335) ===
Q mean: -13.761765
Q std: 20.883530
Actor loss: 13.765719
Action reg: 0.003955
  l1.weight: grad_norm = 0.183989
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.150129
Total gradient norm: 0.438466
=== Actor Training Debug (Iteration 8336) ===
Q mean: -17.682789
Q std: 23.048559
Actor loss: 17.686760
Action reg: 0.003970
  l1.weight: grad_norm = 0.175561
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.145229
Total gradient norm: 0.411347
=== Actor Training Debug (Iteration 8337) ===
Q mean: -15.538589
Q std: 22.934685
Actor loss: 15.542555
Action reg: 0.003965
  l1.weight: grad_norm = 0.275857
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.194610
Total gradient norm: 0.607650
=== Actor Training Debug (Iteration 8338) ===
Q mean: -14.674481
Q std: 20.980593
Actor loss: 14.678464
Action reg: 0.003983
  l1.weight: grad_norm = 0.287715
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.179407
Total gradient norm: 0.518035
=== Actor Training Debug (Iteration 8339) ===
Q mean: -16.128120
Q std: 21.754662
Actor loss: 16.132078
Action reg: 0.003957
  l1.weight: grad_norm = 0.253378
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.181143
Total gradient norm: 0.486754
=== Actor Training Debug (Iteration 8340) ===
Q mean: -17.398529
Q std: 22.737465
Actor loss: 17.402506
Action reg: 0.003976
  l1.weight: grad_norm = 0.261318
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.186757
Total gradient norm: 0.500911
=== Actor Training Debug (Iteration 8341) ===
Q mean: -15.870655
Q std: 22.251930
Actor loss: 15.874617
Action reg: 0.003962
  l1.weight: grad_norm = 0.391842
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.270989
Total gradient norm: 0.648942
=== Actor Training Debug (Iteration 8342) ===
Q mean: -16.865269
Q std: 22.903053
Actor loss: 16.869234
Action reg: 0.003965
  l1.weight: grad_norm = 0.168649
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.129204
Total gradient norm: 0.329329
=== Actor Training Debug (Iteration 8343) ===
Q mean: -14.016766
Q std: 19.340899
Actor loss: 14.020728
Action reg: 0.003963
  l1.weight: grad_norm = 0.267476
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.206227
Total gradient norm: 0.534575
=== Actor Training Debug (Iteration 8344) ===
Q mean: -16.690231
Q std: 23.296194
Actor loss: 16.694170
Action reg: 0.003939
  l1.weight: grad_norm = 0.382998
  l1.bias: grad_norm = 0.000826
  l2.weight: grad_norm = 0.286300
Total gradient norm: 0.864183
=== Actor Training Debug (Iteration 8345) ===
Q mean: -13.490307
Q std: 19.587677
Actor loss: 13.494274
Action reg: 0.003967
  l1.weight: grad_norm = 0.180758
  l1.bias: grad_norm = 0.002533
  l2.weight: grad_norm = 0.146979
Total gradient norm: 0.397126
=== Actor Training Debug (Iteration 8346) ===
Q mean: -16.190041
Q std: 23.111124
Actor loss: 16.193979
Action reg: 0.003938
  l1.weight: grad_norm = 0.263782
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.187161
Total gradient norm: 0.476843
=== Actor Training Debug (Iteration 8347) ===
Q mean: -15.958655
Q std: 22.103378
Actor loss: 15.962630
Action reg: 0.003975
  l1.weight: grad_norm = 0.307282
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.208276
Total gradient norm: 0.539322
=== Actor Training Debug (Iteration 8348) ===
Q mean: -14.573872
Q std: 21.485397
Actor loss: 14.577822
Action reg: 0.003950
  l1.weight: grad_norm = 0.218745
  l1.bias: grad_norm = 0.000615
  l2.weight: grad_norm = 0.185721
Total gradient norm: 0.440649
=== Actor Training Debug (Iteration 8349) ===
Q mean: -13.872189
Q std: 20.221361
Actor loss: 13.876135
Action reg: 0.003947
  l1.weight: grad_norm = 0.265437
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.201439
Total gradient norm: 0.529423
=== Actor Training Debug (Iteration 8350) ===
Q mean: -16.561890
Q std: 22.681376
Actor loss: 16.565834
Action reg: 0.003945
  l1.weight: grad_norm = 0.641061
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.367600
Total gradient norm: 0.983170
=== Actor Training Debug (Iteration 8351) ===
Q mean: -16.248653
Q std: 21.542267
Actor loss: 16.252630
Action reg: 0.003978
  l1.weight: grad_norm = 0.230791
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.162242
Total gradient norm: 0.413621
=== Actor Training Debug (Iteration 8352) ===
Q mean: -12.491613
Q std: 20.359900
Actor loss: 12.495526
Action reg: 0.003913
  l1.weight: grad_norm = 0.234750
  l1.bias: grad_norm = 0.002511
  l2.weight: grad_norm = 0.155471
Total gradient norm: 0.406071
=== Actor Training Debug (Iteration 8353) ===
Q mean: -12.345565
Q std: 18.522327
Actor loss: 12.349524
Action reg: 0.003959
  l1.weight: grad_norm = 0.351903
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.275564
Total gradient norm: 0.817803
=== Actor Training Debug (Iteration 8354) ===
Q mean: -14.883881
Q std: 21.300356
Actor loss: 14.887806
Action reg: 0.003925
  l1.weight: grad_norm = 0.255022
  l1.bias: grad_norm = 0.003057
  l2.weight: grad_norm = 0.180913
Total gradient norm: 0.544738
=== Actor Training Debug (Iteration 8355) ===
Q mean: -14.699236
Q std: 21.691301
Actor loss: 14.703177
Action reg: 0.003941
  l1.weight: grad_norm = 0.605905
  l1.bias: grad_norm = 0.004506
  l2.weight: grad_norm = 0.403479
Total gradient norm: 1.450979
=== Actor Training Debug (Iteration 8356) ===
Q mean: -14.516130
Q std: 20.035517
Actor loss: 14.520103
Action reg: 0.003972
  l1.weight: grad_norm = 0.317224
  l1.bias: grad_norm = 0.002565
  l2.weight: grad_norm = 0.237060
Total gradient norm: 0.713462
=== Actor Training Debug (Iteration 8357) ===
Q mean: -13.862761
Q std: 20.416147
Actor loss: 13.866708
Action reg: 0.003948
  l1.weight: grad_norm = 0.148699
  l1.bias: grad_norm = 0.002730
  l2.weight: grad_norm = 0.136659
Total gradient norm: 0.348847
=== Actor Training Debug (Iteration 8358) ===
Q mean: -15.728021
Q std: 21.342274
Actor loss: 15.731993
Action reg: 0.003972
  l1.weight: grad_norm = 0.165059
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.108745
Total gradient norm: 0.300489
=== Actor Training Debug (Iteration 8359) ===
Q mean: -13.950747
Q std: 21.736782
Actor loss: 13.954690
Action reg: 0.003943
  l1.weight: grad_norm = 0.194351
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.149479
Total gradient norm: 0.397256
=== Actor Training Debug (Iteration 8360) ===
Q mean: -11.838381
Q std: 19.349550
Actor loss: 11.842323
Action reg: 0.003942
  l1.weight: grad_norm = 0.640977
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.478618
Total gradient norm: 1.216251
=== Actor Training Debug (Iteration 8361) ===
Q mean: -13.527696
Q std: 19.474056
Actor loss: 13.531646
Action reg: 0.003950
  l1.weight: grad_norm = 0.471101
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.306388
Total gradient norm: 0.761701
=== Actor Training Debug (Iteration 8362) ===
Q mean: -15.361561
Q std: 21.460272
Actor loss: 15.365532
Action reg: 0.003972
  l1.weight: grad_norm = 0.195360
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.134048
Total gradient norm: 0.343053
=== Actor Training Debug (Iteration 8363) ===
Q mean: -13.962799
Q std: 20.680199
Actor loss: 13.966735
Action reg: 0.003936
  l1.weight: grad_norm = 0.266390
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.182946
Total gradient norm: 0.513570
=== Actor Training Debug (Iteration 8364) ===
Q mean: -15.003960
Q std: 20.769352
Actor loss: 15.007929
Action reg: 0.003969
  l1.weight: grad_norm = 0.137979
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.125028
Total gradient norm: 0.436010
=== Actor Training Debug (Iteration 8365) ===
Q mean: -15.306316
Q std: 20.007093
Actor loss: 15.310270
Action reg: 0.003954
  l1.weight: grad_norm = 0.290053
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.209610
Total gradient norm: 0.538248
=== Actor Training Debug (Iteration 8366) ===
Q mean: -17.965988
Q std: 24.011650
Actor loss: 17.969923
Action reg: 0.003935
  l1.weight: grad_norm = 0.235413
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.178278
Total gradient norm: 0.473802
=== Actor Training Debug (Iteration 8367) ===
Q mean: -17.012081
Q std: 21.966612
Actor loss: 17.016039
Action reg: 0.003958
  l1.weight: grad_norm = 0.144943
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.120888
Total gradient norm: 0.332287
=== Actor Training Debug (Iteration 8368) ===
Q mean: -12.557955
Q std: 18.043447
Actor loss: 12.561916
Action reg: 0.003961
  l1.weight: grad_norm = 0.180127
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.119010
Total gradient norm: 0.311211
=== Actor Training Debug (Iteration 8369) ===
Q mean: -14.297804
Q std: 20.391474
Actor loss: 14.301760
Action reg: 0.003955
  l1.weight: grad_norm = 0.166767
  l1.bias: grad_norm = 0.000717
  l2.weight: grad_norm = 0.127648
Total gradient norm: 0.312814
=== Actor Training Debug (Iteration 8370) ===
Q mean: -15.634111
Q std: 20.594049
Actor loss: 15.638098
Action reg: 0.003986
  l1.weight: grad_norm = 0.545400
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.361881
Total gradient norm: 0.932649
=== Actor Training Debug (Iteration 8371) ===
Q mean: -18.066084
Q std: 22.351868
Actor loss: 18.070047
Action reg: 0.003964
  l1.weight: grad_norm = 0.229276
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.171886
Total gradient norm: 0.415178
=== Actor Training Debug (Iteration 8372) ===
Q mean: -15.214274
Q std: 22.104057
Actor loss: 15.218230
Action reg: 0.003956
  l1.weight: grad_norm = 0.160216
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.108376
Total gradient norm: 0.288316
=== Actor Training Debug (Iteration 8373) ===
Q mean: -14.834898
Q std: 21.264938
Actor loss: 14.838849
Action reg: 0.003951
  l1.weight: grad_norm = 0.228577
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.154983
Total gradient norm: 0.409924
=== Actor Training Debug (Iteration 8374) ===
Q mean: -12.013334
Q std: 17.920479
Actor loss: 12.017289
Action reg: 0.003955
  l1.weight: grad_norm = 0.310548
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.241399
Total gradient norm: 0.650978
=== Actor Training Debug (Iteration 8375) ===
Q mean: -15.165896
Q std: 20.398844
Actor loss: 15.169851
Action reg: 0.003955
  l1.weight: grad_norm = 0.405778
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.279679
Total gradient norm: 0.784946
=== Actor Training Debug (Iteration 8376) ===
Q mean: -15.840189
Q std: 21.937443
Actor loss: 15.844140
Action reg: 0.003951
  l1.weight: grad_norm = 0.322953
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.263924
Total gradient norm: 0.819658
=== Actor Training Debug (Iteration 8377) ===
Q mean: -13.579656
Q std: 21.208427
Actor loss: 13.583621
Action reg: 0.003965
  l1.weight: grad_norm = 0.270606
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.184779
Total gradient norm: 0.471222
=== Actor Training Debug (Iteration 8378) ===
Q mean: -12.959930
Q std: 19.512358
Actor loss: 12.963893
Action reg: 0.003962
  l1.weight: grad_norm = 0.133941
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.094476
Total gradient norm: 0.249419
=== Actor Training Debug (Iteration 8379) ===
Q mean: -14.303844
Q std: 20.478521
Actor loss: 14.307812
Action reg: 0.003968
  l1.weight: grad_norm = 0.225397
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.176819
Total gradient norm: 0.451323
=== Actor Training Debug (Iteration 8380) ===
Q mean: -13.342577
Q std: 20.582794
Actor loss: 13.346545
Action reg: 0.003969
  l1.weight: grad_norm = 0.282010
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.195950
Total gradient norm: 0.578527
=== Actor Training Debug (Iteration 8381) ===
Q mean: -14.448996
Q std: 20.534164
Actor loss: 14.452957
Action reg: 0.003961
  l1.weight: grad_norm = 0.151311
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.111593
Total gradient norm: 0.304089
=== Actor Training Debug (Iteration 8382) ===
Q mean: -12.533780
Q std: 19.274145
Actor loss: 12.537744
Action reg: 0.003964
  l1.weight: grad_norm = 0.159933
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.109339
Total gradient norm: 0.286530
=== Actor Training Debug (Iteration 8383) ===
Q mean: -15.212416
Q std: 22.800737
Actor loss: 15.216377
Action reg: 0.003962
  l1.weight: grad_norm = 0.242577
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.188563
Total gradient norm: 0.528745
=== Actor Training Debug (Iteration 8384) ===
Q mean: -15.169237
Q std: 21.368689
Actor loss: 15.173198
Action reg: 0.003961
  l1.weight: grad_norm = 0.249119
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.194026
Total gradient norm: 0.487123
=== Actor Training Debug (Iteration 8385) ===
Q mean: -13.093053
Q std: 20.194836
Actor loss: 13.097013
Action reg: 0.003960
  l1.weight: grad_norm = 0.397257
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.296912
Total gradient norm: 0.796459
=== Actor Training Debug (Iteration 8386) ===
Q mean: -14.586833
Q std: 22.239943
Actor loss: 14.590793
Action reg: 0.003959
  l1.weight: grad_norm = 0.259138
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.191535
Total gradient norm: 0.605577
=== Actor Training Debug (Iteration 8387) ===
Q mean: -13.211871
Q std: 19.084072
Actor loss: 13.215821
Action reg: 0.003951
  l1.weight: grad_norm = 0.269393
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.189731
Total gradient norm: 0.528351
=== Actor Training Debug (Iteration 8388) ===
Q mean: -12.854593
Q std: 19.252792
Actor loss: 12.858564
Action reg: 0.003971
  l1.weight: grad_norm = 0.163935
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.118955
Total gradient norm: 0.332085
=== Actor Training Debug (Iteration 8389) ===
Q mean: -15.228430
Q std: 19.405451
Actor loss: 15.232413
Action reg: 0.003983
  l1.weight: grad_norm = 0.175758
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.123292
Total gradient norm: 0.323114
=== Actor Training Debug (Iteration 8390) ===
Q mean: -14.691338
Q std: 22.012505
Actor loss: 14.695271
Action reg: 0.003934
  l1.weight: grad_norm = 0.252905
  l1.bias: grad_norm = 0.000815
  l2.weight: grad_norm = 0.181443
Total gradient norm: 0.531117
=== Actor Training Debug (Iteration 8391) ===
Q mean: -13.313499
Q std: 20.676481
Actor loss: 13.317465
Action reg: 0.003965
  l1.weight: grad_norm = 0.244627
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.200754
Total gradient norm: 0.551705
=== Actor Training Debug (Iteration 8392) ===
Q mean: -14.033794
Q std: 20.071785
Actor loss: 14.037765
Action reg: 0.003970
  l1.weight: grad_norm = 0.249313
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.201807
Total gradient norm: 0.512970
=== Actor Training Debug (Iteration 8393) ===
Q mean: -15.633373
Q std: 21.861341
Actor loss: 15.637345
Action reg: 0.003973
  l1.weight: grad_norm = 0.218370
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.154172
Total gradient norm: 0.421989
=== Actor Training Debug (Iteration 8394) ===
Q mean: -15.163759
Q std: 22.072546
Actor loss: 15.167724
Action reg: 0.003965
  l1.weight: grad_norm = 0.136950
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.096215
Total gradient norm: 0.261059
=== Actor Training Debug (Iteration 8395) ===
Q mean: -11.635859
Q std: 18.947109
Actor loss: 11.639799
Action reg: 0.003940
  l1.weight: grad_norm = 0.403691
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.280997
Total gradient norm: 0.763860
=== Actor Training Debug (Iteration 8396) ===
Q mean: -13.899019
Q std: 21.578630
Actor loss: 13.902958
Action reg: 0.003939
  l1.weight: grad_norm = 0.274165
  l1.bias: grad_norm = 0.000889
  l2.weight: grad_norm = 0.176124
Total gradient norm: 0.468883
=== Actor Training Debug (Iteration 8397) ===
Q mean: -15.749306
Q std: 22.101656
Actor loss: 15.753273
Action reg: 0.003967
  l1.weight: grad_norm = 0.229290
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.157455
Total gradient norm: 0.403519
=== Actor Training Debug (Iteration 8398) ===
Q mean: -14.588110
Q std: 20.201155
Actor loss: 14.592085
Action reg: 0.003975
  l1.weight: grad_norm = 0.154829
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.123767
Total gradient norm: 0.340694
=== Actor Training Debug (Iteration 8399) ===
Q mean: -12.423272
Q std: 18.404320
Actor loss: 12.427214
Action reg: 0.003941
  l1.weight: grad_norm = 0.306263
  l1.bias: grad_norm = 0.002241
  l2.weight: grad_norm = 0.207210
Total gradient norm: 0.530010
=== Actor Training Debug (Iteration 8400) ===
Q mean: -14.785660
Q std: 23.098362
Actor loss: 14.789593
Action reg: 0.003933
  l1.weight: grad_norm = 0.360734
  l1.bias: grad_norm = 0.003584
  l2.weight: grad_norm = 0.224998
Total gradient norm: 0.628069
=== Actor Training Debug (Iteration 8401) ===
Q mean: -14.189837
Q std: 21.983807
Actor loss: 14.193789
Action reg: 0.003953
  l1.weight: grad_norm = 0.273702
  l1.bias: grad_norm = 0.001945
  l2.weight: grad_norm = 0.229418
Total gradient norm: 0.579303
=== Actor Training Debug (Iteration 8402) ===
Q mean: -14.415642
Q std: 20.558117
Actor loss: 14.419615
Action reg: 0.003973
  l1.weight: grad_norm = 0.153927
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.121729
Total gradient norm: 0.303489
=== Actor Training Debug (Iteration 8403) ===
Q mean: -15.249588
Q std: 21.220984
Actor loss: 15.253556
Action reg: 0.003968
  l1.weight: grad_norm = 0.465180
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.348479
Total gradient norm: 0.877163
=== Actor Training Debug (Iteration 8404) ===
Q mean: -14.322506
Q std: 20.342628
Actor loss: 14.326475
Action reg: 0.003969
  l1.weight: grad_norm = 0.220081
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.134867
Total gradient norm: 0.362079
=== Actor Training Debug (Iteration 8405) ===
Q mean: -15.018815
Q std: 21.463078
Actor loss: 15.022791
Action reg: 0.003976
  l1.weight: grad_norm = 0.171484
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.120864
Total gradient norm: 0.330452
=== Actor Training Debug (Iteration 8406) ===
Q mean: -13.416242
Q std: 20.109035
Actor loss: 13.420215
Action reg: 0.003973
  l1.weight: grad_norm = 0.728476
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.557277
Total gradient norm: 1.985316
=== Actor Training Debug (Iteration 8407) ===
Q mean: -14.864623
Q std: 19.875017
Actor loss: 14.868590
Action reg: 0.003967
  l1.weight: grad_norm = 0.124477
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.099599
Total gradient norm: 0.286038
=== Actor Training Debug (Iteration 8408) ===
Q mean: -13.290516
Q std: 20.285038
Actor loss: 13.294491
Action reg: 0.003975
  l1.weight: grad_norm = 0.452489
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.276176
Total gradient norm: 0.820331
=== Actor Training Debug (Iteration 8409) ===
Q mean: -14.977069
Q std: 21.553907
Actor loss: 14.981025
Action reg: 0.003956
  l1.weight: grad_norm = 0.161319
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.117097
Total gradient norm: 0.326320
=== Actor Training Debug (Iteration 8410) ===
Q mean: -15.696744
Q std: 22.620493
Actor loss: 15.700680
Action reg: 0.003936
  l1.weight: grad_norm = 0.121838
  l1.bias: grad_norm = 0.000728
  l2.weight: grad_norm = 0.088923
Total gradient norm: 0.254901
=== Actor Training Debug (Iteration 8411) ===
Q mean: -14.366709
Q std: 20.719824
Actor loss: 14.370653
Action reg: 0.003945
  l1.weight: grad_norm = 0.357875
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.238565
Total gradient norm: 0.608670
=== Actor Training Debug (Iteration 8412) ===
Q mean: -14.669037
Q std: 20.855255
Actor loss: 14.672996
Action reg: 0.003959
  l1.weight: grad_norm = 0.240300
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.204866
Total gradient norm: 0.607438
=== Actor Training Debug (Iteration 8413) ===
Q mean: -14.808160
Q std: 21.248964
Actor loss: 14.812129
Action reg: 0.003969
  l1.weight: grad_norm = 0.275747
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.193569
Total gradient norm: 0.518239
=== Actor Training Debug (Iteration 8414) ===
Q mean: -13.727495
Q std: 20.068371
Actor loss: 13.731474
Action reg: 0.003979
  l1.weight: grad_norm = 0.165513
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.119517
Total gradient norm: 0.301733
=== Actor Training Debug (Iteration 8415) ===
Q mean: -16.801208
Q std: 22.985373
Actor loss: 16.805176
Action reg: 0.003967
  l1.weight: grad_norm = 0.365744
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.226255
Total gradient norm: 0.599909
=== Actor Training Debug (Iteration 8416) ===
Q mean: -13.992368
Q std: 19.729002
Actor loss: 13.996317
Action reg: 0.003949
  l1.weight: grad_norm = 0.284367
  l1.bias: grad_norm = 0.000596
  l2.weight: grad_norm = 0.205480
Total gradient norm: 0.680017
=== Actor Training Debug (Iteration 8417) ===
Q mean: -12.854488
Q std: 19.025139
Actor loss: 12.858431
Action reg: 0.003943
  l1.weight: grad_norm = 0.307045
  l1.bias: grad_norm = 0.005004
  l2.weight: grad_norm = 0.233507
Total gradient norm: 0.694346
=== Actor Training Debug (Iteration 8418) ===
Q mean: -15.406366
Q std: 20.803413
Actor loss: 15.410335
Action reg: 0.003968
  l1.weight: grad_norm = 0.230511
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.193754
Total gradient norm: 0.512706
=== Actor Training Debug (Iteration 8419) ===
Q mean: -14.433006
Q std: 21.258230
Actor loss: 14.436983
Action reg: 0.003977
  l1.weight: grad_norm = 0.294662
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.194918
Total gradient norm: 0.527789
=== Actor Training Debug (Iteration 8420) ===
Q mean: -16.009829
Q std: 22.359022
Actor loss: 16.013788
Action reg: 0.003959
  l1.weight: grad_norm = 0.134576
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.094725
Total gradient norm: 0.238295
=== Actor Training Debug (Iteration 8421) ===
Q mean: -14.755987
Q std: 21.081417
Actor loss: 14.759937
Action reg: 0.003950
  l1.weight: grad_norm = 0.226268
  l1.bias: grad_norm = 0.000753
  l2.weight: grad_norm = 0.178866
Total gradient norm: 0.559478
=== Actor Training Debug (Iteration 8422) ===
Q mean: -13.150279
Q std: 19.807814
Actor loss: 13.154219
Action reg: 0.003940
  l1.weight: grad_norm = 0.231777
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.146562
Total gradient norm: 0.480563
=== Actor Training Debug (Iteration 8423) ===
Q mean: -14.195417
Q std: 19.351892
Actor loss: 14.199376
Action reg: 0.003959
  l1.weight: grad_norm = 0.301381
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.195261
Total gradient norm: 0.542924
=== Actor Training Debug (Iteration 8424) ===
Q mean: -14.432020
Q std: 21.315786
Actor loss: 14.435985
Action reg: 0.003964
  l1.weight: grad_norm = 0.174851
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.152821
Total gradient norm: 0.407732
=== Actor Training Debug (Iteration 8425) ===
Q mean: -15.355328
Q std: 21.585621
Actor loss: 15.359303
Action reg: 0.003975
  l1.weight: grad_norm = 0.231350
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.174036
Total gradient norm: 0.450441
=== Actor Training Debug (Iteration 8426) ===
Q mean: -17.921776
Q std: 22.068083
Actor loss: 17.925747
Action reg: 0.003971
  l1.weight: grad_norm = 0.197667
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.141345
Total gradient norm: 0.394935
=== Actor Training Debug (Iteration 8427) ===
Q mean: -13.469521
Q std: 19.434790
Actor loss: 13.473469
Action reg: 0.003948
  l1.weight: grad_norm = 0.268163
  l1.bias: grad_norm = 0.002581
  l2.weight: grad_norm = 0.225101
Total gradient norm: 0.653852
=== Actor Training Debug (Iteration 8428) ===
Q mean: -14.794079
Q std: 21.355461
Actor loss: 14.798066
Action reg: 0.003988
  l1.weight: grad_norm = 0.204418
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.144030
Total gradient norm: 0.395349
=== Actor Training Debug (Iteration 8429) ===
Q mean: -16.995514
Q std: 23.948221
Actor loss: 16.999470
Action reg: 0.003956
  l1.weight: grad_norm = 0.187573
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.134670
Total gradient norm: 0.365318
=== Actor Training Debug (Iteration 8430) ===
Q mean: -13.861263
Q std: 20.412193
Actor loss: 13.865220
Action reg: 0.003957
  l1.weight: grad_norm = 0.545664
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.330545
Total gradient norm: 0.935317
=== Actor Training Debug (Iteration 8431) ===
Q mean: -16.971039
Q std: 22.564680
Actor loss: 16.974977
Action reg: 0.003939
  l1.weight: grad_norm = 0.167290
  l1.bias: grad_norm = 0.002422
  l2.weight: grad_norm = 0.134272
Total gradient norm: 0.394558
=== Actor Training Debug (Iteration 8432) ===
Q mean: -14.873081
Q std: 21.716911
Actor loss: 14.877034
Action reg: 0.003953
  l1.weight: grad_norm = 0.211559
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.161392
Total gradient norm: 0.437410
=== Actor Training Debug (Iteration 8433) ===
Q mean: -12.823934
Q std: 19.763849
Actor loss: 12.827896
Action reg: 0.003963
  l1.weight: grad_norm = 0.138551
  l1.bias: grad_norm = 0.003666
  l2.weight: grad_norm = 0.100795
Total gradient norm: 0.287323
=== Actor Training Debug (Iteration 8434) ===
Q mean: -17.520935
Q std: 23.778353
Actor loss: 17.524878
Action reg: 0.003943
  l1.weight: grad_norm = 0.208337
  l1.bias: grad_norm = 0.003407
  l2.weight: grad_norm = 0.152866
Total gradient norm: 0.462969
=== Actor Training Debug (Iteration 8435) ===
Q mean: -15.186819
Q std: 22.519272
Actor loss: 15.190784
Action reg: 0.003965
  l1.weight: grad_norm = 0.237442
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.170484
Total gradient norm: 0.473649
=== Actor Training Debug (Iteration 8436) ===
Q mean: -15.743543
Q std: 22.053616
Actor loss: 15.747506
Action reg: 0.003964
  l1.weight: grad_norm = 0.337606
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.350759
Total gradient norm: 0.961887
=== Actor Training Debug (Iteration 8437) ===
Q mean: -13.143555
Q std: 18.850027
Actor loss: 13.147509
Action reg: 0.003954
  l1.weight: grad_norm = 0.241341
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.197741
Total gradient norm: 0.480831
=== Actor Training Debug (Iteration 8438) ===
Q mean: -15.303087
Q std: 21.414986
Actor loss: 15.307033
Action reg: 0.003946
  l1.weight: grad_norm = 0.221261
  l1.bias: grad_norm = 0.000561
  l2.weight: grad_norm = 0.181844
Total gradient norm: 0.514720
=== Actor Training Debug (Iteration 8439) ===
Q mean: -13.898547
Q std: 20.503372
Actor loss: 13.902510
Action reg: 0.003962
  l1.weight: grad_norm = 0.352210
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.263878
Total gradient norm: 0.702533
=== Actor Training Debug (Iteration 8440) ===
Q mean: -14.134804
Q std: 20.049026
Actor loss: 14.138755
Action reg: 0.003952
  l1.weight: grad_norm = 0.331800
  l1.bias: grad_norm = 0.003867
  l2.weight: grad_norm = 0.245633
Total gradient norm: 0.673021
=== Actor Training Debug (Iteration 8441) ===
Q mean: -15.306215
Q std: 21.447439
Actor loss: 15.310191
Action reg: 0.003976
  l1.weight: grad_norm = 0.322844
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.216675
Total gradient norm: 0.546266
=== Actor Training Debug (Iteration 8442) ===
Q mean: -14.242986
Q std: 21.587879
Actor loss: 14.246923
Action reg: 0.003937
  l1.weight: grad_norm = 0.309956
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.228914
Total gradient norm: 0.622304
=== Actor Training Debug (Iteration 8443) ===
Q mean: -15.239384
Q std: 21.692324
Actor loss: 15.243347
Action reg: 0.003963
  l1.weight: grad_norm = 0.193458
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.122758
Total gradient norm: 0.341964
=== Actor Training Debug (Iteration 8444) ===
Q mean: -15.244070
Q std: 21.046120
Actor loss: 15.248021
Action reg: 0.003951
  l1.weight: grad_norm = 0.439906
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.268978
Total gradient norm: 0.797103
=== Actor Training Debug (Iteration 8445) ===
Q mean: -16.375351
Q std: 21.343353
Actor loss: 16.379303
Action reg: 0.003952
  l1.weight: grad_norm = 0.329243
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.253729
Total gradient norm: 0.641923
=== Actor Training Debug (Iteration 8446) ===
Q mean: -14.318684
Q std: 19.849701
Actor loss: 14.322622
Action reg: 0.003939
  l1.weight: grad_norm = 0.108176
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.088528
Total gradient norm: 0.229794
=== Actor Training Debug (Iteration 8447) ===
Q mean: -13.279226
Q std: 19.596035
Actor loss: 13.283187
Action reg: 0.003960
  l1.weight: grad_norm = 0.331492
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.231660
Total gradient norm: 0.608745
=== Actor Training Debug (Iteration 8448) ===
Q mean: -15.145365
Q std: 21.368944
Actor loss: 15.149308
Action reg: 0.003943
  l1.weight: grad_norm = 0.291765
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.201956
Total gradient norm: 0.532323
=== Actor Training Debug (Iteration 8449) ===
Q mean: -13.476057
Q std: 20.780630
Actor loss: 13.480003
Action reg: 0.003947
  l1.weight: grad_norm = 0.223951
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.148571
Total gradient norm: 0.412127
=== Actor Training Debug (Iteration 8450) ===
Q mean: -15.451002
Q std: 21.535894
Actor loss: 15.454967
Action reg: 0.003966
  l1.weight: grad_norm = 0.173871
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.133015
Total gradient norm: 0.368007
=== Actor Training Debug (Iteration 8451) ===
Q mean: -15.265995
Q std: 20.433058
Actor loss: 15.269965
Action reg: 0.003970
  l1.weight: grad_norm = 0.234430
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.186812
Total gradient norm: 0.534795
=== Actor Training Debug (Iteration 8452) ===
Q mean: -14.290980
Q std: 19.538778
Actor loss: 14.294930
Action reg: 0.003949
  l1.weight: grad_norm = 0.391916
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.273544
Total gradient norm: 0.742721
=== Actor Training Debug (Iteration 8453) ===
Q mean: -11.559900
Q std: 17.592533
Actor loss: 11.563843
Action reg: 0.003942
  l1.weight: grad_norm = 0.262052
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.158234
Total gradient norm: 0.444791
=== Actor Training Debug (Iteration 8454) ===
Q mean: -16.944946
Q std: 21.597475
Actor loss: 16.948887
Action reg: 0.003941
  l1.weight: grad_norm = 0.202992
  l1.bias: grad_norm = 0.000926
  l2.weight: grad_norm = 0.138392
Total gradient norm: 0.370878
=== Actor Training Debug (Iteration 8455) ===
Q mean: -13.117331
Q std: 18.907101
Actor loss: 13.121305
Action reg: 0.003975
  l1.weight: grad_norm = 0.165616
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.126136
Total gradient norm: 0.360277
=== Actor Training Debug (Iteration 8456) ===
Q mean: -16.487185
Q std: 22.364693
Actor loss: 16.491129
Action reg: 0.003944
  l1.weight: grad_norm = 0.228921
  l1.bias: grad_norm = 0.004547
  l2.weight: grad_norm = 0.157619
Total gradient norm: 0.421879
=== Actor Training Debug (Iteration 8457) ===
Q mean: -14.430532
Q std: 21.248909
Actor loss: 14.434505
Action reg: 0.003973
  l1.weight: grad_norm = 0.413415
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.302195
Total gradient norm: 0.878123
=== Actor Training Debug (Iteration 8458) ===
Q mean: -16.315304
Q std: 22.532501
Actor loss: 16.319260
Action reg: 0.003956
  l1.weight: grad_norm = 0.405283
  l1.bias: grad_norm = 0.002396
  l2.weight: grad_norm = 0.346918
Total gradient norm: 1.020912
=== Actor Training Debug (Iteration 8459) ===
Q mean: -15.747910
Q std: 22.423548
Actor loss: 15.751840
Action reg: 0.003930
  l1.weight: grad_norm = 0.386825
  l1.bias: grad_norm = 0.004643
  l2.weight: grad_norm = 0.255470
Total gradient norm: 0.695931
=== Actor Training Debug (Iteration 8460) ===
Q mean: -14.380388
Q std: 20.242758
Actor loss: 14.384324
Action reg: 0.003935
  l1.weight: grad_norm = 0.254553
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.179447
Total gradient norm: 0.582433
=== Actor Training Debug (Iteration 8461) ===
Q mean: -14.729630
Q std: 20.682877
Actor loss: 14.733583
Action reg: 0.003954
  l1.weight: grad_norm = 0.194685
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.144053
Total gradient norm: 0.390359
=== Actor Training Debug (Iteration 8462) ===
Q mean: -14.060047
Q std: 19.813665
Actor loss: 14.064004
Action reg: 0.003957
  l1.weight: grad_norm = 0.525049
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.375767
Total gradient norm: 1.214968
=== Actor Training Debug (Iteration 8463) ===
Q mean: -14.041611
Q std: 21.482920
Actor loss: 14.045569
Action reg: 0.003958
  l1.weight: grad_norm = 0.246057
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.159381
Total gradient norm: 0.420232
=== Actor Training Debug (Iteration 8464) ===
Q mean: -13.966603
Q std: 20.043566
Actor loss: 13.970571
Action reg: 0.003968
  l1.weight: grad_norm = 0.329081
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.232071
Total gradient norm: 0.626573
=== Actor Training Debug (Iteration 8465) ===
Q mean: -14.066119
Q std: 19.712992
Actor loss: 14.070052
Action reg: 0.003933
  l1.weight: grad_norm = 0.418633
  l1.bias: grad_norm = 0.006652
  l2.weight: grad_norm = 0.320605
Total gradient norm: 1.162481
=== Actor Training Debug (Iteration 8466) ===
Q mean: -14.849384
Q std: 22.187128
Actor loss: 14.853366
Action reg: 0.003981
  l1.weight: grad_norm = 0.264504
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.213829
Total gradient norm: 0.559949
=== Actor Training Debug (Iteration 8467) ===
Q mean: -14.556900
Q std: 20.170155
Actor loss: 14.560863
Action reg: 0.003963
  l1.weight: grad_norm = 0.229288
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.151787
Total gradient norm: 0.387301
=== Actor Training Debug (Iteration 8468) ===
Q mean: -14.283390
Q std: 20.604742
Actor loss: 14.287350
Action reg: 0.003959
  l1.weight: grad_norm = 0.214349
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.137892
Total gradient norm: 0.380892
=== Actor Training Debug (Iteration 8469) ===
Q mean: -15.972378
Q std: 21.050877
Actor loss: 15.976356
Action reg: 0.003977
  l1.weight: grad_norm = 0.270185
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.166001
Total gradient norm: 0.494717
=== Actor Training Debug (Iteration 8470) ===
Q mean: -12.879711
Q std: 20.029732
Actor loss: 12.883641
Action reg: 0.003930
  l1.weight: grad_norm = 0.202667
  l1.bias: grad_norm = 0.000763
  l2.weight: grad_norm = 0.125682
Total gradient norm: 0.337616
=== Actor Training Debug (Iteration 8471) ===
Q mean: -13.507296
Q std: 18.994389
Actor loss: 13.511238
Action reg: 0.003943
  l1.weight: grad_norm = 0.163687
  l1.bias: grad_norm = 0.002833
  l2.weight: grad_norm = 0.116693
Total gradient norm: 0.309251
=== Actor Training Debug (Iteration 8472) ===
Q mean: -14.342841
Q std: 20.935570
Actor loss: 14.346815
Action reg: 0.003974
  l1.weight: grad_norm = 0.167740
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.125270
Total gradient norm: 0.400876
=== Actor Training Debug (Iteration 8473) ===
Q mean: -15.931351
Q std: 21.890285
Actor loss: 15.935314
Action reg: 0.003963
  l1.weight: grad_norm = 0.383384
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.263230
Total gradient norm: 0.701378
=== Actor Training Debug (Iteration 8474) ===
Q mean: -14.052714
Q std: 19.469812
Actor loss: 14.056661
Action reg: 0.003946
  l1.weight: grad_norm = 0.292084
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.213954
Total gradient norm: 0.569601
=== Actor Training Debug (Iteration 8475) ===
Q mean: -17.586538
Q std: 23.080667
Actor loss: 17.590490
Action reg: 0.003951
  l1.weight: grad_norm = 0.178729
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.138714
Total gradient norm: 0.401164
=== Actor Training Debug (Iteration 8476) ===
Q mean: -14.950619
Q std: 20.484982
Actor loss: 14.954576
Action reg: 0.003957
  l1.weight: grad_norm = 0.323770
  l1.bias: grad_norm = 0.001944
  l2.weight: grad_norm = 0.223654
Total gradient norm: 0.571429
=== Actor Training Debug (Iteration 8477) ===
Q mean: -15.475121
Q std: 21.460085
Actor loss: 15.479088
Action reg: 0.003966
  l1.weight: grad_norm = 0.208418
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.121584
Total gradient norm: 0.360747
=== Actor Training Debug (Iteration 8478) ===
Q mean: -13.374072
Q std: 21.023586
Actor loss: 13.378026
Action reg: 0.003954
  l1.weight: grad_norm = 0.513525
  l1.bias: grad_norm = 0.003056
  l2.weight: grad_norm = 0.357266
Total gradient norm: 0.872248
=== Actor Training Debug (Iteration 8479) ===
Q mean: -13.553576
Q std: 19.942171
Actor loss: 13.557531
Action reg: 0.003956
  l1.weight: grad_norm = 0.209398
  l1.bias: grad_norm = 0.001844
  l2.weight: grad_norm = 0.147176
Total gradient norm: 0.364548
=== Actor Training Debug (Iteration 8480) ===
Q mean: -14.224314
Q std: 20.414177
Actor loss: 14.228271
Action reg: 0.003958
  l1.weight: grad_norm = 0.171670
  l1.bias: grad_norm = 0.000562
  l2.weight: grad_norm = 0.116923
Total gradient norm: 0.336718
=== Actor Training Debug (Iteration 8481) ===
Q mean: -14.492277
Q std: 20.802929
Actor loss: 14.496250
Action reg: 0.003973
  l1.weight: grad_norm = 0.175265
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.124874
Total gradient norm: 0.353343
=== Actor Training Debug (Iteration 8482) ===
Q mean: -14.972886
Q std: 20.722734
Actor loss: 14.976823
Action reg: 0.003937
  l1.weight: grad_norm = 0.272747
  l1.bias: grad_norm = 0.000677
  l2.weight: grad_norm = 0.181254
Total gradient norm: 0.496429
=== Actor Training Debug (Iteration 8483) ===
Q mean: -15.499124
Q std: 21.433229
Actor loss: 15.503078
Action reg: 0.003954
  l1.weight: grad_norm = 0.334991
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.266979
Total gradient norm: 0.817938
=== Actor Training Debug (Iteration 8484) ===
Q mean: -16.747381
Q std: 21.807104
Actor loss: 16.751345
Action reg: 0.003964
  l1.weight: grad_norm = 0.866908
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.676200
Total gradient norm: 2.324273
=== Actor Training Debug (Iteration 8485) ===
Q mean: -17.516167
Q std: 22.118252
Actor loss: 17.520140
Action reg: 0.003974
  l1.weight: grad_norm = 0.437267
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.281551
Total gradient norm: 0.706988
=== Actor Training Debug (Iteration 8486) ===
Q mean: -14.094131
Q std: 20.235464
Actor loss: 14.098078
Action reg: 0.003946
  l1.weight: grad_norm = 0.202034
  l1.bias: grad_norm = 0.001889
  l2.weight: grad_norm = 0.152691
Total gradient norm: 0.478447
=== Actor Training Debug (Iteration 8487) ===
Q mean: -15.450720
Q std: 22.779970
Actor loss: 15.454673
Action reg: 0.003953
  l1.weight: grad_norm = 0.174088
  l1.bias: grad_norm = 0.000703
  l2.weight: grad_norm = 0.141895
Total gradient norm: 0.405197
=== Actor Training Debug (Iteration 8488) ===
Q mean: -16.017424
Q std: 21.780615
Actor loss: 16.021383
Action reg: 0.003960
  l1.weight: grad_norm = 0.157411
  l1.bias: grad_norm = 0.000526
  l2.weight: grad_norm = 0.121710
Total gradient norm: 0.336741
=== Actor Training Debug (Iteration 8489) ===
Q mean: -15.951483
Q std: 21.580240
Actor loss: 15.955444
Action reg: 0.003961
  l1.weight: grad_norm = 0.200360
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.189420
Total gradient norm: 0.534680
=== Actor Training Debug (Iteration 8490) ===
Q mean: -15.571269
Q std: 21.624529
Actor loss: 15.575198
Action reg: 0.003929
  l1.weight: grad_norm = 0.318981
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.227111
Total gradient norm: 0.598925
=== Actor Training Debug (Iteration 8491) ===
Q mean: -13.321006
Q std: 19.910683
Actor loss: 13.324944
Action reg: 0.003938
  l1.weight: grad_norm = 0.269479
  l1.bias: grad_norm = 0.002409
  l2.weight: grad_norm = 0.183870
Total gradient norm: 0.521081
=== Actor Training Debug (Iteration 8492) ===
Q mean: -14.778343
Q std: 20.592543
Actor loss: 14.782292
Action reg: 0.003950
  l1.weight: grad_norm = 0.236715
  l1.bias: grad_norm = 0.002521
  l2.weight: grad_norm = 0.164446
Total gradient norm: 0.511908
=== Actor Training Debug (Iteration 8493) ===
Q mean: -16.393902
Q std: 23.445410
Actor loss: 16.397854
Action reg: 0.003952
  l1.weight: grad_norm = 0.292581
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.248780
Total gradient norm: 0.888188
=== Actor Training Debug (Iteration 8494) ===
Q mean: -13.987265
Q std: 20.233244
Actor loss: 13.991215
Action reg: 0.003950
  l1.weight: grad_norm = 0.392003
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.301922
Total gradient norm: 0.998666
=== Actor Training Debug (Iteration 8495) ===
Q mean: -12.562063
Q std: 19.143337
Actor loss: 12.566022
Action reg: 0.003958
  l1.weight: grad_norm = 0.247694
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.153925
Total gradient norm: 0.421780
=== Actor Training Debug (Iteration 8496) ===
Q mean: -15.217699
Q std: 21.247221
Actor loss: 15.221656
Action reg: 0.003957
  l1.weight: grad_norm = 0.390125
  l1.bias: grad_norm = 0.000666
  l2.weight: grad_norm = 0.282141
Total gradient norm: 0.713148
=== Actor Training Debug (Iteration 8497) ===
Q mean: -16.263489
Q std: 21.490705
Actor loss: 16.267450
Action reg: 0.003961
  l1.weight: grad_norm = 0.350478
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.281247
Total gradient norm: 0.741494
=== Actor Training Debug (Iteration 8498) ===
Q mean: -13.560485
Q std: 19.690384
Actor loss: 13.564452
Action reg: 0.003967
  l1.weight: grad_norm = 0.252500
  l1.bias: grad_norm = 0.001097
  l2.weight: grad_norm = 0.196868
Total gradient norm: 0.578087
=== Actor Training Debug (Iteration 8499) ===
Q mean: -16.157177
Q std: 20.949560
Actor loss: 16.161135
Action reg: 0.003958
  l1.weight: grad_norm = 0.248535
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.202583
Total gradient norm: 0.558388
=== Actor Training Debug (Iteration 8500) ===
Q mean: -12.564207
Q std: 18.774017
Actor loss: 12.568142
Action reg: 0.003935
  l1.weight: grad_norm = 0.231714
  l1.bias: grad_norm = 0.001355
  l2.weight: grad_norm = 0.202702
Total gradient norm: 0.604855
  Average reward: -322.884 | Average length: 100.0
Evaluation at episode 135: -322.884
=== Actor Training Debug (Iteration 8501) ===
Q mean: -16.681583
Q std: 21.561274
Actor loss: 16.685551
Action reg: 0.003968
  l1.weight: grad_norm = 0.332321
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.254408
Total gradient norm: 0.744965
=== Actor Training Debug (Iteration 8502) ===
Q mean: -13.757381
Q std: 21.168089
Actor loss: 13.761345
Action reg: 0.003964
  l1.weight: grad_norm = 0.545534
  l1.bias: grad_norm = 0.000534
  l2.weight: grad_norm = 0.369032
Total gradient norm: 1.015920
=== Actor Training Debug (Iteration 8503) ===
Q mean: -17.459270
Q std: 22.232334
Actor loss: 17.463221
Action reg: 0.003950
  l1.weight: grad_norm = 0.150112
  l1.bias: grad_norm = 0.002050
  l2.weight: grad_norm = 0.115874
Total gradient norm: 0.310751
=== Actor Training Debug (Iteration 8504) ===
Q mean: -14.344331
Q std: 20.796602
Actor loss: 14.348301
Action reg: 0.003970
  l1.weight: grad_norm = 0.174822
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.122158
Total gradient norm: 0.352432
=== Actor Training Debug (Iteration 8505) ===
Q mean: -15.171350
Q std: 21.082535
Actor loss: 15.175304
Action reg: 0.003954
  l1.weight: grad_norm = 0.124157
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.097237
Total gradient norm: 0.274033
=== Actor Training Debug (Iteration 8506) ===
Q mean: -15.408297
Q std: 22.607990
Actor loss: 15.412272
Action reg: 0.003976
  l1.weight: grad_norm = 0.143725
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.113701
Total gradient norm: 0.395089
=== Actor Training Debug (Iteration 8507) ===
Q mean: -13.934965
Q std: 20.734270
Actor loss: 13.938929
Action reg: 0.003964
  l1.weight: grad_norm = 0.405213
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.249880
Total gradient norm: 0.631612
=== Actor Training Debug (Iteration 8508) ===
Q mean: -15.031324
Q std: 21.857908
Actor loss: 15.035282
Action reg: 0.003957
  l1.weight: grad_norm = 0.202884
  l1.bias: grad_norm = 0.003575
  l2.weight: grad_norm = 0.136407
Total gradient norm: 0.410601
=== Actor Training Debug (Iteration 8509) ===
Q mean: -15.963104
Q std: 23.444880
Actor loss: 15.967034
Action reg: 0.003930
  l1.weight: grad_norm = 0.336173
  l1.bias: grad_norm = 0.003299
  l2.weight: grad_norm = 0.240064
Total gradient norm: 0.643314
=== Actor Training Debug (Iteration 8510) ===
Q mean: -16.550926
Q std: 20.878551
Actor loss: 16.554905
Action reg: 0.003979
  l1.weight: grad_norm = 0.190159
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.150938
Total gradient norm: 0.402519
=== Actor Training Debug (Iteration 8511) ===
Q mean: -15.838757
Q std: 22.587248
Actor loss: 15.842733
Action reg: 0.003977
  l1.weight: grad_norm = 0.144387
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.101678
Total gradient norm: 0.267632
=== Actor Training Debug (Iteration 8512) ===
Q mean: -14.961329
Q std: 21.433056
Actor loss: 14.965295
Action reg: 0.003966
  l1.weight: grad_norm = 0.169981
  l1.bias: grad_norm = 0.000611
  l2.weight: grad_norm = 0.136218
Total gradient norm: 0.386863
=== Actor Training Debug (Iteration 8513) ===
Q mean: -13.652028
Q std: 19.937326
Actor loss: 13.655988
Action reg: 0.003959
  l1.weight: grad_norm = 0.230679
  l1.bias: grad_norm = 0.001917
  l2.weight: grad_norm = 0.158512
Total gradient norm: 0.400689
=== Actor Training Debug (Iteration 8514) ===
Q mean: -13.892181
Q std: 20.736002
Actor loss: 13.896135
Action reg: 0.003954
  l1.weight: grad_norm = 0.182701
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.137934
Total gradient norm: 0.438037
=== Actor Training Debug (Iteration 8515) ===
Q mean: -15.107643
Q std: 21.505623
Actor loss: 15.111588
Action reg: 0.003944
  l1.weight: grad_norm = 0.373673
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.295524
Total gradient norm: 0.804921
=== Actor Training Debug (Iteration 8516) ===
Q mean: -15.351332
Q std: 22.986292
Actor loss: 15.355300
Action reg: 0.003968
  l1.weight: grad_norm = 0.262166
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.194111
Total gradient norm: 0.582028
=== Actor Training Debug (Iteration 8517) ===
Q mean: -13.747940
Q std: 20.066349
Actor loss: 13.751902
Action reg: 0.003962
  l1.weight: grad_norm = 0.239940
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.185216
Total gradient norm: 0.498125
=== Actor Training Debug (Iteration 8518) ===
Q mean: -13.039469
Q std: 20.228691
Actor loss: 13.043423
Action reg: 0.003954
  l1.weight: grad_norm = 0.419441
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.297892
Total gradient norm: 0.782585
=== Actor Training Debug (Iteration 8519) ===
Q mean: -15.008173
Q std: 21.587927
Actor loss: 15.012115
Action reg: 0.003943
  l1.weight: grad_norm = 0.171870
  l1.bias: grad_norm = 0.002155
  l2.weight: grad_norm = 0.138181
Total gradient norm: 0.365050
=== Actor Training Debug (Iteration 8520) ===
Q mean: -12.872269
Q std: 20.256098
Actor loss: 12.876220
Action reg: 0.003951
  l1.weight: grad_norm = 0.165532
  l1.bias: grad_norm = 0.002290
  l2.weight: grad_norm = 0.130654
Total gradient norm: 0.387951
=== Actor Training Debug (Iteration 8521) ===
Q mean: -13.454494
Q std: 19.674131
Actor loss: 13.458463
Action reg: 0.003968
  l1.weight: grad_norm = 0.162883
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.120885
Total gradient norm: 0.351135
=== Actor Training Debug (Iteration 8522) ===
Q mean: -15.183386
Q std: 22.189743
Actor loss: 15.187325
Action reg: 0.003938
  l1.weight: grad_norm = 1.157173
  l1.bias: grad_norm = 0.000600
  l2.weight: grad_norm = 1.035714
Total gradient norm: 3.038542
=== Actor Training Debug (Iteration 8523) ===
Q mean: -14.611614
Q std: 21.261608
Actor loss: 14.615545
Action reg: 0.003931
  l1.weight: grad_norm = 0.358438
  l1.bias: grad_norm = 0.002283
  l2.weight: grad_norm = 0.304193
Total gradient norm: 1.015910
=== Actor Training Debug (Iteration 8524) ===
Q mean: -14.518549
Q std: 21.889204
Actor loss: 14.522495
Action reg: 0.003947
  l1.weight: grad_norm = 0.306957
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.190361
Total gradient norm: 0.505524
=== Actor Training Debug (Iteration 8525) ===
Q mean: -12.945622
Q std: 21.710545
Actor loss: 12.949594
Action reg: 0.003972
  l1.weight: grad_norm = 0.183451
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.129086
Total gradient norm: 0.339722
=== Actor Training Debug (Iteration 8526) ===
Q mean: -14.994341
Q std: 21.537884
Actor loss: 14.998305
Action reg: 0.003964
  l1.weight: grad_norm = 0.205134
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.153095
Total gradient norm: 0.403815
=== Actor Training Debug (Iteration 8527) ===
Q mean: -15.083391
Q std: 21.494991
Actor loss: 15.087334
Action reg: 0.003943
  l1.weight: grad_norm = 0.314077
  l1.bias: grad_norm = 0.001370
  l2.weight: grad_norm = 0.193099
Total gradient norm: 0.537410
=== Actor Training Debug (Iteration 8528) ===
Q mean: -16.220982
Q std: 21.889307
Actor loss: 16.224928
Action reg: 0.003947
  l1.weight: grad_norm = 0.272786
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.200911
Total gradient norm: 0.501941
=== Actor Training Debug (Iteration 8529) ===
Q mean: -14.585212
Q std: 20.673237
Actor loss: 14.589183
Action reg: 0.003971
  l1.weight: grad_norm = 0.266694
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.183285
Total gradient norm: 0.479113
=== Actor Training Debug (Iteration 8530) ===
Q mean: -13.082129
Q std: 20.281866
Actor loss: 13.086083
Action reg: 0.003955
  l1.weight: grad_norm = 0.226629
  l1.bias: grad_norm = 0.002327
  l2.weight: grad_norm = 0.177413
Total gradient norm: 0.515339
=== Actor Training Debug (Iteration 8531) ===
Q mean: -13.710624
Q std: 21.040298
Actor loss: 13.714589
Action reg: 0.003965
  l1.weight: grad_norm = 0.206995
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.159237
Total gradient norm: 0.414962
=== Actor Training Debug (Iteration 8532) ===
Q mean: -15.862703
Q std: 24.671793
Actor loss: 15.866665
Action reg: 0.003961
  l1.weight: grad_norm = 0.279405
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.224347
Total gradient norm: 0.619121
=== Actor Training Debug (Iteration 8533) ===
Q mean: -15.196896
Q std: 23.047661
Actor loss: 15.200840
Action reg: 0.003944
  l1.weight: grad_norm = 0.201210
  l1.bias: grad_norm = 0.000682
  l2.weight: grad_norm = 0.146781
Total gradient norm: 0.413267
=== Actor Training Debug (Iteration 8534) ===
Q mean: -16.266010
Q std: 21.814270
Actor loss: 16.269978
Action reg: 0.003967
  l1.weight: grad_norm = 0.295234
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.184453
Total gradient norm: 0.519523
=== Actor Training Debug (Iteration 8535) ===
Q mean: -15.257092
Q std: 20.519413
Actor loss: 15.261060
Action reg: 0.003968
  l1.weight: grad_norm = 0.125645
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.075886
Total gradient norm: 0.204141
=== Actor Training Debug (Iteration 8536) ===
Q mean: -17.930590
Q std: 22.801004
Actor loss: 17.934559
Action reg: 0.003970
  l1.weight: grad_norm = 0.271509
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.190889
Total gradient norm: 0.556130
=== Actor Training Debug (Iteration 8537) ===
Q mean: -14.384047
Q std: 20.390532
Actor loss: 14.387999
Action reg: 0.003952
  l1.weight: grad_norm = 0.146551
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.101197
Total gradient norm: 0.308784
=== Actor Training Debug (Iteration 8538) ===
Q mean: -14.784037
Q std: 20.357096
Actor loss: 14.788013
Action reg: 0.003976
  l1.weight: grad_norm = 0.208651
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.147179
Total gradient norm: 0.381359
=== Actor Training Debug (Iteration 8539) ===
Q mean: -16.388241
Q std: 21.732515
Actor loss: 16.392206
Action reg: 0.003966
  l1.weight: grad_norm = 0.232263
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.159785
Total gradient norm: 0.424585
=== Actor Training Debug (Iteration 8540) ===
Q mean: -12.591180
Q std: 19.040886
Actor loss: 12.595136
Action reg: 0.003956
  l1.weight: grad_norm = 0.167502
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.126356
Total gradient norm: 0.342937
=== Actor Training Debug (Iteration 8541) ===
Q mean: -14.489155
Q std: 20.942986
Actor loss: 14.493122
Action reg: 0.003967
  l1.weight: grad_norm = 0.299520
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.192381
Total gradient norm: 0.585700
=== Actor Training Debug (Iteration 8542) ===
Q mean: -15.945410
Q std: 22.662428
Actor loss: 15.949385
Action reg: 0.003975
  l1.weight: grad_norm = 0.357366
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.271171
Total gradient norm: 0.707373
=== Actor Training Debug (Iteration 8543) ===
Q mean: -13.609732
Q std: 19.798639
Actor loss: 13.613676
Action reg: 0.003944
  l1.weight: grad_norm = 0.477188
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.319172
Total gradient norm: 0.861846
=== Actor Training Debug (Iteration 8544) ===
Q mean: -15.348516
Q std: 20.591713
Actor loss: 15.352482
Action reg: 0.003967
  l1.weight: grad_norm = 0.301181
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.199515
Total gradient norm: 0.555453
=== Actor Training Debug (Iteration 8545) ===
Q mean: -14.679377
Q std: 19.098076
Actor loss: 14.683317
Action reg: 0.003940
  l1.weight: grad_norm = 0.308856
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.221374
Total gradient norm: 0.593413
=== Actor Training Debug (Iteration 8546) ===
Q mean: -13.893963
Q std: 21.548561
Actor loss: 13.897920
Action reg: 0.003957
  l1.weight: grad_norm = 0.520894
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.374716
Total gradient norm: 1.176114
=== Actor Training Debug (Iteration 8547) ===
Q mean: -15.066409
Q std: 21.490370
Actor loss: 15.070378
Action reg: 0.003970
  l1.weight: grad_norm = 0.180663
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.132161
Total gradient norm: 0.416021
=== Actor Training Debug (Iteration 8548) ===
Q mean: -16.926552
Q std: 23.151464
Actor loss: 16.930506
Action reg: 0.003954
  l1.weight: grad_norm = 0.387967
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.284023
Total gradient norm: 0.810616
=== Actor Training Debug (Iteration 8549) ===
Q mean: -12.324520
Q std: 17.773329
Actor loss: 12.328489
Action reg: 0.003969
  l1.weight: grad_norm = 0.161569
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.118142
Total gradient norm: 0.331945
=== Actor Training Debug (Iteration 8550) ===
Q mean: -14.780670
Q std: 19.767420
Actor loss: 14.784621
Action reg: 0.003951
  l1.weight: grad_norm = 0.206593
  l1.bias: grad_norm = 0.000681
  l2.weight: grad_norm = 0.152884
Total gradient norm: 0.455676
=== Actor Training Debug (Iteration 8551) ===
Q mean: -13.679122
Q std: 21.879185
Actor loss: 13.683060
Action reg: 0.003938
  l1.weight: grad_norm = 0.250287
  l1.bias: grad_norm = 0.000548
  l2.weight: grad_norm = 0.163044
Total gradient norm: 0.505823
=== Actor Training Debug (Iteration 8552) ===
Q mean: -13.371784
Q std: 19.676388
Actor loss: 13.375723
Action reg: 0.003939
  l1.weight: grad_norm = 0.372895
  l1.bias: grad_norm = 0.000528
  l2.weight: grad_norm = 0.295324
Total gradient norm: 0.685430
=== Actor Training Debug (Iteration 8553) ===
Q mean: -13.680931
Q std: 21.083048
Actor loss: 13.684879
Action reg: 0.003948
  l1.weight: grad_norm = 0.505952
  l1.bias: grad_norm = 0.000604
  l2.weight: grad_norm = 0.293756
Total gradient norm: 0.823052
=== Actor Training Debug (Iteration 8554) ===
Q mean: -12.387409
Q std: 17.338360
Actor loss: 12.391367
Action reg: 0.003958
  l1.weight: grad_norm = 0.226404
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.175450
Total gradient norm: 0.502311
=== Actor Training Debug (Iteration 8555) ===
Q mean: -13.674647
Q std: 20.255377
Actor loss: 13.678615
Action reg: 0.003968
  l1.weight: grad_norm = 0.430345
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.281492
Total gradient norm: 0.778662
=== Actor Training Debug (Iteration 8556) ===
Q mean: -15.051165
Q std: 20.256540
Actor loss: 15.055095
Action reg: 0.003930
  l1.weight: grad_norm = 0.289971
  l1.bias: grad_norm = 0.002410
  l2.weight: grad_norm = 0.205152
Total gradient norm: 0.523601
=== Actor Training Debug (Iteration 8557) ===
Q mean: -15.364195
Q std: 21.700985
Actor loss: 15.368139
Action reg: 0.003945
  l1.weight: grad_norm = 0.165847
  l1.bias: grad_norm = 0.002359
  l2.weight: grad_norm = 0.135571
Total gradient norm: 0.375437
=== Actor Training Debug (Iteration 8558) ===
Q mean: -14.911131
Q std: 22.027414
Actor loss: 14.915065
Action reg: 0.003934
  l1.weight: grad_norm = 0.280344
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.203971
Total gradient norm: 0.570916
=== Actor Training Debug (Iteration 8559) ===
Q mean: -15.612925
Q std: 23.209599
Actor loss: 15.616876
Action reg: 0.003951
  l1.weight: grad_norm = 0.178601
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.129764
Total gradient norm: 0.329510
=== Actor Training Debug (Iteration 8560) ===
Q mean: -13.606529
Q std: 20.213358
Actor loss: 13.610455
Action reg: 0.003925
  l1.weight: grad_norm = 0.274233
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.229630
Total gradient norm: 0.678729
=== Actor Training Debug (Iteration 8561) ===
Q mean: -12.565706
Q std: 18.448874
Actor loss: 12.569676
Action reg: 0.003970
  l1.weight: grad_norm = 0.270961
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.252966
Total gradient norm: 0.623165
=== Actor Training Debug (Iteration 8562) ===
Q mean: -15.798429
Q std: 22.565001
Actor loss: 15.802398
Action reg: 0.003968
  l1.weight: grad_norm = 0.230148
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.164018
Total gradient norm: 0.478981
=== Actor Training Debug (Iteration 8563) ===
Q mean: -14.596981
Q std: 20.909769
Actor loss: 14.600938
Action reg: 0.003957
  l1.weight: grad_norm = 0.314430
  l1.bias: grad_norm = 0.001079
  l2.weight: grad_norm = 0.236209
Total gradient norm: 0.610813
=== Actor Training Debug (Iteration 8564) ===
Q mean: -14.874876
Q std: 21.472317
Actor loss: 14.878839
Action reg: 0.003963
  l1.weight: grad_norm = 0.166923
  l1.bias: grad_norm = 0.000736
  l2.weight: grad_norm = 0.108344
Total gradient norm: 0.293978
=== Actor Training Debug (Iteration 8565) ===
Q mean: -14.254881
Q std: 21.550837
Actor loss: 14.258818
Action reg: 0.003937
  l1.weight: grad_norm = 0.403680
  l1.bias: grad_norm = 0.002889
  l2.weight: grad_norm = 0.366298
Total gradient norm: 1.140883
=== Actor Training Debug (Iteration 8566) ===
Q mean: -15.528782
Q std: 21.238537
Actor loss: 15.532724
Action reg: 0.003942
  l1.weight: grad_norm = 0.191776
  l1.bias: grad_norm = 0.000611
  l2.weight: grad_norm = 0.137896
Total gradient norm: 0.398184
=== Actor Training Debug (Iteration 8567) ===
Q mean: -16.078634
Q std: 20.638657
Actor loss: 16.082609
Action reg: 0.003975
  l1.weight: grad_norm = 0.781706
  l1.bias: grad_norm = 0.000828
  l2.weight: grad_norm = 0.591757
Total gradient norm: 1.631337
=== Actor Training Debug (Iteration 8568) ===
Q mean: -14.886250
Q std: 21.922522
Actor loss: 14.890161
Action reg: 0.003911
  l1.weight: grad_norm = 0.287777
  l1.bias: grad_norm = 0.001230
  l2.weight: grad_norm = 0.223384
Total gradient norm: 0.550014
=== Actor Training Debug (Iteration 8569) ===
Q mean: -12.372294
Q std: 18.634373
Actor loss: 12.376237
Action reg: 0.003943
  l1.weight: grad_norm = 0.296954
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.213265
Total gradient norm: 0.680626
=== Actor Training Debug (Iteration 8570) ===
Q mean: -13.835213
Q std: 18.628815
Actor loss: 13.839188
Action reg: 0.003975
  l1.weight: grad_norm = 0.397522
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.257964
Total gradient norm: 0.692512
=== Actor Training Debug (Iteration 8571) ===
Q mean: -15.137499
Q std: 20.568138
Actor loss: 15.141436
Action reg: 0.003937
  l1.weight: grad_norm = 0.163213
  l1.bias: grad_norm = 0.000660
  l2.weight: grad_norm = 0.134731
Total gradient norm: 0.392790
=== Actor Training Debug (Iteration 8572) ===
Q mean: -16.474440
Q std: 21.592289
Actor loss: 16.478403
Action reg: 0.003964
  l1.weight: grad_norm = 0.313610
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.225197
Total gradient norm: 0.627425
=== Actor Training Debug (Iteration 8573) ===
Q mean: -15.239046
Q std: 21.705122
Actor loss: 15.242994
Action reg: 0.003948
  l1.weight: grad_norm = 0.307586
  l1.bias: grad_norm = 0.004094
  l2.weight: grad_norm = 0.213060
Total gradient norm: 0.724341
=== Actor Training Debug (Iteration 8574) ===
Q mean: -14.934609
Q std: 20.980209
Actor loss: 14.938540
Action reg: 0.003930
  l1.weight: grad_norm = 0.304112
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.239581
Total gradient norm: 0.720071
=== Actor Training Debug (Iteration 8575) ===
Q mean: -14.303718
Q std: 20.237776
Actor loss: 14.307688
Action reg: 0.003970
  l1.weight: grad_norm = 0.236799
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.160854
Total gradient norm: 0.439596
=== Actor Training Debug (Iteration 8576) ===
Q mean: -16.207354
Q std: 23.024666
Actor loss: 16.211311
Action reg: 0.003957
  l1.weight: grad_norm = 0.146433
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.110747
Total gradient norm: 0.340474
=== Actor Training Debug (Iteration 8577) ===
Q mean: -16.724445
Q std: 23.433603
Actor loss: 16.728424
Action reg: 0.003979
  l1.weight: grad_norm = 0.227610
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.181545
Total gradient norm: 0.485728
=== Actor Training Debug (Iteration 8578) ===
Q mean: -15.667273
Q std: 21.649879
Actor loss: 15.671235
Action reg: 0.003962
  l1.weight: grad_norm = 0.260071
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.220752
Total gradient norm: 0.614600
=== Actor Training Debug (Iteration 8579) ===
Q mean: -13.484902
Q std: 20.749296
Actor loss: 13.488845
Action reg: 0.003943
  l1.weight: grad_norm = 0.136777
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.099061
Total gradient norm: 0.294874
=== Actor Training Debug (Iteration 8580) ===
Q mean: -14.718521
Q std: 21.528610
Actor loss: 14.722473
Action reg: 0.003952
  l1.weight: grad_norm = 0.275216
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.230124
Total gradient norm: 0.603407
=== Actor Training Debug (Iteration 8581) ===
Q mean: -13.999418
Q std: 20.740612
Actor loss: 14.003406
Action reg: 0.003987
  l1.weight: grad_norm = 0.172246
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.150887
Total gradient norm: 0.437586
=== Actor Training Debug (Iteration 8582) ===
Q mean: -15.176183
Q std: 21.511793
Actor loss: 15.180146
Action reg: 0.003964
  l1.weight: grad_norm = 0.202744
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.131431
Total gradient norm: 0.346573
=== Actor Training Debug (Iteration 8583) ===
Q mean: -16.120987
Q std: 23.428871
Actor loss: 16.124952
Action reg: 0.003965
  l1.weight: grad_norm = 0.687838
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.421905
Total gradient norm: 1.191345
=== Actor Training Debug (Iteration 8584) ===
Q mean: -15.428219
Q std: 21.316559
Actor loss: 15.432181
Action reg: 0.003963
  l1.weight: grad_norm = 0.256989
  l1.bias: grad_norm = 0.000524
  l2.weight: grad_norm = 0.180726
Total gradient norm: 0.535066
=== Actor Training Debug (Iteration 8585) ===
Q mean: -13.332654
Q std: 20.295519
Actor loss: 13.336637
Action reg: 0.003982
  l1.weight: grad_norm = 0.297539
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.199597
Total gradient norm: 0.581478
=== Actor Training Debug (Iteration 8586) ===
Q mean: -12.969621
Q std: 20.101282
Actor loss: 12.973584
Action reg: 0.003964
  l1.weight: grad_norm = 0.210671
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.168141
Total gradient norm: 0.434148
=== Actor Training Debug (Iteration 8587) ===
Q mean: -13.695274
Q std: 20.863199
Actor loss: 13.699220
Action reg: 0.003946
  l1.weight: grad_norm = 0.101298
  l1.bias: grad_norm = 0.001445
  l2.weight: grad_norm = 0.078650
Total gradient norm: 0.240054
=== Actor Training Debug (Iteration 8588) ===
Q mean: -14.651073
Q std: 21.106575
Actor loss: 14.655043
Action reg: 0.003970
  l1.weight: grad_norm = 0.239317
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.181737
Total gradient norm: 0.547439
=== Actor Training Debug (Iteration 8589) ===
Q mean: -13.790657
Q std: 20.223284
Actor loss: 13.794606
Action reg: 0.003949
  l1.weight: grad_norm = 0.364862
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.267994
Total gradient norm: 0.792525
=== Actor Training Debug (Iteration 8590) ===
Q mean: -12.648701
Q std: 20.593454
Actor loss: 12.652637
Action reg: 0.003937
  l1.weight: grad_norm = 0.295490
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.212267
Total gradient norm: 0.603629
=== Actor Training Debug (Iteration 8591) ===
Q mean: -13.881580
Q std: 21.410688
Actor loss: 13.885531
Action reg: 0.003951
  l1.weight: grad_norm = 0.246255
  l1.bias: grad_norm = 0.003280
  l2.weight: grad_norm = 0.169094
Total gradient norm: 0.471350
=== Actor Training Debug (Iteration 8592) ===
Q mean: -11.748253
Q std: 17.971027
Actor loss: 11.752192
Action reg: 0.003940
  l1.weight: grad_norm = 0.224010
  l1.bias: grad_norm = 0.002167
  l2.weight: grad_norm = 0.216271
Total gradient norm: 0.653420
=== Actor Training Debug (Iteration 8593) ===
Q mean: -13.304525
Q std: 20.309381
Actor loss: 13.308496
Action reg: 0.003972
  l1.weight: grad_norm = 0.162605
  l1.bias: grad_norm = 0.001934
  l2.weight: grad_norm = 0.112043
Total gradient norm: 0.303376
=== Actor Training Debug (Iteration 8594) ===
Q mean: -13.253246
Q std: 21.232443
Actor loss: 13.257209
Action reg: 0.003962
  l1.weight: grad_norm = 0.387573
  l1.bias: grad_norm = 0.002731
  l2.weight: grad_norm = 0.334016
Total gradient norm: 0.959026
=== Actor Training Debug (Iteration 8595) ===
Q mean: -15.973791
Q std: 22.557219
Actor loss: 15.977746
Action reg: 0.003954
  l1.weight: grad_norm = 0.390069
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.279216
Total gradient norm: 0.779895
=== Actor Training Debug (Iteration 8596) ===
Q mean: -14.219404
Q std: 19.860025
Actor loss: 14.223346
Action reg: 0.003941
  l1.weight: grad_norm = 0.310221
  l1.bias: grad_norm = 0.001512
  l2.weight: grad_norm = 0.217824
Total gradient norm: 0.568967
=== Actor Training Debug (Iteration 8597) ===
Q mean: -13.677858
Q std: 20.019989
Actor loss: 13.681825
Action reg: 0.003966
  l1.weight: grad_norm = 0.193927
  l1.bias: grad_norm = 0.001117
  l2.weight: grad_norm = 0.151220
Total gradient norm: 0.384416
=== Actor Training Debug (Iteration 8598) ===
Q mean: -14.816008
Q std: 21.310427
Actor loss: 14.819964
Action reg: 0.003957
  l1.weight: grad_norm = 0.388797
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.268709
Total gradient norm: 0.742730
=== Actor Training Debug (Iteration 8599) ===
Q mean: -16.710197
Q std: 21.058767
Actor loss: 16.714153
Action reg: 0.003956
  l1.weight: grad_norm = 0.282694
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.222327
Total gradient norm: 0.588033
=== Actor Training Debug (Iteration 8600) ===
Q mean: -15.877901
Q std: 21.612995
Actor loss: 15.881858
Action reg: 0.003957
  l1.weight: grad_norm = 0.249478
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.169582
Total gradient norm: 0.446608
=== Actor Training Debug (Iteration 8601) ===
Q mean: -15.341419
Q std: 19.934847
Actor loss: 15.345379
Action reg: 0.003959
  l1.weight: grad_norm = 0.268207
  l1.bias: grad_norm = 0.003154
  l2.weight: grad_norm = 0.193032
Total gradient norm: 0.559556
=== Actor Training Debug (Iteration 8602) ===
Q mean: -15.172556
Q std: 21.151447
Actor loss: 15.176529
Action reg: 0.003973
  l1.weight: grad_norm = 0.135381
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.100314
Total gradient norm: 0.254661
=== Actor Training Debug (Iteration 8603) ===
Q mean: -12.468638
Q std: 18.478563
Actor loss: 12.472577
Action reg: 0.003939
  l1.weight: grad_norm = 0.428215
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.296410
Total gradient norm: 0.791115
=== Actor Training Debug (Iteration 8604) ===
Q mean: -15.582579
Q std: 21.370913
Actor loss: 15.586524
Action reg: 0.003945
  l1.weight: grad_norm = 0.288662
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.212537
Total gradient norm: 0.664165
=== Actor Training Debug (Iteration 8605) ===
Q mean: -15.026128
Q std: 21.318441
Actor loss: 15.030107
Action reg: 0.003980
  l1.weight: grad_norm = 0.252077
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.181343
Total gradient norm: 0.492523
=== Actor Training Debug (Iteration 8606) ===
Q mean: -14.687569
Q std: 19.660658
Actor loss: 14.691511
Action reg: 0.003943
  l1.weight: grad_norm = 0.459489
  l1.bias: grad_norm = 0.000702
  l2.weight: grad_norm = 0.395086
Total gradient norm: 1.368975
=== Actor Training Debug (Iteration 8607) ===
Q mean: -13.323191
Q std: 20.220568
Actor loss: 13.327155
Action reg: 0.003965
  l1.weight: grad_norm = 0.204597
  l1.bias: grad_norm = 0.002205
  l2.weight: grad_norm = 0.143575
Total gradient norm: 0.356542
=== Actor Training Debug (Iteration 8608) ===
Q mean: -12.089932
Q std: 18.857132
Actor loss: 12.093881
Action reg: 0.003948
  l1.weight: grad_norm = 0.342530
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.218706
Total gradient norm: 0.608462
=== Actor Training Debug (Iteration 8609) ===
Q mean: -12.898634
Q std: 19.843304
Actor loss: 12.902571
Action reg: 0.003936
  l1.weight: grad_norm = 0.550016
  l1.bias: grad_norm = 0.002416
  l2.weight: grad_norm = 0.371131
Total gradient norm: 1.010084
=== Actor Training Debug (Iteration 8610) ===
Q mean: -13.562534
Q std: 19.867523
Actor loss: 13.566462
Action reg: 0.003927
  l1.weight: grad_norm = 0.238462
  l1.bias: grad_norm = 0.005101
  l2.weight: grad_norm = 0.162309
Total gradient norm: 0.466340
=== Actor Training Debug (Iteration 8611) ===
Q mean: -14.717297
Q std: 20.753910
Actor loss: 14.721264
Action reg: 0.003968
  l1.weight: grad_norm = 0.312001
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.195122
Total gradient norm: 0.528248
=== Actor Training Debug (Iteration 8612) ===
Q mean: -13.226150
Q std: 20.078238
Actor loss: 13.230075
Action reg: 0.003925
  l1.weight: grad_norm = 0.188773
  l1.bias: grad_norm = 0.000680
  l2.weight: grad_norm = 0.163728
Total gradient norm: 0.446792
=== Actor Training Debug (Iteration 8613) ===
Q mean: -14.663498
Q std: 19.994762
Actor loss: 14.667465
Action reg: 0.003967
  l1.weight: grad_norm = 0.237439
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.184374
Total gradient norm: 0.489317
=== Actor Training Debug (Iteration 8614) ===
Q mean: -13.772154
Q std: 20.983551
Actor loss: 13.776120
Action reg: 0.003967
  l1.weight: grad_norm = 0.443922
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.312496
Total gradient norm: 0.973150
=== Actor Training Debug (Iteration 8615) ===
Q mean: -15.283663
Q std: 20.521450
Actor loss: 15.287623
Action reg: 0.003961
  l1.weight: grad_norm = 0.134662
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.103740
Total gradient norm: 0.284236
=== Actor Training Debug (Iteration 8616) ===
Q mean: -14.817840
Q std: 21.588470
Actor loss: 14.821795
Action reg: 0.003955
  l1.weight: grad_norm = 0.300678
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.223757
Total gradient norm: 0.551308
=== Actor Training Debug (Iteration 8617) ===
Q mean: -13.480127
Q std: 19.502405
Actor loss: 13.484099
Action reg: 0.003972
  l1.weight: grad_norm = 0.084685
  l1.bias: grad_norm = 0.000623
  l2.weight: grad_norm = 0.061900
Total gradient norm: 0.165521
=== Actor Training Debug (Iteration 8618) ===
Q mean: -11.913130
Q std: 17.977114
Actor loss: 11.917080
Action reg: 0.003950
  l1.weight: grad_norm = 0.373571
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.229946
Total gradient norm: 0.586506
=== Actor Training Debug (Iteration 8619) ===
Q mean: -12.991251
Q std: 19.406908
Actor loss: 12.995188
Action reg: 0.003937
  l1.weight: grad_norm = 0.331765
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.243298
Total gradient norm: 0.650562
=== Actor Training Debug (Iteration 8620) ===
Q mean: -13.902279
Q std: 20.250374
Actor loss: 13.906251
Action reg: 0.003973
  l1.weight: grad_norm = 0.248457
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.187041
Total gradient norm: 0.638620
=== Actor Training Debug (Iteration 8621) ===
Q mean: -15.390057
Q std: 20.535475
Actor loss: 15.394038
Action reg: 0.003982
  l1.weight: grad_norm = 0.122926
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.101878
Total gradient norm: 0.249211
=== Actor Training Debug (Iteration 8622) ===
Q mean: -14.585473
Q std: 20.787996
Actor loss: 14.589430
Action reg: 0.003957
  l1.weight: grad_norm = 0.252437
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.203870
Total gradient norm: 0.602817
=== Actor Training Debug (Iteration 8623) ===
Q mean: -14.502913
Q std: 20.311529
Actor loss: 14.506855
Action reg: 0.003942
  l1.weight: grad_norm = 0.304925
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.226921
Total gradient norm: 0.601894
=== Actor Training Debug (Iteration 8624) ===
Q mean: -13.679916
Q std: 20.651964
Actor loss: 13.683858
Action reg: 0.003941
  l1.weight: grad_norm = 0.417695
  l1.bias: grad_norm = 0.002309
  l2.weight: grad_norm = 0.264143
Total gradient norm: 0.898380
=== Actor Training Debug (Iteration 8625) ===
Q mean: -15.501724
Q std: 21.260328
Actor loss: 15.505695
Action reg: 0.003972
  l1.weight: grad_norm = 0.190661
  l1.bias: grad_norm = 0.000561
  l2.weight: grad_norm = 0.149369
Total gradient norm: 0.412065
=== Actor Training Debug (Iteration 8626) ===
Q mean: -15.572404
Q std: 20.680368
Actor loss: 15.576371
Action reg: 0.003968
  l1.weight: grad_norm = 0.384061
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.250747
Total gradient norm: 0.702651
=== Actor Training Debug (Iteration 8627) ===
Q mean: -15.204433
Q std: 20.264488
Actor loss: 15.208397
Action reg: 0.003963
  l1.weight: grad_norm = 0.216055
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.147607
Total gradient norm: 0.368950
=== Actor Training Debug (Iteration 8628) ===
Q mean: -13.133106
Q std: 19.967270
Actor loss: 13.137082
Action reg: 0.003976
  l1.weight: grad_norm = 0.253556
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.182598
Total gradient norm: 0.496167
=== Actor Training Debug (Iteration 8629) ===
Q mean: -16.184771
Q std: 22.284389
Actor loss: 16.188719
Action reg: 0.003949
  l1.weight: grad_norm = 0.404701
  l1.bias: grad_norm = 0.004764
  l2.weight: grad_norm = 0.298960
Total gradient norm: 0.868468
=== Actor Training Debug (Iteration 8630) ===
Q mean: -13.875696
Q std: 20.873564
Actor loss: 13.879659
Action reg: 0.003962
  l1.weight: grad_norm = 0.304105
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.217737
Total gradient norm: 0.612189
=== Actor Training Debug (Iteration 8631) ===
Q mean: -15.105539
Q std: 22.097155
Actor loss: 15.109513
Action reg: 0.003974
  l1.weight: grad_norm = 0.182802
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.135595
Total gradient norm: 0.417028
=== Actor Training Debug (Iteration 8632) ===
Q mean: -16.051426
Q std: 22.895901
Actor loss: 16.055342
Action reg: 0.003916
  l1.weight: grad_norm = 0.206671
  l1.bias: grad_norm = 0.003781
  l2.weight: grad_norm = 0.176923
Total gradient norm: 0.536375
=== Actor Training Debug (Iteration 8633) ===
Q mean: -15.970758
Q std: 22.143236
Actor loss: 15.974710
Action reg: 0.003951
  l1.weight: grad_norm = 0.235257
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.152497
Total gradient norm: 0.427890
=== Actor Training Debug (Iteration 8634) ===
Q mean: -16.854403
Q std: 21.712868
Actor loss: 16.858343
Action reg: 0.003940
  l1.weight: grad_norm = 0.159622
  l1.bias: grad_norm = 0.003518
  l2.weight: grad_norm = 0.114008
Total gradient norm: 0.331112
=== Actor Training Debug (Iteration 8635) ===
Q mean: -17.413668
Q std: 22.644005
Actor loss: 17.417612
Action reg: 0.003944
  l1.weight: grad_norm = 0.323095
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.242735
Total gradient norm: 0.593317
=== Actor Training Debug (Iteration 8636) ===
Q mean: -15.728316
Q std: 21.891956
Actor loss: 15.732274
Action reg: 0.003957
  l1.weight: grad_norm = 0.147880
  l1.bias: grad_norm = 0.000484
  l2.weight: grad_norm = 0.108596
Total gradient norm: 0.310860
=== Actor Training Debug (Iteration 8637) ===
Q mean: -14.707958
Q std: 20.577774
Actor loss: 14.711902
Action reg: 0.003944
  l1.weight: grad_norm = 0.272726
  l1.bias: grad_norm = 0.000853
  l2.weight: grad_norm = 0.213314
Total gradient norm: 0.591395
=== Actor Training Debug (Iteration 8638) ===
Q mean: -13.926416
Q std: 20.803747
Actor loss: 13.930389
Action reg: 0.003973
  l1.weight: grad_norm = 0.242020
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.186676
Total gradient norm: 0.500486
=== Actor Training Debug (Iteration 8639) ===
Q mean: -14.349930
Q std: 20.676491
Actor loss: 14.353833
Action reg: 0.003904
  l1.weight: grad_norm = 0.249670
  l1.bias: grad_norm = 0.002944
  l2.weight: grad_norm = 0.167615
Total gradient norm: 0.507741
=== Actor Training Debug (Iteration 8640) ===
Q mean: -16.330265
Q std: 21.989372
Actor loss: 16.334215
Action reg: 0.003950
  l1.weight: grad_norm = 0.222580
  l1.bias: grad_norm = 0.001478
  l2.weight: grad_norm = 0.159670
Total gradient norm: 0.478985
=== Actor Training Debug (Iteration 8641) ===
Q mean: -14.766031
Q std: 21.934977
Actor loss: 14.769996
Action reg: 0.003965
  l1.weight: grad_norm = 0.290117
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.204621
Total gradient norm: 0.530239
=== Actor Training Debug (Iteration 8642) ===
Q mean: -16.795811
Q std: 23.082567
Actor loss: 16.799791
Action reg: 0.003982
  l1.weight: grad_norm = 0.124374
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.093581
Total gradient norm: 0.242742
=== Actor Training Debug (Iteration 8643) ===
Q mean: -13.900243
Q std: 19.429184
Actor loss: 13.904199
Action reg: 0.003956
  l1.weight: grad_norm = 0.277953
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.226775
Total gradient norm: 0.693802
=== Actor Training Debug (Iteration 8644) ===
Q mean: -17.224083
Q std: 23.335228
Actor loss: 17.228031
Action reg: 0.003948
  l1.weight: grad_norm = 0.489940
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.362149
Total gradient norm: 0.948808
=== Actor Training Debug (Iteration 8645) ===
Q mean: -14.913431
Q std: 20.573458
Actor loss: 14.917390
Action reg: 0.003959
  l1.weight: grad_norm = 0.267506
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.230787
Total gradient norm: 0.575651
=== Actor Training Debug (Iteration 8646) ===
Q mean: -17.019840
Q std: 22.592522
Actor loss: 17.023792
Action reg: 0.003952
  l1.weight: grad_norm = 0.129683
  l1.bias: grad_norm = 0.001778
  l2.weight: grad_norm = 0.094564
Total gradient norm: 0.262417
=== Actor Training Debug (Iteration 8647) ===
Q mean: -16.066963
Q std: 21.472509
Actor loss: 16.070896
Action reg: 0.003933
  l1.weight: grad_norm = 0.417801
  l1.bias: grad_norm = 0.000888
  l2.weight: grad_norm = 0.331599
Total gradient norm: 0.899196
=== Actor Training Debug (Iteration 8648) ===
Q mean: -16.281372
Q std: 22.481096
Actor loss: 16.285332
Action reg: 0.003960
  l1.weight: grad_norm = 0.277968
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.207831
Total gradient norm: 0.523455
=== Actor Training Debug (Iteration 8649) ===
Q mean: -14.423732
Q std: 21.380102
Actor loss: 14.427675
Action reg: 0.003943
  l1.weight: grad_norm = 0.342599
  l1.bias: grad_norm = 0.000589
  l2.weight: grad_norm = 0.243630
Total gradient norm: 0.755598
=== Actor Training Debug (Iteration 8650) ===
Q mean: -17.691099
Q std: 21.022633
Actor loss: 17.695078
Action reg: 0.003979
  l1.weight: grad_norm = 0.375836
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.262489
Total gradient norm: 0.755042
=== Actor Training Debug (Iteration 8651) ===
Q mean: -14.613235
Q std: 20.649939
Actor loss: 14.617211
Action reg: 0.003977
  l1.weight: grad_norm = 0.219697
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.175877
Total gradient norm: 0.548785
=== Actor Training Debug (Iteration 8652) ===
Q mean: -14.946085
Q std: 20.948763
Actor loss: 14.950056
Action reg: 0.003971
  l1.weight: grad_norm = 0.279484
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.173743
Total gradient norm: 0.566939
=== Actor Training Debug (Iteration 8653) ===
Q mean: -13.333055
Q std: 19.551498
Actor loss: 13.337011
Action reg: 0.003956
  l1.weight: grad_norm = 0.187110
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.134230
Total gradient norm: 0.339735
=== Actor Training Debug (Iteration 8654) ===
Q mean: -12.300810
Q std: 18.594154
Actor loss: 12.304749
Action reg: 0.003939
  l1.weight: grad_norm = 0.290906
  l1.bias: grad_norm = 0.002342
  l2.weight: grad_norm = 0.259231
Total gradient norm: 0.727822
=== Actor Training Debug (Iteration 8655) ===
Q mean: -13.769461
Q std: 21.463497
Actor loss: 13.773376
Action reg: 0.003915
  l1.weight: grad_norm = 0.367089
  l1.bias: grad_norm = 0.000510
  l2.weight: grad_norm = 0.264848
Total gradient norm: 0.792174
=== Actor Training Debug (Iteration 8656) ===
Q mean: -14.727163
Q std: 20.136343
Actor loss: 14.731129
Action reg: 0.003965
  l1.weight: grad_norm = 0.152524
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.101958
Total gradient norm: 0.258684
=== Actor Training Debug (Iteration 8657) ===
Q mean: -13.552311
Q std: 19.382103
Actor loss: 13.556264
Action reg: 0.003953
  l1.weight: grad_norm = 0.254863
  l1.bias: grad_norm = 0.001604
  l2.weight: grad_norm = 0.190355
Total gradient norm: 0.490552
=== Actor Training Debug (Iteration 8658) ===
Q mean: -16.323130
Q std: 23.314596
Actor loss: 16.327087
Action reg: 0.003958
  l1.weight: grad_norm = 0.046837
  l1.bias: grad_norm = 0.000788
  l2.weight: grad_norm = 0.036729
Total gradient norm: 0.109040
=== Actor Training Debug (Iteration 8659) ===
Q mean: -13.547810
Q std: 20.460188
Actor loss: 13.551773
Action reg: 0.003963
  l1.weight: grad_norm = 0.277394
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.194003
Total gradient norm: 0.499820
=== Actor Training Debug (Iteration 8660) ===
Q mean: -14.833807
Q std: 20.270548
Actor loss: 14.837776
Action reg: 0.003970
  l1.weight: grad_norm = 0.423635
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.359489
Total gradient norm: 0.981032
=== Actor Training Debug (Iteration 8661) ===
Q mean: -15.361353
Q std: 21.620888
Actor loss: 15.365302
Action reg: 0.003949
  l1.weight: grad_norm = 0.289101
  l1.bias: grad_norm = 0.000824
  l2.weight: grad_norm = 0.216614
Total gradient norm: 0.592063
=== Actor Training Debug (Iteration 8662) ===
Q mean: -13.961558
Q std: 20.582914
Actor loss: 13.965532
Action reg: 0.003974
  l1.weight: grad_norm = 0.182165
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.134069
Total gradient norm: 0.360150
=== Actor Training Debug (Iteration 8663) ===
Q mean: -15.087534
Q std: 20.456800
Actor loss: 15.091498
Action reg: 0.003964
  l1.weight: grad_norm = 0.305468
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.239479
Total gradient norm: 0.709736
=== Actor Training Debug (Iteration 8664) ===
Q mean: -14.486158
Q std: 21.297634
Actor loss: 14.490108
Action reg: 0.003950
  l1.weight: grad_norm = 0.579560
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.367537
Total gradient norm: 1.071057
=== Actor Training Debug (Iteration 8665) ===
Q mean: -15.817394
Q std: 21.412472
Actor loss: 15.821353
Action reg: 0.003959
  l1.weight: grad_norm = 0.323426
  l1.bias: grad_norm = 0.001812
  l2.weight: grad_norm = 0.214645
Total gradient norm: 0.581224
=== Actor Training Debug (Iteration 8666) ===
Q mean: -13.709549
Q std: 19.389494
Actor loss: 13.713512
Action reg: 0.003963
  l1.weight: grad_norm = 0.217399
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.175597
Total gradient norm: 0.463247
=== Actor Training Debug (Iteration 8667) ===
Q mean: -14.435797
Q std: 20.683933
Actor loss: 14.439743
Action reg: 0.003946
  l1.weight: grad_norm = 0.226526
  l1.bias: grad_norm = 0.000684
  l2.weight: grad_norm = 0.198098
Total gradient norm: 0.535740
=== Actor Training Debug (Iteration 8668) ===
Q mean: -13.826136
Q std: 22.275270
Actor loss: 13.830060
Action reg: 0.003925
  l1.weight: grad_norm = 0.269482
  l1.bias: grad_norm = 0.000750
  l2.weight: grad_norm = 0.202425
Total gradient norm: 0.693489
=== Actor Training Debug (Iteration 8669) ===
Q mean: -15.723637
Q std: 20.903282
Actor loss: 15.727609
Action reg: 0.003972
  l1.weight: grad_norm = 0.224717
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.180051
Total gradient norm: 0.602782
=== Actor Training Debug (Iteration 8670) ===
Q mean: -16.976849
Q std: 23.219944
Actor loss: 16.980791
Action reg: 0.003943
  l1.weight: grad_norm = 0.372839
  l1.bias: grad_norm = 0.000656
  l2.weight: grad_norm = 0.256358
Total gradient norm: 0.689986
=== Actor Training Debug (Iteration 8671) ===
Q mean: -16.227179
Q std: 21.823479
Actor loss: 16.231134
Action reg: 0.003957
  l1.weight: grad_norm = 0.250312
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.182118
Total gradient norm: 0.491570
=== Actor Training Debug (Iteration 8672) ===
Q mean: -15.235867
Q std: 20.348391
Actor loss: 15.239826
Action reg: 0.003960
  l1.weight: grad_norm = 0.193458
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.127745
Total gradient norm: 0.351271
=== Actor Training Debug (Iteration 8673) ===
Q mean: -14.480376
Q std: 21.697002
Actor loss: 14.484329
Action reg: 0.003953
  l1.weight: grad_norm = 0.246827
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.202492
Total gradient norm: 0.587981
=== Actor Training Debug (Iteration 8674) ===
Q mean: -13.102670
Q std: 20.400358
Actor loss: 13.106631
Action reg: 0.003962
  l1.weight: grad_norm = 0.435972
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.304210
Total gradient norm: 0.846459
=== Actor Training Debug (Iteration 8675) ===
Q mean: -11.442099
Q std: 18.966639
Actor loss: 11.446044
Action reg: 0.003945
  l1.weight: grad_norm = 0.306950
  l1.bias: grad_norm = 0.001747
  l2.weight: grad_norm = 0.251813
Total gradient norm: 0.740195
=== Actor Training Debug (Iteration 8676) ===
Q mean: -14.835687
Q std: 21.570059
Actor loss: 14.839647
Action reg: 0.003961
  l1.weight: grad_norm = 0.106085
  l1.bias: grad_norm = 0.001575
  l2.weight: grad_norm = 0.073558
Total gradient norm: 0.226262
=== Actor Training Debug (Iteration 8677) ===
Q mean: -14.171574
Q std: 20.760761
Actor loss: 14.175529
Action reg: 0.003955
  l1.weight: grad_norm = 0.249884
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.195395
Total gradient norm: 0.538871
=== Actor Training Debug (Iteration 8678) ===
Q mean: -13.450033
Q std: 20.002893
Actor loss: 13.453971
Action reg: 0.003938
  l1.weight: grad_norm = 0.324667
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.244897
Total gradient norm: 0.667464
=== Actor Training Debug (Iteration 8679) ===
Q mean: -14.521345
Q std: 20.100523
Actor loss: 14.525290
Action reg: 0.003944
  l1.weight: grad_norm = 0.287427
  l1.bias: grad_norm = 0.004146
  l2.weight: grad_norm = 0.212327
Total gradient norm: 0.538823
=== Actor Training Debug (Iteration 8680) ===
Q mean: -16.612434
Q std: 22.464094
Actor loss: 16.616388
Action reg: 0.003953
  l1.weight: grad_norm = 0.235699
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.183330
Total gradient norm: 0.543056
=== Actor Training Debug (Iteration 8681) ===
Q mean: -15.584237
Q std: 22.047234
Actor loss: 15.588175
Action reg: 0.003938
  l1.weight: grad_norm = 0.339201
  l1.bias: grad_norm = 0.002416
  l2.weight: grad_norm = 0.290552
Total gradient norm: 0.821714
=== Actor Training Debug (Iteration 8682) ===
Q mean: -14.330007
Q std: 20.602859
Actor loss: 14.333968
Action reg: 0.003961
  l1.weight: grad_norm = 0.283149
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.222634
Total gradient norm: 0.764527
=== Actor Training Debug (Iteration 8683) ===
Q mean: -11.743774
Q std: 18.711187
Actor loss: 11.747746
Action reg: 0.003972
  l1.weight: grad_norm = 0.520346
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.336255
Total gradient norm: 0.948352
=== Actor Training Debug (Iteration 8684) ===
Q mean: -15.546122
Q std: 19.999134
Actor loss: 15.550077
Action reg: 0.003956
  l1.weight: grad_norm = 0.335484
  l1.bias: grad_norm = 0.004203
  l2.weight: grad_norm = 0.246269
Total gradient norm: 0.693666
=== Actor Training Debug (Iteration 8685) ===
Q mean: -14.432709
Q std: 19.724052
Actor loss: 14.436668
Action reg: 0.003959
  l1.weight: grad_norm = 0.256066
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.178122
Total gradient norm: 0.429305
=== Actor Training Debug (Iteration 8686) ===
Q mean: -17.116764
Q std: 22.277010
Actor loss: 17.120731
Action reg: 0.003967
  l1.weight: grad_norm = 0.290751
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.213580
Total gradient norm: 0.572675
=== Actor Training Debug (Iteration 8687) ===
Q mean: -14.089803
Q std: 18.963657
Actor loss: 14.093730
Action reg: 0.003927
  l1.weight: grad_norm = 0.210540
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.162065
Total gradient norm: 0.417615
=== Actor Training Debug (Iteration 8688) ===
Q mean: -12.816077
Q std: 17.268383
Actor loss: 12.820052
Action reg: 0.003975
  l1.weight: grad_norm = 0.161765
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.133366
Total gradient norm: 0.371169
=== Actor Training Debug (Iteration 8689) ===
Q mean: -13.900618
Q std: 20.709969
Actor loss: 13.904553
Action reg: 0.003936
  l1.weight: grad_norm = 0.270820
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.210503
Total gradient norm: 0.626131
=== Actor Training Debug (Iteration 8690) ===
Q mean: -13.558858
Q std: 20.089558
Actor loss: 13.562814
Action reg: 0.003956
  l1.weight: grad_norm = 0.244199
  l1.bias: grad_norm = 0.001952
  l2.weight: grad_norm = 0.191470
Total gradient norm: 0.501120
=== Actor Training Debug (Iteration 8691) ===
Q mean: -14.293083
Q std: 20.988241
Actor loss: 14.297043
Action reg: 0.003960
  l1.weight: grad_norm = 0.203847
  l1.bias: grad_norm = 0.002114
  l2.weight: grad_norm = 0.153983
Total gradient norm: 0.477934
=== Actor Training Debug (Iteration 8692) ===
Q mean: -13.863161
Q std: 18.642097
Actor loss: 13.867135
Action reg: 0.003974
  l1.weight: grad_norm = 0.126535
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.087052
Total gradient norm: 0.256744
=== Actor Training Debug (Iteration 8693) ===
Q mean: -17.786570
Q std: 23.682983
Actor loss: 17.790535
Action reg: 0.003965
  l1.weight: grad_norm = 0.192866
  l1.bias: grad_norm = 0.001845
  l2.weight: grad_norm = 0.144658
Total gradient norm: 0.412693
=== Actor Training Debug (Iteration 8694) ===
Q mean: -12.964155
Q std: 19.352854
Actor loss: 12.968081
Action reg: 0.003925
  l1.weight: grad_norm = 0.222588
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.169520
Total gradient norm: 0.464722
=== Actor Training Debug (Iteration 8695) ===
Q mean: -14.393293
Q std: 20.758135
Actor loss: 14.397257
Action reg: 0.003964
  l1.weight: grad_norm = 0.131570
  l1.bias: grad_norm = 0.000428
  l2.weight: grad_norm = 0.096457
Total gradient norm: 0.267314
=== Actor Training Debug (Iteration 8696) ===
Q mean: -15.000769
Q std: 22.982204
Actor loss: 15.004740
Action reg: 0.003971
  l1.weight: grad_norm = 0.385735
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.302309
Total gradient norm: 0.887819
=== Actor Training Debug (Iteration 8697) ===
Q mean: -15.127567
Q std: 21.460098
Actor loss: 15.131530
Action reg: 0.003963
  l1.weight: grad_norm = 0.263014
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.187817
Total gradient norm: 0.479689
=== Actor Training Debug (Iteration 8698) ===
Q mean: -14.959140
Q std: 21.755613
Actor loss: 14.963098
Action reg: 0.003958
  l1.weight: grad_norm = 0.355530
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.293677
Total gradient norm: 0.751918
=== Actor Training Debug (Iteration 8699) ===
Q mean: -15.508642
Q std: 20.746681
Actor loss: 15.512567
Action reg: 0.003925
  l1.weight: grad_norm = 0.191927
  l1.bias: grad_norm = 0.000525
  l2.weight: grad_norm = 0.139933
Total gradient norm: 0.403261
=== Actor Training Debug (Iteration 8700) ===
Q mean: -13.412333
Q std: 21.426987
Actor loss: 13.416287
Action reg: 0.003955
  l1.weight: grad_norm = 0.178062
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.137906
Total gradient norm: 0.386556
=== Actor Training Debug (Iteration 8701) ===
Q mean: -13.633677
Q std: 21.459444
Actor loss: 13.637637
Action reg: 0.003959
  l1.weight: grad_norm = 0.121136
  l1.bias: grad_norm = 0.002449
  l2.weight: grad_norm = 0.095609
Total gradient norm: 0.269160
=== Actor Training Debug (Iteration 8702) ===
Q mean: -15.263808
Q std: 20.939568
Actor loss: 15.267779
Action reg: 0.003971
  l1.weight: grad_norm = 0.251799
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.181700
Total gradient norm: 0.507849
=== Actor Training Debug (Iteration 8703) ===
Q mean: -13.284199
Q std: 19.897829
Actor loss: 13.288149
Action reg: 0.003950
  l1.weight: grad_norm = 0.192618
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.152437
Total gradient norm: 0.451903
=== Actor Training Debug (Iteration 8704) ===
Q mean: -14.438883
Q std: 19.881155
Actor loss: 14.442817
Action reg: 0.003934
  l1.weight: grad_norm = 0.365379
  l1.bias: grad_norm = 0.000975
  l2.weight: grad_norm = 0.294749
Total gradient norm: 0.868564
=== Actor Training Debug (Iteration 8705) ===
Q mean: -14.305191
Q std: 21.789509
Actor loss: 14.309134
Action reg: 0.003943
  l1.weight: grad_norm = 0.289287
  l1.bias: grad_norm = 0.000544
  l2.weight: grad_norm = 0.247754
Total gradient norm: 0.691522
=== Actor Training Debug (Iteration 8706) ===
Q mean: -15.868493
Q std: 21.267591
Actor loss: 15.872462
Action reg: 0.003969
  l1.weight: grad_norm = 0.165770
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.136111
Total gradient norm: 0.344478
=== Actor Training Debug (Iteration 8707) ===
Q mean: -14.915935
Q std: 20.131199
Actor loss: 14.919890
Action reg: 0.003955
  l1.weight: grad_norm = 0.229690
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.209873
Total gradient norm: 0.588699
=== Actor Training Debug (Iteration 8708) ===
Q mean: -16.146755
Q std: 21.422237
Actor loss: 16.150703
Action reg: 0.003948
  l1.weight: grad_norm = 0.332108
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.226225
Total gradient norm: 0.652672
=== Actor Training Debug (Iteration 8709) ===
Q mean: -15.655646
Q std: 21.732164
Actor loss: 15.659613
Action reg: 0.003966
  l1.weight: grad_norm = 0.290212
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.222891
Total gradient norm: 0.582874
=== Actor Training Debug (Iteration 8710) ===
Q mean: -14.763983
Q std: 22.070511
Actor loss: 14.767960
Action reg: 0.003977
  l1.weight: grad_norm = 0.260789
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.199196
Total gradient norm: 0.624782
=== Actor Training Debug (Iteration 8711) ===
Q mean: -14.425612
Q std: 21.049538
Actor loss: 14.429551
Action reg: 0.003938
  l1.weight: grad_norm = 0.151486
  l1.bias: grad_norm = 0.001247
  l2.weight: grad_norm = 0.119367
Total gradient norm: 0.319330
=== Actor Training Debug (Iteration 8712) ===
Q mean: -15.668354
Q std: 23.172550
Actor loss: 15.672306
Action reg: 0.003952
  l1.weight: grad_norm = 0.327475
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.242454
Total gradient norm: 0.665232
=== Actor Training Debug (Iteration 8713) ===
Q mean: -16.109049
Q std: 21.569706
Actor loss: 16.113012
Action reg: 0.003964
  l1.weight: grad_norm = 0.239688
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.193836
Total gradient norm: 0.532355
=== Actor Training Debug (Iteration 8714) ===
Q mean: -14.190777
Q std: 19.707083
Actor loss: 14.194717
Action reg: 0.003941
  l1.weight: grad_norm = 0.166485
  l1.bias: grad_norm = 0.000538
  l2.weight: grad_norm = 0.115673
Total gradient norm: 0.304815
=== Actor Training Debug (Iteration 8715) ===
Q mean: -14.221999
Q std: 19.983583
Actor loss: 14.225950
Action reg: 0.003951
  l1.weight: grad_norm = 0.401592
  l1.bias: grad_norm = 0.000964
  l2.weight: grad_norm = 0.259807
Total gradient norm: 0.725272
=== Actor Training Debug (Iteration 8716) ===
Q mean: -16.226166
Q std: 22.787724
Actor loss: 16.230127
Action reg: 0.003962
  l1.weight: grad_norm = 0.222442
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.157711
Total gradient norm: 0.479930
=== Actor Training Debug (Iteration 8717) ===
Q mean: -15.720173
Q std: 22.364170
Actor loss: 15.724135
Action reg: 0.003963
  l1.weight: grad_norm = 0.175453
  l1.bias: grad_norm = 0.001887
  l2.weight: grad_norm = 0.132532
Total gradient norm: 0.347383
=== Actor Training Debug (Iteration 8718) ===
Q mean: -16.280560
Q std: 22.673550
Actor loss: 16.284529
Action reg: 0.003970
  l1.weight: grad_norm = 0.184404
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.129234
Total gradient norm: 0.352338
=== Actor Training Debug (Iteration 8719) ===
Q mean: -11.512094
Q std: 18.323711
Actor loss: 11.516065
Action reg: 0.003971
  l1.weight: grad_norm = 0.126841
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.095774
Total gradient norm: 0.255700
=== Actor Training Debug (Iteration 8720) ===
Q mean: -15.677117
Q std: 20.339510
Actor loss: 15.681086
Action reg: 0.003969
  l1.weight: grad_norm = 0.361841
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.246069
Total gradient norm: 0.663992
=== Actor Training Debug (Iteration 8721) ===
Q mean: -14.805593
Q std: 21.471968
Actor loss: 14.809552
Action reg: 0.003958
  l1.weight: grad_norm = 0.228659
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.153197
Total gradient norm: 0.406090
=== Actor Training Debug (Iteration 8722) ===
Q mean: -12.221581
Q std: 18.434603
Actor loss: 12.225528
Action reg: 0.003946
  l1.weight: grad_norm = 0.340915
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.254878
Total gradient norm: 0.660577
=== Actor Training Debug (Iteration 8723) ===
Q mean: -14.994533
Q std: 21.230827
Actor loss: 14.998487
Action reg: 0.003955
  l1.weight: grad_norm = 0.178740
  l1.bias: grad_norm = 0.000769
  l2.weight: grad_norm = 0.127131
Total gradient norm: 0.362154
=== Actor Training Debug (Iteration 8724) ===
Q mean: -15.820008
Q std: 21.849800
Actor loss: 15.823962
Action reg: 0.003954
  l1.weight: grad_norm = 0.233403
  l1.bias: grad_norm = 0.001367
  l2.weight: grad_norm = 0.163608
Total gradient norm: 0.452041
=== Actor Training Debug (Iteration 8725) ===
Q mean: -11.944849
Q std: 18.334904
Actor loss: 11.948802
Action reg: 0.003953
  l1.weight: grad_norm = 0.421163
  l1.bias: grad_norm = 0.002162
  l2.weight: grad_norm = 0.292674
Total gradient norm: 0.820314
=== Actor Training Debug (Iteration 8726) ===
Q mean: -14.166012
Q std: 19.981783
Actor loss: 14.169980
Action reg: 0.003968
  l1.weight: grad_norm = 0.362432
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.286334
Total gradient norm: 0.803249
=== Actor Training Debug (Iteration 8727) ===
Q mean: -16.791531
Q std: 22.635744
Actor loss: 16.795506
Action reg: 0.003974
  l1.weight: grad_norm = 0.428784
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.336863
Total gradient norm: 0.952841
=== Actor Training Debug (Iteration 8728) ===
Q mean: -17.656109
Q std: 23.126249
Actor loss: 17.660074
Action reg: 0.003966
  l1.weight: grad_norm = 0.361330
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.254613
Total gradient norm: 0.652330
=== Actor Training Debug (Iteration 8729) ===
Q mean: -16.551201
Q std: 20.655457
Actor loss: 16.555164
Action reg: 0.003964
  l1.weight: grad_norm = 0.302817
  l1.bias: grad_norm = 0.000588
  l2.weight: grad_norm = 0.249032
Total gradient norm: 0.765244
=== Actor Training Debug (Iteration 8730) ===
Q mean: -15.568343
Q std: 22.837017
Actor loss: 15.572292
Action reg: 0.003950
  l1.weight: grad_norm = 0.426624
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.301018
Total gradient norm: 0.808753
=== Actor Training Debug (Iteration 8731) ===
Q mean: -13.554523
Q std: 20.953667
Actor loss: 13.558474
Action reg: 0.003950
  l1.weight: grad_norm = 0.115172
  l1.bias: grad_norm = 0.000598
  l2.weight: grad_norm = 0.087370
Total gradient norm: 0.233648
=== Actor Training Debug (Iteration 8732) ===
Q mean: -12.086811
Q std: 19.138559
Actor loss: 12.090771
Action reg: 0.003960
  l1.weight: grad_norm = 0.315189
  l1.bias: grad_norm = 0.001435
  l2.weight: grad_norm = 0.255993
Total gradient norm: 0.729693
=== Actor Training Debug (Iteration 8733) ===
Q mean: -13.997014
Q std: 20.436960
Actor loss: 14.000983
Action reg: 0.003970
  l1.weight: grad_norm = 0.107494
  l1.bias: grad_norm = 0.001222
  l2.weight: grad_norm = 0.090339
Total gradient norm: 0.227359
=== Actor Training Debug (Iteration 8734) ===
Q mean: -15.200930
Q std: 21.166477
Actor loss: 15.204890
Action reg: 0.003961
  l1.weight: grad_norm = 0.321874
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.252874
Total gradient norm: 0.754939
=== Actor Training Debug (Iteration 8735) ===
Q mean: -14.580954
Q std: 20.145924
Actor loss: 14.584927
Action reg: 0.003973
  l1.weight: grad_norm = 0.226481
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.163790
Total gradient norm: 0.501918
=== Actor Training Debug (Iteration 8736) ===
Q mean: -14.809824
Q std: 19.659309
Actor loss: 14.813772
Action reg: 0.003948
  l1.weight: grad_norm = 0.234425
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.170643
Total gradient norm: 0.459713
=== Actor Training Debug (Iteration 8737) ===
Q mean: -16.136059
Q std: 22.034721
Actor loss: 16.140028
Action reg: 0.003968
  l1.weight: grad_norm = 0.293025
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.199438
Total gradient norm: 0.530745
=== Actor Training Debug (Iteration 8738) ===
Q mean: -14.776728
Q std: 21.171062
Actor loss: 14.780686
Action reg: 0.003959
  l1.weight: grad_norm = 0.221741
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.165317
Total gradient norm: 0.428719
=== Actor Training Debug (Iteration 8739) ===
Q mean: -14.962084
Q std: 20.054733
Actor loss: 14.966026
Action reg: 0.003942
  l1.weight: grad_norm = 0.230161
  l1.bias: grad_norm = 0.001558
  l2.weight: grad_norm = 0.184201
Total gradient norm: 0.501253
=== Actor Training Debug (Iteration 8740) ===
Q mean: -14.087979
Q std: 20.999386
Actor loss: 14.091942
Action reg: 0.003963
  l1.weight: grad_norm = 0.189682
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.164064
Total gradient norm: 0.410914
=== Actor Training Debug (Iteration 8741) ===
Q mean: -16.370445
Q std: 22.343315
Actor loss: 16.374395
Action reg: 0.003950
  l1.weight: grad_norm = 0.187297
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.135700
Total gradient norm: 0.447893
=== Actor Training Debug (Iteration 8742) ===
Q mean: -15.781834
Q std: 22.360817
Actor loss: 15.785785
Action reg: 0.003951
  l1.weight: grad_norm = 0.260457
  l1.bias: grad_norm = 0.001503
  l2.weight: grad_norm = 0.196165
Total gradient norm: 0.491092
=== Actor Training Debug (Iteration 8743) ===
Q mean: -16.003061
Q std: 22.456715
Actor loss: 16.007034
Action reg: 0.003973
  l1.weight: grad_norm = 0.173441
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.132136
Total gradient norm: 0.355186
=== Actor Training Debug (Iteration 8744) ===
Q mean: -13.155798
Q std: 19.360527
Actor loss: 13.159749
Action reg: 0.003951
  l1.weight: grad_norm = 0.194925
  l1.bias: grad_norm = 0.000680
  l2.weight: grad_norm = 0.146181
Total gradient norm: 0.404668
=== Actor Training Debug (Iteration 8745) ===
Q mean: -11.915072
Q std: 17.887781
Actor loss: 11.919012
Action reg: 0.003939
  l1.weight: grad_norm = 0.156003
  l1.bias: grad_norm = 0.003563
  l2.weight: grad_norm = 0.143227
Total gradient norm: 0.402480
=== Actor Training Debug (Iteration 8746) ===
Q mean: -14.442669
Q std: 21.310652
Actor loss: 14.446636
Action reg: 0.003967
  l1.weight: grad_norm = 0.252219
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.201608
Total gradient norm: 0.503632
=== Actor Training Debug (Iteration 8747) ===
Q mean: -12.738089
Q std: 20.855722
Actor loss: 12.742066
Action reg: 0.003978
  l1.weight: grad_norm = 0.145685
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.125994
Total gradient norm: 0.365928
=== Actor Training Debug (Iteration 8748) ===
Q mean: -14.935635
Q std: 21.802145
Actor loss: 14.939573
Action reg: 0.003939
  l1.weight: grad_norm = 0.425546
  l1.bias: grad_norm = 0.000563
  l2.weight: grad_norm = 0.314400
Total gradient norm: 0.823503
=== Actor Training Debug (Iteration 8749) ===
Q mean: -15.166446
Q std: 20.875381
Actor loss: 15.170397
Action reg: 0.003951
  l1.weight: grad_norm = 0.294692
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.230649
Total gradient norm: 0.583469
=== Actor Training Debug (Iteration 8750) ===
Q mean: -14.706902
Q std: 20.299326
Actor loss: 14.710853
Action reg: 0.003951
  l1.weight: grad_norm = 0.310889
  l1.bias: grad_norm = 0.001956
  l2.weight: grad_norm = 0.272028
Total gradient norm: 0.647733
=== Actor Training Debug (Iteration 8751) ===
Q mean: -13.967524
Q std: 20.660616
Actor loss: 13.971471
Action reg: 0.003948
  l1.weight: grad_norm = 0.181292
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.146102
Total gradient norm: 0.413544
=== Actor Training Debug (Iteration 8752) ===
Q mean: -14.731622
Q std: 21.520374
Actor loss: 14.735592
Action reg: 0.003970
  l1.weight: grad_norm = 0.174189
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.125016
Total gradient norm: 0.333146
=== Actor Training Debug (Iteration 8753) ===
Q mean: -15.522355
Q std: 21.289389
Actor loss: 15.526308
Action reg: 0.003953
  l1.weight: grad_norm = 0.401945
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.290126
Total gradient norm: 0.897064
=== Actor Training Debug (Iteration 8754) ===
Q mean: -12.462121
Q std: 16.435682
Actor loss: 12.466073
Action reg: 0.003952
  l1.weight: grad_norm = 0.261328
  l1.bias: grad_norm = 0.003702
  l2.weight: grad_norm = 0.164508
Total gradient norm: 0.480231
=== Actor Training Debug (Iteration 8755) ===
Q mean: -17.237225
Q std: 23.072405
Actor loss: 17.241179
Action reg: 0.003954
  l1.weight: grad_norm = 0.218216
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.158618
Total gradient norm: 0.401339
=== Actor Training Debug (Iteration 8756) ===
Q mean: -14.333811
Q std: 20.799728
Actor loss: 14.337774
Action reg: 0.003963
  l1.weight: grad_norm = 0.280537
  l1.bias: grad_norm = 0.001818
  l2.weight: grad_norm = 0.194919
Total gradient norm: 0.510937
=== Actor Training Debug (Iteration 8757) ===
Q mean: -14.960962
Q std: 21.309591
Actor loss: 14.964922
Action reg: 0.003960
  l1.weight: grad_norm = 0.478098
  l1.bias: grad_norm = 0.001871
  l2.weight: grad_norm = 0.378289
Total gradient norm: 1.094831
=== Actor Training Debug (Iteration 8758) ===
Q mean: -14.924314
Q std: 22.115948
Actor loss: 14.928240
Action reg: 0.003926
  l1.weight: grad_norm = 0.229236
  l1.bias: grad_norm = 0.001880
  l2.weight: grad_norm = 0.151703
Total gradient norm: 0.441694
=== Actor Training Debug (Iteration 8759) ===
Q mean: -12.680525
Q std: 20.241838
Actor loss: 12.684495
Action reg: 0.003970
  l1.weight: grad_norm = 0.206792
  l1.bias: grad_norm = 0.001744
  l2.weight: grad_norm = 0.159652
Total gradient norm: 0.427731
=== Actor Training Debug (Iteration 8760) ===
Q mean: -12.843505
Q std: 18.203671
Actor loss: 12.847474
Action reg: 0.003969
  l1.weight: grad_norm = 0.409814
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.247875
Total gradient norm: 0.632061
=== Actor Training Debug (Iteration 8761) ===
Q mean: -16.071733
Q std: 21.647537
Actor loss: 16.075680
Action reg: 0.003946
  l1.weight: grad_norm = 0.268558
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.205440
Total gradient norm: 0.518180
=== Actor Training Debug (Iteration 8762) ===
Q mean: -15.991035
Q std: 21.590340
Actor loss: 15.995001
Action reg: 0.003966
  l1.weight: grad_norm = 0.374962
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.293013
Total gradient norm: 0.717807
=== Actor Training Debug (Iteration 8763) ===
Q mean: -13.567734
Q std: 19.623991
Actor loss: 13.571689
Action reg: 0.003955
  l1.weight: grad_norm = 0.198934
  l1.bias: grad_norm = 0.000630
  l2.weight: grad_norm = 0.167408
Total gradient norm: 0.488679
=== Actor Training Debug (Iteration 8764) ===
Q mean: -13.648369
Q std: 19.919472
Actor loss: 13.652312
Action reg: 0.003943
  l1.weight: grad_norm = 0.507785
  l1.bias: grad_norm = 0.000889
  l2.weight: grad_norm = 0.314297
Total gradient norm: 0.863376
=== Actor Training Debug (Iteration 8765) ===
Q mean: -13.127062
Q std: 19.443869
Actor loss: 13.131020
Action reg: 0.003958
  l1.weight: grad_norm = 0.254878
  l1.bias: grad_norm = 0.000495
  l2.weight: grad_norm = 0.187282
Total gradient norm: 0.576354
=== Actor Training Debug (Iteration 8766) ===
Q mean: -15.324514
Q std: 21.028984
Actor loss: 15.328462
Action reg: 0.003947
  l1.weight: grad_norm = 0.179799
  l1.bias: grad_norm = 0.000557
  l2.weight: grad_norm = 0.131129
Total gradient norm: 0.344493
=== Actor Training Debug (Iteration 8767) ===
Q mean: -13.696923
Q std: 19.537722
Actor loss: 13.700851
Action reg: 0.003928
  l1.weight: grad_norm = 0.130270
  l1.bias: grad_norm = 0.001101
  l2.weight: grad_norm = 0.107366
Total gradient norm: 0.315309
=== Actor Training Debug (Iteration 8768) ===
Q mean: -14.164371
Q std: 20.587524
Actor loss: 14.168323
Action reg: 0.003952
  l1.weight: grad_norm = 0.217044
  l1.bias: grad_norm = 0.001662
  l2.weight: grad_norm = 0.162803
Total gradient norm: 0.434774
=== Actor Training Debug (Iteration 8769) ===
Q mean: -13.527325
Q std: 19.679026
Actor loss: 13.531283
Action reg: 0.003959
  l1.weight: grad_norm = 0.223306
  l1.bias: grad_norm = 0.003365
  l2.weight: grad_norm = 0.171720
Total gradient norm: 0.457799
=== Actor Training Debug (Iteration 8770) ===
Q mean: -15.022627
Q std: 20.888067
Actor loss: 15.026545
Action reg: 0.003918
  l1.weight: grad_norm = 0.200014
  l1.bias: grad_norm = 0.003048
  l2.weight: grad_norm = 0.161051
Total gradient norm: 0.438418
=== Actor Training Debug (Iteration 8771) ===
Q mean: -14.640630
Q std: 20.208246
Actor loss: 14.644576
Action reg: 0.003946
  l1.weight: grad_norm = 0.433266
  l1.bias: grad_norm = 0.001841
  l2.weight: grad_norm = 0.408216
Total gradient norm: 1.080286
=== Actor Training Debug (Iteration 8772) ===
Q mean: -12.861223
Q std: 20.541977
Actor loss: 12.865153
Action reg: 0.003930
  l1.weight: grad_norm = 0.214182
  l1.bias: grad_norm = 0.001692
  l2.weight: grad_norm = 0.168438
Total gradient norm: 0.442143
=== Actor Training Debug (Iteration 8773) ===
Q mean: -13.826654
Q std: 19.149986
Actor loss: 13.830571
Action reg: 0.003917
  l1.weight: grad_norm = 0.363057
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.284476
Total gradient norm: 0.700729
=== Actor Training Debug (Iteration 8774) ===
Q mean: -13.528254
Q std: 20.729742
Actor loss: 13.532231
Action reg: 0.003978
  l1.weight: grad_norm = 0.144275
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.102251
Total gradient norm: 0.289707
=== Actor Training Debug (Iteration 8775) ===
Q mean: -14.794319
Q std: 20.557669
Actor loss: 14.798293
Action reg: 0.003973
  l1.weight: grad_norm = 0.180423
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.142868
Total gradient norm: 0.390065
=== Actor Training Debug (Iteration 8776) ===
Q mean: -15.063553
Q std: 20.445702
Actor loss: 15.067507
Action reg: 0.003954
  l1.weight: grad_norm = 0.285983
  l1.bias: grad_norm = 0.001507
  l2.weight: grad_norm = 0.189648
Total gradient norm: 0.491583
=== Actor Training Debug (Iteration 8777) ===
Q mean: -15.106548
Q std: 20.351330
Actor loss: 15.110524
Action reg: 0.003976
  l1.weight: grad_norm = 0.188844
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.126321
Total gradient norm: 0.349136
=== Actor Training Debug (Iteration 8778) ===
Q mean: -13.493501
Q std: 20.927958
Actor loss: 13.497461
Action reg: 0.003961
  l1.weight: grad_norm = 0.136135
  l1.bias: grad_norm = 0.000692
  l2.weight: grad_norm = 0.096483
Total gradient norm: 0.263812
=== Actor Training Debug (Iteration 8779) ===
Q mean: -12.764347
Q std: 20.207287
Actor loss: 12.768308
Action reg: 0.003961
  l1.weight: grad_norm = 0.293059
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.212482
Total gradient norm: 0.581675
=== Actor Training Debug (Iteration 8780) ===
Q mean: -15.515239
Q std: 22.335592
Actor loss: 15.519197
Action reg: 0.003958
  l1.weight: grad_norm = 0.226586
  l1.bias: grad_norm = 0.001533
  l2.weight: grad_norm = 0.191775
Total gradient norm: 0.559318
=== Actor Training Debug (Iteration 8781) ===
Q mean: -14.636196
Q std: 21.021963
Actor loss: 14.640144
Action reg: 0.003948
  l1.weight: grad_norm = 0.240495
  l1.bias: grad_norm = 0.001894
  l2.weight: grad_norm = 0.188794
Total gradient norm: 0.562037
=== Actor Training Debug (Iteration 8782) ===
Q mean: -14.070594
Q std: 20.592243
Actor loss: 14.074533
Action reg: 0.003939
  l1.weight: grad_norm = 0.221577
  l1.bias: grad_norm = 0.003423
  l2.weight: grad_norm = 0.160637
Total gradient norm: 0.413230
=== Actor Training Debug (Iteration 8783) ===
Q mean: -13.502575
Q std: 19.633446
Actor loss: 13.506525
Action reg: 0.003951
  l1.weight: grad_norm = 0.226536
  l1.bias: grad_norm = 0.001953
  l2.weight: grad_norm = 0.169619
Total gradient norm: 0.475175
=== Actor Training Debug (Iteration 8784) ===
Q mean: -12.757671
Q std: 21.054272
Actor loss: 12.761635
Action reg: 0.003963
  l1.weight: grad_norm = 0.136563
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.111682
Total gradient norm: 0.258206
=== Actor Training Debug (Iteration 8785) ===
Q mean: -13.735896
Q std: 17.917753
Actor loss: 13.739884
Action reg: 0.003989
  l1.weight: grad_norm = 0.188365
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.139474
Total gradient norm: 0.354569
=== Actor Training Debug (Iteration 8786) ===
Q mean: -14.963892
Q std: 20.872623
Actor loss: 14.967859
Action reg: 0.003967
  l1.weight: grad_norm = 0.186133
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.128603
Total gradient norm: 0.338845
=== Actor Training Debug (Iteration 8787) ===
Q mean: -13.486933
Q std: 19.618866
Actor loss: 13.490885
Action reg: 0.003952
  l1.weight: grad_norm = 0.208177
  l1.bias: grad_norm = 0.002361
  l2.weight: grad_norm = 0.171086
Total gradient norm: 0.465237
=== Actor Training Debug (Iteration 8788) ===
Q mean: -13.771792
Q std: 18.463074
Actor loss: 13.775740
Action reg: 0.003948
  l1.weight: grad_norm = 0.299653
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.264994
Total gradient norm: 0.750762
=== Actor Training Debug (Iteration 8789) ===
Q mean: -15.169706
Q std: 20.155249
Actor loss: 15.173669
Action reg: 0.003963
  l1.weight: grad_norm = 0.359893
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.208728
Total gradient norm: 0.537947
=== Actor Training Debug (Iteration 8790) ===
Q mean: -12.424997
Q std: 18.424835
Actor loss: 12.428968
Action reg: 0.003971
  l1.weight: grad_norm = 0.109627
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.089384
Total gradient norm: 0.235581
=== Actor Training Debug (Iteration 8791) ===
Q mean: -18.393745
Q std: 24.110785
Actor loss: 18.397699
Action reg: 0.003953
  l1.weight: grad_norm = 0.151672
  l1.bias: grad_norm = 0.000577
  l2.weight: grad_norm = 0.110174
Total gradient norm: 0.277374
=== Actor Training Debug (Iteration 8792) ===
Q mean: -16.406628
Q std: 23.084122
Actor loss: 16.410582
Action reg: 0.003953
  l1.weight: grad_norm = 0.410616
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.284440
Total gradient norm: 0.748797
=== Actor Training Debug (Iteration 8793) ===
Q mean: -14.009800
Q std: 20.043503
Actor loss: 14.013747
Action reg: 0.003947
  l1.weight: grad_norm = 0.326441
  l1.bias: grad_norm = 0.001608
  l2.weight: grad_norm = 0.252793
Total gradient norm: 0.716025
=== Actor Training Debug (Iteration 8794) ===
Q mean: -15.465654
Q std: 22.397629
Actor loss: 15.469625
Action reg: 0.003972
  l1.weight: grad_norm = 0.274669
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.205708
Total gradient norm: 0.564510
=== Actor Training Debug (Iteration 8795) ===
Q mean: -14.804903
Q std: 22.034220
Actor loss: 14.808862
Action reg: 0.003959
  l1.weight: grad_norm = 0.156139
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.119948
Total gradient norm: 0.344133
=== Actor Training Debug (Iteration 8796) ===
Q mean: -13.997940
Q std: 20.728605
Actor loss: 14.001915
Action reg: 0.003975
  l1.weight: grad_norm = 0.411410
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.261913
Total gradient norm: 0.683838
=== Actor Training Debug (Iteration 8797) ===
Q mean: -14.064122
Q std: 19.269371
Actor loss: 14.068071
Action reg: 0.003949
  l1.weight: grad_norm = 0.259038
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.179692
Total gradient norm: 0.576983
=== Actor Training Debug (Iteration 8798) ===
Q mean: -14.228987
Q std: 21.015408
Actor loss: 14.232951
Action reg: 0.003965
  l1.weight: grad_norm = 0.257124
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.243909
Total gradient norm: 0.808747
=== Actor Training Debug (Iteration 8799) ===
Q mean: -15.723742
Q std: 23.082947
Actor loss: 15.727709
Action reg: 0.003967
  l1.weight: grad_norm = 0.218256
  l1.bias: grad_norm = 0.000634
  l2.weight: grad_norm = 0.186099
Total gradient norm: 0.530019
=== Actor Training Debug (Iteration 8800) ===
Q mean: -16.068995
Q std: 22.951757
Actor loss: 16.072950
Action reg: 0.003955
  l1.weight: grad_norm = 0.118105
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.079698
Total gradient norm: 0.214125
=== Actor Training Debug (Iteration 8801) ===
Q mean: -14.742404
Q std: 21.088223
Actor loss: 14.746375
Action reg: 0.003971
  l1.weight: grad_norm = 0.300066
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.221780
Total gradient norm: 0.638763
=== Actor Training Debug (Iteration 8802) ===
Q mean: -17.905941
Q std: 22.579845
Actor loss: 17.909893
Action reg: 0.003951
  l1.weight: grad_norm = 0.316622
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.209761
Total gradient norm: 0.579846
=== Actor Training Debug (Iteration 8803) ===
Q mean: -16.452866
Q std: 21.378456
Actor loss: 16.456806
Action reg: 0.003940
  l1.weight: grad_norm = 0.290330
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.204684
Total gradient norm: 0.534984
=== Actor Training Debug (Iteration 8804) ===
Q mean: -14.939512
Q std: 21.236990
Actor loss: 14.943459
Action reg: 0.003946
  l1.weight: grad_norm = 0.248303
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.198930
Total gradient norm: 0.556834
=== Actor Training Debug (Iteration 8805) ===
Q mean: -15.177889
Q std: 21.918005
Actor loss: 15.181852
Action reg: 0.003963
  l1.weight: grad_norm = 0.243615
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.194564
Total gradient norm: 0.574705
=== Actor Training Debug (Iteration 8806) ===
Q mean: -14.577203
Q std: 20.152523
Actor loss: 14.581141
Action reg: 0.003938
  l1.weight: grad_norm = 0.225648
  l1.bias: grad_norm = 0.000641
  l2.weight: grad_norm = 0.204685
Total gradient norm: 0.577134
=== Actor Training Debug (Iteration 8807) ===
Q mean: -14.288382
Q std: 20.440508
Actor loss: 14.292339
Action reg: 0.003958
  l1.weight: grad_norm = 0.471065
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.322675
Total gradient norm: 0.843513
=== Actor Training Debug (Iteration 8808) ===
Q mean: -15.945740
Q std: 21.859045
Actor loss: 15.949716
Action reg: 0.003976
  l1.weight: grad_norm = 0.314832
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.217101
Total gradient norm: 0.587520
=== Actor Training Debug (Iteration 8809) ===
Q mean: -14.966572
Q std: 21.535074
Actor loss: 14.970516
Action reg: 0.003944
  l1.weight: grad_norm = 0.107564
  l1.bias: grad_norm = 0.000846
  l2.weight: grad_norm = 0.076753
Total gradient norm: 0.214799
=== Actor Training Debug (Iteration 8810) ===
Q mean: -16.788250
Q std: 22.684311
Actor loss: 16.792200
Action reg: 0.003950
  l1.weight: grad_norm = 0.206141
  l1.bias: grad_norm = 0.000966
  l2.weight: grad_norm = 0.136248
Total gradient norm: 0.369020
=== Actor Training Debug (Iteration 8811) ===
Q mean: -17.167833
Q std: 23.868904
Actor loss: 17.171759
Action reg: 0.003925
  l1.weight: grad_norm = 0.256213
  l1.bias: grad_norm = 0.001097
  l2.weight: grad_norm = 0.192723
Total gradient norm: 0.534518
=== Actor Training Debug (Iteration 8812) ===
Q mean: -14.808525
Q std: 21.613874
Actor loss: 14.812485
Action reg: 0.003960
  l1.weight: grad_norm = 0.411875
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.236934
Total gradient norm: 0.640637
=== Actor Training Debug (Iteration 8813) ===
Q mean: -12.718398
Q std: 20.048285
Actor loss: 12.722326
Action reg: 0.003928
  l1.weight: grad_norm = 0.363626
  l1.bias: grad_norm = 0.002881
  l2.weight: grad_norm = 0.265684
Total gradient norm: 0.900568
=== Actor Training Debug (Iteration 8814) ===
Q mean: -14.224572
Q std: 20.240274
Actor loss: 14.228528
Action reg: 0.003956
  l1.weight: grad_norm = 0.262606
  l1.bias: grad_norm = 0.001585
  l2.weight: grad_norm = 0.188174
Total gradient norm: 0.521571
=== Actor Training Debug (Iteration 8815) ===
Q mean: -14.489635
Q std: 20.358727
Actor loss: 14.493595
Action reg: 0.003959
  l1.weight: grad_norm = 0.378594
  l1.bias: grad_norm = 0.001961
  l2.weight: grad_norm = 0.269578
Total gradient norm: 0.654707
=== Actor Training Debug (Iteration 8816) ===
Q mean: -15.547009
Q std: 21.195858
Actor loss: 15.550968
Action reg: 0.003960
  l1.weight: grad_norm = 0.292249
  l1.bias: grad_norm = 0.001567
  l2.weight: grad_norm = 0.196186
Total gradient norm: 0.513583
=== Actor Training Debug (Iteration 8817) ===
Q mean: -14.031510
Q std: 19.116325
Actor loss: 14.035472
Action reg: 0.003961
  l1.weight: grad_norm = 0.169404
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.128811
Total gradient norm: 0.352036
=== Actor Training Debug (Iteration 8818) ===
Q mean: -16.578785
Q std: 23.063389
Actor loss: 16.582747
Action reg: 0.003962
  l1.weight: grad_norm = 0.143131
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.128354
Total gradient norm: 0.327411
=== Actor Training Debug (Iteration 8819) ===
Q mean: -14.224102
Q std: 22.565960
Actor loss: 14.228071
Action reg: 0.003969
  l1.weight: grad_norm = 0.196999
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.151275
Total gradient norm: 0.441316
=== Actor Training Debug (Iteration 8820) ===
Q mean: -14.048265
Q std: 19.373281
Actor loss: 14.052244
Action reg: 0.003980
  l1.weight: grad_norm = 0.188273
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.151773
Total gradient norm: 0.401457
=== Actor Training Debug (Iteration 8821) ===
Q mean: -15.892057
Q std: 22.169657
Actor loss: 15.896022
Action reg: 0.003965
  l1.weight: grad_norm = 0.321490
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.217284
Total gradient norm: 0.623217
=== Actor Training Debug (Iteration 8822) ===
Q mean: -15.693983
Q std: 21.315607
Actor loss: 15.697934
Action reg: 0.003951
  l1.weight: grad_norm = 0.263614
  l1.bias: grad_norm = 0.000682
  l2.weight: grad_norm = 0.194040
Total gradient norm: 0.551517
=== Actor Training Debug (Iteration 8823) ===
Q mean: -15.060543
Q std: 22.049929
Actor loss: 15.064496
Action reg: 0.003953
  l1.weight: grad_norm = 0.191169
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.145676
Total gradient norm: 0.371920
=== Actor Training Debug (Iteration 8824) ===
Q mean: -15.036477
Q std: 21.538477
Actor loss: 15.040405
Action reg: 0.003928
  l1.weight: grad_norm = 0.244603
  l1.bias: grad_norm = 0.003650
  l2.weight: grad_norm = 0.189066
Total gradient norm: 0.501360
=== Actor Training Debug (Iteration 8825) ===
Q mean: -14.265812
Q std: 20.287533
Actor loss: 14.269758
Action reg: 0.003947
  l1.weight: grad_norm = 0.393407
  l1.bias: grad_norm = 0.002826
  l2.weight: grad_norm = 0.285588
Total gradient norm: 0.793777
=== Actor Training Debug (Iteration 8826) ===
Q mean: -15.850799
Q std: 21.853582
Actor loss: 15.854761
Action reg: 0.003963
  l1.weight: grad_norm = 0.361888
  l1.bias: grad_norm = 0.003063
  l2.weight: grad_norm = 0.247662
Total gradient norm: 0.692103
=== Actor Training Debug (Iteration 8827) ===
Q mean: -13.087688
Q std: 19.763489
Actor loss: 13.091640
Action reg: 0.003952
  l1.weight: grad_norm = 0.627072
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.404403
Total gradient norm: 1.231189
=== Actor Training Debug (Iteration 8828) ===
Q mean: -16.193127
Q std: 22.397455
Actor loss: 16.197084
Action reg: 0.003957
  l1.weight: grad_norm = 0.189867
  l1.bias: grad_norm = 0.001703
  l2.weight: grad_norm = 0.125901
Total gradient norm: 0.348559
=== Actor Training Debug (Iteration 8829) ===
Q mean: -15.261533
Q std: 21.073711
Actor loss: 15.265485
Action reg: 0.003952
  l1.weight: grad_norm = 0.254402
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.205012
Total gradient norm: 0.609027
=== Actor Training Debug (Iteration 8830) ===
Q mean: -13.847149
Q std: 20.611589
Actor loss: 13.851126
Action reg: 0.003977
  l1.weight: grad_norm = 0.286246
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.187839
Total gradient norm: 0.560373
=== Actor Training Debug (Iteration 8831) ===
Q mean: -14.526449
Q std: 20.829750
Actor loss: 14.530416
Action reg: 0.003966
  l1.weight: grad_norm = 0.173782
  l1.bias: grad_norm = 0.001572
  l2.weight: grad_norm = 0.125420
Total gradient norm: 0.387939
=== Actor Training Debug (Iteration 8832) ===
Q mean: -12.479401
Q std: 18.434031
Actor loss: 12.483351
Action reg: 0.003950
  l1.weight: grad_norm = 0.241126
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.179843
Total gradient norm: 0.476887
=== Actor Training Debug (Iteration 8833) ===
Q mean: -15.038065
Q std: 21.046745
Actor loss: 15.042014
Action reg: 0.003949
  l1.weight: grad_norm = 0.414601
  l1.bias: grad_norm = 0.001899
  l2.weight: grad_norm = 0.261974
Total gradient norm: 0.707727
=== Actor Training Debug (Iteration 8834) ===
Q mean: -14.072935
Q std: 19.872562
Actor loss: 14.076907
Action reg: 0.003972
  l1.weight: grad_norm = 0.258094
  l1.bias: grad_norm = 0.001623
  l2.weight: grad_norm = 0.166672
Total gradient norm: 0.430016
=== Actor Training Debug (Iteration 8835) ===
Q mean: -15.302008
Q std: 21.448851
Actor loss: 15.305971
Action reg: 0.003963
  l1.weight: grad_norm = 0.104586
  l1.bias: grad_norm = 0.003418
  l2.weight: grad_norm = 0.081869
Total gradient norm: 0.249842
=== Actor Training Debug (Iteration 8836) ===
Q mean: -14.162198
Q std: 19.969242
Actor loss: 14.166168
Action reg: 0.003970
  l1.weight: grad_norm = 0.199481
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.141163
Total gradient norm: 0.347452
=== Actor Training Debug (Iteration 8837) ===
Q mean: -15.504875
Q std: 21.547422
Actor loss: 15.508835
Action reg: 0.003959
  l1.weight: grad_norm = 0.488863
  l1.bias: grad_norm = 0.001912
  l2.weight: grad_norm = 0.328776
Total gradient norm: 0.894687
=== Actor Training Debug (Iteration 8838) ===
Q mean: -15.634733
Q std: 21.953873
Actor loss: 15.638676
Action reg: 0.003943
  l1.weight: grad_norm = 0.207303
  l1.bias: grad_norm = 0.001730
  l2.weight: grad_norm = 0.180508
Total gradient norm: 0.531866
=== Actor Training Debug (Iteration 8839) ===
Q mean: -13.707843
Q std: 18.749817
Actor loss: 13.711805
Action reg: 0.003962
  l1.weight: grad_norm = 0.198347
  l1.bias: grad_norm = 0.001757
  l2.weight: grad_norm = 0.165142
Total gradient norm: 0.452307
=== Actor Training Debug (Iteration 8840) ===
Q mean: -15.376961
Q std: 21.629480
Actor loss: 15.380900
Action reg: 0.003940
  l1.weight: grad_norm = 0.364734
  l1.bias: grad_norm = 0.004619
  l2.weight: grad_norm = 0.222505
Total gradient norm: 0.650620
=== Actor Training Debug (Iteration 8841) ===
Q mean: -14.081436
Q std: 20.248074
Actor loss: 14.085388
Action reg: 0.003952
  l1.weight: grad_norm = 0.484865
  l1.bias: grad_norm = 0.001639
  l2.weight: grad_norm = 0.375007
Total gradient norm: 1.001425
=== Actor Training Debug (Iteration 8842) ===
Q mean: -14.530791
Q std: 21.706474
Actor loss: 14.534760
Action reg: 0.003969
  l1.weight: grad_norm = 0.163357
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.120162
Total gradient norm: 0.320331
=== Actor Training Debug (Iteration 8843) ===
Q mean: -12.264920
Q std: 17.168571
Actor loss: 12.268901
Action reg: 0.003981
  l1.weight: grad_norm = 0.233748
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.170181
Total gradient norm: 0.482661
=== Actor Training Debug (Iteration 8844) ===
Q mean: -15.338863
Q std: 20.148573
Actor loss: 15.342819
Action reg: 0.003956
  l1.weight: grad_norm = 0.251104
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.204568
Total gradient norm: 0.579156
=== Actor Training Debug (Iteration 8845) ===
Q mean: -17.230724
Q std: 21.516100
Actor loss: 17.234682
Action reg: 0.003958
  l1.weight: grad_norm = 0.392649
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.304725
Total gradient norm: 0.933551
=== Actor Training Debug (Iteration 8846) ===
Q mean: -13.356740
Q std: 19.197088
Actor loss: 13.360690
Action reg: 0.003951
  l1.weight: grad_norm = 0.306234
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.217966
Total gradient norm: 0.669701
=== Actor Training Debug (Iteration 8847) ===
Q mean: -14.952136
Q std: 21.035105
Actor loss: 14.956073
Action reg: 0.003937
  l1.weight: grad_norm = 0.211817
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.170218
Total gradient norm: 0.453536
=== Actor Training Debug (Iteration 8848) ===
Q mean: -12.788111
Q std: 21.178915
Actor loss: 12.792048
Action reg: 0.003938
  l1.weight: grad_norm = 0.305992
  l1.bias: grad_norm = 0.008663
  l2.weight: grad_norm = 0.220123
Total gradient norm: 0.640083
=== Actor Training Debug (Iteration 8849) ===
Q mean: -14.819903
Q std: 21.428986
Actor loss: 14.823854
Action reg: 0.003951
  l1.weight: grad_norm = 0.279483
  l1.bias: grad_norm = 0.000648
  l2.weight: grad_norm = 0.212641
Total gradient norm: 0.562681
=== Actor Training Debug (Iteration 8850) ===
Q mean: -14.542558
Q std: 20.706411
Actor loss: 14.546529
Action reg: 0.003971
  l1.weight: grad_norm = 0.371415
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.310402
Total gradient norm: 0.830313
=== Actor Training Debug (Iteration 8851) ===
Q mean: -14.463562
Q std: 20.069384
Actor loss: 14.467514
Action reg: 0.003952
  l1.weight: grad_norm = 0.486889
  l1.bias: grad_norm = 0.003351
  l2.weight: grad_norm = 0.369883
Total gradient norm: 0.969621
=== Actor Training Debug (Iteration 8852) ===
Q mean: -13.935984
Q std: 19.036684
Actor loss: 13.939939
Action reg: 0.003956
  l1.weight: grad_norm = 0.171710
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.158670
Total gradient norm: 0.401215
=== Actor Training Debug (Iteration 8853) ===
Q mean: -13.520276
Q std: 19.421261
Actor loss: 13.524240
Action reg: 0.003965
  l1.weight: grad_norm = 0.152954
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.114506
Total gradient norm: 0.290222
=== Actor Training Debug (Iteration 8854) ===
Q mean: -15.000662
Q std: 21.903313
Actor loss: 15.004624
Action reg: 0.003962
  l1.weight: grad_norm = 0.184851
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.134080
Total gradient norm: 0.374737
=== Actor Training Debug (Iteration 8855) ===
Q mean: -14.891713
Q std: 21.027674
Actor loss: 14.895675
Action reg: 0.003961
  l1.weight: grad_norm = 0.275328
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.247224
Total gradient norm: 0.733283
=== Actor Training Debug (Iteration 8856) ===
Q mean: -14.955402
Q std: 20.256582
Actor loss: 14.959354
Action reg: 0.003952
  l1.weight: grad_norm = 0.247266
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.193455
Total gradient norm: 0.509660
=== Actor Training Debug (Iteration 8857) ===
Q mean: -14.046471
Q std: 19.221678
Actor loss: 14.050417
Action reg: 0.003946
  l1.weight: grad_norm = 0.219679
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.167910
Total gradient norm: 0.395873
=== Actor Training Debug (Iteration 8858) ===
Q mean: -17.846939
Q std: 23.252033
Actor loss: 17.850899
Action reg: 0.003960
  l1.weight: grad_norm = 0.236415
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.172448
Total gradient norm: 0.447325
=== Actor Training Debug (Iteration 8859) ===
Q mean: -14.443043
Q std: 20.585289
Actor loss: 14.446990
Action reg: 0.003947
  l1.weight: grad_norm = 0.321369
  l1.bias: grad_norm = 0.001204
  l2.weight: grad_norm = 0.207249
Total gradient norm: 0.552407
=== Actor Training Debug (Iteration 8860) ===
Q mean: -14.440104
Q std: 19.407778
Actor loss: 14.444036
Action reg: 0.003932
  l1.weight: grad_norm = 0.243355
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.163998
Total gradient norm: 0.432202
=== Actor Training Debug (Iteration 8861) ===
Q mean: -16.333254
Q std: 22.107479
Actor loss: 16.337221
Action reg: 0.003967
  l1.weight: grad_norm = 0.337592
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.225589
Total gradient norm: 0.596087
=== Actor Training Debug (Iteration 8862) ===
Q mean: -13.623335
Q std: 18.267900
Actor loss: 13.627290
Action reg: 0.003955
  l1.weight: grad_norm = 0.600815
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.412839
Total gradient norm: 1.270211
=== Actor Training Debug (Iteration 8863) ===
Q mean: -15.447064
Q std: 21.005476
Actor loss: 15.451028
Action reg: 0.003964
  l1.weight: grad_norm = 0.212593
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.154474
Total gradient norm: 0.428146
=== Actor Training Debug (Iteration 8864) ===
Q mean: -15.148138
Q std: 20.590691
Actor loss: 15.152080
Action reg: 0.003941
  l1.weight: grad_norm = 0.213133
  l1.bias: grad_norm = 0.001263
  l2.weight: grad_norm = 0.174380
Total gradient norm: 0.478332
=== Actor Training Debug (Iteration 8865) ===
Q mean: -15.259026
Q std: 21.361679
Actor loss: 15.262995
Action reg: 0.003969
  l1.weight: grad_norm = 0.282918
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.181765
Total gradient norm: 0.490589
=== Actor Training Debug (Iteration 8866) ===
Q mean: -13.970577
Q std: 19.733269
Actor loss: 13.974523
Action reg: 0.003945
  l1.weight: grad_norm = 0.278732
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.223953
Total gradient norm: 0.634176
=== Actor Training Debug (Iteration 8867) ===
Q mean: -15.357511
Q std: 21.645226
Actor loss: 15.361470
Action reg: 0.003960
  l1.weight: grad_norm = 0.358508
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.247170
Total gradient norm: 0.669944
=== Actor Training Debug (Iteration 8868) ===
Q mean: -15.951131
Q std: 21.453369
Actor loss: 15.955091
Action reg: 0.003961
  l1.weight: grad_norm = 0.115385
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.083415
Total gradient norm: 0.233831
=== Actor Training Debug (Iteration 8869) ===
Q mean: -15.207683
Q std: 20.774242
Actor loss: 15.211634
Action reg: 0.003951
  l1.weight: grad_norm = 0.249732
  l1.bias: grad_norm = 0.001402
  l2.weight: grad_norm = 0.171080
Total gradient norm: 0.445982
=== Actor Training Debug (Iteration 8870) ===
Q mean: -14.148590
Q std: 21.521183
Actor loss: 14.152557
Action reg: 0.003968
  l1.weight: grad_norm = 0.236660
  l1.bias: grad_norm = 0.001645
  l2.weight: grad_norm = 0.155281
Total gradient norm: 0.448858
=== Actor Training Debug (Iteration 8871) ===
Q mean: -14.567463
Q std: 20.816090
Actor loss: 14.571404
Action reg: 0.003942
  l1.weight: grad_norm = 0.477462
  l1.bias: grad_norm = 0.001546
  l2.weight: grad_norm = 0.302082
Total gradient norm: 0.816572
=== Actor Training Debug (Iteration 8872) ===
Q mean: -13.402927
Q std: 21.628794
Actor loss: 13.406888
Action reg: 0.003961
  l1.weight: grad_norm = 0.197560
  l1.bias: grad_norm = 0.001616
  l2.weight: grad_norm = 0.174775
Total gradient norm: 0.493834
=== Actor Training Debug (Iteration 8873) ===
Q mean: -16.005220
Q std: 22.058256
Actor loss: 16.009195
Action reg: 0.003975
  l1.weight: grad_norm = 0.801510
  l1.bias: grad_norm = 0.000428
  l2.weight: grad_norm = 0.710806
Total gradient norm: 1.935720
=== Actor Training Debug (Iteration 8874) ===
Q mean: -12.341141
Q std: 19.699018
Actor loss: 12.345082
Action reg: 0.003942
  l1.weight: grad_norm = 0.273004
  l1.bias: grad_norm = 0.000651
  l2.weight: grad_norm = 0.208712
Total gradient norm: 0.608054
=== Actor Training Debug (Iteration 8875) ===
Q mean: -15.540266
Q std: 21.007231
Actor loss: 15.544208
Action reg: 0.003941
  l1.weight: grad_norm = 0.367435
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.257740
Total gradient norm: 0.668723
=== Actor Training Debug (Iteration 8876) ===
Q mean: -11.785839
Q std: 17.812569
Actor loss: 11.789804
Action reg: 0.003965
  l1.weight: grad_norm = 0.373719
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.320180
Total gradient norm: 1.033539
=== Actor Training Debug (Iteration 8877) ===
Q mean: -15.548929
Q std: 22.109457
Actor loss: 15.552894
Action reg: 0.003964
  l1.weight: grad_norm = 0.373786
  l1.bias: grad_norm = 0.002048
  l2.weight: grad_norm = 0.335235
Total gradient norm: 0.976369
=== Actor Training Debug (Iteration 8878) ===
Q mean: -15.081589
Q std: 21.548717
Actor loss: 15.085536
Action reg: 0.003947
  l1.weight: grad_norm = 0.250493
  l1.bias: grad_norm = 0.001153
  l2.weight: grad_norm = 0.162917
Total gradient norm: 0.434157
=== Actor Training Debug (Iteration 8879) ===
Q mean: -13.825719
Q std: 19.311743
Actor loss: 13.829688
Action reg: 0.003969
  l1.weight: grad_norm = 0.182547
  l1.bias: grad_norm = 0.001376
  l2.weight: grad_norm = 0.136588
Total gradient norm: 0.327865
=== Actor Training Debug (Iteration 8880) ===
Q mean: -17.822189
Q std: 22.935677
Actor loss: 17.826151
Action reg: 0.003961
  l1.weight: grad_norm = 0.281520
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.205548
Total gradient norm: 0.555740
=== Actor Training Debug (Iteration 8881) ===
Q mean: -16.894564
Q std: 22.091402
Actor loss: 16.898521
Action reg: 0.003957
  l1.weight: grad_norm = 0.170072
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.125268
Total gradient norm: 0.331135
=== Actor Training Debug (Iteration 8882) ===
Q mean: -16.471378
Q std: 22.895012
Actor loss: 16.475321
Action reg: 0.003942
  l1.weight: grad_norm = 0.261445
  l1.bias: grad_norm = 0.000579
  l2.weight: grad_norm = 0.190768
Total gradient norm: 0.538362
=== Actor Training Debug (Iteration 8883) ===
Q mean: -14.140699
Q std: 20.621807
Actor loss: 14.144658
Action reg: 0.003959
  l1.weight: grad_norm = 0.306726
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.222756
Total gradient norm: 0.587299
=== Actor Training Debug (Iteration 8884) ===
Q mean: -15.536455
Q std: 21.461390
Actor loss: 15.540402
Action reg: 0.003947
  l1.weight: grad_norm = 0.172266
  l1.bias: grad_norm = 0.001575
  l2.weight: grad_norm = 0.130271
Total gradient norm: 0.381091
=== Actor Training Debug (Iteration 8885) ===
Q mean: -15.949985
Q std: 21.293072
Actor loss: 15.953949
Action reg: 0.003965
  l1.weight: grad_norm = 0.225572
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.181075
Total gradient norm: 0.488425
=== Actor Training Debug (Iteration 8886) ===
Q mean: -14.816298
Q std: 21.017075
Actor loss: 14.820254
Action reg: 0.003955
  l1.weight: grad_norm = 0.215817
  l1.bias: grad_norm = 0.005213
  l2.weight: grad_norm = 0.162803
Total gradient norm: 0.467286
=== Actor Training Debug (Iteration 8887) ===
Q mean: -15.301796
Q std: 21.999302
Actor loss: 15.305782
Action reg: 0.003987
  l1.weight: grad_norm = 0.197717
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.139132
Total gradient norm: 0.386443
=== Actor Training Debug (Iteration 8888) ===
Q mean: -13.418968
Q std: 21.041817
Actor loss: 13.422879
Action reg: 0.003911
  l1.weight: grad_norm = 0.139978
  l1.bias: grad_norm = 0.000940
  l2.weight: grad_norm = 0.103595
Total gradient norm: 0.302624
=== Actor Training Debug (Iteration 8889) ===
Q mean: -15.159327
Q std: 20.495338
Actor loss: 15.163281
Action reg: 0.003955
  l1.weight: grad_norm = 0.258782
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.183513
Total gradient norm: 0.548453
=== Actor Training Debug (Iteration 8890) ===
Q mean: -13.818523
Q std: 20.944929
Actor loss: 13.822473
Action reg: 0.003949
  l1.weight: grad_norm = 0.238162
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.176334
Total gradient norm: 0.485807
=== Actor Training Debug (Iteration 8891) ===
Q mean: -15.546643
Q std: 21.486330
Actor loss: 15.550604
Action reg: 0.003961
  l1.weight: grad_norm = 0.198702
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.159297
Total gradient norm: 0.411849
=== Actor Training Debug (Iteration 8892) ===
Q mean: -14.967895
Q std: 21.725111
Actor loss: 14.971826
Action reg: 0.003931
  l1.weight: grad_norm = 0.322207
  l1.bias: grad_norm = 0.003115
  l2.weight: grad_norm = 0.226071
Total gradient norm: 0.707264
=== Actor Training Debug (Iteration 8893) ===
Q mean: -13.936510
Q std: 21.056139
Actor loss: 13.940489
Action reg: 0.003979
  l1.weight: grad_norm = 0.194845
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.136326
Total gradient norm: 0.369524
=== Actor Training Debug (Iteration 8894) ===
Q mean: -15.147058
Q std: 20.141678
Actor loss: 15.150991
Action reg: 0.003933
  l1.weight: grad_norm = 0.236259
  l1.bias: grad_norm = 0.001560
  l2.weight: grad_norm = 0.166556
Total gradient norm: 0.487339
=== Actor Training Debug (Iteration 8895) ===
Q mean: -13.429995
Q std: 18.837891
Actor loss: 13.433954
Action reg: 0.003960
  l1.weight: grad_norm = 0.199664
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.158030
Total gradient norm: 0.448237
=== Actor Training Debug (Iteration 8896) ===
Q mean: -15.871130
Q std: 21.303410
Actor loss: 15.875081
Action reg: 0.003951
  l1.weight: grad_norm = 0.444965
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.258206
Total gradient norm: 0.685652
=== Actor Training Debug (Iteration 8897) ===
Q mean: -14.607531
Q std: 21.228193
Actor loss: 14.611477
Action reg: 0.003947
  l1.weight: grad_norm = 0.314420
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.245011
Total gradient norm: 0.709047
=== Actor Training Debug (Iteration 8898) ===
Q mean: -14.179170
Q std: 20.785238
Actor loss: 14.183118
Action reg: 0.003948
  l1.weight: grad_norm = 0.138702
  l1.bias: grad_norm = 0.000534
  l2.weight: grad_norm = 0.118076
Total gradient norm: 0.302391
=== Actor Training Debug (Iteration 8899) ===
Q mean: -15.313105
Q std: 21.706261
Actor loss: 15.317065
Action reg: 0.003961
  l1.weight: grad_norm = 0.247459
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.169617
Total gradient norm: 0.482920
=== Actor Training Debug (Iteration 8900) ===
Q mean: -13.129028
Q std: 18.537498
Actor loss: 13.132973
Action reg: 0.003945
  l1.weight: grad_norm = 0.427607
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.284755
Total gradient norm: 0.798854
=== Actor Training Debug (Iteration 8901) ===
Q mean: -13.337086
Q std: 19.450293
Actor loss: 13.341043
Action reg: 0.003957
  l1.weight: grad_norm = 0.207878
  l1.bias: grad_norm = 0.003144
  l2.weight: grad_norm = 0.172256
Total gradient norm: 0.479259
=== Actor Training Debug (Iteration 8902) ===
Q mean: -13.595399
Q std: 20.010767
Actor loss: 13.599347
Action reg: 0.003948
  l1.weight: grad_norm = 0.337187
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.268464
Total gradient norm: 0.759031
=== Actor Training Debug (Iteration 8903) ===
Q mean: -16.468945
Q std: 22.811914
Actor loss: 16.472900
Action reg: 0.003955
  l1.weight: grad_norm = 0.523305
  l1.bias: grad_norm = 0.001959
  l2.weight: grad_norm = 0.363287
Total gradient norm: 0.944605
=== Actor Training Debug (Iteration 8904) ===
Q mean: -15.563542
Q std: 21.996330
Actor loss: 15.567466
Action reg: 0.003924
  l1.weight: grad_norm = 0.397627
  l1.bias: grad_norm = 0.000910
  l2.weight: grad_norm = 0.279685
Total gradient norm: 0.763197
=== Actor Training Debug (Iteration 8905) ===
Q mean: -15.327614
Q std: 21.174498
Actor loss: 15.331570
Action reg: 0.003956
  l1.weight: grad_norm = 0.194596
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.160906
Total gradient norm: 0.473778
=== Actor Training Debug (Iteration 8906) ===
Q mean: -15.866148
Q std: 22.913712
Actor loss: 15.870091
Action reg: 0.003944
  l1.weight: grad_norm = 0.180117
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.113012
Total gradient norm: 0.290854
=== Actor Training Debug (Iteration 8907) ===
Q mean: -15.384336
Q std: 21.861652
Actor loss: 15.388284
Action reg: 0.003947
  l1.weight: grad_norm = 0.343865
  l1.bias: grad_norm = 0.001720
  l2.weight: grad_norm = 0.270921
Total gradient norm: 0.772931
=== Actor Training Debug (Iteration 8908) ===
Q mean: -13.460325
Q std: 20.088898
Actor loss: 13.464298
Action reg: 0.003973
  l1.weight: grad_norm = 0.251870
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.212082
Total gradient norm: 0.517337
=== Actor Training Debug (Iteration 8909) ===
Q mean: -15.345863
Q std: 20.366081
Actor loss: 15.349813
Action reg: 0.003949
  l1.weight: grad_norm = 0.238597
  l1.bias: grad_norm = 0.001157
  l2.weight: grad_norm = 0.186082
Total gradient norm: 0.600831
=== Actor Training Debug (Iteration 8910) ===
Q mean: -14.973459
Q std: 21.520645
Actor loss: 14.977399
Action reg: 0.003940
  l1.weight: grad_norm = 0.244436
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.166282
Total gradient norm: 0.434023
=== Actor Training Debug (Iteration 8911) ===
Q mean: -14.847610
Q std: 21.307474
Actor loss: 14.851519
Action reg: 0.003909
  l1.weight: grad_norm = 0.368518
  l1.bias: grad_norm = 0.001221
  l2.weight: grad_norm = 0.310327
Total gradient norm: 0.759266
=== Actor Training Debug (Iteration 8912) ===
Q mean: -14.286625
Q std: 20.761209
Actor loss: 14.290563
Action reg: 0.003937
  l1.weight: grad_norm = 0.199839
  l1.bias: grad_norm = 0.000742
  l2.weight: grad_norm = 0.146702
Total gradient norm: 0.398121
=== Actor Training Debug (Iteration 8913) ===
Q mean: -14.731203
Q std: 21.164698
Actor loss: 14.735147
Action reg: 0.003943
  l1.weight: grad_norm = 0.137189
  l1.bias: grad_norm = 0.001914
  l2.weight: grad_norm = 0.092580
Total gradient norm: 0.242414
=== Actor Training Debug (Iteration 8914) ===
Q mean: -12.205696
Q std: 18.225464
Actor loss: 12.209652
Action reg: 0.003956
  l1.weight: grad_norm = 0.305756
  l1.bias: grad_norm = 0.000657
  l2.weight: grad_norm = 0.197839
Total gradient norm: 0.553616
=== Actor Training Debug (Iteration 8915) ===
Q mean: -11.552711
Q std: 17.797255
Actor loss: 11.556651
Action reg: 0.003940
  l1.weight: grad_norm = 0.214429
  l1.bias: grad_norm = 0.003760
  l2.weight: grad_norm = 0.164925
Total gradient norm: 0.435271
=== Actor Training Debug (Iteration 8916) ===
Q mean: -14.681117
Q std: 20.649221
Actor loss: 14.685077
Action reg: 0.003960
  l1.weight: grad_norm = 0.525364
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.377025
Total gradient norm: 1.024646
=== Actor Training Debug (Iteration 8917) ===
Q mean: -15.399784
Q std: 21.211737
Actor loss: 15.403751
Action reg: 0.003967
  l1.weight: grad_norm = 0.380463
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.294038
Total gradient norm: 0.889573
=== Actor Training Debug (Iteration 8918) ===
Q mean: -14.177168
Q std: 19.934998
Actor loss: 14.181134
Action reg: 0.003967
  l1.weight: grad_norm = 0.416094
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.297197
Total gradient norm: 0.832326
=== Actor Training Debug (Iteration 8919) ===
Q mean: -14.272814
Q std: 19.455778
Actor loss: 14.276772
Action reg: 0.003958
  l1.weight: grad_norm = 0.944293
  l1.bias: grad_norm = 0.000881
  l2.weight: grad_norm = 0.738274
Total gradient norm: 1.992940
=== Actor Training Debug (Iteration 8920) ===
Q mean: -13.996219
Q std: 19.595095
Actor loss: 14.000141
Action reg: 0.003922
  l1.weight: grad_norm = 0.258737
  l1.bias: grad_norm = 0.003171
  l2.weight: grad_norm = 0.206078
Total gradient norm: 0.634845
=== Actor Training Debug (Iteration 8921) ===
Q mean: -15.218149
Q std: 22.342371
Actor loss: 15.222123
Action reg: 0.003974
  l1.weight: grad_norm = 0.224159
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.171203
Total gradient norm: 0.476863
=== Actor Training Debug (Iteration 8922) ===
Q mean: -15.049119
Q std: 20.513796
Actor loss: 15.053094
Action reg: 0.003975
  l1.weight: grad_norm = 0.351290
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.231065
Total gradient norm: 0.672072
=== Actor Training Debug (Iteration 8923) ===
Q mean: -14.249464
Q std: 21.135700
Actor loss: 14.253407
Action reg: 0.003942
  l1.weight: grad_norm = 0.264809
  l1.bias: grad_norm = 0.001401
  l2.weight: grad_norm = 0.184664
Total gradient norm: 0.529670
=== Actor Training Debug (Iteration 8924) ===
Q mean: -15.077494
Q std: 20.356445
Actor loss: 15.081446
Action reg: 0.003952
  l1.weight: grad_norm = 0.260755
  l1.bias: grad_norm = 0.001234
  l2.weight: grad_norm = 0.182589
Total gradient norm: 0.484874
=== Actor Training Debug (Iteration 8925) ===
Q mean: -13.112323
Q std: 19.385000
Actor loss: 13.116276
Action reg: 0.003953
  l1.weight: grad_norm = 0.477056
  l1.bias: grad_norm = 0.001322
  l2.weight: grad_norm = 0.366563
Total gradient norm: 0.955323
=== Actor Training Debug (Iteration 8926) ===
Q mean: -15.215736
Q std: 21.122263
Actor loss: 15.219712
Action reg: 0.003976
  l1.weight: grad_norm = 0.232899
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.196826
Total gradient norm: 0.563187
=== Actor Training Debug (Iteration 8927) ===
Q mean: -14.797422
Q std: 21.437462
Actor loss: 14.801376
Action reg: 0.003954
  l1.weight: grad_norm = 0.372429
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.297747
Total gradient norm: 0.777489
=== Actor Training Debug (Iteration 8928) ===
Q mean: -18.325186
Q std: 23.234976
Actor loss: 18.329147
Action reg: 0.003961
  l1.weight: grad_norm = 0.290456
  l1.bias: grad_norm = 0.001865
  l2.weight: grad_norm = 0.230468
Total gradient norm: 0.608616
=== Actor Training Debug (Iteration 8929) ===
Q mean: -14.642135
Q std: 21.214119
Actor loss: 14.646097
Action reg: 0.003963
  l1.weight: grad_norm = 0.221557
  l1.bias: grad_norm = 0.001703
  l2.weight: grad_norm = 0.153744
Total gradient norm: 0.453834
=== Actor Training Debug (Iteration 8930) ===
Q mean: -13.912251
Q std: 20.941311
Actor loss: 13.916214
Action reg: 0.003964
  l1.weight: grad_norm = 0.511001
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.460118
Total gradient norm: 1.673415
=== Actor Training Debug (Iteration 8931) ===
Q mean: -15.213505
Q std: 21.579844
Actor loss: 15.217480
Action reg: 0.003975
  l1.weight: grad_norm = 0.341147
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.223859
Total gradient norm: 0.567416
=== Actor Training Debug (Iteration 8932) ===
Q mean: -13.646774
Q std: 20.282969
Actor loss: 13.650722
Action reg: 0.003947
  l1.weight: grad_norm = 0.190294
  l1.bias: grad_norm = 0.000507
  l2.weight: grad_norm = 0.141930
Total gradient norm: 0.378643
=== Actor Training Debug (Iteration 8933) ===
Q mean: -15.173778
Q std: 22.393467
Actor loss: 15.177756
Action reg: 0.003979
  l1.weight: grad_norm = 0.152157
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.113407
Total gradient norm: 0.289200
=== Actor Training Debug (Iteration 8934) ===
Q mean: -15.876907
Q std: 23.918087
Actor loss: 15.880869
Action reg: 0.003962
  l1.weight: grad_norm = 0.250535
  l1.bias: grad_norm = 0.001702
  l2.weight: grad_norm = 0.184289
Total gradient norm: 0.584949
=== Actor Training Debug (Iteration 8935) ===
Q mean: -14.470630
Q std: 20.316641
Actor loss: 14.474610
Action reg: 0.003980
  l1.weight: grad_norm = 0.223569
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.161136
Total gradient norm: 0.372858
=== Actor Training Debug (Iteration 8936) ===
Q mean: -14.259339
Q std: 20.582636
Actor loss: 14.263302
Action reg: 0.003963
  l1.weight: grad_norm = 0.289678
  l1.bias: grad_norm = 0.001784
  l2.weight: grad_norm = 0.200758
Total gradient norm: 0.506415
=== Actor Training Debug (Iteration 8937) ===
Q mean: -17.419346
Q std: 22.004827
Actor loss: 17.423311
Action reg: 0.003966
  l1.weight: grad_norm = 0.177125
  l1.bias: grad_norm = 0.003398
  l2.weight: grad_norm = 0.123369
Total gradient norm: 0.338765
=== Actor Training Debug (Iteration 8938) ===
Q mean: -14.376779
Q std: 19.635204
Actor loss: 14.380751
Action reg: 0.003972
  l1.weight: grad_norm = 0.233930
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.179025
Total gradient norm: 0.448802
=== Actor Training Debug (Iteration 8939) ===
Q mean: -13.555063
Q std: 20.715012
Actor loss: 13.559009
Action reg: 0.003945
  l1.weight: grad_norm = 0.276395
  l1.bias: grad_norm = 0.003116
  l2.weight: grad_norm = 0.214447
Total gradient norm: 0.601318
=== Actor Training Debug (Iteration 8940) ===
Q mean: -15.472603
Q std: 21.683783
Actor loss: 15.476564
Action reg: 0.003961
  l1.weight: grad_norm = 0.241694
  l1.bias: grad_norm = 0.001070
  l2.weight: grad_norm = 0.181049
Total gradient norm: 0.492789
=== Actor Training Debug (Iteration 8941) ===
Q mean: -12.649956
Q std: 18.081785
Actor loss: 12.653922
Action reg: 0.003966
  l1.weight: grad_norm = 0.139259
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.094595
Total gradient norm: 0.231096
=== Actor Training Debug (Iteration 8942) ===
Q mean: -15.972418
Q std: 22.257673
Actor loss: 15.976373
Action reg: 0.003955
  l1.weight: grad_norm = 0.202605
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.152290
Total gradient norm: 0.420342
=== Actor Training Debug (Iteration 8943) ===
Q mean: -15.022453
Q std: 21.415552
Actor loss: 15.026382
Action reg: 0.003929
  l1.weight: grad_norm = 0.206280
  l1.bias: grad_norm = 0.001237
  l2.weight: grad_norm = 0.155685
Total gradient norm: 0.435418
=== Actor Training Debug (Iteration 8944) ===
Q mean: -16.750816
Q std: 21.698099
Actor loss: 16.754774
Action reg: 0.003957
  l1.weight: grad_norm = 0.316520
  l1.bias: grad_norm = 0.000589
  l2.weight: grad_norm = 0.257957
Total gradient norm: 0.722309
=== Actor Training Debug (Iteration 8945) ===
Q mean: -13.514781
Q std: 18.982056
Actor loss: 13.518725
Action reg: 0.003944
  l1.weight: grad_norm = 0.297469
  l1.bias: grad_norm = 0.000654
  l2.weight: grad_norm = 0.265271
Total gradient norm: 0.650995
=== Actor Training Debug (Iteration 8946) ===
Q mean: -15.038953
Q std: 19.400934
Actor loss: 15.042924
Action reg: 0.003972
  l1.weight: grad_norm = 0.493721
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.347080
Total gradient norm: 0.966006
=== Actor Training Debug (Iteration 8947) ===
Q mean: -15.946988
Q std: 21.604305
Actor loss: 15.950958
Action reg: 0.003970
  l1.weight: grad_norm = 0.216104
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.164687
Total gradient norm: 0.478041
=== Actor Training Debug (Iteration 8948) ===
Q mean: -15.301437
Q std: 21.879225
Actor loss: 15.305387
Action reg: 0.003950
  l1.weight: grad_norm = 0.143317
  l1.bias: grad_norm = 0.002734
  l2.weight: grad_norm = 0.105971
Total gradient norm: 0.315776
=== Actor Training Debug (Iteration 8949) ===
Q mean: -14.120713
Q std: 20.408293
Actor loss: 14.124667
Action reg: 0.003953
  l1.weight: grad_norm = 0.492054
  l1.bias: grad_norm = 0.003067
  l2.weight: grad_norm = 0.417030
Total gradient norm: 1.123478
=== Actor Training Debug (Iteration 8950) ===
Q mean: -14.361823
Q std: 19.815920
Actor loss: 14.365780
Action reg: 0.003956
  l1.weight: grad_norm = 0.405764
  l1.bias: grad_norm = 0.003187
  l2.weight: grad_norm = 0.267251
Total gradient norm: 0.690852
=== Actor Training Debug (Iteration 8951) ===
Q mean: -14.663859
Q std: 20.065710
Actor loss: 14.667804
Action reg: 0.003944
  l1.weight: grad_norm = 0.411211
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.245816
Total gradient norm: 0.714145
=== Actor Training Debug (Iteration 8952) ===
Q mean: -14.171434
Q std: 20.346342
Actor loss: 14.175356
Action reg: 0.003922
  l1.weight: grad_norm = 0.303358
  l1.bias: grad_norm = 0.000971
  l2.weight: grad_norm = 0.235210
Total gradient norm: 0.693382
=== Actor Training Debug (Iteration 8953) ===
Q mean: -14.974089
Q std: 19.888746
Actor loss: 14.978060
Action reg: 0.003971
  l1.weight: grad_norm = 0.225882
  l1.bias: grad_norm = 0.001745
  l2.weight: grad_norm = 0.143861
Total gradient norm: 0.415478
=== Actor Training Debug (Iteration 8954) ===
Q mean: -14.387634
Q std: 21.076302
Actor loss: 14.391594
Action reg: 0.003960
  l1.weight: grad_norm = 0.210443
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.163692
Total gradient norm: 0.527168
=== Actor Training Debug (Iteration 8955) ===
Q mean: -14.714964
Q std: 20.661400
Actor loss: 14.718926
Action reg: 0.003963
  l1.weight: grad_norm = 0.194684
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.135140
Total gradient norm: 0.331105
=== Actor Training Debug (Iteration 8956) ===
Q mean: -15.233994
Q std: 21.242891
Actor loss: 15.237967
Action reg: 0.003973
  l1.weight: grad_norm = 0.136504
  l1.bias: grad_norm = 0.001824
  l2.weight: grad_norm = 0.096850
Total gradient norm: 0.268315
=== Actor Training Debug (Iteration 8957) ===
Q mean: -15.897459
Q std: 20.712667
Actor loss: 15.901387
Action reg: 0.003929
  l1.weight: grad_norm = 0.228076
  l1.bias: grad_norm = 0.005424
  l2.weight: grad_norm = 0.182027
Total gradient norm: 0.510812
=== Actor Training Debug (Iteration 8958) ===
Q mean: -12.587465
Q std: 20.210390
Actor loss: 12.591403
Action reg: 0.003938
  l1.weight: grad_norm = 0.229435
  l1.bias: grad_norm = 0.002099
  l2.weight: grad_norm = 0.181898
Total gradient norm: 0.475567
=== Actor Training Debug (Iteration 8959) ===
Q mean: -13.870517
Q std: 20.733608
Actor loss: 13.874477
Action reg: 0.003961
  l1.weight: grad_norm = 0.239058
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.190657
Total gradient norm: 0.498957
=== Actor Training Debug (Iteration 8960) ===
Q mean: -15.028689
Q std: 20.961218
Actor loss: 15.032641
Action reg: 0.003952
  l1.weight: grad_norm = 0.271768
  l1.bias: grad_norm = 0.001872
  l2.weight: grad_norm = 0.207110
Total gradient norm: 0.622311
=== Actor Training Debug (Iteration 8961) ===
Q mean: -16.068142
Q std: 21.215403
Actor loss: 16.072086
Action reg: 0.003944
  l1.weight: grad_norm = 0.282503
  l1.bias: grad_norm = 0.003051
  l2.weight: grad_norm = 0.184984
Total gradient norm: 0.494507
=== Actor Training Debug (Iteration 8962) ===
Q mean: -14.620455
Q std: 21.274910
Actor loss: 14.624426
Action reg: 0.003971
  l1.weight: grad_norm = 0.302689
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.222524
Total gradient norm: 0.598782
=== Actor Training Debug (Iteration 8963) ===
Q mean: -14.716623
Q std: 19.903799
Actor loss: 14.720572
Action reg: 0.003949
  l1.weight: grad_norm = 0.318231
  l1.bias: grad_norm = 0.001693
  l2.weight: grad_norm = 0.195037
Total gradient norm: 0.551420
=== Actor Training Debug (Iteration 8964) ===
Q mean: -16.356188
Q std: 23.696840
Actor loss: 16.360138
Action reg: 0.003949
  l1.weight: grad_norm = 0.312188
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.244015
Total gradient norm: 0.626052
=== Actor Training Debug (Iteration 8965) ===
Q mean: -12.574636
Q std: 18.884068
Actor loss: 12.578582
Action reg: 0.003946
  l1.weight: grad_norm = 0.299320
  l1.bias: grad_norm = 0.001270
  l2.weight: grad_norm = 0.197606
Total gradient norm: 0.529724
=== Actor Training Debug (Iteration 8966) ===
Q mean: -14.989426
Q std: 21.219423
Actor loss: 14.993383
Action reg: 0.003958
  l1.weight: grad_norm = 0.253811
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.203741
Total gradient norm: 0.531436
=== Actor Training Debug (Iteration 8967) ===
Q mean: -13.857246
Q std: 21.215149
Actor loss: 13.861168
Action reg: 0.003921
  l1.weight: grad_norm = 0.322831
  l1.bias: grad_norm = 0.001156
  l2.weight: grad_norm = 0.238286
Total gradient norm: 0.622127
=== Actor Training Debug (Iteration 8968) ===
Q mean: -15.695518
Q std: 22.400270
Actor loss: 15.699473
Action reg: 0.003955
  l1.weight: grad_norm = 0.323123
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.219722
Total gradient norm: 0.638762
=== Actor Training Debug (Iteration 8969) ===
Q mean: -15.668426
Q std: 19.727097
Actor loss: 15.672387
Action reg: 0.003961
  l1.weight: grad_norm = 0.225059
  l1.bias: grad_norm = 0.003051
  l2.weight: grad_norm = 0.188813
Total gradient norm: 0.518568
=== Actor Training Debug (Iteration 8970) ===
Q mean: -15.624686
Q std: 21.778067
Actor loss: 15.628647
Action reg: 0.003960
  l1.weight: grad_norm = 0.341214
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.244811
Total gradient norm: 0.673293
=== Actor Training Debug (Iteration 8971) ===
Q mean: -15.004908
Q std: 20.226818
Actor loss: 15.008859
Action reg: 0.003951
  l1.weight: grad_norm = 0.285537
  l1.bias: grad_norm = 0.003054
  l2.weight: grad_norm = 0.206020
Total gradient norm: 0.588538
=== Actor Training Debug (Iteration 8972) ===
Q mean: -16.767567
Q std: 22.598846
Actor loss: 16.771524
Action reg: 0.003957
  l1.weight: grad_norm = 0.162909
  l1.bias: grad_norm = 0.000596
  l2.weight: grad_norm = 0.137633
Total gradient norm: 0.369807
=== Actor Training Debug (Iteration 8973) ===
Q mean: -13.463434
Q std: 20.117285
Actor loss: 13.467393
Action reg: 0.003959
  l1.weight: grad_norm = 0.189145
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.138833
Total gradient norm: 0.350837
=== Actor Training Debug (Iteration 8974) ===
Q mean: -12.974915
Q std: 18.904789
Actor loss: 12.978866
Action reg: 0.003951
  l1.weight: grad_norm = 0.263121
  l1.bias: grad_norm = 0.002648
  l2.weight: grad_norm = 0.192801
Total gradient norm: 0.547926
=== Actor Training Debug (Iteration 8975) ===
Q mean: -17.049751
Q std: 22.262106
Actor loss: 17.053703
Action reg: 0.003953
  l1.weight: grad_norm = 0.204900
  l1.bias: grad_norm = 0.003533
  l2.weight: grad_norm = 0.134890
Total gradient norm: 0.386794
=== Actor Training Debug (Iteration 8976) ===
Q mean: -14.852161
Q std: 21.542995
Actor loss: 14.856130
Action reg: 0.003968
  l1.weight: grad_norm = 0.147932
  l1.bias: grad_norm = 0.003270
  l2.weight: grad_norm = 0.117669
Total gradient norm: 0.376035
=== Actor Training Debug (Iteration 8977) ===
Q mean: -14.900726
Q std: 20.232349
Actor loss: 14.904667
Action reg: 0.003941
  l1.weight: grad_norm = 0.315845
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.211734
Total gradient norm: 0.541306
=== Actor Training Debug (Iteration 8978) ===
Q mean: -13.119531
Q std: 20.321514
Actor loss: 13.123486
Action reg: 0.003955
  l1.weight: grad_norm = 0.264644
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.186793
Total gradient norm: 0.528883
=== Actor Training Debug (Iteration 8979) ===
Q mean: -12.401661
Q std: 19.561613
Actor loss: 12.405592
Action reg: 0.003931
  l1.weight: grad_norm = 0.397320
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.307247
Total gradient norm: 0.906531
=== Actor Training Debug (Iteration 8980) ===
Q mean: -14.550258
Q std: 20.882431
Actor loss: 14.554227
Action reg: 0.003969
  l1.weight: grad_norm = 0.205498
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.151150
Total gradient norm: 0.415264
=== Actor Training Debug (Iteration 8981) ===
Q mean: -14.239637
Q std: 20.531414
Actor loss: 14.243565
Action reg: 0.003927
  l1.weight: grad_norm = 0.399067
  l1.bias: grad_norm = 0.001695
  l2.weight: grad_norm = 0.329038
Total gradient norm: 0.828014
=== Actor Training Debug (Iteration 8982) ===
Q mean: -14.767741
Q std: 20.837286
Actor loss: 14.771698
Action reg: 0.003957
  l1.weight: grad_norm = 0.382803
  l1.bias: grad_norm = 0.001454
  l2.weight: grad_norm = 0.267413
Total gradient norm: 0.768792
=== Actor Training Debug (Iteration 8983) ===
Q mean: -14.359978
Q std: 20.710857
Actor loss: 14.363942
Action reg: 0.003964
  l1.weight: grad_norm = 0.105415
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.082868
Total gradient norm: 0.215226
=== Actor Training Debug (Iteration 8984) ===
Q mean: -12.770638
Q std: 18.233433
Actor loss: 12.774592
Action reg: 0.003955
  l1.weight: grad_norm = 0.246383
  l1.bias: grad_norm = 0.002876
  l2.weight: grad_norm = 0.167040
Total gradient norm: 0.447330
=== Actor Training Debug (Iteration 8985) ===
Q mean: -13.902117
Q std: 21.179932
Actor loss: 13.906059
Action reg: 0.003942
  l1.weight: grad_norm = 0.356316
  l1.bias: grad_norm = 0.002649
  l2.weight: grad_norm = 0.237143
Total gradient norm: 0.624950
=== Actor Training Debug (Iteration 8986) ===
Q mean: -15.049714
Q std: 20.663198
Actor loss: 15.053669
Action reg: 0.003955
  l1.weight: grad_norm = 0.164666
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.148073
Total gradient norm: 0.398806
=== Actor Training Debug (Iteration 8987) ===
Q mean: -12.611036
Q std: 17.804121
Actor loss: 12.614983
Action reg: 0.003946
  l1.weight: grad_norm = 0.293595
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.205806
Total gradient norm: 0.553711
=== Actor Training Debug (Iteration 8988) ===
Q mean: -15.631889
Q std: 21.739162
Actor loss: 15.635855
Action reg: 0.003966
  l1.weight: grad_norm = 0.205512
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.175259
Total gradient norm: 0.454690
=== Actor Training Debug (Iteration 8989) ===
Q mean: -16.056835
Q std: 22.694044
Actor loss: 16.060791
Action reg: 0.003957
  l1.weight: grad_norm = 0.228054
  l1.bias: grad_norm = 0.001667
  l2.weight: grad_norm = 0.183113
Total gradient norm: 0.502750
=== Actor Training Debug (Iteration 8990) ===
Q mean: -12.694953
Q std: 19.839720
Actor loss: 12.698908
Action reg: 0.003955
  l1.weight: grad_norm = 0.248377
  l1.bias: grad_norm = 0.001654
  l2.weight: grad_norm = 0.157858
Total gradient norm: 0.411249
=== Actor Training Debug (Iteration 8991) ===
Q mean: -12.880865
Q std: 19.839735
Actor loss: 12.884791
Action reg: 0.003927
  l1.weight: grad_norm = 0.268582
  l1.bias: grad_norm = 0.001566
  l2.weight: grad_norm = 0.204538
Total gradient norm: 0.518313
=== Actor Training Debug (Iteration 8992) ===
Q mean: -16.302877
Q std: 21.972612
Actor loss: 16.306843
Action reg: 0.003965
  l1.weight: grad_norm = 0.357490
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.232203
Total gradient norm: 0.613906
=== Actor Training Debug (Iteration 8993) ===
Q mean: -15.323107
Q std: 21.176302
Actor loss: 15.327075
Action reg: 0.003968
  l1.weight: grad_norm = 0.385641
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.311064
Total gradient norm: 1.053804
=== Actor Training Debug (Iteration 8994) ===
Q mean: -15.048980
Q std: 20.400639
Actor loss: 15.052932
Action reg: 0.003952
  l1.weight: grad_norm = 0.518709
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.391164
Total gradient norm: 1.173080
=== Actor Training Debug (Iteration 8995) ===
Q mean: -13.943095
Q std: 20.969912
Actor loss: 13.947070
Action reg: 0.003975
  l1.weight: grad_norm = 0.329732
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.221918
Total gradient norm: 0.520691
=== Actor Training Debug (Iteration 8996) ===
Q mean: -15.426846
Q std: 21.223528
Actor loss: 15.430789
Action reg: 0.003944
  l1.weight: grad_norm = 0.444713
  l1.bias: grad_norm = 0.001689
  l2.weight: grad_norm = 0.304823
Total gradient norm: 0.862564
=== Actor Training Debug (Iteration 8997) ===
Q mean: -14.646022
Q std: 20.679670
Actor loss: 14.649976
Action reg: 0.003954
  l1.weight: grad_norm = 0.272501
  l1.bias: grad_norm = 0.001583
  l2.weight: grad_norm = 0.188413
Total gradient norm: 0.534460
=== Actor Training Debug (Iteration 8998) ===
Q mean: -16.506603
Q std: 22.443876
Actor loss: 16.510569
Action reg: 0.003966
  l1.weight: grad_norm = 0.253362
  l1.bias: grad_norm = 0.001709
  l2.weight: grad_norm = 0.197723
Total gradient norm: 0.553200
=== Actor Training Debug (Iteration 8999) ===
Q mean: -14.433991
Q std: 20.748888
Actor loss: 14.437957
Action reg: 0.003966
  l1.weight: grad_norm = 0.483899
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.356656
Total gradient norm: 0.967215
=== Actor Training Debug (Iteration 9000) ===
Q mean: -13.367802
Q std: 20.336601
Actor loss: 13.371764
Action reg: 0.003963
  l1.weight: grad_norm = 0.328198
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.230164
Total gradient norm: 0.592962
Step 14000: Critic Loss: 1.2095, Actor Loss: 13.3718, Q Value: -13.3678
  Average reward: -316.271 | Average length: 100.0
Evaluation at episode 140: -316.271
=== Actor Training Debug (Iteration 9001) ===
Q mean: -16.119728
Q std: 23.642256
Actor loss: 16.123692
Action reg: 0.003964
  l1.weight: grad_norm = 0.188427
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.139833
Total gradient norm: 0.365132
=== Actor Training Debug (Iteration 9002) ===
Q mean: -15.445365
Q std: 21.122240
Actor loss: 15.449312
Action reg: 0.003947
  l1.weight: grad_norm = 0.277057
  l1.bias: grad_norm = 0.000637
  l2.weight: grad_norm = 0.198788
Total gradient norm: 0.496275
=== Actor Training Debug (Iteration 9003) ===
Q mean: -15.268082
Q std: 20.987307
Actor loss: 15.272012
Action reg: 0.003931
  l1.weight: grad_norm = 0.174525
  l1.bias: grad_norm = 0.004017
  l2.weight: grad_norm = 0.132265
Total gradient norm: 0.459896
=== Actor Training Debug (Iteration 9004) ===
Q mean: -14.390717
Q std: 20.284649
Actor loss: 14.394686
Action reg: 0.003969
  l1.weight: grad_norm = 0.339869
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.214355
Total gradient norm: 0.567526
=== Actor Training Debug (Iteration 9005) ===
Q mean: -14.876570
Q std: 21.614422
Actor loss: 14.880526
Action reg: 0.003956
  l1.weight: grad_norm = 0.261828
  l1.bias: grad_norm = 0.000510
  l2.weight: grad_norm = 0.214825
Total gradient norm: 0.594299
=== Actor Training Debug (Iteration 9006) ===
Q mean: -15.897645
Q std: 20.854462
Actor loss: 15.901616
Action reg: 0.003971
  l1.weight: grad_norm = 0.257922
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.187097
Total gradient norm: 0.486481
=== Actor Training Debug (Iteration 9007) ===
Q mean: -13.993811
Q std: 20.189747
Actor loss: 13.997773
Action reg: 0.003962
  l1.weight: grad_norm = 0.188495
  l1.bias: grad_norm = 0.001453
  l2.weight: grad_norm = 0.137959
Total gradient norm: 0.400265
=== Actor Training Debug (Iteration 9008) ===
Q mean: -14.332790
Q std: 20.985924
Actor loss: 14.336751
Action reg: 0.003961
  l1.weight: grad_norm = 0.169959
  l1.bias: grad_norm = 0.000544
  l2.weight: grad_norm = 0.138555
Total gradient norm: 0.344349
=== Actor Training Debug (Iteration 9009) ===
Q mean: -12.774057
Q std: 19.183023
Actor loss: 12.778008
Action reg: 0.003951
  l1.weight: grad_norm = 0.131560
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.090407
Total gradient norm: 0.253873
=== Actor Training Debug (Iteration 9010) ===
Q mean: -13.752382
Q std: 20.050186
Actor loss: 13.756338
Action reg: 0.003956
  l1.weight: grad_norm = 0.424631
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.322192
Total gradient norm: 0.854210
=== Actor Training Debug (Iteration 9011) ===
Q mean: -15.535451
Q std: 22.663042
Actor loss: 15.539392
Action reg: 0.003941
  l1.weight: grad_norm = 0.612109
  l1.bias: grad_norm = 0.001783
  l2.weight: grad_norm = 0.411285
Total gradient norm: 1.193154
=== Actor Training Debug (Iteration 9012) ===
Q mean: -12.656399
Q std: 18.468212
Actor loss: 12.660344
Action reg: 0.003945
  l1.weight: grad_norm = 0.126193
  l1.bias: grad_norm = 0.002792
  l2.weight: grad_norm = 0.104225
Total gradient norm: 0.273586
=== Actor Training Debug (Iteration 9013) ===
Q mean: -15.442597
Q std: 21.838970
Actor loss: 15.446551
Action reg: 0.003954
  l1.weight: grad_norm = 0.217709
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.161311
Total gradient norm: 0.444089
=== Actor Training Debug (Iteration 9014) ===
Q mean: -12.468380
Q std: 17.717016
Actor loss: 12.472327
Action reg: 0.003947
  l1.weight: grad_norm = 0.394101
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.316579
Total gradient norm: 0.854486
=== Actor Training Debug (Iteration 9015) ===
Q mean: -16.158314
Q std: 22.332403
Actor loss: 16.162281
Action reg: 0.003966
  l1.weight: grad_norm = 0.299529
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.211079
Total gradient norm: 0.598669
=== Actor Training Debug (Iteration 9016) ===
Q mean: -14.774452
Q std: 21.416441
Actor loss: 14.778407
Action reg: 0.003955
  l1.weight: grad_norm = 0.195215
  l1.bias: grad_norm = 0.002951
  l2.weight: grad_norm = 0.138674
Total gradient norm: 0.351371
=== Actor Training Debug (Iteration 9017) ===
Q mean: -15.423754
Q std: 22.004042
Actor loss: 15.427722
Action reg: 0.003968
  l1.weight: grad_norm = 0.208792
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.157800
Total gradient norm: 0.464976
=== Actor Training Debug (Iteration 9018) ===
Q mean: -16.134430
Q std: 23.355867
Actor loss: 16.138374
Action reg: 0.003945
  l1.weight: grad_norm = 0.191253
  l1.bias: grad_norm = 0.000524
  l2.weight: grad_norm = 0.135999
Total gradient norm: 0.354431
=== Actor Training Debug (Iteration 9019) ===
Q mean: -16.104698
Q std: 21.370729
Actor loss: 16.108675
Action reg: 0.003976
  l1.weight: grad_norm = 0.425147
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.263788
Total gradient norm: 0.681187
=== Actor Training Debug (Iteration 9020) ===
Q mean: -16.768681
Q std: 23.429033
Actor loss: 16.772646
Action reg: 0.003965
  l1.weight: grad_norm = 0.328562
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.240144
Total gradient norm: 0.645791
=== Actor Training Debug (Iteration 9021) ===
Q mean: -14.250696
Q std: 19.685966
Actor loss: 14.254659
Action reg: 0.003963
  l1.weight: grad_norm = 0.310131
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.213536
Total gradient norm: 0.580172
=== Actor Training Debug (Iteration 9022) ===
Q mean: -12.330498
Q std: 18.824184
Actor loss: 12.334476
Action reg: 0.003978
  l1.weight: grad_norm = 0.317427
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.231607
Total gradient norm: 0.592782
=== Actor Training Debug (Iteration 9023) ===
Q mean: -14.290553
Q std: 21.035904
Actor loss: 14.294506
Action reg: 0.003953
  l1.weight: grad_norm = 0.383544
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.314372
Total gradient norm: 0.785968
=== Actor Training Debug (Iteration 9024) ===
Q mean: -11.944072
Q std: 19.418934
Actor loss: 11.948009
Action reg: 0.003937
  l1.weight: grad_norm = 0.263825
  l1.bias: grad_norm = 0.001205
  l2.weight: grad_norm = 0.220714
Total gradient norm: 0.646669
=== Actor Training Debug (Iteration 9025) ===
Q mean: -16.081717
Q std: 21.979319
Actor loss: 16.085667
Action reg: 0.003950
  l1.weight: grad_norm = 0.260420
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.204119
Total gradient norm: 0.538456
=== Actor Training Debug (Iteration 9026) ===
Q mean: -15.389200
Q std: 22.105791
Actor loss: 15.393170
Action reg: 0.003970
  l1.weight: grad_norm = 0.218764
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.144162
Total gradient norm: 0.382406
=== Actor Training Debug (Iteration 9027) ===
Q mean: -14.454828
Q std: 20.849665
Actor loss: 14.458771
Action reg: 0.003942
  l1.weight: grad_norm = 0.360978
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.245949
Total gradient norm: 0.647750
=== Actor Training Debug (Iteration 9028) ===
Q mean: -14.073770
Q std: 19.718330
Actor loss: 14.077740
Action reg: 0.003971
  l1.weight: grad_norm = 0.208497
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.167542
Total gradient norm: 0.439540
=== Actor Training Debug (Iteration 9029) ===
Q mean: -15.028403
Q std: 21.200150
Actor loss: 15.032337
Action reg: 0.003933
  l1.weight: grad_norm = 0.319282
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.217021
Total gradient norm: 0.572722
=== Actor Training Debug (Iteration 9030) ===
Q mean: -14.461870
Q std: 20.900158
Actor loss: 14.465835
Action reg: 0.003965
  l1.weight: grad_norm = 0.164637
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.113471
Total gradient norm: 0.314463
=== Actor Training Debug (Iteration 9031) ===
Q mean: -15.338274
Q std: 21.298101
Actor loss: 15.342244
Action reg: 0.003971
  l1.weight: grad_norm = 0.212313
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.165297
Total gradient norm: 0.459364
=== Actor Training Debug (Iteration 9032) ===
Q mean: -14.076656
Q std: 20.158812
Actor loss: 14.080619
Action reg: 0.003962
  l1.weight: grad_norm = 0.164528
  l1.bias: grad_norm = 0.001328
  l2.weight: grad_norm = 0.119826
Total gradient norm: 0.317290
=== Actor Training Debug (Iteration 9033) ===
Q mean: -12.464065
Q std: 18.187050
Actor loss: 12.468001
Action reg: 0.003937
  l1.weight: grad_norm = 0.464237
  l1.bias: grad_norm = 0.003220
  l2.weight: grad_norm = 0.358028
Total gradient norm: 0.903515
=== Actor Training Debug (Iteration 9034) ===
Q mean: -13.648421
Q std: 19.677284
Actor loss: 13.652370
Action reg: 0.003949
  l1.weight: grad_norm = 0.176331
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.136033
Total gradient norm: 0.367789
=== Actor Training Debug (Iteration 9035) ===
Q mean: -14.586468
Q std: 20.377871
Actor loss: 14.590406
Action reg: 0.003938
  l1.weight: grad_norm = 0.174494
  l1.bias: grad_norm = 0.002174
  l2.weight: grad_norm = 0.119422
Total gradient norm: 0.335191
=== Actor Training Debug (Iteration 9036) ===
Q mean: -15.534578
Q std: 21.881046
Actor loss: 15.538545
Action reg: 0.003966
  l1.weight: grad_norm = 0.457189
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.330490
Total gradient norm: 0.867544
=== Actor Training Debug (Iteration 9037) ===
Q mean: -15.698726
Q std: 20.840860
Actor loss: 15.702693
Action reg: 0.003968
  l1.weight: grad_norm = 0.265880
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.180958
Total gradient norm: 0.517827
=== Actor Training Debug (Iteration 9038) ===
Q mean: -13.918499
Q std: 20.312342
Actor loss: 13.922460
Action reg: 0.003961
  l1.weight: grad_norm = 0.326806
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.233478
Total gradient norm: 0.646128
=== Actor Training Debug (Iteration 9039) ===
Q mean: -15.521315
Q std: 20.940821
Actor loss: 15.525268
Action reg: 0.003953
  l1.weight: grad_norm = 0.792196
  l1.bias: grad_norm = 0.001459
  l2.weight: grad_norm = 0.574548
Total gradient norm: 1.713533
=== Actor Training Debug (Iteration 9040) ===
Q mean: -15.244369
Q std: 21.027622
Actor loss: 15.248331
Action reg: 0.003962
  l1.weight: grad_norm = 0.288528
  l1.bias: grad_norm = 0.001802
  l2.weight: grad_norm = 0.197976
Total gradient norm: 0.527911
=== Actor Training Debug (Iteration 9041) ===
Q mean: -15.826398
Q std: 19.461931
Actor loss: 15.830340
Action reg: 0.003943
  l1.weight: grad_norm = 0.239309
  l1.bias: grad_norm = 0.001728
  l2.weight: grad_norm = 0.181835
Total gradient norm: 0.512900
=== Actor Training Debug (Iteration 9042) ===
Q mean: -14.176775
Q std: 20.687769
Actor loss: 14.180734
Action reg: 0.003958
  l1.weight: grad_norm = 0.559462
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.384127
Total gradient norm: 0.895489
=== Actor Training Debug (Iteration 9043) ===
Q mean: -14.076106
Q std: 18.374481
Actor loss: 14.080078
Action reg: 0.003972
  l1.weight: grad_norm = 0.402627
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.321094
Total gradient norm: 0.824061
=== Actor Training Debug (Iteration 9044) ===
Q mean: -16.529930
Q std: 22.861433
Actor loss: 16.533901
Action reg: 0.003971
  l1.weight: grad_norm = 0.438576
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.300574
Total gradient norm: 0.853892
=== Actor Training Debug (Iteration 9045) ===
Q mean: -15.797808
Q std: 20.817026
Actor loss: 15.801770
Action reg: 0.003963
  l1.weight: grad_norm = 0.218946
  l1.bias: grad_norm = 0.003373
  l2.weight: grad_norm = 0.158120
Total gradient norm: 0.426192
=== Actor Training Debug (Iteration 9046) ===
Q mean: -14.093532
Q std: 20.756416
Actor loss: 14.097462
Action reg: 0.003930
  l1.weight: grad_norm = 0.096863
  l1.bias: grad_norm = 0.001636
  l2.weight: grad_norm = 0.077636
Total gradient norm: 0.216838
=== Actor Training Debug (Iteration 9047) ===
Q mean: -14.281634
Q std: 20.673853
Actor loss: 14.285583
Action reg: 0.003950
  l1.weight: grad_norm = 0.547839
  l1.bias: grad_norm = 0.001302
  l2.weight: grad_norm = 0.393690
Total gradient norm: 1.054705
=== Actor Training Debug (Iteration 9048) ===
Q mean: -16.783215
Q std: 23.133099
Actor loss: 16.787182
Action reg: 0.003967
  l1.weight: grad_norm = 0.416964
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.259877
Total gradient norm: 0.705921
=== Actor Training Debug (Iteration 9049) ===
Q mean: -12.707788
Q std: 18.694525
Actor loss: 12.711746
Action reg: 0.003958
  l1.weight: grad_norm = 0.356758
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.277471
Total gradient norm: 0.781910
=== Actor Training Debug (Iteration 9050) ===
Q mean: -16.862385
Q std: 22.455118
Actor loss: 16.866362
Action reg: 0.003976
  l1.weight: grad_norm = 0.100004
  l1.bias: grad_norm = 0.001432
  l2.weight: grad_norm = 0.096194
Total gradient norm: 0.230673
=== Actor Training Debug (Iteration 9051) ===
Q mean: -14.495058
Q std: 19.736736
Actor loss: 14.499005
Action reg: 0.003948
  l1.weight: grad_norm = 0.261872
  l1.bias: grad_norm = 0.001190
  l2.weight: grad_norm = 0.201107
Total gradient norm: 0.591521
=== Actor Training Debug (Iteration 9052) ===
Q mean: -14.372809
Q std: 21.135832
Actor loss: 14.376785
Action reg: 0.003975
  l1.weight: grad_norm = 0.190718
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.141001
Total gradient norm: 0.358701
=== Actor Training Debug (Iteration 9053) ===
Q mean: -12.634167
Q std: 19.732418
Actor loss: 12.638133
Action reg: 0.003966
  l1.weight: grad_norm = 0.141821
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.109473
Total gradient norm: 0.283129
=== Actor Training Debug (Iteration 9054) ===
Q mean: -14.789121
Q std: 20.557053
Actor loss: 14.793084
Action reg: 0.003963
  l1.weight: grad_norm = 0.233394
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.160839
Total gradient norm: 0.449795
=== Actor Training Debug (Iteration 9055) ===
Q mean: -13.828100
Q std: 19.410084
Actor loss: 13.832059
Action reg: 0.003959
  l1.weight: grad_norm = 0.456460
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.299868
Total gradient norm: 0.779693
=== Actor Training Debug (Iteration 9056) ===
Q mean: -14.708247
Q std: 21.459105
Actor loss: 14.712208
Action reg: 0.003960
  l1.weight: grad_norm = 0.288960
  l1.bias: grad_norm = 0.001588
  l2.weight: grad_norm = 0.192595
Total gradient norm: 0.534928
=== Actor Training Debug (Iteration 9057) ===
Q mean: -13.832771
Q std: 19.743517
Actor loss: 13.836728
Action reg: 0.003957
  l1.weight: grad_norm = 0.149312
  l1.bias: grad_norm = 0.001541
  l2.weight: grad_norm = 0.112901
Total gradient norm: 0.314709
=== Actor Training Debug (Iteration 9058) ===
Q mean: -15.213346
Q std: 21.184456
Actor loss: 15.217313
Action reg: 0.003967
  l1.weight: grad_norm = 0.511010
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.354707
Total gradient norm: 0.957164
=== Actor Training Debug (Iteration 9059) ===
Q mean: -14.212334
Q std: 20.816975
Actor loss: 14.216299
Action reg: 0.003966
  l1.weight: grad_norm = 0.355631
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.273320
Total gradient norm: 0.721736
=== Actor Training Debug (Iteration 9060) ===
Q mean: -13.364017
Q std: 19.989643
Actor loss: 13.367985
Action reg: 0.003967
  l1.weight: grad_norm = 0.319621
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.256693
Total gradient norm: 0.693949
=== Actor Training Debug (Iteration 9061) ===
Q mean: -15.276406
Q std: 21.333853
Actor loss: 15.280354
Action reg: 0.003949
  l1.weight: grad_norm = 0.226318
  l1.bias: grad_norm = 0.000766
  l2.weight: grad_norm = 0.186726
Total gradient norm: 0.508267
=== Actor Training Debug (Iteration 9062) ===
Q mean: -15.136400
Q std: 20.998882
Actor loss: 15.140345
Action reg: 0.003945
  l1.weight: grad_norm = 0.355537
  l1.bias: grad_norm = 0.001012
  l2.weight: grad_norm = 0.271379
Total gradient norm: 0.768501
=== Actor Training Debug (Iteration 9063) ===
Q mean: -14.688511
Q std: 21.126913
Actor loss: 14.692467
Action reg: 0.003956
  l1.weight: grad_norm = 0.349774
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.241710
Total gradient norm: 0.655764
=== Actor Training Debug (Iteration 9064) ===
Q mean: -14.271170
Q std: 20.340652
Actor loss: 14.275133
Action reg: 0.003963
  l1.weight: grad_norm = 0.462691
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.344323
Total gradient norm: 0.872681
=== Actor Training Debug (Iteration 9065) ===
Q mean: -16.312023
Q std: 21.335766
Actor loss: 16.315992
Action reg: 0.003970
  l1.weight: grad_norm = 0.279688
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.213665
Total gradient norm: 0.597313
=== Actor Training Debug (Iteration 9066) ===
Q mean: -12.628531
Q std: 18.364660
Actor loss: 12.632489
Action reg: 0.003958
  l1.weight: grad_norm = 0.453810
  l1.bias: grad_norm = 0.001756
  l2.weight: grad_norm = 0.331856
Total gradient norm: 0.870770
=== Actor Training Debug (Iteration 9067) ===
Q mean: -15.035505
Q std: 21.204647
Actor loss: 15.039457
Action reg: 0.003952
  l1.weight: grad_norm = 0.221816
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.177530
Total gradient norm: 0.403534
=== Actor Training Debug (Iteration 9068) ===
Q mean: -15.882331
Q std: 21.004847
Actor loss: 15.886285
Action reg: 0.003954
  l1.weight: grad_norm = 0.276516
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.222742
Total gradient norm: 0.530777
=== Actor Training Debug (Iteration 9069) ===
Q mean: -16.233948
Q std: 22.152836
Actor loss: 16.237928
Action reg: 0.003981
  l1.weight: grad_norm = 0.275833
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.164740
Total gradient norm: 0.436685
=== Actor Training Debug (Iteration 9070) ===
Q mean: -14.504264
Q std: 20.827019
Actor loss: 14.508226
Action reg: 0.003963
  l1.weight: grad_norm = 0.164071
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.125383
Total gradient norm: 0.361939
=== Actor Training Debug (Iteration 9071) ===
Q mean: -14.747103
Q std: 20.818371
Actor loss: 14.751065
Action reg: 0.003962
  l1.weight: grad_norm = 0.432806
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.300923
Total gradient norm: 0.820018
=== Actor Training Debug (Iteration 9072) ===
Q mean: -15.810453
Q std: 21.854332
Actor loss: 15.814408
Action reg: 0.003955
  l1.weight: grad_norm = 0.223974
  l1.bias: grad_norm = 0.001632
  l2.weight: grad_norm = 0.158780
Total gradient norm: 0.402257
=== Actor Training Debug (Iteration 9073) ===
Q mean: -13.998829
Q std: 20.839384
Actor loss: 14.002795
Action reg: 0.003966
  l1.weight: grad_norm = 0.143476
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.107040
Total gradient norm: 0.304518
=== Actor Training Debug (Iteration 9074) ===
Q mean: -15.767810
Q std: 21.826227
Actor loss: 15.771785
Action reg: 0.003975
  l1.weight: grad_norm = 0.145568
  l1.bias: grad_norm = 0.001511
  l2.weight: grad_norm = 0.105354
Total gradient norm: 0.279963
=== Actor Training Debug (Iteration 9075) ===
Q mean: -15.236073
Q std: 21.355740
Actor loss: 15.240023
Action reg: 0.003949
  l1.weight: grad_norm = 0.378780
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.308202
Total gradient norm: 0.802022
=== Actor Training Debug (Iteration 9076) ===
Q mean: -14.124613
Q std: 19.819429
Actor loss: 14.128564
Action reg: 0.003952
  l1.weight: grad_norm = 0.291088
  l1.bias: grad_norm = 0.001243
  l2.weight: grad_norm = 0.240880
Total gradient norm: 0.871913
=== Actor Training Debug (Iteration 9077) ===
Q mean: -12.578579
Q std: 20.373793
Actor loss: 12.582525
Action reg: 0.003946
  l1.weight: grad_norm = 0.337483
  l1.bias: grad_norm = 0.000738
  l2.weight: grad_norm = 0.221698
Total gradient norm: 0.627333
=== Actor Training Debug (Iteration 9078) ===
Q mean: -15.462469
Q std: 22.124989
Actor loss: 15.466439
Action reg: 0.003970
  l1.weight: grad_norm = 0.363787
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.241736
Total gradient norm: 0.625287
=== Actor Training Debug (Iteration 9079) ===
Q mean: -14.208092
Q std: 19.588909
Actor loss: 14.212049
Action reg: 0.003957
  l1.weight: grad_norm = 0.316298
  l1.bias: grad_norm = 0.001452
  l2.weight: grad_norm = 0.264382
Total gradient norm: 0.751676
=== Actor Training Debug (Iteration 9080) ===
Q mean: -13.943754
Q std: 21.521688
Actor loss: 13.947714
Action reg: 0.003960
  l1.weight: grad_norm = 0.483196
  l1.bias: grad_norm = 0.001413
  l2.weight: grad_norm = 0.430291
Total gradient norm: 0.961314
=== Actor Training Debug (Iteration 9081) ===
Q mean: -14.383027
Q std: 19.504637
Actor loss: 14.386972
Action reg: 0.003946
  l1.weight: grad_norm = 0.398359
  l1.bias: grad_norm = 0.000772
  l2.weight: grad_norm = 0.338025
Total gradient norm: 0.935237
=== Actor Training Debug (Iteration 9082) ===
Q mean: -16.779045
Q std: 22.698095
Actor loss: 16.782990
Action reg: 0.003944
  l1.weight: grad_norm = 0.425140
  l1.bias: grad_norm = 0.001427
  l2.weight: grad_norm = 0.269199
Total gradient norm: 0.738196
=== Actor Training Debug (Iteration 9083) ===
Q mean: -13.986823
Q std: 18.863348
Actor loss: 13.990765
Action reg: 0.003941
  l1.weight: grad_norm = 0.248692
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.183563
Total gradient norm: 0.458032
=== Actor Training Debug (Iteration 9084) ===
Q mean: -12.830338
Q std: 16.822838
Actor loss: 12.834286
Action reg: 0.003948
  l1.weight: grad_norm = 0.209713
  l1.bias: grad_norm = 0.001610
  l2.weight: grad_norm = 0.161085
Total gradient norm: 0.401842
=== Actor Training Debug (Iteration 9085) ===
Q mean: -13.837241
Q std: 20.190536
Actor loss: 13.841188
Action reg: 0.003947
  l1.weight: grad_norm = 0.282522
  l1.bias: grad_norm = 0.000636
  l2.weight: grad_norm = 0.216476
Total gradient norm: 0.567211
=== Actor Training Debug (Iteration 9086) ===
Q mean: -14.427305
Q std: 20.416973
Actor loss: 14.431243
Action reg: 0.003938
  l1.weight: grad_norm = 0.280723
  l1.bias: grad_norm = 0.002726
  l2.weight: grad_norm = 0.226716
Total gradient norm: 0.619295
=== Actor Training Debug (Iteration 9087) ===
Q mean: -14.566608
Q std: 19.837557
Actor loss: 14.570560
Action reg: 0.003952
  l1.weight: grad_norm = 0.593393
  l1.bias: grad_norm = 0.000589
  l2.weight: grad_norm = 0.423667
Total gradient norm: 1.100017
=== Actor Training Debug (Iteration 9088) ===
Q mean: -14.061129
Q std: 20.458191
Actor loss: 14.065065
Action reg: 0.003937
  l1.weight: grad_norm = 0.239533
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.167697
Total gradient norm: 0.459898
=== Actor Training Debug (Iteration 9089) ===
Q mean: -14.846912
Q std: 20.466011
Actor loss: 14.850833
Action reg: 0.003920
  l1.weight: grad_norm = 0.423424
  l1.bias: grad_norm = 0.002048
  l2.weight: grad_norm = 0.348343
Total gradient norm: 0.921890
=== Actor Training Debug (Iteration 9090) ===
Q mean: -15.722815
Q std: 22.377827
Actor loss: 15.726782
Action reg: 0.003967
  l1.weight: grad_norm = 0.162112
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.117263
Total gradient norm: 0.330376
=== Actor Training Debug (Iteration 9091) ===
Q mean: -15.873505
Q std: 22.351730
Actor loss: 15.877479
Action reg: 0.003974
  l1.weight: grad_norm = 0.137154
  l1.bias: grad_norm = 0.000860
  l2.weight: grad_norm = 0.102919
Total gradient norm: 0.280757
=== Actor Training Debug (Iteration 9092) ===
Q mean: -17.282562
Q std: 21.865314
Actor loss: 17.286518
Action reg: 0.003955
  l1.weight: grad_norm = 0.237121
  l1.bias: grad_norm = 0.008815
  l2.weight: grad_norm = 0.185172
Total gradient norm: 0.571303
=== Actor Training Debug (Iteration 9093) ===
Q mean: -14.212856
Q std: 21.077486
Actor loss: 14.216814
Action reg: 0.003958
  l1.weight: grad_norm = 0.277525
  l1.bias: grad_norm = 0.003621
  l2.weight: grad_norm = 0.173943
Total gradient norm: 0.487291
=== Actor Training Debug (Iteration 9094) ===
Q mean: -12.801086
Q std: 16.771307
Actor loss: 12.805052
Action reg: 0.003965
  l1.weight: grad_norm = 0.334313
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.241866
Total gradient norm: 0.645314
=== Actor Training Debug (Iteration 9095) ===
Q mean: -16.160044
Q std: 22.028694
Actor loss: 16.164003
Action reg: 0.003960
  l1.weight: grad_norm = 0.161186
  l1.bias: grad_norm = 0.000827
  l2.weight: grad_norm = 0.144176
Total gradient norm: 0.499820
=== Actor Training Debug (Iteration 9096) ===
Q mean: -14.338451
Q std: 20.425600
Actor loss: 14.342401
Action reg: 0.003949
  l1.weight: grad_norm = 0.372518
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.253812
Total gradient norm: 0.671337
=== Actor Training Debug (Iteration 9097) ===
Q mean: -12.765789
Q std: 19.516211
Actor loss: 12.769731
Action reg: 0.003942
  l1.weight: grad_norm = 0.430412
  l1.bias: grad_norm = 0.003099
  l2.weight: grad_norm = 0.302853
Total gradient norm: 0.764123
=== Actor Training Debug (Iteration 9098) ===
Q mean: -13.848293
Q std: 18.974892
Actor loss: 13.852249
Action reg: 0.003956
  l1.weight: grad_norm = 0.112893
  l1.bias: grad_norm = 0.002636
  l2.weight: grad_norm = 0.087325
Total gradient norm: 0.241036
=== Actor Training Debug (Iteration 9099) ===
Q mean: -15.861840
Q std: 22.973125
Actor loss: 15.865816
Action reg: 0.003976
  l1.weight: grad_norm = 0.340210
  l1.bias: grad_norm = 0.001277
  l2.weight: grad_norm = 0.229464
Total gradient norm: 0.606353
=== Actor Training Debug (Iteration 9100) ===
Q mean: -13.405952
Q std: 20.038631
Actor loss: 13.409909
Action reg: 0.003957
  l1.weight: grad_norm = 0.205469
  l1.bias: grad_norm = 0.001353
  l2.weight: grad_norm = 0.147877
Total gradient norm: 0.375437
Episode 141: Steps=100, Reward=-259.335, Buffer_size=14100
=== Actor Training Debug (Iteration 9101) ===
Q mean: -15.736652
Q std: 22.060053
Actor loss: 15.740608
Action reg: 0.003956
  l1.weight: grad_norm = 0.348698
  l1.bias: grad_norm = 0.002792
  l2.weight: grad_norm = 0.291251
Total gradient norm: 0.760707
=== Actor Training Debug (Iteration 9102) ===
Q mean: -15.030435
Q std: 21.741304
Actor loss: 15.034379
Action reg: 0.003944
  l1.weight: grad_norm = 0.411995
  l1.bias: grad_norm = 0.002624
  l2.weight: grad_norm = 0.306671
Total gradient norm: 0.802723
=== Actor Training Debug (Iteration 9103) ===
Q mean: -16.628588
Q std: 22.222242
Actor loss: 16.632568
Action reg: 0.003980
  l1.weight: grad_norm = 0.225564
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.142608
Total gradient norm: 0.373316
=== Actor Training Debug (Iteration 9104) ===
Q mean: -12.468849
Q std: 20.381639
Actor loss: 12.472803
Action reg: 0.003954
  l1.weight: grad_norm = 0.329453
  l1.bias: grad_norm = 0.001820
  l2.weight: grad_norm = 0.239703
Total gradient norm: 0.635790
=== Actor Training Debug (Iteration 9105) ===
Q mean: -14.124730
Q std: 20.729153
Actor loss: 14.128685
Action reg: 0.003955
  l1.weight: grad_norm = 0.407441
  l1.bias: grad_norm = 0.001732
  l2.weight: grad_norm = 0.290395
Total gradient norm: 0.768470
=== Actor Training Debug (Iteration 9106) ===
Q mean: -14.686649
Q std: 22.040455
Actor loss: 14.690612
Action reg: 0.003962
  l1.weight: grad_norm = 0.273572
  l1.bias: grad_norm = 0.000646
  l2.weight: grad_norm = 0.215424
Total gradient norm: 0.599878
=== Actor Training Debug (Iteration 9107) ===
Q mean: -14.290526
Q std: 20.938282
Actor loss: 14.294488
Action reg: 0.003961
  l1.weight: grad_norm = 0.318743
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.236056
Total gradient norm: 0.617245
=== Actor Training Debug (Iteration 9108) ===
Q mean: -12.817696
Q std: 19.425449
Actor loss: 12.821670
Action reg: 0.003974
  l1.weight: grad_norm = 0.170622
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.141946
Total gradient norm: 0.401368
=== Actor Training Debug (Iteration 9109) ===
Q mean: -16.329620
Q std: 21.891304
Actor loss: 16.333570
Action reg: 0.003951
  l1.weight: grad_norm = 0.307700
  l1.bias: grad_norm = 0.001623
  l2.weight: grad_norm = 0.202101
Total gradient norm: 0.587052
=== Actor Training Debug (Iteration 9110) ===
Q mean: -16.390991
Q std: 21.580898
Actor loss: 16.394968
Action reg: 0.003976
  l1.weight: grad_norm = 0.276972
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.208865
Total gradient norm: 0.521731
=== Actor Training Debug (Iteration 9111) ===
Q mean: -16.130756
Q std: 21.298571
Actor loss: 16.134699
Action reg: 0.003942
  l1.weight: grad_norm = 0.281064
  l1.bias: grad_norm = 0.003869
  l2.weight: grad_norm = 0.192110
Total gradient norm: 0.566182
=== Actor Training Debug (Iteration 9112) ===
Q mean: -14.360953
Q std: 21.877853
Actor loss: 14.364891
Action reg: 0.003938
  l1.weight: grad_norm = 0.527072
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.422767
Total gradient norm: 1.206011
=== Actor Training Debug (Iteration 9113) ===
Q mean: -18.072266
Q std: 23.772654
Actor loss: 18.076227
Action reg: 0.003962
  l1.weight: grad_norm = 0.386399
  l1.bias: grad_norm = 0.001262
  l2.weight: grad_norm = 0.284602
Total gradient norm: 0.886493
=== Actor Training Debug (Iteration 9114) ===
Q mean: -14.100843
Q std: 21.041409
Actor loss: 14.104805
Action reg: 0.003961
  l1.weight: grad_norm = 0.171041
  l1.bias: grad_norm = 0.001147
  l2.weight: grad_norm = 0.153165
Total gradient norm: 0.350971
=== Actor Training Debug (Iteration 9115) ===
Q mean: -15.281855
Q std: 22.502666
Actor loss: 15.285821
Action reg: 0.003966
  l1.weight: grad_norm = 0.336218
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.241398
Total gradient norm: 0.694162
=== Actor Training Debug (Iteration 9116) ===
Q mean: -17.721748
Q std: 23.641394
Actor loss: 17.725708
Action reg: 0.003959
  l1.weight: grad_norm = 0.610307
  l1.bias: grad_norm = 0.002638
  l2.weight: grad_norm = 0.423813
Total gradient norm: 1.483464
=== Actor Training Debug (Iteration 9117) ===
Q mean: -16.244759
Q std: 22.336294
Actor loss: 16.248724
Action reg: 0.003966
  l1.weight: grad_norm = 0.134060
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.112521
Total gradient norm: 0.264668
=== Actor Training Debug (Iteration 9118) ===
Q mean: -15.553648
Q std: 20.598028
Actor loss: 15.557601
Action reg: 0.003953
  l1.weight: grad_norm = 0.271205
  l1.bias: grad_norm = 0.000521
  l2.weight: grad_norm = 0.221708
Total gradient norm: 0.739820
=== Actor Training Debug (Iteration 9119) ===
Q mean: -12.607478
Q std: 19.792801
Actor loss: 12.611439
Action reg: 0.003960
  l1.weight: grad_norm = 0.259769
  l1.bias: grad_norm = 0.003359
  l2.weight: grad_norm = 0.163933
Total gradient norm: 0.440506
=== Actor Training Debug (Iteration 9120) ===
Q mean: -15.897213
Q std: 21.909636
Actor loss: 15.901181
Action reg: 0.003969
  l1.weight: grad_norm = 0.273777
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.182294
Total gradient norm: 0.452842
=== Actor Training Debug (Iteration 9121) ===
Q mean: -14.704623
Q std: 20.663568
Actor loss: 14.708579
Action reg: 0.003956
  l1.weight: grad_norm = 0.316775
  l1.bias: grad_norm = 0.001661
  l2.weight: grad_norm = 0.211285
Total gradient norm: 0.565253
=== Actor Training Debug (Iteration 9122) ===
Q mean: -13.809649
Q std: 19.343983
Actor loss: 13.813601
Action reg: 0.003952
  l1.weight: grad_norm = 0.284632
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.217596
Total gradient norm: 0.617049
=== Actor Training Debug (Iteration 9123) ===
Q mean: -13.997519
Q std: 18.629427
Actor loss: 14.001473
Action reg: 0.003954
  l1.weight: grad_norm = 0.177481
  l1.bias: grad_norm = 0.001720
  l2.weight: grad_norm = 0.131891
Total gradient norm: 0.352917
=== Actor Training Debug (Iteration 9124) ===
Q mean: -16.536465
Q std: 22.016104
Actor loss: 16.540430
Action reg: 0.003965
  l1.weight: grad_norm = 0.347886
  l1.bias: grad_norm = 0.001247
  l2.weight: grad_norm = 0.235389
Total gradient norm: 0.594653
=== Actor Training Debug (Iteration 9125) ===
Q mean: -13.900305
Q std: 19.429070
Actor loss: 13.904251
Action reg: 0.003946
  l1.weight: grad_norm = 0.278588
  l1.bias: grad_norm = 0.001659
  l2.weight: grad_norm = 0.188650
Total gradient norm: 0.492908
=== Actor Training Debug (Iteration 9126) ===
Q mean: -14.297667
Q std: 19.623913
Actor loss: 14.301633
Action reg: 0.003966
  l1.weight: grad_norm = 0.175595
  l1.bias: grad_norm = 0.003020
  l2.weight: grad_norm = 0.133007
Total gradient norm: 0.372399
=== Actor Training Debug (Iteration 9127) ===
Q mean: -16.029270
Q std: 21.403873
Actor loss: 16.033228
Action reg: 0.003958
  l1.weight: grad_norm = 0.178679
  l1.bias: grad_norm = 0.001196
  l2.weight: grad_norm = 0.144924
Total gradient norm: 0.352455
=== Actor Training Debug (Iteration 9128) ===
Q mean: -13.189001
Q std: 17.977350
Actor loss: 13.192950
Action reg: 0.003949
  l1.weight: grad_norm = 0.283307
  l1.bias: grad_norm = 0.004669
  l2.weight: grad_norm = 0.203878
Total gradient norm: 0.554722
=== Actor Training Debug (Iteration 9129) ===
Q mean: -16.411575
Q std: 21.709528
Actor loss: 16.415531
Action reg: 0.003957
  l1.weight: grad_norm = 0.231767
  l1.bias: grad_norm = 0.001549
  l2.weight: grad_norm = 0.168550
Total gradient norm: 0.479766
=== Actor Training Debug (Iteration 9130) ===
Q mean: -13.876595
Q std: 20.134136
Actor loss: 13.880560
Action reg: 0.003965
  l1.weight: grad_norm = 0.196459
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.153071
Total gradient norm: 0.420150
=== Actor Training Debug (Iteration 9131) ===
Q mean: -14.697777
Q std: 21.027065
Actor loss: 14.701721
Action reg: 0.003944
  l1.weight: grad_norm = 0.169275
  l1.bias: grad_norm = 0.001610
  l2.weight: grad_norm = 0.130500
Total gradient norm: 0.346179
=== Actor Training Debug (Iteration 9132) ===
Q mean: -15.316021
Q std: 20.719683
Actor loss: 15.319981
Action reg: 0.003960
  l1.weight: grad_norm = 0.223959
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.161031
Total gradient norm: 0.421172
=== Actor Training Debug (Iteration 9133) ===
Q mean: -15.382962
Q std: 21.154280
Actor loss: 15.386928
Action reg: 0.003966
  l1.weight: grad_norm = 0.196385
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.150277
Total gradient norm: 0.411049
=== Actor Training Debug (Iteration 9134) ===
Q mean: -13.082456
Q std: 21.121973
Actor loss: 13.086401
Action reg: 0.003946
  l1.weight: grad_norm = 0.242556
  l1.bias: grad_norm = 0.000694
  l2.weight: grad_norm = 0.175995
Total gradient norm: 0.515330
=== Actor Training Debug (Iteration 9135) ===
Q mean: -13.116648
Q std: 18.722708
Actor loss: 13.120597
Action reg: 0.003949
  l1.weight: grad_norm = 0.271585
  l1.bias: grad_norm = 0.001554
  l2.weight: grad_norm = 0.173981
Total gradient norm: 0.470373
=== Actor Training Debug (Iteration 9136) ===
Q mean: -17.039875
Q std: 22.394817
Actor loss: 17.043840
Action reg: 0.003966
  l1.weight: grad_norm = 0.264037
  l1.bias: grad_norm = 0.001589
  l2.weight: grad_norm = 0.206845
Total gradient norm: 0.640011
=== Actor Training Debug (Iteration 9137) ===
Q mean: -15.353072
Q std: 22.973738
Actor loss: 15.357045
Action reg: 0.003973
  l1.weight: grad_norm = 0.225541
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.157862
Total gradient norm: 0.410843
=== Actor Training Debug (Iteration 9138) ===
Q mean: -14.446753
Q std: 20.637138
Actor loss: 14.450704
Action reg: 0.003951
  l1.weight: grad_norm = 0.200586
  l1.bias: grad_norm = 0.002584
  l2.weight: grad_norm = 0.159271
Total gradient norm: 0.478067
=== Actor Training Debug (Iteration 9139) ===
Q mean: -13.316848
Q std: 18.428719
Actor loss: 13.320794
Action reg: 0.003946
  l1.weight: grad_norm = 0.394274
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.272979
Total gradient norm: 0.774618
=== Actor Training Debug (Iteration 9140) ===
Q mean: -15.709162
Q std: 21.819260
Actor loss: 15.713098
Action reg: 0.003936
  l1.weight: grad_norm = 0.162548
  l1.bias: grad_norm = 0.007715
  l2.weight: grad_norm = 0.133090
Total gradient norm: 0.427334
=== Actor Training Debug (Iteration 9141) ===
Q mean: -14.182707
Q std: 21.401268
Actor loss: 14.186678
Action reg: 0.003971
  l1.weight: grad_norm = 0.261046
  l1.bias: grad_norm = 0.002289
  l2.weight: grad_norm = 0.213632
Total gradient norm: 0.639146
=== Actor Training Debug (Iteration 9142) ===
Q mean: -18.948374
Q std: 24.155457
Actor loss: 18.952333
Action reg: 0.003960
  l1.weight: grad_norm = 0.558070
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.342031
Total gradient norm: 0.930004
=== Actor Training Debug (Iteration 9143) ===
Q mean: -13.721518
Q std: 20.329550
Actor loss: 13.725468
Action reg: 0.003950
  l1.weight: grad_norm = 0.199332
  l1.bias: grad_norm = 0.001517
  l2.weight: grad_norm = 0.150946
Total gradient norm: 0.411982
=== Actor Training Debug (Iteration 9144) ===
Q mean: -13.411045
Q std: 18.149134
Actor loss: 13.414992
Action reg: 0.003948
  l1.weight: grad_norm = 0.409346
  l1.bias: grad_norm = 0.001542
  l2.weight: grad_norm = 0.263318
Total gradient norm: 0.703189
=== Actor Training Debug (Iteration 9145) ===
Q mean: -12.014579
Q std: 17.941530
Actor loss: 12.018530
Action reg: 0.003951
  l1.weight: grad_norm = 0.138278
  l1.bias: grad_norm = 0.001937
  l2.weight: grad_norm = 0.101072
Total gradient norm: 0.261360
=== Actor Training Debug (Iteration 9146) ===
Q mean: -16.489208
Q std: 22.385502
Actor loss: 16.493156
Action reg: 0.003948
  l1.weight: grad_norm = 0.251059
  l1.bias: grad_norm = 0.001439
  l2.weight: grad_norm = 0.185627
Total gradient norm: 0.499996
=== Actor Training Debug (Iteration 9147) ===
Q mean: -15.638652
Q std: 20.492620
Actor loss: 15.642595
Action reg: 0.003944
  l1.weight: grad_norm = 0.432444
  l1.bias: grad_norm = 0.003117
  l2.weight: grad_norm = 0.367382
Total gradient norm: 1.034640
=== Actor Training Debug (Iteration 9148) ===
Q mean: -14.070280
Q std: 21.002813
Actor loss: 14.074212
Action reg: 0.003932
  l1.weight: grad_norm = 0.235488
  l1.bias: grad_norm = 0.002890
  l2.weight: grad_norm = 0.189505
Total gradient norm: 0.482809
=== Actor Training Debug (Iteration 9149) ===
Q mean: -14.246529
Q std: 21.185787
Actor loss: 14.250477
Action reg: 0.003948
  l1.weight: grad_norm = 0.285748
  l1.bias: grad_norm = 0.002902
  l2.weight: grad_norm = 0.248740
Total gradient norm: 0.637720
=== Actor Training Debug (Iteration 9150) ===
Q mean: -15.587902
Q std: 21.220488
Actor loss: 15.591869
Action reg: 0.003968
  l1.weight: grad_norm = 0.287785
  l1.bias: grad_norm = 0.001410
  l2.weight: grad_norm = 0.194024
Total gradient norm: 0.533243
=== Actor Training Debug (Iteration 9151) ===
Q mean: -14.833253
Q std: 21.548014
Actor loss: 14.837203
Action reg: 0.003950
  l1.weight: grad_norm = 0.278978
  l1.bias: grad_norm = 0.003026
  l2.weight: grad_norm = 0.196591
Total gradient norm: 0.565426
=== Actor Training Debug (Iteration 9152) ===
Q mean: -15.520934
Q std: 19.948139
Actor loss: 15.524906
Action reg: 0.003972
  l1.weight: grad_norm = 0.156875
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.113879
Total gradient norm: 0.320355
=== Actor Training Debug (Iteration 9153) ===
Q mean: -15.811645
Q std: 22.143303
Actor loss: 15.815607
Action reg: 0.003963
  l1.weight: grad_norm = 0.350737
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.283783
Total gradient norm: 0.732467
=== Actor Training Debug (Iteration 9154) ===
Q mean: -13.120836
Q std: 19.846088
Actor loss: 13.124804
Action reg: 0.003968
  l1.weight: grad_norm = 0.300951
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.231014
Total gradient norm: 0.639577
=== Actor Training Debug (Iteration 9155) ===
Q mean: -13.680426
Q std: 19.397148
Actor loss: 13.684358
Action reg: 0.003932
  l1.weight: grad_norm = 0.332202
  l1.bias: grad_norm = 0.000535
  l2.weight: grad_norm = 0.227821
Total gradient norm: 0.552983
=== Actor Training Debug (Iteration 9156) ===
Q mean: -13.980782
Q std: 19.556631
Actor loss: 13.984751
Action reg: 0.003969
  l1.weight: grad_norm = 0.260192
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.192052
Total gradient norm: 0.537858
=== Actor Training Debug (Iteration 9157) ===
Q mean: -13.214670
Q std: 18.041136
Actor loss: 13.218623
Action reg: 0.003953
  l1.weight: grad_norm = 0.218217
  l1.bias: grad_norm = 0.000809
  l2.weight: grad_norm = 0.180002
Total gradient norm: 0.477479
=== Actor Training Debug (Iteration 9158) ===
Q mean: -13.905861
Q std: 20.030323
Actor loss: 13.909819
Action reg: 0.003958
  l1.weight: grad_norm = 0.288707
  l1.bias: grad_norm = 0.003604
  l2.weight: grad_norm = 0.180000
Total gradient norm: 0.470705
=== Actor Training Debug (Iteration 9159) ===
Q mean: -15.837786
Q std: 21.911789
Actor loss: 15.841731
Action reg: 0.003945
  l1.weight: grad_norm = 0.332739
  l1.bias: grad_norm = 0.007028
  l2.weight: grad_norm = 0.205225
Total gradient norm: 0.659109
=== Actor Training Debug (Iteration 9160) ===
Q mean: -16.253611
Q std: 21.313789
Actor loss: 16.257587
Action reg: 0.003977
  l1.weight: grad_norm = 0.171224
  l1.bias: grad_norm = 0.002776
  l2.weight: grad_norm = 0.138077
Total gradient norm: 0.409336
=== Actor Training Debug (Iteration 9161) ===
Q mean: -15.261241
Q std: 21.892071
Actor loss: 15.265189
Action reg: 0.003948
  l1.weight: grad_norm = 0.177722
  l1.bias: grad_norm = 0.001255
  l2.weight: grad_norm = 0.127324
Total gradient norm: 0.320498
=== Actor Training Debug (Iteration 9162) ===
Q mean: -16.577152
Q std: 22.248375
Actor loss: 16.581106
Action reg: 0.003954
  l1.weight: grad_norm = 0.320402
  l1.bias: grad_norm = 0.000701
  l2.weight: grad_norm = 0.205809
Total gradient norm: 0.576756
=== Actor Training Debug (Iteration 9163) ===
Q mean: -14.673180
Q std: 22.189468
Actor loss: 14.677138
Action reg: 0.003959
  l1.weight: grad_norm = 0.322291
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.217360
Total gradient norm: 0.589066
=== Actor Training Debug (Iteration 9164) ===
Q mean: -13.628191
Q std: 22.312954
Actor loss: 13.632153
Action reg: 0.003961
  l1.weight: grad_norm = 0.899695
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.504735
Total gradient norm: 1.518751
=== Actor Training Debug (Iteration 9165) ===
Q mean: -15.618193
Q std: 20.723412
Actor loss: 15.622152
Action reg: 0.003959
  l1.weight: grad_norm = 0.334702
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.254090
Total gradient norm: 0.659966
=== Actor Training Debug (Iteration 9166) ===
Q mean: -12.845914
Q std: 18.402569
Actor loss: 12.849865
Action reg: 0.003951
  l1.weight: grad_norm = 0.407896
  l1.bias: grad_norm = 0.002615
  l2.weight: grad_norm = 0.358541
Total gradient norm: 1.079856
=== Actor Training Debug (Iteration 9167) ===
Q mean: -17.042355
Q std: 21.898932
Actor loss: 17.046324
Action reg: 0.003969
  l1.weight: grad_norm = 0.187088
  l1.bias: grad_norm = 0.003297
  l2.weight: grad_norm = 0.147946
Total gradient norm: 0.393883
=== Actor Training Debug (Iteration 9168) ===
Q mean: -14.634480
Q std: 20.265800
Actor loss: 14.638440
Action reg: 0.003960
  l1.weight: grad_norm = 0.329548
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.240212
Total gradient norm: 0.712142
=== Actor Training Debug (Iteration 9169) ===
Q mean: -13.890108
Q std: 19.137316
Actor loss: 13.894080
Action reg: 0.003972
  l1.weight: grad_norm = 0.238070
  l1.bias: grad_norm = 0.001400
  l2.weight: grad_norm = 0.190001
Total gradient norm: 0.554031
=== Actor Training Debug (Iteration 9170) ===
Q mean: -11.804926
Q std: 17.268564
Actor loss: 11.808883
Action reg: 0.003957
  l1.weight: grad_norm = 0.368529
  l1.bias: grad_norm = 0.003423
  l2.weight: grad_norm = 0.247845
Total gradient norm: 0.646231
=== Actor Training Debug (Iteration 9171) ===
Q mean: -14.396258
Q std: 20.426571
Actor loss: 14.400191
Action reg: 0.003933
  l1.weight: grad_norm = 0.351524
  l1.bias: grad_norm = 0.000811
  l2.weight: grad_norm = 0.285023
Total gradient norm: 0.737532
=== Actor Training Debug (Iteration 9172) ===
Q mean: -15.849419
Q std: 23.986816
Actor loss: 15.853373
Action reg: 0.003954
  l1.weight: grad_norm = 0.204802
  l1.bias: grad_norm = 0.003414
  l2.weight: grad_norm = 0.137102
Total gradient norm: 0.359536
=== Actor Training Debug (Iteration 9173) ===
Q mean: -14.660550
Q std: 20.042799
Actor loss: 14.664510
Action reg: 0.003960
  l1.weight: grad_norm = 0.175776
  l1.bias: grad_norm = 0.003698
  l2.weight: grad_norm = 0.145924
Total gradient norm: 0.448585
=== Actor Training Debug (Iteration 9174) ===
Q mean: -14.251472
Q std: 22.253664
Actor loss: 14.255432
Action reg: 0.003961
  l1.weight: grad_norm = 0.191798
  l1.bias: grad_norm = 0.001602
  l2.weight: grad_norm = 0.156552
Total gradient norm: 0.435411
=== Actor Training Debug (Iteration 9175) ===
Q mean: -15.064775
Q std: 21.245239
Actor loss: 15.068748
Action reg: 0.003974
  l1.weight: grad_norm = 0.168231
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.160907
Total gradient norm: 0.396743
=== Actor Training Debug (Iteration 9176) ===
Q mean: -12.886383
Q std: 19.157532
Actor loss: 12.890359
Action reg: 0.003976
  l1.weight: grad_norm = 0.161585
  l1.bias: grad_norm = 0.001498
  l2.weight: grad_norm = 0.133894
Total gradient norm: 0.398070
=== Actor Training Debug (Iteration 9177) ===
Q mean: -15.004239
Q std: 21.240120
Actor loss: 15.008206
Action reg: 0.003967
  l1.weight: grad_norm = 0.303284
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.215703
Total gradient norm: 0.575792
=== Actor Training Debug (Iteration 9178) ===
Q mean: -16.889687
Q std: 22.667170
Actor loss: 16.893623
Action reg: 0.003937
  l1.weight: grad_norm = 0.309093
  l1.bias: grad_norm = 0.001417
  l2.weight: grad_norm = 0.225450
Total gradient norm: 0.684600
=== Actor Training Debug (Iteration 9179) ===
Q mean: -15.325794
Q std: 20.729900
Actor loss: 15.329747
Action reg: 0.003953
  l1.weight: grad_norm = 0.223101
  l1.bias: grad_norm = 0.002289
  l2.weight: grad_norm = 0.190154
Total gradient norm: 0.534524
=== Actor Training Debug (Iteration 9180) ===
Q mean: -13.239284
Q std: 20.132385
Actor loss: 13.243254
Action reg: 0.003971
  l1.weight: grad_norm = 0.244842
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.155430
Total gradient norm: 0.391063
=== Actor Training Debug (Iteration 9181) ===
Q mean: -14.609770
Q std: 21.487131
Actor loss: 14.613709
Action reg: 0.003940
  l1.weight: grad_norm = 0.178921
  l1.bias: grad_norm = 0.001624
  l2.weight: grad_norm = 0.152116
Total gradient norm: 0.447694
=== Actor Training Debug (Iteration 9182) ===
Q mean: -15.684566
Q std: 22.887552
Actor loss: 15.688524
Action reg: 0.003958
  l1.weight: grad_norm = 0.218978
  l1.bias: grad_norm = 0.001207
  l2.weight: grad_norm = 0.159928
Total gradient norm: 0.402235
=== Actor Training Debug (Iteration 9183) ===
Q mean: -14.527685
Q std: 20.241302
Actor loss: 14.531646
Action reg: 0.003961
  l1.weight: grad_norm = 0.249848
  l1.bias: grad_norm = 0.002130
  l2.weight: grad_norm = 0.181032
Total gradient norm: 0.487760
=== Actor Training Debug (Iteration 9184) ===
Q mean: -13.559048
Q std: 19.807371
Actor loss: 13.562980
Action reg: 0.003932
  l1.weight: grad_norm = 0.402527
  l1.bias: grad_norm = 0.001040
  l2.weight: grad_norm = 0.271075
Total gradient norm: 0.720476
=== Actor Training Debug (Iteration 9185) ===
Q mean: -15.802979
Q std: 20.447258
Actor loss: 15.806951
Action reg: 0.003972
  l1.weight: grad_norm = 0.181311
  l1.bias: grad_norm = 0.001814
  l2.weight: grad_norm = 0.125100
Total gradient norm: 0.369244
=== Actor Training Debug (Iteration 9186) ===
Q mean: -15.601876
Q std: 22.147253
Actor loss: 15.605841
Action reg: 0.003965
  l1.weight: grad_norm = 0.156318
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.105251
Total gradient norm: 0.290259
=== Actor Training Debug (Iteration 9187) ===
Q mean: -13.917629
Q std: 19.618807
Actor loss: 13.921587
Action reg: 0.003958
  l1.weight: grad_norm = 0.197117
  l1.bias: grad_norm = 0.002675
  l2.weight: grad_norm = 0.150390
Total gradient norm: 0.449540
=== Actor Training Debug (Iteration 9188) ===
Q mean: -16.375919
Q std: 22.849012
Actor loss: 16.379868
Action reg: 0.003949
  l1.weight: grad_norm = 0.185544
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.130143
Total gradient norm: 0.348365
=== Actor Training Debug (Iteration 9189) ===
Q mean: -15.746426
Q std: 22.651974
Actor loss: 15.750382
Action reg: 0.003956
  l1.weight: grad_norm = 0.340155
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.239496
Total gradient norm: 0.657592
=== Actor Training Debug (Iteration 9190) ===
Q mean: -16.954033
Q std: 21.698814
Actor loss: 16.957993
Action reg: 0.003960
  l1.weight: grad_norm = 0.218724
  l1.bias: grad_norm = 0.002871
  l2.weight: grad_norm = 0.166433
Total gradient norm: 0.458500
=== Actor Training Debug (Iteration 9191) ===
Q mean: -11.395528
Q std: 17.956470
Actor loss: 11.399473
Action reg: 0.003945
  l1.weight: grad_norm = 0.302868
  l1.bias: grad_norm = 0.001887
  l2.weight: grad_norm = 0.222113
Total gradient norm: 0.560373
=== Actor Training Debug (Iteration 9192) ===
Q mean: -14.616131
Q std: 21.272535
Actor loss: 14.620076
Action reg: 0.003946
  l1.weight: grad_norm = 0.405659
  l1.bias: grad_norm = 0.001866
  l2.weight: grad_norm = 0.339872
Total gradient norm: 0.879478
=== Actor Training Debug (Iteration 9193) ===
Q mean: -14.294874
Q std: 19.842707
Actor loss: 14.298841
Action reg: 0.003966
  l1.weight: grad_norm = 0.283311
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.201817
Total gradient norm: 0.526649
=== Actor Training Debug (Iteration 9194) ===
Q mean: -15.009424
Q std: 21.716690
Actor loss: 15.013396
Action reg: 0.003972
  l1.weight: grad_norm = 0.630696
  l1.bias: grad_norm = 0.001232
  l2.weight: grad_norm = 0.394924
Total gradient norm: 1.025559
=== Actor Training Debug (Iteration 9195) ===
Q mean: -12.715849
Q std: 18.479897
Actor loss: 12.719803
Action reg: 0.003954
  l1.weight: grad_norm = 0.210829
  l1.bias: grad_norm = 0.001325
  l2.weight: grad_norm = 0.187574
Total gradient norm: 0.521683
=== Actor Training Debug (Iteration 9196) ===
Q mean: -15.446465
Q std: 21.918730
Actor loss: 15.450430
Action reg: 0.003964
  l1.weight: grad_norm = 0.334730
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.215793
Total gradient norm: 0.587707
=== Actor Training Debug (Iteration 9197) ===
Q mean: -15.692207
Q std: 21.146961
Actor loss: 15.696156
Action reg: 0.003948
  l1.weight: grad_norm = 0.117569
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.087934
Total gradient norm: 0.235497
=== Actor Training Debug (Iteration 9198) ===
Q mean: -13.959151
Q std: 19.962734
Actor loss: 13.963119
Action reg: 0.003967
  l1.weight: grad_norm = 0.318006
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.245162
Total gradient norm: 0.620607
=== Actor Training Debug (Iteration 9199) ===
Q mean: -14.229883
Q std: 20.232624
Actor loss: 14.233843
Action reg: 0.003959
  l1.weight: grad_norm = 0.254834
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.193804
Total gradient norm: 0.528395
=== Actor Training Debug (Iteration 9200) ===
Q mean: -15.112844
Q std: 21.897449
Actor loss: 15.116803
Action reg: 0.003959
  l1.weight: grad_norm = 0.132537
  l1.bias: grad_norm = 0.007617
  l2.weight: grad_norm = 0.112192
Total gradient norm: 0.404129
=== Actor Training Debug (Iteration 9201) ===
Q mean: -13.899376
Q std: 20.222414
Actor loss: 13.903314
Action reg: 0.003937
  l1.weight: grad_norm = 0.599100
  l1.bias: grad_norm = 0.001028
  l2.weight: grad_norm = 0.451625
Total gradient norm: 1.211022
=== Actor Training Debug (Iteration 9202) ===
Q mean: -16.422756
Q std: 21.877731
Actor loss: 16.426708
Action reg: 0.003953
  l1.weight: grad_norm = 0.288586
  l1.bias: grad_norm = 0.004845
  l2.weight: grad_norm = 0.219741
Total gradient norm: 0.643156
=== Actor Training Debug (Iteration 9203) ===
Q mean: -15.650955
Q std: 22.253822
Actor loss: 15.654910
Action reg: 0.003955
  l1.weight: grad_norm = 0.461127
  l1.bias: grad_norm = 0.001407
  l2.weight: grad_norm = 0.299258
Total gradient norm: 0.849719
=== Actor Training Debug (Iteration 9204) ===
Q mean: -14.545315
Q std: 19.122124
Actor loss: 14.549274
Action reg: 0.003960
  l1.weight: grad_norm = 0.099795
  l1.bias: grad_norm = 0.001029
  l2.weight: grad_norm = 0.074772
Total gradient norm: 0.199629
=== Actor Training Debug (Iteration 9205) ===
Q mean: -15.474979
Q std: 21.599453
Actor loss: 15.478957
Action reg: 0.003978
  l1.weight: grad_norm = 0.224441
  l1.bias: grad_norm = 0.001988
  l2.weight: grad_norm = 0.166519
Total gradient norm: 0.492265
=== Actor Training Debug (Iteration 9206) ===
Q mean: -14.808043
Q std: 20.831249
Actor loss: 14.811991
Action reg: 0.003948
  l1.weight: grad_norm = 0.333695
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.249387
Total gradient norm: 0.636023
=== Actor Training Debug (Iteration 9207) ===
Q mean: -16.300880
Q std: 22.650974
Actor loss: 16.304834
Action reg: 0.003955
  l1.weight: grad_norm = 0.260922
  l1.bias: grad_norm = 0.001977
  l2.weight: grad_norm = 0.182665
Total gradient norm: 0.476785
=== Actor Training Debug (Iteration 9208) ===
Q mean: -12.750937
Q std: 19.010885
Actor loss: 12.754908
Action reg: 0.003970
  l1.weight: grad_norm = 0.196635
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.137469
Total gradient norm: 0.351445
=== Actor Training Debug (Iteration 9209) ===
Q mean: -13.374783
Q std: 18.153402
Actor loss: 13.378730
Action reg: 0.003948
  l1.weight: grad_norm = 0.417214
  l1.bias: grad_norm = 0.002961
  l2.weight: grad_norm = 0.299976
Total gradient norm: 0.819343
=== Actor Training Debug (Iteration 9210) ===
Q mean: -17.889145
Q std: 23.323883
Actor loss: 17.893114
Action reg: 0.003968
  l1.weight: grad_norm = 0.259823
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.193691
Total gradient norm: 0.530957
=== Actor Training Debug (Iteration 9211) ===
Q mean: -16.247482
Q std: 22.878220
Actor loss: 16.251421
Action reg: 0.003939
  l1.weight: grad_norm = 0.179608
  l1.bias: grad_norm = 0.001530
  l2.weight: grad_norm = 0.154513
Total gradient norm: 0.366159
=== Actor Training Debug (Iteration 9212) ===
Q mean: -13.729971
Q std: 20.600311
Actor loss: 13.733921
Action reg: 0.003950
  l1.weight: grad_norm = 0.239895
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.179394
Total gradient norm: 0.457285
=== Actor Training Debug (Iteration 9213) ===
Q mean: -13.261850
Q std: 20.182297
Actor loss: 13.265802
Action reg: 0.003952
  l1.weight: grad_norm = 0.148400
  l1.bias: grad_norm = 0.003041
  l2.weight: grad_norm = 0.117285
Total gradient norm: 0.345149
=== Actor Training Debug (Iteration 9214) ===
Q mean: -15.097242
Q std: 21.630005
Actor loss: 15.101189
Action reg: 0.003947
  l1.weight: grad_norm = 0.260382
  l1.bias: grad_norm = 0.001377
  l2.weight: grad_norm = 0.223915
Total gradient norm: 0.634105
=== Actor Training Debug (Iteration 9215) ===
Q mean: -15.669270
Q std: 19.874359
Actor loss: 15.673221
Action reg: 0.003951
  l1.weight: grad_norm = 0.321094
  l1.bias: grad_norm = 0.001295
  l2.weight: grad_norm = 0.250143
Total gradient norm: 0.619356
=== Actor Training Debug (Iteration 9216) ===
Q mean: -14.393838
Q std: 21.176504
Actor loss: 14.397802
Action reg: 0.003964
  l1.weight: grad_norm = 0.247350
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.203683
Total gradient norm: 0.573699
=== Actor Training Debug (Iteration 9217) ===
Q mean: -14.307411
Q std: 20.102022
Actor loss: 14.311365
Action reg: 0.003954
  l1.weight: grad_norm = 0.242892
  l1.bias: grad_norm = 0.002820
  l2.weight: grad_norm = 0.162987
Total gradient norm: 0.446876
=== Actor Training Debug (Iteration 9218) ===
Q mean: -14.159235
Q std: 21.342352
Actor loss: 14.163177
Action reg: 0.003942
  l1.weight: grad_norm = 0.360468
  l1.bias: grad_norm = 0.002662
  l2.weight: grad_norm = 0.276002
Total gradient norm: 0.765470
=== Actor Training Debug (Iteration 9219) ===
Q mean: -15.347349
Q std: 20.935835
Actor loss: 15.351294
Action reg: 0.003944
  l1.weight: grad_norm = 0.194635
  l1.bias: grad_norm = 0.000582
  l2.weight: grad_norm = 0.137119
Total gradient norm: 0.368492
=== Actor Training Debug (Iteration 9220) ===
Q mean: -15.275553
Q std: 21.673565
Actor loss: 15.279516
Action reg: 0.003963
  l1.weight: grad_norm = 0.537534
  l1.bias: grad_norm = 0.000461
  l2.weight: grad_norm = 0.403201
Total gradient norm: 1.061215
=== Actor Training Debug (Iteration 9221) ===
Q mean: -14.406712
Q std: 20.104155
Actor loss: 14.410690
Action reg: 0.003979
  l1.weight: grad_norm = 0.150462
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.128750
Total gradient norm: 0.352327
=== Actor Training Debug (Iteration 9222) ===
Q mean: -13.539530
Q std: 19.962645
Actor loss: 13.543489
Action reg: 0.003959
  l1.weight: grad_norm = 0.304854
  l1.bias: grad_norm = 0.004108
  l2.weight: grad_norm = 0.208903
Total gradient norm: 0.581040
=== Actor Training Debug (Iteration 9223) ===
Q mean: -13.171551
Q std: 18.627239
Actor loss: 13.175475
Action reg: 0.003924
  l1.weight: grad_norm = 0.251128
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.166690
Total gradient norm: 0.441860
=== Actor Training Debug (Iteration 9224) ===
Q mean: -13.241652
Q std: 20.539249
Actor loss: 13.245596
Action reg: 0.003944
  l1.weight: grad_norm = 0.299460
  l1.bias: grad_norm = 0.002785
  l2.weight: grad_norm = 0.237870
Total gradient norm: 0.610390
=== Actor Training Debug (Iteration 9225) ===
Q mean: -13.629671
Q std: 20.175768
Actor loss: 13.633640
Action reg: 0.003969
  l1.weight: grad_norm = 0.192675
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.141918
Total gradient norm: 0.397579
=== Actor Training Debug (Iteration 9226) ===
Q mean: -16.641684
Q std: 21.088606
Actor loss: 16.645649
Action reg: 0.003966
  l1.weight: grad_norm = 0.167967
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.140789
Total gradient norm: 0.379008
=== Actor Training Debug (Iteration 9227) ===
Q mean: -14.687680
Q std: 20.614616
Actor loss: 14.691645
Action reg: 0.003964
  l1.weight: grad_norm = 0.305244
  l1.bias: grad_norm = 0.002967
  l2.weight: grad_norm = 0.189069
Total gradient norm: 0.529718
=== Actor Training Debug (Iteration 9228) ===
Q mean: -16.271626
Q std: 20.458958
Actor loss: 16.275583
Action reg: 0.003958
  l1.weight: grad_norm = 0.517998
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.423251
Total gradient norm: 1.600788
=== Actor Training Debug (Iteration 9229) ===
Q mean: -14.052719
Q std: 21.319094
Actor loss: 14.056681
Action reg: 0.003961
  l1.weight: grad_norm = 0.230309
  l1.bias: grad_norm = 0.000617
  l2.weight: grad_norm = 0.186733
Total gradient norm: 0.512620
=== Actor Training Debug (Iteration 9230) ===
Q mean: -13.189634
Q std: 19.498880
Actor loss: 13.193607
Action reg: 0.003973
  l1.weight: grad_norm = 0.212718
  l1.bias: grad_norm = 0.003986
  l2.weight: grad_norm = 0.193962
Total gradient norm: 0.562122
=== Actor Training Debug (Iteration 9231) ===
Q mean: -13.246252
Q std: 19.065468
Actor loss: 13.250203
Action reg: 0.003951
  l1.weight: grad_norm = 0.335364
  l1.bias: grad_norm = 0.001025
  l2.weight: grad_norm = 0.254773
Total gradient norm: 0.759170
=== Actor Training Debug (Iteration 9232) ===
Q mean: -13.153933
Q std: 19.494381
Actor loss: 13.157903
Action reg: 0.003970
  l1.weight: grad_norm = 0.315976
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.219439
Total gradient norm: 0.598066
=== Actor Training Debug (Iteration 9233) ===
Q mean: -14.510622
Q std: 21.270138
Actor loss: 14.514573
Action reg: 0.003952
  l1.weight: grad_norm = 0.356676
  l1.bias: grad_norm = 0.001017
  l2.weight: grad_norm = 0.253992
Total gradient norm: 0.626260
=== Actor Training Debug (Iteration 9234) ===
Q mean: -14.169841
Q std: 18.989504
Actor loss: 14.173806
Action reg: 0.003965
  l1.weight: grad_norm = 0.285785
  l1.bias: grad_norm = 0.003079
  l2.weight: grad_norm = 0.232892
Total gradient norm: 0.660774
=== Actor Training Debug (Iteration 9235) ===
Q mean: -13.853086
Q std: 19.473965
Actor loss: 13.857040
Action reg: 0.003955
  l1.weight: grad_norm = 0.134476
  l1.bias: grad_norm = 0.001952
  l2.weight: grad_norm = 0.103572
Total gradient norm: 0.301613
=== Actor Training Debug (Iteration 9236) ===
Q mean: -13.784134
Q std: 20.374645
Actor loss: 13.788078
Action reg: 0.003944
  l1.weight: grad_norm = 0.228749
  l1.bias: grad_norm = 0.002836
  l2.weight: grad_norm = 0.190259
Total gradient norm: 0.486437
=== Actor Training Debug (Iteration 9237) ===
Q mean: -16.351627
Q std: 22.342478
Actor loss: 16.355581
Action reg: 0.003954
  l1.weight: grad_norm = 0.382415
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.244515
Total gradient norm: 0.708931
=== Actor Training Debug (Iteration 9238) ===
Q mean: -14.221680
Q std: 21.090239
Actor loss: 14.225622
Action reg: 0.003942
  l1.weight: grad_norm = 0.532214
  l1.bias: grad_norm = 0.002708
  l2.weight: grad_norm = 0.374931
Total gradient norm: 0.943178
=== Actor Training Debug (Iteration 9239) ===
Q mean: -14.328724
Q std: 20.483463
Actor loss: 14.332677
Action reg: 0.003953
  l1.weight: grad_norm = 0.322250
  l1.bias: grad_norm = 0.001262
  l2.weight: grad_norm = 0.285911
Total gradient norm: 0.752351
=== Actor Training Debug (Iteration 9240) ===
Q mean: -13.375151
Q std: 20.842312
Actor loss: 13.379101
Action reg: 0.003951
  l1.weight: grad_norm = 0.221933
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.181448
Total gradient norm: 0.548686
=== Actor Training Debug (Iteration 9241) ===
Q mean: -15.591156
Q std: 20.888678
Actor loss: 15.595111
Action reg: 0.003955
  l1.weight: grad_norm = 0.227503
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.183223
Total gradient norm: 0.489432
=== Actor Training Debug (Iteration 9242) ===
Q mean: -14.927282
Q std: 21.506540
Actor loss: 14.931250
Action reg: 0.003967
  l1.weight: grad_norm = 0.427243
  l1.bias: grad_norm = 0.001507
  l2.weight: grad_norm = 0.302917
Total gradient norm: 0.808171
=== Actor Training Debug (Iteration 9243) ===
Q mean: -12.440260
Q std: 19.215572
Actor loss: 12.444202
Action reg: 0.003943
  l1.weight: grad_norm = 0.185144
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.128053
Total gradient norm: 0.332970
=== Actor Training Debug (Iteration 9244) ===
Q mean: -14.483158
Q std: 21.449072
Actor loss: 14.487108
Action reg: 0.003950
  l1.weight: grad_norm = 0.245968
  l1.bias: grad_norm = 0.001721
  l2.weight: grad_norm = 0.207206
Total gradient norm: 0.550794
=== Actor Training Debug (Iteration 9245) ===
Q mean: -14.742346
Q std: 21.096474
Actor loss: 14.746309
Action reg: 0.003963
  l1.weight: grad_norm = 0.142484
  l1.bias: grad_norm = 0.001570
  l2.weight: grad_norm = 0.106663
Total gradient norm: 0.282429
=== Actor Training Debug (Iteration 9246) ===
Q mean: -14.715585
Q std: 20.251221
Actor loss: 14.719510
Action reg: 0.003925
  l1.weight: grad_norm = 0.538951
  l1.bias: grad_norm = 0.004339
  l2.weight: grad_norm = 0.373883
Total gradient norm: 1.210157
=== Actor Training Debug (Iteration 9247) ===
Q mean: -15.460836
Q std: 21.304049
Actor loss: 15.464787
Action reg: 0.003950
  l1.weight: grad_norm = 0.171684
  l1.bias: grad_norm = 0.003771
  l2.weight: grad_norm = 0.134434
Total gradient norm: 0.366260
=== Actor Training Debug (Iteration 9248) ===
Q mean: -12.932286
Q std: 18.480898
Actor loss: 12.936225
Action reg: 0.003939
  l1.weight: grad_norm = 0.330906
  l1.bias: grad_norm = 0.002124
  l2.weight: grad_norm = 0.291535
Total gradient norm: 0.783003
=== Actor Training Debug (Iteration 9249) ===
Q mean: -14.832929
Q std: 20.400974
Actor loss: 14.836882
Action reg: 0.003953
  l1.weight: grad_norm = 0.198415
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.148988
Total gradient norm: 0.421855
=== Actor Training Debug (Iteration 9250) ===
Q mean: -15.859479
Q std: 21.707647
Actor loss: 15.863444
Action reg: 0.003965
  l1.weight: grad_norm = 0.288050
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.274214
Total gradient norm: 0.907330
=== Actor Training Debug (Iteration 9251) ===
Q mean: -15.319498
Q std: 21.549519
Actor loss: 15.323444
Action reg: 0.003947
  l1.weight: grad_norm = 0.217961
  l1.bias: grad_norm = 0.004845
  l2.weight: grad_norm = 0.169013
Total gradient norm: 0.498429
=== Actor Training Debug (Iteration 9252) ===
Q mean: -15.216026
Q std: 20.404850
Actor loss: 15.219995
Action reg: 0.003968
  l1.weight: grad_norm = 0.211603
  l1.bias: grad_norm = 0.002737
  l2.weight: grad_norm = 0.166231
Total gradient norm: 0.466452
=== Actor Training Debug (Iteration 9253) ===
Q mean: -14.387870
Q std: 19.255327
Actor loss: 14.391814
Action reg: 0.003944
  l1.weight: grad_norm = 0.429063
  l1.bias: grad_norm = 0.001228
  l2.weight: grad_norm = 0.337658
Total gradient norm: 0.848003
=== Actor Training Debug (Iteration 9254) ===
Q mean: -14.159903
Q std: 19.864872
Actor loss: 14.163854
Action reg: 0.003951
  l1.weight: grad_norm = 0.268922
  l1.bias: grad_norm = 0.001351
  l2.weight: grad_norm = 0.241938
Total gradient norm: 0.636503
=== Actor Training Debug (Iteration 9255) ===
Q mean: -13.913125
Q std: 21.136835
Actor loss: 13.917077
Action reg: 0.003952
  l1.weight: grad_norm = 0.192790
  l1.bias: grad_norm = 0.000961
  l2.weight: grad_norm = 0.141619
Total gradient norm: 0.366023
=== Actor Training Debug (Iteration 9256) ===
Q mean: -17.103794
Q std: 21.926865
Actor loss: 17.107750
Action reg: 0.003956
  l1.weight: grad_norm = 0.268208
  l1.bias: grad_norm = 0.002690
  l2.weight: grad_norm = 0.200635
Total gradient norm: 0.527940
=== Actor Training Debug (Iteration 9257) ===
Q mean: -14.859015
Q std: 20.403629
Actor loss: 14.862962
Action reg: 0.003946
  l1.weight: grad_norm = 0.547575
  l1.bias: grad_norm = 0.004205
  l2.weight: grad_norm = 0.522265
Total gradient norm: 1.391638
=== Actor Training Debug (Iteration 9258) ===
Q mean: -14.212757
Q std: 19.566401
Actor loss: 14.216737
Action reg: 0.003980
  l1.weight: grad_norm = 0.094962
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.074458
Total gradient norm: 0.197170
=== Actor Training Debug (Iteration 9259) ===
Q mean: -14.894723
Q std: 21.361834
Actor loss: 14.898700
Action reg: 0.003977
  l1.weight: grad_norm = 0.196357
  l1.bias: grad_norm = 0.001498
  l2.weight: grad_norm = 0.188373
Total gradient norm: 0.531575
=== Actor Training Debug (Iteration 9260) ===
Q mean: -15.130219
Q std: 21.167179
Actor loss: 15.134171
Action reg: 0.003953
  l1.weight: grad_norm = 0.268406
  l1.bias: grad_norm = 0.001621
  l2.weight: grad_norm = 0.196090
Total gradient norm: 0.597710
=== Actor Training Debug (Iteration 9261) ===
Q mean: -14.226098
Q std: 20.322283
Actor loss: 14.230048
Action reg: 0.003950
  l1.weight: grad_norm = 0.312565
  l1.bias: grad_norm = 0.002547
  l2.weight: grad_norm = 0.254007
Total gradient norm: 0.764088
=== Actor Training Debug (Iteration 9262) ===
Q mean: -14.779263
Q std: 21.177681
Actor loss: 14.783209
Action reg: 0.003946
  l1.weight: grad_norm = 0.254131
  l1.bias: grad_norm = 0.003774
  l2.weight: grad_norm = 0.164194
Total gradient norm: 0.479290
=== Actor Training Debug (Iteration 9263) ===
Q mean: -13.951084
Q std: 21.355711
Actor loss: 13.955046
Action reg: 0.003962
  l1.weight: grad_norm = 0.252710
  l1.bias: grad_norm = 0.004588
  l2.weight: grad_norm = 0.183168
Total gradient norm: 0.534891
=== Actor Training Debug (Iteration 9264) ===
Q mean: -13.400970
Q std: 20.454212
Actor loss: 13.404915
Action reg: 0.003944
  l1.weight: grad_norm = 0.782198
  l1.bias: grad_norm = 0.001856
  l2.weight: grad_norm = 0.485409
Total gradient norm: 1.201053
=== Actor Training Debug (Iteration 9265) ===
Q mean: -15.661928
Q std: 22.203003
Actor loss: 15.665872
Action reg: 0.003944
  l1.weight: grad_norm = 0.301261
  l1.bias: grad_norm = 0.005089
  l2.weight: grad_norm = 0.221447
Total gradient norm: 0.599372
=== Actor Training Debug (Iteration 9266) ===
Q mean: -14.092758
Q std: 18.729548
Actor loss: 14.096727
Action reg: 0.003969
  l1.weight: grad_norm = 0.205604
  l1.bias: grad_norm = 0.003141
  l2.weight: grad_norm = 0.156204
Total gradient norm: 0.451970
=== Actor Training Debug (Iteration 9267) ===
Q mean: -15.162675
Q std: 21.344563
  l1.weight: grad_norm = 0.400071
  l1.bias: grad_norm = 0.014347
  l2.weight: grad_norm = 0.264436
Total gradient norm: 0.902165
=== Actor Training Debug (Iteration 9274) ===
Q mean: -15.182838
Q std: 21.572386
Actor loss: 15.186791
Action reg: 0.003953
  l1.weight: grad_norm = 0.274354
  l1.bias: grad_norm = 0.006076
  l2.weight: grad_norm = 0.247448
Total gradient norm: 0.677897
=== Actor Training Debug (Iteration 9275) ===
Q mean: -17.719212
Q std: 23.201406
Actor loss: 17.723166
Action reg: 0.003955
  l1.weight: grad_norm = 0.256105
  l1.bias: grad_norm = 0.001453
  l2.weight: grad_norm = 0.177074
Total gradient norm: 0.628544
=== Actor Training Debug (Iteration 9276) ===
Q mean: -16.232378
Q std: 21.815329
Actor loss: 16.236347
Action reg: 0.003969
  l1.weight: grad_norm = 0.271587
  l1.bias: grad_norm = 0.003000
  l2.weight: grad_norm = 0.219408
Total gradient norm: 0.618985
=== Actor Training Debug (Iteration 9277) ===
Q mean: -13.559675
Q std: 19.388659
Actor loss: 13.563633
Action reg: 0.003958
  l1.weight: grad_norm = 0.415904
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.296240
Total gradient norm: 0.770736
=== Actor Training Debug (Iteration 9278) ===
Q mean: -12.452070
Q std: 18.598848
Actor loss: 12.456044
Action reg: 0.003974
  l1.weight: grad_norm = 0.271198
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.193741
Total gradient norm: 0.517325
=== Actor Training Debug (Iteration 9279) ===
Q mean: -16.981909
Q std: 22.356438
Actor loss: 16.985878
Action reg: 0.003970
  l1.weight: grad_norm = 0.298681
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.235791
Total gradient norm: 0.665540
=== Actor Training Debug (Iteration 9280) ===
Q mean: -13.038609
Q std: 19.815590
Actor loss: 13.042562
Action reg: 0.003954
  l1.weight: grad_norm = 0.468414
  l1.bias: grad_norm = 0.004124
  l2.weight: grad_norm = 0.387361
Total gradient norm: 0.980233
=== Actor Training Debug (Iteration 9281) ===
Q mean: -17.269611
Q std: 23.611689
Actor loss: 17.273552
Action reg: 0.003940
  l1.weight: grad_norm = 0.626709
  l1.bias: grad_norm = 0.005500
  l2.weight: grad_norm = 0.367443
Total gradient norm: 1.003664
=== Actor Training Debug (Iteration 9282) ===
Q mean: -14.256987
Q std: 19.933908
Actor loss: 14.260942
Action reg: 0.003956
  l1.weight: grad_norm = 0.289474
  l1.bias: grad_norm = 0.004871
  l2.weight: grad_norm = 0.233635
Total gradient norm: 0.636593
=== Actor Training Debug (Iteration 9283) ===
Q mean: -14.171257
Q std: 21.800596
Actor loss: 14.175200
Action reg: 0.003943
  l1.weight: grad_norm = 0.352502
  l1.bias: grad_norm = 0.005104
  l2.weight: grad_norm = 0.269189
Total gradient norm: 0.769330
=== Actor Training Debug (Iteration 9284) ===
Q mean: -14.725050
Q std: 21.046551
Actor loss: 14.728997
Action reg: 0.003947
  l1.weight: grad_norm = 0.205307
  l1.bias: grad_norm = 0.008304
  l2.weight: grad_norm = 0.154949
Total gradient norm: 0.513108
=== Actor Training Debug (Iteration 9285) ===
Q mean: -12.739122
Q std: 19.530390
Actor loss: 12.743077
Action reg: 0.003955
  l1.weight: grad_norm = 0.638187
  l1.bias: grad_norm = 0.001725
  l2.weight: grad_norm = 0.539234
Total gradient norm: 1.322877
=== Actor Training Debug (Iteration 9286) ===
Q mean: -15.437824
Q std: 21.429459
Actor loss: 15.441795
Action reg: 0.003971
  l1.weight: grad_norm = 0.224006
  l1.bias: grad_norm = 0.001379
  l2.weight: grad_norm = 0.251353
Total gradient norm: 0.707955
=== Actor Training Debug (Iteration 9287) ===
Q mean: -16.533218
Q std: 21.377996
Actor loss: 16.537180
Action reg: 0.003962
  l1.weight: grad_norm = 0.206222
  l1.bias: grad_norm = 0.001895
  l2.weight: grad_norm = 0.160671
Total gradient norm: 0.417326
=== Actor Training Debug (Iteration 9288) ===
Q mean: -14.177237
Q std: 20.092394
Actor loss: 14.181194
Action reg: 0.003958
  l1.weight: grad_norm = 0.221684
  l1.bias: grad_norm = 0.001818
  l2.weight: grad_norm = 0.173254
Total gradient norm: 0.450806
=== Actor Training Debug (Iteration 9289) ===
Q mean: -14.951775
Q std: 21.414480
Actor loss: 14.955739
Action reg: 0.003965
  l1.weight: grad_norm = 0.228230
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.171041
Total gradient norm: 0.451162
=== Actor Training Debug (Iteration 9290) ===
Q mean: -14.605522
Q std: 20.025482
Actor loss: 14.609497
Action reg: 0.003975
  l1.weight: grad_norm = 0.183664
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.120021
Total gradient norm: 0.333835
=== Actor Training Debug (Iteration 9291) ===
Q mean: -16.929047
Q std: 22.962116
Actor loss: 16.933020
Action reg: 0.003973
  l1.weight: grad_norm = 0.166410
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.152064
Total gradient norm: 0.405278
=== Actor Training Debug (Iteration 9292) ===
Q mean: -15.105576
Q std: 20.713068
Actor loss: 15.109525
Action reg: 0.003950
  l1.weight: grad_norm = 0.337887
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.271309
Total gradient norm: 0.739992
=== Actor Training Debug (Iteration 9293) ===
Q mean: -14.275109
Q std: 20.474579
Actor loss: 14.279066
Action reg: 0.003957
  l1.weight: grad_norm = 0.160893
  l1.bias: grad_norm = 0.006358
  l2.weight: grad_norm = 0.147193
Total gradient norm: 0.456074
=== Actor Training Debug (Iteration 9294) ===
Q mean: -13.338850
Q std: 19.764294
Actor loss: 13.342812
Action reg: 0.003962
  l1.weight: grad_norm = 0.397144
  l1.bias: grad_norm = 0.006846
  l2.weight: grad_norm = 0.288865
Total gradient norm: 0.753058
=== Actor Training Debug (Iteration 9295) ===
Q mean: -15.915524
Q std: 21.403818
Actor loss: 15.919484
Action reg: 0.003960
  l1.weight: grad_norm = 0.388786
  l1.bias: grad_norm = 0.004282
  l2.weight: grad_norm = 0.304303
Total gradient norm: 0.743463
=== Actor Training Debug (Iteration 9296) ===
Q mean: -15.700042
Q std: 21.364679
Actor loss: 15.704001
Action reg: 0.003960
  l1.weight: grad_norm = 0.521387
  l1.bias: grad_norm = 0.005987
  l2.weight: grad_norm = 0.345563
Total gradient norm: 0.951704
=== Actor Training Debug (Iteration 9297) ===
Q mean: -17.097454
Q std: 23.761032
Actor loss: 17.101410
Action reg: 0.003955
  l1.weight: grad_norm = 0.131496
  l1.bias: grad_norm = 0.002388
  l2.weight: grad_norm = 0.097903
Total gradient norm: 0.263758
=== Actor Training Debug (Iteration 9298) ===
Q mean: -15.274258
Q std: 20.693655
Actor loss: 15.278212
Action reg: 0.003954
  l1.weight: grad_norm = 0.407636
  l1.bias: grad_norm = 0.001427
  l2.weight: grad_norm = 0.285263
Total gradient norm: 0.759393
=== Actor Training Debug (Iteration 9299) ===
Q mean: -15.566891
Q std: 21.031519
Actor loss: 15.570848
Action reg: 0.003958
  l1.weight: grad_norm = 0.312552
  l1.bias: grad_norm = 0.001420
  l2.weight: grad_norm = 0.198066
Total gradient norm: 0.533634
=== Actor Training Debug (Iteration 9300) ===
Q mean: -14.778980
Q std: 22.019974
Actor loss: 14.782948
Action reg: 0.003968
  l1.weight: grad_norm = 0.353443
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.198950
Total gradient norm: 0.546043
=== Actor Training Debug (Iteration 9301) ===
Q mean: -12.280010
Q std: 19.494095
Actor loss: 12.283958
Action reg: 0.003948
  l1.weight: grad_norm = 0.343203
  l1.bias: grad_norm = 0.005736
  l2.weight: grad_norm = 0.255071
Total gradient norm: 0.689154
=== Actor Training Debug (Iteration 9302) ===
Q mean: -16.658260
Q std: 22.387243
Actor loss: 16.662222
Action reg: 0.003961
  l1.weight: grad_norm = 0.314887
  l1.bias: grad_norm = 0.002793
  l2.weight: grad_norm = 0.246983
Total gradient norm: 0.676893
=== Actor Training Debug (Iteration 9303) ===
Q mean: -14.984779
Q std: 19.764278
Actor loss: 14.988742
Action reg: 0.003963
  l1.weight: grad_norm = 0.397357
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.299786
Total gradient norm: 0.703838
=== Actor Training Debug (Iteration 9304) ===
Q mean: -13.580803
Q std: 19.260450
Actor loss: 13.584762
Action reg: 0.003959
  l1.weight: grad_norm = 0.256315
  l1.bias: grad_norm = 0.001905
  l2.weight: grad_norm = 0.185033
Total gradient norm: 0.625660
=== Actor Training Debug (Iteration 9305) ===
Q mean: -15.109041
Q std: 21.734255
Actor loss: 15.112998
Action reg: 0.003957
  l1.weight: grad_norm = 0.479988
  l1.bias: grad_norm = 0.002095
  l2.weight: grad_norm = 0.353272
Total gradient norm: 0.931942
=== Actor Training Debug (Iteration 9306) ===
Q mean: -14.951539
Q std: 21.962120
Actor loss: 14.955474
Action reg: 0.003935
  l1.weight: grad_norm = 0.851798
  l1.bias: grad_norm = 0.003810
  l2.weight: grad_norm = 0.705777
Total gradient norm: 2.523580
=== Actor Training Debug (Iteration 9307) ===
Q mean: -15.680022
Q std: 19.850658
Actor loss: 15.683990
Action reg: 0.003969
  l1.weight: grad_norm = 0.205406
  l1.bias: grad_norm = 0.001853
  l2.weight: grad_norm = 0.186400
Total gradient norm: 0.466222
=== Actor Training Debug (Iteration 9308) ===
Q mean: -14.190104
Q std: 19.393332
Actor loss: 14.194071
Action reg: 0.003967
  l1.weight: grad_norm = 0.376008
  l1.bias: grad_norm = 0.001451
  l2.weight: grad_norm = 0.278451
Total gradient norm: 0.777474
=== Actor Training Debug (Iteration 9309) ===
Q mean: -15.996119
Q std: 21.884607
Actor loss: 16.000071
Action reg: 0.003950
  l1.weight: grad_norm = 0.275999
  l1.bias: grad_norm = 0.004376
  l2.weight: grad_norm = 0.215465
Total gradient norm: 0.608562
=== Actor Training Debug (Iteration 9310) ===
Q mean: -17.939734
Q std: 23.423609
Actor loss: 17.943684
Action reg: 0.003950
  l1.weight: grad_norm = 0.154550
  l1.bias: grad_norm = 0.001966
  l2.weight: grad_norm = 0.131220
Total gradient norm: 0.321705
=== Actor Training Debug (Iteration 9311) ===
Q mean: -13.945271
Q std: 21.031551
Actor loss: 13.949225
Action reg: 0.003955
  l1.weight: grad_norm = 0.407830
  l1.bias: grad_norm = 0.003371
  l2.weight: grad_norm = 0.393225
Total gradient norm: 1.071324
=== Actor Training Debug (Iteration 9312) ===
Q mean: -16.106617
Q std: 22.516785
Actor loss: 16.110567
Action reg: 0.003951
  l1.weight: grad_norm = 0.344330
  l1.bias: grad_norm = 0.001128
  l2.weight: grad_norm = 0.281179
Total gradient norm: 0.745593
=== Actor Training Debug (Iteration 9313) ===
Q mean: -14.034921
Q std: 19.703075
Actor loss: 14.038860
Action reg: 0.003939
  l1.weight: grad_norm = 0.295890
  l1.bias: grad_norm = 0.002226
  l2.weight: grad_norm = 0.217347
Total gradient norm: 0.599447
=== Actor Training Debug (Iteration 9314) ===
Q mean: -16.063711
Q std: 21.883669
Actor loss: 16.067673
Action reg: 0.003962
  l1.weight: grad_norm = 0.232129
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.167194
Total gradient norm: 0.435957
=== Actor Training Debug (Iteration 9315) ===
Q mean: -15.414203
Q std: 21.647917
Actor loss: 15.418164
Action reg: 0.003962
  l1.weight: grad_norm = 0.112013
  l1.bias: grad_norm = 0.006043
  l2.weight: grad_norm = 0.116064
Total gradient norm: 0.456780
=== Actor Training Debug (Iteration 9316) ===
Q mean: -14.559579
Q std: 21.596460
Actor loss: 14.563528
Action reg: 0.003949
  l1.weight: grad_norm = 0.198769
  l1.bias: grad_norm = 0.001642
  l2.weight: grad_norm = 0.169749
Total gradient norm: 0.438191
=== Actor Training Debug (Iteration 9317) ===
Q mean: -12.726473
Q std: 19.230782
Actor loss: 12.730433
Action reg: 0.003960
  l1.weight: grad_norm = 0.248127
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.182680
Total gradient norm: 0.475050
=== Actor Training Debug (Iteration 9318) ===
Q mean: -16.219259
Q std: 21.712208
Actor loss: 16.223228
Action reg: 0.003970
  l1.weight: grad_norm = 0.236299
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.175586
Total gradient norm: 0.500694
=== Actor Training Debug (Iteration 9319) ===
Q mean: -16.461596
Q std: 22.341776
Actor loss: 16.465565
Action reg: 0.003969
  l1.weight: grad_norm = 0.317679
  l1.bias: grad_norm = 0.005171
  l2.weight: grad_norm = 0.246142
Total gradient norm: 0.659113
=== Actor Training Debug (Iteration 9320) ===
Q mean: -13.361296
Q std: 19.767921
Actor loss: 13.365240
Action reg: 0.003945
  l1.weight: grad_norm = 0.238838
  l1.bias: grad_norm = 0.001466
  l2.weight: grad_norm = 0.188982
Total gradient norm: 0.525405
=== Actor Training Debug (Iteration 9321) ===
Q mean: -13.863966
Q std: 19.947622
Actor loss: 13.867929
Action reg: 0.003963
  l1.weight: grad_norm = 0.352572
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.247272
Total gradient norm: 0.662260
=== Actor Training Debug (Iteration 9322) ===
Q mean: -14.861256
Q std: 21.404451
Actor loss: 14.865230
Action reg: 0.003974
  l1.weight: grad_norm = 0.354295
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.239420
Total gradient norm: 0.630228
=== Actor Training Debug (Iteration 9323) ===
Q mean: -14.829329
Q std: 21.198128
Actor loss: 14.833286
Action reg: 0.003958
  l1.weight: grad_norm = 0.251620
  l1.bias: grad_norm = 0.004548
  l2.weight: grad_norm = 0.168634
Total gradient norm: 0.456617
=== Actor Training Debug (Iteration 9324) ===
Q mean: -13.793391
Q std: 20.327351
Actor loss: 13.797371
Action reg: 0.003979
  l1.weight: grad_norm = 0.201546
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.143062
Total gradient norm: 0.370361
=== Actor Training Debug (Iteration 9325) ===
Q mean: -14.377520
Q std: 21.151361
Actor loss: 14.381458
Action reg: 0.003939
  l1.weight: grad_norm = 0.731854
  l1.bias: grad_norm = 0.001941
  l2.weight: grad_norm = 0.538814
Total gradient norm: 1.595787
=== Actor Training Debug (Iteration 9326) ===
Q mean: -14.191172
Q std: 19.549782
Actor loss: 14.195129
Action reg: 0.003958
  l1.weight: grad_norm = 0.490383
  l1.bias: grad_norm = 0.000521
  l2.weight: grad_norm = 0.347668
Total gradient norm: 0.838507
=== Actor Training Debug (Iteration 9327) ===
Q mean: -13.514675
Q std: 19.871572
Actor loss: 13.518635
Action reg: 0.003960
  l1.weight: grad_norm = 0.157169
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.110293
Total gradient norm: 0.289438
=== Actor Training Debug (Iteration 9328) ===
Q mean: -14.064600
Q std: 19.578150
Actor loss: 14.068569
Action reg: 0.003970
  l1.weight: grad_norm = 0.443381
  l1.bias: grad_norm = 0.001804
  l2.weight: grad_norm = 0.329908
Total gradient norm: 0.860611
=== Actor Training Debug (Iteration 9329) ===
Q mean: -16.022972
Q std: 20.934422
Actor loss: 16.026936
Action reg: 0.003964
  l1.weight: grad_norm = 0.154722
  l1.bias: grad_norm = 0.001869
  l2.weight: grad_norm = 0.118562
Total gradient norm: 0.327406
=== Actor Training Debug (Iteration 9330) ===
Q mean: -15.137177
Q std: 21.936270
Actor loss: 15.141133
Action reg: 0.003957
  l1.weight: grad_norm = 0.337161
  l1.bias: grad_norm = 0.001723
  l2.weight: grad_norm = 0.255519
Total gradient norm: 0.577774
=== Actor Training Debug (Iteration 9331) ===
Q mean: -14.280213
Q std: 20.514990
Actor loss: 14.284161
Action reg: 0.003947
  l1.weight: grad_norm = 0.281011
  l1.bias: grad_norm = 0.001951
  l2.weight: grad_norm = 0.214406
Total gradient norm: 0.507482
=== Actor Training Debug (Iteration 9332) ===
Q mean: -15.459017
Q std: 21.254194
Actor loss: 15.462976
Action reg: 0.003958
  l1.weight: grad_norm = 0.341557
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.267784
Total gradient norm: 0.722981
=== Actor Training Debug (Iteration 9333) ===
Q mean: -14.642890
Q std: 20.268089
Actor loss: 14.646850
Action reg: 0.003960
  l1.weight: grad_norm = 0.295853
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.240845
Total gradient norm: 0.679786
=== Actor Training Debug (Iteration 9334) ===
Q mean: -15.531115
Q std: 20.132904
Actor loss: 15.535096
Action reg: 0.003981
  l1.weight: grad_norm = 0.238691
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.157049
Total gradient norm: 0.436773
=== Actor Training Debug (Iteration 9335) ===
Q mean: -14.849340
Q std: 20.858528
Actor loss: 14.853302
Action reg: 0.003962
  l1.weight: grad_norm = 0.494654
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.353066
Total gradient norm: 1.062724
=== Actor Training Debug (Iteration 9336) ===
Q mean: -15.248831
Q std: 20.060696
Actor loss: 15.252802
Action reg: 0.003971
  l1.weight: grad_norm = 0.238647
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.187434
Total gradient norm: 0.475313
=== Actor Training Debug (Iteration 9337) ===
Q mean: -14.556146
Q std: 20.517195
Actor loss: 14.560091
Action reg: 0.003945
  l1.weight: grad_norm = 0.351338
  l1.bias: grad_norm = 0.001505
  l2.weight: grad_norm = 0.256183
Total gradient norm: 0.685912
=== Actor Training Debug (Iteration 9338) ===
Q mean: -17.718071
Q std: 21.914814
Actor loss: 17.722033
Action reg: 0.003961
  l1.weight: grad_norm = 0.249430
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.171975
Total gradient norm: 0.480351
=== Actor Training Debug (Iteration 9339) ===
Q mean: -13.480165
Q std: 18.589392
Actor loss: 13.484132
Action reg: 0.003967
  l1.weight: grad_norm = 0.238912
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.199196
Total gradient norm: 0.503577
=== Actor Training Debug (Iteration 9340) ===
Q mean: -15.198078
Q std: 19.931360
Actor loss: 15.202022
Action reg: 0.003944
  l1.weight: grad_norm = 0.181890
  l1.bias: grad_norm = 0.005140
  l2.weight: grad_norm = 0.138724
Total gradient norm: 0.426556
=== Actor Training Debug (Iteration 9341) ===
Q mean: -14.038301
Q std: 19.986780
Actor loss: 14.042267
Action reg: 0.003966
  l1.weight: grad_norm = 0.249136
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.201923
Total gradient norm: 0.569752
=== Actor Training Debug (Iteration 9342) ===
Q mean: -16.089443
Q std: 21.301516
Actor loss: 16.093412
Action reg: 0.003970
  l1.weight: grad_norm = 0.205280
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.128871
Total gradient norm: 0.344814
=== Actor Training Debug (Iteration 9343) ===
Q mean: -17.425585
Q std: 23.842512
Actor loss: 17.429543
Action reg: 0.003958
  l1.weight: grad_norm = 0.292126
  l1.bias: grad_norm = 0.003507
  l2.weight: grad_norm = 0.218905
Total gradient norm: 0.599546
=== Actor Training Debug (Iteration 9344) ===
Q mean: -17.394482
Q std: 22.972452
Actor loss: 17.398457
Action reg: 0.003976
  l1.weight: grad_norm = 0.111330
  l1.bias: grad_norm = 0.002403
  l2.weight: grad_norm = 0.089985
Total gradient norm: 0.261904
=== Actor Training Debug (Iteration 9345) ===
Q mean: -14.865398
Q std: 21.713877
Actor loss: 14.869366
Action reg: 0.003967
  l1.weight: grad_norm = 0.133141
  l1.bias: grad_norm = 0.001630
  l2.weight: grad_norm = 0.099096
Total gradient norm: 0.263544
=== Actor Training Debug (Iteration 9346) ===
Q mean: -11.944190
Q std: 18.347971
Actor loss: 11.948147
Action reg: 0.003957
  l1.weight: grad_norm = 0.256173
  l1.bias: grad_norm = 0.001753
  l2.weight: grad_norm = 0.215306
Total gradient norm: 0.528748
=== Actor Training Debug (Iteration 9347) ===
Q mean: -15.588905
Q std: 21.848438
Actor loss: 15.592875
Action reg: 0.003970
  l1.weight: grad_norm = 0.333431
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.255408
Total gradient norm: 0.663770
=== Actor Training Debug (Iteration 9348) ===
Q mean: -12.621113
Q std: 20.219238
Actor loss: 12.625075
Action reg: 0.003962
  l1.weight: grad_norm = 0.442458
  l1.bias: grad_norm = 0.001725
  l2.weight: grad_norm = 0.296610
Total gradient norm: 0.767394
=== Actor Training Debug (Iteration 9349) ===
Q mean: -12.341173
Q std: 17.930246
Actor loss: 12.345140
Action reg: 0.003966
  l1.weight: grad_norm = 0.725344
  l1.bias: grad_norm = 0.001664
  l2.weight: grad_norm = 0.497738
Total gradient norm: 1.292491
=== Actor Training Debug (Iteration 9350) ===
Q mean: -17.459999
Q std: 23.124693
Actor loss: 17.463949
Action reg: 0.003950
  l1.weight: grad_norm = 0.119189
  l1.bias: grad_norm = 0.001925
  l2.weight: grad_norm = 0.096181
Total gradient norm: 0.277196
=== Actor Training Debug (Iteration 9351) ===
Q mean: -14.381622
Q std: 19.441624
Actor loss: 14.385577
Action reg: 0.003955
  l1.weight: grad_norm = 0.390331
  l1.bias: grad_norm = 0.001541
  l2.weight: grad_norm = 0.281866
Total gradient norm: 0.750733
=== Actor Training Debug (Iteration 9352) ===
Q mean: -13.512748
Q std: 20.387672
Actor loss: 13.516713
Action reg: 0.003966
  l1.weight: grad_norm = 0.200631
  l1.bias: grad_norm = 0.001924
  l2.weight: grad_norm = 0.165222
Total gradient norm: 0.463743
=== Actor Training Debug (Iteration 9353) ===
Q mean: -15.706082
Q std: 22.848482
Actor loss: 15.710049
Action reg: 0.003966
  l1.weight: grad_norm = 0.310174
  l1.bias: grad_norm = 0.001499
  l2.weight: grad_norm = 0.240870
Total gradient norm: 0.630480
=== Actor Training Debug (Iteration 9354) ===
Q mean: -15.116142
Q std: 22.747940
Actor loss: 15.120111
Action reg: 0.003968
  l1.weight: grad_norm = 0.254213
  l1.bias: grad_norm = 0.001418
  l2.weight: grad_norm = 0.197418
Total gradient norm: 0.534282
=== Actor Training Debug (Iteration 9355) ===
Q mean: -13.603069
Q std: 20.240561
Actor loss: 13.607013
Action reg: 0.003944
  l1.weight: grad_norm = 0.303065
  l1.bias: grad_norm = 0.003589
  l2.weight: grad_norm = 0.262248
Total gradient norm: 0.678253
=== Actor Training Debug (Iteration 9356) ===
Q mean: -13.599010
Q std: 19.456171
Actor loss: 13.602924
Action reg: 0.003915
  l1.weight: grad_norm = 0.317774
  l1.bias: grad_norm = 0.002769
  l2.weight: grad_norm = 0.255312
Total gradient norm: 0.761036
=== Actor Training Debug (Iteration 9357) ===
Q mean: -13.111218
Q std: 18.388937
Actor loss: 13.115174
Action reg: 0.003956
  l1.weight: grad_norm = 0.202745
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.161752
Total gradient norm: 0.491421
=== Actor Training Debug (Iteration 9358) ===
Q mean: -14.313698
Q std: 20.816168
Actor loss: 14.317657
Action reg: 0.003960
  l1.weight: grad_norm = 0.339668
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.245281
Total gradient norm: 0.672118
=== Actor Training Debug (Iteration 9359) ===
Q mean: -13.619796
Q std: 20.036629
Actor loss: 13.623737
Action reg: 0.003941
  l1.weight: grad_norm = 0.380053
  l1.bias: grad_norm = 0.002877
  l2.weight: grad_norm = 0.250803
Total gradient norm: 0.695487
=== Actor Training Debug (Iteration 9360) ===
Q mean: -15.159878
Q std: 21.058950
Actor loss: 15.163841
Action reg: 0.003963
  l1.weight: grad_norm = 0.323660
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.242265
Total gradient norm: 0.617338
=== Actor Training Debug (Iteration 9361) ===
Q mean: -14.187848
Q std: 21.371065
Actor loss: 14.191809
Action reg: 0.003960
  l1.weight: grad_norm = 0.226400
  l1.bias: grad_norm = 0.001678
  l2.weight: grad_norm = 0.162741
Total gradient norm: 0.451456
=== Actor Training Debug (Iteration 9362) ===
Q mean: -14.086254
Q std: 19.036922
Actor loss: 14.090221
Action reg: 0.003967
  l1.weight: grad_norm = 0.233998
  l1.bias: grad_norm = 0.001835
  l2.weight: grad_norm = 0.197224
Total gradient norm: 0.504916
=== Actor Training Debug (Iteration 9363) ===
Q mean: -13.216833
Q std: 18.925961
Actor loss: 13.220798
Action reg: 0.003965
  l1.weight: grad_norm = 0.173231
  l1.bias: grad_norm = 0.000682
  l2.weight: grad_norm = 0.128336
Total gradient norm: 0.332712
=== Actor Training Debug (Iteration 9364) ===
Q mean: -16.342213
Q std: 22.037106
Actor loss: 16.346188
Action reg: 0.003974
  l1.weight: grad_norm = 0.311723
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.225468
Total gradient norm: 0.635516
=== Actor Training Debug (Iteration 9365) ===
Q mean: -11.710641
Q std: 19.005066
Actor loss: 11.714609
Action reg: 0.003968
  l1.weight: grad_norm = 0.193842
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.148661
Total gradient norm: 0.413084
=== Actor Training Debug (Iteration 9366) ===
Q mean: -14.210840
Q std: 20.054722
Actor loss: 14.214808
Action reg: 0.003968
  l1.weight: grad_norm = 0.158555
  l1.bias: grad_norm = 0.002487
  l2.weight: grad_norm = 0.109734
Total gradient norm: 0.295833
=== Actor Training Debug (Iteration 9367) ===
Q mean: -15.939592
Q std: 22.602301
Actor loss: 15.943564
Action reg: 0.003972
  l1.weight: grad_norm = 0.306280
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.213593
Total gradient norm: 0.534401
=== Actor Training Debug (Iteration 9368) ===
Q mean: -16.174828
Q std: 22.155277
Actor loss: 16.178791
Action reg: 0.003963
  l1.weight: grad_norm = 0.284763
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.204218
Total gradient norm: 0.566159
=== Actor Training Debug (Iteration 9369) ===
Q mean: -16.053196
Q std: 22.169037
Actor loss: 16.057146
Action reg: 0.003950
  l1.weight: grad_norm = 0.207808
  l1.bias: grad_norm = 0.001568
  l2.weight: grad_norm = 0.158455
Total gradient norm: 0.442927
=== Actor Training Debug (Iteration 9370) ===
Q mean: -13.653502
Q std: 19.394388
Actor loss: 13.657455
Action reg: 0.003954
  l1.weight: grad_norm = 0.193402
  l1.bias: grad_norm = 0.006927
  l2.weight: grad_norm = 0.141744
Total gradient norm: 0.446060
=== Actor Training Debug (Iteration 9371) ===
Q mean: -14.331903
Q std: 19.186115
Actor loss: 14.335854
Action reg: 0.003951
  l1.weight: grad_norm = 0.202331
  l1.bias: grad_norm = 0.002344
  l2.weight: grad_norm = 0.138948
Total gradient norm: 0.392503
=== Actor Training Debug (Iteration 9372) ===
Q mean: -14.833630
Q std: 20.890238
Actor loss: 14.837566
Action reg: 0.003936
  l1.weight: grad_norm = 0.177077
  l1.bias: grad_norm = 0.003730
  l2.weight: grad_norm = 0.134504
Total gradient norm: 0.373227
=== Actor Training Debug (Iteration 9373) ===
Q mean: -16.907656
Q std: 23.007671
Actor loss: 16.911625
Action reg: 0.003969
  l1.weight: grad_norm = 0.283829
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.205951
Total gradient norm: 0.547318
=== Actor Training Debug (Iteration 9374) ===
Q mean: -15.375340
Q std: 20.912247
Actor loss: 15.379267
Action reg: 0.003927
  l1.weight: grad_norm = 0.275378
  l1.bias: grad_norm = 0.002785
  l2.weight: grad_norm = 0.216889
Total gradient norm: 0.626643
=== Actor Training Debug (Iteration 9375) ===
Q mean: -15.534273
Q std: 20.971584
Actor loss: 15.538205
Action reg: 0.003932
  l1.weight: grad_norm = 0.485486
  l1.bias: grad_norm = 0.001571
  l2.weight: grad_norm = 0.352373
Total gradient norm: 0.906758
=== Actor Training Debug (Iteration 9376) ===
Q mean: -14.652020
Q std: 21.981346
Actor loss: 14.655986
Action reg: 0.003965
  l1.weight: grad_norm = 0.306001
  l1.bias: grad_norm = 0.001380
  l2.weight: grad_norm = 0.215195
Total gradient norm: 0.615785
=== Actor Training Debug (Iteration 9377) ===
Q mean: -12.656168
Q std: 18.642599
Actor loss: 12.660130
Action reg: 0.003961
  l1.weight: grad_norm = 0.313287
  l1.bias: grad_norm = 0.001764
  l2.weight: grad_norm = 0.242148
Total gradient norm: 0.654363
=== Actor Training Debug (Iteration 9378) ===
Q mean: -15.693336
Q std: 19.802818
Actor loss: 15.697294
Action reg: 0.003957
  l1.weight: grad_norm = 0.253631
  l1.bias: grad_norm = 0.001792
  l2.weight: grad_norm = 0.192182
Total gradient norm: 0.500188
=== Actor Training Debug (Iteration 9379) ===
Q mean: -16.335255
Q std: 21.654701
Actor loss: 16.339218
Action reg: 0.003963
  l1.weight: grad_norm = 0.304991
  l1.bias: grad_norm = 0.001463
  l2.weight: grad_norm = 0.245013
Total gradient norm: 0.677471
=== Actor Training Debug (Iteration 9380) ===
Q mean: -13.155657
Q std: 19.996922
Actor loss: 13.159615
Action reg: 0.003958
  l1.weight: grad_norm = 0.436708
  l1.bias: grad_norm = 0.002953
  l2.weight: grad_norm = 0.332358
Total gradient norm: 0.964271
=== Actor Training Debug (Iteration 9381) ===
Q mean: -14.259992
Q std: 22.153723
Actor loss: 14.263946
Action reg: 0.003954
  l1.weight: grad_norm = 0.485640
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.326319
Total gradient norm: 0.866431
=== Actor Training Debug (Iteration 9382) ===
Q mean: -11.835312
Q std: 18.445095
Actor loss: 11.839274
Action reg: 0.003963
  l1.weight: grad_norm = 0.292473
  l1.bias: grad_norm = 0.001728
  l2.weight: grad_norm = 0.216427
Total gradient norm: 0.556142
=== Actor Training Debug (Iteration 9383) ===
Q mean: -15.899102
Q std: 21.677652
Actor loss: 15.903062
Action reg: 0.003960
  l1.weight: grad_norm = 0.206083
  l1.bias: grad_norm = 0.001413
  l2.weight: grad_norm = 0.137804
Total gradient norm: 0.366712
=== Actor Training Debug (Iteration 9384) ===
Q mean: -13.837425
Q std: 19.006081
Actor loss: 13.841377
Action reg: 0.003952
  l1.weight: grad_norm = 0.361610
  l1.bias: grad_norm = 0.001210
  l2.weight: grad_norm = 0.285421
Total gradient norm: 0.805503
=== Actor Training Debug (Iteration 9385) ===
Q mean: -13.526509
Q std: 19.514778
Actor loss: 13.530468
Action reg: 0.003959
  l1.weight: grad_norm = 0.108770
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.084975
Total gradient norm: 0.228435
=== Actor Training Debug (Iteration 9386) ===
Q mean: -14.327300
Q std: 20.592882
Actor loss: 14.331258
Action reg: 0.003958
  l1.weight: grad_norm = 0.542897
  l1.bias: grad_norm = 0.001870
  l2.weight: grad_norm = 0.441890
Total gradient norm: 1.232656
=== Actor Training Debug (Iteration 9387) ===
Q mean: -12.621022
Q std: 18.312054
Actor loss: 12.624969
Action reg: 0.003947
  l1.weight: grad_norm = 0.358141
  l1.bias: grad_norm = 0.002417
  l2.weight: grad_norm = 0.262894
Total gradient norm: 0.664567
=== Actor Training Debug (Iteration 9388) ===
Q mean: -15.649309
Q std: 22.136566
Actor loss: 15.653275
Action reg: 0.003967
  l1.weight: grad_norm = 0.230017
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.206353
Total gradient norm: 0.568865
=== Actor Training Debug (Iteration 9389) ===
Q mean: -14.726359
Q std: 20.387190
Actor loss: 14.730319
Action reg: 0.003959
  l1.weight: grad_norm = 0.179225
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.129862
Total gradient norm: 0.353055
=== Actor Training Debug (Iteration 9390) ===
Q mean: -16.559010
Q std: 22.210682
Actor loss: 16.562990
Action reg: 0.003980
  l1.weight: grad_norm = 0.169883
  l1.bias: grad_norm = 0.001818
  l2.weight: grad_norm = 0.138643
Total gradient norm: 0.346292
=== Actor Training Debug (Iteration 9391) ===
Q mean: -15.197032
Q std: 20.856146
Actor loss: 15.201005
Action reg: 0.003973
  l1.weight: grad_norm = 0.211779
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.164360
Total gradient norm: 0.449141
=== Actor Training Debug (Iteration 9392) ===
Q mean: -13.612274
Q std: 18.995279
Actor loss: 13.616239
Action reg: 0.003964
  l1.weight: grad_norm = 0.244131
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.186786
Total gradient norm: 0.476173
=== Actor Training Debug (Iteration 9393) ===
Q mean: -13.241404
Q std: 19.243610
Actor loss: 13.245336
Action reg: 0.003932
  l1.weight: grad_norm = 0.278095
  l1.bias: grad_norm = 0.003210
  l2.weight: grad_norm = 0.221586
Total gradient norm: 0.603598
=== Actor Training Debug (Iteration 9394) ===
Q mean: -16.025112
Q std: 21.207079
Actor loss: 16.029087
Action reg: 0.003975
  l1.weight: grad_norm = 0.280030
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.200846
Total gradient norm: 0.562566
=== Actor Training Debug (Iteration 9395) ===
Q mean: -11.742924
Q std: 18.531254
Actor loss: 11.746873
Action reg: 0.003949
  l1.weight: grad_norm = 0.313943
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.245302
Total gradient norm: 0.613882
=== Actor Training Debug (Iteration 9396) ===
Q mean: -15.567887
Q std: 22.448771
Actor loss: 15.571828
Action reg: 0.003941
  l1.weight: grad_norm = 0.214757
  l1.bias: grad_norm = 0.002895
  l2.weight: grad_norm = 0.161056
Total gradient norm: 0.461755
=== Actor Training Debug (Iteration 9397) ===
Q mean: -15.920213
Q std: 23.383581
Actor loss: 15.924174
Action reg: 0.003961
  l1.weight: grad_norm = 0.265546
  l1.bias: grad_norm = 0.003668
  l2.weight: grad_norm = 0.205600
Total gradient norm: 0.590397
=== Actor Training Debug (Iteration 9398) ===
Q mean: -13.208162
Q std: 18.987642
Actor loss: 13.212078
Action reg: 0.003916
  l1.weight: grad_norm = 0.266527
  l1.bias: grad_norm = 0.008102
  l2.weight: grad_norm = 0.207470
Total gradient norm: 0.573128
=== Actor Training Debug (Iteration 9399) ===
Q mean: -13.725224
Q std: 17.588923
Actor loss: 13.729184
Action reg: 0.003961
  l1.weight: grad_norm = 0.409225
  l1.bias: grad_norm = 0.002696
  l2.weight: grad_norm = 0.299293
Total gradient norm: 0.703152
=== Actor Training Debug (Iteration 9400) ===
Q mean: -16.681000
Q std: 21.449579
Actor loss: 16.684956
Action reg: 0.003956
  l1.weight: grad_norm = 0.495178
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.325980
Total gradient norm: 0.847495
=== Actor Training Debug (Iteration 9401) ===
Q mean: -13.906852
Q std: 19.526962
Actor loss: 13.910817
Action reg: 0.003966
  l1.weight: grad_norm = 0.325078
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.258181
Total gradient norm: 0.721218
=== Actor Training Debug (Iteration 9402) ===
Q mean: -13.181097
Q std: 19.844481
Actor loss: 13.185075
Action reg: 0.003978
  l1.weight: grad_norm = 0.385847
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.291339
Total gradient norm: 0.815242
=== Actor Training Debug (Iteration 9403) ===
Q mean: -13.829919
Q std: 19.633940
Actor loss: 13.833870
Action reg: 0.003951
  l1.weight: grad_norm = 0.251235
  l1.bias: grad_norm = 0.004146
  l2.weight: grad_norm = 0.166963
Total gradient norm: 0.467396
=== Actor Training Debug (Iteration 9404) ===
Q mean: -14.540068
Q std: 20.625465
Actor loss: 14.544031
Action reg: 0.003963
  l1.weight: grad_norm = 0.110201
  l1.bias: grad_norm = 0.000958
  l2.weight: grad_norm = 0.086853
Total gradient norm: 0.239440
=== Actor Training Debug (Iteration 9405) ===
Q mean: -16.183170
Q std: 22.982300
Actor loss: 16.187111
Action reg: 0.003940
  l1.weight: grad_norm = 0.350697
  l1.bias: grad_norm = 0.004100
  l2.weight: grad_norm = 0.235790
Total gradient norm: 0.656890
=== Actor Training Debug (Iteration 9406) ===
Q mean: -14.071700
Q std: 19.727013
Actor loss: 14.075674
Action reg: 0.003974
  l1.weight: grad_norm = 0.249982
  l1.bias: grad_norm = 0.001487
  l2.weight: grad_norm = 0.182620
Total gradient norm: 0.454268
=== Actor Training Debug (Iteration 9407) ===
Q mean: -12.714046
Q std: 18.985876
Actor loss: 12.717988
Action reg: 0.003942
  l1.weight: grad_norm = 0.287563
  l1.bias: grad_norm = 0.000998
  l2.weight: grad_norm = 0.205613
Total gradient norm: 0.506540
=== Actor Training Debug (Iteration 9408) ===
Q mean: -12.987896
Q std: 16.234858
Actor loss: 12.991853
Action reg: 0.003957
  l1.weight: grad_norm = 0.194842
  l1.bias: grad_norm = 0.002523
  l2.weight: grad_norm = 0.126556
Total gradient norm: 0.360094
=== Actor Training Debug (Iteration 9409) ===
Q mean: -15.006877
Q std: 21.099495
Actor loss: 15.010839
Action reg: 0.003963
  l1.weight: grad_norm = 0.403783
  l1.bias: grad_norm = 0.002798
  l2.weight: grad_norm = 0.245954
Total gradient norm: 0.680364
=== Actor Training Debug (Iteration 9410) ===
Q mean: -15.118509
Q std: 20.682980
Actor loss: 15.122455
Action reg: 0.003945
  l1.weight: grad_norm = 0.304157
  l1.bias: grad_norm = 0.005023
  l2.weight: grad_norm = 0.241928
Total gradient norm: 0.649078
=== Actor Training Debug (Iteration 9411) ===
Q mean: -14.788343
Q std: 20.531361
Actor loss: 14.792297
Action reg: 0.003954
  l1.weight: grad_norm = 0.425157
  l1.bias: grad_norm = 0.001250
  l2.weight: grad_norm = 0.305909
Total gradient norm: 0.788389
=== Actor Training Debug (Iteration 9412) ===
Q mean: -16.207367
Q std: 22.490740
Actor loss: 16.211346
Action reg: 0.003979
  l1.weight: grad_norm = 0.128767
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.138395
Total gradient norm: 0.394355
=== Actor Training Debug (Iteration 9413) ===
Q mean: -13.090265
Q std: 20.373270
Actor loss: 13.094240
Action reg: 0.003975
  l1.weight: grad_norm = 0.230986
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.157644
Total gradient norm: 0.401672
=== Actor Training Debug (Iteration 9414) ===
Q mean: -13.572750
Q std: 20.264112
Actor loss: 13.576717
Action reg: 0.003968
  l1.weight: grad_norm = 0.375671
  l1.bias: grad_norm = 0.001755
  l2.weight: grad_norm = 0.265935
Total gradient norm: 0.692019
=== Actor Training Debug (Iteration 9415) ===
Q mean: -14.257917
Q std: 21.341574
Actor loss: 14.261888
Action reg: 0.003970
  l1.weight: grad_norm = 0.286298
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.197718
Total gradient norm: 0.567192
=== Actor Training Debug (Iteration 9416) ===
Q mean: -17.552034
Q std: 22.538363
Actor loss: 17.556005
Action reg: 0.003972
  l1.weight: grad_norm = 0.161029
  l1.bias: grad_norm = 0.003321
  l2.weight: grad_norm = 0.128455
Total gradient norm: 0.377219
=== Actor Training Debug (Iteration 9417) ===
Q mean: -14.878920
Q std: 21.318895
Actor loss: 14.882883
Action reg: 0.003963
  l1.weight: grad_norm = 0.350927
  l1.bias: grad_norm = 0.003024
  l2.weight: grad_norm = 0.281404
Total gradient norm: 0.794300
=== Actor Training Debug (Iteration 9418) ===
Q mean: -14.909520
Q std: 21.573298
Actor loss: 14.913470
Action reg: 0.003950
  l1.weight: grad_norm = 0.225956
  l1.bias: grad_norm = 0.001794
  l2.weight: grad_norm = 0.167281
Total gradient norm: 0.413233
=== Actor Training Debug (Iteration 9419) ===
Q mean: -11.991485
Q std: 18.664742
Actor loss: 11.995438
Action reg: 0.003953
  l1.weight: grad_norm = 0.208857
  l1.bias: grad_norm = 0.002849
  l2.weight: grad_norm = 0.148752
Total gradient norm: 0.384041
=== Actor Training Debug (Iteration 9420) ===
Q mean: -15.040680
Q std: 20.927172
Actor loss: 15.044641
Action reg: 0.003962
  l1.weight: grad_norm = 0.125054
  l1.bias: grad_norm = 0.000597
  l2.weight: grad_norm = 0.105167
Total gradient norm: 0.315651
=== Actor Training Debug (Iteration 9421) ===
Q mean: -14.379478
Q std: 19.845779
Actor loss: 14.383435
Action reg: 0.003958
  l1.weight: grad_norm = 0.320567
  l1.bias: grad_norm = 0.001501
  l2.weight: grad_norm = 0.234305
Total gradient norm: 0.601457
=== Actor Training Debug (Iteration 9422) ===
Q mean: -14.175989
Q std: 19.328535
Actor loss: 14.179944
Action reg: 0.003955
  l1.weight: grad_norm = 0.193123
  l1.bias: grad_norm = 0.001114
  l2.weight: grad_norm = 0.142768
Total gradient norm: 0.391233
=== Actor Training Debug (Iteration 9423) ===
Q mean: -15.192594
Q std: 21.670156
Actor loss: 15.196561
Action reg: 0.003967
  l1.weight: grad_norm = 0.143048
  l1.bias: grad_norm = 0.000906
  l2.weight: grad_norm = 0.116531
Total gradient norm: 0.321124
=== Actor Training Debug (Iteration 9424) ===
Q mean: -15.920115
Q std: 21.218891
Actor loss: 15.924053
Action reg: 0.003939
  l1.weight: grad_norm = 0.184083
  l1.bias: grad_norm = 0.000979
  l2.weight: grad_norm = 0.143192
Total gradient norm: 0.388791
=== Actor Training Debug (Iteration 9425) ===
Q mean: -14.852917
Q std: 20.690439
Actor loss: 14.856877
Action reg: 0.003960
  l1.weight: grad_norm = 0.148478
  l1.bias: grad_norm = 0.002656
  l2.weight: grad_norm = 0.117612
Total gradient norm: 0.302653
=== Actor Training Debug (Iteration 9426) ===
Q mean: -14.789690
Q std: 20.343699
Actor loss: 14.793658
Action reg: 0.003968
  l1.weight: grad_norm = 0.158643
  l1.bias: grad_norm = 0.001950
  l2.weight: grad_norm = 0.119664
Total gradient norm: 0.368304
=== Actor Training Debug (Iteration 9427) ===
Q mean: -13.977116
Q std: 20.887617
Actor loss: 13.981093
Action reg: 0.003978
  l1.weight: grad_norm = 0.170703
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.129776
Total gradient norm: 0.341843
=== Actor Training Debug (Iteration 9428) ===
Q mean: -11.641940
Q std: 16.567581
Actor loss: 11.645898
Action reg: 0.003958
  l1.weight: grad_norm = 0.187832
  l1.bias: grad_norm = 0.002540
  l2.weight: grad_norm = 0.144468
Total gradient norm: 0.423120
=== Actor Training Debug (Iteration 9429) ===
Q mean: -15.663463
Q std: 22.097633
Actor loss: 15.667412
Action reg: 0.003949
  l1.weight: grad_norm = 0.260343
  l1.bias: grad_norm = 0.002315
  l2.weight: grad_norm = 0.195603
Total gradient norm: 0.580148
=== Actor Training Debug (Iteration 9430) ===
Q mean: -15.032532
Q std: 21.602850
Actor loss: 15.036491
Action reg: 0.003959
  l1.weight: grad_norm = 0.302980
  l1.bias: grad_norm = 0.001401
  l2.weight: grad_norm = 0.236912
Total gradient norm: 0.636533
=== Actor Training Debug (Iteration 9431) ===
Q mean: -12.800767
Q std: 18.713396
Actor loss: 12.804722
Action reg: 0.003955
  l1.weight: grad_norm = 0.579351
  l1.bias: grad_norm = 0.001448
  l2.weight: grad_norm = 0.475720
Total gradient norm: 1.659619
=== Actor Training Debug (Iteration 9432) ===
Q mean: -16.470884
Q std: 23.442236
Actor loss: 16.474840
Action reg: 0.003955
  l1.weight: grad_norm = 0.278899
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.198818
Total gradient norm: 0.512739
=== Actor Training Debug (Iteration 9433) ===
Q mean: -14.566174
Q std: 19.978123
Actor loss: 14.570142
Action reg: 0.003968
  l1.weight: grad_norm = 0.199560
  l1.bias: grad_norm = 0.003198
  l2.weight: grad_norm = 0.151798
Total gradient norm: 0.421022
=== Actor Training Debug (Iteration 9434) ===
Q mean: -16.159159
Q std: 22.950884
Actor loss: 16.163109
Action reg: 0.003951
  l1.weight: grad_norm = 0.245682
  l1.bias: grad_norm = 0.002533
  l2.weight: grad_norm = 0.187962
Total gradient norm: 0.489122
=== Actor Training Debug (Iteration 9435) ===
Q mean: -12.345434
Q std: 19.666840
Actor loss: 12.349392
Action reg: 0.003957
  l1.weight: grad_norm = 0.379759
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.252883
Total gradient norm: 0.691002
=== Actor Training Debug (Iteration 9436) ===
Q mean: -14.596838
Q std: 21.244770
Actor loss: 14.600803
Action reg: 0.003965
  l1.weight: grad_norm = 0.171475
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.131359
Total gradient norm: 0.343828
=== Actor Training Debug (Iteration 9437) ===
Q mean: -13.234454
Q std: 20.725679
Actor loss: 13.238403
Action reg: 0.003949
  l1.weight: grad_norm = 0.320661
  l1.bias: grad_norm = 0.001265
  l2.weight: grad_norm = 0.268740
Total gradient norm: 0.714475
=== Actor Training Debug (Iteration 9438) ===
Q mean: -14.178246
Q std: 20.219494
Actor loss: 14.182190
Action reg: 0.003944
  l1.weight: grad_norm = 0.532377
  l1.bias: grad_norm = 0.002731
  l2.weight: grad_norm = 0.394762
Total gradient norm: 1.072220
=== Actor Training Debug (Iteration 9439) ===
Q mean: -16.504953
Q std: 21.492298
Actor loss: 16.508917
Action reg: 0.003963
  l1.weight: grad_norm = 0.293907
  l1.bias: grad_norm = 0.001889
  l2.weight: grad_norm = 0.221550
Total gradient norm: 0.543544
=== Actor Training Debug (Iteration 9440) ===
Q mean: -13.660389
Q std: 21.006798
Actor loss: 13.664361
Action reg: 0.003972
  l1.weight: grad_norm = 0.213118
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.160259
Total gradient norm: 0.426023
=== Actor Training Debug (Iteration 9441) ===
Q mean: -14.349406
Q std: 19.518608
Actor loss: 14.353351
Action reg: 0.003944
  l1.weight: grad_norm = 0.324337
  l1.bias: grad_norm = 0.005446
  l2.weight: grad_norm = 0.212256
Total gradient norm: 0.530694
=== Actor Training Debug (Iteration 9442) ===
Q mean: -14.667922
Q std: 20.097561
Actor loss: 14.671876
Action reg: 0.003954
  l1.weight: grad_norm = 0.234116
  l1.bias: grad_norm = 0.001406
  l2.weight: grad_norm = 0.152432
Total gradient norm: 0.402599
=== Actor Training Debug (Iteration 9443) ===
Q mean: -16.646843
Q std: 22.104818
Actor loss: 16.650803
Action reg: 0.003961
  l1.weight: grad_norm = 0.220599
  l1.bias: grad_norm = 0.001500
  l2.weight: grad_norm = 0.192522
Total gradient norm: 0.524864
=== Actor Training Debug (Iteration 9444) ===
Q mean: -13.426321
Q std: 19.978283
Actor loss: 13.430277
Action reg: 0.003955
  l1.weight: grad_norm = 0.250245
  l1.bias: grad_norm = 0.002713
  l2.weight: grad_norm = 0.177923
Total gradient norm: 0.449762
=== Actor Training Debug (Iteration 9445) ===
Q mean: -15.633540
Q std: 22.184477
Actor loss: 15.637485
Action reg: 0.003944
  l1.weight: grad_norm = 0.149326
  l1.bias: grad_norm = 0.001738
  l2.weight: grad_norm = 0.112044
Total gradient norm: 0.289993
=== Actor Training Debug (Iteration 9446) ===
Q mean: -16.495005
Q std: 22.626732
Actor loss: 16.498947
Action reg: 0.003943
  l1.weight: grad_norm = 0.274045
  l1.bias: grad_norm = 0.004697
  l2.weight: grad_norm = 0.216747
Total gradient norm: 0.676608
=== Actor Training Debug (Iteration 9447) ===
Q mean: -14.712559
Q std: 20.871267
Actor loss: 14.716534
Action reg: 0.003975
  l1.weight: grad_norm = 0.219436
  l1.bias: grad_norm = 0.001619
  l2.weight: grad_norm = 0.159752
Total gradient norm: 0.420116
=== Actor Training Debug (Iteration 9448) ===
Q mean: -15.630124
Q std: 21.931913
Actor loss: 15.634055
Action reg: 0.003931
  l1.weight: grad_norm = 0.337387
  l1.bias: grad_norm = 0.001528
  l2.weight: grad_norm = 0.253496
Total gradient norm: 0.655685
=== Actor Training Debug (Iteration 9449) ===
Q mean: -13.731390
Q std: 18.567324
Actor loss: 13.735343
Action reg: 0.003953
  l1.weight: grad_norm = 0.428733
  l1.bias: grad_norm = 0.001365
  l2.weight: grad_norm = 0.292771
Total gradient norm: 0.802342
=== Actor Training Debug (Iteration 9450) ===
Q mean: -13.673647
Q std: 19.725077
Actor loss: 13.677593
Action reg: 0.003947
  l1.weight: grad_norm = 0.237494
  l1.bias: grad_norm = 0.001003
  l2.weight: grad_norm = 0.165226
Total gradient norm: 0.458790
=== Actor Training Debug (Iteration 9451) ===
Q mean: -13.879698
Q std: 20.740940
Actor loss: 13.883658
Action reg: 0.003960
  l1.weight: grad_norm = 0.405838
  l1.bias: grad_norm = 0.001120
  l2.weight: grad_norm = 0.309393
Total gradient norm: 0.981268
=== Actor Training Debug (Iteration 9452) ===
Q mean: -13.236087
Q std: 20.283049
Actor loss: 13.240026
Action reg: 0.003940
  l1.weight: grad_norm = 0.527016
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.376104
Total gradient norm: 0.914836
=== Actor Training Debug (Iteration 9453) ===
Q mean: -13.779621
Q std: 21.666613
Actor loss: 13.783579
Action reg: 0.003958
  l1.weight: grad_norm = 0.235652
  l1.bias: grad_norm = 0.002873
  l2.weight: grad_norm = 0.160027
Total gradient norm: 0.423384
=== Actor Training Debug (Iteration 9454) ===
Q mean: -16.048855
Q std: 21.759596
Actor loss: 16.052832
Action reg: 0.003976
  l1.weight: grad_norm = 0.250228
  l1.bias: grad_norm = 0.001101
  l2.weight: grad_norm = 0.193047
Total gradient norm: 0.569008
=== Actor Training Debug (Iteration 9455) ===
Q mean: -15.603692
Q std: 21.705070
Actor loss: 15.607624
Action reg: 0.003932
  l1.weight: grad_norm = 0.305959
  l1.bias: grad_norm = 0.006095
  l2.weight: grad_norm = 0.253931
Total gradient norm: 0.759926
=== Actor Training Debug (Iteration 9456) ===
Q mean: -13.119084
Q std: 19.559256
Actor loss: 13.123037
Action reg: 0.003953
  l1.weight: grad_norm = 0.351084
  l1.bias: grad_norm = 0.001302
  l2.weight: grad_norm = 0.279465
Total gradient norm: 0.666129
=== Actor Training Debug (Iteration 9457) ===
Q mean: -16.016500
Q std: 20.506014
Actor loss: 16.020464
Action reg: 0.003964
  l1.weight: grad_norm = 0.259026
  l1.bias: grad_norm = 0.000799
  l2.weight: grad_norm = 0.204940
Total gradient norm: 0.598928
=== Actor Training Debug (Iteration 9458) ===
Q mean: -14.563225
Q std: 19.017357
Actor loss: 14.567192
Action reg: 0.003967
  l1.weight: grad_norm = 0.196830
  l1.bias: grad_norm = 0.001582
  l2.weight: grad_norm = 0.139611
Total gradient norm: 0.366968
=== Actor Training Debug (Iteration 9459) ===
Q mean: -14.589243
Q std: 21.640228
Actor loss: 14.593190
Action reg: 0.003947
  l1.weight: grad_norm = 0.218737
  l1.bias: grad_norm = 0.003021
  l2.weight: grad_norm = 0.166807
Total gradient norm: 0.476242
=== Actor Training Debug (Iteration 9460) ===
Q mean: -13.991813
Q std: 21.148949
Actor loss: 13.995755
Action reg: 0.003942
  l1.weight: grad_norm = 0.281947
  l1.bias: grad_norm = 0.003212
  l2.weight: grad_norm = 0.187934
Total gradient norm: 0.563233
=== Actor Training Debug (Iteration 9461) ===
Q mean: -13.737253
Q std: 20.981478
Actor loss: 13.741196
Action reg: 0.003942
  l1.weight: grad_norm = 0.274318
  l1.bias: grad_norm = 0.002399
  l2.weight: grad_norm = 0.196875
Total gradient norm: 0.517488
=== Actor Training Debug (Iteration 9462) ===
Q mean: -15.896307
Q std: 20.861605
Actor loss: 15.900265
Action reg: 0.003957
  l1.weight: grad_norm = 0.293646
  l1.bias: grad_norm = 0.001497
  l2.weight: grad_norm = 0.241821
Total gradient norm: 0.611661
=== Actor Training Debug (Iteration 9463) ===
Q mean: -15.671335
Q std: 23.069391
Actor loss: 15.675292
Action reg: 0.003956
  l1.weight: grad_norm = 0.415056
  l1.bias: grad_norm = 0.003405
  l2.weight: grad_norm = 0.274469
Total gradient norm: 0.708425
=== Actor Training Debug (Iteration 9464) ===
Q mean: -14.244755
Q std: 20.515579
Actor loss: 14.248717
Action reg: 0.003963
  l1.weight: grad_norm = 0.355787
  l1.bias: grad_norm = 0.001219
  l2.weight: grad_norm = 0.235336
Total gradient norm: 0.579508
=== Actor Training Debug (Iteration 9465) ===
Q mean: -13.485647
Q std: 20.047611
Actor loss: 13.489602
Action reg: 0.003955
  l1.weight: grad_norm = 0.293775
  l1.bias: grad_norm = 0.003649
  l2.weight: grad_norm = 0.197904
Total gradient norm: 0.524978
=== Actor Training Debug (Iteration 9466) ===
Q mean: -15.472548
Q std: 20.464949
Actor loss: 15.476505
Action reg: 0.003957
  l1.weight: grad_norm = 0.388254
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.301078
Total gradient norm: 0.880796
=== Actor Training Debug (Iteration 9467) ===
Q mean: -14.588562
Q std: 19.703846
Actor loss: 14.592527
Action reg: 0.003966
  l1.weight: grad_norm = 0.423171
  l1.bias: grad_norm = 0.000948
  l2.weight: grad_norm = 0.281670
Total gradient norm: 0.841254
=== Actor Training Debug (Iteration 9468) ===
Q mean: -13.172550
Q std: 21.184683
Actor loss: 13.176517
Action reg: 0.003967
  l1.weight: grad_norm = 0.179963
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.159473
Total gradient norm: 0.507998
=== Actor Training Debug (Iteration 9469) ===
Q mean: -16.816290
Q std: 21.471010
Actor loss: 16.820255
Action reg: 0.003965
  l1.weight: grad_norm = 0.367347
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.267345
Total gradient norm: 0.711155
=== Actor Training Debug (Iteration 9470) ===
Q mean: -16.399710
Q std: 22.446140
Actor loss: 16.403671
Action reg: 0.003961
  l1.weight: grad_norm = 0.281340
  l1.bias: grad_norm = 0.001258
  l2.weight: grad_norm = 0.229149
Total gradient norm: 0.592462
=== Actor Training Debug (Iteration 9471) ===
Q mean: -16.452473
Q std: 21.960100
Actor loss: 16.456448
Action reg: 0.003974
  l1.weight: grad_norm = 0.155959
  l1.bias: grad_norm = 0.001584
  l2.weight: grad_norm = 0.121104
Total gradient norm: 0.349334
=== Actor Training Debug (Iteration 9472) ===
Q mean: -16.490452
Q std: 20.929789
Actor loss: 16.494415
Action reg: 0.003964
  l1.weight: grad_norm = 0.177998
  l1.bias: grad_norm = 0.001265
  l2.weight: grad_norm = 0.154163
Total gradient norm: 0.370522
=== Actor Training Debug (Iteration 9473) ===
Q mean: -13.663626
Q std: 20.474743
Actor loss: 13.667602
Action reg: 0.003976
  l1.weight: grad_norm = 0.157883
  l1.bias: grad_norm = 0.001313
  l2.weight: grad_norm = 0.138024
Total gradient norm: 0.346493
=== Actor Training Debug (Iteration 9474) ===
Q mean: -15.571777
Q std: 20.846378
Actor loss: 15.575711
Action reg: 0.003934
  l1.weight: grad_norm = 0.372767
  l1.bias: grad_norm = 0.004959
  l2.weight: grad_norm = 0.258505
Total gradient norm: 0.730212
=== Actor Training Debug (Iteration 9475) ===
Q mean: -13.494232
Q std: 19.387472
Actor loss: 13.498198
Action reg: 0.003965
  l1.weight: grad_norm = 0.306894
  l1.bias: grad_norm = 0.006108
  l2.weight: grad_norm = 0.225413
Total gradient norm: 0.784706
=== Actor Training Debug (Iteration 9476) ===
Q mean: -13.881354
Q std: 22.150517
Actor loss: 13.885316
Action reg: 0.003962
  l1.weight: grad_norm = 0.237174
  l1.bias: grad_norm = 0.001814
  l2.weight: grad_norm = 0.170122
Total gradient norm: 0.464246
=== Actor Training Debug (Iteration 9477) ===
Q mean: -15.294428
Q std: 22.505917
Actor loss: 15.298401
Action reg: 0.003973
  l1.weight: grad_norm = 0.289073
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.199757
Total gradient norm: 0.548776
=== Actor Training Debug (Iteration 9478) ===
Q mean: -16.784832
Q std: 22.150124
Actor loss: 16.788801
Action reg: 0.003969
  l1.weight: grad_norm = 0.263513
  l1.bias: grad_norm = 0.002643
  l2.weight: grad_norm = 0.202507
Total gradient norm: 0.557262
=== Actor Training Debug (Iteration 9479) ===
Q mean: -17.427559
Q std: 21.997087
Actor loss: 17.431517
Action reg: 0.003957
  l1.weight: grad_norm = 0.413944
  l1.bias: grad_norm = 0.001587
  l2.weight: grad_norm = 0.300199
Total gradient norm: 1.022885
=== Actor Training Debug (Iteration 9480) ===
Q mean: -13.501233
Q std: 19.934187
Actor loss: 13.505198
Action reg: 0.003966
  l1.weight: grad_norm = 0.274662
  l1.bias: grad_norm = 0.001653
  l2.weight: grad_norm = 0.194199
Total gradient norm: 0.541240
=== Actor Training Debug (Iteration 9481) ===
Q mean: -13.371464
Q std: 18.373819
Actor loss: 13.375394
Action reg: 0.003930
  l1.weight: grad_norm = 0.214509
  l1.bias: grad_norm = 0.004026
  l2.weight: grad_norm = 0.170824
Total gradient norm: 0.508467
=== Actor Training Debug (Iteration 9482) ===
Q mean: -14.137377
Q std: 20.119921
Actor loss: 14.141327
Action reg: 0.003950
  l1.weight: grad_norm = 0.320252
  l1.bias: grad_norm = 0.002457
  l2.weight: grad_norm = 0.211352
Total gradient norm: 0.586740
=== Actor Training Debug (Iteration 9483) ===
Q mean: -12.940296
Q std: 20.061007
Actor loss: 12.944251
Action reg: 0.003955
  l1.weight: grad_norm = 0.678342
  l1.bias: grad_norm = 0.002691
  l2.weight: grad_norm = 0.440745
Total gradient norm: 1.225966
=== Actor Training Debug (Iteration 9484) ===
Q mean: -14.283561
Q std: 19.884626
Actor loss: 14.287519
Action reg: 0.003958
  l1.weight: grad_norm = 0.267631
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.219820
Total gradient norm: 0.693435
=== Actor Training Debug (Iteration 9485) ===
Q mean: -15.856770
Q std: 21.000412
Actor loss: 15.860726
Action reg: 0.003957
  l1.weight: grad_norm = 0.190109
  l1.bias: grad_norm = 0.001978
  l2.weight: grad_norm = 0.148012
Total gradient norm: 0.458569
=== Actor Training Debug (Iteration 9486) ===
Q mean: -15.381184
Q std: 20.578562
Actor loss: 15.385148
Action reg: 0.003965
  l1.weight: grad_norm = 0.269276
  l1.bias: grad_norm = 0.001927
  l2.weight: grad_norm = 0.223535
Total gradient norm: 0.573982
=== Actor Training Debug (Iteration 9487) ===
Q mean: -15.225121
Q std: 20.826035
Actor loss: 15.229097
Action reg: 0.003977
  l1.weight: grad_norm = 0.103143
  l1.bias: grad_norm = 0.001268
  l2.weight: grad_norm = 0.085147
Total gradient norm: 0.238566
=== Actor Training Debug (Iteration 9488) ===
Q mean: -13.357400
Q std: 18.473980
Actor loss: 13.361363
Action reg: 0.003964
  l1.weight: grad_norm = 0.240298
  l1.bias: grad_norm = 0.004310
  l2.weight: grad_norm = 0.193302
Total gradient norm: 0.553818
=== Actor Training Debug (Iteration 9489) ===
Q mean: -17.374100
Q std: 22.008797
Actor loss: 17.378061
Action reg: 0.003961
  l1.weight: grad_norm = 0.344144
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.248949
Total gradient norm: 0.653768
=== Actor Training Debug (Iteration 9490) ===
Q mean: -13.648140
Q std: 21.644819
Actor loss: 13.652092
Action reg: 0.003952
  l1.weight: grad_norm = 0.294328
  l1.bias: grad_norm = 0.001399
  l2.weight: grad_norm = 0.241185
Total gradient norm: 0.594150
=== Actor Training Debug (Iteration 9491) ===
Q mean: -12.986011
Q std: 19.009829
Actor loss: 12.989968
Action reg: 0.003958
  l1.weight: grad_norm = 0.287569
  l1.bias: grad_norm = 0.004923
  l2.weight: grad_norm = 0.202603
Total gradient norm: 0.574992
=== Actor Training Debug (Iteration 9492) ===
Q mean: -15.304826
Q std: 21.488718
Actor loss: 15.308796
Action reg: 0.003970
  l1.weight: grad_norm = 0.149850
  l1.bias: grad_norm = 0.001011
  l2.weight: grad_norm = 0.112879
Total gradient norm: 0.281572
=== Actor Training Debug (Iteration 9493) ===
Q mean: -13.863548
Q std: 20.912397
Actor loss: 13.867516
Action reg: 0.003968
  l1.weight: grad_norm = 0.367384
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.267780
Total gradient norm: 0.738626
=== Actor Training Debug (Iteration 9494) ===
Q mean: -14.631524
Q std: 20.714411
Actor loss: 14.635473
Action reg: 0.003949
  l1.weight: grad_norm = 0.392149
  l1.bias: grad_norm = 0.004256
  l2.weight: grad_norm = 0.332233
Total gradient norm: 0.857649
=== Actor Training Debug (Iteration 9495) ===
Q mean: -16.423527
Q std: 22.771173
Actor loss: 16.427494
Action reg: 0.003967
  l1.weight: grad_norm = 0.319309
  l1.bias: grad_norm = 0.003854
  l2.weight: grad_norm = 0.256068
Total gradient norm: 0.741689
=== Actor Training Debug (Iteration 9496) ===
Q mean: -13.218616
Q std: 17.626936
Actor loss: 13.222588
Action reg: 0.003971
  l1.weight: grad_norm = 0.173044
  l1.bias: grad_norm = 0.001793
  l2.weight: grad_norm = 0.147195
Total gradient norm: 0.369226
=== Actor Training Debug (Iteration 9497) ===
Q mean: -13.474415
Q std: 19.729862
Actor loss: 13.478383
Action reg: 0.003968
  l1.weight: grad_norm = 0.483987
  l1.bias: grad_norm = 0.002739
  l2.weight: grad_norm = 0.276534
Total gradient norm: 0.795332
=== Actor Training Debug (Iteration 9498) ===
Q mean: -13.616886
Q std: 18.584904
Actor loss: 13.620847
Action reg: 0.003960
  l1.weight: grad_norm = 0.564496
  l1.bias: grad_norm = 0.005672
  l2.weight: grad_norm = 0.433871
Total gradient norm: 1.018127
=== Actor Training Debug (Iteration 9499) ===
Q mean: -15.602844
Q std: 21.956465
Actor loss: 15.606799
Action reg: 0.003955
  l1.weight: grad_norm = 0.284375
  l1.bias: grad_norm = 0.001130
  l2.weight: grad_norm = 0.215709
Total gradient norm: 0.636353
=== Actor Training Debug (Iteration 9500) ===
Q mean: -12.897696
Q std: 18.159220
Actor loss: 12.901649
Action reg: 0.003952
  l1.weight: grad_norm = 0.370875
  l1.bias: grad_norm = 0.007222
  l2.weight: grad_norm = 0.266259
Total gradient norm: 0.739684
  Average reward: -318.943 | Average length: 100.0
Evaluation at episode 145: -318.943
=== Actor Training Debug (Iteration 9501) ===
Q mean: -14.104837
Q std: 20.917070
Actor loss: 14.108805
Action reg: 0.003967
  l1.weight: grad_norm = 0.261392
  l1.bias: grad_norm = 0.002098
  l2.weight: grad_norm = 0.201623
Total gradient norm: 0.500194
=== Actor Training Debug (Iteration 9502) ===
Q mean: -15.394661
Q std: 21.921583
Actor loss: 15.398631
Action reg: 0.003971
  l1.weight: grad_norm = 0.295055
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.186201
Total gradient norm: 0.529766
=== Actor Training Debug (Iteration 9503) ===
Q mean: -13.518959
Q std: 20.979382
Actor loss: 13.522924
Action reg: 0.003965
  l1.weight: grad_norm = 0.153800
  l1.bias: grad_norm = 0.002242
  l2.weight: grad_norm = 0.123696
Total gradient norm: 0.333488
=== Actor Training Debug (Iteration 9504) ===
Q mean: -18.204586
Q std: 23.294500
Actor loss: 18.208557
Action reg: 0.003971
  l1.weight: grad_norm = 0.141886
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.133395
Total gradient norm: 0.361559
=== Actor Training Debug (Iteration 9505) ===
Q mean: -15.323055
Q std: 21.298208
Actor loss: 15.327009
Action reg: 0.003954
  l1.weight: grad_norm = 0.318929
  l1.bias: grad_norm = 0.002656
  l2.weight: grad_norm = 0.270453
Total gradient norm: 0.798640
=== Actor Training Debug (Iteration 9506) ===
Q mean: -14.258524
Q std: 18.777416
Actor loss: 14.262469
Action reg: 0.003945
  l1.weight: grad_norm = 0.344168
  l1.bias: grad_norm = 0.003223
  l2.weight: grad_norm = 0.264864
Total gradient norm: 0.730999
=== Actor Training Debug (Iteration 9507) ===
Q mean: -15.282346
Q std: 21.016600
Actor loss: 15.286294
Action reg: 0.003948
  l1.weight: grad_norm = 0.384004
  l1.bias: grad_norm = 0.003278
  l2.weight: grad_norm = 0.321808
Total gradient norm: 0.762177
=== Actor Training Debug (Iteration 9508) ===
Q mean: -13.942518
Q std: 19.857756
Actor loss: 13.946486
Action reg: 0.003968
  l1.weight: grad_norm = 0.292880
  l1.bias: grad_norm = 0.001702
  l2.weight: grad_norm = 0.194841
Total gradient norm: 0.503072
=== Actor Training Debug (Iteration 9509) ===
Q mean: -12.247959
Q std: 18.269493
Actor loss: 12.251916
Action reg: 0.003956
  l1.weight: grad_norm = 0.278442
  l1.bias: grad_norm = 0.002606
  l2.weight: grad_norm = 0.223102
Total gradient norm: 0.652209
=== Actor Training Debug (Iteration 9510) ===
Q mean: -13.162781
Q std: 19.347534
Actor loss: 13.166735
Action reg: 0.003954
  l1.weight: grad_norm = 0.382383
  l1.bias: grad_norm = 0.001295
  l2.weight: grad_norm = 0.317700
Total gradient norm: 1.023151
=== Actor Training Debug (Iteration 9511) ===
Q mean: -15.237892
Q std: 20.070501
Actor loss: 15.241874
Action reg: 0.003981
  l1.weight: grad_norm = 0.222429
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.161083
Total gradient norm: 0.402374
=== Actor Training Debug (Iteration 9512) ===
Q mean: -16.424326
Q std: 22.188097
Actor loss: 16.428305
Action reg: 0.003979
  l1.weight: grad_norm = 0.179352
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.124984
Total gradient norm: 0.352221
=== Actor Training Debug (Iteration 9513) ===
Q mean: -12.247250
Q std: 17.488842
Actor loss: 12.251211
Action reg: 0.003962
  l1.weight: grad_norm = 0.254535
  l1.bias: grad_norm = 0.000900
  l2.weight: grad_norm = 0.185175
Total gradient norm: 0.519257
=== Actor Training Debug (Iteration 9514) ===
Q mean: -16.657883
Q std: 23.270300
Actor loss: 16.661852
Action reg: 0.003969
  l1.weight: grad_norm = 0.338756
  l1.bias: grad_norm = 0.000942
  l2.weight: grad_norm = 0.256164
Total gradient norm: 0.716419
=== Actor Training Debug (Iteration 9515) ===
Q mean: -15.667993
Q std: 22.968647
Actor loss: 15.671940
Action reg: 0.003947
  l1.weight: grad_norm = 0.366244
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.311443
Total gradient norm: 0.957973
=== Actor Training Debug (Iteration 9516) ===
Q mean: -13.392575
Q std: 20.901253
Actor loss: 13.396526
Action reg: 0.003951
  l1.weight: grad_norm = 0.366143
  l1.bias: grad_norm = 0.000936
  l2.weight: grad_norm = 0.267303
Total gradient norm: 0.755653
=== Actor Training Debug (Iteration 9517) ===
Q mean: -16.642397
Q std: 22.113012
Actor loss: 16.646376
Action reg: 0.003978
  l1.weight: grad_norm = 0.121767
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.086105
Total gradient norm: 0.221778
=== Actor Training Debug (Iteration 9518) ===
Q mean: -16.170055
Q std: 22.565945
Actor loss: 16.174021
Action reg: 0.003965
  l1.weight: grad_norm = 0.239538
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.155508
Total gradient norm: 0.433034
=== Actor Training Debug (Iteration 9519) ===
Q mean: -13.386917
Q std: 20.373764
Actor loss: 13.390877
Action reg: 0.003959
  l1.weight: grad_norm = 0.202681
  l1.bias: grad_norm = 0.001091
  l2.weight: grad_norm = 0.154590
Total gradient norm: 0.399163
=== Actor Training Debug (Iteration 9520) ===
Q mean: -14.177746
Q std: 21.080563
Actor loss: 14.181711
Action reg: 0.003965
  l1.weight: grad_norm = 0.295776
  l1.bias: grad_norm = 0.002311
  l2.weight: grad_norm = 0.213301
Total gradient norm: 0.656190
=== Actor Training Debug (Iteration 9521) ===
Q mean: -13.659412
Q std: 19.947302
Actor loss: 13.663366
Action reg: 0.003953
  l1.weight: grad_norm = 0.329424
  l1.bias: grad_norm = 0.001463
  l2.weight: grad_norm = 0.251761
Total gradient norm: 0.723573
=== Actor Training Debug (Iteration 9522) ===
Q mean: -14.233271
Q std: 20.092459
Actor loss: 14.237229
Action reg: 0.003959
  l1.weight: grad_norm = 0.282003
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.210958
Total gradient norm: 0.545511
=== Actor Training Debug (Iteration 9523) ===
Q mean: -15.526466
Q std: 21.903585
Actor loss: 15.530408
Action reg: 0.003942
  l1.weight: grad_norm = 0.344433
  l1.bias: grad_norm = 0.003624
  l2.weight: grad_norm = 0.256003
Total gradient norm: 0.637557
=== Actor Training Debug (Iteration 9524) ===
Q mean: -13.142841
Q std: 19.330660
Actor loss: 13.146796
Action reg: 0.003955
  l1.weight: grad_norm = 0.267512
  l1.bias: grad_norm = 0.007179
  l2.weight: grad_norm = 0.203986
Total gradient norm: 0.607708
=== Actor Training Debug (Iteration 9525) ===
Q mean: -14.758353
Q std: 21.205885
Actor loss: 14.762319
Action reg: 0.003965
  l1.weight: grad_norm = 0.406644
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.360038
Total gradient norm: 0.996607
=== Actor Training Debug (Iteration 9526) ===
Q mean: -15.770515
Q std: 21.951588
Actor loss: 15.774476
Action reg: 0.003961
  l1.weight: grad_norm = 0.305871
  l1.bias: grad_norm = 0.001196
  l2.weight: grad_norm = 0.201032
Total gradient norm: 0.503832
=== Actor Training Debug (Iteration 9527) ===
Q mean: -14.786964
Q std: 21.347164
Actor loss: 14.790912
Action reg: 0.003948
  l1.weight: grad_norm = 0.380177
  l1.bias: grad_norm = 0.001081
  l2.weight: grad_norm = 0.242108
Total gradient norm: 0.697245
=== Actor Training Debug (Iteration 9528) ===
Q mean: -18.167711
Q std: 24.590298
Actor loss: 18.171669
Action reg: 0.003959
  l1.weight: grad_norm = 0.402890
  l1.bias: grad_norm = 0.002818
  l2.weight: grad_norm = 0.281249
Total gradient norm: 0.763491
=== Actor Training Debug (Iteration 9529) ===
Q mean: -16.108564
Q std: 21.512920
Actor loss: 16.112535
Action reg: 0.003971
  l1.weight: grad_norm = 0.203848
  l1.bias: grad_norm = 0.001597
  l2.weight: grad_norm = 0.157368
Total gradient norm: 0.419260
=== Actor Training Debug (Iteration 9530) ===
Q mean: -16.805950
Q std: 23.260530
Actor loss: 16.809914
Action reg: 0.003964
  l1.weight: grad_norm = 0.275431
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.186793
Total gradient norm: 0.473342
=== Actor Training Debug (Iteration 9531) ===
Q mean: -15.765071
Q std: 21.291567
Actor loss: 15.769021
Action reg: 0.003950
  l1.weight: grad_norm = 0.299686
  l1.bias: grad_norm = 0.002955
  l2.weight: grad_norm = 0.257636
Total gradient norm: 0.745364
=== Actor Training Debug (Iteration 9532) ===
Q mean: -14.185415
Q std: 20.502918
Actor loss: 14.189381
Action reg: 0.003965
  l1.weight: grad_norm = 0.171283
  l1.bias: grad_norm = 0.002554
  l2.weight: grad_norm = 0.135470
Total gradient norm: 0.353279
=== Actor Training Debug (Iteration 9533) ===
Q mean: -15.760104
Q std: 21.858734
Actor loss: 15.764067
Action reg: 0.003963
  l1.weight: grad_norm = 0.286130
  l1.bias: grad_norm = 0.001027
  l2.weight: grad_norm = 0.242745
Total gradient norm: 0.723465
=== Actor Training Debug (Iteration 9534) ===
Q mean: -16.407974
Q std: 23.367670
Actor loss: 16.411930
Action reg: 0.003957
  l1.weight: grad_norm = 0.300813
  l1.bias: grad_norm = 0.004504
  l2.weight: grad_norm = 0.208225
Total gradient norm: 0.585000
=== Actor Training Debug (Iteration 9535) ===
Q mean: -16.767780
Q std: 22.622221
Actor loss: 16.771725
Action reg: 0.003945
  l1.weight: grad_norm = 0.219974
  l1.bias: grad_norm = 0.005081
  l2.weight: grad_norm = 0.171314
Total gradient norm: 0.478768
=== Actor Training Debug (Iteration 9536) ===
Q mean: -14.730423
Q std: 19.650602
Actor loss: 14.734385
Action reg: 0.003962
  l1.weight: grad_norm = 0.234884
  l1.bias: grad_norm = 0.001455
  l2.weight: grad_norm = 0.181920
Total gradient norm: 0.582638
=== Actor Training Debug (Iteration 9537) ===
Q mean: -14.657717
Q std: 19.889000
Actor loss: 14.661684
Action reg: 0.003967
  l1.weight: grad_norm = 0.139773
  l1.bias: grad_norm = 0.000807
  l2.weight: grad_norm = 0.118549
Total gradient norm: 0.346672
=== Actor Training Debug (Iteration 9538) ===
Q mean: -14.893959
Q std: 22.390993
Actor loss: 14.897914
Action reg: 0.003955
  l1.weight: grad_norm = 0.401610
  l1.bias: grad_norm = 0.002087
  l2.weight: grad_norm = 0.256196
Total gradient norm: 0.675962
=== Actor Training Debug (Iteration 9539) ===
Q mean: -13.094316
Q std: 17.721802
Actor loss: 13.098275
Action reg: 0.003959
  l1.weight: grad_norm = 0.235205
  l1.bias: grad_norm = 0.001730
  l2.weight: grad_norm = 0.166371
Total gradient norm: 0.462466
=== Actor Training Debug (Iteration 9540) ===
Q mean: -12.806047
Q std: 17.913803
Actor loss: 12.809998
Action reg: 0.003950
  l1.weight: grad_norm = 0.204758
  l1.bias: grad_norm = 0.004574
  l2.weight: grad_norm = 0.153120
Total gradient norm: 0.453579
=== Actor Training Debug (Iteration 9541) ===
Q mean: -14.459591
Q std: 21.090803
Actor loss: 14.463555
Action reg: 0.003965
  l1.weight: grad_norm = 0.407412
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.299371
Total gradient norm: 0.783639
=== Actor Training Debug (Iteration 9542) ===
Q mean: -13.659163
Q std: 19.672396
Actor loss: 13.663114
Action reg: 0.003951
  l1.weight: grad_norm = 0.236981
  l1.bias: grad_norm = 0.001351
  l2.weight: grad_norm = 0.173486
Total gradient norm: 0.440916
=== Actor Training Debug (Iteration 9543) ===
Q mean: -15.017675
Q std: 21.210226
Actor loss: 15.021621
Action reg: 0.003946
  l1.weight: grad_norm = 0.216121
  l1.bias: grad_norm = 0.004935
  l2.weight: grad_norm = 0.172637
Total gradient norm: 0.435144
=== Actor Training Debug (Iteration 9544) ===
Q mean: -15.105618
Q std: 19.370718
Actor loss: 15.109582
Action reg: 0.003964
  l1.weight: grad_norm = 0.102643
  l1.bias: grad_norm = 0.000970
  l2.weight: grad_norm = 0.063078
Total gradient norm: 0.166906
=== Actor Training Debug (Iteration 9545) ===
Q mean: -14.507797
Q std: 21.173391
Actor loss: 14.511755
Action reg: 0.003957
  l1.weight: grad_norm = 0.220891
  l1.bias: grad_norm = 0.003890
  l2.weight: grad_norm = 0.179827
Total gradient norm: 0.466025
=== Actor Training Debug (Iteration 9546) ===
Q mean: -13.852520
Q std: 20.399054
Actor loss: 13.856488
Action reg: 0.003969
  l1.weight: grad_norm = 0.336638
  l1.bias: grad_norm = 0.002466
  l2.weight: grad_norm = 0.225692
Total gradient norm: 0.568310
=== Actor Training Debug (Iteration 9547) ===
Q mean: -15.845719
Q std: 22.743160
Actor loss: 15.849675
Action reg: 0.003956
  l1.weight: grad_norm = 0.461568
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.351486
Total gradient norm: 0.892236
=== Actor Training Debug (Iteration 9548) ===
Q mean: -15.325670
Q std: 20.020569
Actor loss: 15.329634
Action reg: 0.003963
  l1.weight: grad_norm = 0.333407
  l1.bias: grad_norm = 0.001462
  l2.weight: grad_norm = 0.280442
Total gradient norm: 0.753889
=== Actor Training Debug (Iteration 9549) ===
Q mean: -15.948858
Q std: 21.093969
Actor loss: 15.952830
Action reg: 0.003972
  l1.weight: grad_norm = 0.185468
  l1.bias: grad_norm = 0.002729
  l2.weight: grad_norm = 0.145131
Total gradient norm: 0.398714
=== Actor Training Debug (Iteration 9550) ===
Q mean: -15.285637
Q std: 20.648964
Actor loss: 15.289610
Action reg: 0.003973
  l1.weight: grad_norm = 0.264343
  l1.bias: grad_norm = 0.001604
  l2.weight: grad_norm = 0.199400
Total gradient norm: 0.503815
=== Actor Training Debug (Iteration 9551) ===
Q mean: -14.304834
Q std: 21.313505
Actor loss: 14.308772
Action reg: 0.003938
  l1.weight: grad_norm = 0.213127
  l1.bias: grad_norm = 0.004388
  l2.weight: grad_norm = 0.150235
Total gradient norm: 0.430761
=== Actor Training Debug (Iteration 9552) ===
Q mean: -13.522444
Q std: 19.083416
Actor loss: 13.526402
Action reg: 0.003959
  l1.weight: grad_norm = 0.483661
  l1.bias: grad_norm = 0.000734
  l2.weight: grad_norm = 0.436882
Total gradient norm: 1.395059
=== Actor Training Debug (Iteration 9553) ===
Q mean: -14.523388
Q std: 21.504932
Actor loss: 14.527341
Action reg: 0.003953
  l1.weight: grad_norm = 0.329809
  l1.bias: grad_norm = 0.004874
  l2.weight: grad_norm = 0.227703
Total gradient norm: 0.630354
=== Actor Training Debug (Iteration 9554) ===
Q mean: -15.757606
Q std: 21.360073
Actor loss: 15.761586
Action reg: 0.003980
  l1.weight: grad_norm = 0.255978
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.209940
Total gradient norm: 0.580877
=== Actor Training Debug (Iteration 9555) ===
Q mean: -15.095209
Q std: 20.781929
Actor loss: 15.099165
Action reg: 0.003956
  l1.weight: grad_norm = 0.402403
  l1.bias: grad_norm = 0.002410
  l2.weight: grad_norm = 0.339236
Total gradient norm: 0.986569
=== Actor Training Debug (Iteration 9556) ===
Q mean: -15.038656
Q std: 20.813744
Actor loss: 15.042621
Action reg: 0.003965
  l1.weight: grad_norm = 0.161627
  l1.bias: grad_norm = 0.001246
  l2.weight: grad_norm = 0.127363
Total gradient norm: 0.311680
=== Actor Training Debug (Iteration 9557) ===
Q mean: -13.459061
Q std: 19.220304
Actor loss: 13.463033
Action reg: 0.003972
  l1.weight: grad_norm = 0.412506
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.352241
Total gradient norm: 0.936790
=== Actor Training Debug (Iteration 9558) ===
Q mean: -14.199537
Q std: 21.037088
Actor loss: 14.203485
Action reg: 0.003948
  l1.weight: grad_norm = 0.279246
  l1.bias: grad_norm = 0.001616
  l2.weight: grad_norm = 0.234267
Total gradient norm: 0.545973
=== Actor Training Debug (Iteration 9559) ===
Q mean: -15.161430
Q std: 20.755760
Actor loss: 15.165381
Action reg: 0.003952
  l1.weight: grad_norm = 0.335538
  l1.bias: grad_norm = 0.003053
  l2.weight: grad_norm = 0.266286
Total gradient norm: 0.701587
=== Actor Training Debug (Iteration 9560) ===
Q mean: -16.905489
Q std: 23.145227
Actor loss: 16.909441
Action reg: 0.003953
  l1.weight: grad_norm = 0.246389
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.194451
Total gradient norm: 0.496140
=== Actor Training Debug (Iteration 9561) ===
Q mean: -15.236602
Q std: 20.902134
Actor loss: 15.240582
Action reg: 0.003981
  l1.weight: grad_norm = 0.230123
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.165110
Total gradient norm: 0.451043
=== Actor Training Debug (Iteration 9562) ===
Q mean: -15.648874
Q std: 21.920198
Actor loss: 15.652837
Action reg: 0.003962
  l1.weight: grad_norm = 0.301219
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.229835
Total gradient norm: 0.644042
=== Actor Training Debug (Iteration 9563) ===
Q mean: -15.456602
Q std: 22.982546
Actor loss: 15.460570
Action reg: 0.003968
  l1.weight: grad_norm = 0.669377
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.525834
Total gradient norm: 1.302408
=== Actor Training Debug (Iteration 9564) ===
Q mean: -15.748487
Q std: 21.951374
Actor loss: 15.752462
Action reg: 0.003974
  l1.weight: grad_norm = 0.136098
  l1.bias: grad_norm = 0.001310
  l2.weight: grad_norm = 0.097649
Total gradient norm: 0.260255
=== Actor Training Debug (Iteration 9565) ===
Q mean: -15.151030
Q std: 21.633183
Actor loss: 15.154989
Action reg: 0.003959
  l1.weight: grad_norm = 0.258762
  l1.bias: grad_norm = 0.001365
  l2.weight: grad_norm = 0.177612
Total gradient norm: 0.517329
=== Actor Training Debug (Iteration 9566) ===
Q mean: -16.651865
Q std: 22.353046
Actor loss: 16.655827
Action reg: 0.003962
  l1.weight: grad_norm = 0.188061
  l1.bias: grad_norm = 0.001213
  l2.weight: grad_norm = 0.116079
Total gradient norm: 0.291911
=== Actor Training Debug (Iteration 9567) ===
Q mean: -13.440153
Q std: 19.614523
Actor loss: 13.444119
Action reg: 0.003966
  l1.weight: grad_norm = 0.350586
  l1.bias: grad_norm = 0.002492
  l2.weight: grad_norm = 0.212004
Total gradient norm: 0.560131
=== Actor Training Debug (Iteration 9568) ===
Q mean: -13.936571
Q std: 19.559622
Actor loss: 13.940525
Action reg: 0.003954
  l1.weight: grad_norm = 0.265917
  l1.bias: grad_norm = 0.003927
  l2.weight: grad_norm = 0.203248
Total gradient norm: 0.542579
=== Actor Training Debug (Iteration 9569) ===
Q mean: -16.371704
Q std: 22.189205
Actor loss: 16.375654
Action reg: 0.003950
  l1.weight: grad_norm = 0.228418
  l1.bias: grad_norm = 0.001478
  l2.weight: grad_norm = 0.165001
Total gradient norm: 0.446487
=== Actor Training Debug (Iteration 9570) ===
Q mean: -14.986225
Q std: 21.281143
Actor loss: 14.990191
Action reg: 0.003966
  l1.weight: grad_norm = 0.221665
  l1.bias: grad_norm = 0.000674
  l2.weight: grad_norm = 0.168732
Total gradient norm: 0.473991
=== Actor Training Debug (Iteration 9571) ===
Q mean: -16.067095
Q std: 23.030756
Actor loss: 16.071056
Action reg: 0.003962
  l1.weight: grad_norm = 0.305280
  l1.bias: grad_norm = 0.003260
  l2.weight: grad_norm = 0.241290
Total gradient norm: 0.653221
=== Actor Training Debug (Iteration 9572) ===
Q mean: -15.700564
Q std: 21.355036
Actor loss: 15.704538
Action reg: 0.003974
  l1.weight: grad_norm = 0.281846
  l1.bias: grad_norm = 0.001667
  l2.weight: grad_norm = 0.214322
Total gradient norm: 0.606771
=== Actor Training Debug (Iteration 9573) ===
Q mean: -14.959539
Q std: 21.271547
Actor loss: 14.963501
Action reg: 0.003962
  l1.weight: grad_norm = 0.139647
  l1.bias: grad_norm = 0.001902
  l2.weight: grad_norm = 0.102885
Total gradient norm: 0.305298
=== Actor Training Debug (Iteration 9574) ===
Q mean: -14.422447
Q std: 20.811838
Actor loss: 14.426430
Action reg: 0.003983
  l1.weight: grad_norm = 0.108534
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.078560
Total gradient norm: 0.206342
=== Actor Training Debug (Iteration 9575) ===
Q mean: -16.067499
Q std: 20.966612
Actor loss: 16.071470
Action reg: 0.003972
  l1.weight: grad_norm = 0.399788
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.274033
Total gradient norm: 0.689707
=== Actor Training Debug (Iteration 9576) ===
Q mean: -13.739169
Q std: 19.907869
Actor loss: 13.743145
Action reg: 0.003976
  l1.weight: grad_norm = 0.210073
  l1.bias: grad_norm = 0.005425
  l2.weight: grad_norm = 0.167558
Total gradient norm: 0.482435
=== Actor Training Debug (Iteration 9577) ===
Q mean: -15.456161
Q std: 20.287001
Actor loss: 15.460114
Action reg: 0.003953
  l1.weight: grad_norm = 0.258851
  l1.bias: grad_norm = 0.004172
  l2.weight: grad_norm = 0.216567
Total gradient norm: 0.659521
=== Actor Training Debug (Iteration 9578) ===
Q mean: -14.247067
Q std: 21.258791
Actor loss: 14.251029
Action reg: 0.003961
  l1.weight: grad_norm = 0.331679
  l1.bias: grad_norm = 0.003167
  l2.weight: grad_norm = 0.192431
Total gradient norm: 0.542597
=== Actor Training Debug (Iteration 9579) ===
Q mean: -14.077878
Q std: 20.717932
Actor loss: 14.081845
Action reg: 0.003968
  l1.weight: grad_norm = 0.326077
  l1.bias: grad_norm = 0.001410
  l2.weight: grad_norm = 0.238269
Total gradient norm: 0.580097
=== Actor Training Debug (Iteration 9580) ===
Q mean: -15.728354
Q std: 21.716352
Actor loss: 15.732337
Action reg: 0.003983
  l1.weight: grad_norm = 0.436487
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.310967
Total gradient norm: 0.801671
=== Actor Training Debug (Iteration 9581) ===
Q mean: -12.339756
Q std: 17.197592
Actor loss: 12.343717
Action reg: 0.003960
  l1.weight: grad_norm = 0.085922
  l1.bias: grad_norm = 0.001474
  l2.weight: grad_norm = 0.072590
Total gradient norm: 0.218931
=== Actor Training Debug (Iteration 9582) ===
Q mean: -13.651091
Q std: 20.039745
Actor loss: 13.655035
Action reg: 0.003944
  l1.weight: grad_norm = 0.401485
  l1.bias: grad_norm = 0.001478
  l2.weight: grad_norm = 0.315101
Total gradient norm: 0.833279
=== Actor Training Debug (Iteration 9583) ===
Q mean: -15.362724
Q std: 20.531563
Actor loss: 15.366702
Action reg: 0.003978
  l1.weight: grad_norm = 0.330465
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.257642
Total gradient norm: 0.687913
=== Actor Training Debug (Iteration 9584) ===
Q mean: -14.738688
Q std: 21.086145
Actor loss: 14.742654
Action reg: 0.003967
  l1.weight: grad_norm = 0.436538
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.310778
Total gradient norm: 1.051109
=== Actor Training Debug (Iteration 9585) ===
Q mean: -15.797352
Q std: 22.733177
Actor loss: 15.801313
Action reg: 0.003961
  l1.weight: grad_norm = 0.268501
  l1.bias: grad_norm = 0.001842
  l2.weight: grad_norm = 0.190526
Total gradient norm: 0.503494
=== Actor Training Debug (Iteration 9586) ===
Q mean: -15.499559
Q std: 21.957302
Actor loss: 15.503521
Action reg: 0.003962
  l1.weight: grad_norm = 0.419594
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.287511
Total gradient norm: 0.797298
=== Actor Training Debug (Iteration 9587) ===
Q mean: -13.883139
Q std: 21.553562
Actor loss: 13.887101
Action reg: 0.003963
  l1.weight: grad_norm = 0.383760
  l1.bias: grad_norm = 0.001502
  l2.weight: grad_norm = 0.282153
Total gradient norm: 0.878723
=== Actor Training Debug (Iteration 9588) ===
Q mean: -15.101946
Q std: 21.094591
Actor loss: 15.105911
Action reg: 0.003965
  l1.weight: grad_norm = 0.357268
  l1.bias: grad_norm = 0.001732
  l2.weight: grad_norm = 0.256314
Total gradient norm: 0.674759
=== Actor Training Debug (Iteration 9589) ===
Q mean: -16.002832
Q std: 21.588417
Actor loss: 16.006786
Action reg: 0.003954
  l1.weight: grad_norm = 0.310507
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.232138
Total gradient norm: 0.611732
=== Actor Training Debug (Iteration 9590) ===
Q mean: -15.480944
Q std: 21.136890
Actor loss: 15.484900
Action reg: 0.003957
  l1.weight: grad_norm = 0.351802
  l1.bias: grad_norm = 0.001502
  l2.weight: grad_norm = 0.259159
Total gradient norm: 0.778856
=== Actor Training Debug (Iteration 9591) ===
Q mean: -15.060433
Q std: 21.209120
Actor loss: 15.064398
Action reg: 0.003964
  l1.weight: grad_norm = 0.365158
  l1.bias: grad_norm = 0.001301
  l2.weight: grad_norm = 0.251712
Total gradient norm: 0.664220
=== Actor Training Debug (Iteration 9592) ===
Q mean: -14.122279
Q std: 20.331480
Actor loss: 14.126252
Action reg: 0.003973
  l1.weight: grad_norm = 0.196458
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.138117
Total gradient norm: 0.344971
=== Actor Training Debug (Iteration 9593) ===
Q mean: -15.052008
Q std: 21.818745
Actor loss: 15.055958
Action reg: 0.003950
  l1.weight: grad_norm = 0.252058
  l1.bias: grad_norm = 0.005072
  l2.weight: grad_norm = 0.190615
Total gradient norm: 0.497666
=== Actor Training Debug (Iteration 9594) ===
Q mean: -16.471634
Q std: 22.207720
Actor loss: 16.475571
Action reg: 0.003937
  l1.weight: grad_norm = 0.305596
  l1.bias: grad_norm = 0.001034
  l2.weight: grad_norm = 0.211778
Total gradient norm: 0.553875
=== Actor Training Debug (Iteration 9595) ===
Q mean: -12.246572
Q std: 18.196550
Actor loss: 12.250519
Action reg: 0.003946
  l1.weight: grad_norm = 0.349212
  l1.bias: grad_norm = 0.002353
  l2.weight: grad_norm = 0.255867
Total gradient norm: 0.638152
=== Actor Training Debug (Iteration 9596) ===
Q mean: -12.387555
Q std: 19.319424
Actor loss: 12.391505
Action reg: 0.003950
  l1.weight: grad_norm = 0.211600
  l1.bias: grad_norm = 0.008559
  l2.weight: grad_norm = 0.182050
Total gradient norm: 0.587899
=== Actor Training Debug (Iteration 9597) ===
Q mean: -13.780266
Q std: 19.840620
Actor loss: 13.784232
Action reg: 0.003967
  l1.weight: grad_norm = 0.222082
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.145398
Total gradient norm: 0.405487
=== Actor Training Debug (Iteration 9598) ===
Q mean: -15.724309
Q std: 21.288649
Actor loss: 15.728271
Action reg: 0.003962
  l1.weight: grad_norm = 0.295742
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.193539
Total gradient norm: 0.498877
=== Actor Training Debug (Iteration 9599) ===
Q mean: -15.045801
Q std: 21.528360
Actor loss: 15.049765
Action reg: 0.003964
  l1.weight: grad_norm = 0.272353
  l1.bias: grad_norm = 0.001935
  l2.weight: grad_norm = 0.205619
Total gradient norm: 0.534103
=== Actor Training Debug (Iteration 9600) ===
Q mean: -16.010639
Q std: 21.337610
Actor loss: 16.014624
Action reg: 0.003984
  l1.weight: grad_norm = 0.426058
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.264701
Total gradient norm: 0.763635
=== Actor Training Debug (Iteration 9601) ===
Q mean: -14.581627
Q std: 20.792046
Actor loss: 14.585584
Action reg: 0.003957
  l1.weight: grad_norm = 0.369537
  l1.bias: grad_norm = 0.001237
  l2.weight: grad_norm = 0.256866
Total gradient norm: 0.689963
=== Actor Training Debug (Iteration 9602) ===
Q mean: -13.902803
Q std: 19.819519
Actor loss: 13.906758
Action reg: 0.003955
  l1.weight: grad_norm = 0.315833
  l1.bias: grad_norm = 0.001193
  l2.weight: grad_norm = 0.237439
Total gradient norm: 0.616709
=== Actor Training Debug (Iteration 9603) ===
Q mean: -13.589476
Q std: 19.909044
Actor loss: 13.593441
Action reg: 0.003965
  l1.weight: grad_norm = 0.330592
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.235283
Total gradient norm: 0.649816
=== Actor Training Debug (Iteration 9604) ===
Q mean: -15.702610
Q std: 21.407728
Actor loss: 15.706559
Action reg: 0.003949
  l1.weight: grad_norm = 0.557706
  l1.bias: grad_norm = 0.003804
  l2.weight: grad_norm = 0.439903
Total gradient norm: 1.143257
=== Actor Training Debug (Iteration 9605) ===
Q mean: -13.277817
Q std: 18.343615
Actor loss: 13.281780
Action reg: 0.003963
  l1.weight: grad_norm = 0.236351
  l1.bias: grad_norm = 0.001906
  l2.weight: grad_norm = 0.169496
Total gradient norm: 0.502684
=== Actor Training Debug (Iteration 9606) ===
Q mean: -14.460011
Q std: 20.764868
Actor loss: 14.463978
Action reg: 0.003968
  l1.weight: grad_norm = 0.287670
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.210534
Total gradient norm: 0.572977
=== Actor Training Debug (Iteration 9607) ===
Q mean: -15.109850
Q std: 22.301535
Actor loss: 15.113809
Action reg: 0.003958
  l1.weight: grad_norm = 0.162102
  l1.bias: grad_norm = 0.002999
  l2.weight: grad_norm = 0.125657
Total gradient norm: 0.342720
=== Actor Training Debug (Iteration 9608) ===
Q mean: -14.979620
Q std: 21.363079
Actor loss: 14.983588
Action reg: 0.003968
  l1.weight: grad_norm = 0.441078
  l1.bias: grad_norm = 0.002518
  l2.weight: grad_norm = 0.363573
Total gradient norm: 0.925619
=== Actor Training Debug (Iteration 9609) ===
Q mean: -14.925722
Q std: 20.540096
Actor loss: 14.929693
Action reg: 0.003972
  l1.weight: grad_norm = 0.186433
  l1.bias: grad_norm = 0.001938
  l2.weight: grad_norm = 0.134131
Total gradient norm: 0.381043
=== Actor Training Debug (Iteration 9610) ===
Q mean: -14.162131
Q std: 19.889181
Actor loss: 14.166088
Action reg: 0.003957
  l1.weight: grad_norm = 0.249613
  l1.bias: grad_norm = 0.002998
  l2.weight: grad_norm = 0.218444
Total gradient norm: 0.596180
=== Actor Training Debug (Iteration 9611) ===
Q mean: -15.094091
Q std: 20.266500
Actor loss: 15.098056
Action reg: 0.003964
  l1.weight: grad_norm = 0.255783
  l1.bias: grad_norm = 0.001650
  l2.weight: grad_norm = 0.185563
Total gradient norm: 0.480051
=== Actor Training Debug (Iteration 9612) ===
Q mean: -15.137623
Q std: 21.774542
Actor loss: 15.141603
Action reg: 0.003980
  l1.weight: grad_norm = 0.228464
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.154496
Total gradient norm: 0.398916
=== Actor Training Debug (Iteration 9613) ===
Q mean: -15.101171
Q std: 21.064976
Actor loss: 15.105138
Action reg: 0.003968
  l1.weight: grad_norm = 0.540881
  l1.bias: grad_norm = 0.001128
  l2.weight: grad_norm = 0.335158
Total gradient norm: 1.013845
=== Actor Training Debug (Iteration 9614) ===
Q mean: -15.203472
Q std: 19.856209
Actor loss: 15.207431
Action reg: 0.003959
  l1.weight: grad_norm = 0.209174
  l1.bias: grad_norm = 0.008838
  l2.weight: grad_norm = 0.161528
Total gradient norm: 0.536545
=== Actor Training Debug (Iteration 9615) ===
Q mean: -16.795910
Q std: 23.580721
Actor loss: 16.799866
Action reg: 0.003956
  l1.weight: grad_norm = 0.236800
  l1.bias: grad_norm = 0.003734
  l2.weight: grad_norm = 0.178479
Total gradient norm: 0.485406
=== Actor Training Debug (Iteration 9616) ===
Q mean: -15.282533
Q std: 21.944078
Actor loss: 15.286493
Action reg: 0.003961
  l1.weight: grad_norm = 0.209792
  l1.bias: grad_norm = 0.001274
  l2.weight: grad_norm = 0.135745
Total gradient norm: 0.397669
=== Actor Training Debug (Iteration 9617) ===
Q mean: -14.802472
Q std: 21.529013
Actor loss: 14.806433
Action reg: 0.003961
  l1.weight: grad_norm = 0.309052
  l1.bias: grad_norm = 0.003007
  l2.weight: grad_norm = 0.249818
Total gradient norm: 0.652810
=== Actor Training Debug (Iteration 9618) ===
Q mean: -15.627109
Q std: 20.984341
Actor loss: 15.631083
Action reg: 0.003974
  l1.weight: grad_norm = 0.309229
  l1.bias: grad_norm = 0.000616
  l2.weight: grad_norm = 0.185179
Total gradient norm: 0.496848
=== Actor Training Debug (Iteration 9619) ===
Q mean: -15.417185
Q std: 21.005499
Actor loss: 15.421141
Action reg: 0.003956
  l1.weight: grad_norm = 0.359760
  l1.bias: grad_norm = 0.001505
  l2.weight: grad_norm = 0.272916
Total gradient norm: 0.727820
=== Actor Training Debug (Iteration 9620) ===
Q mean: -14.432678
Q std: 21.263184
Actor loss: 14.436633
Action reg: 0.003954
  l1.weight: grad_norm = 0.270649
  l1.bias: grad_norm = 0.001465
  l2.weight: grad_norm = 0.191681
Total gradient norm: 0.537081
=== Actor Training Debug (Iteration 9621) ===
Q mean: -13.257588
Q std: 19.040190
Actor loss: 13.261555
Action reg: 0.003966
  l1.weight: grad_norm = 0.376050
  l1.bias: grad_norm = 0.001316
  l2.weight: grad_norm = 0.254033
Total gradient norm: 0.673325
=== Actor Training Debug (Iteration 9622) ===
Q mean: -12.856054
Q std: 19.798887
Actor loss: 12.860023
Action reg: 0.003968
  l1.weight: grad_norm = 0.286401
  l1.bias: grad_norm = 0.001644
  l2.weight: grad_norm = 0.173937
Total gradient norm: 0.443599
=== Actor Training Debug (Iteration 9623) ===
Q mean: -13.899898
Q std: 20.298044
Actor loss: 13.903862
Action reg: 0.003965
  l1.weight: grad_norm = 0.313465
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.226499
Total gradient norm: 0.659426
=== Actor Training Debug (Iteration 9624) ===
Q mean: -15.393147
Q std: 20.493269
Actor loss: 15.397119
Action reg: 0.003972
  l1.weight: grad_norm = 0.140833
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.097054
Total gradient norm: 0.286261
=== Actor Training Debug (Iteration 9625) ===
Q mean: -14.463881
Q std: 20.522636
Actor loss: 14.467850
Action reg: 0.003969
  l1.weight: grad_norm = 0.334210
  l1.bias: grad_norm = 0.001080
  l2.weight: grad_norm = 0.280947
Total gradient norm: 0.709966
=== Actor Training Debug (Iteration 9626) ===
Q mean: -12.727631
Q std: 18.735346
Actor loss: 12.731584
Action reg: 0.003953
  l1.weight: grad_norm = 0.310625
  l1.bias: grad_norm = 0.003335
  l2.weight: grad_norm = 0.235624
Total gradient norm: 0.595475
=== Actor Training Debug (Iteration 9627) ===
Q mean: -15.486762
Q std: 20.490887
Actor loss: 15.490738
Action reg: 0.003975
  l1.weight: grad_norm = 0.359822
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.248356
Total gradient norm: 0.639571
=== Actor Training Debug (Iteration 9628) ===
Q mean: -14.373873
Q std: 20.785473
Actor loss: 14.377846
Action reg: 0.003973
  l1.weight: grad_norm = 0.176087
  l1.bias: grad_norm = 0.001043
  l2.weight: grad_norm = 0.139328
Total gradient norm: 0.393855
=== Actor Training Debug (Iteration 9629) ===
Q mean: -12.880800
Q std: 18.041597
Actor loss: 12.884756
Action reg: 0.003956
  l1.weight: grad_norm = 0.261281
  l1.bias: grad_norm = 0.001065
  l2.weight: grad_norm = 0.192929
Total gradient norm: 0.498218
=== Actor Training Debug (Iteration 9630) ===
Q mean: -14.315472
Q std: 18.246588
Actor loss: 14.319454
Action reg: 0.003983
  l1.weight: grad_norm = 0.236145
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.203133
Total gradient norm: 0.596248
=== Actor Training Debug (Iteration 9631) ===
Q mean: -16.304279
Q std: 22.126219
Actor loss: 16.308247
Action reg: 0.003967
  l1.weight: grad_norm = 0.130925
  l1.bias: grad_norm = 0.001712
  l2.weight: grad_norm = 0.105086
Total gradient norm: 0.271939
=== Actor Training Debug (Iteration 9632) ===
Q mean: -15.208038
Q std: 21.789959
Actor loss: 15.212006
Action reg: 0.003968
  l1.weight: grad_norm = 0.314617
  l1.bias: grad_norm = 0.000989
  l2.weight: grad_norm = 0.230084
Total gradient norm: 0.635901
=== Actor Training Debug (Iteration 9633) ===
Q mean: -14.743036
Q std: 21.076078
Actor loss: 14.746994
Action reg: 0.003958
  l1.weight: grad_norm = 0.404533
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.268891
Total gradient norm: 0.717632
=== Actor Training Debug (Iteration 9634) ===
Q mean: -17.888000
Q std: 23.504009
Actor loss: 17.891977
Action reg: 0.003977
  l1.weight: grad_norm = 0.336110
  l1.bias: grad_norm = 0.001461
  l2.weight: grad_norm = 0.221613
Total gradient norm: 0.664721
=== Actor Training Debug (Iteration 9635) ===
Q mean: -15.896762
Q std: 21.232662
Actor loss: 15.900728
Action reg: 0.003967
  l1.weight: grad_norm = 0.198449
  l1.bias: grad_norm = 0.003188
  l2.weight: grad_norm = 0.158736
Total gradient norm: 0.425929
=== Actor Training Debug (Iteration 9636) ===
Q mean: -16.651215
Q std: 22.345930
Actor loss: 16.655174
Action reg: 0.003960
  l1.weight: grad_norm = 0.235200
  l1.bias: grad_norm = 0.001195
  l2.weight: grad_norm = 0.168119
Total gradient norm: 0.425314
=== Actor Training Debug (Iteration 9637) ===
Q mean: -14.393345
Q std: 19.350407
Actor loss: 14.397310
Action reg: 0.003965
  l1.weight: grad_norm = 0.239738
  l1.bias: grad_norm = 0.003060
  l2.weight: grad_norm = 0.154452
Total gradient norm: 0.441784
=== Actor Training Debug (Iteration 9638) ===
Q mean: -14.691919
Q std: 20.835793
Actor loss: 14.695864
Action reg: 0.003944
  l1.weight: grad_norm = 0.330329
  l1.bias: grad_norm = 0.002192
  l2.weight: grad_norm = 0.220232
Total gradient norm: 0.601035
=== Actor Training Debug (Iteration 9639) ===
Q mean: -17.464901
Q std: 21.492109
Actor loss: 17.468870
Action reg: 0.003969
  l1.weight: grad_norm = 0.411726
  l1.bias: grad_norm = 0.000595
  l2.weight: grad_norm = 0.275851
Total gradient norm: 0.722402
=== Actor Training Debug (Iteration 9640) ===
Q mean: -17.233913
Q std: 21.605331
Actor loss: 17.237896
Action reg: 0.003982
  l1.weight: grad_norm = 0.064249
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.053009
Total gradient norm: 0.147415
=== Actor Training Debug (Iteration 9641) ===
Q mean: -17.269651
Q std: 22.964901
Actor loss: 17.273621
Action reg: 0.003970
  l1.weight: grad_norm = 0.261430
  l1.bias: grad_norm = 0.001191
  l2.weight: grad_norm = 0.203251
Total gradient norm: 0.534210
=== Actor Training Debug (Iteration 9642) ===
Q mean: -13.308697
Q std: 18.254564
Actor loss: 13.312654
Action reg: 0.003957
  l1.weight: grad_norm = 0.297192
  l1.bias: grad_norm = 0.001103
  l2.weight: grad_norm = 0.213914
Total gradient norm: 0.568395
=== Actor Training Debug (Iteration 9643) ===
Q mean: -15.638753
Q std: 21.180044
Actor loss: 15.642725
Action reg: 0.003972
  l1.weight: grad_norm = 0.193845
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.139139
Total gradient norm: 0.365720
=== Actor Training Debug (Iteration 9644) ===
Q mean: -14.718845
Q std: 19.805418
Actor loss: 14.722800
Action reg: 0.003954
  l1.weight: grad_norm = 0.496379
  l1.bias: grad_norm = 0.001174
  l2.weight: grad_norm = 0.387566
Total gradient norm: 0.937371
=== Actor Training Debug (Iteration 9645) ===
Q mean: -15.929682
Q std: 21.355436
Actor loss: 15.933650
Action reg: 0.003968
  l1.weight: grad_norm = 0.172226
  l1.bias: grad_norm = 0.001853
  l2.weight: grad_norm = 0.125509
Total gradient norm: 0.360642
=== Actor Training Debug (Iteration 9646) ===
Q mean: -15.245447
Q std: 19.938431
Actor loss: 15.249411
Action reg: 0.003964
  l1.weight: grad_norm = 0.147310
  l1.bias: grad_norm = 0.000924
  l2.weight: grad_norm = 0.117354
Total gradient norm: 0.339239
=== Actor Training Debug (Iteration 9647) ===
Q mean: -17.352606
Q std: 22.656614
Actor loss: 17.356571
Action reg: 0.003965
  l1.weight: grad_norm = 0.220074
  l1.bias: grad_norm = 0.001718
  l2.weight: grad_norm = 0.171740
Total gradient norm: 0.464406
=== Actor Training Debug (Iteration 9648) ===
Q mean: -16.178577
Q std: 22.867523
Actor loss: 16.182550
Action reg: 0.003974
  l1.weight: grad_norm = 0.398155
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.278535
Total gradient norm: 0.749021
=== Actor Training Debug (Iteration 9649) ===
Q mean: -15.495771
Q std: 21.775217
Actor loss: 15.499745
Action reg: 0.003974
  l1.weight: grad_norm = 0.127287
  l1.bias: grad_norm = 0.002234
  l2.weight: grad_norm = 0.082537
Total gradient norm: 0.236630
=== Actor Training Debug (Iteration 9650) ===
Q mean: -15.277387
Q std: 21.883568
Actor loss: 15.281352
Action reg: 0.003965
  l1.weight: grad_norm = 0.222255
  l1.bias: grad_norm = 0.002675
  l2.weight: grad_norm = 0.175107
Total gradient norm: 0.461041
=== Actor Training Debug (Iteration 9651) ===
Q mean: -15.301569
Q std: 20.648657
Actor loss: 15.305543
Action reg: 0.003974
  l1.weight: grad_norm = 0.316819
  l1.bias: grad_norm = 0.001298
  l2.weight: grad_norm = 0.242933
Total gradient norm: 0.605306
=== Actor Training Debug (Iteration 9652) ===
Q mean: -13.225149
Q std: 19.921301
Actor loss: 13.229094
Action reg: 0.003944
  l1.weight: grad_norm = 0.219615
  l1.bias: grad_norm = 0.003924
  l2.weight: grad_norm = 0.157520
Total gradient norm: 0.405145
=== Actor Training Debug (Iteration 9653) ===
Q mean: -14.019341
Q std: 20.083544
Actor loss: 14.023302
Action reg: 0.003962
  l1.weight: grad_norm = 0.275589
  l1.bias: grad_norm = 0.002266
  l2.weight: grad_norm = 0.215156
Total gradient norm: 0.616019
=== Actor Training Debug (Iteration 9654) ===
Q mean: -14.254225
Q std: 20.035877
Actor loss: 14.258199
Action reg: 0.003974
  l1.weight: grad_norm = 0.382407
  l1.bias: grad_norm = 0.001189
  l2.weight: grad_norm = 0.252781
Total gradient norm: 0.681587
=== Actor Training Debug (Iteration 9655) ===
Q mean: -16.352848
Q std: 22.451426
Actor loss: 16.356808
Action reg: 0.003960
  l1.weight: grad_norm = 0.170299
  l1.bias: grad_norm = 0.001643
  l2.weight: grad_norm = 0.112243
Total gradient norm: 0.284434
=== Actor Training Debug (Iteration 9656) ===
Q mean: -14.124300
Q std: 20.053911
Actor loss: 14.128274
Action reg: 0.003974
  l1.weight: grad_norm = 0.118239
  l1.bias: grad_norm = 0.001047
  l2.weight: grad_norm = 0.089969
Total gradient norm: 0.242498
=== Actor Training Debug (Iteration 9657) ===
Q mean: -14.968603
Q std: 21.595566
Actor loss: 14.972575
Action reg: 0.003972
  l1.weight: grad_norm = 0.393245
  l1.bias: grad_norm = 0.001414
  l2.weight: grad_norm = 0.255755
Total gradient norm: 0.654890
=== Actor Training Debug (Iteration 9658) ===
Q mean: -14.700136
Q std: 20.985533
Actor loss: 14.704105
Action reg: 0.003969
  l1.weight: grad_norm = 0.260516
  l1.bias: grad_norm = 0.000730
  l2.weight: grad_norm = 0.202440
Total gradient norm: 0.554071
=== Actor Training Debug (Iteration 9659) ===
Q mean: -14.645910
Q std: 20.186163
Actor loss: 14.649875
Action reg: 0.003964
  l1.weight: grad_norm = 0.256093
  l1.bias: grad_norm = 0.001354
  l2.weight: grad_norm = 0.181065
Total gradient norm: 0.471422
=== Actor Training Debug (Iteration 9660) ===
Q mean: -15.191464
Q std: 20.288998
Actor loss: 15.195437
Action reg: 0.003973
  l1.weight: grad_norm = 0.195307
  l1.bias: grad_norm = 0.000649
  l2.weight: grad_norm = 0.185030
Total gradient norm: 0.482106
=== Actor Training Debug (Iteration 9661) ===
Q mean: -15.029420
Q std: 20.180286
Actor loss: 15.033389
Action reg: 0.003970
  l1.weight: grad_norm = 0.190500
  l1.bias: grad_norm = 0.003582
  l2.weight: grad_norm = 0.157652
Total gradient norm: 0.437202
=== Actor Training Debug (Iteration 9662) ===
Q mean: -13.493605
Q std: 20.822359
Actor loss: 13.497557
Action reg: 0.003952
  l1.weight: grad_norm = 0.305168
  l1.bias: grad_norm = 0.003160
  l2.weight: grad_norm = 0.225414
Total gradient norm: 0.608449
=== Actor Training Debug (Iteration 9663) ===
Q mean: -13.060191
Q std: 19.073040
Actor loss: 13.064162
Action reg: 0.003971
  l1.weight: grad_norm = 0.117044
  l1.bias: grad_norm = 0.001394
  l2.weight: grad_norm = 0.095545
Total gradient norm: 0.235597
=== Actor Training Debug (Iteration 9664) ===
Q mean: -15.649775
Q std: 20.811588
Actor loss: 15.653743
Action reg: 0.003968
  l1.weight: grad_norm = 0.204844
  l1.bias: grad_norm = 0.000859
  l2.weight: grad_norm = 0.159225
Total gradient norm: 0.419712
=== Actor Training Debug (Iteration 9665) ===
Q mean: -15.400927
Q std: 20.674362
Actor loss: 15.404902
Action reg: 0.003974
  l1.weight: grad_norm = 0.239291
  l1.bias: grad_norm = 0.001210
  l2.weight: grad_norm = 0.219753
Total gradient norm: 0.698362
=== Actor Training Debug (Iteration 9666) ===
Q mean: -13.936449
Q std: 20.377716
Actor loss: 13.940403
Action reg: 0.003953
  l1.weight: grad_norm = 0.365743
  l1.bias: grad_norm = 0.005599
  l2.weight: grad_norm = 0.260485
Total gradient norm: 0.675801
=== Actor Training Debug (Iteration 9667) ===
Q mean: -15.149670
Q std: 20.722013
Actor loss: 15.153631
Action reg: 0.003962
  l1.weight: grad_norm = 0.260266
  l1.bias: grad_norm = 0.001648
  l2.weight: grad_norm = 0.185377
Total gradient norm: 0.583077
=== Actor Training Debug (Iteration 9668) ===
Q mean: -14.016582
Q std: 18.893187
Actor loss: 14.020549
Action reg: 0.003967
  l1.weight: grad_norm = 0.377699
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.336350
Total gradient norm: 0.837882
=== Actor Training Debug (Iteration 9669) ===
Q mean: -15.383220
Q std: 22.280466
Actor loss: 15.387192
Action reg: 0.003972
  l1.weight: grad_norm = 0.242147
  l1.bias: grad_norm = 0.000942
  l2.weight: grad_norm = 0.184125
Total gradient norm: 0.510507
=== Actor Training Debug (Iteration 9670) ===
Q mean: -15.493500
Q std: 22.362640
Actor loss: 15.497473
Action reg: 0.003973
  l1.weight: grad_norm = 0.202974
  l1.bias: grad_norm = 0.000747
  l2.weight: grad_norm = 0.154902
Total gradient norm: 0.383313
=== Actor Training Debug (Iteration 9671) ===
Q mean: -14.375151
Q std: 20.712257
Actor loss: 14.379107
Action reg: 0.003956
  l1.weight: grad_norm = 0.356237
  l1.bias: grad_norm = 0.001150
  l2.weight: grad_norm = 0.276664
Total gradient norm: 0.709991
=== Actor Training Debug (Iteration 9672) ===
Q mean: -15.341407
Q std: 21.900307
Actor loss: 15.345370
Action reg: 0.003963
  l1.weight: grad_norm = 0.517386
  l1.bias: grad_norm = 0.000648
  l2.weight: grad_norm = 0.362231
Total gradient norm: 1.011523
=== Actor Training Debug (Iteration 9673) ===
Q mean: -13.263978
Q std: 18.676542
Actor loss: 13.267941
Action reg: 0.003964
  l1.weight: grad_norm = 0.250944
  l1.bias: grad_norm = 0.000998
  l2.weight: grad_norm = 0.238571
Total gradient norm: 0.624527
=== Actor Training Debug (Iteration 9674) ===
Q mean: -14.503460
Q std: 19.784584
Actor loss: 14.507433
Action reg: 0.003973
  l1.weight: grad_norm = 0.164135
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.119483
Total gradient norm: 0.314349
=== Actor Training Debug (Iteration 9675) ===
Q mean: -16.527342
Q std: 23.224304
Actor loss: 16.531296
Action reg: 0.003955
  l1.weight: grad_norm = 0.280679
  l1.bias: grad_norm = 0.001055
  l2.weight: grad_norm = 0.207333
Total gradient norm: 0.566516
=== Actor Training Debug (Iteration 9676) ===
Q mean: -14.642549
Q std: 21.427540
Actor loss: 14.646514
Action reg: 0.003965
  l1.weight: grad_norm = 0.180961
  l1.bias: grad_norm = 0.001569
  l2.weight: grad_norm = 0.142313
Total gradient norm: 0.430604
=== Actor Training Debug (Iteration 9677) ===
Q mean: -13.196463
Q std: 18.079538
Actor loss: 13.200418
Action reg: 0.003955
  l1.weight: grad_norm = 0.208322
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.182652
Total gradient norm: 0.537415
=== Actor Training Debug (Iteration 9678) ===
Q mean: -14.299817
Q std: 20.293385
Actor loss: 14.303773
Action reg: 0.003956
  l1.weight: grad_norm = 0.242055
  l1.bias: grad_norm = 0.003105
  l2.weight: grad_norm = 0.189996
Total gradient norm: 0.521892
=== Actor Training Debug (Iteration 9679) ===
Q mean: -16.236580
Q std: 22.066225
Actor loss: 16.240555
Action reg: 0.003975
  l1.weight: grad_norm = 0.239861
  l1.bias: grad_norm = 0.000795
  l2.weight: grad_norm = 0.165581
Total gradient norm: 0.455043
=== Actor Training Debug (Iteration 9680) ===
Q mean: -12.645011
Q std: 17.603931
Actor loss: 12.648973
Action reg: 0.003962
  l1.weight: grad_norm = 0.490903
  l1.bias: grad_norm = 0.001951
  l2.weight: grad_norm = 0.345955
Total gradient norm: 0.857829
=== Actor Training Debug (Iteration 9681) ===
Q mean: -17.400799
Q std: 23.501722
Actor loss: 17.404758
Action reg: 0.003960
  l1.weight: grad_norm = 0.239245
  l1.bias: grad_norm = 0.002801
  l2.weight: grad_norm = 0.183051
Total gradient norm: 0.523836
=== Actor Training Debug (Iteration 9682) ===
Q mean: -14.715977
Q std: 18.853451
Actor loss: 14.719940
Action reg: 0.003963
  l1.weight: grad_norm = 0.365447
  l1.bias: grad_norm = 0.000826
  l2.weight: grad_norm = 0.266266
Total gradient norm: 0.753595
=== Actor Training Debug (Iteration 9683) ===
Q mean: -13.978502
Q std: 20.353382
Actor loss: 13.982464
Action reg: 0.003962
  l1.weight: grad_norm = 0.375358
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.308092
Total gradient norm: 0.829060
=== Actor Training Debug (Iteration 9684) ===
Q mean: -14.798771
Q std: 20.314350
Actor loss: 14.802727
Action reg: 0.003955
  l1.weight: grad_norm = 0.399691
  l1.bias: grad_norm = 0.001138
  l2.weight: grad_norm = 0.281011
Total gradient norm: 0.779552
=== Actor Training Debug (Iteration 9685) ===
Q mean: -14.188484
Q std: 19.012705
Actor loss: 14.192440
Action reg: 0.003956
  l1.weight: grad_norm = 0.535034
  l1.bias: grad_norm = 0.003845
  l2.weight: grad_norm = 0.387608
Total gradient norm: 1.193429
=== Actor Training Debug (Iteration 9686) ===
Q mean: -15.468449
Q std: 21.724741
Actor loss: 15.472413
Action reg: 0.003964
  l1.weight: grad_norm = 0.203674
  l1.bias: grad_norm = 0.002344
  l2.weight: grad_norm = 0.186016
Total gradient norm: 0.490652
=== Actor Training Debug (Iteration 9687) ===
Q mean: -13.574889
Q std: 18.467337
Actor loss: 13.578844
Action reg: 0.003954
  l1.weight: grad_norm = 0.177589
  l1.bias: grad_norm = 0.001340
  l2.weight: grad_norm = 0.141789
Total gradient norm: 0.373972
=== Actor Training Debug (Iteration 9688) ===
Q mean: -17.221775
Q std: 23.120077
Actor loss: 17.225731
Action reg: 0.003956
  l1.weight: grad_norm = 0.196164
  l1.bias: grad_norm = 0.002539
  l2.weight: grad_norm = 0.166536
Total gradient norm: 0.458362
=== Actor Training Debug (Iteration 9689) ===
Q mean: -16.258169
Q std: 22.996424
Actor loss: 16.262133
Action reg: 0.003963
  l1.weight: grad_norm = 0.389765
  l1.bias: grad_norm = 0.000626
  l2.weight: grad_norm = 0.301282
Total gradient norm: 0.774649
=== Actor Training Debug (Iteration 9690) ===
Q mean: -15.315054
Q std: 21.639410
Actor loss: 15.319026
Action reg: 0.003972
  l1.weight: grad_norm = 0.323015
  l1.bias: grad_norm = 0.001635
  l2.weight: grad_norm = 0.250412
Total gradient norm: 0.654469
=== Actor Training Debug (Iteration 9691) ===
Q mean: -12.103615
Q std: 17.241108
Actor loss: 12.107581
Action reg: 0.003966
  l1.weight: grad_norm = 0.368806
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.263993
Total gradient norm: 0.666549
=== Actor Training Debug (Iteration 9692) ===
Q mean: -13.403315
Q std: 18.431234
Actor loss: 13.407290
Action reg: 0.003976
  l1.weight: grad_norm = 0.216591
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.195557
Total gradient norm: 0.520333
=== Actor Training Debug (Iteration 9693) ===
Q mean: -14.743944
Q std: 20.342152
Actor loss: 14.747916
Action reg: 0.003972
  l1.weight: grad_norm = 0.207734
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.169822
Total gradient norm: 0.460771
=== Actor Training Debug (Iteration 9694) ===
Q mean: -14.001236
Q std: 19.574947
Actor loss: 14.005188
Action reg: 0.003952
  l1.weight: grad_norm = 0.143939
  l1.bias: grad_norm = 0.002362
  l2.weight: grad_norm = 0.117268
Total gradient norm: 0.320416
=== Actor Training Debug (Iteration 9695) ===
Q mean: -12.637781
Q std: 19.131556
Actor loss: 12.641738
Action reg: 0.003957
  l1.weight: grad_norm = 0.260063
  l1.bias: grad_norm = 0.003132
  l2.weight: grad_norm = 0.230641
Total gradient norm: 0.633756
=== Actor Training Debug (Iteration 9696) ===
Q mean: -16.024632
Q std: 21.570585
Actor loss: 16.028606
Action reg: 0.003975
  l1.weight: grad_norm = 0.284091
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.191836
Total gradient norm: 0.523471
=== Actor Training Debug (Iteration 9697) ===
Q mean: -14.190069
Q std: 19.880764
Actor loss: 14.194032
Action reg: 0.003962
  l1.weight: grad_norm = 0.263644
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.208781
Total gradient norm: 0.549757
=== Actor Training Debug (Iteration 9698) ===
Q mean: -15.201487
Q std: 20.810438
Actor loss: 15.205444
Action reg: 0.003957
  l1.weight: grad_norm = 0.597363
  l1.bias: grad_norm = 0.000692
  l2.weight: grad_norm = 0.449545
Total gradient norm: 1.320566
=== Actor Training Debug (Iteration 9699) ===
Q mean: -13.093328
Q std: 18.792862
Actor loss: 13.097281
Action reg: 0.003952
  l1.weight: grad_norm = 0.276063
  l1.bias: grad_norm = 0.000425
  l2.weight: grad_norm = 0.196184
Total gradient norm: 0.491603
=== Actor Training Debug (Iteration 9700) ===
Q mean: -15.283369
Q std: 20.110725
Actor loss: 15.287337
Action reg: 0.003968
  l1.weight: grad_norm = 0.294550
  l1.bias: grad_norm = 0.001018
  l2.weight: grad_norm = 0.233228
Total gradient norm: 0.636326
=== Actor Training Debug (Iteration 9701) ===
Q mean: -15.034338
Q std: 20.540174
Actor loss: 15.038291
Action reg: 0.003953
  l1.weight: grad_norm = 0.343471
  l1.bias: grad_norm = 0.001562
  l2.weight: grad_norm = 0.254664
Total gradient norm: 0.736703
=== Actor Training Debug (Iteration 9702) ===
Q mean: -15.279186
Q std: 20.319056
Actor loss: 15.283160
Action reg: 0.003974
  l1.weight: grad_norm = 0.256252
  l1.bias: grad_norm = 0.000915
  l2.weight: grad_norm = 0.165433
Total gradient norm: 0.439994
=== Actor Training Debug (Iteration 9703) ===
Q mean: -14.343422
Q std: 20.699375
Actor loss: 14.347386
Action reg: 0.003965
  l1.weight: grad_norm = 0.259543
  l1.bias: grad_norm = 0.000725
  l2.weight: grad_norm = 0.211535
Total gradient norm: 0.565341
=== Actor Training Debug (Iteration 9704) ===
Q mean: -15.201065
Q std: 19.968166
Actor loss: 15.205032
Action reg: 0.003967
  l1.weight: grad_norm = 0.151354
  l1.bias: grad_norm = 0.001756
  l2.weight: grad_norm = 0.109324
Total gradient norm: 0.288494
=== Actor Training Debug (Iteration 9705) ===
Q mean: -15.636291
Q std: 20.588846
Actor loss: 15.640250
Action reg: 0.003959
  l1.weight: grad_norm = 0.291610
  l1.bias: grad_norm = 0.001220
  l2.weight: grad_norm = 0.218693
Total gradient norm: 0.544219
=== Actor Training Debug (Iteration 9706) ===
Q mean: -13.869417
Q std: 20.069626
Actor loss: 13.873380
Action reg: 0.003963
  l1.weight: grad_norm = 0.303675
  l1.bias: grad_norm = 0.007850
  l2.weight: grad_norm = 0.228210
Total gradient norm: 0.623406
=== Actor Training Debug (Iteration 9707) ===
Q mean: -14.251637
Q std: 19.308825
Actor loss: 14.255616
Action reg: 0.003980
  l1.weight: grad_norm = 0.178947
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.136377
Total gradient norm: 0.373964
=== Actor Training Debug (Iteration 9708) ===
Q mean: -14.177252
Q std: 18.147318
Actor loss: 14.181214
Action reg: 0.003963
  l1.weight: grad_norm = 0.383024
  l1.bias: grad_norm = 0.001388
  l2.weight: grad_norm = 0.304672
Total gradient norm: 0.852297
=== Actor Training Debug (Iteration 9709) ===
Q mean: -13.072714
Q std: 18.367655
Actor loss: 13.076682
Action reg: 0.003968
  l1.weight: grad_norm = 0.404343
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.284779
Total gradient norm: 0.767415
=== Actor Training Debug (Iteration 9710) ===
Q mean: -14.703678
Q std: 19.648643
Actor loss: 14.707646
Action reg: 0.003968
  l1.weight: grad_norm = 0.225831
  l1.bias: grad_norm = 0.000973
  l2.weight: grad_norm = 0.143753
Total gradient norm: 0.381827
=== Actor Training Debug (Iteration 9711) ===
Q mean: -14.043036
Q std: 19.808012
Actor loss: 14.047003
Action reg: 0.003967
  l1.weight: grad_norm = 0.427389
  l1.bias: grad_norm = 0.000714
  l2.weight: grad_norm = 0.336304
Total gradient norm: 0.852016
=== Actor Training Debug (Iteration 9712) ===
Q mean: -12.779088
Q std: 19.489349
Actor loss: 12.783053
Action reg: 0.003966
  l1.weight: grad_norm = 0.449302
  l1.bias: grad_norm = 0.000729
  l2.weight: grad_norm = 0.375392
Total gradient norm: 1.449257
=== Actor Training Debug (Iteration 9713) ===
Q mean: -13.824902
Q std: 20.126379
Actor loss: 13.828839
Action reg: 0.003937
  l1.weight: grad_norm = 0.316318
  l1.bias: grad_norm = 0.003058
  l2.weight: grad_norm = 0.254299
Total gradient norm: 0.691832
=== Actor Training Debug (Iteration 9714) ===
Q mean: -17.019020
Q std: 21.354742
Actor loss: 17.022987
Action reg: 0.003968
  l1.weight: grad_norm = 0.237908
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.179246
Total gradient norm: 0.446699
=== Actor Training Debug (Iteration 9715) ===
Q mean: -16.162888
Q std: 21.937193
Actor loss: 16.166847
Action reg: 0.003960
  l1.weight: grad_norm = 0.455846
  l1.bias: grad_norm = 0.001089
  l2.weight: grad_norm = 0.331799
Total gradient norm: 0.890950
=== Actor Training Debug (Iteration 9716) ===
Q mean: -15.021251
Q std: 20.891338
Actor loss: 15.025224
Action reg: 0.003973
  l1.weight: grad_norm = 0.304327
  l1.bias: grad_norm = 0.000882
  l2.weight: grad_norm = 0.222800
Total gradient norm: 0.534757
=== Actor Training Debug (Iteration 9717) ===
Q mean: -15.811089
Q std: 22.506603
Actor loss: 15.815061
Action reg: 0.003973
  l1.weight: grad_norm = 0.411017
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.315365
Total gradient norm: 0.722918
=== Actor Training Debug (Iteration 9718) ===
Q mean: -13.606088
Q std: 19.344999
Actor loss: 13.610063
Action reg: 0.003975
  l1.weight: grad_norm = 0.194220
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.138762
Total gradient norm: 0.372844
=== Actor Training Debug (Iteration 9719) ===
Q mean: -12.927818
Q std: 19.394838
Actor loss: 12.931779
Action reg: 0.003960
  l1.weight: grad_norm = 0.463874
  l1.bias: grad_norm = 0.001082
  l2.weight: grad_norm = 0.315009
Total gradient norm: 0.795285
=== Actor Training Debug (Iteration 9720) ===
Q mean: -14.011111
Q std: 19.311768
Actor loss: 14.015079
Action reg: 0.003967
  l1.weight: grad_norm = 0.205106
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.148129
Total gradient norm: 0.402343
=== Actor Training Debug (Iteration 9721) ===
Q mean: -14.105136
Q std: 19.805052
Actor loss: 14.109089
Action reg: 0.003953
  l1.weight: grad_norm = 0.341419
  l1.bias: grad_norm = 0.001739
  l2.weight: grad_norm = 0.274976
Total gradient norm: 0.721906
=== Actor Training Debug (Iteration 9722) ===
Q mean: -16.568783
Q std: 22.239841
Actor loss: 16.572767
Action reg: 0.003985
  l1.weight: grad_norm = 0.296513
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.197281
Total gradient norm: 0.523480
=== Actor Training Debug (Iteration 9723) ===
Q mean: -16.035528
Q std: 22.418472
Actor loss: 16.039494
Action reg: 0.003966
  l1.weight: grad_norm = 0.425960
  l1.bias: grad_norm = 0.001625
  l2.weight: grad_norm = 0.290926
Total gradient norm: 0.812842
=== Actor Training Debug (Iteration 9724) ===
Q mean: -14.559719
Q std: 21.530518
Actor loss: 14.563694
Action reg: 0.003975
  l1.weight: grad_norm = 0.219432
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.170740
Total gradient norm: 0.417821
=== Actor Training Debug (Iteration 9725) ===
Q mean: -15.146452
Q std: 20.431076
Actor loss: 15.150424
Action reg: 0.003972
  l1.weight: grad_norm = 0.329377
  l1.bias: grad_norm = 0.001459
  l2.weight: grad_norm = 0.223934
Total gradient norm: 0.567667
=== Actor Training Debug (Iteration 9726) ===
Q mean: -16.581890
Q std: 21.596003
Actor loss: 16.585861
Action reg: 0.003970
  l1.weight: grad_norm = 0.424066
  l1.bias: grad_norm = 0.001744
  l2.weight: grad_norm = 0.289934
Total gradient norm: 0.779697
=== Actor Training Debug (Iteration 9727) ===
Q mean: -17.039516
Q std: 21.683176
Actor loss: 17.043491
Action reg: 0.003975
  l1.weight: grad_norm = 0.219515
  l1.bias: grad_norm = 0.002781
  l2.weight: grad_norm = 0.193053
Total gradient norm: 0.537742
=== Actor Training Debug (Iteration 9728) ===
Q mean: -15.871950
Q std: 21.932232
Actor loss: 15.875919
Action reg: 0.003969
  l1.weight: grad_norm = 0.174249
  l1.bias: grad_norm = 0.003797
  l2.weight: grad_norm = 0.145654
Total gradient norm: 0.430122
=== Actor Training Debug (Iteration 9729) ===
Q mean: -13.425699
Q std: 19.716480
Actor loss: 13.429663
Action reg: 0.003964
  l1.weight: grad_norm = 0.265100
  l1.bias: grad_norm = 0.001375
  l2.weight: grad_norm = 0.197997
Total gradient norm: 0.539656
=== Actor Training Debug (Iteration 9730) ===
Q mean: -12.559853
Q std: 17.365541
Actor loss: 12.563824
Action reg: 0.003971
  l1.weight: grad_norm = 0.223554
  l1.bias: grad_norm = 0.001194
  l2.weight: grad_norm = 0.186040
Total gradient norm: 0.480763
=== Actor Training Debug (Iteration 9731) ===
Q mean: -15.205840
Q std: 20.781368
Actor loss: 15.209813
Action reg: 0.003973
  l1.weight: grad_norm = 0.164459
  l1.bias: grad_norm = 0.001638
  l2.weight: grad_norm = 0.113972
Total gradient norm: 0.284736
=== Actor Training Debug (Iteration 9732) ===
Q mean: -13.949784
Q std: 20.366123
Actor loss: 13.953758
Action reg: 0.003974
  l1.weight: grad_norm = 0.300255
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.228146
Total gradient norm: 0.599548
=== Actor Training Debug (Iteration 9733) ===
Q mean: -14.010711
Q std: 19.407841
Actor loss: 14.014691
Action reg: 0.003981
  l1.weight: grad_norm = 0.343054
  l1.bias: grad_norm = 0.000793
  l2.weight: grad_norm = 0.227893
Total gradient norm: 0.569883
=== Actor Training Debug (Iteration 9734) ===
Q mean: -14.609005
Q std: 19.988573
Actor loss: 14.612961
Action reg: 0.003956
  l1.weight: grad_norm = 0.252235
  l1.bias: grad_norm = 0.002100
  l2.weight: grad_norm = 0.179622
Total gradient norm: 0.530650
=== Actor Training Debug (Iteration 9735) ===
Q mean: -15.319355
Q std: 21.958242
Actor loss: 15.323313
Action reg: 0.003958
  l1.weight: grad_norm = 0.395160
  l1.bias: grad_norm = 0.001711
  l2.weight: grad_norm = 0.310647
Total gradient norm: 0.829848
=== Actor Training Debug (Iteration 9736) ===
Q mean: -14.906435
Q std: 21.158295
Actor loss: 14.910402
Action reg: 0.003967
  l1.weight: grad_norm = 0.279348
  l1.bias: grad_norm = 0.001042
  l2.weight: grad_norm = 0.196646
Total gradient norm: 0.572923
=== Actor Training Debug (Iteration 9737) ===
Q mean: -15.766891
Q std: 22.048546
Actor loss: 15.770867
Action reg: 0.003976
  l1.weight: grad_norm = 0.154623
  l1.bias: grad_norm = 0.001125
  l2.weight: grad_norm = 0.108089
Total gradient norm: 0.293303
=== Actor Training Debug (Iteration 9738) ===
Q mean: -15.474949
Q std: 22.043407
Actor loss: 15.478917
Action reg: 0.003968
  l1.weight: grad_norm = 0.364208
  l1.bias: grad_norm = 0.002658
  l2.weight: grad_norm = 0.240336
Total gradient norm: 0.647672
=== Actor Training Debug (Iteration 9739) ===
Q mean: -14.051002
Q std: 19.967493
Actor loss: 14.054974
Action reg: 0.003972
  l1.weight: grad_norm = 0.234433
  l1.bias: grad_norm = 0.001215
  l2.weight: grad_norm = 0.186793
Total gradient norm: 0.497432
=== Actor Training Debug (Iteration 9740) ===
Q mean: -14.540589
Q std: 20.474678
Actor loss: 14.544550
Action reg: 0.003961
  l1.weight: grad_norm = 0.388236
  l1.bias: grad_norm = 0.004445
  l2.weight: grad_norm = 0.263051
Total gradient norm: 0.691206
=== Actor Training Debug (Iteration 9741) ===
Q mean: -15.082447
Q std: 22.715902
Actor loss: 15.086412
Action reg: 0.003966
  l1.weight: grad_norm = 0.277349
  l1.bias: grad_norm = 0.001985
  l2.weight: grad_norm = 0.195754
Total gradient norm: 0.505554
=== Actor Training Debug (Iteration 9742) ===
Q mean: -15.939157
Q std: 19.931902
Actor loss: 15.943126
Action reg: 0.003969
  l1.weight: grad_norm = 0.224564
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.149517
Total gradient norm: 0.406436
=== Actor Training Debug (Iteration 9743) ===
Q mean: -14.060855
Q std: 19.510111
Actor loss: 14.064820
Action reg: 0.003965
  l1.weight: grad_norm = 0.281544
  l1.bias: grad_norm = 0.001309
  l2.weight: grad_norm = 0.197914
Total gradient norm: 0.555932
=== Actor Training Debug (Iteration 9744) ===
Q mean: -15.541902
Q std: 20.825167
Actor loss: 15.545873
Action reg: 0.003971
  l1.weight: grad_norm = 0.337086
  l1.bias: grad_norm = 0.002665
  l2.weight: grad_norm = 0.268568
Total gradient norm: 0.710109
=== Actor Training Debug (Iteration 9745) ===
Q mean: -14.151297
Q std: 21.352812
Actor loss: 14.155258
Action reg: 0.003962
  l1.weight: grad_norm = 0.505023
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.339230
Total gradient norm: 0.889551
=== Actor Training Debug (Iteration 9746) ===
Q mean: -15.772779
Q std: 21.276970
Actor loss: 15.776747
Action reg: 0.003967
  l1.weight: grad_norm = 0.242287
  l1.bias: grad_norm = 0.001126
  l2.weight: grad_norm = 0.163739
Total gradient norm: 0.421304
=== Actor Training Debug (Iteration 9747) ===
Q mean: -15.474360
Q std: 20.964996
Actor loss: 15.478334
Action reg: 0.003974
  l1.weight: grad_norm = 0.155506
  l1.bias: grad_norm = 0.001020
  l2.weight: grad_norm = 0.117563
Total gradient norm: 0.352224
=== Actor Training Debug (Iteration 9748) ===
Q mean: -15.994057
Q std: 21.276106
Actor loss: 15.998036
Action reg: 0.003980
  l1.weight: grad_norm = 0.178504
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.127927
Total gradient norm: 0.377794
=== Actor Training Debug (Iteration 9749) ===
Q mean: -16.706289
Q std: 22.655714
Actor loss: 16.710262
Action reg: 0.003974
  l1.weight: grad_norm = 0.186061
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.151043
Total gradient norm: 0.498900
=== Actor Training Debug (Iteration 9750) ===
Q mean: -16.984009
Q std: 22.551390
Actor loss: 16.987968
Action reg: 0.003960
  l1.weight: grad_norm = 0.533915
  l1.bias: grad_norm = 0.002508
  l2.weight: grad_norm = 0.427077
Total gradient norm: 1.182172
=== Actor Training Debug (Iteration 9751) ===
Q mean: -15.948759
Q std: 20.913908
Actor loss: 15.952731
Action reg: 0.003972
  l1.weight: grad_norm = 0.308451
  l1.bias: grad_norm = 0.000813
  l2.weight: grad_norm = 0.242708
Total gradient norm: 0.599815
=== Actor Training Debug (Iteration 9752) ===
Q mean: -13.967198
Q std: 19.539976
Actor loss: 13.971161
Action reg: 0.003962
  l1.weight: grad_norm = 0.196361
  l1.bias: grad_norm = 0.006568
  l2.weight: grad_norm = 0.161125
Total gradient norm: 0.438538
=== Actor Training Debug (Iteration 9753) ===
Q mean: -13.069662
Q std: 18.751572
Actor loss: 13.073620
Action reg: 0.003958
  l1.weight: grad_norm = 0.322170
  l1.bias: grad_norm = 0.002950
  l2.weight: grad_norm = 0.264998
Total gradient norm: 0.733623
=== Actor Training Debug (Iteration 9754) ===
Q mean: -14.698973
Q std: 19.712296
Actor loss: 14.702941
Action reg: 0.003968
  l1.weight: grad_norm = 0.204225
  l1.bias: grad_norm = 0.001201
  l2.weight: grad_norm = 0.149507
Total gradient norm: 0.389403
=== Actor Training Debug (Iteration 9755) ===
Q mean: -12.191893
Q std: 17.635576
Actor loss: 12.195865
Action reg: 0.003972
  l1.weight: grad_norm = 0.181645
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.140038
Total gradient norm: 0.391643
=== Actor Training Debug (Iteration 9756) ===
Q mean: -17.941525
Q std: 22.947687
Actor loss: 17.945490
Action reg: 0.003966
  l1.weight: grad_norm = 0.306624
  l1.bias: grad_norm = 0.000947
  l2.weight: grad_norm = 0.240307
Total gradient norm: 0.780492
=== Actor Training Debug (Iteration 9757) ===
Q mean: -14.816649
Q std: 20.441658
Actor loss: 14.820628
Action reg: 0.003978
  l1.weight: grad_norm = 0.112056
  l1.bias: grad_norm = 0.001161
  l2.weight: grad_norm = 0.079291
Total gradient norm: 0.214681
=== Actor Training Debug (Iteration 9758) ===
Q mean: -16.319431
Q std: 22.033533
Actor loss: 16.323399
Action reg: 0.003967
  l1.weight: grad_norm = 0.328702
  l1.bias: grad_norm = 0.000860
  l2.weight: grad_norm = 0.237708
Total gradient norm: 0.622577
=== Actor Training Debug (Iteration 9759) ===
Q mean: -15.062658
Q std: 20.932194
Actor loss: 15.066616
Action reg: 0.003958
  l1.weight: grad_norm = 0.286333
  l1.bias: grad_norm = 0.001450
  l2.weight: grad_norm = 0.247053
Total gradient norm: 0.566580
=== Actor Training Debug (Iteration 9760) ===
Q mean: -14.833271
Q std: 20.342627
Actor loss: 14.837225
Action reg: 0.003954
  l1.weight: grad_norm = 0.340542
  l1.bias: grad_norm = 0.001808
  l2.weight: grad_norm = 0.257131
Total gradient norm: 0.621046
=== Actor Training Debug (Iteration 9761) ===
Q mean: -15.571260
Q std: 21.816711
Actor loss: 15.575220
Action reg: 0.003960
  l1.weight: grad_norm = 0.232671
  l1.bias: grad_norm = 0.002336
  l2.weight: grad_norm = 0.176674
Total gradient norm: 0.450634
=== Actor Training Debug (Iteration 9762) ===
Q mean: -14.803877
Q std: 21.087017
Actor loss: 14.807823
Action reg: 0.003946
  l1.weight: grad_norm = 0.463848
  l1.bias: grad_norm = 0.003503
  l2.weight: grad_norm = 0.292599
Total gradient norm: 0.774107
=== Actor Training Debug (Iteration 9763) ===
Q mean: -14.204210
Q std: 20.084812
Actor loss: 14.208177
Action reg: 0.003966
  l1.weight: grad_norm = 0.204688
  l1.bias: grad_norm = 0.002524
  l2.weight: grad_norm = 0.142944
Total gradient norm: 0.341291
=== Actor Training Debug (Iteration 9764) ===
Q mean: -15.557716
Q std: 20.612135
Actor loss: 15.561691
Action reg: 0.003975
  l1.weight: grad_norm = 0.297999
  l1.bias: grad_norm = 0.000565
  l2.weight: grad_norm = 0.200710
Total gradient norm: 0.571881
=== Actor Training Debug (Iteration 9765) ===
Q mean: -16.225784
Q std: 20.578873
Actor loss: 16.229744
Action reg: 0.003960
  l1.weight: grad_norm = 0.329937
  l1.bias: grad_norm = 0.001777
  l2.weight: grad_norm = 0.234697
Total gradient norm: 0.685680
=== Actor Training Debug (Iteration 9766) ===
Q mean: -15.257023
Q std: 21.071798
Actor loss: 15.260985
Action reg: 0.003962
  l1.weight: grad_norm = 0.371298
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.313465
Total gradient norm: 0.808411
=== Actor Training Debug (Iteration 9767) ===
Q mean: -13.572642
Q std: 20.420162
Actor loss: 13.576607
Action reg: 0.003965
  l1.weight: grad_norm = 0.769888
  l1.bias: grad_norm = 0.001302
  l2.weight: grad_norm = 0.466247
Total gradient norm: 1.415917
=== Actor Training Debug (Iteration 9768) ===
Q mean: -14.724236
Q std: 20.496536
Actor loss: 14.728194
Action reg: 0.003958
  l1.weight: grad_norm = 0.299426
  l1.bias: grad_norm = 0.002769
  l2.weight: grad_norm = 0.211162
Total gradient norm: 0.561850
=== Actor Training Debug (Iteration 9769) ===
Q mean: -15.220051
Q std: 20.384521
Actor loss: 15.224024
Action reg: 0.003973
  l1.weight: grad_norm = 0.202146
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.161553
Total gradient norm: 0.424626
=== Actor Training Debug (Iteration 9770) ===
Q mean: -15.042731
Q std: 19.387617
Actor loss: 15.046702
Action reg: 0.003972
  l1.weight: grad_norm = 0.276277
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.206294
Total gradient norm: 0.528842
=== Actor Training Debug (Iteration 9771) ===
Q mean: -13.819251
Q std: 19.441433
Actor loss: 13.823212
Action reg: 0.003961
  l1.weight: grad_norm = 0.310286
  l1.bias: grad_norm = 0.001339
  l2.weight: grad_norm = 0.195505
Total gradient norm: 0.507772
=== Actor Training Debug (Iteration 9772) ===
Q mean: -14.052441
Q std: 19.761419
Actor loss: 14.056400
Action reg: 0.003960
  l1.weight: grad_norm = 0.238455
  l1.bias: grad_norm = 0.001447
  l2.weight: grad_norm = 0.169917
Total gradient norm: 0.444781
=== Actor Training Debug (Iteration 9773) ===
Q mean: -15.153771
Q std: 21.053658
Actor loss: 15.157732
Action reg: 0.003960
  l1.weight: grad_norm = 0.513208
  l1.bias: grad_norm = 0.001487
  l2.weight: grad_norm = 0.369456
Total gradient norm: 1.284697
=== Actor Training Debug (Iteration 9774) ===
Q mean: -16.251854
Q std: 22.429689
Actor loss: 16.255816
Action reg: 0.003962
  l1.weight: grad_norm = 0.307237
  l1.bias: grad_norm = 0.001789
  l2.weight: grad_norm = 0.213688
Total gradient norm: 0.637990
=== Actor Training Debug (Iteration 9775) ===
Q mean: -16.963381
Q std: 22.009771
Actor loss: 16.967340
Action reg: 0.003959
  l1.weight: grad_norm = 0.311276
  l1.bias: grad_norm = 0.002679
  l2.weight: grad_norm = 0.243308
Total gradient norm: 0.639260
=== Actor Training Debug (Iteration 9776) ===
Q mean: -16.370441
Q std: 20.779860
Actor loss: 16.374401
Action reg: 0.003960
  l1.weight: grad_norm = 0.554184
  l1.bias: grad_norm = 0.001867
  l2.weight: grad_norm = 0.381098
Total gradient norm: 1.165842
=== Actor Training Debug (Iteration 9777) ===
Q mean: -14.127045
Q std: 18.190611
Actor loss: 14.131009
Action reg: 0.003964
  l1.weight: grad_norm = 0.321705
  l1.bias: grad_norm = 0.001524
  l2.weight: grad_norm = 0.243061
Total gradient norm: 0.665499
=== Actor Training Debug (Iteration 9778) ===
Q mean: -12.744116
Q std: 18.713242
Actor loss: 12.748077
Action reg: 0.003961
  l1.weight: grad_norm = 0.466214
  l1.bias: grad_norm = 0.001197
  l2.weight: grad_norm = 0.275311
Total gradient norm: 0.753589
=== Actor Training Debug (Iteration 9779) ===
Q mean: -15.235294
Q std: 22.019808
Actor loss: 15.239228
Action reg: 0.003934
  l1.weight: grad_norm = 0.236662
  l1.bias: grad_norm = 0.003434
  l2.weight: grad_norm = 0.189043
Total gradient norm: 0.530598
=== Actor Training Debug (Iteration 9780) ===
Q mean: -14.798588
Q std: 20.975401
Actor loss: 14.802562
Action reg: 0.003974
  l1.weight: grad_norm = 0.211737
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.168913
Total gradient norm: 0.506137
=== Actor Training Debug (Iteration 9781) ===
Q mean: -13.482248
Q std: 18.503595
Actor loss: 13.486213
Action reg: 0.003964
  l1.weight: grad_norm = 0.175898
  l1.bias: grad_norm = 0.002520
  l2.weight: grad_norm = 0.153548
Total gradient norm: 0.435201
=== Actor Training Debug (Iteration 9782) ===
Q mean: -16.585135
Q std: 21.474810
Actor loss: 16.589092
Action reg: 0.003958
  l1.weight: grad_norm = 0.308375
  l1.bias: grad_norm = 0.002779
  l2.weight: grad_norm = 0.248386
Total gradient norm: 0.618853
=== Actor Training Debug (Iteration 9783) ===
Q mean: -13.218568
Q std: 18.763132
Actor loss: 13.222512
Action reg: 0.003945
  l1.weight: grad_norm = 0.524734
  l1.bias: grad_norm = 0.000744
  l2.weight: grad_norm = 0.361600
Total gradient norm: 1.039745
=== Actor Training Debug (Iteration 9784) ===
Q mean: -13.507135
Q std: 18.402617
Actor loss: 13.511099
Action reg: 0.003964
  l1.weight: grad_norm = 0.342280
  l1.bias: grad_norm = 0.001133
  l2.weight: grad_norm = 0.299167
Total gradient norm: 0.738547
=== Actor Training Debug (Iteration 9785) ===
Q mean: -16.119982
Q std: 21.508144
Actor loss: 16.123953
Action reg: 0.003972
  l1.weight: grad_norm = 0.160828
  l1.bias: grad_norm = 0.002690
  l2.weight: grad_norm = 0.131486
Total gradient norm: 0.400353
=== Actor Training Debug (Iteration 9786) ===
Q mean: -16.342684
Q std: 22.712326
Actor loss: 16.346649
Action reg: 0.003966
  l1.weight: grad_norm = 0.391905
  l1.bias: grad_norm = 0.001488
  l2.weight: grad_norm = 0.245991
Total gradient norm: 0.611729
=== Actor Training Debug (Iteration 9787) ===
Q mean: -17.463461
Q std: 21.516294
Actor loss: 17.467438
Action reg: 0.003977
  l1.weight: grad_norm = 0.230712
  l1.bias: grad_norm = 0.001728
  l2.weight: grad_norm = 0.153643
Total gradient norm: 0.393455
=== Actor Training Debug (Iteration 9788) ===
Q mean: -15.365211
Q std: 19.992266
Actor loss: 15.369155
Action reg: 0.003944
  l1.weight: grad_norm = 0.318683
  l1.bias: grad_norm = 0.003513
  l2.weight: grad_norm = 0.202780
Total gradient norm: 0.515292
=== Actor Training Debug (Iteration 9789) ===
Q mean: -14.948493
Q std: 21.931589
Actor loss: 14.952469
Action reg: 0.003976
  l1.weight: grad_norm = 0.196215
  l1.bias: grad_norm = 0.003713
  l2.weight: grad_norm = 0.132156
Total gradient norm: 0.421349
=== Actor Training Debug (Iteration 9790) ===
Q mean: -13.378055
Q std: 20.564402
Actor loss: 13.382015
Action reg: 0.003960
  l1.weight: grad_norm = 0.380531
  l1.bias: grad_norm = 0.001231
  l2.weight: grad_norm = 0.287536
Total gradient norm: 0.822143
=== Actor Training Debug (Iteration 9791) ===
Q mean: -14.187263
Q std: 19.084589
Actor loss: 14.191229
Action reg: 0.003966
  l1.weight: grad_norm = 0.267963
  l1.bias: grad_norm = 0.000738
  l2.weight: grad_norm = 0.228219
Total gradient norm: 0.577783
=== Actor Training Debug (Iteration 9792) ===
Q mean: -15.454852
Q std: 20.221996
Actor loss: 15.458818
Action reg: 0.003966
  l1.weight: grad_norm = 0.272154
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.218781
Total gradient norm: 0.623124
=== Actor Training Debug (Iteration 9793) ===
Q mean: -14.867222
Q std: 21.164316
Actor loss: 14.871191
Action reg: 0.003969
  l1.weight: grad_norm = 0.245528
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.207272
Total gradient norm: 0.524301
=== Actor Training Debug (Iteration 9794) ===
Q mean: -15.061481
Q std: 21.523800
Actor loss: 15.065446
Action reg: 0.003966
  l1.weight: grad_norm = 0.308648
  l1.bias: grad_norm = 0.001261
  l2.weight: grad_norm = 0.218982
Total gradient norm: 0.545552
=== Actor Training Debug (Iteration 9795) ===
Q mean: -16.440405
Q std: 22.083025
Actor loss: 16.444372
Action reg: 0.003968
  l1.weight: grad_norm = 0.109680
  l1.bias: grad_norm = 0.003052
  l2.weight: grad_norm = 0.089986
Total gradient norm: 0.273730
=== Actor Training Debug (Iteration 9796) ===
Q mean: -16.094513
Q std: 21.174524
Actor loss: 16.098482
Action reg: 0.003970
  l1.weight: grad_norm = 0.375216
  l1.bias: grad_norm = 0.001328
  l2.weight: grad_norm = 0.208115
Total gradient norm: 0.587436
=== Actor Training Debug (Iteration 9797) ===
Q mean: -15.444606
Q std: 20.820911
Actor loss: 15.448578
Action reg: 0.003972
  l1.weight: grad_norm = 0.341289
  l1.bias: grad_norm = 0.001463
  l2.weight: grad_norm = 0.272287
Total gradient norm: 0.673395
=== Actor Training Debug (Iteration 9798) ===
Q mean: -13.853622
Q std: 20.547581
Actor loss: 13.857601
Action reg: 0.003979
  l1.weight: grad_norm = 0.073793
  l1.bias: grad_norm = 0.001637
  l2.weight: grad_norm = 0.050116
Total gradient norm: 0.151987
=== Actor Training Debug (Iteration 9799) ===
Q mean: -15.088217
Q std: 19.369120
Actor loss: 15.092184
Action reg: 0.003967
  l1.weight: grad_norm = 0.197279
  l1.bias: grad_norm = 0.000745
  l2.weight: grad_norm = 0.150381
Total gradient norm: 0.398101
=== Actor Training Debug (Iteration 9800) ===
Q mean: -14.897584
Q std: 20.480299
Actor loss: 14.901555
Action reg: 0.003971
  l1.weight: grad_norm = 0.340307
  l1.bias: grad_norm = 0.001042
  l2.weight: grad_norm = 0.261169
Total gradient norm: 0.683899
=== Actor Training Debug (Iteration 9801) ===
Q mean: -14.594783
Q std: 20.063047
Actor loss: 14.598759
Action reg: 0.003976
  l1.weight: grad_norm = 0.237851
  l1.bias: grad_norm = 0.002198
  l2.weight: grad_norm = 0.183007
Total gradient norm: 0.505195
=== Actor Training Debug (Iteration 9802) ===
Q mean: -16.111715
Q std: 21.625507
Actor loss: 16.115671
Action reg: 0.003955
  l1.weight: grad_norm = 0.253744
  l1.bias: grad_norm = 0.001522
  l2.weight: grad_norm = 0.223030
Total gradient norm: 0.572388
=== Actor Training Debug (Iteration 9803) ===
Q mean: -16.695040
Q std: 22.413465
Actor loss: 16.699009
Action reg: 0.003969
  l1.weight: grad_norm = 0.194667
  l1.bias: grad_norm = 0.001973
  l2.weight: grad_norm = 0.122532
Total gradient norm: 0.319949
=== Actor Training Debug (Iteration 9804) ===
Q mean: -16.135057
Q std: 21.281862
Actor loss: 16.139032
Action reg: 0.003975
  l1.weight: grad_norm = 0.393146
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.267451
Total gradient norm: 0.710613
=== Actor Training Debug (Iteration 9805) ===
Q mean: -15.374678
Q std: 22.121233
Actor loss: 15.378635
Action reg: 0.003958
  l1.weight: grad_norm = 0.290235
  l1.bias: grad_norm = 0.001918
  l2.weight: grad_norm = 0.227992
Total gradient norm: 0.575058
=== Actor Training Debug (Iteration 9806) ===
Q mean: -15.048368
Q std: 20.332750
Actor loss: 15.052327
Action reg: 0.003960
  l1.weight: grad_norm = 0.198362
  l1.bias: grad_norm = 0.001239
  l2.weight: grad_norm = 0.146907
Total gradient norm: 0.391294
=== Actor Training Debug (Iteration 9807) ===
Q mean: -13.407218
Q std: 19.504892
Actor loss: 13.411190
Action reg: 0.003972
  l1.weight: grad_norm = 0.294805
  l1.bias: grad_norm = 0.000748
  l2.weight: grad_norm = 0.211024
Total gradient norm: 0.498156
=== Actor Training Debug (Iteration 9808) ===
Q mean: -14.691320
Q std: 20.980204
Actor loss: 14.695292
Action reg: 0.003972
  l1.weight: grad_norm = 0.175651
  l1.bias: grad_norm = 0.001436
  l2.weight: grad_norm = 0.144616
Total gradient norm: 0.372334
=== Actor Training Debug (Iteration 9809) ===
Q mean: -13.352377
Q std: 18.295630
Actor loss: 13.356338
Action reg: 0.003960
  l1.weight: grad_norm = 0.251154
  l1.bias: grad_norm = 0.001727
  l2.weight: grad_norm = 0.161654
Total gradient norm: 0.455773
=== Actor Training Debug (Iteration 9810) ===
Q mean: -12.891235
Q std: 18.360552
Actor loss: 12.895205
Action reg: 0.003970
  l1.weight: grad_norm = 0.256656
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.203178
Total gradient norm: 0.533557
=== Actor Training Debug (Iteration 9811) ===
Q mean: -16.189827
Q std: 22.227358
Actor loss: 16.193789
Action reg: 0.003962
  l1.weight: grad_norm = 0.275388
  l1.bias: grad_norm = 0.001656
  l2.weight: grad_norm = 0.184599
Total gradient norm: 0.476326
=== Actor Training Debug (Iteration 9812) ===
Q mean: -16.456593
Q std: 22.240963
Actor loss: 16.460552
Action reg: 0.003960
  l1.weight: grad_norm = 0.501841
  l1.bias: grad_norm = 0.001009
  l2.weight: grad_norm = 0.421545
Total gradient norm: 1.403365
=== Actor Training Debug (Iteration 9813) ===
Q mean: -16.050165
Q std: 21.644608
Actor loss: 16.054140
Action reg: 0.003975
  l1.weight: grad_norm = 0.239745
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.177393
Total gradient norm: 0.463963
=== Actor Training Debug (Iteration 9814) ===
Q mean: -13.116240
Q std: 18.980190
Actor loss: 13.120192
Action reg: 0.003952
  l1.weight: grad_norm = 0.329035
  l1.bias: grad_norm = 0.001688
  l2.weight: grad_norm = 0.233050
Total gradient norm: 0.731004
=== Actor Training Debug (Iteration 9815) ===
Q mean: -14.687442
Q std: 20.388023
Actor loss: 14.691404
Action reg: 0.003962
  l1.weight: grad_norm = 0.393686
  l1.bias: grad_norm = 0.000930
  l2.weight: grad_norm = 0.255281
Total gradient norm: 0.656365
=== Actor Training Debug (Iteration 9816) ===
Q mean: -16.780222
Q std: 22.319086
Actor loss: 16.784184
Action reg: 0.003961
  l1.weight: grad_norm = 0.243049
  l1.bias: grad_norm = 0.000582
  l2.weight: grad_norm = 0.180705
Total gradient norm: 0.484297
=== Actor Training Debug (Iteration 9817) ===
Q mean: -12.941677
Q std: 18.401865
Actor loss: 12.945653
Action reg: 0.003976
  l1.weight: grad_norm = 0.341499
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.272983
Total gradient norm: 0.804472
=== Actor Training Debug (Iteration 9818) ===
Q mean: -14.716314
Q std: 21.358088
Actor loss: 14.720273
Action reg: 0.003959
  l1.weight: grad_norm = 0.203386
  l1.bias: grad_norm = 0.002833
  l2.weight: grad_norm = 0.153027
Total gradient norm: 0.420145
=== Actor Training Debug (Iteration 9819) ===
Q mean: -14.500001
Q std: 20.057194
Actor loss: 14.503969
Action reg: 0.003968
  l1.weight: grad_norm = 0.257936
  l1.bias: grad_norm = 0.000569
  l2.weight: grad_norm = 0.162590
Total gradient norm: 0.445271
=== Actor Training Debug (Iteration 9820) ===
Q mean: -15.519001
Q std: 21.814804
Actor loss: 15.522959
Action reg: 0.003958
  l1.weight: grad_norm = 0.354826
  l1.bias: grad_norm = 0.000718
  l2.weight: grad_norm = 0.253873
Total gradient norm: 0.682759
=== Actor Training Debug (Iteration 9821) ===
Q mean: -16.534477
Q std: 21.675205
Actor loss: 16.538448
Action reg: 0.003971
  l1.weight: grad_norm = 0.356519
  l1.bias: grad_norm = 0.000670
  l2.weight: grad_norm = 0.245959
Total gradient norm: 0.651478
=== Actor Training Debug (Iteration 9822) ===
Q mean: -13.687551
Q std: 19.562204
Actor loss: 13.691518
Action reg: 0.003966
  l1.weight: grad_norm = 0.205739
  l1.bias: grad_norm = 0.001108
  l2.weight: grad_norm = 0.161674
Total gradient norm: 0.395216
=== Actor Training Debug (Iteration 9823) ===
Q mean: -13.306054
Q std: 18.773989
Actor loss: 13.310033
Action reg: 0.003979
  l1.weight: grad_norm = 0.460047
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.286461
Total gradient norm: 0.730756
=== Actor Training Debug (Iteration 9824) ===
Q mean: -14.035094
Q std: 20.923590
Actor loss: 14.039062
Action reg: 0.003967
  l1.weight: grad_norm = 0.519238
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.411125
Total gradient norm: 1.128682
=== Actor Training Debug (Iteration 9825) ===
Q mean: -16.339455
Q std: 22.292053
Actor loss: 16.343426
Action reg: 0.003971
  l1.weight: grad_norm = 0.599729
  l1.bias: grad_norm = 0.000703
  l2.weight: grad_norm = 0.475537
Total gradient norm: 1.249424
=== Actor Training Debug (Iteration 9826) ===
Q mean: -16.156055
Q std: 22.129738
Actor loss: 16.160034
Action reg: 0.003979
  l1.weight: grad_norm = 0.413980
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.263571
Total gradient norm: 0.708492
=== Actor Training Debug (Iteration 9827) ===
Q mean: -15.313519
Q std: 22.194248
Actor loss: 15.317492
Action reg: 0.003973
  l1.weight: grad_norm = 0.204696
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.162945
Total gradient norm: 0.402787
=== Actor Training Debug (Iteration 9828) ===
Q mean: -16.630688
Q std: 23.345871
Actor loss: 16.634647
Action reg: 0.003959
  l1.weight: grad_norm = 0.367017
  l1.bias: grad_norm = 0.001748
  l2.weight: grad_norm = 0.255069
Total gradient norm: 0.734298
=== Actor Training Debug (Iteration 9829) ===
Q mean: -14.107046
Q std: 20.445299
Actor loss: 14.111002
Action reg: 0.003956
  l1.weight: grad_norm = 0.409831
  l1.bias: grad_norm = 0.001213
  l2.weight: grad_norm = 0.328621
Total gradient norm: 0.836767
=== Actor Training Debug (Iteration 9830) ===
Q mean: -12.642945
Q std: 18.864565
Actor loss: 12.646915
Action reg: 0.003971
  l1.weight: grad_norm = 0.342304
  l1.bias: grad_norm = 0.000949
  l2.weight: grad_norm = 0.251468
Total gradient norm: 0.703545
=== Actor Training Debug (Iteration 9831) ===
Q mean: -14.228492
Q std: 18.901600
Actor loss: 14.232458
Action reg: 0.003966
  l1.weight: grad_norm = 0.192196
  l1.bias: grad_norm = 0.000945
  l2.weight: grad_norm = 0.137199
Total gradient norm: 0.372583
=== Actor Training Debug (Iteration 9832) ===
Q mean: -16.301409
Q std: 21.409315
Actor loss: 16.305380
Action reg: 0.003971
  l1.weight: grad_norm = 0.222330
  l1.bias: grad_norm = 0.001036
  l2.weight: grad_norm = 0.148267
Total gradient norm: 0.387844
=== Actor Training Debug (Iteration 9833) ===
Q mean: -14.001101
Q std: 20.826921
Actor loss: 14.005075
Action reg: 0.003974
  l1.weight: grad_norm = 0.213609
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.164344
Total gradient norm: 0.473893
=== Actor Training Debug (Iteration 9834) ===
Q mean: -15.563954
Q std: 20.370157
Actor loss: 15.567922
Action reg: 0.003967
  l1.weight: grad_norm = 0.375133
  l1.bias: grad_norm = 0.003480
  l2.weight: grad_norm = 0.334937
Total gradient norm: 0.900137
=== Actor Training Debug (Iteration 9835) ===
Q mean: -14.315187
Q std: 19.277866
Actor loss: 14.319147
Action reg: 0.003961
  l1.weight: grad_norm = 0.298421
  l1.bias: grad_norm = 0.000613
  l2.weight: grad_norm = 0.232905
Total gradient norm: 0.569156
=== Actor Training Debug (Iteration 9836) ===
Q mean: -13.597088
Q std: 18.742701
Actor loss: 13.601060
Action reg: 0.003972
  l1.weight: grad_norm = 0.226544
  l1.bias: grad_norm = 0.001145
  l2.weight: grad_norm = 0.211003
Total gradient norm: 0.612160
=== Actor Training Debug (Iteration 9837) ===
Q mean: -16.224617
Q std: 20.438185
Actor loss: 16.228586
Action reg: 0.003969
  l1.weight: grad_norm = 0.270256
  l1.bias: grad_norm = 0.000845
  l2.weight: grad_norm = 0.196115
Total gradient norm: 0.534605
=== Actor Training Debug (Iteration 9838) ===
Q mean: -14.479302
Q std: 19.861166
Actor loss: 14.483270
Action reg: 0.003967
  l1.weight: grad_norm = 0.227304
  l1.bias: grad_norm = 0.001388
  l2.weight: grad_norm = 0.166127
Total gradient norm: 0.431120
=== Actor Training Debug (Iteration 9839) ===
Q mean: -13.996251
Q std: 20.671186
Actor loss: 14.000212
Action reg: 0.003961
  l1.weight: grad_norm = 0.389402
  l1.bias: grad_norm = 0.001206
  l2.weight: grad_norm = 0.284579
Total gradient norm: 0.714829
=== Actor Training Debug (Iteration 9840) ===
Q mean: -14.460970
Q std: 20.492504
Actor loss: 14.464931
Action reg: 0.003961
  l1.weight: grad_norm = 0.451944
  l1.bias: grad_norm = 0.002132
  l2.weight: grad_norm = 0.309213
Total gradient norm: 0.761035
=== Actor Training Debug (Iteration 9841) ===
Q mean: -16.176455
Q std: 21.653049
Actor loss: 16.180428
Action reg: 0.003973
  l1.weight: grad_norm = 0.185316
  l1.bias: grad_norm = 0.002373
  l2.weight: grad_norm = 0.121491
Total gradient norm: 0.366361
=== Actor Training Debug (Iteration 9842) ===
Q mean: -15.004807
Q std: 19.893705
Actor loss: 15.008782
Action reg: 0.003976
  l1.weight: grad_norm = 0.199285
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.173820
Total gradient norm: 0.520147
=== Actor Training Debug (Iteration 9843) ===
Q mean: -14.432999
Q std: 21.162050
Actor loss: 14.436969
Action reg: 0.003970
  l1.weight: grad_norm = 0.825660
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.576368
Total gradient norm: 1.564724
=== Actor Training Debug (Iteration 9844) ===
Q mean: -15.449499
Q std: 20.019812
Actor loss: 15.453462
Action reg: 0.003963
  l1.weight: grad_norm = 0.232813
  l1.bias: grad_norm = 0.002450
  l2.weight: grad_norm = 0.157048
Total gradient norm: 0.424005
=== Actor Training Debug (Iteration 9845) ===
Q mean: -14.940178
Q std: 21.369131
Actor loss: 14.944144
Action reg: 0.003966
  l1.weight: grad_norm = 0.386073
  l1.bias: grad_norm = 0.003023
  l2.weight: grad_norm = 0.286573
Total gradient norm: 0.701125
=== Actor Training Debug (Iteration 9846) ===
Q mean: -14.737862
Q std: 19.417995
Actor loss: 14.741821
Action reg: 0.003960
  l1.weight: grad_norm = 0.487348
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.339658
Total gradient norm: 0.881000
=== Actor Training Debug (Iteration 9847) ===
Q mean: -15.420571
Q std: 21.477922
Actor loss: 15.424543
Action reg: 0.003972
  l1.weight: grad_norm = 0.261944
  l1.bias: grad_norm = 0.000850
  l2.weight: grad_norm = 0.179215
Total gradient norm: 0.434377
=== Actor Training Debug (Iteration 9848) ===
Q mean: -14.178091
Q std: 20.403730
Actor loss: 14.182055
Action reg: 0.003964
  l1.weight: grad_norm = 0.292121
  l1.bias: grad_norm = 0.001636
  l2.weight: grad_norm = 0.252288
Total gradient norm: 0.693920
=== Actor Training Debug (Iteration 9849) ===
Q mean: -16.831812
Q std: 22.375029
Actor loss: 16.835781
Action reg: 0.003970
  l1.weight: grad_norm = 0.301835
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.220860
Total gradient norm: 0.533794
=== Actor Training Debug (Iteration 9850) ===
Q mean: -14.945007
Q std: 21.243111
Actor loss: 14.948970
Action reg: 0.003962
  l1.weight: grad_norm = 0.257699
  l1.bias: grad_norm = 0.001864
  l2.weight: grad_norm = 0.190718
Total gradient norm: 0.549257
=== Actor Training Debug (Iteration 9851) ===
Q mean: -14.797403
Q std: 19.734289
Actor loss: 14.801360
Action reg: 0.003957
  l1.weight: grad_norm = 0.554637
  l1.bias: grad_norm = 0.000656
  l2.weight: grad_norm = 0.393329
Total gradient norm: 1.144451
=== Actor Training Debug (Iteration 9852) ===
Q mean: -17.096531
Q std: 22.318214
Actor loss: 17.100496
Action reg: 0.003966
  l1.weight: grad_norm = 0.192346
  l1.bias: grad_norm = 0.000562
  l2.weight: grad_norm = 0.154462
Total gradient norm: 0.422375
=== Actor Training Debug (Iteration 9853) ===
Q mean: -15.606247
Q std: 21.327065
Actor loss: 15.610221
Action reg: 0.003974
  l1.weight: grad_norm = 0.206139
  l1.bias: grad_norm = 0.001030
  l2.weight: grad_norm = 0.151253
Total gradient norm: 0.428600
=== Actor Training Debug (Iteration 9854) ===
Q mean: -13.501319
Q std: 19.517052
Actor loss: 13.505285
Action reg: 0.003966
  l1.weight: grad_norm = 0.293395
  l1.bias: grad_norm = 0.000728
  l2.weight: grad_norm = 0.223326
Total gradient norm: 0.576778
=== Actor Training Debug (Iteration 9855) ===
Q mean: -16.639877
Q std: 23.476107
Actor loss: 16.643845
Action reg: 0.003967
  l1.weight: grad_norm = 0.240520
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.174339
Total gradient norm: 0.413816
=== Actor Training Debug (Iteration 9856) ===
Q mean: -16.327034
Q std: 21.782148
Actor loss: 16.331007
Action reg: 0.003973
  l1.weight: grad_norm = 0.274449
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.209044
Total gradient norm: 0.550734
=== Actor Training Debug (Iteration 9857) ===
Q mean: -15.713882
Q std: 21.434633
Actor loss: 15.717852
Action reg: 0.003969
  l1.weight: grad_norm = 0.216164
  l1.bias: grad_norm = 0.000978
  l2.weight: grad_norm = 0.172362
Total gradient norm: 0.457716
=== Actor Training Debug (Iteration 9858) ===
Q mean: -13.936686
Q std: 19.616041
Actor loss: 13.940655
Action reg: 0.003969
  l1.weight: grad_norm = 0.340553
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.249676
Total gradient norm: 0.787222
=== Actor Training Debug (Iteration 9859) ===
Q mean: -15.592273
Q std: 20.958981
Actor loss: 15.596245
Action reg: 0.003972
  l1.weight: grad_norm = 0.291440
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.196273
Total gradient norm: 0.490299
=== Actor Training Debug (Iteration 9860) ===
Q mean: -16.250530
Q std: 21.091093
Actor loss: 16.254492
Action reg: 0.003962
  l1.weight: grad_norm = 0.197905
  l1.bias: grad_norm = 0.000935
  l2.weight: grad_norm = 0.133885
Total gradient norm: 0.382718
=== Actor Training Debug (Iteration 9861) ===
Q mean: -15.401173
Q std: 20.866911
Actor loss: 15.405149
Action reg: 0.003977
  l1.weight: grad_norm = 0.321780
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.197121
Total gradient norm: 0.547823
=== Actor Training Debug (Iteration 9862) ===
Q mean: -13.354630
Q std: 19.154860
Actor loss: 13.358603
Action reg: 0.003972
  l1.weight: grad_norm = 0.264872
  l1.bias: grad_norm = 0.000592
  l2.weight: grad_norm = 0.200147
Total gradient norm: 0.547223
=== Actor Training Debug (Iteration 9863) ===
Q mean: -14.978266
Q std: 21.533710
Actor loss: 14.982231
Action reg: 0.003966
  l1.weight: grad_norm = 0.272547
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.215920
Total gradient norm: 0.663588
=== Actor Training Debug (Iteration 9864) ===
Q mean: -13.514311
Q std: 20.231476
Actor loss: 13.518290
Action reg: 0.003978
  l1.weight: grad_norm = 0.172795
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.137021
Total gradient norm: 0.375158
=== Actor Training Debug (Iteration 9865) ===
Q mean: -14.543053
Q std: 20.111597
Actor loss: 14.547016
Action reg: 0.003963
  l1.weight: grad_norm = 0.164313
  l1.bias: grad_norm = 0.001178
  l2.weight: grad_norm = 0.148915
Total gradient norm: 0.440289
=== Actor Training Debug (Iteration 9866) ===
Q mean: -14.845633
Q std: 21.487526
Actor loss: 14.849611
Action reg: 0.003979
  l1.weight: grad_norm = 0.394594
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.246856
Total gradient norm: 0.664192
=== Actor Training Debug (Iteration 9867) ===
Q mean: -14.659542
Q std: 18.681833
Actor loss: 14.663532
Action reg: 0.003990
  l1.weight: grad_norm = 0.121098
  l1.bias: grad_norm = 0.000468
  l2.weight: grad_norm = 0.095035
Total gradient norm: 0.276797
=== Actor Training Debug (Iteration 9868) ===
Q mean: -14.545265
Q std: 20.979704
Actor loss: 14.549244
Action reg: 0.003979
  l1.weight: grad_norm = 0.136429
  l1.bias: grad_norm = 0.001011
  l2.weight: grad_norm = 0.112985
Total gradient norm: 0.305616
=== Actor Training Debug (Iteration 9869) ===
Q mean: -13.605848
Q std: 19.372791
Actor loss: 13.609835
Action reg: 0.003986
  l1.weight: grad_norm = 0.187925
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.148426
Total gradient norm: 0.370244
=== Actor Training Debug (Iteration 9870) ===
Q mean: -15.085912
Q std: 20.572405
Actor loss: 15.089875
Action reg: 0.003963
  l1.weight: grad_norm = 0.211612
  l1.bias: grad_norm = 0.000988
  l2.weight: grad_norm = 0.162105
Total gradient norm: 0.432975
=== Actor Training Debug (Iteration 9871) ===
Q mean: -15.368314
Q std: 21.214470
Actor loss: 15.372277
Action reg: 0.003963
  l1.weight: grad_norm = 0.215950
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.169514
Total gradient norm: 0.449443
=== Actor Training Debug (Iteration 9872) ===
Q mean: -14.777979
Q std: 21.322557
Actor loss: 14.781950
Action reg: 0.003971
  l1.weight: grad_norm = 0.135109
  l1.bias: grad_norm = 0.000795
  l2.weight: grad_norm = 0.098591
Total gradient norm: 0.301821
=== Actor Training Debug (Iteration 9873) ===
Q mean: -14.025141
Q std: 19.115057
Actor loss: 14.029119
Action reg: 0.003978
  l1.weight: grad_norm = 0.202174
  l1.bias: grad_norm = 0.000975
  l2.weight: grad_norm = 0.140186
Total gradient norm: 0.356844
=== Actor Training Debug (Iteration 9874) ===
Q mean: -16.465366
Q std: 22.577030
Actor loss: 16.469336
Action reg: 0.003969
  l1.weight: grad_norm = 0.280417
  l1.bias: grad_norm = 0.000818
  l2.weight: grad_norm = 0.223613
Total gradient norm: 0.541308
=== Actor Training Debug (Iteration 9875) ===
Q mean: -17.332710
Q std: 22.760885
Actor loss: 17.336685
Action reg: 0.003975
  l1.weight: grad_norm = 0.252891
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.177201
Total gradient norm: 0.443051
=== Actor Training Debug (Iteration 9876) ===
Q mean: -16.256857
Q std: 22.235962
Actor loss: 16.260838
Action reg: 0.003981
  l1.weight: grad_norm = 0.251992
  l1.bias: grad_norm = 0.000902
  l2.weight: grad_norm = 0.183982
Total gradient norm: 0.549089
=== Actor Training Debug (Iteration 9877) ===
Q mean: -13.784351
Q std: 20.405617
Actor loss: 13.788319
Action reg: 0.003967
  l1.weight: grad_norm = 0.267849
  l1.bias: grad_norm = 0.001205
  l2.weight: grad_norm = 0.190232
Total gradient norm: 0.501567
=== Actor Training Debug (Iteration 9878) ===
Q mean: -15.721678
Q std: 22.817535
Actor loss: 15.725643
Action reg: 0.003966
  l1.weight: grad_norm = 0.365488
  l1.bias: grad_norm = 0.003338
  l2.weight: grad_norm = 0.265886
Total gradient norm: 0.723595
=== Actor Training Debug (Iteration 9879) ===
Q mean: -16.232651
Q std: 21.491987
Actor loss: 16.236620
Action reg: 0.003969
  l1.weight: grad_norm = 0.197098
  l1.bias: grad_norm = 0.000579
  l2.weight: grad_norm = 0.132391
Total gradient norm: 0.355285
=== Actor Training Debug (Iteration 9880) ===
Q mean: -16.530575
Q std: 22.736496
Actor loss: 16.534538
Action reg: 0.003963
  l1.weight: grad_norm = 0.430726
  l1.bias: grad_norm = 0.001369
  l2.weight: grad_norm = 0.348369
Total gradient norm: 0.959651
=== Actor Training Debug (Iteration 9881) ===
Q mean: -15.023021
Q std: 20.245022
Actor loss: 15.026996
Action reg: 0.003975
  l1.weight: grad_norm = 0.149249
  l1.bias: grad_norm = 0.002440
  l2.weight: grad_norm = 0.122767
Total gradient norm: 0.311479
=== Actor Training Debug (Iteration 9882) ===
Q mean: -11.460608
Q std: 15.519803
Actor loss: 11.464585
Action reg: 0.003978
  l1.weight: grad_norm = 0.386290
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.237756
Total gradient norm: 0.693776
=== Actor Training Debug (Iteration 9883) ===
Q mean: -13.521103
Q std: 18.588503
Actor loss: 13.525058
Action reg: 0.003955
  l1.weight: grad_norm = 0.530260
  l1.bias: grad_norm = 0.001541
  l2.weight: grad_norm = 0.355218
Total gradient norm: 0.904380
=== Actor Training Debug (Iteration 9884) ===
Q mean: -15.849359
Q std: 21.133852
Actor loss: 15.853332
Action reg: 0.003973
  l1.weight: grad_norm = 0.378371
  l1.bias: grad_norm = 0.003340
  l2.weight: grad_norm = 0.269215
Total gradient norm: 0.737692
=== Actor Training Debug (Iteration 9885) ===
Q mean: -13.470463
Q std: 20.511953
Actor loss: 13.474437
Action reg: 0.003974
  l1.weight: grad_norm = 0.166582
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.122209
Total gradient norm: 0.288642
=== Actor Training Debug (Iteration 9886) ===
Q mean: -16.628244
Q std: 22.282917
Actor loss: 16.632214
Action reg: 0.003969
  l1.weight: grad_norm = 0.162762
  l1.bias: grad_norm = 0.000913
  l2.weight: grad_norm = 0.117027
Total gradient norm: 0.300539
=== Actor Training Debug (Iteration 9887) ===
Q mean: -14.650706
Q std: 19.638920
Actor loss: 14.654671
Action reg: 0.003964
  l1.weight: grad_norm = 0.430206
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.308764
Total gradient norm: 0.827678
=== Actor Training Debug (Iteration 9888) ===
Q mean: -15.071331
Q std: 20.099442
Actor loss: 15.075295
Action reg: 0.003964
  l1.weight: grad_norm = 0.294028
  l1.bias: grad_norm = 0.001721
  l2.weight: grad_norm = 0.211410
Total gradient norm: 0.611984
=== Actor Training Debug (Iteration 9889) ===
Q mean: -15.941205
Q std: 20.995461
Actor loss: 15.945170
Action reg: 0.003965
  l1.weight: grad_norm = 0.263351
  l1.bias: grad_norm = 0.000651
  l2.weight: grad_norm = 0.208250
Total gradient norm: 0.539029
=== Actor Training Debug (Iteration 9890) ===
Q mean: -14.050184
Q std: 19.363588
Actor loss: 14.054149
Action reg: 0.003964
  l1.weight: grad_norm = 0.225500
  l1.bias: grad_norm = 0.000996
  l2.weight: grad_norm = 0.172161
Total gradient norm: 0.433498
=== Actor Training Debug (Iteration 9891) ===
Q mean: -11.568231
Q std: 19.521946
Actor loss: 11.572189
Action reg: 0.003959
  l1.weight: grad_norm = 0.275142
  l1.bias: grad_norm = 0.001665
  l2.weight: grad_norm = 0.211884
Total gradient norm: 0.532667
=== Actor Training Debug (Iteration 9892) ===
Q mean: -11.359860
Q std: 16.592710
Actor loss: 11.363821
Action reg: 0.003961
  l1.weight: grad_norm = 0.335628
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.224687
Total gradient norm: 0.599893
=== Actor Training Debug (Iteration 9893) ===
Q mean: -14.916418
Q std: 19.701712
Actor loss: 14.920399
Action reg: 0.003980
  l1.weight: grad_norm = 0.433204
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.282398
Total gradient norm: 0.732010
=== Actor Training Debug (Iteration 9894) ===
Q mean: -14.229647
Q std: 19.933052
Actor loss: 14.233596
Action reg: 0.003949
  l1.weight: grad_norm = 0.319018
  l1.bias: grad_norm = 0.000962
  l2.weight: grad_norm = 0.229910
Total gradient norm: 0.628591
=== Actor Training Debug (Iteration 9895) ===
Q mean: -14.277740
Q std: 19.138847
Actor loss: 14.281711
Action reg: 0.003971
  l1.weight: grad_norm = 0.216404
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.161819
Total gradient norm: 0.433779
=== Actor Training Debug (Iteration 9896) ===
Q mean: -14.944602
Q std: 21.082169
Actor loss: 14.948567
Action reg: 0.003966
  l1.weight: grad_norm = 0.493383
  l1.bias: grad_norm = 0.000791
  l2.weight: grad_norm = 0.367846
Total gradient norm: 1.154636
=== Actor Training Debug (Iteration 9897) ===
Q mean: -14.009779
Q std: 20.754425
Actor loss: 14.013752
Action reg: 0.003973
  l1.weight: grad_norm = 0.134394
  l1.bias: grad_norm = 0.001320
  l2.weight: grad_norm = 0.100395
Total gradient norm: 0.276972
=== Actor Training Debug (Iteration 9898) ===
Q mean: -16.833355
Q std: 22.558641
Actor loss: 16.837330
Action reg: 0.003975
  l1.weight: grad_norm = 0.210735
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.157381
Total gradient norm: 0.406101
=== Actor Training Debug (Iteration 9899) ===
Q mean: -15.131655
Q std: 19.748243
Actor loss: 15.135613
Action reg: 0.003959
  l1.weight: grad_norm = 0.351401
  l1.bias: grad_norm = 0.000910
  l2.weight: grad_norm = 0.255700
Total gradient norm: 0.629986
=== Actor Training Debug (Iteration 9900) ===
Q mean: -15.165168
Q std: 20.930876
Actor loss: 15.169144
Action reg: 0.003975
  l1.weight: grad_norm = 0.169239
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.139825
Total gradient norm: 0.402643
=== Actor Training Debug (Iteration 9901) ===
Q mean: -14.045132
Q std: 19.690685
Actor loss: 14.049095
Action reg: 0.003963
  l1.weight: grad_norm = 0.255958
  l1.bias: grad_norm = 0.001004
  l2.weight: grad_norm = 0.192716
Total gradient norm: 0.540414
=== Actor Training Debug (Iteration 9902) ===
Q mean: -15.820444
Q std: 20.630037
Actor loss: 15.824408
Action reg: 0.003963
  l1.weight: grad_norm = 0.453428
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.299480
Total gradient norm: 0.831200
=== Actor Training Debug (Iteration 9903) ===
Q mean: -15.115577
Q std: 21.605665
Actor loss: 15.119538
Action reg: 0.003962
  l1.weight: grad_norm = 0.377821
  l1.bias: grad_norm = 0.001040
  l2.weight: grad_norm = 0.285482
Total gradient norm: 0.740168
=== Actor Training Debug (Iteration 9904) ===
Q mean: -15.076360
Q std: 20.241720
Actor loss: 15.080316
Action reg: 0.003956
  l1.weight: grad_norm = 0.270470
  l1.bias: grad_norm = 0.001443
  l2.weight: grad_norm = 0.205923
Total gradient norm: 0.576955
=== Actor Training Debug (Iteration 9905) ===
Q mean: -16.433624
Q std: 20.652958
Actor loss: 16.437603
Action reg: 0.003980
  l1.weight: grad_norm = 0.138066
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.097867
Total gradient norm: 0.260355
=== Actor Training Debug (Iteration 9906) ===
Q mean: -16.574888
Q std: 22.220676
Actor loss: 16.578863
Action reg: 0.003975
  l1.weight: grad_norm = 0.183223
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.145637
Total gradient norm: 0.385041
=== Actor Training Debug (Iteration 9907) ===
Q mean: -12.834611
Q std: 18.248981
Actor loss: 12.838584
Action reg: 0.003973
  l1.weight: grad_norm = 0.267392
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.184720
Total gradient norm: 0.526429
=== Actor Training Debug (Iteration 9908) ===
Q mean: -11.902390
Q std: 16.894608
Actor loss: 11.906346
Action reg: 0.003957
  l1.weight: grad_norm = 0.440537
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.350410
Total gradient norm: 1.079198
=== Actor Training Debug (Iteration 9909) ===
Q mean: -14.094021
Q std: 20.128620
Actor loss: 14.097986
Action reg: 0.003965
  l1.weight: grad_norm = 0.332603
  l1.bias: grad_norm = 0.001110
  l2.weight: grad_norm = 0.226188
Total gradient norm: 0.581487
=== Actor Training Debug (Iteration 9910) ===
Q mean: -15.889059
Q std: 20.503807
Actor loss: 15.893021
Action reg: 0.003962
  l1.weight: grad_norm = 0.245984
  l1.bias: grad_norm = 0.002912
  l2.weight: grad_norm = 0.188012
Total gradient norm: 0.558838
=== Actor Training Debug (Iteration 9911) ===
Q mean: -17.841475
Q std: 23.335644
Actor loss: 17.845438
Action reg: 0.003963
  l1.weight: grad_norm = 0.314894
  l1.bias: grad_norm = 0.000739
  l2.weight: grad_norm = 0.251063
Total gradient norm: 0.658725
=== Actor Training Debug (Iteration 9912) ===
Q mean: -14.252837
Q std: 20.192522
Actor loss: 14.256802
Action reg: 0.003965
  l1.weight: grad_norm = 0.355723
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.226472
Total gradient norm: 0.636752
=== Actor Training Debug (Iteration 9913) ===
Q mean: -14.685720
Q std: 20.851917
Actor loss: 14.689682
Action reg: 0.003962
  l1.weight: grad_norm = 0.363651
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.263331
Total gradient norm: 0.777054
=== Actor Training Debug (Iteration 9914) ===
Q mean: -12.839147
Q std: 19.288546
Actor loss: 12.843111
Action reg: 0.003964
  l1.weight: grad_norm = 0.247563
  l1.bias: grad_norm = 0.001092
  l2.weight: grad_norm = 0.165790
Total gradient norm: 0.414054
=== Actor Training Debug (Iteration 9915) ===
Q mean: -14.877719
Q std: 20.177591
Actor loss: 14.881694
Action reg: 0.003975
  l1.weight: grad_norm = 0.325571
  l1.bias: grad_norm = 0.000862
  l2.weight: grad_norm = 0.208443
Total gradient norm: 0.505682
=== Actor Training Debug (Iteration 9916) ===
Q mean: -14.874327
Q std: 21.246384
Actor loss: 14.878300
Action reg: 0.003973
  l1.weight: grad_norm = 0.411587
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.296012
Total gradient norm: 0.773381
=== Actor Training Debug (Iteration 9917) ===
Q mean: -17.108355
Q std: 22.260580
Actor loss: 17.112324
Action reg: 0.003970
  l1.weight: grad_norm = 0.252239
  l1.bias: grad_norm = 0.000609
  l2.weight: grad_norm = 0.175617
Total gradient norm: 0.427530
=== Actor Training Debug (Iteration 9918) ===
Q mean: -15.301333
Q std: 22.219959
Actor loss: 15.305301
Action reg: 0.003967
  l1.weight: grad_norm = 0.293054
  l1.bias: grad_norm = 0.000661
  l2.weight: grad_norm = 0.195641
Total gradient norm: 0.497843
=== Actor Training Debug (Iteration 9919) ===
Q mean: -14.210489
Q std: 21.190041
Actor loss: 14.214454
Action reg: 0.003965
  l1.weight: grad_norm = 0.336402
  l1.bias: grad_norm = 0.000428
  l2.weight: grad_norm = 0.254052
Total gradient norm: 0.656383
=== Actor Training Debug (Iteration 9920) ===
Q mean: -15.194291
Q std: 21.614985
Actor loss: 15.198247
Action reg: 0.003955
  l1.weight: grad_norm = 0.317037
  l1.bias: grad_norm = 0.001485
  l2.weight: grad_norm = 0.224729
Total gradient norm: 0.613354
=== Actor Training Debug (Iteration 9921) ===
Q mean: -15.368557
Q std: 21.705317
Actor loss: 15.372529
Action reg: 0.003972
  l1.weight: grad_norm = 0.215843
  l1.bias: grad_norm = 0.001745
  l2.weight: grad_norm = 0.180319
Total gradient norm: 0.538749
=== Actor Training Debug (Iteration 9922) ===
Q mean: -15.784479
Q std: 20.990007
Actor loss: 15.788447
Action reg: 0.003968
  l1.weight: grad_norm = 0.539189
  l1.bias: grad_norm = 0.000983
  l2.weight: grad_norm = 0.331999
Total gradient norm: 0.870266
=== Actor Training Debug (Iteration 9923) ===
Q mean: -15.635163
Q std: 21.785683
Actor loss: 15.639130
Action reg: 0.003967
  l1.weight: grad_norm = 0.885841
  l1.bias: grad_norm = 0.001120
  l2.weight: grad_norm = 0.508534
Total gradient norm: 1.559490
=== Actor Training Debug (Iteration 9924) ===
Q mean: -16.804638
Q std: 23.706007
Actor loss: 16.808596
Action reg: 0.003957
  l1.weight: grad_norm = 0.264487
  l1.bias: grad_norm = 0.005630
  l2.weight: grad_norm = 0.196759
Total gradient norm: 0.524435
=== Actor Training Debug (Iteration 9925) ===
Q mean: -13.626136
Q std: 20.098989
Actor loss: 13.630094
Action reg: 0.003958
  l1.weight: grad_norm = 0.494415
  l1.bias: grad_norm = 0.003887
  l2.weight: grad_norm = 0.374465
Total gradient norm: 1.397314
=== Actor Training Debug (Iteration 9926) ===
Q mean: -14.092180
Q std: 19.572533
Actor loss: 14.096149
Action reg: 0.003969
  l1.weight: grad_norm = 0.279530
  l1.bias: grad_norm = 0.000890
  l2.weight: grad_norm = 0.192101
Total gradient norm: 0.508344
=== Actor Training Debug (Iteration 9927) ===
Q mean: -15.792172
Q std: 21.251202
Actor loss: 15.796139
Action reg: 0.003966
  l1.weight: grad_norm = 0.209409
  l1.bias: grad_norm = 0.001359
  l2.weight: grad_norm = 0.153043
Total gradient norm: 0.465010
=== Actor Training Debug (Iteration 9928) ===
Q mean: -16.770494
Q std: 21.555288
Actor loss: 16.774452
Action reg: 0.003959
  l1.weight: grad_norm = 0.180798
  l1.bias: grad_norm = 0.001403
  l2.weight: grad_norm = 0.141033
Total gradient norm: 0.363010
=== Actor Training Debug (Iteration 9929) ===
Q mean: -15.719270
Q std: 20.151371
Actor loss: 15.723246
Action reg: 0.003976
  l1.weight: grad_norm = 0.139215
  l1.bias: grad_norm = 0.000939
  l2.weight: grad_norm = 0.129910
Total gradient norm: 0.363121
=== Actor Training Debug (Iteration 9930) ===
Q mean: -18.082432
Q std: 22.656910
Actor loss: 18.086418
Action reg: 0.003986
  l1.weight: grad_norm = 0.160898
  l1.bias: grad_norm = 0.001597
  l2.weight: grad_norm = 0.135074
Total gradient norm: 0.390269
=== Actor Training Debug (Iteration 9931) ===
Q mean: -16.655308
Q std: 24.110081
Actor loss: 16.659275
Action reg: 0.003967
  l1.weight: grad_norm = 0.492503
  l1.bias: grad_norm = 0.001250
  l2.weight: grad_norm = 0.315714
Total gradient norm: 0.858667
=== Actor Training Debug (Iteration 9932) ===
Q mean: -14.534540
Q std: 20.200935
Actor loss: 14.538521
Action reg: 0.003980
  l1.weight: grad_norm = 0.424091
  l1.bias: grad_norm = 0.000986
  l2.weight: grad_norm = 0.324507
Total gradient norm: 0.916418
=== Actor Training Debug (Iteration 9933) ===
Q mean: -12.831436
Q std: 17.888086
Actor loss: 12.835410
Action reg: 0.003974
  l1.weight: grad_norm = 0.263466
  l1.bias: grad_norm = 0.000912
  l2.weight: grad_norm = 0.215579
Total gradient norm: 0.583125
=== Actor Training Debug (Iteration 9934) ===
Q mean: -17.027014
Q std: 21.342382
Actor loss: 17.030991
Action reg: 0.003977
  l1.weight: grad_norm = 0.392573
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.251696
Total gradient norm: 0.796415
=== Actor Training Debug (Iteration 9935) ===
Q mean: -14.077135
Q std: 19.929564
Actor loss: 14.081101
Action reg: 0.003966
  l1.weight: grad_norm = 0.219150
  l1.bias: grad_norm = 0.001967
  l2.weight: grad_norm = 0.157459
Total gradient norm: 0.433988
=== Actor Training Debug (Iteration 9936) ===
Q mean: -14.384350
Q std: 20.324348
Actor loss: 14.388309
Action reg: 0.003959
  l1.weight: grad_norm = 0.334078
  l1.bias: grad_norm = 0.001732
  l2.weight: grad_norm = 0.232699
Total gradient norm: 0.750086
=== Actor Training Debug (Iteration 9937) ===
Q mean: -16.087132
Q std: 21.145741
Actor loss: 16.091105
Action reg: 0.003973
  l1.weight: grad_norm = 0.312388
  l1.bias: grad_norm = 0.000767
  l2.weight: grad_norm = 0.232446
Total gradient norm: 0.612449
=== Actor Training Debug (Iteration 9938) ===
Q mean: -15.010769
Q std: 20.665916
Actor loss: 15.014735
Action reg: 0.003967
  l1.weight: grad_norm = 0.300567
  l1.bias: grad_norm = 0.000988
  l2.weight: grad_norm = 0.224531
Total gradient norm: 0.578740
=== Actor Training Debug (Iteration 9939) ===
Q mean: -15.080339
Q std: 21.432634
Actor loss: 15.084308
Action reg: 0.003968
  l1.weight: grad_norm = 0.360459
  l1.bias: grad_norm = 0.003354
  l2.weight: grad_norm = 0.294372
Total gradient norm: 0.828626
=== Actor Training Debug (Iteration 9940) ===
Q mean: -13.669216
Q std: 18.892839
Actor loss: 13.673186
Action reg: 0.003970
  l1.weight: grad_norm = 0.176762
  l1.bias: grad_norm = 0.000807
  l2.weight: grad_norm = 0.134607
Total gradient norm: 0.381206
=== Actor Training Debug (Iteration 9941) ===
Q mean: -11.687984
Q std: 17.166687
Actor loss: 11.691927
Action reg: 0.003943
  l1.weight: grad_norm = 0.312982
  l1.bias: grad_norm = 0.001905
  l2.weight: grad_norm = 0.234106
Total gradient norm: 0.620043
=== Actor Training Debug (Iteration 9942) ===
Q mean: -15.612388
Q std: 22.011883
Actor loss: 15.616357
Action reg: 0.003969
  l1.weight: grad_norm = 0.200680
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.142920
Total gradient norm: 0.377807
=== Actor Training Debug (Iteration 9943) ===
Q mean: -13.493813
Q std: 19.761621
Actor loss: 13.497761
Action reg: 0.003948
  l1.weight: grad_norm = 0.301787
  l1.bias: grad_norm = 0.004357
  l2.weight: grad_norm = 0.250442
Total gradient norm: 0.621793
=== Actor Training Debug (Iteration 9944) ===
Q mean: -12.754559
Q std: 17.652546
Actor loss: 12.758545
Action reg: 0.003987
  l1.weight: grad_norm = 0.096192
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.080602
Total gradient norm: 0.207866
=== Actor Training Debug (Iteration 9945) ===
Q mean: -15.556190
Q std: 21.275398
Actor loss: 15.560160
Action reg: 0.003969
  l1.weight: grad_norm = 0.253926
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.205488
Total gradient norm: 0.525780
=== Actor Training Debug (Iteration 9946) ===
Q mean: -13.924307
Q std: 20.473320
Actor loss: 13.928282
Action reg: 0.003975
  l1.weight: grad_norm = 0.292892
  l1.bias: grad_norm = 0.000737
  l2.weight: grad_norm = 0.197582
Total gradient norm: 0.526247
=== Actor Training Debug (Iteration 9947) ===
Q mean: -14.036459
Q std: 19.782328
Actor loss: 14.040426
Action reg: 0.003967
  l1.weight: grad_norm = 0.407523
  l1.bias: grad_norm = 0.000827
  l2.weight: grad_norm = 0.265762
Total gradient norm: 0.686731
=== Actor Training Debug (Iteration 9948) ===
Q mean: -14.786045
Q std: 19.935196
Actor loss: 14.790008
Action reg: 0.003962
  l1.weight: grad_norm = 0.331342
  l1.bias: grad_norm = 0.001355
  l2.weight: grad_norm = 0.236346
Total gradient norm: 0.579059
=== Actor Training Debug (Iteration 9949) ===
Q mean: -15.985802
Q std: 21.737022
Actor loss: 15.989777
Action reg: 0.003975
  l1.weight: grad_norm = 0.245695
  l1.bias: grad_norm = 0.000734
  l2.weight: grad_norm = 0.201208
Total gradient norm: 0.547587
=== Actor Training Debug (Iteration 9950) ===
Q mean: -14.420366
Q std: 20.688118
Actor loss: 14.424327
Action reg: 0.003961
  l1.weight: grad_norm = 0.260517
  l1.bias: grad_norm = 0.000729
  l2.weight: grad_norm = 0.191050
Total gradient norm: 0.511020
=== Actor Training Debug (Iteration 9951) ===
Q mean: -14.548036
Q std: 19.599342
Actor loss: 14.551992
Action reg: 0.003957
  l1.weight: grad_norm = 0.240789
  l1.bias: grad_norm = 0.000949
  l2.weight: grad_norm = 0.173253
Total gradient norm: 0.465996
=== Actor Training Debug (Iteration 9952) ===
Q mean: -13.655297
Q std: 19.381977
Actor loss: 13.659270
Action reg: 0.003973
  l1.weight: grad_norm = 0.556606
  l1.bias: grad_norm = 0.000910
  l2.weight: grad_norm = 0.385607
Total gradient norm: 1.089849
=== Actor Training Debug (Iteration 9953) ===
Q mean: -14.591054
Q std: 21.071899
Actor loss: 14.595019
Action reg: 0.003966
  l1.weight: grad_norm = 0.350879
  l1.bias: grad_norm = 0.000525
  l2.weight: grad_norm = 0.240173
Total gradient norm: 0.694819
=== Actor Training Debug (Iteration 9954) ===
Q mean: -14.689904
Q std: 19.014986
Actor loss: 14.693857
Action reg: 0.003953
  l1.weight: grad_norm = 0.173421
  l1.bias: grad_norm = 0.002119
  l2.weight: grad_norm = 0.141393
Total gradient norm: 0.387637
=== Actor Training Debug (Iteration 9955) ===
Q mean: -14.925060
Q std: 20.525448
Actor loss: 14.929021
Action reg: 0.003960
  l1.weight: grad_norm = 0.207266
  l1.bias: grad_norm = 0.000793
  l2.weight: grad_norm = 0.162925
Total gradient norm: 0.436457
=== Actor Training Debug (Iteration 9956) ===
Q mean: -16.207785
Q std: 22.609432
Actor loss: 16.211752
Action reg: 0.003968
  l1.weight: grad_norm = 0.385656
  l1.bias: grad_norm = 0.001163
  l2.weight: grad_norm = 0.276839
Total gradient norm: 0.756368
=== Actor Training Debug (Iteration 9957) ===
Q mean: -15.571636
Q std: 22.048548
Actor loss: 15.575610
Action reg: 0.003974
  l1.weight: grad_norm = 0.174883
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.107762
Total gradient norm: 0.282417
=== Actor Training Debug (Iteration 9958) ===
Q mean: -16.502769
Q std: 22.252680
Actor loss: 16.506744
Action reg: 0.003974
  l1.weight: grad_norm = 0.372248
  l1.bias: grad_norm = 0.000654
  l2.weight: grad_norm = 0.255601
Total gradient norm: 0.756065
=== Actor Training Debug (Iteration 9959) ===
Q mean: -15.196733
Q std: 21.691133
Actor loss: 15.200690
Action reg: 0.003957
  l1.weight: grad_norm = 0.290637
  l1.bias: grad_norm = 0.001283
  l2.weight: grad_norm = 0.189124
Total gradient norm: 0.502593
=== Actor Training Debug (Iteration 9960) ===
Q mean: -15.661058
Q std: 21.771986
Actor loss: 15.665030
Action reg: 0.003971
  l1.weight: grad_norm = 0.147992
  l1.bias: grad_norm = 0.002318
  l2.weight: grad_norm = 0.109562
Total gradient norm: 0.294933
=== Actor Training Debug (Iteration 9961) ===
Q mean: -14.085634
Q std: 19.599443
Actor loss: 14.089593
Action reg: 0.003959
  l1.weight: grad_norm = 0.301584
  l1.bias: grad_norm = 0.001055
  l2.weight: grad_norm = 0.252717
Total gradient norm: 0.640508
=== Actor Training Debug (Iteration 9962) ===
Q mean: -17.083353
Q std: 23.089033
Actor loss: 17.087320
Action reg: 0.003968
  l1.weight: grad_norm = 0.235658
  l1.bias: grad_norm = 0.001209
  l2.weight: grad_norm = 0.173311
Total gradient norm: 0.462243
=== Actor Training Debug (Iteration 9963) ===
Q mean: -15.477427
Q std: 21.474613
Actor loss: 15.481398
Action reg: 0.003970
  l1.weight: grad_norm = 0.371172
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.326040
Total gradient norm: 0.921357
=== Actor Training Debug (Iteration 9964) ===
Q mean: -18.819189
Q std: 24.883604
Actor loss: 18.823162
Action reg: 0.003972
  l1.weight: grad_norm = 0.181014
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.130395
Total gradient norm: 0.329832
=== Actor Training Debug (Iteration 9965) ===
Q mean: -13.810639
Q std: 20.933100
Actor loss: 13.814612
Action reg: 0.003973
  l1.weight: grad_norm = 0.290322
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.227995
Total gradient norm: 0.607523
=== Actor Training Debug (Iteration 9966) ===
Q mean: -16.593958
Q std: 21.327845
Actor loss: 16.597925
Action reg: 0.003967
  l1.weight: grad_norm = 0.255147
  l1.bias: grad_norm = 0.000904
  l2.weight: grad_norm = 0.205897
Total gradient norm: 0.547941
=== Actor Training Debug (Iteration 9967) ===
Q mean: -16.818195
Q std: 22.740698
Actor loss: 16.822163
Action reg: 0.003966
  l1.weight: grad_norm = 0.147278
  l1.bias: grad_norm = 0.001248
  l2.weight: grad_norm = 0.121500
Total gradient norm: 0.341649
=== Actor Training Debug (Iteration 9968) ===
Q mean: -15.609290
Q std: 20.016554
Actor loss: 15.613258
Action reg: 0.003969
  l1.weight: grad_norm = 0.329756
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.214185
Total gradient norm: 0.670883
=== Actor Training Debug (Iteration 9969) ===
Q mean: -13.767143
Q std: 19.360909
Actor loss: 13.771119
Action reg: 0.003976
  l1.weight: grad_norm = 0.259991
  l1.bias: grad_norm = 0.000678
  l2.weight: grad_norm = 0.168116
Total gradient norm: 0.440957
=== Actor Training Debug (Iteration 9970) ===
Q mean: -14.440062
Q std: 18.744358
Actor loss: 14.444028
Action reg: 0.003967
  l1.weight: grad_norm = 0.329386
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.243706
Total gradient norm: 0.653495
=== Actor Training Debug (Iteration 9971) ===
Q mean: -13.717546
Q std: 18.289501
Actor loss: 13.721508
Action reg: 0.003963
  l1.weight: grad_norm = 0.315987
  l1.bias: grad_norm = 0.001821
  l2.weight: grad_norm = 0.260174
Total gradient norm: 0.667465
=== Actor Training Debug (Iteration 9972) ===
Q mean: -14.311434
Q std: 19.149048
Actor loss: 14.315404
Action reg: 0.003970
  l1.weight: grad_norm = 0.400521
  l1.bias: grad_norm = 0.000772
  l2.weight: grad_norm = 0.284829
Total gradient norm: 0.860517
=== Actor Training Debug (Iteration 9973) ===
Q mean: -14.437170
Q std: 19.191685
Actor loss: 14.441128
Action reg: 0.003957
  l1.weight: grad_norm = 0.417463
  l1.bias: grad_norm = 0.001724
  l2.weight: grad_norm = 0.274306
Total gradient norm: 0.689491
=== Actor Training Debug (Iteration 9974) ===
Q mean: -13.184486
Q std: 20.626749
Actor loss: 13.188445
Action reg: 0.003959
  l1.weight: grad_norm = 0.280897
  l1.bias: grad_norm = 0.001221
  l2.weight: grad_norm = 0.217205
Total gradient norm: 0.593580
=== Actor Training Debug (Iteration 9975) ===
Q mean: -12.572902
Q std: 18.858477
Actor loss: 12.576870
Action reg: 0.003968
  l1.weight: grad_norm = 0.364951
  l1.bias: grad_norm = 0.001128
  l2.weight: grad_norm = 0.226836
Total gradient norm: 0.643086
=== Actor Training Debug (Iteration 9976) ===
Q mean: -16.359167
Q std: 20.630444
Actor loss: 16.363129
Action reg: 0.003962
  l1.weight: grad_norm = 0.260490
  l1.bias: grad_norm = 0.001377
  l2.weight: grad_norm = 0.215023
Total gradient norm: 0.616557
=== Actor Training Debug (Iteration 9977) ===
Q mean: -16.797295
Q std: 22.128418
Actor loss: 16.801268
Action reg: 0.003973
  l1.weight: grad_norm = 0.298062
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.243979
Total gradient norm: 0.591495
=== Actor Training Debug (Iteration 9978) ===
Q mean: -12.772604
Q std: 17.768900
Actor loss: 12.776544
Action reg: 0.003939
  l1.weight: grad_norm = 0.192479
  l1.bias: grad_norm = 0.002568
  l2.weight: grad_norm = 0.152881
Total gradient norm: 0.412916
=== Actor Training Debug (Iteration 9979) ===
Q mean: -15.578015
Q std: 21.795263
Actor loss: 15.581967
Action reg: 0.003952
  l1.weight: grad_norm = 0.215080
  l1.bias: grad_norm = 0.001622
  l2.weight: grad_norm = 0.169105
Total gradient norm: 0.560557
=== Actor Training Debug (Iteration 9980) ===
Q mean: -13.428581
Q std: 18.485210
Actor loss: 13.432533
Action reg: 0.003952
  l1.weight: grad_norm = 0.452889
  l1.bias: grad_norm = 0.002303
  l2.weight: grad_norm = 0.292535
Total gradient norm: 0.827671
=== Actor Training Debug (Iteration 9981) ===
Q mean: -14.534020
Q std: 21.842129
Actor loss: 14.537966
Action reg: 0.003945
  l1.weight: grad_norm = 0.209566
  l1.bias: grad_norm = 0.001623
  l2.weight: grad_norm = 0.170250
Total gradient norm: 0.486842
=== Actor Training Debug (Iteration 9982) ===
Q mean: -16.877577
Q std: 22.813263
Actor loss: 16.881533
Action reg: 0.003955
  l1.weight: grad_norm = 0.354003
  l1.bias: grad_norm = 0.002214
  l2.weight: grad_norm = 0.242558
Total gradient norm: 0.595459
=== Actor Training Debug (Iteration 9983) ===
Q mean: -16.065266
Q std: 22.006975
Actor loss: 16.069233
Action reg: 0.003967
  l1.weight: grad_norm = 0.130820
  l1.bias: grad_norm = 0.004315
  l2.weight: grad_norm = 0.104868
Total gradient norm: 0.300811
=== Actor Training Debug (Iteration 9984) ===
Q mean: -15.982821
Q std: 20.950251
Actor loss: 15.986784
Action reg: 0.003962
  l1.weight: grad_norm = 0.351832
  l1.bias: grad_norm = 0.000803
  l2.weight: grad_norm = 0.241753
Total gradient norm: 0.598441
=== Actor Training Debug (Iteration 9985) ===
Q mean: -13.829771
Q std: 20.808786
Actor loss: 13.833737
Action reg: 0.003966
  l1.weight: grad_norm = 0.292829
  l1.bias: grad_norm = 0.002255
  l2.weight: grad_norm = 0.195500
Total gradient norm: 0.509091
=== Actor Training Debug (Iteration 9986) ===
Q mean: -15.675279
Q std: 21.403162
Actor loss: 15.679235
Action reg: 0.003956
  l1.weight: grad_norm = 0.272924
  l1.bias: grad_norm = 0.002020
  l2.weight: grad_norm = 0.216164
Total gradient norm: 0.572653
=== Actor Training Debug (Iteration 9987) ===
Q mean: -16.286016
Q std: 21.045910
Actor loss: 16.289984
Action reg: 0.003968
  l1.weight: grad_norm = 0.230104
  l1.bias: grad_norm = 0.002045
  l2.weight: grad_norm = 0.192398
Total gradient norm: 0.499307
=== Actor Training Debug (Iteration 9988) ===
Q mean: -14.208292
Q std: 19.798969
Actor loss: 14.212255
Action reg: 0.003963
  l1.weight: grad_norm = 0.253217
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.174845
Total gradient norm: 0.469668
=== Actor Training Debug (Iteration 9989) ===
Q mean: -14.969839
Q std: 19.635103
Actor loss: 14.973796
Action reg: 0.003957
  l1.weight: grad_norm = 0.277739
  l1.bias: grad_norm = 0.001696
  l2.weight: grad_norm = 0.216724
Total gradient norm: 0.578528
=== Actor Training Debug (Iteration 9990) ===
Q mean: -12.246962
Q std: 18.120377
Actor loss: 12.250923
Action reg: 0.003962
  l1.weight: grad_norm = 0.253104
  l1.bias: grad_norm = 0.003194
  l2.weight: grad_norm = 0.194619
Total gradient norm: 0.547753
=== Actor Training Debug (Iteration 9991) ===
Q mean: -14.750368
Q std: 18.879551
Actor loss: 14.754329
Action reg: 0.003961
  l1.weight: grad_norm = 0.469964
  l1.bias: grad_norm = 0.001287
  l2.weight: grad_norm = 0.270768
Total gradient norm: 0.733059
=== Actor Training Debug (Iteration 9992) ===
Q mean: -16.841038
Q std: 22.389395
Actor loss: 16.845007
Action reg: 0.003969
  l1.weight: grad_norm = 0.314621
  l1.bias: grad_norm = 0.001408
  l2.weight: grad_norm = 0.226269
Total gradient norm: 0.599301
=== Actor Training Debug (Iteration 9993) ===
Q mean: -15.648506
Q std: 19.890413
Actor loss: 15.652465
Action reg: 0.003958
  l1.weight: grad_norm = 0.307497
  l1.bias: grad_norm = 0.001414
  l2.weight: grad_norm = 0.258598
Total gradient norm: 0.705406
=== Actor Training Debug (Iteration 9994) ===
Q mean: -12.923333
Q std: 18.084854
Actor loss: 12.927306
Action reg: 0.003973
  l1.weight: grad_norm = 0.244952
  l1.bias: grad_norm = 0.000965
  l2.weight: grad_norm = 0.161265
Total gradient norm: 0.432098
=== Actor Training Debug (Iteration 9995) ===
Q mean: -16.327255
Q std: 21.944252
Actor loss: 16.331219
Action reg: 0.003963
  l1.weight: grad_norm = 0.373431
  l1.bias: grad_norm = 0.001326
  l2.weight: grad_norm = 0.264236
Total gradient norm: 0.708516
=== Actor Training Debug (Iteration 9996) ===
Q mean: -14.192509
Q std: 20.176317
Actor loss: 14.196461
Action reg: 0.003952
  l1.weight: grad_norm = 0.337563
  l1.bias: grad_norm = 0.001882
  l2.weight: grad_norm = 0.247072
Total gradient norm: 0.651636
=== Actor Training Debug (Iteration 9997) ===
Q mean: -15.716670
Q std: 21.539219
Actor loss: 15.720650
Action reg: 0.003980
  l1.weight: grad_norm = 0.543791
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.364947
Total gradient norm: 0.953723
=== Actor Training Debug (Iteration 9998) ===
Q mean: -13.912292
Q std: 19.343426
Actor loss: 13.916265
Action reg: 0.003974
  l1.weight: grad_norm = 0.274423
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.198744
Total gradient norm: 0.556580
=== Actor Training Debug (Iteration 9999) ===
Q mean: -14.372908
Q std: 19.485338
Actor loss: 14.376865
Action reg: 0.003957
  l1.weight: grad_norm = 0.419040
  l1.bias: grad_norm = 0.001531
  l2.weight: grad_norm = 0.299004
Total gradient norm: 0.772310
=== Actor Training Debug (Iteration 10000) ===
Q mean: -15.505682
Q std: 19.373840
Actor loss: 15.509645
Action reg: 0.003963
  l1.weight: grad_norm = 0.111481
  l1.bias: grad_norm = 0.002573
  l2.weight: grad_norm = 0.097472
Total gradient norm: 0.265502
Step 15000: Critic Loss: 1.6867, Actor Loss: 15.5096, Q Value: -15.5057
  Average reward: -321.835 | Average length: 100.0
Evaluation at episode 150: -321.835
=== Actor Training Debug (Iteration 10001) ===
Q mean: -14.519819
Q std: 21.564190
Actor loss: 14.523782
Action reg: 0.003962
  l1.weight: grad_norm = 0.337446
  l1.bias: grad_norm = 0.001065
  l2.weight: grad_norm = 0.236165
Total gradient norm: 0.590507
=== Actor Training Debug (Iteration 10002) ===
Q mean: -15.691436
Q std: 21.962429
Actor loss: 15.695410
Action reg: 0.003974
  l1.weight: grad_norm = 0.283876
  l1.bias: grad_norm = 0.001049
  l2.weight: grad_norm = 0.197642
Total gradient norm: 0.505768
=== Actor Training Debug (Iteration 10003) ===
Q mean: -14.536052
Q std: 19.919308
Actor loss: 14.540028
Action reg: 0.003976
  l1.weight: grad_norm = 0.175059
  l1.bias: grad_norm = 0.001113
  l2.weight: grad_norm = 0.136661
Total gradient norm: 0.335830
=== Actor Training Debug (Iteration 10004) ===
Q mean: -13.513275
Q std: 17.940575
Actor loss: 13.517245
Action reg: 0.003970
  l1.weight: grad_norm = 0.500709
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.344976
Total gradient norm: 0.852255
=== Actor Training Debug (Iteration 10005) ===
Q mean: -14.692721
Q std: 21.598553
Actor loss: 14.696677
Action reg: 0.003956
  l1.weight: grad_norm = 0.369739
  l1.bias: grad_norm = 0.001645
  l2.weight: grad_norm = 0.243740
Total gradient norm: 0.660827
=== Actor Training Debug (Iteration 10006) ===
Q mean: -13.571434
Q std: 19.954645
Actor loss: 13.575394
Action reg: 0.003960
  l1.weight: grad_norm = 0.364866
  l1.bias: grad_norm = 0.001388
  l2.weight: grad_norm = 0.260481
Total gradient norm: 0.791266
=== Actor Training Debug (Iteration 10007) ===
Q mean: -16.418442
Q std: 22.013430
Actor loss: 16.422396
Action reg: 0.003953
  l1.weight: grad_norm = 0.637414
  l1.bias: grad_norm = 0.001293
  l2.weight: grad_norm = 0.460612
Total gradient norm: 1.166482
=== Actor Training Debug (Iteration 10008) ===
Q mean: -17.139185
Q std: 22.672178
Actor loss: 17.143154
Action reg: 0.003968
  l1.weight: grad_norm = 0.275269
  l1.bias: grad_norm = 0.000893
  l2.weight: grad_norm = 0.203550
Total gradient norm: 0.520890
=== Actor Training Debug (Iteration 10009) ===
Q mean: -15.043158
Q std: 20.939983
Actor loss: 15.047111
Action reg: 0.003953
  l1.weight: grad_norm = 0.375823
  l1.bias: grad_norm = 0.002585
  l2.weight: grad_norm = 0.227765
Total gradient norm: 0.638734
=== Actor Training Debug (Iteration 10010) ===
Q mean: -15.880085
Q std: 20.735729
Actor loss: 15.884053
Action reg: 0.003968
  l1.weight: grad_norm = 0.360201
  l1.bias: grad_norm = 0.001097
  l2.weight: grad_norm = 0.293106
Total gradient norm: 0.786601
=== Actor Training Debug (Iteration 10011) ===
Q mean: -14.740263
Q std: 20.561798
Actor loss: 14.744205
Action reg: 0.003943
  l1.weight: grad_norm = 0.332698
  l1.bias: grad_norm = 0.001927
  l2.weight: grad_norm = 0.235469
Total gradient norm: 0.606079
=== Actor Training Debug (Iteration 10012) ===
Q mean: -12.217228
Q std: 19.177679
Actor loss: 12.221182
Action reg: 0.003954
  l1.weight: grad_norm = 0.329085
  l1.bias: grad_norm = 0.002734
  l2.weight: grad_norm = 0.276361
Total gradient norm: 0.759224
=== Actor Training Debug (Iteration 10013) ===
Q mean: -13.050562
Q std: 19.792400
Actor loss: 13.054531
Action reg: 0.003969
  l1.weight: grad_norm = 0.394569
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.286562
Total gradient norm: 0.787936
=== Actor Training Debug (Iteration 10014) ===
Q mean: -13.261580
Q std: 18.154305
Actor loss: 13.265541
Action reg: 0.003962
  l1.weight: grad_norm = 0.238227
  l1.bias: grad_norm = 0.006349
  l2.weight: grad_norm = 0.163764
Total gradient norm: 0.477544
=== Actor Training Debug (Iteration 10015) ===
Q mean: -13.540249
Q std: 18.548235
Actor loss: 13.544216
Action reg: 0.003968
  l1.weight: grad_norm = 0.182243
  l1.bias: grad_norm = 0.002279
  l2.weight: grad_norm = 0.163127
Total gradient norm: 0.435910
=== Actor Training Debug (Iteration 10016) ===
Q mean: -14.570372
Q std: 19.686598
Actor loss: 14.574336
Action reg: 0.003965
  l1.weight: grad_norm = 0.291786
  l1.bias: grad_norm = 0.003866
  l2.weight: grad_norm = 0.218658
Total gradient norm: 0.578981
=== Actor Training Debug (Iteration 10017) ===
Q mean: -13.600205
Q std: 19.809059
Actor loss: 13.604165
Action reg: 0.003960
  l1.weight: grad_norm = 0.374443
  l1.bias: grad_norm = 0.000862
  l2.weight: grad_norm = 0.275954
Total gradient norm: 0.698552
=== Actor Training Debug (Iteration 10018) ===
Q mean: -12.897186
Q std: 19.804314
Actor loss: 12.901141
Action reg: 0.003955
  l1.weight: grad_norm = 0.249338
  l1.bias: grad_norm = 0.002276
  l2.weight: grad_norm = 0.176395
Total gradient norm: 0.458271
=== Actor Training Debug (Iteration 10019) ===
Q mean: -16.091717
Q std: 23.276781
Actor loss: 16.095671
Action reg: 0.003954
  l1.weight: grad_norm = 0.338378
  l1.bias: grad_norm = 0.002127
  l2.weight: grad_norm = 0.239846
Total gradient norm: 0.665362
=== Actor Training Debug (Iteration 10020) ===
Q mean: -16.951923
Q std: 22.493893
Actor loss: 16.955885
Action reg: 0.003961
  l1.weight: grad_norm = 0.363041
  l1.bias: grad_norm = 0.003183
  l2.weight: grad_norm = 0.306415
Total gradient norm: 0.807789
=== Actor Training Debug (Iteration 10021) ===
Q mean: -12.393553
Q std: 18.210758
Actor loss: 12.397503
Action reg: 0.003950
  l1.weight: grad_norm = 0.357972
  l1.bias: grad_norm = 0.002104
  l2.weight: grad_norm = 0.280659
Total gradient norm: 0.904699
=== Actor Training Debug (Iteration 10022) ===
Q mean: -15.740545
Q std: 21.759363
Actor loss: 15.744520
Action reg: 0.003975
  l1.weight: grad_norm = 0.255635
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.172952
Total gradient norm: 0.428100
=== Actor Training Debug (Iteration 10023) ===
Q mean: -13.411928
Q std: 19.841116
Actor loss: 13.415892
Action reg: 0.003963
  l1.weight: grad_norm = 0.253682
  l1.bias: grad_norm = 0.002479
  l2.weight: grad_norm = 0.205196
Total gradient norm: 0.495879
=== Actor Training Debug (Iteration 10024) ===
Q mean: -16.117617
Q std: 22.038038
Actor loss: 16.121601
Action reg: 0.003984
  l1.weight: grad_norm = 0.157850
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.132304
Total gradient norm: 0.349409
=== Actor Training Debug (Iteration 10025) ===
Q mean: -15.497581
Q std: 21.360115
Actor loss: 15.501546
Action reg: 0.003964
  l1.weight: grad_norm = 0.411377
  l1.bias: grad_norm = 0.000985
  l2.weight: grad_norm = 0.314213
Total gradient norm: 0.863620
=== Actor Training Debug (Iteration 10026) ===
Q mean: -14.413527
Q std: 20.107851
Actor loss: 14.417480
Action reg: 0.003954
  l1.weight: grad_norm = 0.275439
  l1.bias: grad_norm = 0.001445
  l2.weight: grad_norm = 0.214673
Total gradient norm: 0.526322
=== Actor Training Debug (Iteration 10027) ===
Q mean: -13.076401
Q std: 18.841261
Actor loss: 13.080364
Action reg: 0.003964
  l1.weight: grad_norm = 0.147851
  l1.bias: grad_norm = 0.001975
  l2.weight: grad_norm = 0.105706
Total gradient norm: 0.272785
=== Actor Training Debug (Iteration 10028) ===
Q mean: -12.917273
Q std: 19.501644
Actor loss: 12.921215
Action reg: 0.003942
  l1.weight: grad_norm = 0.316643
  l1.bias: grad_norm = 0.001825
  l2.weight: grad_norm = 0.264681
Total gradient norm: 0.629252
=== Actor Training Debug (Iteration 10029) ===
Q mean: -14.189697
Q std: 20.047493
Actor loss: 14.193649
Action reg: 0.003952
  l1.weight: grad_norm = 0.228051
  l1.bias: grad_norm = 0.003300
  l2.weight: grad_norm = 0.173224
Total gradient norm: 0.453703
=== Actor Training Debug (Iteration 10030) ===
Q mean: -13.050528
Q std: 19.043186
Actor loss: 13.054495
Action reg: 0.003967
  l1.weight: grad_norm = 0.281520
  l1.bias: grad_norm = 0.000999
  l2.weight: grad_norm = 0.196152
Total gradient norm: 0.522195
=== Actor Training Debug (Iteration 10031) ===
Q mean: -13.615742
Q std: 20.164322
Actor loss: 13.619713
Action reg: 0.003971
  l1.weight: grad_norm = 0.214408
  l1.bias: grad_norm = 0.000853
  l2.weight: grad_norm = 0.145878
Total gradient norm: 0.446989
=== Actor Training Debug (Iteration 10032) ===
Q mean: -16.043640
Q std: 21.078077
Actor loss: 16.047590
Action reg: 0.003950
  l1.weight: grad_norm = 0.321211
  l1.bias: grad_norm = 0.001106
  l2.weight: grad_norm = 0.250204
Total gradient norm: 0.668656
=== Actor Training Debug (Iteration 10033) ===
Q mean: -14.483785
Q std: 20.316109
Actor loss: 14.487757
Action reg: 0.003972
  l1.weight: grad_norm = 0.361157
  l1.bias: grad_norm = 0.002041
  l2.weight: grad_norm = 0.242438
Total gradient norm: 0.676833
=== Actor Training Debug (Iteration 10034) ===
Q mean: -14.457416
Q std: 18.893177
Actor loss: 14.461381
Action reg: 0.003965
  l1.weight: grad_norm = 0.396790
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.268417
Total gradient norm: 0.743292
=== Actor Training Debug (Iteration 10035) ===
Q mean: -14.118697
Q std: 20.303370
Actor loss: 14.122670
Action reg: 0.003973
  l1.weight: grad_norm = 0.189958
  l1.bias: grad_norm = 0.000808
  l2.weight: grad_norm = 0.146314
Total gradient norm: 0.392263
=== Actor Training Debug (Iteration 10036) ===
Q mean: -15.977417
Q std: 21.062510
Actor loss: 15.981385
Action reg: 0.003968
  l1.weight: grad_norm = 0.245689
  l1.bias: grad_norm = 0.000872
  l2.weight: grad_norm = 0.175639
Total gradient norm: 0.433210
=== Actor Training Debug (Iteration 10037) ===
Q mean: -13.925148
Q std: 19.308565
Actor loss: 13.929122
Action reg: 0.003974
  l1.weight: grad_norm = 0.274458
  l1.bias: grad_norm = 0.000877
  l2.weight: grad_norm = 0.207494
Total gradient norm: 0.529270
=== Actor Training Debug (Iteration 10038) ===
Q mean: -14.516027
Q std: 20.825077
Actor loss: 14.519977
Action reg: 0.003949
  l1.weight: grad_norm = 0.194262
  l1.bias: grad_norm = 0.002411
  l2.weight: grad_norm = 0.148782
Total gradient norm: 0.418511
=== Actor Training Debug (Iteration 10039) ===
Q mean: -14.295862
Q std: 20.878517
Actor loss: 14.299827
Action reg: 0.003964
  l1.weight: grad_norm = 0.366822
  l1.bias: grad_norm = 0.000842
  l2.weight: grad_norm = 0.233213
Total gradient norm: 0.624424
=== Actor Training Debug (Iteration 10040) ===
Q mean: -17.046154
Q std: 22.100796
Actor loss: 17.050125
Action reg: 0.003972
  l1.weight: grad_norm = 0.427445
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.346832
Total gradient norm: 1.148679
=== Actor Training Debug (Iteration 10041) ===
Q mean: -15.056719
Q std: 21.150595
Actor loss: 15.060684
Action reg: 0.003965
  l1.weight: grad_norm = 0.580326
  l1.bias: grad_norm = 0.000765
  l2.weight: grad_norm = 0.377029
Total gradient norm: 0.990486
=== Actor Training Debug (Iteration 10042) ===
Q mean: -14.640280
Q std: 19.514639
Actor loss: 14.644238
Action reg: 0.003959
  l1.weight: grad_norm = 0.336490
  l1.bias: grad_norm = 0.001420
  l2.weight: grad_norm = 0.267705
Total gradient norm: 0.728921
=== Actor Training Debug (Iteration 10043) ===
Q mean: -14.112833
Q std: 20.858906
Actor loss: 14.116808
Action reg: 0.003974
  l1.weight: grad_norm = 0.454317
  l1.bias: grad_norm = 0.000556
  l2.weight: grad_norm = 0.354558
Total gradient norm: 1.073304
=== Actor Training Debug (Iteration 10044) ===
Q mean: -13.914807
Q std: 19.590673
Actor loss: 13.918754
Action reg: 0.003946
  l1.weight: grad_norm = 0.266863
  l1.bias: grad_norm = 0.002377
  l2.weight: grad_norm = 0.212873
Total gradient norm: 0.531510
=== Actor Training Debug (Iteration 10045) ===
Q mean: -14.806130
Q std: 21.359842
Actor loss: 14.810081
Action reg: 0.003951
  l1.weight: grad_norm = 0.428558
  l1.bias: grad_norm = 0.001656
  l2.weight: grad_norm = 0.292070
Total gradient norm: 0.800555
=== Actor Training Debug (Iteration 10046) ===
Q mean: -14.653334
Q std: 21.430656
Actor loss: 14.657297
Action reg: 0.003964
  l1.weight: grad_norm = 0.209521
  l1.bias: grad_norm = 0.001132
  l2.weight: grad_norm = 0.148259
Total gradient norm: 0.383592
=== Actor Training Debug (Iteration 10047) ===
Q mean: -13.918142
Q std: 18.710247
Actor loss: 13.922101
Action reg: 0.003959
  l1.weight: grad_norm = 0.243259
  l1.bias: grad_norm = 0.004906
  l2.weight: grad_norm = 0.236006
Total gradient norm: 0.665602
=== Actor Training Debug (Iteration 10048) ===
Q mean: -16.273602
Q std: 23.567619
Actor loss: 16.277565
Action reg: 0.003964
  l1.weight: grad_norm = 0.169214
  l1.bias: grad_norm = 0.001772
  l2.weight: grad_norm = 0.136210
Total gradient norm: 0.395224
=== Actor Training Debug (Iteration 10049) ===
Q mean: -15.546988
Q std: 20.837433
Actor loss: 15.550935
Action reg: 0.003947
  l1.weight: grad_norm = 0.273218
  l1.bias: grad_norm = 0.002462
  l2.weight: grad_norm = 0.178357
Total gradient norm: 0.540730
=== Actor Training Debug (Iteration 10050) ===
Q mean: -15.136528
Q std: 20.222460
Actor loss: 15.140483
Action reg: 0.003955
  l1.weight: grad_norm = 0.191956
  l1.bias: grad_norm = 0.002383
  l2.weight: grad_norm = 0.158791
Total gradient norm: 0.414873
=== Actor Training Debug (Iteration 10051) ===
Q mean: -15.908388
Q std: 22.800484
Actor loss: 15.912350
Action reg: 0.003961
  l1.weight: grad_norm = 0.212769
  l1.bias: grad_norm = 0.004099
  l2.weight: grad_norm = 0.152548
Total gradient norm: 0.446291
=== Actor Training Debug (Iteration 10052) ===
Q mean: -11.797203
Q std: 17.662704
Actor loss: 11.801169
Action reg: 0.003966
  l1.weight: grad_norm = 0.227225
  l1.bias: grad_norm = 0.001184
  l2.weight: grad_norm = 0.180287
Total gradient norm: 0.454799
=== Actor Training Debug (Iteration 10053) ===
Q mean: -14.139330
Q std: 20.608828
Actor loss: 14.143272
Action reg: 0.003943
  l1.weight: grad_norm = 0.252659
  l1.bias: grad_norm = 0.002462
  l2.weight: grad_norm = 0.212444
Total gradient norm: 0.628124
=== Actor Training Debug (Iteration 10054) ===
Q mean: -16.735832
Q std: 22.035986
Actor loss: 16.739801
Action reg: 0.003968
  l1.weight: grad_norm = 0.348388
  l1.bias: grad_norm = 0.001596
  l2.weight: grad_norm = 0.280633
Total gradient norm: 0.754954
=== Actor Training Debug (Iteration 10055) ===
Q mean: -14.747099
Q std: 21.109632
Actor loss: 14.751086
Action reg: 0.003987
  l1.weight: grad_norm = 0.185690
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.134467
Total gradient norm: 0.318347
=== Actor Training Debug (Iteration 10056) ===
Q mean: -16.135624
Q std: 22.223669
Actor loss: 16.139589
Action reg: 0.003966
  l1.weight: grad_norm = 0.301275
  l1.bias: grad_norm = 0.000560
  l2.weight: grad_norm = 0.184436
Total gradient norm: 0.508295
=== Actor Training Debug (Iteration 10057) ===
Q mean: -15.603683
Q std: 20.444992
Actor loss: 15.607647
Action reg: 0.003964
  l1.weight: grad_norm = 0.246169
  l1.bias: grad_norm = 0.001336
  l2.weight: grad_norm = 0.187589
Total gradient norm: 0.476991
=== Actor Training Debug (Iteration 10058) ===
Q mean: -14.690053
Q std: 20.338697
Actor loss: 14.693988
Action reg: 0.003935
  l1.weight: grad_norm = 0.383396
  l1.bias: grad_norm = 0.005524
  l2.weight: grad_norm = 0.243565
Total gradient norm: 0.669051
=== Actor Training Debug (Iteration 10059) ===
Q mean: -14.667901
Q std: 20.283485
Actor loss: 14.671871
Action reg: 0.003970
  l1.weight: grad_norm = 0.161320
  l1.bias: grad_norm = 0.002654
  l2.weight: grad_norm = 0.129238
Total gradient norm: 0.328684
=== Actor Training Debug (Iteration 10060) ===
Q mean: -15.985567
Q std: 20.489895
Actor loss: 15.989531
Action reg: 0.003963
  l1.weight: grad_norm = 0.185765
  l1.bias: grad_norm = 0.002629
  l2.weight: grad_norm = 0.124279
Total gradient norm: 0.327612
=== Actor Training Debug (Iteration 10061) ===
Q mean: -16.109125
Q std: 21.811234
Actor loss: 16.113087
Action reg: 0.003962
  l1.weight: grad_norm = 0.327053
  l1.bias: grad_norm = 0.002440
  l2.weight: grad_norm = 0.256670
Total gradient norm: 0.788484
=== Actor Training Debug (Iteration 10062) ===
Q mean: -15.007576
Q std: 21.729954
Actor loss: 15.011529
Action reg: 0.003953
  l1.weight: grad_norm = 0.402026
  l1.bias: grad_norm = 0.003767
  l2.weight: grad_norm = 0.296606
Total gradient norm: 0.755932
=== Actor Training Debug (Iteration 10063) ===
Q mean: -17.631641
Q std: 22.652699
Actor loss: 17.635618
Action reg: 0.003977
  l1.weight: grad_norm = 0.211577
  l1.bias: grad_norm = 0.001171
  l2.weight: grad_norm = 0.155178
Total gradient norm: 0.442431
=== Actor Training Debug (Iteration 10064) ===
Q mean: -14.493078
Q std: 20.629963
Actor loss: 14.497035
Action reg: 0.003957
  l1.weight: grad_norm = 0.525274
  l1.bias: grad_norm = 0.001786
  l2.weight: grad_norm = 0.391433
Total gradient norm: 1.071871
=== Actor Training Debug (Iteration 10065) ===
Q mean: -13.322268
Q std: 18.324076
Actor loss: 13.326235
Action reg: 0.003967
  l1.weight: grad_norm = 0.478086
  l1.bias: grad_norm = 0.003089
  l2.weight: grad_norm = 0.388854
Total gradient norm: 1.157810
=== Actor Training Debug (Iteration 10066) ===
Q mean: -16.876654
Q std: 21.558607
Actor loss: 16.880608
Action reg: 0.003954
  l1.weight: grad_norm = 0.384406
  l1.bias: grad_norm = 0.003690
  l2.weight: grad_norm = 0.317398
Total gradient norm: 0.862398
=== Actor Training Debug (Iteration 10067) ===
Q mean: -16.628819
Q std: 21.285975
Actor loss: 16.632782
Action reg: 0.003964
  l1.weight: grad_norm = 0.509879
  l1.bias: grad_norm = 0.001503
  l2.weight: grad_norm = 0.401930
Total gradient norm: 1.263362
=== Actor Training Debug (Iteration 10068) ===
Q mean: -14.323877
Q std: 21.628330
Actor loss: 14.327840
Action reg: 0.003962
  l1.weight: grad_norm = 0.244192
  l1.bias: grad_norm = 0.002190
  l2.weight: grad_norm = 0.182658
Total gradient norm: 0.484134
=== Actor Training Debug (Iteration 10069) ===
Q mean: -15.622963
Q std: 21.277407
Actor loss: 15.626921
Action reg: 0.003958
  l1.weight: grad_norm = 0.299943
  l1.bias: grad_norm = 0.001648
  l2.weight: grad_norm = 0.230398
Total gradient norm: 0.604056
=== Actor Training Debug (Iteration 10070) ===
Q mean: -17.601925
Q std: 23.297688
Actor loss: 17.605875
Action reg: 0.003951
  l1.weight: grad_norm = 0.304134
  l1.bias: grad_norm = 0.002180
  l2.weight: grad_norm = 0.203043
Total gradient norm: 0.546691
=== Actor Training Debug (Iteration 10071) ===
Q mean: -14.873664
Q std: 20.787428
Actor loss: 14.877634
Action reg: 0.003970
  l1.weight: grad_norm = 0.269148
  l1.bias: grad_norm = 0.000862
  l2.weight: grad_norm = 0.188463
Total gradient norm: 0.497158
=== Actor Training Debug (Iteration 10072) ===
Q mean: -14.611945
Q std: 20.452440
Actor loss: 14.615893
Action reg: 0.003948
  l1.weight: grad_norm = 0.454088
  l1.bias: grad_norm = 0.001807
  l2.weight: grad_norm = 0.324739
Total gradient norm: 0.908169
=== Actor Training Debug (Iteration 10073) ===
Q mean: -16.138615
Q std: 22.408102
Actor loss: 16.142578
Action reg: 0.003964
  l1.weight: grad_norm = 0.293463
  l1.bias: grad_norm = 0.000657
  l2.weight: grad_norm = 0.232872
Total gradient norm: 0.611887
=== Actor Training Debug (Iteration 10074) ===
Q mean: -14.685831
Q std: 19.930540
Actor loss: 14.689783
Action reg: 0.003952
  l1.weight: grad_norm = 0.124616
  l1.bias: grad_norm = 0.002814
  l2.weight: grad_norm = 0.089487
Total gradient norm: 0.261260
=== Actor Training Debug (Iteration 10075) ===
Q mean: -15.746882
Q std: 20.483225
Actor loss: 15.750856
Action reg: 0.003974
  l1.weight: grad_norm = 0.166150
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.106150
Total gradient norm: 0.301531
=== Actor Training Debug (Iteration 10076) ===
Q mean: -15.084036
Q std: 19.411501
Actor loss: 15.088002
Action reg: 0.003966
  l1.weight: grad_norm = 0.309140
  l1.bias: grad_norm = 0.000928
  l2.weight: grad_norm = 0.254274
Total gradient norm: 0.601390
=== Actor Training Debug (Iteration 10077) ===
Q mean: -15.167524
Q std: 21.348200
Actor loss: 15.171498
Action reg: 0.003974
  l1.weight: grad_norm = 0.167862
  l1.bias: grad_norm = 0.000980
  l2.weight: grad_norm = 0.120908
Total gradient norm: 0.325026
=== Actor Training Debug (Iteration 10078) ===
Q mean: -15.645064
Q std: 22.146780
Actor loss: 15.649035
Action reg: 0.003970
  l1.weight: grad_norm = 0.270072
  l1.bias: grad_norm = 0.001345
  l2.weight: grad_norm = 0.219338
Total gradient norm: 0.586512
=== Actor Training Debug (Iteration 10079) ===
Q mean: -17.411987
Q std: 23.483418
Actor loss: 17.415968
Action reg: 0.003981
  l1.weight: grad_norm = 0.158260
  l1.bias: grad_norm = 0.000825
  l2.weight: grad_norm = 0.127777
Total gradient norm: 0.367863
=== Actor Training Debug (Iteration 10080) ===
Q mean: -16.334824
Q std: 22.458353
Actor loss: 16.338781
Action reg: 0.003958
  l1.weight: grad_norm = 0.281904
  l1.bias: grad_norm = 0.001773
  l2.weight: grad_norm = 0.240570
Total gradient norm: 0.699719
=== Actor Training Debug (Iteration 10081) ===
Q mean: -14.503222
Q std: 19.626886
Actor loss: 14.507184
Action reg: 0.003962
  l1.weight: grad_norm = 0.256862
  l1.bias: grad_norm = 0.001570
  l2.weight: grad_norm = 0.224999
Total gradient norm: 0.678933
=== Actor Training Debug (Iteration 10082) ===
Q mean: -14.814345
Q std: 20.666752
Actor loss: 14.818312
Action reg: 0.003966
  l1.weight: grad_norm = 0.349749
  l1.bias: grad_norm = 0.001837
  l2.weight: grad_norm = 0.269024
Total gradient norm: 0.744615
=== Actor Training Debug (Iteration 10083) ===
Q mean: -15.430362
Q std: 21.283480
Actor loss: 15.434298
Action reg: 0.003936
  l1.weight: grad_norm = 0.592603
  l1.bias: grad_norm = 0.002554
  l2.weight: grad_norm = 0.403864
Total gradient norm: 1.116526
=== Actor Training Debug (Iteration 10084) ===
Q mean: -15.680803
Q std: 21.944799
Actor loss: 15.684767
Action reg: 0.003963
  l1.weight: grad_norm = 0.238985
  l1.bias: grad_norm = 0.000769
  l2.weight: grad_norm = 0.179483
Total gradient norm: 0.473802
=== Actor Training Debug (Iteration 10085) ===
Q mean: -13.177549
Q std: 19.635509
Actor loss: 13.181510
Action reg: 0.003961
  l1.weight: grad_norm = 0.283811
  l1.bias: grad_norm = 0.001155
  l2.weight: grad_norm = 0.238069
Total gradient norm: 0.671064
=== Actor Training Debug (Iteration 10086) ===
Q mean: -15.826839
Q std: 22.501305
Actor loss: 15.830791
Action reg: 0.003951
  l1.weight: grad_norm = 0.222706
  l1.bias: grad_norm = 0.001321
  l2.weight: grad_norm = 0.181238
Total gradient norm: 0.458436
=== Actor Training Debug (Iteration 10087) ===
Q mean: -15.424849
Q std: 21.085131
Actor loss: 15.428798
Action reg: 0.003949
  l1.weight: grad_norm = 0.314156
  l1.bias: grad_norm = 0.001792
  l2.weight: grad_norm = 0.258841
Total gradient norm: 0.719991
=== Actor Training Debug (Iteration 10088) ===
Q mean: -15.426811
Q std: 21.762209
Actor loss: 15.430768
Action reg: 0.003956
  l1.weight: grad_norm = 0.610254
  l1.bias: grad_norm = 0.001220
  l2.weight: grad_norm = 0.449813
Total gradient norm: 1.577586
=== Actor Training Debug (Iteration 10089) ===
Q mean: -15.273670
Q std: 18.707363
Actor loss: 15.277636
Action reg: 0.003965
  l1.weight: grad_norm = 0.242026
  l1.bias: grad_norm = 0.002269
  l2.weight: grad_norm = 0.179299
Total gradient norm: 0.457774
=== Actor Training Debug (Iteration 10090) ===
Q mean: -17.605253
Q std: 23.087030
Actor loss: 17.609217
Action reg: 0.003964
  l1.weight: grad_norm = 1.112835
  l1.bias: grad_norm = 0.002143
  l2.weight: grad_norm = 0.859440
Total gradient norm: 2.950614
=== Actor Training Debug (Iteration 10091) ===
Q mean: -14.097991
Q std: 19.057890
Actor loss: 14.101946
Action reg: 0.003955
  l1.weight: grad_norm = 0.360900
  l1.bias: grad_norm = 0.003569
  l2.weight: grad_norm = 0.329233
Total gradient norm: 0.791351
=== Actor Training Debug (Iteration 10092) ===
Q mean: -13.600296
Q std: 20.793964
Actor loss: 13.604234
Action reg: 0.003938
  l1.weight: grad_norm = 0.242099
  l1.bias: grad_norm = 0.004839
  l2.weight: grad_norm = 0.188016
Total gradient norm: 0.529503
=== Actor Training Debug (Iteration 10093) ===
Q mean: -15.399239
Q std: 21.839502
Actor loss: 15.403207
Action reg: 0.003969
  l1.weight: grad_norm = 0.399780
  l1.bias: grad_norm = 0.000856
  l2.weight: grad_norm = 0.353570
Total gradient norm: 1.309297
=== Actor Training Debug (Iteration 10094) ===
Q mean: -14.406580
Q std: 20.544317
Actor loss: 14.410545
Action reg: 0.003966
  l1.weight: grad_norm = 0.192905
  l1.bias: grad_norm = 0.001756
  l2.weight: grad_norm = 0.139891
Total gradient norm: 0.400926
=== Actor Training Debug (Iteration 10095) ===
Q mean: -15.286308
Q std: 20.443703
Actor loss: 15.290257
Action reg: 0.003950
  l1.weight: grad_norm = 0.405193
  l1.bias: grad_norm = 0.003198
  l2.weight: grad_norm = 0.235109
Total gradient norm: 0.632466
=== Actor Training Debug (Iteration 10096) ===
Q mean: -13.552052
Q std: 18.192434
Actor loss: 13.556021
Action reg: 0.003969
  l1.weight: grad_norm = 0.340829
  l1.bias: grad_norm = 0.001036
  l2.weight: grad_norm = 0.250052
Total gradient norm: 0.664417
=== Actor Training Debug (Iteration 10097) ===
Q mean: -14.055647
Q std: 19.770285
Actor loss: 14.059591
Action reg: 0.003944
  l1.weight: grad_norm = 0.177702
  l1.bias: grad_norm = 0.003779
  l2.weight: grad_norm = 0.151497
Total gradient norm: 0.396472
=== Actor Training Debug (Iteration 10098) ===
Q mean: -15.600975
Q std: 21.531012
Actor loss: 15.604937
Action reg: 0.003962
  l1.weight: grad_norm = 0.259492
  l1.bias: grad_norm = 0.010463
  l2.weight: grad_norm = 0.212140
Total gradient norm: 0.606274
=== Actor Training Debug (Iteration 10114) ===
Q mean: -15.757561
Q std: 22.122154
Actor loss: 15.761524
Action reg: 0.003963
  l1.weight: grad_norm = 0.269881
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.204293
Total gradient norm: 0.583068
=== Actor Training Debug (Iteration 10115) ===
Q mean: -14.545963
Q std: 19.465046
Actor loss: 14.549936
Action reg: 0.003973
  l1.weight: grad_norm = 0.282622
  l1.bias: grad_norm = 0.000556
  l2.weight: grad_norm = 0.190297
Total gradient norm: 0.543934
=== Actor Training Debug (Iteration 10116) ===
Q mean: -13.490466
Q std: 18.917484
Actor loss: 13.494423
Action reg: 0.003957
  l1.weight: grad_norm = 0.254387
  l1.bias: grad_norm = 0.003510
  l2.weight: grad_norm = 0.188720
Total gradient norm: 0.487255
=== Actor Training Debug (Iteration 10117) ===
Q mean: -16.120659
Q std: 21.205517
Actor loss: 16.124617
Action reg: 0.003958
  l1.weight: grad_norm = 0.268060
  l1.bias: grad_norm = 0.001553
  l2.weight: grad_norm = 0.193344
Total gradient norm: 0.569849
=== Actor Training Debug (Iteration 10118) ===
Q mean: -15.620645
Q std: 20.687914
Actor loss: 15.624600
Action reg: 0.003955
  l1.weight: grad_norm = 0.364399
  l1.bias: grad_norm = 0.002199
  l2.weight: grad_norm = 0.287130
Total gradient norm: 0.864445
=== Actor Training Debug (Iteration 10119) ===
Q mean: -15.616287
Q std: 21.243315
Actor loss: 15.620254
Action reg: 0.003966
  l1.weight: grad_norm = 0.217654
  l1.bias: grad_norm = 0.001446
  l2.weight: grad_norm = 0.145804
Total gradient norm: 0.377377
=== Actor Training Debug (Iteration 10120) ===
Q mean: -16.032578
Q std: 21.650343
Actor loss: 16.036537
Action reg: 0.003959
  l1.weight: grad_norm = 0.258524
  l1.bias: grad_norm = 0.001543
  l2.weight: grad_norm = 0.166493
Total gradient norm: 0.432188
=== Actor Training Debug (Iteration 10121) ===
Q mean: -16.863110
Q std: 21.839838
Actor loss: 16.867077
Action reg: 0.003967
  l1.weight: grad_norm = 0.537253
  l1.bias: grad_norm = 0.001231
  l2.weight: grad_norm = 0.385440
Total gradient norm: 1.130039
=== Actor Training Debug (Iteration 10122) ===
Q mean: -15.658773
Q std: 20.301317
Actor loss: 15.662731
Action reg: 0.003958
  l1.weight: grad_norm = 0.227938
  l1.bias: grad_norm = 0.010504
  l2.weight: grad_norm = 0.189001
Total gradient norm: 0.536081
=== Actor Training Debug (Iteration 10123) ===
Q mean: -15.298272
Q std: 20.162003
Actor loss: 15.302219
Action reg: 0.003947
  l1.weight: grad_norm = 0.559509
  l1.bias: grad_norm = 0.013182
  l2.weight: grad_norm = 0.500245
Total gradient norm: 1.287520
=== Actor Training Debug (Iteration 10124) ===
Q mean: -11.811931
Q std: 14.832323
Actor loss: 11.815898
Action reg: 0.003968
  l1.weight: grad_norm = 0.357680
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.336452
Total gradient norm: 0.918859
=== Actor Training Debug (Iteration 10125) ===
Q mean: -15.462308
Q std: 21.231380
Actor loss: 15.466271
Action reg: 0.003963
  l1.weight: grad_norm = 0.206263
  l1.bias: grad_norm = 0.001529
  l2.weight: grad_norm = 0.162896
Total gradient norm: 0.483489
=== Actor Training Debug (Iteration 10126) ===
Q mean: -14.129164
Q std: 20.072359
Actor loss: 14.133120
Action reg: 0.003956
  l1.weight: grad_norm = 0.270123
  l1.bias: grad_norm = 0.001513
  l2.weight: grad_norm = 0.201824
Total gradient norm: 0.511988
=== Actor Training Debug (Iteration 10127) ===
Q mean: -16.937948
Q std: 23.578854
Actor loss: 16.941914
Action reg: 0.003966
  l1.weight: grad_norm = 0.458822
  l1.bias: grad_norm = 0.003179
  l2.weight: grad_norm = 0.346534
Total gradient norm: 0.844278
=== Actor Training Debug (Iteration 10128) ===
Q mean: -16.101654
Q std: 19.970186
Actor loss: 16.105619
Action reg: 0.003965
  l1.weight: grad_norm = 0.770095
  l1.bias: grad_norm = 0.003296
  l2.weight: grad_norm = 0.738137
Total gradient norm: 1.844064
=== Actor Training Debug (Iteration 10129) ===
Q mean: -14.565645
Q std: 20.922541
Actor loss: 14.569603
Action reg: 0.003958
  l1.weight: grad_norm = 0.227638
  l1.bias: grad_norm = 0.001915
  l2.weight: grad_norm = 0.161383
Total gradient norm: 0.448126
=== Actor Training Debug (Iteration 10130) ===
Q mean: -15.361878
Q std: 20.351366
Actor loss: 15.365860
Action reg: 0.003982
  l1.weight: grad_norm = 0.318658
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.259544
Total gradient norm: 0.640399
=== Actor Training Debug (Iteration 10131) ===
Q mean: -14.554273
Q std: 20.550018
Actor loss: 14.558237
Action reg: 0.003965
  l1.weight: grad_norm = 0.338603
  l1.bias: grad_norm = 0.001025
  l2.weight: grad_norm = 0.258899
Total gradient norm: 0.675265
=== Actor Training Debug (Iteration 10132) ===
Q mean: -13.758607
Q std: 20.677675
Actor loss: 13.762565
Action reg: 0.003958
  l1.weight: grad_norm = 0.554855
  l1.bias: grad_norm = 0.001070
  l2.weight: grad_norm = 0.364187
Total gradient norm: 1.100769
=== Actor Training Debug (Iteration 10133) ===
Q mean: -15.647293
Q std: 21.837387
Actor loss: 15.651263
Action reg: 0.003970
  l1.weight: grad_norm = 0.281875
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.169432
