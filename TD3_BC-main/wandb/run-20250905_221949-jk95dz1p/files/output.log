开始纯在线训练...
Episode 1: Steps=100, Reward=-154.573, Buffer_size=100
Episode 2: Steps=100, Reward=-160.848, Buffer_size=200
Episode 3: Steps=100, Reward=-151.122, Buffer_size=300
Episode 4: Steps=100, Reward=-152.591, Buffer_size=400
Episode 5: Steps=100, Reward=-127.908, Buffer_size=500
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 5: -240.623
Episode 6: Steps=100, Reward=-150.553, Buffer_size=600
Episode 7: Steps=100, Reward=-150.502, Buffer_size=700
Episode 8: Steps=100, Reward=-150.376, Buffer_size=800
Episode 9: Steps=100, Reward=-149.831, Buffer_size=900
Episode 10: Steps=100, Reward=-150.902, Buffer_size=1000
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 10: -240.623
Episode 11: Steps=100, Reward=-147.831, Buffer_size=1100
Episode 12: Steps=100, Reward=-155.442, Buffer_size=1200
Episode 13: Steps=100, Reward=-158.878, Buffer_size=1300
Episode 14: Steps=100, Reward=-150.464, Buffer_size=1400
Episode 15: Steps=100, Reward=-152.528, Buffer_size=1500
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 15: -240.623
Episode 16: Steps=100, Reward=-156.267, Buffer_size=1600
Episode 17: Steps=100, Reward=-157.021, Buffer_size=1700
Episode 18: Steps=100, Reward=-162.626, Buffer_size=1800
Episode 19: Steps=100, Reward=-147.235, Buffer_size=1900
Episode 20: Steps=100, Reward=-161.113, Buffer_size=2000
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 20: -240.623
Episode 21: Steps=100, Reward=-160.060, Buffer_size=2100
Episode 22: Steps=100, Reward=-154.304, Buffer_size=2200
Episode 23: Steps=100, Reward=-154.528, Buffer_size=2300
Episode 24: Steps=100, Reward=-154.073, Buffer_size=2400
Episode 25: Steps=100, Reward=-150.863, Buffer_size=2500
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 25: -240.623
Episode 26: Steps=100, Reward=-146.767, Buffer_size=2600
Episode 27: Steps=100, Reward=-157.442, Buffer_size=2700
Episode 28: Steps=100, Reward=-136.330, Buffer_size=2800
Episode 29: Steps=100, Reward=-143.279, Buffer_size=2900
Episode 30: Steps=100, Reward=-154.984, Buffer_size=3000
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 30: -240.623
Episode 31: Steps=100, Reward=-146.803, Buffer_size=3100
Episode 32: Steps=100, Reward=-152.283, Buffer_size=3200
Episode 33: Steps=100, Reward=-137.993, Buffer_size=3300
Episode 34: Steps=100, Reward=-159.785, Buffer_size=3400
Episode 35: Steps=100, Reward=-144.149, Buffer_size=3500
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 35: -240.623
Episode 36: Steps=100, Reward=-150.694, Buffer_size=3600
Episode 37: Steps=100, Reward=-143.822, Buffer_size=3700
Episode 38: Steps=100, Reward=-157.107, Buffer_size=3800
Episode 39: Steps=100, Reward=-158.899, Buffer_size=3900
Episode 40: Steps=100, Reward=-150.654, Buffer_size=4000
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 40: -240.623
Episode 41: Steps=100, Reward=-162.312, Buffer_size=4100
Episode 42: Steps=100, Reward=-157.842, Buffer_size=4200
Episode 43: Steps=100, Reward=-132.279, Buffer_size=4300
Episode 44: Steps=100, Reward=-145.504, Buffer_size=4400
Episode 45: Steps=100, Reward=-156.988, Buffer_size=4500
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 45: -240.623
Episode 46: Steps=100, Reward=-158.838, Buffer_size=4600
Episode 47: Steps=100, Reward=-151.615, Buffer_size=4700
Episode 48: Steps=100, Reward=-157.917, Buffer_size=4800
Episode 49: Steps=100, Reward=-152.105, Buffer_size=4900
Episode 50: Steps=100, Reward=-157.551, Buffer_size=5000
  Average reward: -240.623 | Average length: 100.0
Evaluation at episode 50: -240.623
=== Actor Training Debug (Iteration 1) ===
Q mean: -1.346833
Q std: 2.402100
Actor loss: 1.349649
Action reg: 0.002816
  l1.weight: grad_norm = 0.014575
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.026512
Total gradient norm: 1.261597
=== Actor Training Debug (Iteration 2) ===
Q mean: -6.827372
Q std: 2.504695
Actor loss: 6.830763
Action reg: 0.003391
  l1.weight: grad_norm = 0.009493
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.015521
Total gradient norm: 0.710848
=== Actor Training Debug (Iteration 3) ===
Q mean: -7.847452
Q std: 2.469554
Actor loss: 7.851107
Action reg: 0.003655
  l1.weight: grad_norm = 0.003949
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.006280
Total gradient norm: 0.267914
=== Actor Training Debug (Iteration 4) ===
Q mean: -7.559144
Q std: 2.123508
Actor loss: 7.562925
Action reg: 0.003781
  l1.weight: grad_norm = 0.003084
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.004901
Total gradient norm: 0.181652
=== Actor Training Debug (Iteration 5) ===
Q mean: -5.568866
Q std: 1.764278
Actor loss: 5.572638
Action reg: 0.003772
  l1.weight: grad_norm = 0.001855
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002506
Total gradient norm: 0.093450
=== Actor Training Debug (Iteration 6) ===
Q mean: -3.437434
Q std: 1.493967
Actor loss: 3.441236
Action reg: 0.003802
  l1.weight: grad_norm = 0.001803
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002422
Total gradient norm: 0.073311
=== Actor Training Debug (Iteration 7) ===
Q mean: -1.694779
Q std: 1.298496
Actor loss: 1.698586
Action reg: 0.003807
  l1.weight: grad_norm = 0.001662
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002386
Total gradient norm: 0.090488
=== Actor Training Debug (Iteration 8) ===
Q mean: -1.505858
Q std: 1.298844
Actor loss: 1.509678
Action reg: 0.003820
  l1.weight: grad_norm = 0.001906
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002546
Total gradient norm: 0.085221
=== Actor Training Debug (Iteration 9) ===
Q mean: -2.125010
Q std: 1.389313
Actor loss: 2.128831
Action reg: 0.003821
  l1.weight: grad_norm = 0.001807
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002380
Total gradient norm: 0.078536
=== Actor Training Debug (Iteration 10) ===
Q mean: -3.691775
Q std: 1.753552
Actor loss: 3.695637
Action reg: 0.003863
  l1.weight: grad_norm = 0.001057
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001227
Total gradient norm: 0.029546
=== Actor Training Debug (Iteration 11) ===
Q mean: -4.495640
Q std: 2.109959
Actor loss: 4.499528
Action reg: 0.003888
  l1.weight: grad_norm = 0.001794
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002817
Total gradient norm: 0.110469
=== Actor Training Debug (Iteration 12) ===
Q mean: -4.538203
Q std: 2.278990
Actor loss: 4.542057
Action reg: 0.003854
  l1.weight: grad_norm = 0.001151
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001476
Total gradient norm: 0.051491
=== Actor Training Debug (Iteration 13) ===
Q mean: -3.701731
Q std: 2.126945
Actor loss: 3.705604
Action reg: 0.003873
  l1.weight: grad_norm = 0.001282
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001535
Total gradient norm: 0.047963
=== Actor Training Debug (Iteration 14) ===
Q mean: -2.781935
Q std: 2.022822
Actor loss: 2.785819
Action reg: 0.003884
  l1.weight: grad_norm = 0.001510
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001856
Total gradient norm: 0.060344
=== Actor Training Debug (Iteration 15) ===
Q mean: -2.770541
Q std: 1.948311
Actor loss: 2.774428
Action reg: 0.003887
  l1.weight: grad_norm = 0.000930
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001365
Total gradient norm: 0.049068
=== Actor Training Debug (Iteration 16) ===
Q mean: -3.111936
Q std: 1.955308
Actor loss: 3.115818
Action reg: 0.003882
  l1.weight: grad_norm = 0.001236
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001424
Total gradient norm: 0.037384
=== Actor Training Debug (Iteration 17) ===
Q mean: -4.013020
Q std: 2.086252
Actor loss: 4.016951
Action reg: 0.003931
  l1.weight: grad_norm = 0.000824
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001019
Total gradient norm: 0.030250
=== Actor Training Debug (Iteration 18) ===
Q mean: -4.613854
Q std: 2.187089
Actor loss: 4.617782
Action reg: 0.003928
  l1.weight: grad_norm = 0.000938
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001048
Total gradient norm: 0.028072
=== Actor Training Debug (Iteration 19) ===
Q mean: -4.088619
Q std: 2.102370
Actor loss: 4.092511
Action reg: 0.003892
  l1.weight: grad_norm = 0.000737
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000945
Total gradient norm: 0.025471
=== Actor Training Debug (Iteration 20) ===
Q mean: -3.457133
Q std: 1.919930
Actor loss: 3.461030
Action reg: 0.003897
  l1.weight: grad_norm = 0.001309
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001804
Total gradient norm: 0.055866
=== Actor Training Debug (Iteration 21) ===
Q mean: -2.770695
Q std: 1.736526
Actor loss: 2.774599
Action reg: 0.003904
  l1.weight: grad_norm = 0.001174
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001399
Total gradient norm: 0.034633
=== Actor Training Debug (Iteration 22) ===
Q mean: -2.578691
Q std: 1.706410
Actor loss: 2.582573
Action reg: 0.003881
  l1.weight: grad_norm = 0.001240
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001728
Total gradient norm: 0.052611
=== Actor Training Debug (Iteration 23) ===
Q mean: -3.059013
Q std: 1.794679
Actor loss: 3.062934
Action reg: 0.003921
  l1.weight: grad_norm = 0.001306
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001891
Total gradient norm: 0.058428
=== Actor Training Debug (Iteration 24) ===
Q mean: -3.876227
Q std: 1.895469
Actor loss: 3.880124
Action reg: 0.003896
  l1.weight: grad_norm = 0.001026
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001188
Total gradient norm: 0.034801
=== Actor Training Debug (Iteration 25) ===
Q mean: -3.737554
Q std: 1.993452
Actor loss: 3.741425
Action reg: 0.003872
  l1.weight: grad_norm = 0.001002
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001295
Total gradient norm: 0.037182
=== Actor Training Debug (Iteration 26) ===
Q mean: -3.400512
Q std: 1.824567
Actor loss: 3.404420
Action reg: 0.003908
  l1.weight: grad_norm = 0.001103
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001330
Total gradient norm: 0.040159
=== Actor Training Debug (Iteration 27) ===
Q mean: -3.108281
Q std: 1.789077
Actor loss: 3.112175
Action reg: 0.003894
  l1.weight: grad_norm = 0.001605
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001944
Total gradient norm: 0.058311
=== Actor Training Debug (Iteration 28) ===
Q mean: -3.012594
Q std: 1.701153
Actor loss: 3.016462
Action reg: 0.003868
  l1.weight: grad_norm = 0.001393
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001660
Total gradient norm: 0.038134
=== Actor Training Debug (Iteration 29) ===
Q mean: -3.720889
Q std: 1.892535
Actor loss: 3.724795
Action reg: 0.003906
  l1.weight: grad_norm = 0.001668
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001951
Total gradient norm: 0.049649
=== Actor Training Debug (Iteration 30) ===
Q mean: -3.755322
Q std: 1.724599
Actor loss: 3.759226
Action reg: 0.003905
  l1.weight: grad_norm = 0.000993
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001182
Total gradient norm: 0.030334
=== Actor Training Debug (Iteration 31) ===
Q mean: -3.523873
Q std: 1.811052
Actor loss: 3.527749
Action reg: 0.003877
  l1.weight: grad_norm = 0.000991
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001313
Total gradient norm: 0.039677
=== Actor Training Debug (Iteration 32) ===
Q mean: -3.079912
Q std: 1.725416
Actor loss: 3.083775
Action reg: 0.003863
  l1.weight: grad_norm = 0.001309
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001427
Total gradient norm: 0.032014
=== Actor Training Debug (Iteration 33) ===
Q mean: -3.013016
Q std: 1.750615
Actor loss: 3.016815
Action reg: 0.003799
  l1.weight: grad_norm = 0.001219
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001342
Total gradient norm: 0.035463
=== Actor Training Debug (Iteration 34) ===
Q mean: -3.250513
Q std: 1.823793
Actor loss: 3.254369
Action reg: 0.003856
  l1.weight: grad_norm = 0.001041
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001315
Total gradient norm: 0.038115
=== Actor Training Debug (Iteration 35) ===
Q mean: -3.403018
Q std: 1.857643
Actor loss: 3.406860
Action reg: 0.003842
  l1.weight: grad_norm = 0.001454
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001724
Total gradient norm: 0.042156
=== Actor Training Debug (Iteration 36) ===
Q mean: -4.053230
Q std: 2.033427
Actor loss: 4.057111
Action reg: 0.003880
  l1.weight: grad_norm = 0.000847
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000906
Total gradient norm: 0.020151
=== Actor Training Debug (Iteration 37) ===
Q mean: -3.606131
Q std: 1.836254
Actor loss: 3.610022
Action reg: 0.003891
  l1.weight: grad_norm = 0.000856
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000968
Total gradient norm: 0.024116
=== Actor Training Debug (Iteration 38) ===
Q mean: -3.490109
Q std: 1.913577
Actor loss: 3.493993
Action reg: 0.003883
  l1.weight: grad_norm = 0.001270
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001415
Total gradient norm: 0.034517
=== Actor Training Debug (Iteration 39) ===
Q mean: -2.700635
Q std: 1.834744
Actor loss: 2.704463
Action reg: 0.003828
  l1.weight: grad_norm = 0.001284
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001578
Total gradient norm: 0.044376
=== Actor Training Debug (Iteration 40) ===
Q mean: -2.240829
Q std: 1.705905
Actor loss: 2.244733
Action reg: 0.003904
  l1.weight: grad_norm = 0.001291
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001697
Total gradient norm: 0.047012
=== Actor Training Debug (Iteration 41) ===
Q mean: -2.245025
Q std: 1.795284
Actor loss: 2.248939
Action reg: 0.003914
  l1.weight: grad_norm = 0.001199
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001598
Total gradient norm: 0.045787
=== Actor Training Debug (Iteration 42) ===
Q mean: -3.027117
Q std: 1.907877
Actor loss: 3.031035
Action reg: 0.003918
  l1.weight: grad_norm = 0.001284
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001721
Total gradient norm: 0.046308
=== Actor Training Debug (Iteration 43) ===
Q mean: -3.732605
Q std: 2.072815
Actor loss: 3.736493
Action reg: 0.003888
  l1.weight: grad_norm = 0.001374
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001605
Total gradient norm: 0.045417
=== Actor Training Debug (Iteration 44) ===
Q mean: -4.209560
Q std: 2.187097
Actor loss: 4.213456
Action reg: 0.003896
  l1.weight: grad_norm = 0.001169
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001256
Total gradient norm: 0.024135
=== Actor Training Debug (Iteration 45) ===
Q mean: -3.906574
Q std: 1.999555
Actor loss: 3.910394
Action reg: 0.003820
  l1.weight: grad_norm = 0.001187
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001474
Total gradient norm: 0.035022
=== Actor Training Debug (Iteration 46) ===
Q mean: -3.318830
Q std: 1.671689
Actor loss: 3.322698
Action reg: 0.003868
  l1.weight: grad_norm = 0.001565
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002046
Total gradient norm: 0.050821
=== Actor Training Debug (Iteration 47) ===
Q mean: -3.333576
Q std: 1.778904
Actor loss: 3.337447
Action reg: 0.003871
  l1.weight: grad_norm = 0.000797
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000929
Total gradient norm: 0.021586
=== Actor Training Debug (Iteration 48) ===
Q mean: -3.210367
Q std: 1.679726
Actor loss: 3.214204
Action reg: 0.003837
  l1.weight: grad_norm = 0.001632
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001936
Total gradient norm: 0.057959
=== Actor Training Debug (Iteration 49) ===
Q mean: -3.304206
Q std: 1.571438
Actor loss: 3.308102
Action reg: 0.003896
  l1.weight: grad_norm = 0.001323
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001843
Total gradient norm: 0.050659
=== Actor Training Debug (Iteration 50) ===
Q mean: -3.388257
Q std: 1.593894
Actor loss: 3.392158
Action reg: 0.003901
  l1.weight: grad_norm = 0.001233
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001416
Total gradient norm: 0.032692
=== Actor Training Debug (Iteration 51) ===
Q mean: -3.714063
Q std: 1.684339
Actor loss: 3.717916
Action reg: 0.003853
  l1.weight: grad_norm = 0.001331
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001516
Total gradient norm: 0.032404
=== Actor Training Debug (Iteration 52) ===
Q mean: -4.007795
Q std: 1.758502
Actor loss: 4.011705
Action reg: 0.003910
  l1.weight: grad_norm = 0.000908
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000974
Total gradient norm: 0.020473
=== Actor Training Debug (Iteration 53) ===
Q mean: -3.561104
Q std: 1.744315
Actor loss: 3.564971
Action reg: 0.003867
  l1.weight: grad_norm = 0.001168
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001292
Total gradient norm: 0.024827
=== Actor Training Debug (Iteration 54) ===
Q mean: -3.079251
Q std: 1.755756
Actor loss: 3.083104
Action reg: 0.003853
  l1.weight: grad_norm = 0.001531
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002170
Total gradient norm: 0.064118
=== Actor Training Debug (Iteration 55) ===
Q mean: -2.814970
Q std: 1.627836
Actor loss: 2.818853
Action reg: 0.003883
  l1.weight: grad_norm = 0.001594
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002153
Total gradient norm: 0.057433
=== Actor Training Debug (Iteration 56) ===
Q mean: -3.289157
Q std: 1.945122
Actor loss: 3.293036
Action reg: 0.003879
  l1.weight: grad_norm = 0.001178
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001557
Total gradient norm: 0.047012
=== Actor Training Debug (Iteration 57) ===
Q mean: -3.686415
Q std: 1.965046
Actor loss: 3.690276
Action reg: 0.003861
  l1.weight: grad_norm = 0.001920
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002724
Total gradient norm: 0.092154
=== Actor Training Debug (Iteration 58) ===
Q mean: -3.579750
Q std: 2.014634
Actor loss: 3.583582
Action reg: 0.003832
  l1.weight: grad_norm = 0.001697
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001894
Total gradient norm: 0.037816
=== Actor Training Debug (Iteration 59) ===
Q mean: -3.248339
Q std: 1.931508
Actor loss: 3.252217
Action reg: 0.003878
  l1.weight: grad_norm = 0.001189
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001398
Total gradient norm: 0.039828
=== Actor Training Debug (Iteration 60) ===
Q mean: -3.065594
Q std: 1.735985
Actor loss: 3.069469
Action reg: 0.003875
  l1.weight: grad_norm = 0.001140
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001502
Total gradient norm: 0.036044
=== Actor Training Debug (Iteration 61) ===
Q mean: -3.273282
Q std: 1.757147
Actor loss: 3.277114
Action reg: 0.003832
  l1.weight: grad_norm = 0.001974
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002289
Total gradient norm: 0.056677
=== Actor Training Debug (Iteration 62) ===
Q mean: -3.536502
Q std: 1.778170
Actor loss: 3.540386
Action reg: 0.003884
  l1.weight: grad_norm = 0.001429
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001930
Total gradient norm: 0.053048
=== Actor Training Debug (Iteration 63) ===
Q mean: -3.708654
Q std: 1.737346
Actor loss: 3.712540
Action reg: 0.003886
  l1.weight: grad_norm = 0.001672
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002080
Total gradient norm: 0.060874
=== Actor Training Debug (Iteration 64) ===
Q mean: -3.372391
Q std: 1.768411
Actor loss: 3.376297
Action reg: 0.003906
  l1.weight: grad_norm = 0.001048
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001244
Total gradient norm: 0.028608
=== Actor Training Debug (Iteration 65) ===
Q mean: -3.120332
Q std: 1.568886
Actor loss: 3.124253
Action reg: 0.003922
  l1.weight: grad_norm = 0.001100
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001235
Total gradient norm: 0.024522
=== Actor Training Debug (Iteration 66) ===
Q mean: -3.261734
Q std: 1.662220
Actor loss: 3.265626
Action reg: 0.003891
  l1.weight: grad_norm = 0.001117
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001387
Total gradient norm: 0.034522
=== Actor Training Debug (Iteration 67) ===
Q mean: -3.637754
Q std: 1.757668
Actor loss: 3.641656
Action reg: 0.003902
  l1.weight: grad_norm = 0.001156
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001225
Total gradient norm: 0.023580
=== Actor Training Debug (Iteration 68) ===
Q mean: -3.942682
Q std: 1.862701
Actor loss: 3.946564
Action reg: 0.003882
  l1.weight: grad_norm = 0.001202
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001358
Total gradient norm: 0.026243
=== Actor Training Debug (Iteration 69) ===
Q mean: -3.505477
Q std: 1.802254
Actor loss: 3.509362
Action reg: 0.003884
  l1.weight: grad_norm = 0.001636
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001916
Total gradient norm: 0.040802
=== Actor Training Debug (Iteration 70) ===
Q mean: -3.374919
Q std: 1.859804
Actor loss: 3.378810
Action reg: 0.003891
  l1.weight: grad_norm = 0.001129
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001199
Total gradient norm: 0.021793
=== Actor Training Debug (Iteration 71) ===
Q mean: -3.276828
Q std: 1.896378
Actor loss: 3.280681
Action reg: 0.003853
  l1.weight: grad_norm = 0.002191
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003037
Total gradient norm: 0.083456
=== Actor Training Debug (Iteration 72) ===
Q mean: -3.558784
Q std: 1.954738
Actor loss: 3.562605
Action reg: 0.003822
  l1.weight: grad_norm = 0.001333
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001771
Total gradient norm: 0.047228
=== Actor Training Debug (Iteration 73) ===
Q mean: -3.609854
Q std: 2.076167
Actor loss: 3.613720
Action reg: 0.003866
  l1.weight: grad_norm = 0.001279
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001432
Total gradient norm: 0.034018
=== Actor Training Debug (Iteration 74) ===
Q mean: -3.406789
Q std: 1.820496
Actor loss: 3.410666
Action reg: 0.003877
  l1.weight: grad_norm = 0.001103
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001477
Total gradient norm: 0.039902
=== Actor Training Debug (Iteration 75) ===
Q mean: -3.091381
Q std: 1.814020
Actor loss: 3.095251
Action reg: 0.003871
  l1.weight: grad_norm = 0.001425
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001960
Total gradient norm: 0.052891
=== Actor Training Debug (Iteration 76) ===
Q mean: -3.366667
Q std: 1.798491
Actor loss: 3.370540
Action reg: 0.003873
  l1.weight: grad_norm = 0.001485
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001904
Total gradient norm: 0.043444
=== Actor Training Debug (Iteration 77) ===
Q mean: -3.543674
Q std: 1.855661
Actor loss: 3.547523
Action reg: 0.003850
  l1.weight: grad_norm = 0.001264
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001597
Total gradient norm: 0.036071
=== Actor Training Debug (Iteration 78) ===
Q mean: -3.595995
Q std: 1.754715
Actor loss: 3.599877
Action reg: 0.003882
  l1.weight: grad_norm = 0.002273
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002881
Total gradient norm: 0.071188
=== Actor Training Debug (Iteration 79) ===
Q mean: -3.184802
Q std: 1.740460
Actor loss: 3.188675
Action reg: 0.003873
  l1.weight: grad_norm = 0.001305
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001782
Total gradient norm: 0.047749
=== Actor Training Debug (Iteration 80) ===
Q mean: -3.312061
Q std: 1.760295
Actor loss: 3.315931
Action reg: 0.003869
  l1.weight: grad_norm = 0.001098
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001247
Total gradient norm: 0.025532
=== Actor Training Debug (Iteration 81) ===
Q mean: -3.290988
Q std: 1.769481
Actor loss: 3.294837
Action reg: 0.003849
  l1.weight: grad_norm = 0.001420
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001641
Total gradient norm: 0.035631
=== Actor Training Debug (Iteration 82) ===
Q mean: -3.646103
Q std: 1.880711
Actor loss: 3.650005
Action reg: 0.003902
  l1.weight: grad_norm = 0.001079
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001297
Total gradient norm: 0.028136
=== Actor Training Debug (Iteration 83) ===
Q mean: -3.610378
Q std: 1.866540
Actor loss: 3.614232
Action reg: 0.003854
  l1.weight: grad_norm = 0.001636
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002462
Total gradient norm: 0.071354
=== Actor Training Debug (Iteration 84) ===
Q mean: -3.355650
Q std: 1.968684
Actor loss: 3.359491
Action reg: 0.003841
  l1.weight: grad_norm = 0.001040
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001426
Total gradient norm: 0.038709
=== Actor Training Debug (Iteration 85) ===
Q mean: -3.433958
Q std: 1.956241
Actor loss: 3.437813
Action reg: 0.003855
  l1.weight: grad_norm = 0.001882
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002527
Total gradient norm: 0.067849
=== Actor Training Debug (Iteration 86) ===
Q mean: -3.242676
Q std: 1.994523
Actor loss: 3.246570
Action reg: 0.003895
  l1.weight: grad_norm = 0.001007
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001288
Total gradient norm: 0.029178
=== Actor Training Debug (Iteration 87) ===
Q mean: -3.436314
Q std: 1.781745
Actor loss: 3.440176
Action reg: 0.003862
  l1.weight: grad_norm = 0.001141
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001372
Total gradient norm: 0.034023
=== Actor Training Debug (Iteration 88) ===
Q mean: -3.565827
Q std: 1.998320
Actor loss: 3.569632
Action reg: 0.003805
  l1.weight: grad_norm = 0.001027
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001316
Total gradient norm: 0.032579
=== Actor Training Debug (Iteration 89) ===
Q mean: -3.398964
Q std: 1.750082
Actor loss: 3.402807
Action reg: 0.003843
  l1.weight: grad_norm = 0.001520
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001907
Total gradient norm: 0.043988
=== Actor Training Debug (Iteration 90) ===
Q mean: -3.048620
Q std: 1.779595
Actor loss: 3.052504
Action reg: 0.003884
  l1.weight: grad_norm = 0.001222
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001568
Total gradient norm: 0.032781
=== Actor Training Debug (Iteration 91) ===
Q mean: -3.090544
Q std: 1.726050
Actor loss: 3.094426
Action reg: 0.003882
  l1.weight: grad_norm = 0.001081
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001215
Total gradient norm: 0.025720
=== Actor Training Debug (Iteration 92) ===
Q mean: -3.365093
Q std: 1.759739
Actor loss: 3.368969
Action reg: 0.003876
  l1.weight: grad_norm = 0.001203
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001496
Total gradient norm: 0.037401
=== Actor Training Debug (Iteration 93) ===
Q mean: -3.481750
Q std: 1.843665
Actor loss: 3.485593
Action reg: 0.003843
  l1.weight: grad_norm = 0.001508
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001820
Total gradient norm: 0.048645
=== Actor Training Debug (Iteration 94) ===
Q mean: -3.424799
Q std: 1.858455
Actor loss: 3.428590
Action reg: 0.003791
  l1.weight: grad_norm = 0.001206
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001463
Total gradient norm: 0.035339
=== Actor Training Debug (Iteration 95) ===
Q mean: -3.505555
Q std: 1.783437
Actor loss: 3.509443
Action reg: 0.003888
  l1.weight: grad_norm = 0.001333
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001674
Total gradient norm: 0.039646
=== Actor Training Debug (Iteration 96) ===
Q mean: -3.093085
Q std: 1.887300
Actor loss: 3.096934
Action reg: 0.003849
  l1.weight: grad_norm = 0.001196
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001664
Total gradient norm: 0.045925
=== Actor Training Debug (Iteration 97) ===
Q mean: -3.186780
Q std: 1.804002
Actor loss: 3.190626
Action reg: 0.003846
  l1.weight: grad_norm = 0.001410
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002016
Total gradient norm: 0.055424
=== Actor Training Debug (Iteration 98) ===
Q mean: -3.432319
Q std: 1.773661
Actor loss: 3.436147
Action reg: 0.003829
  l1.weight: grad_norm = 0.001808
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002242
Total gradient norm: 0.054688
=== Actor Training Debug (Iteration 99) ===
Q mean: -3.684437
Q std: 1.967957
Actor loss: 3.688281
Action reg: 0.003844
  l1.weight: grad_norm = 0.001227
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001530
Total gradient norm: 0.035588
=== Actor Training Debug (Iteration 100) ===
Q mean: -3.943811
Q std: 2.007407
Actor loss: 3.947653
Action reg: 0.003841
  l1.weight: grad_norm = 0.001851
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002488
Total gradient norm: 0.061338
Episode 51: Steps=100, Reward=-272.017, Buffer_size=5100
=== Actor Training Debug (Iteration 101) ===
Q mean: -3.706159
Q std: 1.885409
Actor loss: 3.710009
Action reg: 0.003851
  l1.weight: grad_norm = 0.001210
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001462
Total gradient norm: 0.030351
=== Actor Training Debug (Iteration 102) ===
Q mean: -3.187310
Q std: 1.858925
Actor loss: 3.191133
Action reg: 0.003823
  l1.weight: grad_norm = 0.001463
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001978
Total gradient norm: 0.055368
=== Actor Training Debug (Iteration 103) ===
Q mean: -3.220098
Q std: 1.708593
Actor loss: 3.223886
Action reg: 0.003787
  l1.weight: grad_norm = 0.002478
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003826
Total gradient norm: 0.119054
=== Actor Training Debug (Iteration 104) ===
Q mean: -3.230505
Q std: 1.850114
Actor loss: 3.234341
Action reg: 0.003836
  l1.weight: grad_norm = 0.001241
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001626
Total gradient norm: 0.041815
=== Actor Training Debug (Iteration 105) ===
Q mean: -3.871622
Q std: 2.061692
Actor loss: 3.875381
Action reg: 0.003759
  l1.weight: grad_norm = 0.001648
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002163
Total gradient norm: 0.052983
=== Actor Training Debug (Iteration 106) ===
Q mean: -4.119498
Q std: 1.992846
Actor loss: 4.123317
Action reg: 0.003819
  l1.weight: grad_norm = 0.001328
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001569
Total gradient norm: 0.027831
=== Actor Training Debug (Iteration 107) ===
Q mean: -3.818370
Q std: 1.811142
Actor loss: 3.822238
Action reg: 0.003868
  l1.weight: grad_norm = 0.001520
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002274
Total gradient norm: 0.062150
=== Actor Training Debug (Iteration 108) ===
Q mean: -2.954154
Q std: 1.811576
Actor loss: 2.957986
Action reg: 0.003832
  l1.weight: grad_norm = 0.002133
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002961
Total gradient norm: 0.078796
=== Actor Training Debug (Iteration 109) ===
Q mean: -2.542148
Q std: 1.603736
Actor loss: 2.545984
Action reg: 0.003836
  l1.weight: grad_norm = 0.001408
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001908
Total gradient norm: 0.053861
=== Actor Training Debug (Iteration 110) ===
Q mean: -2.929671
Q std: 1.744875
Actor loss: 2.933521
Action reg: 0.003850
  l1.weight: grad_norm = 0.001205
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001473
Total gradient norm: 0.032854
=== Actor Training Debug (Iteration 111) ===
Q mean: -3.885072
Q std: 2.039575
Actor loss: 3.888840
Action reg: 0.003768
  l1.weight: grad_norm = 0.002742
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004047
Total gradient norm: 0.125399
=== Actor Training Debug (Iteration 112) ===
Q mean: -4.150120
Q std: 1.967323
Actor loss: 4.153965
Action reg: 0.003845
  l1.weight: grad_norm = 0.001554
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002176
Total gradient norm: 0.057925
=== Actor Training Debug (Iteration 113) ===
Q mean: -4.132232
Q std: 1.873984
Actor loss: 4.136111
Action reg: 0.003879
  l1.weight: grad_norm = 0.001214
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001394
Total gradient norm: 0.031834
=== Actor Training Debug (Iteration 114) ===
Q mean: -3.232215
Q std: 1.759125
Actor loss: 3.236067
Action reg: 0.003852
  l1.weight: grad_norm = 0.001735
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002426
Total gradient norm: 0.061982
=== Actor Training Debug (Iteration 115) ===
Q mean: -2.990332
Q std: 1.726533
Actor loss: 2.994148
Action reg: 0.003816
  l1.weight: grad_norm = 0.002600
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004096
Total gradient norm: 0.116687
=== Actor Training Debug (Iteration 116) ===
Q mean: -3.121406
Q std: 1.832995
Actor loss: 3.125262
Action reg: 0.003856
  l1.weight: grad_norm = 0.001397
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001785
Total gradient norm: 0.038436
=== Actor Training Debug (Iteration 117) ===
Q mean: -3.767887
Q std: 1.871498
Actor loss: 3.771715
Action reg: 0.003828
  l1.weight: grad_norm = 0.001448
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001670
Total gradient norm: 0.033614
=== Actor Training Debug (Iteration 118) ===
Q mean: -3.977541
Q std: 1.946645
Actor loss: 3.981387
Action reg: 0.003846
  l1.weight: grad_norm = 0.001355
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001777
Total gradient norm: 0.043618
=== Actor Training Debug (Iteration 119) ===
Q mean: -3.397869
Q std: 1.840353
Actor loss: 3.401686
Action reg: 0.003817
  l1.weight: grad_norm = 0.001279
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001364
Total gradient norm: 0.025425
=== Actor Training Debug (Iteration 120) ===
Q mean: -2.691763
Q std: 1.673114
Actor loss: 2.695577
Action reg: 0.003814
  l1.weight: grad_norm = 0.001237
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001618
Total gradient norm: 0.039609
=== Actor Training Debug (Iteration 121) ===
Q mean: -2.449469
Q std: 1.744971
Actor loss: 2.453348
Action reg: 0.003879
  l1.weight: grad_norm = 0.001726
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002405
Total gradient norm: 0.061979
=== Actor Training Debug (Iteration 122) ===
Q mean: -2.749193
Q std: 1.739110
Actor loss: 2.753001
Action reg: 0.003808
  l1.weight: grad_norm = 0.001784
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002708
Total gradient norm: 0.079119
=== Actor Training Debug (Iteration 123) ===
Q mean: -3.518556
Q std: 2.027045
Actor loss: 3.522357
Action reg: 0.003801
  l1.weight: grad_norm = 0.001674
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002650
Total gradient norm: 0.070580
=== Actor Training Debug (Iteration 124) ===
Q mean: -4.004094
Q std: 2.041176
Actor loss: 4.007933
Action reg: 0.003838
  l1.weight: grad_norm = 0.001594
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002315
Total gradient norm: 0.067810
=== Actor Training Debug (Iteration 125) ===
Q mean: -3.687573
Q std: 1.909190
Actor loss: 3.691421
Action reg: 0.003848
  l1.weight: grad_norm = 0.001497
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002038
Total gradient norm: 0.052544
=== Actor Training Debug (Iteration 126) ===
Q mean: -2.813175
Q std: 1.814161
Actor loss: 2.817026
Action reg: 0.003851
  l1.weight: grad_norm = 0.001349
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001758
Total gradient norm: 0.042600
=== Actor Training Debug (Iteration 127) ===
Q mean: -2.857100
Q std: 1.783988
Actor loss: 2.860942
Action reg: 0.003842
  l1.weight: grad_norm = 0.001722
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002064
Total gradient norm: 0.049490
=== Actor Training Debug (Iteration 128) ===
Q mean: -3.007959
Q std: 1.859928
Actor loss: 3.011789
Action reg: 0.003830
  l1.weight: grad_norm = 0.002113
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002842
Total gradient norm: 0.071027
=== Actor Training Debug (Iteration 129) ===
Q mean: -3.877490
Q std: 1.815154
Actor loss: 3.881358
Action reg: 0.003869
  l1.weight: grad_norm = 0.002085
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002842
Total gradient norm: 0.081264
=== Actor Training Debug (Iteration 130) ===
Q mean: -4.185805
Q std: 1.884104
Actor loss: 4.189684
Action reg: 0.003879
  l1.weight: grad_norm = 0.002797
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.004316
Total gradient norm: 0.133495
=== Actor Training Debug (Iteration 131) ===
Q mean: -3.858345
Q std: 2.004055
Actor loss: 3.862202
Action reg: 0.003857
  l1.weight: grad_norm = 0.001530
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002039
Total gradient norm: 0.053561
=== Actor Training Debug (Iteration 132) ===
Q mean: -3.285570
Q std: 1.790030
Actor loss: 3.289439
Action reg: 0.003870
  l1.weight: grad_norm = 0.001549
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002043
Total gradient norm: 0.053947
=== Actor Training Debug (Iteration 133) ===
Q mean: -3.011764
Q std: 1.757836
Actor loss: 3.015641
Action reg: 0.003878
  l1.weight: grad_norm = 0.001696
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002103
Total gradient norm: 0.051720
=== Actor Training Debug (Iteration 134) ===
Q mean: -3.052024
Q std: 1.895428
Actor loss: 3.055819
Action reg: 0.003795
  l1.weight: grad_norm = 0.002170
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002919
Total gradient norm: 0.082147
=== Actor Training Debug (Iteration 135) ===
Q mean: -3.722917
Q std: 1.978119
Actor loss: 3.726776
Action reg: 0.003859
  l1.weight: grad_norm = 0.001460
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001833
Total gradient norm: 0.033757
=== Actor Training Debug (Iteration 136) ===
Q mean: -3.465096
Q std: 2.019367
Actor loss: 3.468892
Action reg: 0.003796
  l1.weight: grad_norm = 0.001216
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001493
Total gradient norm: 0.027780
=== Actor Training Debug (Iteration 137) ===
Q mean: -3.417763
Q std: 1.830055
Actor loss: 3.421631
Action reg: 0.003869
  l1.weight: grad_norm = 0.001388
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001983
Total gradient norm: 0.055283
=== Actor Training Debug (Iteration 138) ===
Q mean: -3.180408
Q std: 1.844378
Actor loss: 3.184253
Action reg: 0.003845
  l1.weight: grad_norm = 0.001659
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002463
Total gradient norm: 0.067397
=== Actor Training Debug (Iteration 139) ===
Q mean: -3.128410
Q std: 1.755054
Actor loss: 3.132252
Action reg: 0.003842
  l1.weight: grad_norm = 0.001385
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001852
Total gradient norm: 0.043693
=== Actor Training Debug (Iteration 140) ===
Q mean: -3.761157
Q std: 1.929488
Actor loss: 3.765019
Action reg: 0.003862
  l1.weight: grad_norm = 0.001217
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001522
Total gradient norm: 0.033434
=== Actor Training Debug (Iteration 141) ===
Q mean: -3.490700
Q std: 1.914199
Actor loss: 3.494513
Action reg: 0.003813
  l1.weight: grad_norm = 0.001591
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002035
Total gradient norm: 0.048407
=== Actor Training Debug (Iteration 142) ===
Q mean: -3.477257
Q std: 1.788740
Actor loss: 3.481093
Action reg: 0.003837
  l1.weight: grad_norm = 0.001906
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002689
Total gradient norm: 0.069472
=== Actor Training Debug (Iteration 143) ===
Q mean: -3.235532
Q std: 1.928269
Actor loss: 3.239352
Action reg: 0.003820
  l1.weight: grad_norm = 0.001931
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002430
Total gradient norm: 0.054732
=== Actor Training Debug (Iteration 144) ===
Q mean: -3.234427
Q std: 1.826439
Actor loss: 3.238276
Action reg: 0.003849
  l1.weight: grad_norm = 0.001721
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002131
Total gradient norm: 0.046097
=== Actor Training Debug (Iteration 145) ===
Q mean: -3.453819
Q std: 2.033506
Actor loss: 3.457573
Action reg: 0.003754
  l1.weight: grad_norm = 0.001853
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002433
Total gradient norm: 0.061059
=== Actor Training Debug (Iteration 146) ===
Q mean: -3.553565
Q std: 1.848550
Actor loss: 3.557415
Action reg: 0.003850
  l1.weight: grad_norm = 0.001147
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001440
Total gradient norm: 0.033353
=== Actor Training Debug (Iteration 147) ===
Q mean: -3.582790
Q std: 1.769414
Actor loss: 3.586683
Action reg: 0.003893
  l1.weight: grad_norm = 0.002243
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003255
Total gradient norm: 0.096848
=== Actor Training Debug (Iteration 148) ===
Q mean: -3.286914
Q std: 1.825325
Actor loss: 3.290772
Action reg: 0.003858
  l1.weight: grad_norm = 0.001397
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001573
Total gradient norm: 0.031470
=== Actor Training Debug (Iteration 149) ===
Q mean: -3.019419
Q std: 1.833150
Actor loss: 3.023250
Action reg: 0.003831
  l1.weight: grad_norm = 0.001749
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002328
Total gradient norm: 0.053669
=== Actor Training Debug (Iteration 150) ===
Q mean: -3.234836
Q std: 1.795055
Actor loss: 3.238689
Action reg: 0.003853
  l1.weight: grad_norm = 0.001971
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002400
Total gradient norm: 0.045956
=== Actor Training Debug (Iteration 151) ===
Q mean: -3.714221
Q std: 2.129494
Actor loss: 3.718063
Action reg: 0.003843
  l1.weight: grad_norm = 0.001349
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001568
Total gradient norm: 0.033483
=== Actor Training Debug (Iteration 152) ===
Q mean: -3.772586
Q std: 2.037539
Actor loss: 3.776421
Action reg: 0.003834
  l1.weight: grad_norm = 0.001498
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001919
Total gradient norm: 0.046860
=== Actor Training Debug (Iteration 153) ===
Q mean: -3.384855
Q std: 1.890875
Actor loss: 3.388664
Action reg: 0.003810
  l1.weight: grad_norm = 0.002039
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002392
Total gradient norm: 0.054000
=== Actor Training Debug (Iteration 154) ===
Q mean: -2.846099
Q std: 1.839650
Actor loss: 2.849927
Action reg: 0.003828
  l1.weight: grad_norm = 0.001727
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002022
Total gradient norm: 0.050715
=== Actor Training Debug (Iteration 155) ===
Q mean: -3.153603
Q std: 1.991788
Actor loss: 3.157439
Action reg: 0.003836
  l1.weight: grad_norm = 0.001765
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002287
Total gradient norm: 0.045641
=== Actor Training Debug (Iteration 156) ===
Q mean: -3.461228
Q std: 2.025393
Actor loss: 3.465068
Action reg: 0.003839
  l1.weight: grad_norm = 0.001989
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002631
Total gradient norm: 0.062371
=== Actor Training Debug (Iteration 157) ===
Q mean: -3.879108
Q std: 1.929851
Actor loss: 3.882954
Action reg: 0.003846
  l1.weight: grad_norm = 0.001349
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001704
Total gradient norm: 0.039896
=== Actor Training Debug (Iteration 158) ===
Q mean: -3.677816
Q std: 1.975955
Actor loss: 3.681688
Action reg: 0.003872
  l1.weight: grad_norm = 0.001339
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001463
Total gradient norm: 0.026797
=== Actor Training Debug (Iteration 159) ===
Q mean: -3.338497
Q std: 1.842760
Actor loss: 3.342368
Action reg: 0.003871
  l1.weight: grad_norm = 0.001234
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001473
Total gradient norm: 0.030539
=== Actor Training Debug (Iteration 160) ===
Q mean: -3.707016
Q std: 1.686326
Actor loss: 3.710852
Action reg: 0.003836
  l1.weight: grad_norm = 0.002052
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002511
Total gradient norm: 0.055633
=== Actor Training Debug (Iteration 161) ===
Q mean: -3.395208
Q std: 1.733581
Actor loss: 3.399074
Action reg: 0.003866
  l1.weight: grad_norm = 0.001971
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002730
Total gradient norm: 0.070941
=== Actor Training Debug (Iteration 162) ===
Q mean: -3.224812
Q std: 1.714873
Actor loss: 3.228667
Action reg: 0.003855
  l1.weight: grad_norm = 0.001619
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002304
Total gradient norm: 0.065568
=== Actor Training Debug (Iteration 163) ===
Q mean: -3.293048
Q std: 1.785223
Actor loss: 3.296845
Action reg: 0.003797
  l1.weight: grad_norm = 0.001742
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002496
Total gradient norm: 0.063550
=== Actor Training Debug (Iteration 164) ===
Q mean: -3.660229
Q std: 1.753623
Actor loss: 3.664063
Action reg: 0.003834
  l1.weight: grad_norm = 0.001417
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002088
Total gradient norm: 0.052045
=== Actor Training Debug (Iteration 165) ===
Q mean: -3.637708
Q std: 1.912451
Actor loss: 3.641500
Action reg: 0.003792
  l1.weight: grad_norm = 0.002128
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003594
Total gradient norm: 0.117545
=== Actor Training Debug (Iteration 166) ===
Q mean: -3.851142
Q std: 1.916773
Actor loss: 3.854986
Action reg: 0.003844
  l1.weight: grad_norm = 0.001530
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001696
Total gradient norm: 0.030217
=== Actor Training Debug (Iteration 167) ===
Q mean: -2.810432
Q std: 1.820589
Actor loss: 2.814285
Action reg: 0.003853
  l1.weight: grad_norm = 0.001936
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002627
Total gradient norm: 0.074617
=== Actor Training Debug (Iteration 168) ===
Q mean: -2.799190
Q std: 1.655019
Actor loss: 2.803046
Action reg: 0.003856
  l1.weight: grad_norm = 0.001308
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001571
Total gradient norm: 0.031094
=== Actor Training Debug (Iteration 169) ===
Q mean: -2.998225
Q std: 1.720129
Actor loss: 3.002108
Action reg: 0.003882
  l1.weight: grad_norm = 0.002174
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002646
Total gradient norm: 0.061236
=== Actor Training Debug (Iteration 170) ===
Q mean: -3.963848
Q std: 2.053698
Actor loss: 3.967683
Action reg: 0.003835
  l1.weight: grad_norm = 0.001970
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002233
Total gradient norm: 0.041834
=== Actor Training Debug (Iteration 171) ===
Q mean: -4.025472
Q std: 1.903728
Actor loss: 4.029305
Action reg: 0.003833
  l1.weight: grad_norm = 0.001276
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001538
Total gradient norm: 0.029135
=== Actor Training Debug (Iteration 172) ===
Q mean: -3.854306
Q std: 1.901303
Actor loss: 3.858192
Action reg: 0.003886
  l1.weight: grad_norm = 0.001441
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002030
Total gradient norm: 0.057763
=== Actor Training Debug (Iteration 173) ===
Q mean: -2.971250
Q std: 1.708633
Actor loss: 2.975107
Action reg: 0.003857
  l1.weight: grad_norm = 0.001417
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001809
Total gradient norm: 0.043465
=== Actor Training Debug (Iteration 174) ===
Q mean: -2.698868
Q std: 1.787034
Actor loss: 2.702648
Action reg: 0.003780
  l1.weight: grad_norm = 0.001492
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001881
Total gradient norm: 0.039183
=== Actor Training Debug (Iteration 175) ===
Q mean: -3.177776
Q std: 1.912222
Actor loss: 3.181687
Action reg: 0.003910
  l1.weight: grad_norm = 0.001434
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002076
Total gradient norm: 0.055752
=== Actor Training Debug (Iteration 176) ===
Q mean: -3.485590
Q std: 1.860447
Actor loss: 3.489428
Action reg: 0.003838
  l1.weight: grad_norm = 0.001885
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002876
Total gradient norm: 0.082650
=== Actor Training Debug (Iteration 177) ===
Q mean: -3.892530
Q std: 1.930040
Actor loss: 3.896370
Action reg: 0.003840
  l1.weight: grad_norm = 0.001312
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001568
Total gradient norm: 0.030572
=== Actor Training Debug (Iteration 178) ===
Q mean: -3.852480
Q std: 1.998374
Actor loss: 3.856285
Action reg: 0.003805
  l1.weight: grad_norm = 0.001559
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001977
Total gradient norm: 0.049443
=== Actor Training Debug (Iteration 179) ===
Q mean: -3.604166
Q std: 1.850935
Actor loss: 3.608000
Action reg: 0.003834
  l1.weight: grad_norm = 0.002523
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003536
Total gradient norm: 0.096361
=== Actor Training Debug (Iteration 180) ===
Q mean: -3.193321
Q std: 1.767222
Actor loss: 3.197179
Action reg: 0.003858
  l1.weight: grad_norm = 0.001610
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002423
Total gradient norm: 0.063419
=== Actor Training Debug (Iteration 181) ===
Q mean: -2.860521
Q std: 1.802534
Actor loss: 2.864366
Action reg: 0.003845
  l1.weight: grad_norm = 0.002113
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002758
Total gradient norm: 0.075526
=== Actor Training Debug (Iteration 182) ===
Q mean: -2.811184
Q std: 1.859028
Actor loss: 2.815046
Action reg: 0.003861
  l1.weight: grad_norm = 0.001971
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002555
Total gradient norm: 0.062809
=== Actor Training Debug (Iteration 183) ===
Q mean: -3.377667
Q std: 1.939329
Actor loss: 3.381556
Action reg: 0.003889
  l1.weight: grad_norm = 0.001658
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002113
Total gradient norm: 0.049440
=== Actor Training Debug (Iteration 184) ===
Q mean: -3.961232
Q std: 2.138653
Actor loss: 3.965122
Action reg: 0.003890
  l1.weight: grad_norm = 0.001363
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001805
Total gradient norm: 0.048293
=== Actor Training Debug (Iteration 185) ===
Q mean: -4.151742
Q std: 2.095143
Actor loss: 4.155608
Action reg: 0.003866
  l1.weight: grad_norm = 0.002398
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003351
Total gradient norm: 0.086544
=== Actor Training Debug (Iteration 186) ===
Q mean: -3.940206
Q std: 2.091100
Actor loss: 3.944071
Action reg: 0.003865
  l1.weight: grad_norm = 0.002039
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002448
Total gradient norm: 0.053520
=== Actor Training Debug (Iteration 187) ===
Q mean: -3.220440
Q std: 1.927065
Actor loss: 3.224279
Action reg: 0.003840
  l1.weight: grad_norm = 0.001885
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002457
Total gradient norm: 0.060954
=== Actor Training Debug (Iteration 188) ===
Q mean: -2.843429
Q std: 1.851730
Actor loss: 2.847294
Action reg: 0.003865
  l1.weight: grad_norm = 0.001939
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002781
Total gradient norm: 0.077553
=== Actor Training Debug (Iteration 189) ===
Q mean: -3.040408
Q std: 1.882376
Actor loss: 3.044319
Action reg: 0.003911
  l1.weight: grad_norm = 0.001465
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001918
Total gradient norm: 0.046293
=== Actor Training Debug (Iteration 190) ===
Q mean: -3.769650
Q std: 1.810580
Actor loss: 3.773506
Action reg: 0.003856
  l1.weight: grad_norm = 0.001951
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002334
Total gradient norm: 0.044071
=== Actor Training Debug (Iteration 191) ===
Q mean: -4.213958
Q std: 1.931724
Actor loss: 4.217779
Action reg: 0.003821
  l1.weight: grad_norm = 0.001589
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001908
Total gradient norm: 0.035212
=== Actor Training Debug (Iteration 192) ===
Q mean: -4.310134
Q std: 1.897116
Actor loss: 4.313976
Action reg: 0.003842
  l1.weight: grad_norm = 0.003052
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004962
Total gradient norm: 0.158675
=== Actor Training Debug (Iteration 193) ===
Q mean: -3.351174
Q std: 1.785517
Actor loss: 3.355007
Action reg: 0.003833
  l1.weight: grad_norm = 0.001398
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001612
Total gradient norm: 0.029370
=== Actor Training Debug (Iteration 194) ===
Q mean: -3.035544
Q std: 1.868750
Actor loss: 3.039384
Action reg: 0.003840
  l1.weight: grad_norm = 0.001388
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001685
Total gradient norm: 0.037000
=== Actor Training Debug (Iteration 195) ===
Q mean: -3.085909
Q std: 1.854000
Actor loss: 3.089722
Action reg: 0.003813
  l1.weight: grad_norm = 0.001350
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001527
Total gradient norm: 0.023320
=== Actor Training Debug (Iteration 196) ===
Q mean: -3.526183
Q std: 1.919849
Actor loss: 3.530052
Action reg: 0.003869
  l1.weight: grad_norm = 0.001301
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001561
Total gradient norm: 0.025824
=== Actor Training Debug (Iteration 197) ===
Q mean: -3.604456
Q std: 2.080667
Actor loss: 3.608290
Action reg: 0.003834
  l1.weight: grad_norm = 0.001648
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001968
Total gradient norm: 0.037421
=== Actor Training Debug (Iteration 198) ===
Q mean: -3.585186
Q std: 1.817249
Actor loss: 3.589028
Action reg: 0.003842
  l1.weight: grad_norm = 0.001550
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001946
Total gradient norm: 0.045229
=== Actor Training Debug (Iteration 199) ===
Q mean: -3.470095
Q std: 1.944613
Actor loss: 3.473947
Action reg: 0.003852
  l1.weight: grad_norm = 0.001463
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002166
Total gradient norm: 0.061983
=== Actor Training Debug (Iteration 200) ===
Q mean: -3.506117
Q std: 1.901294
Actor loss: 3.509984
Action reg: 0.003867
  l1.weight: grad_norm = 0.001313
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001563
Total gradient norm: 0.029374
=== Actor Training Debug (Iteration 201) ===
Q mean: -3.665668
Q std: 1.991590
Actor loss: 3.669503
Action reg: 0.003835
  l1.weight: grad_norm = 0.001272
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001469
Total gradient norm: 0.028646
=== Actor Training Debug (Iteration 202) ===
Q mean: -3.670660
Q std: 1.901577
Actor loss: 3.674503
Action reg: 0.003842
  l1.weight: grad_norm = 0.001682
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002043
Total gradient norm: 0.035553
=== Actor Training Debug (Iteration 203) ===
Q mean: -3.317914
Q std: 1.812243
Actor loss: 3.321731
Action reg: 0.003817
  l1.weight: grad_norm = 0.002206
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002974
Total gradient norm: 0.074309
=== Actor Training Debug (Iteration 204) ===
Q mean: -3.138520
Q std: 1.856689
Actor loss: 3.142364
Action reg: 0.003844
  l1.weight: grad_norm = 0.001695
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002235
Total gradient norm: 0.051773
=== Actor Training Debug (Iteration 205) ===
Q mean: -3.010039
Q std: 1.963555
Actor loss: 3.013891
Action reg: 0.003852
  l1.weight: grad_norm = 0.001863
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002510
Total gradient norm: 0.062190
=== Actor Training Debug (Iteration 206) ===
Q mean: -3.356827
Q std: 1.930808
Actor loss: 3.360715
Action reg: 0.003888
  l1.weight: grad_norm = 0.001356
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001606
Total gradient norm: 0.027665
=== Actor Training Debug (Iteration 207) ===
Q mean: -4.008162
Q std: 1.978232
Actor loss: 4.012008
Action reg: 0.003847
  l1.weight: grad_norm = 0.001335
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001631
Total gradient norm: 0.034621
=== Actor Training Debug (Iteration 208) ===
Q mean: -4.158840
Q std: 1.927150
Actor loss: 4.162672
Action reg: 0.003832
  l1.weight: grad_norm = 0.001590
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002067
Total gradient norm: 0.043706
=== Actor Training Debug (Iteration 209) ===
Q mean: -3.972775
Q std: 1.901278
Actor loss: 3.976652
Action reg: 0.003877
  l1.weight: grad_norm = 0.001329
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001696
Total gradient norm: 0.030673
=== Actor Training Debug (Iteration 210) ===
Q mean: -3.072764
Q std: 1.796698
Actor loss: 3.076617
Action reg: 0.003853
  l1.weight: grad_norm = 0.001216
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001357
Total gradient norm: 0.018867
=== Actor Training Debug (Iteration 211) ===
Q mean: -2.787527
Q std: 1.727919
Actor loss: 2.791357
Action reg: 0.003830
  l1.weight: grad_norm = 0.001623
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002381
Total gradient norm: 0.058886
=== Actor Training Debug (Iteration 212) ===
Q mean: -2.933111
Q std: 1.866839
Actor loss: 2.936974
Action reg: 0.003863
  l1.weight: grad_norm = 0.002443
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003333
Total gradient norm: 0.080160
=== Actor Training Debug (Iteration 213) ===
Q mean: -3.620068
Q std: 1.921916
Actor loss: 3.623921
Action reg: 0.003853
  l1.weight: grad_norm = 0.001450
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001713
Total gradient norm: 0.034085
=== Actor Training Debug (Iteration 214) ===
Q mean: -3.840535
Q std: 2.033325
Actor loss: 3.844411
Action reg: 0.003876
  l1.weight: grad_norm = 0.001630
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002030
Total gradient norm: 0.043912
=== Actor Training Debug (Iteration 215) ===
Q mean: -3.882753
Q std: 1.960243
Actor loss: 3.886633
Action reg: 0.003880
  l1.weight: grad_norm = 0.002027
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002758
Total gradient norm: 0.069307
=== Actor Training Debug (Iteration 216) ===
Q mean: -3.219215
Q std: 1.884638
Actor loss: 3.223134
Action reg: 0.003919
  l1.weight: grad_norm = 0.001299
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001575
Total gradient norm: 0.031810
=== Actor Training Debug (Iteration 217) ===
Q mean: -3.089610
Q std: 1.820204
Actor loss: 3.093499
Action reg: 0.003889
  l1.weight: grad_norm = 0.002208
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003186
Total gradient norm: 0.079982
=== Actor Training Debug (Iteration 218) ===
Q mean: -3.155731
Q std: 1.809081
Actor loss: 3.159600
Action reg: 0.003868
  l1.weight: grad_norm = 0.001639
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002371
Total gradient norm: 0.052016
=== Actor Training Debug (Iteration 219) ===
Q mean: -3.679129
Q std: 1.842476
Actor loss: 3.682963
Action reg: 0.003835
  l1.weight: grad_norm = 0.001633
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002217
Total gradient norm: 0.052464
=== Actor Training Debug (Iteration 220) ===
Q mean: -3.818216
Q std: 1.859053
Actor loss: 3.822073
Action reg: 0.003858
  l1.weight: grad_norm = 0.001848
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002334
Total gradient norm: 0.036566
=== Actor Training Debug (Iteration 221) ===
Q mean: -3.219932
Q std: 1.694791
Actor loss: 3.223777
Action reg: 0.003845
  l1.weight: grad_norm = 0.001572
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002134
Total gradient norm: 0.053254
=== Actor Training Debug (Iteration 222) ===
Q mean: -3.166716
Q std: 1.842291
Actor loss: 3.170533
Action reg: 0.003817
  l1.weight: grad_norm = 0.001951
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002599
Total gradient norm: 0.060760
=== Actor Training Debug (Iteration 223) ===
Q mean: -3.556172
Q std: 1.878924
Actor loss: 3.560018
Action reg: 0.003846
  l1.weight: grad_norm = 0.002552
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004016
Total gradient norm: 0.106803
=== Actor Training Debug (Iteration 224) ===
Q mean: -3.833160
Q std: 1.940534
Actor loss: 3.837036
Action reg: 0.003876
  l1.weight: grad_norm = 0.001537
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001869
Total gradient norm: 0.027827
=== Actor Training Debug (Iteration 225) ===
Q mean: -3.240401
Q std: 1.898138
Actor loss: 3.244245
Action reg: 0.003844
  l1.weight: grad_norm = 0.001047
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001229
Total gradient norm: 0.021012
=== Actor Training Debug (Iteration 226) ===
Q mean: -3.280733
Q std: 1.897741
Actor loss: 3.284592
Action reg: 0.003859
  l1.weight: grad_norm = 0.002357
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003638
Total gradient norm: 0.099219
=== Actor Training Debug (Iteration 227) ===
Q mean: -3.417925
Q std: 2.049272
Actor loss: 3.421771
Action reg: 0.003846
  l1.weight: grad_norm = 0.001607
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001994
Total gradient norm: 0.031532
=== Actor Training Debug (Iteration 228) ===
Q mean: -3.936951
Q std: 2.006371
Actor loss: 3.940840
Action reg: 0.003889
  l1.weight: grad_norm = 0.002157
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003234
Total gradient norm: 0.085317
=== Actor Training Debug (Iteration 229) ===
Q mean: -3.719743
Q std: 1.991916
Actor loss: 3.723583
Action reg: 0.003840
  l1.weight: grad_norm = 0.001311
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001674
Total gradient norm: 0.032738
=== Actor Training Debug (Iteration 230) ===
Q mean: -3.407381
Q std: 2.016724
Actor loss: 3.411251
Action reg: 0.003869
  l1.weight: grad_norm = 0.001491
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001834
Total gradient norm: 0.032997
=== Actor Training Debug (Iteration 231) ===
Q mean: -3.085459
Q std: 1.846796
Actor loss: 3.089314
Action reg: 0.003855
  l1.weight: grad_norm = 0.001163
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001489
Total gradient norm: 0.028809
=== Actor Training Debug (Iteration 232) ===
Q mean: -3.531460
Q std: 1.942334
Actor loss: 3.535340
Action reg: 0.003880
  l1.weight: grad_norm = 0.001434
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001776
Total gradient norm: 0.031725
=== Actor Training Debug (Iteration 233) ===
Q mean: -3.553535
Q std: 2.031977
Actor loss: 3.557364
Action reg: 0.003828
  l1.weight: grad_norm = 0.001008
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001212
Total gradient norm: 0.023852
=== Actor Training Debug (Iteration 234) ===
Q mean: -3.677845
Q std: 1.877436
Actor loss: 3.681710
Action reg: 0.003864
  l1.weight: grad_norm = 0.002293
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003040
Total gradient norm: 0.072519
=== Actor Training Debug (Iteration 235) ===
Q mean: -3.576753
Q std: 1.798264
Actor loss: 3.580610
Action reg: 0.003857
  l1.weight: grad_norm = 0.002094
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003357
Total gradient norm: 0.098726
=== Actor Training Debug (Iteration 236) ===
Q mean: -3.494812
Q std: 1.835860
Actor loss: 3.498659
Action reg: 0.003847
  l1.weight: grad_norm = 0.001200
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001507
Total gradient norm: 0.022928
=== Actor Training Debug (Iteration 237) ===
Q mean: -3.635148
Q std: 1.866383
Actor loss: 3.638980
Action reg: 0.003832
  l1.weight: grad_norm = 0.002197
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003586
Total gradient norm: 0.085076
=== Actor Training Debug (Iteration 238) ===
Q mean: -3.953062
Q std: 2.095796
Actor loss: 3.956923
Action reg: 0.003861
  l1.weight: grad_norm = 0.001432
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001749
Total gradient norm: 0.030406
=== Actor Training Debug (Iteration 239) ===
Q mean: -3.396511
Q std: 2.039894
Actor loss: 3.400367
Action reg: 0.003856
  l1.weight: grad_norm = 0.001371
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001681
Total gradient norm: 0.029921
=== Actor Training Debug (Iteration 240) ===
Q mean: -3.301651
Q std: 1.913435
Actor loss: 3.305490
Action reg: 0.003839
  l1.weight: grad_norm = 0.001747
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002377
Total gradient norm: 0.044431
=== Actor Training Debug (Iteration 241) ===
Q mean: -3.622192
Q std: 1.853792
Actor loss: 3.626070
Action reg: 0.003878
  l1.weight: grad_norm = 0.001387
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001672
Total gradient norm: 0.032982
=== Actor Training Debug (Iteration 242) ===
Q mean: -3.422709
Q std: 2.047811
Actor loss: 3.426515
Action reg: 0.003806
  l1.weight: grad_norm = 0.001520
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001814
Total gradient norm: 0.029644
=== Actor Training Debug (Iteration 243) ===
Q mean: -3.532382
Q std: 1.994617
Actor loss: 3.536252
Action reg: 0.003869
  l1.weight: grad_norm = 0.001859
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002603
Total gradient norm: 0.053295
=== Actor Training Debug (Iteration 244) ===
Q mean: -3.418636
Q std: 1.980778
Actor loss: 3.422517
Action reg: 0.003881
  l1.weight: grad_norm = 0.001329
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001614
Total gradient norm: 0.030379
=== Actor Training Debug (Iteration 245) ===
Q mean: -3.252643
Q std: 1.812264
Actor loss: 3.256475
Action reg: 0.003832
  l1.weight: grad_norm = 0.001903
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002765
Total gradient norm: 0.061401
=== Actor Training Debug (Iteration 246) ===
Q mean: -3.033359
Q std: 1.763239
Actor loss: 3.037198
Action reg: 0.003839
  l1.weight: grad_norm = 0.002768
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003900
Total gradient norm: 0.077043
=== Actor Training Debug (Iteration 247) ===
Q mean: -3.579494
Q std: 2.058087
Actor loss: 3.583319
Action reg: 0.003825
  l1.weight: grad_norm = 0.001361
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001653
Total gradient norm: 0.023272
=== Actor Training Debug (Iteration 248) ===
Q mean: -3.727440
Q std: 2.073585
Actor loss: 3.731320
Action reg: 0.003880
  l1.weight: grad_norm = 0.001661
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002489
Total gradient norm: 0.059779
=== Actor Training Debug (Iteration 249) ===
Q mean: -3.478750
Q std: 1.856544
Actor loss: 3.482611
Action reg: 0.003861
  l1.weight: grad_norm = 0.002249
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002754
Total gradient norm: 0.065580
=== Actor Training Debug (Iteration 250) ===
Q mean: -3.395517
Q std: 1.985997
Actor loss: 3.399403
Action reg: 0.003886
  l1.weight: grad_norm = 0.002323
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002856
Total gradient norm: 0.048924
=== Actor Training Debug (Iteration 251) ===
Q mean: -3.464720
Q std: 1.971097
Actor loss: 3.468568
Action reg: 0.003848
  l1.weight: grad_norm = 0.002117
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003169
Total gradient norm: 0.081399
=== Actor Training Debug (Iteration 252) ===
Q mean: -3.613246
Q std: 1.937986
Actor loss: 3.617090
Action reg: 0.003843
  l1.weight: grad_norm = 0.002009
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003337
Total gradient norm: 0.081027
=== Actor Training Debug (Iteration 253) ===
Q mean: -3.173513
Q std: 1.860197
Actor loss: 3.177418
Action reg: 0.003905
  l1.weight: grad_norm = 0.001034
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001181
Total gradient norm: 0.020778
=== Actor Training Debug (Iteration 254) ===
Q mean: -3.495097
Q std: 1.940662
Actor loss: 3.498978
Action reg: 0.003881
  l1.weight: grad_norm = 0.001575
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002040
Total gradient norm: 0.035559
=== Actor Training Debug (Iteration 255) ===
Q mean: -3.673601
Q std: 1.865360
Actor loss: 3.677475
Action reg: 0.003874
  l1.weight: grad_norm = 0.001451
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001774
Total gradient norm: 0.029749
=== Actor Training Debug (Iteration 256) ===
Q mean: -3.313624
Q std: 1.957805
Actor loss: 3.317484
Action reg: 0.003860
  l1.weight: grad_norm = 0.001652
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002228
Total gradient norm: 0.037027
=== Actor Training Debug (Iteration 257) ===
Q mean: -3.394312
Q std: 1.946282
Actor loss: 3.398173
Action reg: 0.003861
  l1.weight: grad_norm = 0.001317
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001678
Total gradient norm: 0.037227
=== Actor Training Debug (Iteration 258) ===
Q mean: -3.786642
Q std: 2.039961
Actor loss: 3.790526
Action reg: 0.003884
  l1.weight: grad_norm = 0.001270
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001520
Total gradient norm: 0.024818
=== Actor Training Debug (Iteration 259) ===
Q mean: -3.545992
Q std: 2.094948
Actor loss: 3.549891
Action reg: 0.003899
  l1.weight: grad_norm = 0.001403
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001675
Total gradient norm: 0.028294
=== Actor Training Debug (Iteration 260) ===
Q mean: -3.663344
Q std: 2.061152
Actor loss: 3.667203
Action reg: 0.003859
  l1.weight: grad_norm = 0.001947
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002352
Total gradient norm: 0.043911
=== Actor Training Debug (Iteration 261) ===
Q mean: -3.922447
Q std: 2.063033
Actor loss: 3.926349
Action reg: 0.003901
  l1.weight: grad_norm = 0.001433
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001896
Total gradient norm: 0.040876
=== Actor Training Debug (Iteration 262) ===
Q mean: -3.964481
Q std: 1.969397
Actor loss: 3.968398
Action reg: 0.003916
  l1.weight: grad_norm = 0.001153
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001444
Total gradient norm: 0.028164
=== Actor Training Debug (Iteration 263) ===
Q mean: -3.465713
Q std: 1.790143
Actor loss: 3.469626
Action reg: 0.003914
  l1.weight: grad_norm = 0.001206
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001424
Total gradient norm: 0.026730
=== Actor Training Debug (Iteration 264) ===
Q mean: -3.238952
Q std: 1.807079
Actor loss: 3.242856
Action reg: 0.003904
  l1.weight: grad_norm = 0.001168
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001596
Total gradient norm: 0.037275
=== Actor Training Debug (Iteration 265) ===
Q mean: -3.454750
Q std: 1.992089
Actor loss: 3.458614
Action reg: 0.003864
  l1.weight: grad_norm = 0.001592
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002290
Total gradient norm: 0.059767
=== Actor Training Debug (Iteration 266) ===
Q mean: -4.126802
Q std: 2.040741
Actor loss: 4.130694
Action reg: 0.003892
  l1.weight: grad_norm = 0.001902
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002387
Total gradient norm: 0.039371
=== Actor Training Debug (Iteration 267) ===
Q mean: -4.099355
Q std: 2.146585
Actor loss: 4.103266
Action reg: 0.003912
  l1.weight: grad_norm = 0.001254
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001751
Total gradient norm: 0.043460
=== Actor Training Debug (Iteration 268) ===
Q mean: -3.718845
Q std: 2.087520
Actor loss: 3.722760
Action reg: 0.003915
  l1.weight: grad_norm = 0.001601
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002005
Total gradient norm: 0.031578
=== Actor Training Debug (Iteration 269) ===
Q mean: -2.990242
Q std: 1.919857
Actor loss: 2.994125
Action reg: 0.003883
  l1.weight: grad_norm = 0.001261
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001618
Total gradient norm: 0.034231
=== Actor Training Debug (Iteration 270) ===
Q mean: -3.211395
Q std: 1.968992
Actor loss: 3.215294
Action reg: 0.003899
  l1.weight: grad_norm = 0.001279
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001604
Total gradient norm: 0.027737
=== Actor Training Debug (Iteration 271) ===
Q mean: -3.286348
Q std: 1.827522
Actor loss: 3.290239
Action reg: 0.003891
  l1.weight: grad_norm = 0.001337
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001983
Total gradient norm: 0.047870
=== Actor Training Debug (Iteration 272) ===
Q mean: -4.402291
Q std: 2.155201
Actor loss: 4.406128
Action reg: 0.003837
  l1.weight: grad_norm = 0.002996
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005173
Total gradient norm: 0.133700
=== Actor Training Debug (Iteration 273) ===
Q mean: -4.396520
Q std: 2.065198
Actor loss: 4.400414
Action reg: 0.003895
  l1.weight: grad_norm = 0.001502
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002019
Total gradient norm: 0.039708
=== Actor Training Debug (Iteration 274) ===
Q mean: -4.128205
Q std: 2.068964
Actor loss: 4.132101
Action reg: 0.003896
  l1.weight: grad_norm = 0.001283
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001932
Total gradient norm: 0.050811
=== Actor Training Debug (Iteration 275) ===
Q mean: -3.282545
Q std: 1.844980
Actor loss: 3.286453
Action reg: 0.003908
  l1.weight: grad_norm = 0.001624
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002130
Total gradient norm: 0.046753
=== Actor Training Debug (Iteration 276) ===
Q mean: -2.981435
Q std: 1.813290
Actor loss: 2.985300
Action reg: 0.003865
  l1.weight: grad_norm = 0.001178
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001561
Total gradient norm: 0.025296
=== Actor Training Debug (Iteration 277) ===
Q mean: -3.413156
Q std: 1.902393
Actor loss: 3.417077
Action reg: 0.003920
  l1.weight: grad_norm = 0.002268
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002966
Total gradient norm: 0.067191
=== Actor Training Debug (Iteration 278) ===
Q mean: -4.306637
Q std: 1.967275
Actor loss: 4.310533
Action reg: 0.003895
  l1.weight: grad_norm = 0.001748
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002073
Total gradient norm: 0.027419
=== Actor Training Debug (Iteration 279) ===
Q mean: -4.164664
Q std: 2.053803
Actor loss: 4.168562
Action reg: 0.003898
  l1.weight: grad_norm = 0.002062
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002499
Total gradient norm: 0.041477
=== Actor Training Debug (Iteration 280) ===
Q mean: -3.983238
Q std: 2.088248
Actor loss: 3.987062
Action reg: 0.003824
  l1.weight: grad_norm = 0.001216
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001518
Total gradient norm: 0.026577
=== Actor Training Debug (Iteration 281) ===
Q mean: -3.069247
Q std: 1.924881
Actor loss: 3.073116
Action reg: 0.003869
  l1.weight: grad_norm = 0.001822
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002570
Total gradient norm: 0.061815
=== Actor Training Debug (Iteration 282) ===
Q mean: -2.989328
Q std: 2.013271
Actor loss: 2.993233
Action reg: 0.003904
  l1.weight: grad_norm = 0.001507
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002060
Total gradient norm: 0.050567
=== Actor Training Debug (Iteration 283) ===
Q mean: -3.271894
Q std: 2.021629
Actor loss: 3.275783
Action reg: 0.003889
  l1.weight: grad_norm = 0.000914
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001171
Total gradient norm: 0.020413
=== Actor Training Debug (Iteration 284) ===
Q mean: -3.612833
Q std: 2.014298
Actor loss: 3.616711
Action reg: 0.003879
  l1.weight: grad_norm = 0.001947
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003348
Total gradient norm: 0.085650
=== Actor Training Debug (Iteration 285) ===
Q mean: -3.574389
Q std: 2.204881
Actor loss: 3.578261
Action reg: 0.003873
  l1.weight: grad_norm = 0.001405
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001811
Total gradient norm: 0.038929
=== Actor Training Debug (Iteration 286) ===
Q mean: -3.556612
Q std: 1.856094
Actor loss: 3.560544
Action reg: 0.003931
  l1.weight: grad_norm = 0.000793
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001102
Total gradient norm: 0.025094
=== Actor Training Debug (Iteration 287) ===
Q mean: -3.349151
Q std: 1.827780
Actor loss: 3.353032
Action reg: 0.003881
  l1.weight: grad_norm = 0.001321
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001613
Total gradient norm: 0.033904
=== Actor Training Debug (Iteration 288) ===
Q mean: -3.376216
Q std: 1.894174
Actor loss: 3.380087
Action reg: 0.003870
  l1.weight: grad_norm = 0.001412
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001821
Total gradient norm: 0.040313
=== Actor Training Debug (Iteration 289) ===
Q mean: -3.665884
Q std: 1.822037
Actor loss: 3.669785
Action reg: 0.003901
  l1.weight: grad_norm = 0.001235
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001702
Total gradient norm: 0.038422
=== Actor Training Debug (Iteration 290) ===
Q mean: -3.771752
Q std: 2.014264
Actor loss: 3.775594
Action reg: 0.003842
  l1.weight: grad_norm = 0.001389
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002097
Total gradient norm: 0.052927
=== Actor Training Debug (Iteration 291) ===
Q mean: -3.430405
Q std: 1.800149
Actor loss: 3.434284
Action reg: 0.003879
  l1.weight: grad_norm = 0.000963
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001130
Total gradient norm: 0.022015
=== Actor Training Debug (Iteration 292) ===
Q mean: -3.476444
Q std: 1.910161
Actor loss: 3.480345
Action reg: 0.003901
  l1.weight: grad_norm = 0.001288
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001493
Total gradient norm: 0.026198
=== Actor Training Debug (Iteration 293) ===
Q mean: -3.478821
Q std: 1.992858
Actor loss: 3.482740
Action reg: 0.003919
  l1.weight: grad_norm = 0.001017
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001337
Total gradient norm: 0.030840
=== Actor Training Debug (Iteration 294) ===
Q mean: -3.406271
Q std: 1.967007
Actor loss: 3.410128
Action reg: 0.003857
  l1.weight: grad_norm = 0.001216
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001439
Total gradient norm: 0.021994
=== Actor Training Debug (Iteration 295) ===
Q mean: -3.505347
Q std: 2.149818
Actor loss: 3.509224
Action reg: 0.003877
  l1.weight: grad_norm = 0.001164
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001360
Total gradient norm: 0.023430
=== Actor Training Debug (Iteration 296) ===
Q mean: -3.418131
Q std: 2.201158
Actor loss: 3.422069
Action reg: 0.003938
  l1.weight: grad_norm = 0.000997
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001317
Total gradient norm: 0.025875
=== Actor Training Debug (Iteration 297) ===
Q mean: -3.599509
Q std: 2.138544
Actor loss: 3.603468
Action reg: 0.003959
  l1.weight: grad_norm = 0.001161
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001440
Total gradient norm: 0.025440
=== Actor Training Debug (Iteration 298) ===
Q mean: -3.545872
Q std: 2.120745
Actor loss: 3.549778
Action reg: 0.003906
  l1.weight: grad_norm = 0.001197
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001432
Total gradient norm: 0.024548
=== Actor Training Debug (Iteration 299) ===
Q mean: -3.634567
Q std: 1.984519
Actor loss: 3.638491
Action reg: 0.003923
  l1.weight: grad_norm = 0.001242
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001604
Total gradient norm: 0.034098
=== Actor Training Debug (Iteration 300) ===
Q mean: -3.768201
Q std: 2.012396
Actor loss: 3.772121
Action reg: 0.003920
  l1.weight: grad_norm = 0.001409
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001739
Total gradient norm: 0.033125
=== Actor Training Debug (Iteration 301) ===
Q mean: -3.827735
Q std: 1.967101
Actor loss: 3.831628
Action reg: 0.003893
  l1.weight: grad_norm = 0.000714
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000814
Total gradient norm: 0.014280
=== Actor Training Debug (Iteration 302) ===
Q mean: -3.819772
Q std: 1.976009
Actor loss: 3.823675
Action reg: 0.003903
  l1.weight: grad_norm = 0.001240
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001562
Total gradient norm: 0.030548
=== Actor Training Debug (Iteration 303) ===
Q mean: -3.747481
Q std: 1.754671
Actor loss: 3.751402
Action reg: 0.003920
  l1.weight: grad_norm = 0.001800
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002042
Total gradient norm: 0.028850
=== Actor Training Debug (Iteration 304) ===
Q mean: -3.530277
Q std: 1.870101
Actor loss: 3.534226
Action reg: 0.003949
  l1.weight: grad_norm = 0.000998
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001450
Total gradient norm: 0.024466
=== Actor Training Debug (Iteration 305) ===
Q mean: -3.470793
Q std: 1.773195
Actor loss: 3.474742
Action reg: 0.003949
  l1.weight: grad_norm = 0.001185
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001338
Total gradient norm: 0.017907
=== Actor Training Debug (Iteration 306) ===
Q mean: -3.366491
Q std: 1.934723
Actor loss: 3.370424
Action reg: 0.003933
  l1.weight: grad_norm = 0.001837
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002452
Total gradient norm: 0.040719
=== Actor Training Debug (Iteration 307) ===
Q mean: -3.436341
Q std: 2.086560
Actor loss: 3.440246
Action reg: 0.003904
  l1.weight: grad_norm = 0.001513
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001911
Total gradient norm: 0.026390
=== Actor Training Debug (Iteration 308) ===
Q mean: -3.536045
Q std: 2.007204
Actor loss: 3.539953
Action reg: 0.003908
  l1.weight: grad_norm = 0.000974
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001220
Total gradient norm: 0.023757
=== Actor Training Debug (Iteration 309) ===
Q mean: -3.684545
Q std: 2.141129
Actor loss: 3.688463
Action reg: 0.003919
  l1.weight: grad_norm = 0.001331
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001673
Total gradient norm: 0.033370
=== Actor Training Debug (Iteration 310) ===
Q mean: -3.704944
Q std: 2.100129
Actor loss: 3.708828
Action reg: 0.003884
  l1.weight: grad_norm = 0.001183
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001463
Total gradient norm: 0.023618
=== Actor Training Debug (Iteration 311) ===
Q mean: -3.358412
Q std: 1.990044
Actor loss: 3.362320
Action reg: 0.003908
  l1.weight: grad_norm = 0.001521
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001999
Total gradient norm: 0.038221
=== Actor Training Debug (Iteration 312) ===
Q mean: -3.426062
Q std: 1.981360
Actor loss: 3.429979
Action reg: 0.003917
  l1.weight: grad_norm = 0.001278
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001580
Total gradient norm: 0.025721
=== Actor Training Debug (Iteration 313) ===
Q mean: -3.634594
Q std: 1.819442
Actor loss: 3.638519
Action reg: 0.003924
  l1.weight: grad_norm = 0.001069
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001344
Total gradient norm: 0.021183
=== Actor Training Debug (Iteration 314) ===
Q mean: -3.724214
Q std: 2.044323
Actor loss: 3.728125
Action reg: 0.003911
  l1.weight: grad_norm = 0.001412
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002029
Total gradient norm: 0.048256
=== Actor Training Debug (Iteration 315) ===
Q mean: -3.328064
Q std: 1.794896
Actor loss: 3.331989
Action reg: 0.003925
  l1.weight: grad_norm = 0.001774
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002400
Total gradient norm: 0.049158
=== Actor Training Debug (Iteration 316) ===
Q mean: -3.382632
Q std: 2.111940
Actor loss: 3.386473
Action reg: 0.003841
  l1.weight: grad_norm = 0.001118
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001352
Total gradient norm: 0.019926
=== Actor Training Debug (Iteration 317) ===
Q mean: -3.833531
Q std: 2.061944
Actor loss: 3.837411
Action reg: 0.003881
  l1.weight: grad_norm = 0.001637
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002072
Total gradient norm: 0.037433
=== Actor Training Debug (Iteration 318) ===
Q mean: -3.759649
Q std: 2.084239
Actor loss: 3.763590
Action reg: 0.003940
  l1.weight: grad_norm = 0.001383
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001717
Total gradient norm: 0.026938
=== Actor Training Debug (Iteration 319) ===
Q mean: -3.580690
Q std: 2.101554
Actor loss: 3.584588
Action reg: 0.003898
  l1.weight: grad_norm = 0.001411
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001738
Total gradient norm: 0.034429
=== Actor Training Debug (Iteration 320) ===
Q mean: -3.349587
Q std: 2.042802
Actor loss: 3.353486
Action reg: 0.003898
  l1.weight: grad_norm = 0.000993
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001219
Total gradient norm: 0.023914
=== Actor Training Debug (Iteration 321) ===
Q mean: -3.599330
Q std: 1.917645
Actor loss: 3.603219
Action reg: 0.003888
  l1.weight: grad_norm = 0.001173
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001672
Total gradient norm: 0.031672
=== Actor Training Debug (Iteration 322) ===
Q mean: -3.673476
Q std: 2.116680
Actor loss: 3.677356
Action reg: 0.003881
  l1.weight: grad_norm = 0.001242
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001755
Total gradient norm: 0.042013
=== Actor Training Debug (Iteration 323) ===
Q mean: -3.602529
Q std: 1.915626
Actor loss: 3.606464
Action reg: 0.003935
  l1.weight: grad_norm = 0.001099
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001335
Total gradient norm: 0.019683
=== Actor Training Debug (Iteration 324) ===
Q mean: -3.308438
Q std: 1.860420
Actor loss: 3.312375
Action reg: 0.003937
  l1.weight: grad_norm = 0.001311
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001621
Total gradient norm: 0.032530
=== Actor Training Debug (Iteration 325) ===
Q mean: -3.524594
Q std: 2.157490
Actor loss: 3.528533
Action reg: 0.003939
  l1.weight: grad_norm = 0.000940
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001109
Total gradient norm: 0.016621
=== Actor Training Debug (Iteration 326) ===
Q mean: -3.738548
Q std: 1.955637
Actor loss: 3.742452
Action reg: 0.003904
  l1.weight: grad_norm = 0.001862
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.002660
Total gradient norm: 0.055749
=== Actor Training Debug (Iteration 327) ===
Q mean: -3.800849
Q std: 2.030714
Actor loss: 3.804743
Action reg: 0.003893
  l1.weight: grad_norm = 0.000552
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000699
Total gradient norm: 0.012522
=== Actor Training Debug (Iteration 328) ===
Q mean: -3.198519
Q std: 2.111592
Actor loss: 3.202403
Action reg: 0.003884
  l1.weight: grad_norm = 0.000989
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001213
Total gradient norm: 0.021191
=== Actor Training Debug (Iteration 329) ===
Q mean: -3.717730
Q std: 2.023556
Actor loss: 3.721666
Action reg: 0.003937
  l1.weight: grad_norm = 0.000900
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001072
Total gradient norm: 0.017490
=== Actor Training Debug (Iteration 330) ===
Q mean: -3.680160
Q std: 2.081367
Actor loss: 3.684061
Action reg: 0.003902
  l1.weight: grad_norm = 0.000860
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001087
Total gradient norm: 0.014946
=== Actor Training Debug (Iteration 331) ===
Q mean: -3.849730
Q std: 2.104164
Actor loss: 3.853672
Action reg: 0.003942
  l1.weight: grad_norm = 0.000940
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001220
Total gradient norm: 0.021593
=== Actor Training Debug (Iteration 332) ===
Q mean: -3.046440
Q std: 1.981773
Actor loss: 3.050377
Action reg: 0.003937
  l1.weight: grad_norm = 0.001133
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001575
Total gradient norm: 0.036381
=== Actor Training Debug (Iteration 333) ===
Q mean: -3.000854
Q std: 1.897872
Actor loss: 3.004780
Action reg: 0.003926
  l1.weight: grad_norm = 0.000875
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001043
Total gradient norm: 0.016913
=== Actor Training Debug (Iteration 334) ===
Q mean: -3.313111
Q std: 1.979333
Actor loss: 3.317073
Action reg: 0.003963
  l1.weight: grad_norm = 0.000859
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001206
Total gradient norm: 0.027275
=== Actor Training Debug (Iteration 335) ===
Q mean: -3.877171
Q std: 2.131202
Actor loss: 3.881096
Action reg: 0.003925
  l1.weight: grad_norm = 0.000358
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000438
Total gradient norm: 0.008108
=== Actor Training Debug (Iteration 336) ===
Q mean: -4.053232
Q std: 2.213805
Actor loss: 4.057162
Action reg: 0.003930
  l1.weight: grad_norm = 0.001105
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001396
Total gradient norm: 0.029392
=== Actor Training Debug (Iteration 337) ===
Q mean: -4.381217
Q std: 2.158794
Actor loss: 4.385184
Action reg: 0.003966
  l1.weight: grad_norm = 0.000540
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000650
Total gradient norm: 0.010921
=== Actor Training Debug (Iteration 338) ===
Q mean: -3.858422
Q std: 2.192179
Actor loss: 3.862379
Action reg: 0.003957
  l1.weight: grad_norm = 0.000773
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000948
Total gradient norm: 0.018362
=== Actor Training Debug (Iteration 339) ===
Q mean: -3.006933
Q std: 2.049629
Actor loss: 3.010881
Action reg: 0.003947
  l1.weight: grad_norm = 0.000923
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001091
Total gradient norm: 0.020266
=== Actor Training Debug (Iteration 340) ===
Q mean: -2.827064
Q std: 1.813896
Actor loss: 2.830997
Action reg: 0.003933
  l1.weight: grad_norm = 0.000595
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000754
Total gradient norm: 0.013651
=== Actor Training Debug (Iteration 341) ===
Q mean: -3.202210
Q std: 2.017907
Actor loss: 3.206170
Action reg: 0.003960
  l1.weight: grad_norm = 0.000564
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000763
Total gradient norm: 0.016494
=== Actor Training Debug (Iteration 342) ===
Q mean: -3.944944
Q std: 2.275542
Actor loss: 3.948843
Action reg: 0.003898
  l1.weight: grad_norm = 0.000494
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000611
Total gradient norm: 0.013556
=== Actor Training Debug (Iteration 343) ===
Q mean: -3.835919
Q std: 2.127654
Actor loss: 3.839867
Action reg: 0.003948
  l1.weight: grad_norm = 0.000728
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000926
Total gradient norm: 0.019685
=== Actor Training Debug (Iteration 344) ===
Q mean: -3.701935
Q std: 2.026308
Actor loss: 3.705853
Action reg: 0.003917
  l1.weight: grad_norm = 0.000791
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000947
Total gradient norm: 0.019080
=== Actor Training Debug (Iteration 345) ===
Q mean: -3.267293
Q std: 1.760929
Actor loss: 3.271176
Action reg: 0.003883
  l1.weight: grad_norm = 0.000684
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000797
Total gradient norm: 0.010986
=== Actor Training Debug (Iteration 346) ===
Q mean: -3.084496
Q std: 2.013060
Actor loss: 3.088440
Action reg: 0.003944
  l1.weight: grad_norm = 0.000698
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000889
Total gradient norm: 0.017509
=== Actor Training Debug (Iteration 347) ===
Q mean: -3.622317
Q std: 1.993552
Actor loss: 3.626260
Action reg: 0.003942
  l1.weight: grad_norm = 0.001054
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001359
Total gradient norm: 0.024929
=== Actor Training Debug (Iteration 348) ===
Q mean: -4.173943
Q std: 2.120399
Actor loss: 4.177877
Action reg: 0.003934
  l1.weight: grad_norm = 0.000830
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000952
Total gradient norm: 0.017220
=== Actor Training Debug (Iteration 349) ===
Q mean: -4.404643
Q std: 2.088697
Actor loss: 4.408582
Action reg: 0.003939
  l1.weight: grad_norm = 0.000370
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000472
Total gradient norm: 0.008002
=== Actor Training Debug (Iteration 350) ===
Q mean: -4.071746
Q std: 2.092296
Actor loss: 4.075701
Action reg: 0.003955
  l1.weight: grad_norm = 0.000706
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000873
Total gradient norm: 0.015109
=== Actor Training Debug (Iteration 351) ===
Q mean: -3.327972
Q std: 1.865633
Actor loss: 3.331930
Action reg: 0.003958
  l1.weight: grad_norm = 0.000857
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001029
Total gradient norm: 0.020094
=== Actor Training Debug (Iteration 352) ===
Q mean: -3.223290
Q std: 2.202821
Actor loss: 3.227255
Action reg: 0.003965
  l1.weight: grad_norm = 0.000333
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000423
Total gradient norm: 0.008725
=== Actor Training Debug (Iteration 353) ===
Q mean: -3.575689
Q std: 2.008276
Actor loss: 3.579659
Action reg: 0.003970
  l1.weight: grad_norm = 0.000470
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000590
Total gradient norm: 0.012173
=== Actor Training Debug (Iteration 354) ===
Q mean: -3.652440
Q std: 2.204165
Actor loss: 3.656386
Action reg: 0.003946
  l1.weight: grad_norm = 0.000903
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001186
Total gradient norm: 0.023563
=== Actor Training Debug (Iteration 355) ===
Q mean: -4.007306
Q std: 2.411602
Actor loss: 4.011175
Action reg: 0.003869
  l1.weight: grad_norm = 0.000764
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001064
Total gradient norm: 0.019757
=== Actor Training Debug (Iteration 356) ===
Q mean: -3.744103
Q std: 2.210424
Actor loss: 3.748034
Action reg: 0.003931
  l1.weight: grad_norm = 0.000796
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001026
Total gradient norm: 0.021243
=== Actor Training Debug (Iteration 357) ===
Q mean: -3.382515
Q std: 2.079463
Actor loss: 3.386416
Action reg: 0.003900
  l1.weight: grad_norm = 0.000860
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001151
Total gradient norm: 0.023248
=== Actor Training Debug (Iteration 358) ===
Q mean: -3.370378
Q std: 1.985813
Actor loss: 3.374256
Action reg: 0.003879
  l1.weight: grad_norm = 0.000523
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000590
Total gradient norm: 0.010603
=== Actor Training Debug (Iteration 359) ===
Q mean: -3.389874
Q std: 1.936315
Actor loss: 3.393832
Action reg: 0.003958
  l1.weight: grad_norm = 0.000862
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000982
Total gradient norm: 0.014761
=== Actor Training Debug (Iteration 360) ===
Q mean: -3.944906
Q std: 1.966853
Actor loss: 3.948799
Action reg: 0.003893
  l1.weight: grad_norm = 0.000647
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000755
Total gradient norm: 0.016771
=== Actor Training Debug (Iteration 361) ===
Q mean: -3.762831
Q std: 2.018619
Actor loss: 3.766740
Action reg: 0.003909
  l1.weight: grad_norm = 0.000875
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000893
Total gradient norm: 0.011054
=== Actor Training Debug (Iteration 362) ===
Q mean: -3.156312
Q std: 1.872583
Actor loss: 3.160219
Action reg: 0.003907
  l1.weight: grad_norm = 0.000614
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000743
Total gradient norm: 0.010872
=== Actor Training Debug (Iteration 363) ===
Q mean: -3.388752
Q std: 1.996059
Actor loss: 3.392695
Action reg: 0.003943
  l1.weight: grad_norm = 0.001012
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001274
Total gradient norm: 0.019989
=== Actor Training Debug (Iteration 364) ===
Q mean: -3.486693
Q std: 2.087566
Actor loss: 3.490648
Action reg: 0.003955
  l1.weight: grad_norm = 0.000768
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000924
Total gradient norm: 0.016852
=== Actor Training Debug (Iteration 365) ===
Q mean: -3.665558
Q std: 2.123097
Actor loss: 3.669474
Action reg: 0.003916
  l1.weight: grad_norm = 0.000991
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001340
Total gradient norm: 0.029298
=== Actor Training Debug (Iteration 366) ===
Q mean: -3.446760
Q std: 2.172814
Actor loss: 3.450689
Action reg: 0.003929
  l1.weight: grad_norm = 0.000835
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000967
Total gradient norm: 0.014652
=== Actor Training Debug (Iteration 367) ===
Q mean: -3.519886
Q std: 2.127284
Actor loss: 3.523789
Action reg: 0.003904
  l1.weight: grad_norm = 0.000782
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000958
Total gradient norm: 0.013720
=== Actor Training Debug (Iteration 368) ===
Q mean: -3.546822
Q std: 2.230965
Actor loss: 3.550777
Action reg: 0.003955
  l1.weight: grad_norm = 0.000832
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001092
Total gradient norm: 0.022806
=== Actor Training Debug (Iteration 369) ===
Q mean: -3.590050
Q std: 2.216390
Actor loss: 3.594008
Action reg: 0.003958
  l1.weight: grad_norm = 0.000340
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000474
Total gradient norm: 0.007035
=== Actor Training Debug (Iteration 370) ===
Q mean: -3.983623
Q std: 2.220220
Actor loss: 3.987605
Action reg: 0.003982
  l1.weight: grad_norm = 0.000333
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000464
Total gradient norm: 0.008557
=== Actor Training Debug (Iteration 371) ===
Q mean: -3.758786
Q std: 2.140151
Actor loss: 3.762720
Action reg: 0.003934
  l1.weight: grad_norm = 0.000590
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000801
Total gradient norm: 0.013272
=== Actor Training Debug (Iteration 372) ===
Q mean: -3.462825
Q std: 2.045104
Actor loss: 3.466707
Action reg: 0.003883
  l1.weight: grad_norm = 0.000563
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000679
Total gradient norm: 0.013030
=== Actor Training Debug (Iteration 373) ===
Q mean: -3.253279
Q std: 1.885852
Actor loss: 3.257225
Action reg: 0.003946
  l1.weight: grad_norm = 0.000503
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000621
Total gradient norm: 0.011099
=== Actor Training Debug (Iteration 374) ===
Q mean: -3.532297
Q std: 2.012743
Actor loss: 3.536285
Action reg: 0.003988
  l1.weight: grad_norm = 0.000429
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000545
Total gradient norm: 0.010054
=== Actor Training Debug (Iteration 375) ===
Q mean: -4.161893
Q std: 2.145496
Actor loss: 4.165825
Action reg: 0.003933
  l1.weight: grad_norm = 0.000307
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000361
Total gradient norm: 0.005281
=== Actor Training Debug (Iteration 376) ===
Q mean: -3.895311
Q std: 2.164087
Actor loss: 3.899209
Action reg: 0.003898
  l1.weight: grad_norm = 0.000762
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000878
Total gradient norm: 0.013266
=== Actor Training Debug (Iteration 377) ===
Q mean: -3.493881
Q std: 1.995731
Actor loss: 3.497828
Action reg: 0.003947
  l1.weight: grad_norm = 0.000589
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000772
Total gradient norm: 0.017014
=== Actor Training Debug (Iteration 378) ===
Q mean: -3.300190
Q std: 2.019360
Actor loss: 3.304129
Action reg: 0.003939
  l1.weight: grad_norm = 0.000832
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001047
Total gradient norm: 0.016326
=== Actor Training Debug (Iteration 379) ===
Q mean: -3.570204
Q std: 2.051703
Actor loss: 3.574180
Action reg: 0.003976
  l1.weight: grad_norm = 0.001003
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001231
Total gradient norm: 0.020680
=== Actor Training Debug (Iteration 380) ===
Q mean: -3.835159
Q std: 2.342942
Actor loss: 3.839076
Action reg: 0.003916
  l1.weight: grad_norm = 0.000742
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000897
Total gradient norm: 0.018108
=== Actor Training Debug (Iteration 381) ===
Q mean: -3.655208
Q std: 2.081143
Actor loss: 3.659181
Action reg: 0.003973
  l1.weight: grad_norm = 0.000826
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000979
Total gradient norm: 0.016288
=== Actor Training Debug (Iteration 382) ===
Q mean: -3.813070
Q std: 2.089497
Actor loss: 3.817031
Action reg: 0.003961
  l1.weight: grad_norm = 0.000960
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001226
Total gradient norm: 0.024056
=== Actor Training Debug (Iteration 383) ===
Q mean: -3.855464
Q std: 1.960277
Actor loss: 3.859420
Action reg: 0.003956
  l1.weight: grad_norm = 0.001824
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002037
Total gradient norm: 0.044180
=== Actor Training Debug (Iteration 384) ===
Q mean: -3.858759
Q std: 2.253121
Actor loss: 3.862736
Action reg: 0.003977
  l1.weight: grad_norm = 0.001152
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001328
Total gradient norm: 0.028066
=== Actor Training Debug (Iteration 385) ===
Q mean: -3.888304
Q std: 2.170196
Actor loss: 3.892256
Action reg: 0.003952
  l1.weight: grad_norm = 0.000512
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000668
Total gradient norm: 0.011010
=== Actor Training Debug (Iteration 386) ===
Q mean: -3.707581
Q std: 2.109140
Actor loss: 3.711567
Action reg: 0.003986
  l1.weight: grad_norm = 0.000575
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000757
Total gradient norm: 0.013770
=== Actor Training Debug (Iteration 387) ===
Q mean: -3.789257
Q std: 2.201546
Actor loss: 3.793199
Action reg: 0.003942
  l1.weight: grad_norm = 0.000478
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000636
Total gradient norm: 0.008340
=== Actor Training Debug (Iteration 388) ===
Q mean: -3.509150
Q std: 2.109456
Actor loss: 3.513139
Action reg: 0.003989
  l1.weight: grad_norm = 0.000196
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000234
Total gradient norm: 0.004618
=== Actor Training Debug (Iteration 389) ===
Q mean: -3.869935
Q std: 2.170527
Actor loss: 3.873874
Action reg: 0.003939
  l1.weight: grad_norm = 0.000619
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000751
Total gradient norm: 0.011641
=== Actor Training Debug (Iteration 390) ===
Q mean: -3.759906
Q std: 2.096722
Actor loss: 3.763865
Action reg: 0.003959
  l1.weight: grad_norm = 0.000986
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001229
Total gradient norm: 0.023609
=== Actor Training Debug (Iteration 391) ===
Q mean: -3.601202
Q std: 2.114849
Actor loss: 3.605157
Action reg: 0.003955
  l1.weight: grad_norm = 0.000852
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001237
Total gradient norm: 0.022870
=== Actor Training Debug (Iteration 392) ===
Q mean: -3.266620
Q std: 2.020460
Actor loss: 3.270528
Action reg: 0.003908
  l1.weight: grad_norm = 0.000785
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000935
Total gradient norm: 0.012329
=== Actor Training Debug (Iteration 393) ===
Q mean: -3.495787
Q std: 2.044852
Actor loss: 3.499691
Action reg: 0.003904
  l1.weight: grad_norm = 0.000447
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000653
Total gradient norm: 0.013313
=== Actor Training Debug (Iteration 394) ===
Q mean: -3.705420
Q std: 2.132721
Actor loss: 3.709358
Action reg: 0.003938
  l1.weight: grad_norm = 0.001313
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001673
Total gradient norm: 0.033744
=== Actor Training Debug (Iteration 395) ===
Q mean: -4.052635
Q std: 2.270119
Actor loss: 4.056513
Action reg: 0.003878
  l1.weight: grad_norm = 0.000767
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000918
Total gradient norm: 0.013367
=== Actor Training Debug (Iteration 396) ===
Q mean: -3.600183
Q std: 2.158998
Actor loss: 3.604153
Action reg: 0.003970
  l1.weight: grad_norm = 0.000447
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000728
Total gradient norm: 0.018487
=== Actor Training Debug (Iteration 397) ===
Q mean: -3.621690
Q std: 2.020033
Actor loss: 3.625659
Action reg: 0.003970
  l1.weight: grad_norm = 0.000354
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000436
Total gradient norm: 0.006745
=== Actor Training Debug (Iteration 398) ===
Q mean: -3.829461
Q std: 2.159712
Actor loss: 3.833400
Action reg: 0.003939
  l1.weight: grad_norm = 0.001199
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001421
Total gradient norm: 0.021105
=== Actor Training Debug (Iteration 399) ===
Q mean: -3.684144
Q std: 2.019563
Actor loss: 3.688098
Action reg: 0.003954
  l1.weight: grad_norm = 0.001155
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001451
Total gradient norm: 0.020532
=== Actor Training Debug (Iteration 400) ===
Q mean: -3.863421
Q std: 2.169859
Actor loss: 3.867379
Action reg: 0.003958
  l1.weight: grad_norm = 0.000794
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001081
Total gradient norm: 0.015281
=== Actor Training Debug (Iteration 401) ===
Q mean: -3.563763
Q std: 2.076428
Actor loss: 3.567701
Action reg: 0.003938
  l1.weight: grad_norm = 0.000806
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001050
Total gradient norm: 0.019164
=== Actor Training Debug (Iteration 402) ===
Q mean: -3.326811
Q std: 2.221447
Actor loss: 3.330715
Action reg: 0.003904
  l1.weight: grad_norm = 0.000514
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.000619
Total gradient norm: 0.011194
=== Actor Training Debug (Iteration 403) ===
Q mean: -3.272159
Q std: 2.135685
Actor loss: 3.276131
Action reg: 0.003972
  l1.weight: grad_norm = 0.002570
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003465
Total gradient norm: 0.032921
=== Actor Training Debug (Iteration 404) ===
Q mean: -3.469933
Q std: 2.021827
Actor loss: 3.473877
Action reg: 0.003945
  l1.weight: grad_norm = 0.000406
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000542
Total gradient norm: 0.009945
=== Actor Training Debug (Iteration 405) ===
Q mean: -3.777054
Q std: 2.105199
Actor loss: 3.781041
Action reg: 0.003987
  l1.weight: grad_norm = 0.001346
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001879
Total gradient norm: 0.019968
=== Actor Training Debug (Iteration 406) ===
Q mean: -3.927934
Q std: 2.207228
Actor loss: 3.931886
Action reg: 0.003953
  l1.weight: grad_norm = 0.002577
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.003536
Total gradient norm: 0.039986
=== Actor Training Debug (Iteration 407) ===
Q mean: -4.110134
Q std: 2.170277
Actor loss: 4.114108
Action reg: 0.003974
  l1.weight: grad_norm = 0.003701
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.006271
Total gradient norm: 0.057882
=== Actor Training Debug (Iteration 408) ===
Q mean: -3.892504
Q std: 1.958583
Actor loss: 3.896453
Action reg: 0.003949
  l1.weight: grad_norm = 0.005150
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.008786
Total gradient norm: 0.083314
=== Actor Training Debug (Iteration 409) ===
Q mean: -3.194689
Q std: 2.266614
Actor loss: 3.198593
Action reg: 0.003904
  l1.weight: grad_norm = 0.003048
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005698
Total gradient norm: 0.060476
=== Actor Training Debug (Iteration 410) ===
Q mean: -3.238360
Q std: 2.093232
Actor loss: 3.242254
Action reg: 0.003894
  l1.weight: grad_norm = 0.018688
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.032043
Total gradient norm: 0.259466
=== Actor Training Debug (Iteration 411) ===
Q mean: -3.450081
Q std: 2.296408
Actor loss: 3.454011
Action reg: 0.003930
  l1.weight: grad_norm = 0.023812
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.043264
Total gradient norm: 0.464388
=== Actor Training Debug (Iteration 412) ===
Q mean: -3.827819
Q std: 2.078578
Actor loss: 3.831765
Action reg: 0.003945
  l1.weight: grad_norm = 0.015367
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.024547
Total gradient norm: 0.384663
=== Actor Training Debug (Iteration 413) ===
Q mean: -3.835371
Q std: 2.081175
Actor loss: 3.839337
Action reg: 0.003966
  l1.weight: grad_norm = 0.000651
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000958
Total gradient norm: 0.013745
=== Actor Training Debug (Iteration 414) ===
Q mean: -3.939820
Q std: 2.165212
Actor loss: 3.943758
Action reg: 0.003938
  l1.weight: grad_norm = 0.001608
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.002743
Total gradient norm: 0.037970
=== Actor Training Debug (Iteration 415) ===
Q mean: -3.541738
Q std: 2.174531
Actor loss: 3.545660
Action reg: 0.003922
  l1.weight: grad_norm = 0.010202
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.016749
Total gradient norm: 0.195964
=== Actor Training Debug (Iteration 416) ===
Q mean: -3.732770
Q std: 2.152004
Actor loss: 3.736623
Action reg: 0.003853
  l1.weight: grad_norm = 0.017840
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.034463
Total gradient norm: 0.385456
=== Actor Training Debug (Iteration 417) ===
Q mean: -3.854023
Q std: 2.147811
Actor loss: 3.857926
Action reg: 0.003903
  l1.weight: grad_norm = 0.013260
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.027771
Total gradient norm: 0.292767
=== Actor Training Debug (Iteration 418) ===
Q mean: -3.615231
Q std: 2.192310
Actor loss: 3.619174
Action reg: 0.003943
  l1.weight: grad_norm = 0.004908
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.007628
Total gradient norm: 0.129188
=== Actor Training Debug (Iteration 419) ===
Q mean: -3.481482
Q std: 2.205340
Actor loss: 3.485388
Action reg: 0.003906
  l1.weight: grad_norm = 0.013743
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.023261
Total gradient norm: 0.385028
=== Actor Training Debug (Iteration 420) ===
Q mean: -3.576242
Q std: 2.371223
Actor loss: 3.580123
Action reg: 0.003882
  l1.weight: grad_norm = 0.014195
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.024707
Total gradient norm: 0.377526
=== Actor Training Debug (Iteration 421) ===
Q mean: -4.027455
Q std: 2.337509
Actor loss: 4.031355
Action reg: 0.003900
  l1.weight: grad_norm = 0.009339
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.019201
Total gradient norm: 0.314261
=== Actor Training Debug (Iteration 422) ===
Q mean: -3.820665
Q std: 2.632318
Actor loss: 3.824571
Action reg: 0.003906
  l1.weight: grad_norm = 0.003041
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.004954
Total gradient norm: 0.082931
=== Actor Training Debug (Iteration 423) ===
Q mean: -3.551460
Q std: 2.258823
Actor loss: 3.555378
Action reg: 0.003918
  l1.weight: grad_norm = 0.001020
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.001322
Total gradient norm: 0.016939
=== Actor Training Debug (Iteration 424) ===
Q mean: -3.650874
Q std: 2.454472
Actor loss: 3.654831
Action reg: 0.003957
  l1.weight: grad_norm = 0.001295
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001758
Total gradient norm: 0.030965
=== Actor Training Debug (Iteration 425) ===
Q mean: -3.577722
Q std: 2.082119
Actor loss: 3.581670
Action reg: 0.003948
  l1.weight: grad_norm = 0.001903
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002624
Total gradient norm: 0.054081
=== Actor Training Debug (Iteration 426) ===
Q mean: -3.761355
Q std: 2.159505
Actor loss: 3.765301
Action reg: 0.003946
  l1.weight: grad_norm = 0.000499
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000708
Total gradient norm: 0.007839
=== Actor Training Debug (Iteration 427) ===
Q mean: -3.678187
Q std: 2.241768
Actor loss: 3.682140
Action reg: 0.003953
  l1.weight: grad_norm = 0.000067
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.000098
Total gradient norm: 0.001632
=== Actor Training Debug (Iteration 428) ===
Q mean: -3.677216
Q std: 2.029567
Actor loss: 3.681198
Action reg: 0.003982
  l1.weight: grad_norm = 0.000062
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000078
Total gradient norm: 0.000855
=== Actor Training Debug (Iteration 429) ===
Q mean: -4.111904
Q std: 2.218359
Actor loss: 4.115887
Action reg: 0.003983
  l1.weight: grad_norm = 0.000032
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.000050
Total gradient norm: 0.001130
=== Actor Training Debug (Iteration 430) ===
Q mean: -3.972589
Q std: 2.384011
Actor loss: 3.976526
Action reg: 0.003938
  l1.weight: grad_norm = 0.000050
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.000066
Total gradient norm: 0.001238
=== Actor Training Debug (Iteration 431) ===
Q mean: -3.483858
Q std: 2.313356
Actor loss: 3.487780
Action reg: 0.003922
  l1.weight: grad_norm = 0.000080
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000113
Total gradient norm: 0.004419
=== Actor Training Debug (Iteration 432) ===
Q mean: -3.141287
Q std: 2.048110
Actor loss: 3.145255
Action reg: 0.003968
  l1.weight: grad_norm = 0.000125
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000153
Total gradient norm: 0.002481
=== Actor Training Debug (Iteration 433) ===
Q mean: -3.411201
Q std: 2.128808
Actor loss: 3.415180
Action reg: 0.003979
  l1.weight: grad_norm = 0.000149
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.000208
Total gradient norm: 0.008120
=== Actor Training Debug (Iteration 434) ===
Q mean: -3.646074
Q std: 2.335790
Actor loss: 3.650038
Action reg: 0.003964
  l1.weight: grad_norm = 0.000758
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000890
Total gradient norm: 0.015506
=== Actor Training Debug (Iteration 435) ===
Q mean: -3.862601
Q std: 2.328826
Actor loss: 3.866480
Action reg: 0.003879
  l1.weight: grad_norm = 0.002184
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002714
Total gradient norm: 0.046981
=== Actor Training Debug (Iteration 436) ===
Q mean: -3.975008
Q std: 2.390301
Actor loss: 3.978925
Action reg: 0.003917
  l1.weight: grad_norm = 0.002469
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.003109
Total gradient norm: 0.073919
=== Actor Training Debug (Iteration 437) ===
Q mean: -3.969290
Q std: 2.248773
Actor loss: 3.973185
Action reg: 0.003895
  l1.weight: grad_norm = 0.003159
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004756
Total gradient norm: 0.176490
=== Actor Training Debug (Iteration 438) ===
Q mean: -3.605227
Q std: 2.060121
Actor loss: 3.609084
Action reg: 0.003857
  l1.weight: grad_norm = 0.005361
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.006713
Total gradient norm: 0.121453
=== Actor Training Debug (Iteration 439) ===
Q mean: -3.116020
Q std: 2.075051
Actor loss: 3.119878
Action reg: 0.003858
  l1.weight: grad_norm = 0.006274
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.008631
Total gradient norm: 0.190033
=== Actor Training Debug (Iteration 440) ===
Q mean: -3.322474
Q std: 2.086084
Actor loss: 3.326385
Action reg: 0.003912
  l1.weight: grad_norm = 0.004050
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006203
Total gradient norm: 0.160372
=== Actor Training Debug (Iteration 441) ===
Q mean: -3.440660
Q std: 2.313480
Actor loss: 3.444574
Action reg: 0.003914
  l1.weight: grad_norm = 0.003419
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004458
Total gradient norm: 0.086764
=== Actor Training Debug (Iteration 442) ===
Q mean: -3.827463
Q std: 2.074371
Actor loss: 3.831355
Action reg: 0.003892
  l1.weight: grad_norm = 0.002760
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.003858
Total gradient norm: 0.093772
=== Actor Training Debug (Iteration 443) ===
Q mean: -3.679489
Q std: 2.065937
Actor loss: 3.683382
Action reg: 0.003893
  l1.weight: grad_norm = 0.002015
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.002645
Total gradient norm: 0.060407
=== Actor Training Debug (Iteration 444) ===
Q mean: -3.609322
Q std: 2.159228
Actor loss: 3.613244
Action reg: 0.003922
  l1.weight: grad_norm = 0.003756
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005829
Total gradient norm: 0.160918
=== Actor Training Debug (Iteration 445) ===
Q mean: -3.524155
Q std: 2.267131
Actor loss: 3.528035
Action reg: 0.003881
  l1.weight: grad_norm = 0.003009
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.004785
Total gradient norm: 0.121988
=== Actor Training Debug (Iteration 446) ===
Q mean: -3.706564
Q std: 2.193562
Actor loss: 3.710512
Action reg: 0.003948
  l1.weight: grad_norm = 0.001106
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001199
Total gradient norm: 0.011874
=== Actor Training Debug (Iteration 447) ===
Q mean: -3.922539
Q std: 2.257075
Actor loss: 3.926492
Action reg: 0.003953
  l1.weight: grad_norm = 0.002213
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002624
Total gradient norm: 0.043681
=== Actor Training Debug (Iteration 448) ===
Q mean: -3.996955
Q std: 2.392905
Actor loss: 4.000870
Action reg: 0.003914
  l1.weight: grad_norm = 0.001562
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.001630
Total gradient norm: 0.015520
=== Actor Training Debug (Iteration 449) ===
Q mean: -3.853167
Q std: 2.107589
Actor loss: 3.857095
Action reg: 0.003928
  l1.weight: grad_norm = 0.001140
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.001308
Total gradient norm: 0.016138
=== Actor Training Debug (Iteration 450) ===
Q mean: -3.568224
Q std: 2.338298
Actor loss: 3.572169
Action reg: 0.003945
  l1.weight: grad_norm = 0.001165
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001285
Total gradient norm: 0.012706
=== Actor Training Debug (Iteration 451) ===
Q mean: -3.590340
Q std: 2.340301
Actor loss: 3.594288
Action reg: 0.003948
  l1.weight: grad_norm = 0.001914
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001833
Total gradient norm: 0.014481
=== Actor Training Debug (Iteration 452) ===
Q mean: -3.563858
Q std: 2.290923
Actor loss: 3.567797
Action reg: 0.003939
  l1.weight: grad_norm = 0.002709
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002680
Total gradient norm: 0.023167
=== Actor Training Debug (Iteration 453) ===
Q mean: -3.561868
Q std: 2.246217
Actor loss: 3.565810
Action reg: 0.003942
  l1.weight: grad_norm = 0.001859
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002460
Total gradient norm: 0.037133
=== Actor Training Debug (Iteration 454) ===
Q mean: -3.683063
Q std: 2.424102
Actor loss: 3.686983
Action reg: 0.003920
  l1.weight: grad_norm = 0.002849
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.002755
Total gradient norm: 0.023725
=== Actor Training Debug (Iteration 455) ===
Q mean: -3.891568
Q std: 2.276789
Actor loss: 3.895498
Action reg: 0.003930
  l1.weight: grad_norm = 0.002064
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002343
Total gradient norm: 0.030822
=== Actor Training Debug (Iteration 456) ===
Q mean: -3.561593
Q std: 2.068326
Actor loss: 3.565549
Action reg: 0.003955
  l1.weight: grad_norm = 0.001571
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001784
Total gradient norm: 0.025053
=== Actor Training Debug (Iteration 457) ===
Q mean: -3.600203
Q std: 2.119225
Actor loss: 3.604173
Action reg: 0.003970
  l1.weight: grad_norm = 0.001003
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001153
Total gradient norm: 0.017776
=== Actor Training Debug (Iteration 458) ===
Q mean: -3.846759
Q std: 2.470543
Actor loss: 3.850681
Action reg: 0.003922
  l1.weight: grad_norm = 0.003155
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002951
Total gradient norm: 0.020851
=== Actor Training Debug (Iteration 459) ===
Q mean: -3.982622
Q std: 2.161862
Actor loss: 3.986568
Action reg: 0.003946
  l1.weight: grad_norm = 0.005164
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.005605
Total gradient norm: 0.050572
=== Actor Training Debug (Iteration 460) ===
Q mean: -3.696373
Q std: 2.118792
Actor loss: 3.700288
Action reg: 0.003914
  l1.weight: grad_norm = 0.002385
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.002710
Total gradient norm: 0.037316
=== Actor Training Debug (Iteration 461) ===
Q mean: -3.534769
Q std: 2.211875
Actor loss: 3.538718
Action reg: 0.003949
  l1.weight: grad_norm = 0.001902
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.002401
Total gradient norm: 0.033761
=== Actor Training Debug (Iteration 462) ===
Q mean: -3.487308
Q std: 2.184535
Actor loss: 3.491287
Action reg: 0.003979
  l1.weight: grad_norm = 0.002842
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003040
Total gradient norm: 0.026738
=== Actor Training Debug (Iteration 463) ===
Q mean: -4.147059
Q std: 2.317091
Actor loss: 4.151015
Action reg: 0.003955
  l1.weight: grad_norm = 0.001435
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.001627
Total gradient norm: 0.017639
=== Actor Training Debug (Iteration 464) ===
Q mean: -4.404747
Q std: 2.359976
Actor loss: 4.408720
Action reg: 0.003974
  l1.weight: grad_norm = 0.001766
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001695
Total gradient norm: 0.012830
=== Actor Training Debug (Iteration 465) ===
Q mean: -3.727314
Q std: 2.321866
Actor loss: 3.731256
Action reg: 0.003941
  l1.weight: grad_norm = 0.002542
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002653
Total gradient norm: 0.018512
=== Actor Training Debug (Iteration 466) ===
Q mean: -3.646932
Q std: 2.375818
Actor loss: 3.650904
Action reg: 0.003972
  l1.weight: grad_norm = 0.000959
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001134
Total gradient norm: 0.010391
=== Actor Training Debug (Iteration 467) ===
Q mean: -3.711324
Q std: 2.238622
Actor loss: 3.715273
Action reg: 0.003948
  l1.weight: grad_norm = 0.001528
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001752
Total gradient norm: 0.019902
=== Actor Training Debug (Iteration 468) ===
Q mean: -4.006292
Q std: 2.461846
Actor loss: 4.010219
Action reg: 0.003927
  l1.weight: grad_norm = 0.001583
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.001881
Total gradient norm: 0.021916
=== Actor Training Debug (Iteration 469) ===
Q mean: -3.750722
Q std: 2.455454
Actor loss: 3.754636
Action reg: 0.003914
  l1.weight: grad_norm = 0.003239
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.003911
Total gradient norm: 0.049003
=== Actor Training Debug (Iteration 470) ===
Q mean: -3.826326
Q std: 2.142563
Actor loss: 3.830288
Action reg: 0.003962
  l1.weight: grad_norm = 0.000818
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001091
Total gradient norm: 0.015079
=== Actor Training Debug (Iteration 471) ===
Q mean: -3.511416
Q std: 2.321705
Actor loss: 3.515341
Action reg: 0.003925
  l1.weight: grad_norm = 0.001720
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.002185
Total gradient norm: 0.024618
=== Actor Training Debug (Iteration 472) ===
Q mean: -4.115510
Q std: 2.307822
Actor loss: 4.119476
Action reg: 0.003965
  l1.weight: grad_norm = 0.002605
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.002453
Total gradient norm: 0.016782
=== Actor Training Debug (Iteration 473) ===
Q mean: -3.679664
Q std: 2.278841
Actor loss: 3.683619
Action reg: 0.003955
  l1.weight: grad_norm = 0.002590
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002644
Total gradient norm: 0.021976
=== Actor Training Debug (Iteration 474) ===
Q mean: -3.418174
Q std: 2.328319
Actor loss: 3.422118
Action reg: 0.003945
  l1.weight: grad_norm = 0.002078
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.002003
Total gradient norm: 0.010972
=== Actor Training Debug (Iteration 475) ===
Q mean: -3.493608
Q std: 2.645772
Actor loss: 3.497554
Action reg: 0.003946
  l1.weight: grad_norm = 0.000850
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.000805
Total gradient norm: 0.005828
=== Actor Training Debug (Iteration 476) ===
Q mean: -3.683798
Q std: 2.479587
Actor loss: 3.687769
Action reg: 0.003972
  l1.weight: grad_norm = 0.001251
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001639
Total gradient norm: 0.022881
=== Actor Training Debug (Iteration 477) ===
Q mean: -3.900902
Q std: 2.458931
Actor loss: 3.904836
Action reg: 0.003934
  l1.weight: grad_norm = 0.002207
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.002708
Total gradient norm: 0.033608
=== Actor Training Debug (Iteration 478) ===
Q mean: -4.037403
Q std: 2.524808
Actor loss: 4.041374
Action reg: 0.003971
  l1.weight: grad_norm = 0.002123
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001889
Total gradient norm: 0.011152
=== Actor Training Debug (Iteration 479) ===
Q mean: -3.543742
Q std: 2.414576
Actor loss: 3.547677
Action reg: 0.003935
  l1.weight: grad_norm = 0.003344
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.003539
Total gradient norm: 0.045734
=== Actor Training Debug (Iteration 480) ===
Q mean: -3.741294
Q std: 2.316227
Actor loss: 3.745257
Action reg: 0.003963
  l1.weight: grad_norm = 0.001707
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001751
Total gradient norm: 0.014707
=== Actor Training Debug (Iteration 481) ===
Q mean: -3.736216
Q std: 2.314308
Actor loss: 3.740185
Action reg: 0.003969
  l1.weight: grad_norm = 0.001863
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.001860
Total gradient norm: 0.017455
=== Actor Training Debug (Iteration 482) ===
Q mean: -3.649506
Q std: 2.295074
Actor loss: 3.653436
Action reg: 0.003930
  l1.weight: grad_norm = 0.001601
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.001919
Total gradient norm: 0.024677
=== Actor Training Debug (Iteration 483) ===
Q mean: -3.988932
Q std: 2.342514
Actor loss: 3.992905
Action reg: 0.003973
  l1.weight: grad_norm = 0.001520
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001972
Total gradient norm: 0.025140
=== Actor Training Debug (Iteration 484) ===
Q mean: -3.762696
Q std: 2.224023
Actor loss: 3.766654
Action reg: 0.003959
  l1.weight: grad_norm = 0.000659
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.000642
Total gradient norm: 0.004600
=== Actor Training Debug (Iteration 485) ===
Q mean: -4.159390
Q std: 2.521963
Actor loss: 4.163357
Action reg: 0.003967
  l1.weight: grad_norm = 0.001378
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001374
Total gradient norm: 0.009189
=== Actor Training Debug (Iteration 486) ===
Q mean: -4.049829
Q std: 2.418657
Actor loss: 4.053767
Action reg: 0.003937
  l1.weight: grad_norm = 0.002874
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003420
Total gradient norm: 0.039122
=== Actor Training Debug (Iteration 487) ===
Q mean: -3.635989
Q std: 2.382209
Actor loss: 3.639921
Action reg: 0.003932
  l1.weight: grad_norm = 0.002537
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002469
Total gradient norm: 0.027742
=== Actor Training Debug (Iteration 488) ===
Q mean: -3.249180
Q std: 2.170809
Actor loss: 3.253082
Action reg: 0.003902
  l1.weight: grad_norm = 0.002020
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.002095
Total gradient norm: 0.016560
=== Actor Training Debug (Iteration 489) ===
Q mean: -3.432481
Q std: 2.251972
Actor loss: 3.436388
Action reg: 0.003907
  l1.weight: grad_norm = 0.002413
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.002996
Total gradient norm: 0.040398
=== Actor Training Debug (Iteration 490) ===
Q mean: -3.719153
Q std: 2.235177
Actor loss: 3.723101
Action reg: 0.003948
  l1.weight: grad_norm = 0.003430
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004140
Total gradient norm: 0.045715
=== Actor Training Debug (Iteration 491) ===
Q mean: -4.008379
Q std: 2.532251
Actor loss: 4.012295
Action reg: 0.003916
  l1.weight: grad_norm = 0.016158
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.024057
Total gradient norm: 0.325742
=== Actor Training Debug (Iteration 492) ===
Q mean: -4.369369
Q std: 2.756060
Actor loss: 4.373250
Action reg: 0.003882
  l1.weight: grad_norm = 0.020583
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.034586
Total gradient norm: 0.498216
=== Actor Training Debug (Iteration 493) ===
Q mean: -3.934371
Q std: 2.390085
Actor loss: 3.938268
Action reg: 0.003897
  l1.weight: grad_norm = 0.008766
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.012156
Total gradient norm: 0.135012
=== Actor Training Debug (Iteration 494) ===
Q mean: -3.592004
Q std: 2.531626
Actor loss: 3.595843
Action reg: 0.003839
  l1.weight: grad_norm = 0.016073
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.027021
Total gradient norm: 0.314386
=== Actor Training Debug (Iteration 495) ===
Q mean: -3.419148
Q std: 2.302979
Actor loss: 3.423061
Action reg: 0.003913
  l1.weight: grad_norm = 0.009342
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.012473
Total gradient norm: 0.094032
=== Actor Training Debug (Iteration 496) ===
Q mean: -3.695462
Q std: 2.272727
Actor loss: 3.699442
Action reg: 0.003980
  l1.weight: grad_norm = 0.004074
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005665
Total gradient norm: 0.075576
=== Actor Training Debug (Iteration 497) ===
Q mean: -3.868617
Q std: 2.598826
Actor loss: 3.872542
Action reg: 0.003925
  l1.weight: grad_norm = 0.002674
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002657
Total gradient norm: 0.020017
=== Actor Training Debug (Iteration 498) ===
Q mean: -3.903282
Q std: 2.515704
Actor loss: 3.907210
Action reg: 0.003928
  l1.weight: grad_norm = 0.002007
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.002076
Total gradient norm: 0.011782
=== Actor Training Debug (Iteration 499) ===
Q mean: -4.224167
Q std: 2.713884
Actor loss: 4.228116
Action reg: 0.003949
  l1.weight: grad_norm = 0.002608
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.002933
Total gradient norm: 0.030042
=== Actor Training Debug (Iteration 500) ===
Q mean: -3.670305
Q std: 2.406813
Actor loss: 3.674286
Action reg: 0.003981
  l1.weight: grad_norm = 0.003106
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003868
Total gradient norm: 0.049288
  Average reward: -350.704 | Average length: 100.0
Evaluation at episode 55: -350.704
=== Actor Training Debug (Iteration 501) ===
Q mean: -3.639800
Q std: 2.331449
Actor loss: 3.643761
Action reg: 0.003961
  l1.weight: grad_norm = 0.001450
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.001696
Total gradient norm: 0.014285
=== Actor Training Debug (Iteration 502) ===
Q mean: -3.771321
Q std: 2.474739
Actor loss: 3.775274
Action reg: 0.003953
  l1.weight: grad_norm = 0.002751
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.003333
Total gradient norm: 0.041293
=== Actor Training Debug (Iteration 503) ===
Q mean: -4.040709
Q std: 2.358433
Actor loss: 4.044667
Action reg: 0.003959
  l1.weight: grad_norm = 0.001093
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.001143
Total gradient norm: 0.011764
=== Actor Training Debug (Iteration 504) ===
Q mean: -3.884752
Q std: 2.439373
Actor loss: 3.888697
Action reg: 0.003945
  l1.weight: grad_norm = 0.001187
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001462
Total gradient norm: 0.014192
=== Actor Training Debug (Iteration 505) ===
Q mean: -4.015741
Q std: 2.743118
Actor loss: 4.019681
Action reg: 0.003940
  l1.weight: grad_norm = 0.002466
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.002700
Total gradient norm: 0.025602
=== Actor Training Debug (Iteration 506) ===
Q mean: -3.970273
Q std: 2.505207
Actor loss: 3.974230
Action reg: 0.003957
  l1.weight: grad_norm = 0.000869
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000806
Total gradient norm: 0.005389
=== Actor Training Debug (Iteration 507) ===
Q mean: -3.821490
Q std: 2.451785
Actor loss: 3.825457
Action reg: 0.003968
  l1.weight: grad_norm = 0.002658
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003202
Total gradient norm: 0.042467
=== Actor Training Debug (Iteration 508) ===
Q mean: -3.634707
Q std: 2.316459
Actor loss: 3.638650
Action reg: 0.003943
  l1.weight: grad_norm = 0.001507
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001534
Total gradient norm: 0.014924
=== Actor Training Debug (Iteration 509) ===
Q mean: -3.693794
Q std: 2.608046
Actor loss: 3.697738
Action reg: 0.003944
  l1.weight: grad_norm = 0.001495
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.001597
Total gradient norm: 0.011992
=== Actor Training Debug (Iteration 510) ===
Q mean: -4.138183
Q std: 2.638790
Actor loss: 4.142127
Action reg: 0.003943
  l1.weight: grad_norm = 0.001789
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001951
Total gradient norm: 0.022238
=== Actor Training Debug (Iteration 511) ===
Q mean: -3.684225
Q std: 2.500150
Actor loss: 3.688154
Action reg: 0.003929
  l1.weight: grad_norm = 0.001826
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.002159
Total gradient norm: 0.022904
=== Actor Training Debug (Iteration 512) ===
Q mean: -3.901088
Q std: 2.351031
Actor loss: 3.905029
Action reg: 0.003942
  l1.weight: grad_norm = 0.001519
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.001568
Total gradient norm: 0.010149
=== Actor Training Debug (Iteration 513) ===
Q mean: -3.628281
Q std: 2.608155
Actor loss: 3.632211
Action reg: 0.003930
  l1.weight: grad_norm = 0.002648
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.003310
Total gradient norm: 0.044910
=== Actor Training Debug (Iteration 514) ===
Q mean: -3.682060
Q std: 2.387921
Actor loss: 3.686030
Action reg: 0.003970
  l1.weight: grad_norm = 0.001508
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001711
Total gradient norm: 0.015017
=== Actor Training Debug (Iteration 515) ===
Q mean: -3.758739
Q std: 2.481131
Actor loss: 3.762666
Action reg: 0.003928
  l1.weight: grad_norm = 0.000872
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.000952
Total gradient norm: 0.010508
=== Actor Training Debug (Iteration 516) ===
Q mean: -3.716428
Q std: 2.398880
Actor loss: 3.720408
Action reg: 0.003980
  l1.weight: grad_norm = 0.001785
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002110
Total gradient norm: 0.019358
=== Actor Training Debug (Iteration 517) ===
Q mean: -4.197259
Q std: 2.649865
Actor loss: 4.201221
Action reg: 0.003961
  l1.weight: grad_norm = 0.002443
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002774
Total gradient norm: 0.023855
=== Actor Training Debug (Iteration 518) ===
Q mean: -4.094375
Q std: 2.534125
Actor loss: 4.098342
Action reg: 0.003967
  l1.weight: grad_norm = 0.002736
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003297
Total gradient norm: 0.041710
=== Actor Training Debug (Iteration 519) ===
Q mean: -3.513779
Q std: 2.274994
Actor loss: 3.517736
Action reg: 0.003957
  l1.weight: grad_norm = 0.001541
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.002178
Total gradient norm: 0.034002
=== Actor Training Debug (Iteration 520) ===
Q mean: -3.501074
Q std: 2.180323
Actor loss: 3.505045
Action reg: 0.003971
  l1.weight: grad_norm = 0.001589
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001759
Total gradient norm: 0.021898
=== Actor Training Debug (Iteration 521) ===
Q mean: -4.129567
Q std: 2.633449
Actor loss: 4.133502
Action reg: 0.003935
  l1.weight: grad_norm = 0.000558
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.000563
Total gradient norm: 0.004305
=== Actor Training Debug (Iteration 522) ===
Q mean: -4.176839
Q std: 2.507371
Actor loss: 4.180797
Action reg: 0.003958
  l1.weight: grad_norm = 0.001288
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.001591
Total gradient norm: 0.018062
=== Actor Training Debug (Iteration 523) ===
Q mean: -4.159766
Q std: 2.612413
Actor loss: 4.163722
Action reg: 0.003956
  l1.weight: grad_norm = 0.001725
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.001606
Total gradient norm: 0.009802
=== Actor Training Debug (Iteration 524) ===
Q mean: -3.288554
Q std: 2.459468
Actor loss: 3.292459
Action reg: 0.003904
  l1.weight: grad_norm = 0.002142
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.002281
Total gradient norm: 0.016392
=== Actor Training Debug (Iteration 525) ===
Q mean: -3.527367
Q std: 2.694340
Actor loss: 3.531294
Action reg: 0.003927
  l1.weight: grad_norm = 0.003079
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.003067
Total gradient norm: 0.033949
=== Actor Training Debug (Iteration 526) ===
Q mean: -3.566188
Q std: 2.470603
Actor loss: 3.570117
Action reg: 0.003929
  l1.weight: grad_norm = 0.002370
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.003193
Total gradient norm: 0.043586
=== Actor Training Debug (Iteration 527) ===
Q mean: -4.109364
Q std: 2.643835
Actor loss: 4.113284
Action reg: 0.003920
  l1.weight: grad_norm = 0.003221
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.003802
Total gradient norm: 0.042588
=== Actor Training Debug (Iteration 528) ===
Q mean: -4.313407
Q std: 2.580784
Actor loss: 4.317339
Action reg: 0.003932
  l1.weight: grad_norm = 0.002227
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.002334
Total gradient norm: 0.013764
=== Actor Training Debug (Iteration 529) ===
Q mean: -3.866720
Q std: 2.315830
Actor loss: 3.870660
Action reg: 0.003940
  l1.weight: grad_norm = 0.001404
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.001494
Total gradient norm: 0.010462
=== Actor Training Debug (Iteration 530) ===
Q mean: -3.413102
Q std: 2.228381
Actor loss: 3.417072
Action reg: 0.003970
  l1.weight: grad_norm = 0.001917
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001865
Total gradient norm: 0.015868
=== Actor Training Debug (Iteration 531) ===
Q mean: -3.315026
Q std: 2.444508
Actor loss: 3.318995
Action reg: 0.003969
  l1.weight: grad_norm = 0.000998
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001023
Total gradient norm: 0.007239
=== Actor Training Debug (Iteration 532) ===
Q mean: -3.495673
Q std: 2.261631
Actor loss: 3.499599
Action reg: 0.003926
  l1.weight: grad_norm = 0.001938
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.001843
Total gradient norm: 0.016910
=== Actor Training Debug (Iteration 533) ===
Q mean: -4.607898
Q std: 2.670174
Actor loss: 4.611842
Action reg: 0.003944
  l1.weight: grad_norm = 0.002403
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.002740
Total gradient norm: 0.029016
=== Actor Training Debug (Iteration 534) ===
Q mean: -4.641833
Q std: 2.631177
Actor loss: 4.645764
Action reg: 0.003932
  l1.weight: grad_norm = 0.001752
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.001878
Total gradient norm: 0.019041
=== Actor Training Debug (Iteration 535) ===
Q mean: -4.210778
Q std: 2.644063
Actor loss: 4.214700
Action reg: 0.003922
  l1.weight: grad_norm = 0.002582
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.002672
Total gradient norm: 0.016110
=== Actor Training Debug (Iteration 536) ===
Q mean: -3.633006
Q std: 2.440059
Actor loss: 3.636944
Action reg: 0.003938
  l1.weight: grad_norm = 0.002265
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002367
Total gradient norm: 0.018617
=== Actor Training Debug (Iteration 537) ===
Q mean: -3.071398
Q std: 2.448764
Actor loss: 3.075350
Action reg: 0.003952
  l1.weight: grad_norm = 0.003533
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.004368
Total gradient norm: 0.045198
=== Actor Training Debug (Iteration 538) ===
Q mean: -3.410360
Q std: 2.534040
Actor loss: 3.414283
Action reg: 0.003923
  l1.weight: grad_norm = 0.001468
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.001669
Total gradient norm: 0.015001
=== Actor Training Debug (Iteration 539) ===
Q mean: -4.395082
Q std: 2.662526
Actor loss: 4.399071
Action reg: 0.003989
  l1.weight: grad_norm = 0.000983
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001013
Total gradient norm: 0.005689
=== Actor Training Debug (Iteration 540) ===
Q mean: -4.464603
Q std: 2.613005
Actor loss: 4.468559
Action reg: 0.003956
  l1.weight: grad_norm = 0.001874
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.002145
Total gradient norm: 0.025455
=== Actor Training Debug (Iteration 541) ===
Q mean: -4.017716
Q std: 2.425650
Actor loss: 4.021680
Action reg: 0.003963
  l1.weight: grad_norm = 0.003361
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004255
Total gradient norm: 0.057583
=== Actor Training Debug (Iteration 542) ===
Q mean: -3.481085
Q std: 2.293288
Actor loss: 3.485033
Action reg: 0.003948
  l1.weight: grad_norm = 0.003256
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.004546
Total gradient norm: 0.065461
=== Actor Training Debug (Iteration 543) ===
Q mean: -3.146436
Q std: 2.253032
Actor loss: 3.150378
Action reg: 0.003942
  l1.weight: grad_norm = 0.001419
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.001309
Total gradient norm: 0.010906
=== Actor Training Debug (Iteration 544) ===
Q mean: -3.562085
Q std: 2.312950
Actor loss: 3.566054
Action reg: 0.003969
  l1.weight: grad_norm = 0.001581
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001979
Total gradient norm: 0.025347
=== Actor Training Debug (Iteration 545) ===
Q mean: -4.155232
Q std: 2.737447
Actor loss: 4.159139
Action reg: 0.003907
  l1.weight: grad_norm = 0.003616
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.003445
Total gradient norm: 0.020395
=== Actor Training Debug (Iteration 546) ===
Q mean: -4.586827
Q std: 2.772334
Actor loss: 4.590790
Action reg: 0.003962
  l1.weight: grad_norm = 0.001974
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.001997
Total gradient norm: 0.011458
=== Actor Training Debug (Iteration 547) ===
Q mean: -4.515599
Q std: 2.645861
Actor loss: 4.519590
Action reg: 0.003991
  l1.weight: grad_norm = 0.000895
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000941
Total gradient norm: 0.006822
=== Actor Training Debug (Iteration 548) ===
Q mean: -3.422925
Q std: 2.393216
Actor loss: 3.426879
Action reg: 0.003954
  l1.weight: grad_norm = 0.002428
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.002519
Total gradient norm: 0.028574
=== Actor Training Debug (Iteration 549) ===
Q mean: -3.463818
Q std: 2.383509
Actor loss: 3.467766
Action reg: 0.003948
  l1.weight: grad_norm = 0.000973
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.000979
Total gradient norm: 0.011026
=== Actor Training Debug (Iteration 550) ===
Q mean: -4.033191
Q std: 2.753055
Actor loss: 4.037122
Action reg: 0.003931
  l1.weight: grad_norm = 0.002426
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.002578
Total gradient norm: 0.018238
=== Actor Training Debug (Iteration 551) ===
Q mean: -4.320720
Q std: 2.680078
Actor loss: 4.324695
Action reg: 0.003975
  l1.weight: grad_norm = 0.000958
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001129
Total gradient norm: 0.013315
=== Actor Training Debug (Iteration 552) ===
Q mean: -4.190295
Q std: 2.661008
Actor loss: 4.194170
Action reg: 0.003875
  l1.weight: grad_norm = 0.001098
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.001232
Total gradient norm: 0.015823
=== Actor Training Debug (Iteration 553) ===
Q mean: -3.837512
Q std: 2.757609
Actor loss: 3.841482
Action reg: 0.003970
  l1.weight: grad_norm = 0.002131
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002524
Total gradient norm: 0.034203
=== Actor Training Debug (Iteration 554) ===
Q mean: -3.424517
Q std: 2.357710
Actor loss: 3.428429
Action reg: 0.003912
  l1.weight: grad_norm = 0.001964
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.002371
Total gradient norm: 0.028772
=== Actor Training Debug (Iteration 555) ===
Q mean: -3.427115
Q std: 2.390914
Actor loss: 3.431106
Action reg: 0.003991
  l1.weight: grad_norm = 0.001393
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001397
Total gradient norm: 0.007630
=== Actor Training Debug (Iteration 556) ===
Q mean: -4.027338
Q std: 2.613845
Actor loss: 4.031300
Action reg: 0.003963
  l1.weight: grad_norm = 0.001865
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.002180
Total gradient norm: 0.028460
=== Actor Training Debug (Iteration 557) ===
Q mean: -4.100425
Q std: 2.564848
Actor loss: 4.104399
Action reg: 0.003974
  l1.weight: grad_norm = 0.001585
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002093
Total gradient norm: 0.023353
=== Actor Training Debug (Iteration 558) ===
Q mean: -3.924326
Q std: 2.632611
Actor loss: 3.928302
Action reg: 0.003976
  l1.weight: grad_norm = 0.003637
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004067
Total gradient norm: 0.041695
=== Actor Training Debug (Iteration 559) ===
Q mean: -3.777686
Q std: 2.539057
Actor loss: 3.781643
Action reg: 0.003958
  l1.weight: grad_norm = 0.001528
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.001585
Total gradient norm: 0.010740
=== Actor Training Debug (Iteration 560) ===
Q mean: -3.587452
Q std: 2.483790
Actor loss: 3.591406
Action reg: 0.003954
  l1.weight: grad_norm = 0.002215
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002576
Total gradient norm: 0.024433
=== Actor Training Debug (Iteration 561) ===
Q mean: -4.101733
Q std: 2.650600
Actor loss: 4.105669
Action reg: 0.003936
  l1.weight: grad_norm = 0.003708
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.004069
Total gradient norm: 0.032567
=== Actor Training Debug (Iteration 562) ===
Q mean: -4.064083
Q std: 2.952297
Actor loss: 4.068048
Action reg: 0.003965
  l1.weight: grad_norm = 0.003393
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.003518
Total gradient norm: 0.024266
=== Actor Training Debug (Iteration 563) ===
Q mean: -3.634782
Q std: 2.625915
Actor loss: 3.638688
Action reg: 0.003906
  l1.weight: grad_norm = 0.003340
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.004807
Total gradient norm: 0.066743
=== Actor Training Debug (Iteration 564) ===
Q mean: -4.080513
Q std: 2.726189
Actor loss: 4.084485
Action reg: 0.003971
  l1.weight: grad_norm = 0.002423
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.002741
Total gradient norm: 0.030035
=== Actor Training Debug (Iteration 565) ===
Q mean: -3.828417
Q std: 2.605549
Actor loss: 3.832371
Action reg: 0.003954
  l1.weight: grad_norm = 0.002945
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.004065
Total gradient norm: 0.062396
=== Actor Training Debug (Iteration 566) ===
Q mean: -3.455155
Q std: 2.580907
Actor loss: 3.459099
Action reg: 0.003944
  l1.weight: grad_norm = 0.002991
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.003884
Total gradient norm: 0.041346
=== Actor Training Debug (Iteration 567) ===
Q mean: -3.910429
Q std: 2.678556
Actor loss: 3.914371
Action reg: 0.003943
  l1.weight: grad_norm = 0.000791
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.000856
Total gradient norm: 0.007471
=== Actor Training Debug (Iteration 568) ===
Q mean: -4.026479
Q std: 2.989879
Actor loss: 4.030458
Action reg: 0.003980
  l1.weight: grad_norm = 0.003607
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.004618
Total gradient norm: 0.051980
=== Actor Training Debug (Iteration 569) ===
Q mean: -4.211040
Q std: 2.883039
Actor loss: 4.214983
Action reg: 0.003942
  l1.weight: grad_norm = 0.002491
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.002538
Total gradient norm: 0.024446
=== Actor Training Debug (Iteration 570) ===
Q mean: -3.225826
Q std: 2.577428
Actor loss: 3.229781
Action reg: 0.003955
  l1.weight: grad_norm = 0.001829
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001813
Total gradient norm: 0.009362
=== Actor Training Debug (Iteration 571) ===
Q mean: -3.648648
Q std: 2.614688
Actor loss: 3.652593
Action reg: 0.003944
  l1.weight: grad_norm = 0.001646
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.001951
Total gradient norm: 0.026804
=== Actor Training Debug (Iteration 572) ===
Q mean: -4.192052
Q std: 2.660879
Actor loss: 4.195998
Action reg: 0.003946
  l1.weight: grad_norm = 0.002584
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.003197
Total gradient norm: 0.031343
=== Actor Training Debug (Iteration 573) ===
Q mean: -4.285412
Q std: 2.786484
Actor loss: 4.289337
Action reg: 0.003925
  l1.weight: grad_norm = 0.001970
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.002263
Total gradient norm: 0.022944
=== Actor Training Debug (Iteration 574) ===
Q mean: -3.565550
Q std: 2.361572
Actor loss: 3.569489
Action reg: 0.003940
  l1.weight: grad_norm = 0.002362
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.003043
Total gradient norm: 0.041636
=== Actor Training Debug (Iteration 575) ===
Q mean: -4.180545
Q std: 2.792693
Actor loss: 4.184515
Action reg: 0.003969
  l1.weight: grad_norm = 0.002094
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002517
Total gradient norm: 0.024797
=== Actor Training Debug (Iteration 576) ===
Q mean: -3.805927
Q std: 2.491273
Actor loss: 3.809845
Action reg: 0.003919
  l1.weight: grad_norm = 0.000842
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.000837
Total gradient norm: 0.005941
=== Actor Training Debug (Iteration 577) ===
Q mean: -4.009157
Q std: 2.779358
Actor loss: 4.013107
Action reg: 0.003950
  l1.weight: grad_norm = 0.004917
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.005949
Total gradient norm: 0.066668
=== Actor Training Debug (Iteration 578) ===
Q mean: -3.723745
Q std: 2.744493
Actor loss: 3.727671
Action reg: 0.003926
  l1.weight: grad_norm = 0.001167
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.001274
Total gradient norm: 0.008593
=== Actor Training Debug (Iteration 579) ===
Q mean: -3.870426
Q std: 2.912559
Actor loss: 3.874351
Action reg: 0.003925
  l1.weight: grad_norm = 0.001316
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.001408
Total gradient norm: 0.011086
=== Actor Training Debug (Iteration 580) ===
Q mean: -3.769210
Q std: 2.553126
Actor loss: 3.773159
Action reg: 0.003949
  l1.weight: grad_norm = 0.001410
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.001381
Total gradient norm: 0.009044
=== Actor Training Debug (Iteration 581) ===
Q mean: -3.787414
Q std: 2.587740
Actor loss: 3.791383
Action reg: 0.003970
  l1.weight: grad_norm = 0.002935
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.003512
Total gradient norm: 0.041057
=== Actor Training Debug (Iteration 582) ===
Q mean: -3.854024
Q std: 2.792736
Actor loss: 3.857961
Action reg: 0.003937
  l1.weight: grad_norm = 0.002495
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.003052
Total gradient norm: 0.039377
=== Actor Training Debug (Iteration 583) ===
Q mean: -3.898210
Q std: 2.579124
Actor loss: 3.902119
Action reg: 0.003909
  l1.weight: grad_norm = 0.002459
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.002541
Total gradient norm: 0.019867
=== Actor Training Debug (Iteration 584) ===
Q mean: -3.879669
Q std: 2.810184
Actor loss: 3.883580
Action reg: 0.003911
  l1.weight: grad_norm = 0.005382
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.006965
Total gradient norm: 0.092501
=== Actor Training Debug (Iteration 585) ===
Q mean: -3.680796
Q std: 2.590742
Actor loss: 3.684728
Action reg: 0.003932
  l1.weight: grad_norm = 0.003557
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.004465
Total gradient norm: 0.043793
=== Actor Training Debug (Iteration 586) ===
Q mean: -4.348901
Q std: 2.801681
Actor loss: 4.352890
Action reg: 0.003990
  l1.weight: grad_norm = 0.002680
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003557
Total gradient norm: 0.048071
=== Actor Training Debug (Iteration 587) ===
Q mean: -3.739726
Q std: 2.472148
Actor loss: 3.743642
Action reg: 0.003917
  l1.weight: grad_norm = 0.004987
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.005917
Total gradient norm: 0.059424
=== Actor Training Debug (Iteration 588) ===
Q mean: -3.594985
Q std: 2.567991
Actor loss: 3.598935
Action reg: 0.003950
  l1.weight: grad_norm = 0.001884
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.002508
Total gradient norm: 0.030660
=== Actor Training Debug (Iteration 589) ===
Q mean: -3.686805
Q std: 2.692021
Actor loss: 3.690745
Action reg: 0.003940
  l1.weight: grad_norm = 0.002939
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.002892
Total gradient norm: 0.017763
=== Actor Training Debug (Iteration 590) ===
Q mean: -4.260163
Q std: 2.884135
Actor loss: 4.264120
Action reg: 0.003956
  l1.weight: grad_norm = 0.001824
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.002269
Total gradient norm: 0.024278
=== Actor Training Debug (Iteration 591) ===
Q mean: -3.828268
Q std: 2.675733
Actor loss: 3.832237
Action reg: 0.003969
  l1.weight: grad_norm = 0.004906
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.006538
Total gradient norm: 0.081066
=== Actor Training Debug (Iteration 592) ===
Q mean: -3.864750
Q std: 2.558622
Actor loss: 3.868703
Action reg: 0.003954
  l1.weight: grad_norm = 0.002793
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003559
Total gradient norm: 0.038550
=== Actor Training Debug (Iteration 593) ===
Q mean: -3.948070
Q std: 2.620838
Actor loss: 3.952016
Action reg: 0.003945
  l1.weight: grad_norm = 0.003786
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.004014
Total gradient norm: 0.034359
=== Actor Training Debug (Iteration 594) ===
Q mean: -3.647028
Q std: 2.451222
Actor loss: 3.650995
Action reg: 0.003967
  l1.weight: grad_norm = 0.002397
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.002423
Total gradient norm: 0.014185
=== Actor Training Debug (Iteration 595) ===
Q mean: -3.928731
Q std: 2.794909
Actor loss: 3.932703
Action reg: 0.003972
  l1.weight: grad_norm = 0.001554
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.001486
Total gradient norm: 0.010224
=== Actor Training Debug (Iteration 596) ===
Q mean: -3.962718
Q std: 2.916491
Actor loss: 3.966652
Action reg: 0.003934
  l1.weight: grad_norm = 0.003829
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004585
Total gradient norm: 0.052337
=== Actor Training Debug (Iteration 597) ===
Q mean: -3.721177
Q std: 2.722795
Actor loss: 3.725137
Action reg: 0.003960
  l1.weight: grad_norm = 0.001370
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.001511
Total gradient norm: 0.016256
=== Actor Training Debug (Iteration 598) ===
Q mean: -3.907379
Q std: 2.903720
Actor loss: 3.911324
Action reg: 0.003945
  l1.weight: grad_norm = 0.002601
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.002763
Total gradient norm: 0.019353
=== Actor Training Debug (Iteration 599) ===
Q mean: -3.951176
Q std: 2.707605
Actor loss: 3.955164
Action reg: 0.003988
  l1.weight: grad_norm = 0.001829
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001846
Total gradient norm: 0.011932
=== Actor Training Debug (Iteration 600) ===
Q mean: -3.633851
Q std: 3.028604
Actor loss: 3.637826
Action reg: 0.003975
  l1.weight: grad_norm = 0.001504
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.001535
Total gradient norm: 0.010458
=== Actor Training Debug (Iteration 601) ===
Q mean: -3.396952
Q std: 2.879009
Actor loss: 3.400914
Action reg: 0.003963
  l1.weight: grad_norm = 0.001729
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.002193
Total gradient norm: 0.027912
=== Actor Training Debug (Iteration 602) ===
Q mean: -4.100443
Q std: 3.087795
Actor loss: 4.104376
Action reg: 0.003934
  l1.weight: grad_norm = 0.002135
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.002248
Total gradient norm: 0.019574
=== Actor Training Debug (Iteration 603) ===
Q mean: -4.099163
Q std: 2.849076
Actor loss: 4.103102
Action reg: 0.003939
  l1.weight: grad_norm = 0.002621
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.003186
Total gradient norm: 0.043865
=== Actor Training Debug (Iteration 604) ===
Q mean: -3.476400
Q std: 2.662843
Actor loss: 3.480348
Action reg: 0.003947
  l1.weight: grad_norm = 0.004276
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.005579
Total gradient norm: 0.072729
=== Actor Training Debug (Iteration 605) ===
Q mean: -3.611866
Q std: 2.905733
Actor loss: 3.615810
Action reg: 0.003945
  l1.weight: grad_norm = 0.001111
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.001110
Total gradient norm: 0.007491
=== Actor Training Debug (Iteration 606) ===
Q mean: -3.797110
Q std: 2.654539
Actor loss: 3.801079
Action reg: 0.003969
  l1.weight: grad_norm = 0.003781
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.003736
Total gradient norm: 0.021807
=== Actor Training Debug (Iteration 607) ===
Q mean: -4.092698
Q std: 2.809019
Actor loss: 4.096674
Action reg: 0.003976
  l1.weight: grad_norm = 0.001867
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.001800
Total gradient norm: 0.014854
=== Actor Training Debug (Iteration 608) ===
Q mean: -4.458733
Q std: 2.728472
Actor loss: 4.462671
Action reg: 0.003939
  l1.weight: grad_norm = 0.002498
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.003024
Total gradient norm: 0.036522
=== Actor Training Debug (Iteration 609) ===
Q mean: -4.035419
Q std: 2.918773
Actor loss: 4.039333
Action reg: 0.003914
  l1.weight: grad_norm = 0.003724
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.004097
Total gradient norm: 0.041110
=== Actor Training Debug (Iteration 610) ===
Q mean: -4.000868
Q std: 2.753545
Actor loss: 4.004795
Action reg: 0.003927
  l1.weight: grad_norm = 0.003642
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.004581
Total gradient norm: 0.059465
=== Actor Training Debug (Iteration 611) ===
Q mean: -3.948825
Q std: 2.695139
Actor loss: 3.952777
Action reg: 0.003952
  l1.weight: grad_norm = 0.002216
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.002289
Total gradient norm: 0.010784
=== Actor Training Debug (Iteration 612) ===
Q mean: -4.144744
Q std: 2.679264
Actor loss: 4.148674
Action reg: 0.003930
  l1.weight: grad_norm = 0.001141
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.001275
Total gradient norm: 0.013423
=== Actor Training Debug (Iteration 613) ===
Q mean: -4.284887
Q std: 2.850302
Actor loss: 4.288859
Action reg: 0.003972
  l1.weight: grad_norm = 0.003067
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003147
Total gradient norm: 0.032871
=== Actor Training Debug (Iteration 614) ===
Q mean: -3.751519
Q std: 2.680850
Actor loss: 3.755463
Action reg: 0.003943
  l1.weight: grad_norm = 0.002254
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.002673
Total gradient norm: 0.036627
=== Actor Training Debug (Iteration 615) ===
Q mean: -3.645748
Q std: 2.715957
Actor loss: 3.649687
Action reg: 0.003939
  l1.weight: grad_norm = 0.002277
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.002818
Total gradient norm: 0.032421
=== Actor Training Debug (Iteration 616) ===
Q mean: -3.708653
Q std: 2.875222
Actor loss: 3.712570
Action reg: 0.003917
  l1.weight: grad_norm = 0.003998
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.004765
Total gradient norm: 0.061519
=== Actor Training Debug (Iteration 617) ===
Q mean: -4.145520
Q std: 2.653880
Actor loss: 4.149503
Action reg: 0.003983
  l1.weight: grad_norm = 0.001990
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001992
Total gradient norm: 0.011436
=== Actor Training Debug (Iteration 618) ===
Q mean: -3.723469
Q std: 2.656332
Actor loss: 3.727417
Action reg: 0.003948
  l1.weight: grad_norm = 0.001716
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.002163
Total gradient norm: 0.023357
=== Actor Training Debug (Iteration 619) ===
Q mean: -4.106995
Q std: 2.896364
Actor loss: 4.110909
Action reg: 0.003914
  l1.weight: grad_norm = 0.002134
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.002184
Total gradient norm: 0.012205
=== Actor Training Debug (Iteration 620) ===
Q mean: -3.836828
Q std: 2.754806
Actor loss: 3.840806
Action reg: 0.003978
  l1.weight: grad_norm = 0.001721
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.001959
Total gradient norm: 0.021366
=== Actor Training Debug (Iteration 621) ===
Q mean: -4.021739
Q std: 2.584495
Actor loss: 4.025718
Action reg: 0.003979
  l1.weight: grad_norm = 0.002503
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002630
Total gradient norm: 0.015141
=== Actor Training Debug (Iteration 622) ===
Q mean: -3.636353
Q std: 2.767791
Actor loss: 3.640315
Action reg: 0.003962
  l1.weight: grad_norm = 0.000972
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001034
Total gradient norm: 0.008573
=== Actor Training Debug (Iteration 623) ===
Q mean: -3.426224
Q std: 2.549909
Actor loss: 3.430177
Action reg: 0.003954
  l1.weight: grad_norm = 0.003827
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.003742
Total gradient norm: 0.021258
=== Actor Training Debug (Iteration 624) ===
Q mean: -4.543971
Q std: 3.076532
Actor loss: 4.547913
Action reg: 0.003942
  l1.weight: grad_norm = 0.002131
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002165
Total gradient norm: 0.016047
=== Actor Training Debug (Iteration 625) ===
Q mean: -4.035284
Q std: 3.041457
Actor loss: 4.039250
Action reg: 0.003966
  l1.weight: grad_norm = 0.001060
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.001254
Total gradient norm: 0.014613
=== Actor Training Debug (Iteration 626) ===
Q mean: -3.673373
Q std: 2.824271
Actor loss: 3.677310
Action reg: 0.003937
  l1.weight: grad_norm = 0.002825
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.003502
Total gradient norm: 0.041963
=== Actor Training Debug (Iteration 627) ===
Q mean: -3.844773
Q std: 2.740861
Actor loss: 3.848715
Action reg: 0.003943
  l1.weight: grad_norm = 0.001996
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.002032
Total gradient norm: 0.020436
=== Actor Training Debug (Iteration 628) ===
Q mean: -4.047956
Q std: 2.665419
Actor loss: 4.051942
Action reg: 0.003985
  l1.weight: grad_norm = 0.002447
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002616
Total gradient norm: 0.026591
=== Actor Training Debug (Iteration 629) ===
Q mean: -3.907364
Q std: 2.768981
Actor loss: 3.911265
Action reg: 0.003901
  l1.weight: grad_norm = 0.003480
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.003777
Total gradient norm: 0.022370
=== Actor Training Debug (Iteration 630) ===
Q mean: -4.123985
Q std: 2.795243
Actor loss: 4.127942
Action reg: 0.003956
  l1.weight: grad_norm = 0.003385
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.003471
Total gradient norm: 0.035373
=== Actor Training Debug (Iteration 631) ===
Q mean: -3.754751
Q std: 2.748864
Actor loss: 3.758697
Action reg: 0.003946
  l1.weight: grad_norm = 0.002493
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.002290
Total gradient norm: 0.015201
=== Actor Training Debug (Iteration 632) ===
Q mean: -3.535747
Q std: 2.962554
Actor loss: 3.539679
Action reg: 0.003932
  l1.weight: grad_norm = 0.003031
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.002904
Total gradient norm: 0.017914
=== Actor Training Debug (Iteration 633) ===
Q mean: -3.216495
Q std: 2.755225
Actor loss: 3.220442
Action reg: 0.003947
  l1.weight: grad_norm = 0.003297
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.003312
Total gradient norm: 0.027500
=== Actor Training Debug (Iteration 634) ===
Q mean: -3.396574
Q std: 3.161649
Actor loss: 3.400495
Action reg: 0.003921
  l1.weight: grad_norm = 0.003573
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.004043
Total gradient norm: 0.043555
=== Actor Training Debug (Iteration 635) ===
Q mean: -3.773184
Q std: 2.660616
Actor loss: 3.777150
Action reg: 0.003966
  l1.weight: grad_norm = 0.000542
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.000542
Total gradient norm: 0.004035
=== Actor Training Debug (Iteration 636) ===
Q mean: -4.270480
Q std: 3.254335
Actor loss: 4.274415
Action reg: 0.003935
  l1.weight: grad_norm = 0.001822
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.001872
Total gradient norm: 0.013473
=== Actor Training Debug (Iteration 637) ===
Q mean: -4.619267
Q std: 3.168038
Actor loss: 4.623200
Action reg: 0.003933
  l1.weight: grad_norm = 0.001549
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.001682
Total gradient norm: 0.014744
=== Actor Training Debug (Iteration 638) ===
Q mean: -4.224370
Q std: 2.977526
Actor loss: 4.228303
Action reg: 0.003932
  l1.weight: grad_norm = 0.003036
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.004028
Total gradient norm: 0.043984
=== Actor Training Debug (Iteration 639) ===
Q mean: -3.894366
Q std: 2.869810
Actor loss: 3.898328
Action reg: 0.003962
  l1.weight: grad_norm = 0.002736
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.003149
Total gradient norm: 0.032374
=== Actor Training Debug (Iteration 640) ===
Q mean: -3.412616
Q std: 2.623829
Actor loss: 3.416519
Action reg: 0.003903
  l1.weight: grad_norm = 0.002584
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.002815
Total gradient norm: 0.025113
=== Actor Training Debug (Iteration 641) ===
Q mean: -3.382385
Q std: 2.767186
Actor loss: 3.386352
Action reg: 0.003967
  l1.weight: grad_norm = 0.001589
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.001770
Total gradient norm: 0.014281
=== Actor Training Debug (Iteration 642) ===
Q mean: -3.773680
Q std: 2.679155
Actor loss: 3.777629
Action reg: 0.003949
  l1.weight: grad_norm = 0.001109
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.001224
Total gradient norm: 0.013541
=== Actor Training Debug (Iteration 643) ===
Q mean: -4.128926
Q std: 2.637599
Actor loss: 4.132868
Action reg: 0.003942
  l1.weight: grad_norm = 0.001226
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.001153
Total gradient norm: 0.005716
=== Actor Training Debug (Iteration 644) ===
Q mean: -4.299499
Q std: 2.697782
Actor loss: 4.303451
Action reg: 0.003952
  l1.weight: grad_norm = 0.002302
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.002547
Total gradient norm: 0.029028
=== Actor Training Debug (Iteration 645) ===
Q mean: -4.417387
Q std: 2.882692
Actor loss: 4.421316
Action reg: 0.003928
  l1.weight: grad_norm = 0.003101
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.003945
Total gradient norm: 0.042905
=== Actor Training Debug (Iteration 646) ===
Q mean: -4.081411
Q std: 2.586804
Actor loss: 4.085347
Action reg: 0.003936
  l1.weight: grad_norm = 0.002503
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.002599
Total gradient norm: 0.024278
=== Actor Training Debug (Iteration 647) ===
Q mean: -3.557347
Q std: 2.684464
Actor loss: 3.561334
Action reg: 0.003987
  l1.weight: grad_norm = 0.001862
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002018
Total gradient norm: 0.016837
=== Actor Training Debug (Iteration 648) ===
Q mean: -3.175558
Q std: 2.765262
Actor loss: 3.179474
Action reg: 0.003916
  l1.weight: grad_norm = 0.002894
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.003764
Total gradient norm: 0.053311
=== Actor Training Debug (Iteration 649) ===
Q mean: -3.847708
Q std: 3.255083
Actor loss: 3.851697
Action reg: 0.003990
  l1.weight: grad_norm = 0.002603
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003023
Total gradient norm: 0.022885
=== Actor Training Debug (Iteration 650) ===
Q mean: -4.063096
Q std: 2.838924
Actor loss: 4.067047
Action reg: 0.003951
  l1.weight: grad_norm = 0.001302
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.001447
Total gradient norm: 0.010679
=== Actor Training Debug (Iteration 651) ===
Q mean: -4.311902
Q std: 2.888592
Actor loss: 4.315825
Action reg: 0.003923
  l1.weight: grad_norm = 0.001390
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.001435
Total gradient norm: 0.011652
=== Actor Training Debug (Iteration 652) ===
Q mean: -4.685598
Q std: 3.336820
Actor loss: 4.689544
Action reg: 0.003946
  l1.weight: grad_norm = 0.003051
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.003484
Total gradient norm: 0.039822
=== Actor Training Debug (Iteration 653) ===
Q mean: -4.191693
Q std: 3.153018
Actor loss: 4.195603
Action reg: 0.003910
  l1.weight: grad_norm = 0.003115
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.004001
Total gradient norm: 0.058519
=== Actor Training Debug (Iteration 654) ===
Q mean: -3.606148
Q std: 2.916189
Actor loss: 3.610123
Action reg: 0.003976
  l1.weight: grad_norm = 0.000872
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000864
Total gradient norm: 0.004700
=== Actor Training Debug (Iteration 655) ===
Q mean: -3.873579
Q std: 3.055628
Actor loss: 3.877485
Action reg: 0.003906
  l1.weight: grad_norm = 0.001144
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.001262
Total gradient norm: 0.008517
=== Actor Training Debug (Iteration 656) ===
Q mean: -3.890395
Q std: 2.814418
Actor loss: 3.894334
Action reg: 0.003939
  l1.weight: grad_norm = 0.001120
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.001148
Total gradient norm: 0.009340
=== Actor Training Debug (Iteration 657) ===
Q mean: -4.791806
Q std: 3.056903
Actor loss: 4.795759
Action reg: 0.003953
  l1.weight: grad_norm = 0.002710
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.002699
Total gradient norm: 0.020792
=== Actor Training Debug (Iteration 658) ===
Q mean: -4.876113
Q std: 3.122728
Actor loss: 4.880070
Action reg: 0.003956
  l1.weight: grad_norm = 0.000968
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.001063
Total gradient norm: 0.010535
=== Actor Training Debug (Iteration 659) ===
Q mean: -3.989859
Q std: 2.754838
Actor loss: 3.993829
Action reg: 0.003970
  l1.weight: grad_norm = 0.002267
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.002807
Total gradient norm: 0.034129
=== Actor Training Debug (Iteration 660) ===
Q mean: -3.521780
Q std: 3.002286
Actor loss: 3.525760
Action reg: 0.003980
  l1.weight: grad_norm = 0.001939
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002154
Total gradient norm: 0.025127
=== Actor Training Debug (Iteration 661) ===
Q mean: -4.035540
Q std: 2.859881
Actor loss: 4.039487
Action reg: 0.003947
  l1.weight: grad_norm = 0.001609
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.001816
Total gradient norm: 0.021208
=== Actor Training Debug (Iteration 662) ===
Q mean: -3.869510
Q std: 2.732840
Actor loss: 3.873459
Action reg: 0.003949
  l1.weight: grad_norm = 0.004227
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004654
Total gradient norm: 0.048406
=== Actor Training Debug (Iteration 663) ===
Q mean: -4.170197
Q std: 2.980641
Actor loss: 4.174149
Action reg: 0.003952
  l1.weight: grad_norm = 0.002232
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.002279
Total gradient norm: 0.014565
=== Actor Training Debug (Iteration 664) ===
Q mean: -4.161209
Q std: 2.807811
Actor loss: 4.165173
Action reg: 0.003964
  l1.weight: grad_norm = 0.001370
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.001425
Total gradient norm: 0.012385
=== Actor Training Debug (Iteration 665) ===
Q mean: -4.173548
Q std: 3.057153
Actor loss: 4.177507
Action reg: 0.003959
  l1.weight: grad_norm = 0.002922
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.003562
Total gradient norm: 0.041198
=== Actor Training Debug (Iteration 666) ===
Q mean: -4.100844
Q std: 3.232888
Actor loss: 4.104787
Action reg: 0.003943
  l1.weight: grad_norm = 0.003282
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.003694
Total gradient norm: 0.042397
=== Actor Training Debug (Iteration 667) ===
Q mean: -3.580195
Q std: 3.054906
Actor loss: 3.584127
Action reg: 0.003932
  l1.weight: grad_norm = 0.001377
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.001348
Total gradient norm: 0.007428
=== Actor Training Debug (Iteration 668) ===
Q mean: -4.136668
Q std: 3.048864
Actor loss: 4.140643
Action reg: 0.003975
  l1.weight: grad_norm = 0.002499
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.002663
Total gradient norm: 0.028673
=== Actor Training Debug (Iteration 669) ===
Q mean: -4.122599
Q std: 3.245352
Actor loss: 4.126484
Action reg: 0.003885
  l1.weight: grad_norm = 0.003809
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.003760
Total gradient norm: 0.026908
=== Actor Training Debug (Iteration 670) ===
Q mean: -4.409596
Q std: 2.982576
Actor loss: 4.413535
Action reg: 0.003938
  l1.weight: grad_norm = 0.000986
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.001040
Total gradient norm: 0.008643
=== Actor Training Debug (Iteration 671) ===
Q mean: -4.341287
Q std: 3.097420
Actor loss: 4.345238
Action reg: 0.003952
  l1.weight: grad_norm = 0.000791
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.000832
Total gradient norm: 0.004752
=== Actor Training Debug (Iteration 672) ===
Q mean: -4.282874
Q std: 2.965335
Actor loss: 4.286797
Action reg: 0.003922
  l1.weight: grad_norm = 0.002098
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.002322
Total gradient norm: 0.027143
=== Actor Training Debug (Iteration 673) ===
Q mean: -3.572085
Q std: 2.900101
Actor loss: 3.576066
Action reg: 0.003981
  l1.weight: grad_norm = 0.002978
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002999
Total gradient norm: 0.029045
=== Actor Training Debug (Iteration 674) ===
Q mean: -3.518678
Q std: 2.786734
Actor loss: 3.522608
Action reg: 0.003930
  l1.weight: grad_norm = 0.001730
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.001841
Total gradient norm: 0.016876
=== Actor Training Debug (Iteration 675) ===
Q mean: -3.925379
Q std: 2.833755
Actor loss: 3.929327
Action reg: 0.003947
  l1.weight: grad_norm = 0.001028
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.001121
Total gradient norm: 0.012694
=== Actor Training Debug (Iteration 676) ===
Q mean: -4.359296
Q std: 3.265855
Actor loss: 4.363287
Action reg: 0.003991
  l1.weight: grad_norm = 0.001559
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001780
Total gradient norm: 0.016757
=== Actor Training Debug (Iteration 677) ===
Q mean: -4.781068
Q std: 3.212515
Actor loss: 4.785012
Action reg: 0.003944
  l1.weight: grad_norm = 0.002226
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.002527
Total gradient norm: 0.024653
=== Actor Training Debug (Iteration 678) ===
Q mean: -4.452987
Q std: 2.992590
Actor loss: 4.456950
Action reg: 0.003962
  l1.weight: grad_norm = 0.000702
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.000739
Total gradient norm: 0.004431
=== Actor Training Debug (Iteration 679) ===
Q mean: -3.887419
Q std: 2.978620
Actor loss: 3.891367
Action reg: 0.003948
  l1.weight: grad_norm = 0.002456
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.002998
Total gradient norm: 0.031478
=== Actor Training Debug (Iteration 680) ===
Q mean: -3.800539
Q std: 2.942104
Actor loss: 3.804532
Action reg: 0.003992
  l1.weight: grad_norm = 0.001131
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001389
Total gradient norm: 0.014606
=== Actor Training Debug (Iteration 681) ===
Q mean: -4.435664
Q std: 3.095097
Actor loss: 4.439613
Action reg: 0.003949
  l1.weight: grad_norm = 0.002451
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.002929
Total gradient norm: 0.029989
=== Actor Training Debug (Iteration 682) ===
Q mean: -4.193910
Q std: 3.067431
Actor loss: 4.197846
Action reg: 0.003936
  l1.weight: grad_norm = 0.002430
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.002807
Total gradient norm: 0.032538
=== Actor Training Debug (Iteration 683) ===
Q mean: -4.055014
Q std: 3.119558
Actor loss: 4.058949
Action reg: 0.003934
  l1.weight: grad_norm = 0.003236
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.003629
Total gradient norm: 0.027603
=== Actor Training Debug (Iteration 684) ===
Q mean: -4.115377
Q std: 3.071388
Actor loss: 4.119314
Action reg: 0.003936
  l1.weight: grad_norm = 0.001469
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.001678
Total gradient norm: 0.013270
=== Actor Training Debug (Iteration 685) ===
Q mean: -4.004983
Q std: 2.768482
Actor loss: 4.008901
Action reg: 0.003918
  l1.weight: grad_norm = 0.002559
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.002751
Total gradient norm: 0.028443
=== Actor Training Debug (Iteration 686) ===
Q mean: -4.216595
Q std: 3.013307
Actor loss: 4.220564
Action reg: 0.003969
  l1.weight: grad_norm = 0.002327
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.002542
Total gradient norm: 0.029477
=== Actor Training Debug (Iteration 687) ===
Q mean: -4.081712
Q std: 3.080452
Actor loss: 4.085660
Action reg: 0.003948
  l1.weight: grad_norm = 0.003033
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.003419
Total gradient norm: 0.037049
=== Actor Training Debug (Iteration 688) ===
Q mean: -4.395977
Q std: 3.139956
Actor loss: 4.399926
Action reg: 0.003949
  l1.weight: grad_norm = 0.002668
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.003262
Total gradient norm: 0.043507
=== Actor Training Debug (Iteration 689) ===
Q mean: -4.161897
Q std: 2.837680
Actor loss: 4.165825
Action reg: 0.003928
  l1.weight: grad_norm = 0.001087
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.001205
Total gradient norm: 0.008747
=== Actor Training Debug (Iteration 690) ===
Q mean: -4.377645
Q std: 3.097949
Actor loss: 4.381600
Action reg: 0.003954
  l1.weight: grad_norm = 0.003876
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.004998
Total gradient norm: 0.062087
=== Actor Training Debug (Iteration 691) ===
Q mean: -4.156196
Q std: 3.165660
Actor loss: 4.160165
Action reg: 0.003969
  l1.weight: grad_norm = 0.000857
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.000867
Total gradient norm: 0.004758
=== Actor Training Debug (Iteration 692) ===
Q mean: -4.141298
Q std: 2.833483
Actor loss: 4.145240
Action reg: 0.003941
  l1.weight: grad_norm = 0.000955
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.001159
Total gradient norm: 0.011795
=== Actor Training Debug (Iteration 693) ===
Q mean: -3.967387
Q std: 3.199564
Actor loss: 3.971343
Action reg: 0.003956
  l1.weight: grad_norm = 0.002431
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.002675
Total gradient norm: 0.027460
=== Actor Training Debug (Iteration 694) ===
Q mean: -3.823027
Q std: 3.176328
Actor loss: 3.826963
Action reg: 0.003936
  l1.weight: grad_norm = 0.002770
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.002953
Total gradient norm: 0.030474
=== Actor Training Debug (Iteration 695) ===
Q mean: -3.749234
Q std: 3.049122
Actor loss: 3.753151
Action reg: 0.003917
  l1.weight: grad_norm = 0.002277
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.002321
Total gradient norm: 0.011112
=== Actor Training Debug (Iteration 696) ===
Q mean: -3.946720
Q std: 3.080343
Actor loss: 3.950634
Action reg: 0.003914
  l1.weight: grad_norm = 0.003149
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.003157
Total gradient norm: 0.016607
=== Actor Training Debug (Iteration 697) ===
Q mean: -4.500896
Q std: 3.158798
Actor loss: 4.504830
Action reg: 0.003934
  l1.weight: grad_norm = 0.002366
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.002623
Total gradient norm: 0.028677
=== Actor Training Debug (Iteration 698) ===
Q mean: -4.449286
Q std: 3.335504
Actor loss: 4.453263
Action reg: 0.003978
  l1.weight: grad_norm = 0.000815
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.000843
Total gradient norm: 0.004833
=== Actor Training Debug (Iteration 699) ===
Q mean: -4.254384
Q std: 3.271361
Actor loss: 4.258359
Action reg: 0.003975
  l1.weight: grad_norm = 0.001645
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.001803
Total gradient norm: 0.019542
=== Actor Training Debug (Iteration 700) ===
Q mean: -3.941050
Q std: 3.090979
Actor loss: 3.945029
Action reg: 0.003979
  l1.weight: grad_norm = 0.000995
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001057
Total gradient norm: 0.009482
=== Actor Training Debug (Iteration 701) ===
Q mean: -4.132215
Q std: 3.018487
Actor loss: 4.136200
Action reg: 0.003985
  l1.weight: grad_norm = 0.003838
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003683
Total gradient norm: 0.029945
=== Actor Training Debug (Iteration 702) ===
Q mean: -4.426518
Q std: 3.312572
Actor loss: 4.430468
Action reg: 0.003949
  l1.weight: grad_norm = 0.003243
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.003422
Total gradient norm: 0.039149
=== Actor Training Debug (Iteration 703) ===
Q mean: -4.366573
Q std: 3.283757
Actor loss: 4.370531
Action reg: 0.003957
  l1.weight: grad_norm = 0.006226
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.006964
Total gradient norm: 0.051922
=== Actor Training Debug (Iteration 704) ===
Q mean: -4.098164
Q std: 2.919091
Actor loss: 4.102087
Action reg: 0.003923
  l1.weight: grad_norm = 0.005101
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.005703
Total gradient norm: 0.058744
=== Actor Training Debug (Iteration 705) ===
Q mean: -4.129536
Q std: 3.094307
Actor loss: 4.133509
Action reg: 0.003972
  l1.weight: grad_norm = 0.002648
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.002930
Total gradient norm: 0.037091
=== Actor Training Debug (Iteration 706) ===
Q mean: -4.187353
Q std: 3.046708
Actor loss: 4.191319
Action reg: 0.003966
  l1.weight: grad_norm = 0.004507
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.005823
Total gradient norm: 0.074671
=== Actor Training Debug (Iteration 707) ===
Q mean: -4.377293
Q std: 3.216648
Actor loss: 4.381208
Action reg: 0.003915
  l1.weight: grad_norm = 0.004100
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.004764
Total gradient norm: 0.044869
=== Actor Training Debug (Iteration 708) ===
Q mean: -3.532936
Q std: 3.035171
Actor loss: 3.536881
Action reg: 0.003945
  l1.weight: grad_norm = 0.005218
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.005736
Total gradient norm: 0.074351
=== Actor Training Debug (Iteration 709) ===
Q mean: -3.648686
Q std: 3.142748
Actor loss: 3.652641
Action reg: 0.003955
  l1.weight: grad_norm = 0.003433
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.004417
Total gradient norm: 0.059130
=== Actor Training Debug (Iteration 710) ===
Q mean: -3.954091
Q std: 3.221004
Actor loss: 3.958044
Action reg: 0.003954
  l1.weight: grad_norm = 0.003262
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.003725
Total gradient norm: 0.038537
=== Actor Training Debug (Iteration 711) ===
Q mean: -4.744734
Q std: 3.548031
Actor loss: 4.748676
Action reg: 0.003942
  l1.weight: grad_norm = 0.003090
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.003162
Total gradient norm: 0.022186
=== Actor Training Debug (Iteration 712) ===
Q mean: -4.186396
Q std: 3.135100
Actor loss: 4.190373
Action reg: 0.003977
  l1.weight: grad_norm = 0.002171
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.002458
Total gradient norm: 0.019496
=== Actor Training Debug (Iteration 713) ===
Q mean: -4.181982
Q std: 3.306967
Actor loss: 4.185931
Action reg: 0.003949
  l1.weight: grad_norm = 0.002505
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.002521
Total gradient norm: 0.016203
=== Actor Training Debug (Iteration 714) ===
Q mean: -3.772519
Q std: 3.145644
Actor loss: 3.776448
Action reg: 0.003929
  l1.weight: grad_norm = 0.003073
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.003330
Total gradient norm: 0.029864
=== Actor Training Debug (Iteration 715) ===
Q mean: -4.167251
Q std: 3.098533
Actor loss: 4.171189
Action reg: 0.003938
  l1.weight: grad_norm = 0.003605
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.004619
Total gradient norm: 0.061568
=== Actor Training Debug (Iteration 716) ===
Q mean: -3.862096
Q std: 2.855134
Actor loss: 3.866026
Action reg: 0.003930
  l1.weight: grad_norm = 0.002049
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.002331
Total gradient norm: 0.023010
=== Actor Training Debug (Iteration 717) ===
Q mean: -4.652167
Q std: 3.185541
Actor loss: 4.656108
Action reg: 0.003940
  l1.weight: grad_norm = 0.002853
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.003613
Total gradient norm: 0.050734
=== Actor Training Debug (Iteration 718) ===
Q mean: -5.049292
Q std: 3.243536
Actor loss: 5.053267
Action reg: 0.003976
  l1.weight: grad_norm = 0.001948
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.002483
Total gradient norm: 0.030477
=== Actor Training Debug (Iteration 719) ===
Q mean: -4.748926
Q std: 3.149732
Actor loss: 4.752877
Action reg: 0.003951
  l1.weight: grad_norm = 0.003600
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.003889
Total gradient norm: 0.036437
=== Actor Training Debug (Iteration 720) ===
Q mean: -3.820332
Q std: 2.920579
Actor loss: 3.824318
Action reg: 0.003987
  l1.weight: grad_norm = 0.002132
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002109
Total gradient norm: 0.014629
=== Actor Training Debug (Iteration 721) ===
Q mean: -3.875463
Q std: 3.424193
Actor loss: 3.879402
Action reg: 0.003939
  l1.weight: grad_norm = 0.005093
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.006075
Total gradient norm: 0.047666
=== Actor Training Debug (Iteration 722) ===
Q mean: -4.253213
Q std: 3.341720
Actor loss: 4.257181
Action reg: 0.003967
  l1.weight: grad_norm = 0.005220
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.006104
Total gradient norm: 0.050307
=== Actor Training Debug (Iteration 723) ===
Q mean: -4.386485
Q std: 3.298856
Actor loss: 4.390453
Action reg: 0.003968
  l1.weight: grad_norm = 0.002347
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.002378
Total gradient norm: 0.015489
=== Actor Training Debug (Iteration 724) ===
Q mean: -4.450132
Q std: 3.548356
Actor loss: 4.454080
Action reg: 0.003948
  l1.weight: grad_norm = 0.004138
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.004370
Total gradient norm: 0.043522
=== Actor Training Debug (Iteration 725) ===
Q mean: -4.007107
Q std: 3.219490
Actor loss: 4.011082
Action reg: 0.003975
  l1.weight: grad_norm = 0.002747
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.003144
Total gradient norm: 0.042645
=== Actor Training Debug (Iteration 726) ===
Q mean: -4.211117
Q std: 3.239834
Actor loss: 4.215069
Action reg: 0.003952
  l1.weight: grad_norm = 0.005097
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.005969
Total gradient norm: 0.069938
=== Actor Training Debug (Iteration 727) ===
Q mean: -4.259567
Q std: 3.303850
Actor loss: 4.263549
Action reg: 0.003983
  l1.weight: grad_norm = 0.005545
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006589
Total gradient norm: 0.074335
=== Actor Training Debug (Iteration 728) ===
Q mean: -4.520683
Q std: 3.272380
Actor loss: 4.524632
Action reg: 0.003949
  l1.weight: grad_norm = 0.003923
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.004483
Total gradient norm: 0.041500
=== Actor Training Debug (Iteration 729) ===
Q mean: -4.305920
Q std: 3.188743
Actor loss: 4.309855
Action reg: 0.003935
  l1.weight: grad_norm = 0.004868
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.005204
Total gradient norm: 0.028973
=== Actor Training Debug (Iteration 730) ===
Q mean: -4.273703
Q std: 3.122707
Actor loss: 4.277667
Action reg: 0.003964
  l1.weight: grad_norm = 0.002367
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.002493
Total gradient norm: 0.014406
=== Actor Training Debug (Iteration 731) ===
Q mean: -4.104794
Q std: 3.115429
Actor loss: 4.108742
Action reg: 0.003949
  l1.weight: grad_norm = 0.005964
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.007640
Total gradient norm: 0.095992
=== Actor Training Debug (Iteration 732) ===
Q mean: -4.433831
Q std: 3.466501
Actor loss: 4.437770
Action reg: 0.003939
  l1.weight: grad_norm = 0.001580
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.001629
Total gradient norm: 0.011370
=== Actor Training Debug (Iteration 733) ===
Q mean: -4.430703
Q std: 3.431203
Actor loss: 4.434642
Action reg: 0.003939
  l1.weight: grad_norm = 0.002798
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.002883
Total gradient norm: 0.017391
=== Actor Training Debug (Iteration 734) ===
Q mean: -3.787907
Q std: 3.169843
Actor loss: 3.791824
Action reg: 0.003917
  l1.weight: grad_norm = 0.002152
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.002177
Total gradient norm: 0.011739
=== Actor Training Debug (Iteration 735) ===
Q mean: -4.452854
Q std: 3.324039
Actor loss: 4.456831
Action reg: 0.003977
  l1.weight: grad_norm = 0.002203
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.002142
Total gradient norm: 0.013085
=== Actor Training Debug (Iteration 736) ===
Q mean: -4.558031
Q std: 3.436866
Actor loss: 4.561992
Action reg: 0.003961
  l1.weight: grad_norm = 0.001527
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.001552
Total gradient norm: 0.010201
=== Actor Training Debug (Iteration 737) ===
Q mean: -4.498040
Q std: 3.213019
Actor loss: 4.501979
Action reg: 0.003939
  l1.weight: grad_norm = 0.003446
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.003302
Total gradient norm: 0.020424
=== Actor Training Debug (Iteration 738) ===
Q mean: -4.462607
Q std: 3.161454
Actor loss: 4.466548
Action reg: 0.003941
  l1.weight: grad_norm = 0.002765
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.003217
Total gradient norm: 0.040429
=== Actor Training Debug (Iteration 739) ===
Q mean: -4.061238
Q std: 3.184104
Actor loss: 4.065216
Action reg: 0.003978
  l1.weight: grad_norm = 0.003109
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003266
Total gradient norm: 0.028149
=== Actor Training Debug (Iteration 740) ===
Q mean: -4.053554
Q std: 3.073980
Actor loss: 4.057516
Action reg: 0.003962
  l1.weight: grad_norm = 0.002594
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.003011
Total gradient norm: 0.031563
=== Actor Training Debug (Iteration 741) ===
Q mean: -4.363760
Q std: 3.433596
Actor loss: 4.367712
Action reg: 0.003952
  l1.weight: grad_norm = 0.001319
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.001273
Total gradient norm: 0.006283
=== Actor Training Debug (Iteration 742) ===
Q mean: -4.163508
Q std: 3.247247
Actor loss: 4.167445
Action reg: 0.003936
  l1.weight: grad_norm = 0.002556
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.002862
Total gradient norm: 0.032003
=== Actor Training Debug (Iteration 743) ===
Q mean: -4.161202
Q std: 3.465252
Actor loss: 4.165178
Action reg: 0.003976
  l1.weight: grad_norm = 0.004865
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.005813
Total gradient norm: 0.078561
=== Actor Training Debug (Iteration 744) ===
Q mean: -4.071386
Q std: 3.328411
Actor loss: 4.075372
Action reg: 0.003986
  l1.weight: grad_norm = 0.002666
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002998
Total gradient norm: 0.030649
=== Actor Training Debug (Iteration 745) ===
Q mean: -4.118302
Q std: 3.238067
Actor loss: 4.122273
Action reg: 0.003970
  l1.weight: grad_norm = 0.000997
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.001120
Total gradient norm: 0.012770
=== Actor Training Debug (Iteration 746) ===
Q mean: -4.155956
Q std: 3.189683
Actor loss: 4.159887
Action reg: 0.003931
  l1.weight: grad_norm = 0.002332
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.002439
Total gradient norm: 0.020744
=== Actor Training Debug (Iteration 747) ===
Q mean: -4.175472
Q std: 3.366706
Actor loss: 4.179429
Action reg: 0.003956
  l1.weight: grad_norm = 0.002619
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.002705
Total gradient norm: 0.023857
=== Actor Training Debug (Iteration 748) ===
Q mean: -3.930856
Q std: 3.392998
Actor loss: 3.934784
Action reg: 0.003928
  l1.weight: grad_norm = 0.001838
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.002053
Total gradient norm: 0.015170
=== Actor Training Debug (Iteration 749) ===
Q mean: -4.514312
Q std: 3.498469
Actor loss: 4.518288
Action reg: 0.003976
  l1.weight: grad_norm = 0.004279
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.004193
Total gradient norm: 0.036965
=== Actor Training Debug (Iteration 750) ===
Q mean: -4.480898
Q std: 3.094147
Actor loss: 4.484877
Action reg: 0.003979
  l1.weight: grad_norm = 0.001795
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.001815
Total gradient norm: 0.011387
=== Actor Training Debug (Iteration 751) ===
Q mean: -4.134297
Q std: 3.322403
Actor loss: 4.138251
Action reg: 0.003953
  l1.weight: grad_norm = 0.003047
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.003837
Total gradient norm: 0.054757
=== Actor Training Debug (Iteration 752) ===
Q mean: -3.908469
Q std: 3.628080
Actor loss: 3.912422
Action reg: 0.003952
  l1.weight: grad_norm = 0.002458
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.002543
Total gradient norm: 0.015823
=== Actor Training Debug (Iteration 753) ===
Q mean: -3.994462
Q std: 3.104938
Actor loss: 3.998442
Action reg: 0.003979
  l1.weight: grad_norm = 0.002506
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.002599
Total gradient norm: 0.018875
=== Actor Training Debug (Iteration 754) ===
Q mean: -4.507543
Q std: 3.513929
Actor loss: 4.511532
Action reg: 0.003989
  l1.weight: grad_norm = 0.001402
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001362
Total gradient norm: 0.010979
=== Actor Training Debug (Iteration 755) ===
Q mean: -4.618942
Q std: 3.473471
Actor loss: 4.622894
Action reg: 0.003953
  l1.weight: grad_norm = 0.004488
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.005481
Total gradient norm: 0.073594
=== Actor Training Debug (Iteration 756) ===
Q mean: -4.111698
Q std: 3.289855
Actor loss: 4.115636
Action reg: 0.003938
  l1.weight: grad_norm = 0.002231
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.002131
Total gradient norm: 0.014857
=== Actor Training Debug (Iteration 757) ===
Q mean: -4.444270
Q std: 3.407450
Actor loss: 4.448251
Action reg: 0.003982
  l1.weight: grad_norm = 0.001959
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.002378
Total gradient norm: 0.027179
=== Actor Training Debug (Iteration 758) ===
Q mean: -4.494919
Q std: 3.155186
Actor loss: 4.498889
Action reg: 0.003970
  l1.weight: grad_norm = 0.002970
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.003051
Total gradient norm: 0.025045
=== Actor Training Debug (Iteration 759) ===
Q mean: -4.276120
Q std: 3.142462
Actor loss: 4.280101
Action reg: 0.003981
  l1.weight: grad_norm = 0.003748
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003690
Total gradient norm: 0.020786
=== Actor Training Debug (Iteration 760) ===
Q mean: -3.924890
Q std: 3.013627
Actor loss: 3.928872
Action reg: 0.003982
  l1.weight: grad_norm = 0.001344
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.001320
Total gradient norm: 0.008757
=== Actor Training Debug (Iteration 761) ===
Q mean: -3.941152
Q std: 3.152824
Actor loss: 3.945110
Action reg: 0.003959
  l1.weight: grad_norm = 0.002927
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.003003
Total gradient norm: 0.021122
=== Actor Training Debug (Iteration 762) ===
Q mean: -4.910638
Q std: 3.800794
Actor loss: 4.914628
Action reg: 0.003990
  l1.weight: grad_norm = 0.002954
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003673
Total gradient norm: 0.047182
=== Actor Training Debug (Iteration 763) ===
Q mean: -4.806524
Q std: 3.405962
Actor loss: 4.810497
Action reg: 0.003973
  l1.weight: grad_norm = 0.001841
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.001951
Total gradient norm: 0.017128
=== Actor Training Debug (Iteration 764) ===
Q mean: -4.364034
Q std: 3.357689
Actor loss: 4.367996
Action reg: 0.003962
  l1.weight: grad_norm = 0.007902
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.009444
Total gradient norm: 0.106864
=== Actor Training Debug (Iteration 765) ===
Q mean: -4.000764
Q std: 3.438160
Actor loss: 4.004674
Action reg: 0.003911
  l1.weight: grad_norm = 0.003729
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.003471
Total gradient norm: 0.018983
=== Actor Training Debug (Iteration 766) ===
Q mean: -4.128419
Q std: 3.256322
Actor loss: 4.132353
Action reg: 0.003934
  l1.weight: grad_norm = 0.001078
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.001056
Total gradient norm: 0.006565
=== Actor Training Debug (Iteration 767) ===
Q mean: -4.572714
Q std: 3.225559
Actor loss: 4.576664
Action reg: 0.003950
  l1.weight: grad_norm = 0.001154
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.001178
Total gradient norm: 0.011678
=== Actor Training Debug (Iteration 768) ===
Q mean: -4.841699
Q std: 3.295556
Actor loss: 4.845682
Action reg: 0.003983
  l1.weight: grad_norm = 0.004481
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005256
Total gradient norm: 0.056020
=== Actor Training Debug (Iteration 769) ===
Q mean: -4.500051
Q std: 3.332249
Actor loss: 4.504016
Action reg: 0.003966
  l1.weight: grad_norm = 0.002301
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.002740
Total gradient norm: 0.028893
=== Actor Training Debug (Iteration 770) ===
Q mean: -3.903133
Q std: 3.129970
Actor loss: 3.907086
Action reg: 0.003953
  l1.weight: grad_norm = 0.001379
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.001380
Total gradient norm: 0.011926
=== Actor Training Debug (Iteration 771) ===
Q mean: -4.224447
Q std: 3.462996
Actor loss: 4.228408
Action reg: 0.003961
  l1.weight: grad_norm = 0.002672
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.003310
Total gradient norm: 0.040880
=== Actor Training Debug (Iteration 772) ===
Q mean: -4.281825
Q std: 3.402805
Actor loss: 4.285772
Action reg: 0.003947
  l1.weight: grad_norm = 0.002727
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.003303
Total gradient norm: 0.044528
=== Actor Training Debug (Iteration 773) ===
Q mean: -4.673006
Q std: 3.555275
Actor loss: 4.676950
Action reg: 0.003943
  l1.weight: grad_norm = 0.003475
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.004161
Total gradient norm: 0.051638
=== Actor Training Debug (Iteration 774) ===
Q mean: -4.009342
Q std: 3.138696
Actor loss: 4.013286
Action reg: 0.003944
  l1.weight: grad_norm = 0.001597
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.001795
Total gradient norm: 0.016086
=== Actor Training Debug (Iteration 775) ===
Q mean: -4.229404
Q std: 3.044124
Actor loss: 4.233386
Action reg: 0.003981
  l1.weight: grad_norm = 0.001076
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.001158
Total gradient norm: 0.012832
=== Actor Training Debug (Iteration 776) ===
Q mean: -4.501305
Q std: 3.437378
Actor loss: 4.505252
Action reg: 0.003948
  l1.weight: grad_norm = 0.002341
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.002263
Total gradient norm: 0.011296
=== Actor Training Debug (Iteration 777) ===
Q mean: -4.228995
Q std: 3.398492
Actor loss: 4.232971
Action reg: 0.003976
  l1.weight: grad_norm = 0.003261
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.003151
Total gradient norm: 0.022070
=== Actor Training Debug (Iteration 778) ===
Q mean: -4.130507
Q std: 3.570555
Actor loss: 4.134485
Action reg: 0.003979
  l1.weight: grad_norm = 0.005613
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.005715
Total gradient norm: 0.040816
=== Actor Training Debug (Iteration 779) ===
Q mean: -4.139053
Q std: 3.286878
Actor loss: 4.143025
Action reg: 0.003972
  l1.weight: grad_norm = 0.000614
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.000639
Total gradient norm: 0.005459
=== Actor Training Debug (Iteration 780) ===
Q mean: -4.300366
Q std: 3.435331
Actor loss: 4.304319
Action reg: 0.003953
  l1.weight: grad_norm = 0.002340
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.002408
Total gradient norm: 0.019012
=== Actor Training Debug (Iteration 781) ===
Q mean: -4.836339
Q std: 3.544638
Actor loss: 4.840310
Action reg: 0.003970
  l1.weight: grad_norm = 0.003777
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.003782
Total gradient norm: 0.036439
=== Actor Training Debug (Iteration 782) ===
Q mean: -4.461616
Q std: 3.257380
Actor loss: 4.465589
Action reg: 0.003973
  l1.weight: grad_norm = 0.001895
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.002044
Total gradient norm: 0.015322
=== Actor Training Debug (Iteration 783) ===
Q mean: -4.162634
Q std: 3.330324
Actor loss: 4.166588
Action reg: 0.003954
  l1.weight: grad_norm = 0.001962
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.001855
Total gradient norm: 0.011021
=== Actor Training Debug (Iteration 784) ===
Q mean: -3.920038
Q std: 3.362863
Actor loss: 3.923994
Action reg: 0.003956
  l1.weight: grad_norm = 0.002387
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.002649
Total gradient norm: 0.028116
=== Actor Training Debug (Iteration 785) ===
Q mean: -4.704084
Q std: 3.664095
Actor loss: 4.708062
Action reg: 0.003978
  l1.weight: grad_norm = 0.000674
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.000668
Total gradient norm: 0.003453
=== Actor Training Debug (Iteration 786) ===
Q mean: -4.515033
Q std: 3.459160
Actor loss: 4.518993
Action reg: 0.003960
  l1.weight: grad_norm = 0.002668
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.003121
Total gradient norm: 0.038000
=== Actor Training Debug (Iteration 787) ===
Q mean: -4.330142
Q std: 3.434671
Actor loss: 4.334105
Action reg: 0.003962
  l1.weight: grad_norm = 0.003133
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.003663
Total gradient norm: 0.038209
=== Actor Training Debug (Iteration 788) ===
Q mean: -4.383375
Q std: 3.197205
Actor loss: 4.387361
Action reg: 0.003985
  l1.weight: grad_norm = 0.002156
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.002444
Total gradient norm: 0.026535
=== Actor Training Debug (Iteration 789) ===
Q mean: -4.134442
Q std: 3.545361
Actor loss: 4.138400
Action reg: 0.003958
  l1.weight: grad_norm = 0.003145
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.002858
Total gradient norm: 0.013888
=== Actor Training Debug (Iteration 790) ===
Q mean: -4.227216
Q std: 3.428155
Actor loss: 4.231186
Action reg: 0.003970
  l1.weight: grad_norm = 0.003510
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.003667
Total gradient norm: 0.028205
=== Actor Training Debug (Iteration 791) ===
Q mean: -4.077497
Q std: 3.536500
Actor loss: 4.081460
Action reg: 0.003963
  l1.weight: grad_norm = 0.004969
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.004505
Total gradient norm: 0.025833
=== Actor Training Debug (Iteration 792) ===
Q mean: -3.883869
Q std: 3.069055
Actor loss: 3.887832
Action reg: 0.003962
  l1.weight: grad_norm = 0.003875
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.004836
Total gradient norm: 0.049444
=== Actor Training Debug (Iteration 793) ===
Q mean: -4.424247
Q std: 3.398379
Actor loss: 4.428174
Action reg: 0.003927
  l1.weight: grad_norm = 0.005167
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.004795
Total gradient norm: 0.027262
=== Actor Training Debug (Iteration 794) ===
Q mean: -4.574782
Q std: 3.670656
Actor loss: 4.578755
Action reg: 0.003972
  l1.weight: grad_norm = 0.002527
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.002551
Total gradient norm: 0.014313
=== Actor Training Debug (Iteration 795) ===
Q mean: -4.413680
Q std: 3.640519
Actor loss: 4.417642
Action reg: 0.003961
  l1.weight: grad_norm = 0.003458
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.003282
Total gradient norm: 0.016808
=== Actor Training Debug (Iteration 796) ===
Q mean: -4.245375
Q std: 3.344583
Actor loss: 4.249338
Action reg: 0.003963
  l1.weight: grad_norm = 0.005346
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.005583
Total gradient norm: 0.050646
=== Actor Training Debug (Iteration 797) ===
Q mean: -4.232399
Q std: 3.360600
Actor loss: 4.236364
Action reg: 0.003965
  l1.weight: grad_norm = 0.004439
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.004801
Total gradient norm: 0.043637
=== Actor Training Debug (Iteration 798) ===
Q mean: -4.143605
Q std: 3.492837
Actor loss: 4.147554
Action reg: 0.003950
  l1.weight: grad_norm = 0.003732
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.004110
Total gradient norm: 0.041115
=== Actor Training Debug (Iteration 799) ===
Q mean: -4.244308
Q std: 3.106969
Actor loss: 4.248252
Action reg: 0.003945
  l1.weight: grad_norm = 0.002977
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.003566
Total gradient norm: 0.037270
=== Actor Training Debug (Iteration 800) ===
Q mean: -4.410218
Q std: 3.327199
Actor loss: 4.414208
Action reg: 0.003990
  l1.weight: grad_norm = 0.000788
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.000773
Total gradient norm: 0.006003
=== Actor Training Debug (Iteration 801) ===
Q mean: -4.288100
Q std: 3.210001
Actor loss: 4.292091
Action reg: 0.003991
  l1.weight: grad_norm = 0.001165
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001062
Total gradient norm: 0.006322
=== Actor Training Debug (Iteration 802) ===
Q mean: -4.720254
Q std: 3.209415
Actor loss: 4.724235
Action reg: 0.003981
  l1.weight: grad_norm = 0.001646
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.001684
Total gradient norm: 0.009634
=== Actor Training Debug (Iteration 803) ===
Q mean: -4.569750
Q std: 3.657315
Actor loss: 4.573743
Action reg: 0.003993
  l1.weight: grad_norm = 0.002454
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002700
Total gradient norm: 0.028826
=== Actor Training Debug (Iteration 804) ===
Q mean: -4.518426
Q std: 3.546911
Actor loss: 4.522395
Action reg: 0.003969
  l1.weight: grad_norm = 0.003358
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.003497
Total gradient norm: 0.019481
=== Actor Training Debug (Iteration 805) ===
Q mean: -4.592739
Q std: 3.719190
Actor loss: 4.596721
Action reg: 0.003982
  l1.weight: grad_norm = 0.005010
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.005123
Total gradient norm: 0.039771
=== Actor Training Debug (Iteration 806) ===
Q mean: -4.476343
Q std: 3.545297
Actor loss: 4.480307
Action reg: 0.003964
  l1.weight: grad_norm = 0.004426
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.004514
Total gradient norm: 0.026068
=== Actor Training Debug (Iteration 807) ===
Q mean: -4.748916
Q std: 3.605933
Actor loss: 4.752870
Action reg: 0.003954
  l1.weight: grad_norm = 0.004376
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.004234
Total gradient norm: 0.030394
=== Actor Training Debug (Iteration 808) ===
Q mean: -4.533807
Q std: 3.389059
Actor loss: 4.537788
Action reg: 0.003982
  l1.weight: grad_norm = 0.001779
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.001827
Total gradient norm: 0.015625
=== Actor Training Debug (Iteration 809) ===
Q mean: -4.699071
Q std: 3.785077
Actor loss: 4.703047
Action reg: 0.003976
  l1.weight: grad_norm = 0.005227
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.006326
Total gradient norm: 0.073700
=== Actor Training Debug (Iteration 810) ===
Q mean: -4.304765
Q std: 3.271577
Actor loss: 4.308727
Action reg: 0.003962
  l1.weight: grad_norm = 0.002709
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.002442
Total gradient norm: 0.012168
=== Actor Training Debug (Iteration 811) ===
Q mean: -4.477983
Q std: 3.711963
Actor loss: 4.481967
Action reg: 0.003984
  l1.weight: grad_norm = 0.003968
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.004103
Total gradient norm: 0.035709
=== Actor Training Debug (Iteration 812) ===
Q mean: -4.465442
Q std: 3.774195
Actor loss: 4.469386
Action reg: 0.003944
  l1.weight: grad_norm = 0.002828
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.002898
Total gradient norm: 0.017709
=== Actor Training Debug (Iteration 813) ===
Q mean: -4.610183
Q std: 3.334984
Actor loss: 4.614108
Action reg: 0.003925
Action reg: 0.003970
  l1.weight: grad_norm = 0.005070
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.004478
Total gradient norm: 0.026435
=== Actor Training Debug (Iteration 844) ===
Q mean: -4.211075
Q std: 3.511031
Actor loss: 4.215044
Action reg: 0.003969
  l1.weight: grad_norm = 0.002652
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.002342
Total gradient norm: 0.014737
=== Actor Training Debug (Iteration 845) ===
Q mean: -4.202725
Q std: 3.643845
Actor loss: 4.206706
Action reg: 0.003980
  l1.weight: grad_norm = 0.003376
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.003280
Total gradient norm: 0.017421
=== Actor Training Debug (Iteration 846) ===
Q mean: -4.680675
Q std: 3.545941
Actor loss: 4.684654
Action reg: 0.003979
  l1.weight: grad_norm = 0.001919
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.001876
Total gradient norm: 0.014574
=== Actor Training Debug (Iteration 847) ===
Q mean: -4.918290
Q std: 3.863226
Actor loss: 4.922256
Action reg: 0.003967
  l1.weight: grad_norm = 0.001461
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.001523
Total gradient norm: 0.010303
=== Actor Training Debug (Iteration 848) ===
Q mean: -4.693566
Q std: 4.062215
Actor loss: 4.697542
Action reg: 0.003976
  l1.weight: grad_norm = 0.002399
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.002941
Total gradient norm: 0.036863
=== Actor Training Debug (Iteration 849) ===
Q mean: -4.279140
Q std: 3.504998
Actor loss: 4.283123
Action reg: 0.003983
  l1.weight: grad_norm = 0.002459
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002537
Total gradient norm: 0.026855
=== Actor Training Debug (Iteration 850) ===
Q mean: -4.266562
Q std: 3.742712
Actor loss: 4.270521
Action reg: 0.003958
  l1.weight: grad_norm = 0.003137
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.003080
Total gradient norm: 0.028744
=== Actor Training Debug (Iteration 851) ===
Q mean: -5.267391
Q std: 4.033485
Actor loss: 5.271370
Action reg: 0.003979
  l1.weight: grad_norm = 0.001829
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.001780
Total gradient norm: 0.012820
=== Actor Training Debug (Iteration 852) ===
Q mean: -5.017086
Q std: 3.871201
Actor loss: 5.021072
Action reg: 0.003986
  l1.weight: grad_norm = 0.002470
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002474
Total gradient norm: 0.017009
=== Actor Training Debug (Iteration 853) ===
Q mean: -4.254631
Q std: 3.499137
Actor loss: 4.258595
Action reg: 0.003965
  l1.weight: grad_norm = 0.001071
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.000998
Total gradient norm: 0.006285
=== Actor Training Debug (Iteration 854) ===
Q mean: -3.859460
Q std: 3.732084
Actor loss: 3.863441
Action reg: 0.003981
  l1.weight: grad_norm = 0.005151
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.005039
Total gradient norm: 0.034718
=== Actor Training Debug (Iteration 855) ===
Q mean: -4.835371
Q std: 3.952958
Actor loss: 4.839334
Action reg: 0.003963
  l1.weight: grad_norm = 0.002853
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.003115
Total gradient norm: 0.027476
=== Actor Training Debug (Iteration 856) ===
Q mean: -4.424281
Q std: 3.707306
Actor loss: 4.428248
Action reg: 0.003968
  l1.weight: grad_norm = 0.001143
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.000990
Total gradient norm: 0.006908
=== Actor Training Debug (Iteration 857) ===
Q mean: -4.614950
Q std: 3.323972
Actor loss: 4.618905
Action reg: 0.003955
  l1.weight: grad_norm = 0.002107
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.002101
Total gradient norm: 0.014169
=== Actor Training Debug (Iteration 858) ===
Q mean: -4.274763
Q std: 3.952458
Actor loss: 4.278726
Action reg: 0.003963
  l1.weight: grad_norm = 0.003163
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.003283
Total gradient norm: 0.028943
=== Actor Training Debug (Iteration 859) ===
Q mean: -5.024241
Q std: 3.786768
Actor loss: 5.028220
Action reg: 0.003979
  l1.weight: grad_norm = 0.001555
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.001523
Total gradient norm: 0.010737
=== Actor Training Debug (Iteration 860) ===
Q mean: -4.571505
Q std: 3.559075
Actor loss: 4.575475
Action reg: 0.003970
  l1.weight: grad_norm = 0.001745
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.001969
Total gradient norm: 0.022253
=== Actor Training Debug (Iteration 861) ===
Q mean: -4.377964
Q std: 3.577064
Actor loss: 4.381942
Action reg: 0.003978
  l1.weight: grad_norm = 0.002655
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002679
Total gradient norm: 0.014819
=== Actor Training Debug (Iteration 862) ===
Q mean: -4.502350
Q std: 3.795234
Actor loss: 4.506330
Action reg: 0.003979
  l1.weight: grad_norm = 0.000766
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.000724
Total gradient norm: 0.005758
=== Actor Training Debug (Iteration 863) ===
Q mean: -4.225164
Q std: 3.566251
Actor loss: 4.229145
Action reg: 0.003980
  l1.weight: grad_norm = 0.001227
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.001029
Total gradient norm: 0.006785
=== Actor Training Debug (Iteration 864) ===
Q mean: -4.605712
Q std: 3.634915
Actor loss: 4.609682
Action reg: 0.003970
  l1.weight: grad_norm = 0.005610
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.005950
Total gradient norm: 0.048930
=== Actor Training Debug (Iteration 865) ===
Q mean: -4.248280
Q std: 3.756571
Actor loss: 4.252240
Action reg: 0.003960
  l1.weight: grad_norm = 0.005156
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.006200
Total gradient norm: 0.070538
=== Actor Training Debug (Iteration 866) ===
Q mean: -4.610197
Q std: 3.948437
Actor loss: 4.614165
Action reg: 0.003968
  l1.weight: grad_norm = 0.004992
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.004577
Total gradient norm: 0.032870
=== Actor Training Debug (Iteration 867) ===
Q mean: -4.588952
Q std: 3.805222
Actor loss: 4.592929
Action reg: 0.003977
  l1.weight: grad_norm = 0.002086
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.002093
Total gradient norm: 0.011767
=== Actor Training Debug (Iteration 868) ===
Q mean: -4.270082
Q std: 3.709899
Actor loss: 4.274064
Action reg: 0.003982
  l1.weight: grad_norm = 0.002039
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.002243
Total gradient norm: 0.023851
=== Actor Training Debug (Iteration 869) ===
Q mean: -4.578268
Q std: 3.746643
Actor loss: 4.582256
Action reg: 0.003988
  l1.weight: grad_norm = 0.002335
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.002455
Total gradient norm: 0.024362
=== Actor Training Debug (Iteration 870) ===
Q mean: -4.456737
Q std: 3.734375
Actor loss: 4.460723
Action reg: 0.003987
  l1.weight: grad_norm = 0.004295
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004711
Total gradient norm: 0.054281
=== Actor Training Debug (Iteration 871) ===
Q mean: -4.615798
Q std: 3.879591
Actor loss: 4.619777
Action reg: 0.003979
  l1.weight: grad_norm = 0.002373
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.002713
Total gradient norm: 0.033073
=== Actor Training Debug (Iteration 872) ===
Q mean: -4.479087
Q std: 4.025224
Actor loss: 4.483056
Action reg: 0.003969
  l1.weight: grad_norm = 0.002957
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.002681
Total gradient norm: 0.018188
=== Actor Training Debug (Iteration 873) ===
Q mean: -4.487945
Q std: 3.917199
Actor loss: 4.491928
Action reg: 0.003983
  l1.weight: grad_norm = 0.002357
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.002451
Total gradient norm: 0.027347
=== Actor Training Debug (Iteration 874) ===
Q mean: -5.091101
Q std: 4.209075
Actor loss: 5.095078
Action reg: 0.003977
  l1.weight: grad_norm = 0.003702
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.003588
Total gradient norm: 0.027830
=== Actor Training Debug (Iteration 875) ===
Q mean: -4.949486
Q std: 3.931802
Actor loss: 4.953466
Action reg: 0.003980
  l1.weight: grad_norm = 0.007210
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.008441
Total gradient norm: 0.084246
=== Actor Training Debug (Iteration 876) ===
Q mean: -4.242493
Q std: 3.853695
Actor loss: 4.246476
Action reg: 0.003983
  l1.weight: grad_norm = 0.002202
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.002300
Total gradient norm: 0.023796
=== Actor Training Debug (Iteration 877) ===
Q mean: -4.550644
Q std: 3.791547
Actor loss: 4.554622
Action reg: 0.003978
  l1.weight: grad_norm = 0.007505
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.007285
Total gradient norm: 0.067753
=== Actor Training Debug (Iteration 878) ===
Q mean: -4.697948
Q std: 3.994516
Actor loss: 4.701926
Action reg: 0.003978
  l1.weight: grad_norm = 0.004424
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.004665
Total gradient norm: 0.042573
=== Actor Training Debug (Iteration 879) ===
Q mean: -4.770455
Q std: 3.768199
Actor loss: 4.774427
Action reg: 0.003971
  l1.weight: grad_norm = 0.006174
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.005902
Total gradient norm: 0.033567
=== Actor Training Debug (Iteration 880) ===
Q mean: -4.456418
Q std: 3.836216
Actor loss: 4.460387
Action reg: 0.003969
  l1.weight: grad_norm = 0.004962
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.005330
Total gradient norm: 0.059798
=== Actor Training Debug (Iteration 881) ===
Q mean: -4.353247
Q std: 3.799227
Actor loss: 4.357226
Action reg: 0.003979
  l1.weight: grad_norm = 0.004362
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.004042
Total gradient norm: 0.027213
=== Actor Training Debug (Iteration 882) ===
Q mean: -4.656600
Q std: 3.820354
Actor loss: 4.660561
Action reg: 0.003960
  l1.weight: grad_norm = 0.002240
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.002482
Total gradient norm: 0.028723
=== Actor Training Debug (Iteration 883) ===
Q mean: -5.047923
Q std: 4.024117
Actor loss: 5.051904
Action reg: 0.003981
  l1.weight: grad_norm = 0.003358
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.003465
Total gradient norm: 0.023436
=== Actor Training Debug (Iteration 884) ===
Q mean: -4.813360
Q std: 3.826942
Actor loss: 4.817347
Action reg: 0.003986
  l1.weight: grad_norm = 0.001391
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.001386
Total gradient norm: 0.007836
=== Actor Training Debug (Iteration 885) ===
Q mean: -4.696215
Q std: 4.063426
Actor loss: 4.700204
Action reg: 0.003989
  l1.weight: grad_norm = 0.004889
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005025
Total gradient norm: 0.033165
=== Actor Training Debug (Iteration 886) ===
Q mean: -4.564656
Q std: 3.942963
Actor loss: 4.568625
Action reg: 0.003969
  l1.weight: grad_norm = 0.002273
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.002533
Total gradient norm: 0.027378
=== Actor Training Debug (Iteration 887) ===
Q mean: -5.061880
Q std: 4.196617
Actor loss: 5.065863
Action reg: 0.003983
  l1.weight: grad_norm = 0.000795
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.000771
Total gradient norm: 0.008271
=== Actor Training Debug (Iteration 888) ===
Q mean: -4.498415
Q std: 3.625783
Actor loss: 4.502392
Action reg: 0.003977
  l1.weight: grad_norm = 0.001142
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.001107
Total gradient norm: 0.007055
=== Actor Training Debug (Iteration 889) ===
Q mean: -4.756168
Q std: 3.745394
Actor loss: 4.760154
Action reg: 0.003986
  l1.weight: grad_norm = 0.003093
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.003252
Total gradient norm: 0.031870
=== Actor Training Debug (Iteration 890) ===
Q mean: -4.620589
Q std: 3.790868
Actor loss: 4.624574
Action reg: 0.003984
  l1.weight: grad_norm = 0.000781
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.000661
Total gradient norm: 0.003064
=== Actor Training Debug (Iteration 891) ===
Q mean: -4.193777
Q std: 3.556315
Actor loss: 4.197769
Action reg: 0.003992
  l1.weight: grad_norm = 0.002777
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002568
Total gradient norm: 0.017371
=== Actor Training Debug (Iteration 892) ===
Q mean: -4.538918
Q std: 3.737015
Actor loss: 4.542901
Action reg: 0.003983
  l1.weight: grad_norm = 0.002602
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.002461
Total gradient norm: 0.022102
=== Actor Training Debug (Iteration 893) ===
Q mean: -5.096712
Q std: 3.824886
Actor loss: 5.100689
Action reg: 0.003977
  l1.weight: grad_norm = 0.003775
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004003
Total gradient norm: 0.028903
=== Actor Training Debug (Iteration 894) ===
Q mean: -4.911500
Q std: 4.170889
Actor loss: 4.915479
Action reg: 0.003979
  l1.weight: grad_norm = 0.007286
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.006773
Total gradient norm: 0.043088
=== Actor Training Debug (Iteration 895) ===
Q mean: -4.116742
Q std: 3.387996
Actor loss: 4.120721
Action reg: 0.003979
  l1.weight: grad_norm = 0.002907
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.002777
Total gradient norm: 0.015138
=== Actor Training Debug (Iteration 896) ===
Q mean: -4.250085
Q std: 3.740235
Actor loss: 4.254063
Action reg: 0.003978
  l1.weight: grad_norm = 0.002589
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.002618
Total gradient norm: 0.013908
=== Actor Training Debug (Iteration 897) ===
Q mean: -5.025928
Q std: 3.750913
Actor loss: 5.029910
Action reg: 0.003983
  l1.weight: grad_norm = 0.001825
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.001770
Total gradient norm: 0.013817
=== Actor Training Debug (Iteration 898) ===
Q mean: -4.686504
Q std: 3.885747
Actor loss: 4.690486
Action reg: 0.003982
  l1.weight: grad_norm = 0.006235
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.005966
Total gradient norm: 0.047684
=== Actor Training Debug (Iteration 899) ===
Q mean: -4.609770
Q std: 3.907910
Actor loss: 4.613746
Action reg: 0.003977
  l1.weight: grad_norm = 0.004314
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.004013
Total gradient norm: 0.017179
=== Actor Training Debug (Iteration 900) ===
Q mean: -4.469722
Q std: 4.122594
Actor loss: 4.473687
Action reg: 0.003965
  l1.weight: grad_norm = 0.002590
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.002653
Total gradient norm: 0.022217
=== Actor Training Debug (Iteration 901) ===
Q mean: -4.882652
Q std: 4.299958
Actor loss: 4.886624
Action reg: 0.003972
  l1.weight: grad_norm = 0.004007
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.003706
Total gradient norm: 0.017733
=== Actor Training Debug (Iteration 902) ===
Q mean: -4.894045
Q std: 3.972099
Actor loss: 4.898025
Action reg: 0.003980
  l1.weight: grad_norm = 0.003857
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004002
Total gradient norm: 0.034296
=== Actor Training Debug (Iteration 903) ===
Q mean: -4.457741
Q std: 3.990275
Actor loss: 4.461730
Action reg: 0.003990
  l1.weight: grad_norm = 0.004098
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003483
Total gradient norm: 0.017702
=== Actor Training Debug (Iteration 904) ===
Q mean: -4.785093
Q std: 3.957442
Actor loss: 4.789070
Action reg: 0.003976
  l1.weight: grad_norm = 0.003138
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.003747
Total gradient norm: 0.043821
=== Actor Training Debug (Iteration 905) ===
Q mean: -4.908134
Q std: 3.790736
Actor loss: 4.912127
Action reg: 0.003993
  l1.weight: grad_norm = 0.003156
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003018
Total gradient norm: 0.023753
=== Actor Training Debug (Iteration 906) ===
Q mean: -4.565543
Q std: 3.686598
Actor loss: 4.569527
Action reg: 0.003983
  l1.weight: grad_norm = 0.002216
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002205
Total gradient norm: 0.011804
=== Actor Training Debug (Iteration 907) ===
Q mean: -4.804029
Q std: 4.031878
Actor loss: 4.807991
Action reg: 0.003961
  l1.weight: grad_norm = 0.007303
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.007090
Total gradient norm: 0.056999
=== Actor Training Debug (Iteration 908) ===
Q mean: -3.935860
Q std: 3.486028
Actor loss: 3.939843
Action reg: 0.003983
  l1.weight: grad_norm = 0.003918
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003637
Total gradient norm: 0.018829
=== Actor Training Debug (Iteration 909) ===
Q mean: -4.485640
Q std: 4.058834
Actor loss: 4.489621
Action reg: 0.003981
  l1.weight: grad_norm = 0.007507
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.007232
Total gradient norm: 0.063765
=== Actor Training Debug (Iteration 910) ===
Q mean: -5.138920
Q std: 4.304770
Actor loss: 5.142901
Action reg: 0.003981
  l1.weight: grad_norm = 0.001004
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.001053
Total gradient norm: 0.009285
=== Actor Training Debug (Iteration 911) ===
Q mean: -5.184328
Q std: 3.785073
Actor loss: 5.188308
Action reg: 0.003980
  l1.weight: grad_norm = 0.001091
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.001091
Total gradient norm: 0.009304
=== Actor Training Debug (Iteration 912) ===
Q mean: -4.750398
Q std: 4.267826
Actor loss: 4.754351
Action reg: 0.003953
  l1.weight: grad_norm = 0.002862
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.002463
Total gradient norm: 0.010733
=== Actor Training Debug (Iteration 913) ===
Q mean: -4.613849
Q std: 4.076227
Actor loss: 4.617823
Action reg: 0.003974
  l1.weight: grad_norm = 0.003138
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.003010
Total gradient norm: 0.025837
=== Actor Training Debug (Iteration 914) ===
Q mean: -4.394979
Q std: 3.836453
Actor loss: 4.398965
Action reg: 0.003986
  l1.weight: grad_norm = 0.004810
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.004986
Total gradient norm: 0.028085
=== Actor Training Debug (Iteration 915) ===
Q mean: -4.900427
Q std: 4.274764
Actor loss: 4.904396
Action reg: 0.003969
  l1.weight: grad_norm = 0.008433
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.008703
Total gradient norm: 0.087635
=== Actor Training Debug (Iteration 916) ===
Q mean: -4.749750
Q std: 4.063141
Actor loss: 4.753709
Action reg: 0.003959
  l1.weight: grad_norm = 0.002613
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.002826
Total gradient norm: 0.026778
=== Actor Training Debug (Iteration 917) ===
Q mean: -4.856000
Q std: 4.057412
Actor loss: 4.859957
Action reg: 0.003957
  l1.weight: grad_norm = 0.004096
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.003584
Total gradient norm: 0.032219
=== Actor Training Debug (Iteration 918) ===
Q mean: -4.468251
Q std: 3.769531
Actor loss: 4.472229
Action reg: 0.003977
  l1.weight: grad_norm = 0.001493
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.001513
Total gradient norm: 0.010118
=== Actor Training Debug (Iteration 919) ===
Q mean: -4.847957
Q std: 4.032661
Actor loss: 4.851924
Action reg: 0.003967
  l1.weight: grad_norm = 0.003929
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.004543
Total gradient norm: 0.053684
=== Actor Training Debug (Iteration 920) ===
Q mean: -4.992226
Q std: 4.052082
Actor loss: 4.996203
Action reg: 0.003977
  l1.weight: grad_norm = 0.005216
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.005117
Total gradient norm: 0.041181
=== Actor Training Debug (Iteration 921) ===
Q mean: -4.951875
Q std: 4.138628
Actor loss: 4.955857
Action reg: 0.003982
  l1.weight: grad_norm = 0.002190
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.002279
Total gradient norm: 0.015321
=== Actor Training Debug (Iteration 922) ===
Q mean: -4.463129
Q std: 4.001600
Actor loss: 4.467116
Action reg: 0.003988
  l1.weight: grad_norm = 0.001111
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.001206
Total gradient norm: 0.013274
=== Actor Training Debug (Iteration 923) ===
Q mean: -4.823612
Q std: 4.032833
Actor loss: 4.827584
Action reg: 0.003972
  l1.weight: grad_norm = 0.025339
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.026664
Total gradient norm: 0.222970
=== Actor Training Debug (Iteration 924) ===
Q mean: -5.260404
Q std: 4.341749
Actor loss: 5.264393
Action reg: 0.003989
  l1.weight: grad_norm = 0.002965
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002858
Total gradient norm: 0.024420
=== Actor Training Debug (Iteration 925) ===
Q mean: -4.575019
Q std: 3.832599
Actor loss: 4.578988
Action reg: 0.003969
  l1.weight: grad_norm = 0.008679
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.007805
Total gradient norm: 0.053097
=== Actor Training Debug (Iteration 926) ===
Q mean: -4.575669
Q std: 4.193694
Actor loss: 4.579649
Action reg: 0.003980
  l1.weight: grad_norm = 0.004334
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.004821
Total gradient norm: 0.047219
=== Actor Training Debug (Iteration 927) ===
Q mean: -4.863679
Q std: 4.188631
Actor loss: 4.867633
Action reg: 0.003954
  l1.weight: grad_norm = 0.005224
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.005141
Total gradient norm: 0.037657
=== Actor Training Debug (Iteration 928) ===
Q mean: -4.389963
Q std: 3.951427
Actor loss: 4.393943
Action reg: 0.003981
  l1.weight: grad_norm = 0.004173
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.003981
Total gradient norm: 0.032647
=== Actor Training Debug (Iteration 929) ===
Q mean: -4.580317
Q std: 3.911253
Actor loss: 4.584285
Action reg: 0.003968
  l1.weight: grad_norm = 0.006375
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.007239
Total gradient norm: 0.076001
=== Actor Training Debug (Iteration 930) ===
Q mean: -4.930248
Q std: 4.347090
Actor loss: 4.934234
Action reg: 0.003986
  l1.weight: grad_norm = 0.002868
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.002750
Total gradient norm: 0.020588
=== Actor Training Debug (Iteration 931) ===
Q mean: -4.618439
Q std: 3.967434
Actor loss: 4.622412
Action reg: 0.003973
  l1.weight: grad_norm = 0.004977
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.004689
Total gradient norm: 0.022317
=== Actor Training Debug (Iteration 932) ===
Q mean: -4.134310
Q std: 3.628522
Actor loss: 4.138279
Action reg: 0.003969
  l1.weight: grad_norm = 0.005152
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.005888
Total gradient norm: 0.074143
=== Actor Training Debug (Iteration 933) ===
Q mean: -4.783434
Q std: 4.375683
Actor loss: 4.787413
Action reg: 0.003979
  l1.weight: grad_norm = 0.004181
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.004232
Total gradient norm: 0.020442
=== Actor Training Debug (Iteration 934) ===
Q mean: -4.668623
Q std: 4.145857
Actor loss: 4.672602
Action reg: 0.003979
  l1.weight: grad_norm = 0.002915
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.002772
Total gradient norm: 0.018894
=== Actor Training Debug (Iteration 935) ===
Q mean: -5.364012
Q std: 4.307264
Actor loss: 5.367999
Action reg: 0.003987
  l1.weight: grad_norm = 0.002650
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002262
Total gradient norm: 0.009119
=== Actor Training Debug (Iteration 936) ===
Q mean: -4.363834
Q std: 4.002863
Actor loss: 4.367812
Action reg: 0.003978
  l1.weight: grad_norm = 0.002755
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.002553
Total gradient norm: 0.018997
=== Actor Training Debug (Iteration 937) ===
Q mean: -5.095882
Q std: 4.189730
Actor loss: 5.099869
Action reg: 0.003986
  l1.weight: grad_norm = 0.005362
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005021
Total gradient norm: 0.049347
=== Actor Training Debug (Iteration 938) ===
Q mean: -5.077452
Q std: 4.395669
Actor loss: 5.081433
Action reg: 0.003981
  l1.weight: grad_norm = 0.003038
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.002588
Total gradient norm: 0.017760
=== Actor Training Debug (Iteration 939) ===
Q mean: -4.567780
Q std: 3.862496
Actor loss: 4.571756
Action reg: 0.003977
  l1.weight: grad_norm = 0.002712
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.002702
Total gradient norm: 0.018977
=== Actor Training Debug (Iteration 940) ===
Q mean: -4.472036
Q std: 4.152908
Actor loss: 4.476023
Action reg: 0.003987
  l1.weight: grad_norm = 0.002890
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002872
Total gradient norm: 0.014502
=== Actor Training Debug (Iteration 941) ===
Q mean: -4.590498
Q std: 4.155449
Actor loss: 4.594481
Action reg: 0.003983
  l1.weight: grad_norm = 0.007249
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.006385
Total gradient norm: 0.043006
=== Actor Training Debug (Iteration 942) ===
Q mean: -5.113625
Q std: 4.168974
Actor loss: 5.117606
Action reg: 0.003982
  l1.weight: grad_norm = 0.008077
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.008120
Total gradient norm: 0.079184
=== Actor Training Debug (Iteration 943) ===
Q mean: -5.280937
Q std: 4.131492
Actor loss: 5.284903
Action reg: 0.003966
  l1.weight: grad_norm = 0.002752
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.002918
Total gradient norm: 0.015801
=== Actor Training Debug (Iteration 944) ===
Q mean: -5.537779
Q std: 4.193717
Actor loss: 5.541772
Action reg: 0.003994
  l1.weight: grad_norm = 0.001746
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001688
Total gradient norm: 0.014343
=== Actor Training Debug (Iteration 945) ===
Q mean: -4.555202
Q std: 3.626344
Actor loss: 4.559174
Action reg: 0.003971
  l1.weight: grad_norm = 0.012179
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.013271
Total gradient norm: 0.116978
=== Actor Training Debug (Iteration 946) ===
Q mean: -4.700504
Q std: 4.347899
Actor loss: 4.704486
Action reg: 0.003982
  l1.weight: grad_norm = 0.005477
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.006145
Total gradient norm: 0.054612
=== Actor Training Debug (Iteration 947) ===
Q mean: -4.669136
Q std: 3.991254
Actor loss: 4.673119
Action reg: 0.003983
  l1.weight: grad_norm = 0.006361
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.006912
Total gradient norm: 0.058265
=== Actor Training Debug (Iteration 948) ===
Q mean: -4.894320
Q std: 4.239276
Actor loss: 4.898306
Action reg: 0.003986
  l1.weight: grad_norm = 0.001381
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.001351
Total gradient norm: 0.007824
=== Actor Training Debug (Iteration 949) ===
Q mean: -4.724512
Q std: 4.056642
Actor loss: 4.728497
Action reg: 0.003985
  l1.weight: grad_norm = 0.004181
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.003839
Total gradient norm: 0.023410
=== Actor Training Debug (Iteration 950) ===
Q mean: -4.227019
Q std: 4.121436
Actor loss: 4.230973
Action reg: 0.003954
  l1.weight: grad_norm = 0.003143
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.002866
Total gradient norm: 0.021353
=== Actor Training Debug (Iteration 951) ===
Q mean: -4.782325
Q std: 4.306383
Actor loss: 4.786288
Action reg: 0.003963
  l1.weight: grad_norm = 0.008199
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.006939
Total gradient norm: 0.055425
=== Actor Training Debug (Iteration 952) ===
Q mean: -4.978024
Q std: 4.321090
Actor loss: 4.981982
Action reg: 0.003957
  l1.weight: grad_norm = 0.004973
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.005172
Total gradient norm: 0.037658
=== Actor Training Debug (Iteration 953) ===
Q mean: -4.924742
Q std: 4.325708
Actor loss: 4.928728
Action reg: 0.003986
  l1.weight: grad_norm = 0.000915
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.000983
Total gradient norm: 0.008840
=== Actor Training Debug (Iteration 954) ===
Q mean: -5.390409
Q std: 4.402063
Actor loss: 5.394382
Action reg: 0.003973
  l1.weight: grad_norm = 0.005561
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.005193
Total gradient norm: 0.034228
=== Actor Training Debug (Iteration 955) ===
Q mean: -4.833210
Q std: 3.981313
Actor loss: 4.837205
Action reg: 0.003995
  l1.weight: grad_norm = 0.001323
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001132
Total gradient norm: 0.006487
=== Actor Training Debug (Iteration 956) ===
Q mean: -5.013077
Q std: 4.119557
Actor loss: 5.017042
Action reg: 0.003965
  l1.weight: grad_norm = 0.002074
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.002355
Total gradient norm: 0.030388
=== Actor Training Debug (Iteration 957) ===
Q mean: -5.011480
Q std: 4.315148
Actor loss: 5.015459
Action reg: 0.003979
  l1.weight: grad_norm = 0.002809
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.002728
Total gradient norm: 0.020391
=== Actor Training Debug (Iteration 958) ===
Q mean: -4.364985
Q std: 3.827387
Actor loss: 4.368958
Action reg: 0.003973
  l1.weight: grad_norm = 0.009208
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.010631
Total gradient norm: 0.105900
=== Actor Training Debug (Iteration 959) ===
Q mean: -4.845129
Q std: 4.224646
Actor loss: 4.849111
Action reg: 0.003982
  l1.weight: grad_norm = 0.004023
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003645
Total gradient norm: 0.029758
=== Actor Training Debug (Iteration 960) ===
Q mean: -4.874384
Q std: 4.026165
Actor loss: 4.878370
Action reg: 0.003986
  l1.weight: grad_norm = 0.004433
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.004400
Total gradient norm: 0.034116
=== Actor Training Debug (Iteration 961) ===
Q mean: -4.623164
Q std: 4.132145
Actor loss: 4.627146
Action reg: 0.003983
  l1.weight: grad_norm = 0.004064
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.004254
Total gradient norm: 0.041811
=== Actor Training Debug (Iteration 962) ===
Q mean: -4.884233
Q std: 4.279812
Actor loss: 4.888213
Action reg: 0.003979
  l1.weight: grad_norm = 0.005217
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.005328
Total gradient norm: 0.052457
=== Actor Training Debug (Iteration 963) ===
Q mean: -4.524822
Q std: 4.197154
Actor loss: 4.528811
Action reg: 0.003989
  l1.weight: grad_norm = 0.005104
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.004819
Total gradient norm: 0.041296
=== Actor Training Debug (Iteration 964) ===
Q mean: -4.939431
Q std: 4.196094
Actor loss: 4.943422
Action reg: 0.003991
  l1.weight: grad_norm = 0.001825
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001374
Total gradient norm: 0.007507
=== Actor Training Debug (Iteration 965) ===
Q mean: -5.605265
Q std: 4.605213
Actor loss: 5.609246
Action reg: 0.003981
  l1.weight: grad_norm = 0.004253
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.004492
Total gradient norm: 0.037500
=== Actor Training Debug (Iteration 966) ===
Q mean: -4.785510
Q std: 4.209605
Actor loss: 4.789498
Action reg: 0.003988
  l1.weight: grad_norm = 0.000763
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.000765
Total gradient norm: 0.004218
=== Actor Training Debug (Iteration 967) ===
Q mean: -4.855698
Q std: 4.432838
Actor loss: 4.859671
Action reg: 0.003973
  l1.weight: grad_norm = 0.005906
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.006078
Total gradient norm: 0.049091
=== Actor Training Debug (Iteration 968) ===
Q mean: -5.399426
Q std: 4.391304
Actor loss: 5.403409
Action reg: 0.003983
  l1.weight: grad_norm = 0.000569
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.000496
Total gradient norm: 0.003914
=== Actor Training Debug (Iteration 969) ===
Q mean: -4.993016
Q std: 4.142159
Actor loss: 4.996994
Action reg: 0.003978
  l1.weight: grad_norm = 0.009575
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.009231
Total gradient norm: 0.061648
=== Actor Training Debug (Iteration 970) ===
Q mean: -4.681166
Q std: 4.180561
Actor loss: 4.685162
Action reg: 0.003996
  l1.weight: grad_norm = 0.002609
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002722
Total gradient norm: 0.025752
=== Actor Training Debug (Iteration 971) ===
Q mean: -4.720495
Q std: 4.369912
Actor loss: 4.724459
Action reg: 0.003964
  l1.weight: grad_norm = 0.007124
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.005799
Total gradient norm: 0.028683
=== Actor Training Debug (Iteration 972) ===
Q mean: -4.589372
Q std: 4.508184
Actor loss: 4.593367
Action reg: 0.003995
  l1.weight: grad_norm = 0.001910
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001464
Total gradient norm: 0.008555
=== Actor Training Debug (Iteration 973) ===
Q mean: -4.883522
Q std: 4.239009
Actor loss: 4.887494
Action reg: 0.003972
  l1.weight: grad_norm = 0.004129
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.004013
Total gradient norm: 0.039128
=== Actor Training Debug (Iteration 974) ===
Q mean: -4.806173
Q std: 3.777607
Actor loss: 4.810143
Action reg: 0.003970
  l1.weight: grad_norm = 0.002109
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.002254
Total gradient norm: 0.016636
=== Actor Training Debug (Iteration 975) ===
Q mean: -4.997896
Q std: 4.258471
Actor loss: 5.001864
Action reg: 0.003968
  l1.weight: grad_norm = 0.005839
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.005683
Total gradient norm: 0.036829
=== Actor Training Debug (Iteration 976) ===
Q mean: -4.910153
Q std: 4.413888
Actor loss: 4.914128
Action reg: 0.003975
  l1.weight: grad_norm = 0.005777
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.005521
Total gradient norm: 0.037986
=== Actor Training Debug (Iteration 977) ===
Q mean: -4.059042
Q std: 4.015688
Actor loss: 4.063009
Action reg: 0.003967
  l1.weight: grad_norm = 0.005517
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.004946
Total gradient norm: 0.041212
=== Actor Training Debug (Iteration 978) ===
Q mean: -5.049797
Q std: 4.388760
Actor loss: 5.053780
Action reg: 0.003984
  l1.weight: grad_norm = 0.006010
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.005725
Total gradient norm: 0.034533
=== Actor Training Debug (Iteration 979) ===
Q mean: -5.329535
Q std: 4.373014
Actor loss: 5.333512
Action reg: 0.003977
  l1.weight: grad_norm = 0.005407
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.004731
Total gradient norm: 0.025562
=== Actor Training Debug (Iteration 980) ===
Q mean: -5.078897
Q std: 4.178742
Actor loss: 5.082882
Action reg: 0.003985
  l1.weight: grad_norm = 0.002871
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002768
Total gradient norm: 0.012398
=== Actor Training Debug (Iteration 981) ===
Q mean: -4.629748
Q std: 4.188503
Actor loss: 4.633721
Action reg: 0.003973
  l1.weight: grad_norm = 0.001248
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.001114
Total gradient norm: 0.008851
=== Actor Training Debug (Iteration 982) ===
Q mean: -4.757726
Q std: 4.350741
Actor loss: 4.761706
Action reg: 0.003980
  l1.weight: grad_norm = 0.003429
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.002887
Total gradient norm: 0.016465
=== Actor Training Debug (Iteration 983) ===
Q mean: -5.348620
Q std: 4.332041
Actor loss: 5.352593
Action reg: 0.003973
  l1.weight: grad_norm = 0.005922
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.005444
Total gradient norm: 0.021499
=== Actor Training Debug (Iteration 984) ===
Q mean: -5.028576
Q std: 4.263863
Actor loss: 5.032561
Action reg: 0.003985
  l1.weight: grad_norm = 0.000911
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.000963
Total gradient norm: 0.008861
=== Actor Training Debug (Iteration 985) ===
Q mean: -4.157082
Q std: 4.195717
Actor loss: 4.161035
Action reg: 0.003953
  l1.weight: grad_norm = 0.004432
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.004411
Total gradient norm: 0.025774
=== Actor Training Debug (Iteration 986) ===
Q mean: -5.018846
Q std: 4.368077
Actor loss: 5.022831
Action reg: 0.003986
  l1.weight: grad_norm = 0.008204
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.008266
Total gradient norm: 0.072622
=== Actor Training Debug (Iteration 987) ===
Q mean: -5.332284
Q std: 4.704323
Actor loss: 5.336274
Action reg: 0.003990
  l1.weight: grad_norm = 0.003309
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003548
Total gradient norm: 0.035761
=== Actor Training Debug (Iteration 988) ===
Q mean: -4.875993
Q std: 4.369063
Actor loss: 4.879976
Action reg: 0.003983
  l1.weight: grad_norm = 0.001907
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.001531
Total gradient norm: 0.007939
=== Actor Training Debug (Iteration 989) ===
Q mean: -4.888266
Q std: 4.457495
Actor loss: 4.892250
Action reg: 0.003985
  l1.weight: grad_norm = 0.002571
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.002278
Total gradient norm: 0.009584
=== Actor Training Debug (Iteration 990) ===
Q mean: -4.715094
Q std: 4.074406
Actor loss: 4.719060
Action reg: 0.003966
  l1.weight: grad_norm = 0.003112
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.003040
Total gradient norm: 0.014889
=== Actor Training Debug (Iteration 991) ===
Q mean: -5.169317
Q std: 4.562550
Actor loss: 5.173283
Action reg: 0.003966
  l1.weight: grad_norm = 0.005350
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.004814
Total gradient norm: 0.021337
=== Actor Training Debug (Iteration 992) ===
Q mean: -5.391217
Q std: 5.210769
Actor loss: 5.395201
Action reg: 0.003983
  l1.weight: grad_norm = 0.001014
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.000965
Total gradient norm: 0.004685
=== Actor Training Debug (Iteration 993) ===
Q mean: -4.713222
Q std: 4.606107
Actor loss: 4.717196
Action reg: 0.003975
  l1.weight: grad_norm = 0.010457
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.009738
Total gradient norm: 0.084075
=== Actor Training Debug (Iteration 994) ===
Q mean: -5.160582
Q std: 4.629077
Actor loss: 5.164570
Action reg: 0.003988
  l1.weight: grad_norm = 0.004003
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004263
Total gradient norm: 0.029239
=== Actor Training Debug (Iteration 995) ===
Q mean: -5.125809
Q std: 4.795150
Actor loss: 5.129795
Action reg: 0.003986
  l1.weight: grad_norm = 0.002825
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.002561
Total gradient norm: 0.012066
=== Actor Training Debug (Iteration 996) ===
Q mean: -4.701050
Q std: 4.369623
Actor loss: 4.705020
Action reg: 0.003970
  l1.weight: grad_norm = 0.002721
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.002740
Total gradient norm: 0.027629
=== Actor Training Debug (Iteration 997) ===
Q mean: -4.534970
Q std: 4.280003
Actor loss: 4.538950
Action reg: 0.003980
  l1.weight: grad_norm = 0.004313
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.003931
Total gradient norm: 0.028808
=== Actor Training Debug (Iteration 998) ===
Q mean: -4.617268
Q std: 4.233127
Actor loss: 4.621252
Action reg: 0.003984
  l1.weight: grad_norm = 0.001977
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.002209
Total gradient norm: 0.019702
=== Actor Training Debug (Iteration 999) ===
Q mean: -5.405955
Q std: 4.635296
Actor loss: 5.409937
Action reg: 0.003981
  l1.weight: grad_norm = 0.005220
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.005190
Total gradient norm: 0.038352
=== Actor Training Debug (Iteration 1000) ===
Q mean: -5.153563
Q std: 4.469612
Actor loss: 5.157546
Action reg: 0.003982
  l1.weight: grad_norm = 0.007208
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.006885
Total gradient norm: 0.034979
Step 6000: Critic Loss: 0.8411, Actor Loss: 5.1575, Q Value: -5.1536
  Average reward: -304.749 | Average length: 100.0
Evaluation at episode 60: -304.749
=== Actor Training Debug (Iteration 1001) ===
Q mean: -4.967439
Q std: 4.320588
Actor loss: 4.971421
Action reg: 0.003983
  l1.weight: grad_norm = 0.003610
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003244
Total gradient norm: 0.018581
=== Actor Training Debug (Iteration 1002) ===
Q mean: -4.350859
Q std: 4.163997
Actor loss: 4.354835
Action reg: 0.003976
  l1.weight: grad_norm = 0.002036
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.001916
Total gradient norm: 0.008641
=== Actor Training Debug (Iteration 1003) ===
Q mean: -4.805718
Q std: 4.661598
Actor loss: 4.809687
Action reg: 0.003969
  l1.weight: grad_norm = 0.005980
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.005744
Total gradient norm: 0.037584
=== Actor Training Debug (Iteration 1004) ===
Q mean: -4.930033
Q std: 4.719880
Actor loss: 4.934020
Action reg: 0.003987
  l1.weight: grad_norm = 0.001012
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.000988
Total gradient norm: 0.006346
=== Actor Training Debug (Iteration 1005) ===
Q mean: -4.985216
Q std: 4.556811
Actor loss: 4.989196
Action reg: 0.003980
  l1.weight: grad_norm = 0.002617
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.002274
Total gradient norm: 0.012179
=== Actor Training Debug (Iteration 1006) ===
Q mean: -4.828239
Q std: 4.596290
Actor loss: 4.832220
Action reg: 0.003981
  l1.weight: grad_norm = 0.004637
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.004337
Total gradient norm: 0.019029
=== Actor Training Debug (Iteration 1007) ===
Q mean: -5.066500
Q std: 4.508884
Actor loss: 5.070487
Action reg: 0.003987
  l1.weight: grad_norm = 0.009413
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.009794
Total gradient norm: 0.101755
=== Actor Training Debug (Iteration 1008) ===
Q mean: -5.581338
Q std: 4.809532
Actor loss: 5.585319
Action reg: 0.003981
  l1.weight: grad_norm = 0.004422
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.003785
Total gradient norm: 0.018267
=== Actor Training Debug (Iteration 1009) ===
Q mean: -4.377085
Q std: 4.056586
Actor loss: 4.381063
Action reg: 0.003978
  l1.weight: grad_norm = 0.010711
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.009232
Total gradient norm: 0.038040
=== Actor Training Debug (Iteration 1010) ===
Q mean: -5.031059
Q std: 4.550056
Actor loss: 5.035035
Action reg: 0.003976
  l1.weight: grad_norm = 0.004366
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.003821
Total gradient norm: 0.022528
=== Actor Training Debug (Iteration 1011) ===
Q mean: -4.036641
Q std: 4.177660
Actor loss: 4.040625
Action reg: 0.003984
  l1.weight: grad_norm = 0.004730
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.004164
Total gradient norm: 0.031670
=== Actor Training Debug (Iteration 1012) ===
Q mean: -4.904301
Q std: 4.466121
Actor loss: 4.908284
Action reg: 0.003983
  l1.weight: grad_norm = 0.006333
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.005589
Total gradient norm: 0.026805
=== Actor Training Debug (Iteration 1013) ===
Q mean: -5.073341
Q std: 4.321145
Actor loss: 5.077331
Action reg: 0.003990
  l1.weight: grad_norm = 0.001398
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.001390
Total gradient norm: 0.013855
=== Actor Training Debug (Iteration 1014) ===
Q mean: -5.801255
Q std: 5.070425
Actor loss: 5.805223
Action reg: 0.003967
  l1.weight: grad_norm = 0.003541
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.003458
Total gradient norm: 0.018573
=== Actor Training Debug (Iteration 1015) ===
Q mean: -5.249238
Q std: 4.770703
Actor loss: 5.253214
Action reg: 0.003976
  l1.weight: grad_norm = 0.005184
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.005486
Total gradient norm: 0.040610
=== Actor Training Debug (Iteration 1016) ===
Q mean: -4.453732
Q std: 4.748125
Actor loss: 4.457711
Action reg: 0.003978
  l1.weight: grad_norm = 0.002808
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.003481
Total gradient norm: 0.039291
=== Actor Training Debug (Iteration 1017) ===
Q mean: -4.382136
Q std: 4.567273
Actor loss: 4.386126
Action reg: 0.003990
  l1.weight: grad_norm = 0.003842
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.003175
Total gradient norm: 0.019159
=== Actor Training Debug (Iteration 1018) ===
Q mean: -5.052853
Q std: 4.290329
Actor loss: 5.056819
Action reg: 0.003967
  l1.weight: grad_norm = 0.007494
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.005889
Total gradient norm: 0.040874
=== Actor Training Debug (Iteration 1019) ===
Q mean: -5.599966
Q std: 4.417879
Actor loss: 5.603954
Action reg: 0.003988
  l1.weight: grad_norm = 0.002129
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.002103
Total gradient norm: 0.016006
=== Actor Training Debug (Iteration 1020) ===
Q mean: -5.484619
Q std: 4.431462
Actor loss: 5.488605
Action reg: 0.003986
  l1.weight: grad_norm = 0.002891
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.002545
Total gradient norm: 0.009133
=== Actor Training Debug (Iteration 1021) ===
Q mean: -5.365149
Q std: 4.829682
Actor loss: 5.369136
Action reg: 0.003987
  l1.weight: grad_norm = 0.004287
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.004106
Total gradient norm: 0.029226
=== Actor Training Debug (Iteration 1022) ===
Q mean: -4.683950
Q std: 4.560615
Actor loss: 4.687922
Action reg: 0.003971
  l1.weight: grad_norm = 0.008195
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.008148
Total gradient norm: 0.058024
=== Actor Training Debug (Iteration 1023) ===
Q mean: -4.261767
Q std: 4.531250
Actor loss: 4.265749
Action reg: 0.003982
  l1.weight: grad_norm = 0.011378
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.011408
Total gradient norm: 0.083741
=== Actor Training Debug (Iteration 1024) ===
Q mean: -4.669587
Q std: 4.644365
Actor loss: 4.673558
Action reg: 0.003972
  l1.weight: grad_norm = 0.003577
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.003138
Total gradient norm: 0.018339
=== Actor Training Debug (Iteration 1025) ===
Q mean: -5.333680
Q std: 4.403968
Actor loss: 5.337655
Action reg: 0.003975
  l1.weight: grad_norm = 0.002434
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.002340
Total gradient norm: 0.015570
=== Actor Training Debug (Iteration 1026) ===
Q mean: -5.605394
Q std: 4.697902
Actor loss: 5.609368
Action reg: 0.003973
  l1.weight: grad_norm = 0.007556
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.006828
Total gradient norm: 0.042183
=== Actor Training Debug (Iteration 1027) ===
Q mean: -5.166555
Q std: 4.373691
Actor loss: 5.170542
Action reg: 0.003987
  l1.weight: grad_norm = 0.004400
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.003827
Total gradient norm: 0.018127
=== Actor Training Debug (Iteration 1028) ===
Q mean: -5.102256
Q std: 4.421990
Actor loss: 5.106235
Action reg: 0.003979
  l1.weight: grad_norm = 0.003012
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.002693
Total gradient norm: 0.018756
=== Actor Training Debug (Iteration 1029) ===
Q mean: -4.868422
Q std: 4.349980
Actor loss: 4.872403
Action reg: 0.003982
  l1.weight: grad_norm = 0.000799
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.000809
Total gradient norm: 0.009012
=== Actor Training Debug (Iteration 1030) ===
Q mean: -5.277801
Q std: 4.400440
Actor loss: 5.281790
Action reg: 0.003990
  l1.weight: grad_norm = 0.003339
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.003047
Total gradient norm: 0.021849
=== Actor Training Debug (Iteration 1031) ===
Q mean: -5.446358
Q std: 4.833631
Actor loss: 5.450341
Action reg: 0.003983
  l1.weight: grad_norm = 0.001626
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.001800
Total gradient norm: 0.007939
=== Actor Training Debug (Iteration 1032) ===
Q mean: -4.246282
Q std: 4.104630
Actor loss: 4.250276
Action reg: 0.003994
  l1.weight: grad_norm = 0.000944
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.000999
Total gradient norm: 0.009633
=== Actor Training Debug (Iteration 1033) ===
Q mean: -4.777283
Q std: 4.395455
Actor loss: 4.781271
Action reg: 0.003988
  l1.weight: grad_norm = 0.002811
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.002666
Total gradient norm: 0.017974
=== Actor Training Debug (Iteration 1034) ===
Q mean: -5.244859
Q std: 4.422815
Actor loss: 5.248844
Action reg: 0.003985
  l1.weight: grad_norm = 0.004626
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.005051
Total gradient norm: 0.041445
=== Actor Training Debug (Iteration 1035) ===
Q mean: -5.383741
Q std: 4.644258
Actor loss: 5.387722
Action reg: 0.003981
  l1.weight: grad_norm = 0.005860
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005183
Total gradient norm: 0.026505
=== Actor Training Debug (Iteration 1036) ===
Q mean: -4.904860
Q std: 4.616373
Actor loss: 4.908842
Action reg: 0.003981
  l1.weight: grad_norm = 0.002743
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.002731
Total gradient norm: 0.024901
=== Actor Training Debug (Iteration 1037) ===
Q mean: -4.925889
Q std: 4.758924
Actor loss: 4.929874
Action reg: 0.003986
  l1.weight: grad_norm = 0.001929
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.001572
Total gradient norm: 0.011649
=== Actor Training Debug (Iteration 1038) ===
Q mean: -4.686894
Q std: 4.575385
Actor loss: 4.690877
Action reg: 0.003983
  l1.weight: grad_norm = 0.001201
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.001110
Total gradient norm: 0.006306
=== Actor Training Debug (Iteration 1039) ===
Q mean: -4.556185
Q std: 4.114627
Actor loss: 4.560156
Action reg: 0.003971
  l1.weight: grad_norm = 0.007172
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.006700
Total gradient norm: 0.051935
=== Actor Training Debug (Iteration 1040) ===
Q mean: -5.477331
Q std: 4.668507
Actor loss: 5.481315
Action reg: 0.003984
  l1.weight: grad_norm = 0.011678
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.010460
Total gradient norm: 0.069803
=== Actor Training Debug (Iteration 1041) ===
Q mean: -5.514998
Q std: 4.537190
Actor loss: 5.518971
Action reg: 0.003973
  l1.weight: grad_norm = 0.000648
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.000618
Total gradient norm: 0.006318
=== Actor Training Debug (Iteration 1042) ===
Q mean: -5.170195
Q std: 4.213118
Actor loss: 5.174193
Action reg: 0.003998
  l1.weight: grad_norm = 0.000717
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.000565
Total gradient norm: 0.002262
=== Actor Training Debug (Iteration 1043) ===
Q mean: -4.964332
Q std: 4.197135
Actor loss: 4.968322
Action reg: 0.003990
  l1.weight: grad_norm = 0.002654
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.002830
Total gradient norm: 0.023059
=== Actor Training Debug (Iteration 1044) ===
Q mean: -4.292971
Q std: 4.341268
Actor loss: 4.296937
Action reg: 0.003966
  l1.weight: grad_norm = 0.006113
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.005180
Total gradient norm: 0.031600
=== Actor Training Debug (Iteration 1045) ===
Q mean: -5.180549
Q std: 4.219189
Actor loss: 5.184532
Action reg: 0.003983
  l1.weight: grad_norm = 0.005962
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.005437
Total gradient norm: 0.027744
=== Actor Training Debug (Iteration 1046) ===
Q mean: -5.087777
Q std: 4.728583
Actor loss: 5.091760
Action reg: 0.003983
  l1.weight: grad_norm = 0.003024
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.002380
Total gradient norm: 0.010520
=== Actor Training Debug (Iteration 1047) ===
Q mean: -5.129156
Q std: 4.872221
Actor loss: 5.133140
Action reg: 0.003985
  l1.weight: grad_norm = 0.004624
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004477
Total gradient norm: 0.018831
=== Actor Training Debug (Iteration 1048) ===
Q mean: -4.975383
Q std: 4.154173
Actor loss: 4.979371
Action reg: 0.003988
  l1.weight: grad_norm = 0.002108
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001787
Total gradient norm: 0.008025
=== Actor Training Debug (Iteration 1049) ===
Q mean: -5.473865
Q std: 4.654499
Actor loss: 5.477831
Action reg: 0.003967
  l1.weight: grad_norm = 0.003174
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.003657
Total gradient norm: 0.034961
=== Actor Training Debug (Iteration 1050) ===
Q mean: -5.247401
Q std: 4.365703
Actor loss: 5.251380
Action reg: 0.003979
  l1.weight: grad_norm = 0.013498
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.014300
Total gradient norm: 0.118419
=== Actor Training Debug (Iteration 1051) ===
Q mean: -5.089106
Q std: 4.802277
Actor loss: 5.093094
Action reg: 0.003988
  l1.weight: grad_norm = 0.006977
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.006051
Total gradient norm: 0.037288
=== Actor Training Debug (Iteration 1052) ===
Q mean: -4.665449
Q std: 4.650009
Actor loss: 4.669416
Action reg: 0.003968
  l1.weight: grad_norm = 0.001851
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.001713
Total gradient norm: 0.016921
=== Actor Training Debug (Iteration 1053) ===
Q mean: -5.054624
Q std: 4.904773
Actor loss: 5.058608
Action reg: 0.003984
  l1.weight: grad_norm = 0.016432
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.016426
Total gradient norm: 0.111912
=== Actor Training Debug (Iteration 1054) ===
Q mean: -4.901657
Q std: 4.531667
Actor loss: 4.905631
Action reg: 0.003974
  l1.weight: grad_norm = 0.011254
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.011465
Total gradient norm: 0.072534
=== Actor Training Debug (Iteration 1055) ===
Q mean: -5.373858
Q std: 4.836684
Actor loss: 5.377835
Action reg: 0.003977
  l1.weight: grad_norm = 0.007230
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.007542
Total gradient norm: 0.052973
=== Actor Training Debug (Iteration 1056) ===
Q mean: -5.150587
Q std: 4.797843
Actor loss: 5.154562
Action reg: 0.003975
  l1.weight: grad_norm = 0.003949
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.003592
Total gradient norm: 0.022647
=== Actor Training Debug (Iteration 1057) ===
Q mean: -4.800858
Q std: 4.388392
Actor loss: 4.804833
Action reg: 0.003975
  l1.weight: grad_norm = 0.005642
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.005553
Total gradient norm: 0.046035
=== Actor Training Debug (Iteration 1058) ===
Q mean: -5.085440
Q std: 4.541434
Actor loss: 5.089422
Action reg: 0.003981
  l1.weight: grad_norm = 0.008314
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.008020
Total gradient norm: 0.063474
=== Actor Training Debug (Iteration 1059) ===
Q mean: -5.303845
Q std: 4.788387
Actor loss: 5.307812
Action reg: 0.003966
  l1.weight: grad_norm = 0.006216
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.005948
Total gradient norm: 0.047204
=== Actor Training Debug (Iteration 1060) ===
Q mean: -4.855494
Q std: 4.569438
Actor loss: 4.859460
Action reg: 0.003966
  l1.weight: grad_norm = 0.018329
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.015941
Total gradient norm: 0.122722
=== Actor Training Debug (Iteration 1061) ===
Q mean: -5.164233
Q std: 4.571501
Actor loss: 5.168219
Action reg: 0.003985
  l1.weight: grad_norm = 0.001121
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.001153
Total gradient norm: 0.007117
=== Actor Training Debug (Iteration 1062) ===
Q mean: -4.712672
Q std: 4.494205
Actor loss: 4.716653
Action reg: 0.003981
  l1.weight: grad_norm = 0.008769
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.006608
Total gradient norm: 0.029217
=== Actor Training Debug (Iteration 1063) ===
Q mean: -5.626817
Q std: 4.834290
Actor loss: 5.630791
Action reg: 0.003974
  l1.weight: grad_norm = 0.005562
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.005426
Total gradient norm: 0.033505
=== Actor Training Debug (Iteration 1064) ===
Q mean: -5.489100
Q std: 4.583571
Actor loss: 5.493073
Action reg: 0.003974
  l1.weight: grad_norm = 0.014597
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.016346
Total gradient norm: 0.149704
=== Actor Training Debug (Iteration 1065) ===
Q mean: -4.776888
Q std: 4.794009
Actor loss: 4.780876
Action reg: 0.003988
  l1.weight: grad_norm = 0.003087
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.003182
Total gradient norm: 0.014662
=== Actor Training Debug (Iteration 1066) ===
Q mean: -4.706043
Q std: 4.546755
Actor loss: 4.710011
Action reg: 0.003968
  l1.weight: grad_norm = 0.016337
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.017585
Total gradient norm: 0.150444
=== Actor Training Debug (Iteration 1067) ===
Q mean: -5.280486
Q std: 4.522225
Actor loss: 5.284460
Action reg: 0.003974
  l1.weight: grad_norm = 0.012754
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.011052
Total gradient norm: 0.069685
=== Actor Training Debug (Iteration 1068) ===
Q mean: -5.310442
Q std: 4.791376
Actor loss: 5.314424
Action reg: 0.003982
  l1.weight: grad_norm = 0.012304
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.011806
Total gradient norm: 0.076586
=== Actor Training Debug (Iteration 1069) ===
Q mean: -5.018455
Q std: 4.620174
Actor loss: 5.022429
Action reg: 0.003975
  l1.weight: grad_norm = 0.005332
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004923
Total gradient norm: 0.028196
=== Actor Training Debug (Iteration 1070) ===
Q mean: -4.704829
Q std: 4.480471
Actor loss: 4.708822
Action reg: 0.003993
  l1.weight: grad_norm = 0.002853
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002740
Total gradient norm: 0.013351
=== Actor Training Debug (Iteration 1071) ===
Q mean: -4.505878
Q std: 4.265666
Actor loss: 4.509859
Action reg: 0.003980
  l1.weight: grad_norm = 0.005661
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005618
Total gradient norm: 0.043074
=== Actor Training Debug (Iteration 1072) ===
Q mean: -4.674721
Q std: 4.867510
Actor loss: 4.678703
Action reg: 0.003983
  l1.weight: grad_norm = 0.003735
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.004184
Total gradient norm: 0.032697
=== Actor Training Debug (Iteration 1073) ===
Q mean: -5.054341
Q std: 4.737812
Actor loss: 5.058319
Action reg: 0.003978
  l1.weight: grad_norm = 0.007575
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.006392
Total gradient norm: 0.027378
=== Actor Training Debug (Iteration 1074) ===
Q mean: -5.417678
Q std: 4.862331
Actor loss: 5.421665
Action reg: 0.003987
  l1.weight: grad_norm = 0.003921
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.003564
Total gradient norm: 0.021409
=== Actor Training Debug (Iteration 1075) ===
Q mean: -5.307826
Q std: 4.538347
Actor loss: 5.311809
Action reg: 0.003983
  l1.weight: grad_norm = 0.007188
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.006166
Total gradient norm: 0.042755
=== Actor Training Debug (Iteration 1076) ===
Q mean: -4.633339
Q std: 4.059002
Actor loss: 4.637331
Action reg: 0.003992
  l1.weight: grad_norm = 0.006353
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006245
Total gradient norm: 0.042715
=== Actor Training Debug (Iteration 1077) ===
Q mean: -5.398623
Q std: 4.794165
Actor loss: 5.402589
Action reg: 0.003966
  l1.weight: grad_norm = 0.007697
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.006846
Total gradient norm: 0.035306
=== Actor Training Debug (Iteration 1078) ===
Q mean: -5.142118
Q std: 4.926970
Actor loss: 5.146096
Action reg: 0.003978
  l1.weight: grad_norm = 0.002531
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.002454
Total gradient norm: 0.019640
=== Actor Training Debug (Iteration 1079) ===
Q mean: -5.240835
Q std: 5.079320
Actor loss: 5.244819
Action reg: 0.003983
  l1.weight: grad_norm = 0.010024
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.008570
Total gradient norm: 0.058880
=== Actor Training Debug (Iteration 1080) ===
Q mean: -4.959494
Q std: 4.854879
Actor loss: 4.963476
Action reg: 0.003982
  l1.weight: grad_norm = 0.011954
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.009711
Total gradient norm: 0.060274
=== Actor Training Debug (Iteration 1081) ===
Q mean: -4.785571
Q std: 4.676804
Actor loss: 4.789544
Action reg: 0.003972
  l1.weight: grad_norm = 0.007479
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.006243
Total gradient norm: 0.029429
=== Actor Training Debug (Iteration 1082) ===
Q mean: -5.303510
Q std: 4.959122
Actor loss: 5.307498
Action reg: 0.003988
  l1.weight: grad_norm = 0.004176
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.003587
Total gradient norm: 0.020242
=== Actor Training Debug (Iteration 1083) ===
Q mean: -5.396559
Q std: 5.011835
Actor loss: 5.400542
Action reg: 0.003983
  l1.weight: grad_norm = 0.005694
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.005947
Total gradient norm: 0.050886
=== Actor Training Debug (Iteration 1084) ===
Q mean: -4.857391
Q std: 4.513551
Actor loss: 4.861376
Action reg: 0.003985
  l1.weight: grad_norm = 0.003296
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.003544
Total gradient norm: 0.034996
=== Actor Training Debug (Iteration 1085) ===
Q mean: -4.679208
Q std: 4.517862
Actor loss: 4.683192
Action reg: 0.003984
  l1.weight: grad_norm = 0.003974
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003301
Total gradient norm: 0.025651
=== Actor Training Debug (Iteration 1086) ===
Q mean: -5.295055
Q std: 4.717962
Actor loss: 5.299039
Action reg: 0.003983
  l1.weight: grad_norm = 0.000619
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.000470
Total gradient norm: 0.002574
=== Actor Training Debug (Iteration 1087) ===
Q mean: -4.799633
Q std: 4.962670
Actor loss: 4.803623
Action reg: 0.003990
  l1.weight: grad_norm = 0.006365
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.004802
Total gradient norm: 0.021259
=== Actor Training Debug (Iteration 1088) ===
Q mean: -5.030427
Q std: 4.789935
Actor loss: 5.034406
Action reg: 0.003978
  l1.weight: grad_norm = 0.007428
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.008531
Total gradient norm: 0.093013
=== Actor Training Debug (Iteration 1089) ===
Q mean: -4.766255
Q std: 4.605032
Actor loss: 4.770236
Action reg: 0.003980
  l1.weight: grad_norm = 0.006015
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.004946
Total gradient norm: 0.027000
=== Actor Training Debug (Iteration 1090) ===
Q mean: -5.634279
Q std: 4.799072
Actor loss: 5.638249
Action reg: 0.003970
  l1.weight: grad_norm = 0.001727
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.001408
Total gradient norm: 0.006167
=== Actor Training Debug (Iteration 1091) ===
Q mean: -5.466452
Q std: 4.933251
Actor loss: 5.470426
Action reg: 0.003973
  l1.weight: grad_norm = 0.006102
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.006492
Total gradient norm: 0.061836
=== Actor Training Debug (Iteration 1092) ===
Q mean: -5.164403
Q std: 4.798965
Actor loss: 5.168376
Action reg: 0.003973
  l1.weight: grad_norm = 0.005003
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.004411
Total gradient norm: 0.027538
=== Actor Training Debug (Iteration 1093) ===
Q mean: -5.630354
Q std: 5.126276
Actor loss: 5.634344
Action reg: 0.003990
  l1.weight: grad_norm = 0.004646
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004281
Total gradient norm: 0.019869
=== Actor Training Debug (Iteration 1094) ===
Q mean: -5.596067
Q std: 5.014922
Actor loss: 5.600060
Action reg: 0.003992
  l1.weight: grad_norm = 0.004887
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004326
Total gradient norm: 0.033435
=== Actor Training Debug (Iteration 1095) ===
Q mean: -5.301641
Q std: 4.807622
Actor loss: 5.305617
Action reg: 0.003977
  l1.weight: grad_norm = 0.005253
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.005086
Total gradient norm: 0.042164
=== Actor Training Debug (Iteration 1096) ===
Q mean: -5.056649
Q std: 5.272703
Actor loss: 5.060628
Action reg: 0.003979
  l1.weight: grad_norm = 0.004994
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.005206
Total gradient norm: 0.049612
=== Actor Training Debug (Iteration 1097) ===
Q mean: -5.609086
Q std: 5.045058
Actor loss: 5.613069
Action reg: 0.003983
  l1.weight: grad_norm = 0.009121
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.008909
Total gradient norm: 0.052940
=== Actor Training Debug (Iteration 1098) ===
Q mean: -5.406066
Q std: 4.731497
Actor loss: 5.410054
Action reg: 0.003988
  l1.weight: grad_norm = 0.005874
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005883
Total gradient norm: 0.048107
=== Actor Training Debug (Iteration 1099) ===
Q mean: -4.550288
Q std: 4.508865
Actor loss: 4.554277
Action reg: 0.003989
  l1.weight: grad_norm = 0.000658
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.000589
Total gradient norm: 0.002186
=== Actor Training Debug (Iteration 1100) ===
Q mean: -5.257070
Q std: 4.816007
Actor loss: 5.261052
Action reg: 0.003982
  l1.weight: grad_norm = 0.004521
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.003896
Total gradient norm: 0.022983
Episode 61: Steps=100, Reward=-286.473, Buffer_size=6100
=== Actor Training Debug (Iteration 1101) ===
Q mean: -4.919383
Q std: 4.757713
Actor loss: 4.923366
Action reg: 0.003984
  l1.weight: grad_norm = 0.002949
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002802
Total gradient norm: 0.014748
=== Actor Training Debug (Iteration 1102) ===
Q mean: -4.822268
Q std: 4.852066
Actor loss: 4.826235
Action reg: 0.003967
  l1.weight: grad_norm = 0.020685
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.023406
Total gradient norm: 0.260034
=== Actor Training Debug (Iteration 1103) ===
Q mean: -5.826331
Q std: 5.241864
Actor loss: 5.830306
Action reg: 0.003975
  l1.weight: grad_norm = 0.008312
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.008803
Total gradient norm: 0.086550
=== Actor Training Debug (Iteration 1104) ===
Q mean: -4.810176
Q std: 4.715043
Actor loss: 4.814163
Action reg: 0.003987
  l1.weight: grad_norm = 0.002928
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.002634
Total gradient norm: 0.015026
=== Actor Training Debug (Iteration 1105) ===
Q mean: -5.492910
Q std: 4.943409
Actor loss: 5.496887
Action reg: 0.003977
  l1.weight: grad_norm = 0.004966
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.004678
Total gradient norm: 0.029943
=== Actor Training Debug (Iteration 1106) ===
Q mean: -6.015815
Q std: 5.417301
Actor loss: 6.019803
Action reg: 0.003988
  l1.weight: grad_norm = 0.002353
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002508
Total gradient norm: 0.018688
=== Actor Training Debug (Iteration 1107) ===
Q mean: -5.172263
Q std: 4.419341
Actor loss: 5.176251
Action reg: 0.003989
  l1.weight: grad_norm = 0.004594
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004841
Total gradient norm: 0.032005
=== Actor Training Debug (Iteration 1108) ===
Q mean: -5.162609
Q std: 5.118486
Actor loss: 5.166589
Action reg: 0.003980
  l1.weight: grad_norm = 0.006631
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.006953
Total gradient norm: 0.060863
=== Actor Training Debug (Iteration 1109) ===
Q mean: -5.548603
Q std: 5.674203
Actor loss: 5.552590
Action reg: 0.003988
  l1.weight: grad_norm = 0.000826
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.000724
Total gradient norm: 0.003064
=== Actor Training Debug (Iteration 1110) ===
Q mean: -4.904115
Q std: 4.795548
Actor loss: 4.908108
Action reg: 0.003992
  l1.weight: grad_norm = 0.001638
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.001439
Total gradient norm: 0.008829
=== Actor Training Debug (Iteration 1111) ===
Q mean: -5.435884
Q std: 4.964429
Actor loss: 5.439860
Action reg: 0.003976
  l1.weight: grad_norm = 0.005562
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.004259
Total gradient norm: 0.020489
=== Actor Training Debug (Iteration 1112) ===
Q mean: -6.020727
Q std: 5.027462
Actor loss: 6.024713
Action reg: 0.003986
  l1.weight: grad_norm = 0.010854
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.010158
Total gradient norm: 0.060638
=== Actor Training Debug (Iteration 1113) ===
Q mean: -5.597795
Q std: 5.224202
Actor loss: 5.601774
Action reg: 0.003978
  l1.weight: grad_norm = 0.029956
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.030464
Total gradient norm: 0.204089
=== Actor Training Debug (Iteration 1114) ===
Q mean: -4.924774
Q std: 4.770183
Actor loss: 4.928746
Action reg: 0.003972
  l1.weight: grad_norm = 0.006896
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.006072
Total gradient norm: 0.030564
=== Actor Training Debug (Iteration 1115) ===
Q mean: -5.587860
Q std: 5.150974
Actor loss: 5.591843
Action reg: 0.003984
  l1.weight: grad_norm = 0.006700
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.005589
Total gradient norm: 0.027155
=== Actor Training Debug (Iteration 1116) ===
Q mean: -5.363075
Q std: 4.930867
Actor loss: 5.367051
Action reg: 0.003976
  l1.weight: grad_norm = 0.012652
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.011559
Total gradient norm: 0.080938
=== Actor Training Debug (Iteration 1117) ===
Q mean: -5.190032
Q std: 4.660996
Actor loss: 5.194016
Action reg: 0.003983
  l1.weight: grad_norm = 0.005909
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.005620
Total gradient norm: 0.049606
=== Actor Training Debug (Iteration 1118) ===
Q mean: -5.519545
Q std: 4.788915
Actor loss: 5.523532
Action reg: 0.003988
  l1.weight: grad_norm = 0.008553
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.006699
Total gradient norm: 0.048465
=== Actor Training Debug (Iteration 1119) ===
Q mean: -5.361023
Q std: 4.843120
Actor loss: 5.364997
Action reg: 0.003974
  l1.weight: grad_norm = 0.009014
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.008526
Total gradient norm: 0.050187
=== Actor Training Debug (Iteration 1120) ===
Q mean: -5.287611
Q std: 4.914203
Actor loss: 5.291598
Action reg: 0.003987
  l1.weight: grad_norm = 0.006159
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.005973
Total gradient norm: 0.048159
=== Actor Training Debug (Iteration 1121) ===
Q mean: -5.490213
Q std: 5.274931
Actor loss: 5.494194
Action reg: 0.003980
  l1.weight: grad_norm = 0.006139
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.005431
Total gradient norm: 0.031949
=== Actor Training Debug (Iteration 1122) ===
Q mean: -5.609160
Q std: 5.027564
Actor loss: 5.613154
Action reg: 0.003994
  l1.weight: grad_norm = 0.002341
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001943
Total gradient norm: 0.009180
=== Actor Training Debug (Iteration 1123) ===
Q mean: -5.164200
Q std: 5.165960
Actor loss: 5.168181
Action reg: 0.003981
  l1.weight: grad_norm = 0.004992
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004432
Total gradient norm: 0.019287
=== Actor Training Debug (Iteration 1124) ===
Q mean: -4.691640
Q std: 4.650935
Actor loss: 4.695623
Action reg: 0.003982
  l1.weight: grad_norm = 0.006529
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.005989
Total gradient norm: 0.034070
=== Actor Training Debug (Iteration 1125) ===
Q mean: -4.712642
Q std: 4.456764
Actor loss: 4.716618
Action reg: 0.003976
  l1.weight: grad_norm = 0.007988
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.007324
Total gradient norm: 0.040849
=== Actor Training Debug (Iteration 1126) ===
Q mean: -5.539756
Q std: 5.339005
Actor loss: 5.543736
Action reg: 0.003979
  l1.weight: grad_norm = 0.013206
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.011806
Total gradient norm: 0.090463
=== Actor Training Debug (Iteration 1127) ===
Q mean: -5.650994
Q std: 5.048982
Actor loss: 5.654964
Action reg: 0.003970
  l1.weight: grad_norm = 0.011722
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.011992
Total gradient norm: 0.122300
=== Actor Training Debug (Iteration 1128) ===
Q mean: -5.518594
Q std: 4.809192
Actor loss: 5.522587
Action reg: 0.003993
  l1.weight: grad_norm = 0.002996
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002655
Total gradient norm: 0.012644
=== Actor Training Debug (Iteration 1129) ===
Q mean: -4.856001
Q std: 4.475429
Actor loss: 4.859998
Action reg: 0.003996
  l1.weight: grad_norm = 0.006043
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.006148
Total gradient norm: 0.034094
=== Actor Training Debug (Iteration 1130) ===
Q mean: -4.711164
Q std: 4.373474
Actor loss: 4.715138
Action reg: 0.003974
  l1.weight: grad_norm = 0.006630
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.005184
Total gradient norm: 0.022217
=== Actor Training Debug (Iteration 1131) ===
Q mean: -5.809133
Q std: 4.921632
Actor loss: 5.813110
Action reg: 0.003978
  l1.weight: grad_norm = 0.006186
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.006366
Total gradient norm: 0.056663
=== Actor Training Debug (Iteration 1132) ===
Q mean: -5.246402
Q std: 4.882451
Actor loss: 5.250384
Action reg: 0.003983
  l1.weight: grad_norm = 0.002876
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002899
Total gradient norm: 0.015534
=== Actor Training Debug (Iteration 1133) ===
Q mean: -5.296572
Q std: 4.721975
Actor loss: 5.300559
Action reg: 0.003987
  l1.weight: grad_norm = 0.005818
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.005083
Total gradient norm: 0.024324
=== Actor Training Debug (Iteration 1134) ===
Q mean: -5.842835
Q std: 5.576567
Actor loss: 5.846823
Action reg: 0.003987
  l1.weight: grad_norm = 0.007371
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.006852
Total gradient norm: 0.052195
=== Actor Training Debug (Iteration 1135) ===
Q mean: -5.081675
Q std: 5.224782
Actor loss: 5.085641
Action reg: 0.003966
  l1.weight: grad_norm = 0.002449
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.002349
Total gradient norm: 0.016683
=== Actor Training Debug (Iteration 1136) ===
Q mean: -5.363593
Q std: 5.316984
Actor loss: 5.367583
Action reg: 0.003991
  l1.weight: grad_norm = 0.000910
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.000843
Total gradient norm: 0.005778
=== Actor Training Debug (Iteration 1137) ===
Q mean: -5.284785
Q std: 4.857600
Actor loss: 5.288764
Action reg: 0.003979
  l1.weight: grad_norm = 0.005223
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004546
Total gradient norm: 0.022321
=== Actor Training Debug (Iteration 1138) ===
Q mean: -5.104256
Q std: 4.763688
Actor loss: 5.108227
Action reg: 0.003971
  l1.weight: grad_norm = 0.001862
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.001833
Total gradient norm: 0.008747
=== Actor Training Debug (Iteration 1139) ===
Q mean: -5.255366
Q std: 5.041348
Actor loss: 5.259353
Action reg: 0.003987
  l1.weight: grad_norm = 0.005666
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.004995
Total gradient norm: 0.034286
=== Actor Training Debug (Iteration 1140) ===
Q mean: -5.206182
Q std: 4.951370
Actor loss: 5.210162
Action reg: 0.003980
  l1.weight: grad_norm = 0.004337
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.003472
Total gradient norm: 0.014761
=== Actor Training Debug (Iteration 1141) ===
Q mean: -5.305296
Q std: 4.478723
Actor loss: 5.309274
Action reg: 0.003977
  l1.weight: grad_norm = 0.002717
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.002064
Total gradient norm: 0.009684
=== Actor Training Debug (Iteration 1142) ===
Q mean: -5.738973
Q std: 4.779041
Actor loss: 5.742958
Action reg: 0.003984
  l1.weight: grad_norm = 0.005087
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003960
Total gradient norm: 0.022479
=== Actor Training Debug (Iteration 1143) ===
Q mean: -4.880246
Q std: 4.682710
Actor loss: 4.884228
Action reg: 0.003982
  l1.weight: grad_norm = 0.009066
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.007634
Total gradient norm: 0.049257
=== Actor Training Debug (Iteration 1144) ===
Q mean: -5.433555
Q std: 5.313686
Actor loss: 5.437538
Action reg: 0.003983
  l1.weight: grad_norm = 0.016365
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.013750
Total gradient norm: 0.119779
=== Actor Training Debug (Iteration 1145) ===
Q mean: -5.155408
Q std: 5.194839
Actor loss: 5.159400
Action reg: 0.003992
  l1.weight: grad_norm = 0.002795
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.002551
Total gradient norm: 0.012486
=== Actor Training Debug (Iteration 1146) ===
Q mean: -5.768592
Q std: 5.565044
Actor loss: 5.772581
Action reg: 0.003989
  l1.weight: grad_norm = 0.006567
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.005363
Total gradient norm: 0.018271
=== Actor Training Debug (Iteration 1147) ===
Q mean: -5.767016
Q std: 5.350935
Actor loss: 5.771006
Action reg: 0.003990
  l1.weight: grad_norm = 0.004730
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.004560
Total gradient norm: 0.033805
=== Actor Training Debug (Iteration 1148) ===
Q mean: -5.587626
Q std: 5.173623
Actor loss: 5.591605
Action reg: 0.003978
  l1.weight: grad_norm = 0.002957
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.002702
Total gradient norm: 0.014863
=== Actor Training Debug (Iteration 1149) ===
Q mean: -4.651056
Q std: 4.715647
Actor loss: 4.655031
Action reg: 0.003975
  l1.weight: grad_norm = 0.006760
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.006681
Total gradient norm: 0.053994
=== Actor Training Debug (Iteration 1150) ===
Q mean: -5.513079
Q std: 5.188773
Actor loss: 5.517058
Action reg: 0.003979
  l1.weight: grad_norm = 0.007309
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.005965
Total gradient norm: 0.031704
=== Actor Training Debug (Iteration 1151) ===
Q mean: -5.286882
Q std: 5.001420
Actor loss: 5.290852
Action reg: 0.003970
  l1.weight: grad_norm = 0.003843
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.003428
Total gradient norm: 0.016435
=== Actor Training Debug (Iteration 1152) ===
Q mean: -5.314518
Q std: 4.861975
Actor loss: 5.318509
Action reg: 0.003991
  l1.weight: grad_norm = 0.002854
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002399
Total gradient norm: 0.013151
=== Actor Training Debug (Iteration 1153) ===
Q mean: -4.896512
Q std: 4.843327
Actor loss: 4.900494
Action reg: 0.003982
  l1.weight: grad_norm = 0.000986
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.000881
Total gradient norm: 0.003742
=== Actor Training Debug (Iteration 1154) ===
Q mean: -5.440937
Q std: 4.903063
Actor loss: 5.444922
Action reg: 0.003985
  l1.weight: grad_norm = 0.016320
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.012534
Total gradient norm: 0.065428
=== Actor Training Debug (Iteration 1155) ===
Q mean: -5.347578
Q std: 4.768952
Actor loss: 5.351559
Action reg: 0.003981
  l1.weight: grad_norm = 0.011871
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.009715
Total gradient norm: 0.049062
=== Actor Training Debug (Iteration 1156) ===
Q mean: -5.496283
Q std: 5.093842
Actor loss: 5.500270
Action reg: 0.003987
  l1.weight: grad_norm = 0.002230
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.001841
Total gradient norm: 0.010619
=== Actor Training Debug (Iteration 1157) ===
Q mean: -5.299196
Q std: 5.324087
Actor loss: 5.303183
Action reg: 0.003987
  l1.weight: grad_norm = 0.004592
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.003733
Total gradient norm: 0.024918
=== Actor Training Debug (Iteration 1158) ===
Q mean: -5.439255
Q std: 5.362389
Actor loss: 5.443232
Action reg: 0.003977
  l1.weight: grad_norm = 0.008578
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.008072
Total gradient norm: 0.067605
=== Actor Training Debug (Iteration 1159) ===
Q mean: -5.288909
Q std: 5.142132
Actor loss: 5.292901
Action reg: 0.003992
  l1.weight: grad_norm = 0.006793
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006238
Total gradient norm: 0.035321
=== Actor Training Debug (Iteration 1160) ===
Q mean: -5.168621
Q std: 4.731682
Actor loss: 5.172606
Action reg: 0.003985
  l1.weight: grad_norm = 0.006829
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.006941
Total gradient norm: 0.055045
=== Actor Training Debug (Iteration 1161) ===
Q mean: -5.092356
Q std: 4.965063
Actor loss: 5.096343
Action reg: 0.003987
  l1.weight: grad_norm = 0.001345
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.001313
Total gradient norm: 0.006821
=== Actor Training Debug (Iteration 1162) ===
Q mean: -5.611554
Q std: 5.269650
Actor loss: 5.615525
Action reg: 0.003971
  l1.weight: grad_norm = 0.005976
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.004649
Total gradient norm: 0.021555
=== Actor Training Debug (Iteration 1163) ===
Q mean: -6.122818
Q std: 5.584289
Actor loss: 6.126800
Action reg: 0.003981
  l1.weight: grad_norm = 0.001082
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.001106
Total gradient norm: 0.005386
=== Actor Training Debug (Iteration 1164) ===
Q mean: -5.614899
Q std: 5.534544
Actor loss: 5.618868
Action reg: 0.003969
  l1.weight: grad_norm = 0.007371
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.005332
Total gradient norm: 0.020675
=== Actor Training Debug (Iteration 1165) ===
Q mean: -5.449411
Q std: 5.424307
Actor loss: 5.453388
Action reg: 0.003977
  l1.weight: grad_norm = 0.006493
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.006136
Total gradient norm: 0.039213
=== Actor Training Debug (Iteration 1166) ===
Q mean: -5.430320
Q std: 5.358653
Actor loss: 5.434297
Action reg: 0.003977
  l1.weight: grad_norm = 0.007771
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.006828
Total gradient norm: 0.033533
=== Actor Training Debug (Iteration 1167) ===
Q mean: -5.547477
Q std: 5.226677
Actor loss: 5.551455
Action reg: 0.003979
  l1.weight: grad_norm = 0.009280
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.007950
Total gradient norm: 0.037614
=== Actor Training Debug (Iteration 1168) ===
Q mean: -5.416606
Q std: 4.958543
Actor loss: 5.420593
Action reg: 0.003987
  l1.weight: grad_norm = 0.001151
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001037
Total gradient norm: 0.004316
=== Actor Training Debug (Iteration 1169) ===
Q mean: -4.795074
Q std: 4.848771
Actor loss: 4.799050
Action reg: 0.003977
  l1.weight: grad_norm = 0.007780
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.007240
Total gradient norm: 0.041807
=== Actor Training Debug (Iteration 1170) ===
Q mean: -5.156612
Q std: 4.930189
Actor loss: 5.160594
Action reg: 0.003982
  l1.weight: grad_norm = 0.004711
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.004148
Total gradient norm: 0.027209
=== Actor Training Debug (Iteration 1171) ===
Q mean: -5.220042
Q std: 4.914663
Actor loss: 5.224033
Action reg: 0.003990
  l1.weight: grad_norm = 0.003583
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.002661
Total gradient norm: 0.013204
=== Actor Training Debug (Iteration 1172) ===
Q mean: -5.441227
Q std: 5.415848
Actor loss: 5.445220
Action reg: 0.003993
  l1.weight: grad_norm = 0.000576
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000554
Total gradient norm: 0.003131
=== Actor Training Debug (Iteration 1173) ===
Q mean: -5.688151
Q std: 5.362785
Actor loss: 5.692139
Action reg: 0.003988
  l1.weight: grad_norm = 0.007753
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005872
Total gradient norm: 0.033756
=== Actor Training Debug (Iteration 1174) ===
Q mean: -4.901374
Q std: 5.459190
Actor loss: 4.905356
Action reg: 0.003983
  l1.weight: grad_norm = 0.006929
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004999
Total gradient norm: 0.020735
=== Actor Training Debug (Iteration 1175) ===
Q mean: -5.167785
Q std: 5.570136
Actor loss: 5.171770
Action reg: 0.003985
  l1.weight: grad_norm = 0.008809
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.007481
Total gradient norm: 0.046940
=== Actor Training Debug (Iteration 1176) ===
Q mean: -5.329712
Q std: 5.248042
Actor loss: 5.333694
Action reg: 0.003981
  l1.weight: grad_norm = 0.013480
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.011224
Total gradient norm: 0.043193
=== Actor Training Debug (Iteration 1177) ===
Q mean: -5.640666
Q std: 5.574512
Actor loss: 5.644650
Action reg: 0.003984
  l1.weight: grad_norm = 0.005981
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.005438
Total gradient norm: 0.034606
=== Actor Training Debug (Iteration 1178) ===
Q mean: -5.878409
Q std: 5.276585
Actor loss: 5.882402
Action reg: 0.003992
  l1.weight: grad_norm = 0.006872
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005672
Total gradient norm: 0.033697
=== Actor Training Debug (Iteration 1179) ===
Q mean: -5.244252
Q std: 4.992261
Actor loss: 5.248234
Action reg: 0.003982
  l1.weight: grad_norm = 0.005740
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.004906
Total gradient norm: 0.032906
=== Actor Training Debug (Iteration 1180) ===
Q mean: -5.516015
Q std: 5.343522
Actor loss: 5.520003
Action reg: 0.003988
  l1.weight: grad_norm = 0.001954
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.001691
Total gradient norm: 0.007894
=== Actor Training Debug (Iteration 1181) ===
Q mean: -5.066040
Q std: 4.850985
Actor loss: 5.070018
Action reg: 0.003979
  l1.weight: grad_norm = 0.004289
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.003544
Total gradient norm: 0.019093
=== Actor Training Debug (Iteration 1182) ===
Q mean: -5.175294
Q std: 5.462824
Actor loss: 5.179273
Action reg: 0.003979
  l1.weight: grad_norm = 0.010393
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.009115
Total gradient norm: 0.054887
=== Actor Training Debug (Iteration 1183) ===
Q mean: -5.749310
Q std: 5.307700
Actor loss: 5.753306
Action reg: 0.003995
  l1.weight: grad_norm = 0.009062
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.009151
Total gradient norm: 0.053525
=== Actor Training Debug (Iteration 1184) ===
Q mean: -5.417279
Q std: 5.140681
Actor loss: 5.421265
Action reg: 0.003986
  l1.weight: grad_norm = 0.005055
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.005004
Total gradient norm: 0.028872
=== Actor Training Debug (Iteration 1185) ===
Q mean: -5.058278
Q std: 5.181017
Actor loss: 5.062263
Action reg: 0.003985
  l1.weight: grad_norm = 0.002016
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.001733
Total gradient norm: 0.008216
=== Actor Training Debug (Iteration 1186) ===
Q mean: -4.590643
Q std: 4.720253
Actor loss: 4.594626
Action reg: 0.003984
  l1.weight: grad_norm = 0.006078
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.005666
Total gradient norm: 0.026901
=== Actor Training Debug (Iteration 1187) ===
Q mean: -6.349308
Q std: 5.638696
Actor loss: 6.353297
Action reg: 0.003989
  l1.weight: grad_norm = 0.004659
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.004612
Total gradient norm: 0.024827
=== Actor Training Debug (Iteration 1188) ===
Q mean: -5.727136
Q std: 4.996671
Actor loss: 5.731110
Action reg: 0.003973
  l1.weight: grad_norm = 0.008629
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.007271
Total gradient norm: 0.034181
=== Actor Training Debug (Iteration 1189) ===
Q mean: -5.644245
Q std: 5.574809
Actor loss: 5.648238
Action reg: 0.003993
  l1.weight: grad_norm = 0.003071
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.002932
Total gradient norm: 0.021021
=== Actor Training Debug (Iteration 1190) ===
Q mean: -5.335899
Q std: 5.248235
Actor loss: 5.339881
Action reg: 0.003982
  l1.weight: grad_norm = 0.003672
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.004779
Total gradient norm: 0.036303
=== Actor Training Debug (Iteration 1191) ===
Q mean: -5.486772
Q std: 5.227801
Actor loss: 5.490765
Action reg: 0.003994
  l1.weight: grad_norm = 0.000885
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.000868
Total gradient norm: 0.004160
=== Actor Training Debug (Iteration 1192) ===
Q mean: -5.882206
Q std: 5.454981
Actor loss: 5.886184
Action reg: 0.003978
  l1.weight: grad_norm = 0.009251
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.009326
Total gradient norm: 0.056669
=== Actor Training Debug (Iteration 1193) ===
Q mean: -5.918238
Q std: 5.615612
Actor loss: 5.922232
Action reg: 0.003994
  l1.weight: grad_norm = 0.001904
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001669
Total gradient norm: 0.009274
=== Actor Training Debug (Iteration 1194) ===
Q mean: -4.960104
Q std: 5.177635
Actor loss: 4.964093
Action reg: 0.003989
  l1.weight: grad_norm = 0.008215
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.006871
Total gradient norm: 0.030148
=== Actor Training Debug (Iteration 1195) ===
Q mean: -4.928743
Q std: 4.715339
Actor loss: 4.932734
Action reg: 0.003991
  l1.weight: grad_norm = 0.010831
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.008369
Total gradient norm: 0.033149
=== Actor Training Debug (Iteration 1196) ===
Q mean: -5.344415
Q std: 5.342949
Actor loss: 5.348410
Action reg: 0.003995
  l1.weight: grad_norm = 0.003115
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002987
Total gradient norm: 0.027186
=== Actor Training Debug (Iteration 1197) ===
Q mean: -6.213043
Q std: 5.403608
Actor loss: 6.217032
Action reg: 0.003989
  l1.weight: grad_norm = 0.003678
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.002987
Total gradient norm: 0.012483
=== Actor Training Debug (Iteration 1198) ===
Q mean: -6.193493
Q std: 5.437536
Actor loss: 6.197475
Action reg: 0.003982
  l1.weight: grad_norm = 0.014596
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.010907
Total gradient norm: 0.057151
=== Actor Training Debug (Iteration 1199) ===
Q mean: -5.110272
Q std: 4.942698
Actor loss: 5.114254
Action reg: 0.003981
  l1.weight: grad_norm = 0.009414
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.008288
Total gradient norm: 0.037536
=== Actor Training Debug (Iteration 1200) ===
Q mean: -5.480511
Q std: 5.597207
Actor loss: 5.484501
Action reg: 0.003990
  l1.weight: grad_norm = 0.001443
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001318
Total gradient norm: 0.008026
=== Actor Training Debug (Iteration 1201) ===
Q mean: -5.453965
Q std: 5.389527
Actor loss: 5.457963
Action reg: 0.003998
  l1.weight: grad_norm = 0.001172
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.000967
Total gradient norm: 0.004460
=== Actor Training Debug (Iteration 1202) ===
Q mean: -5.208479
Q std: 5.043703
Actor loss: 5.212461
Action reg: 0.003982
  l1.weight: grad_norm = 0.012529
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.011308
Total gradient norm: 0.060242
=== Actor Training Debug (Iteration 1203) ===
Q mean: -5.703396
Q std: 4.920324
Actor loss: 5.707391
Action reg: 0.003995
  l1.weight: grad_norm = 0.001226
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001007
Total gradient norm: 0.003624
=== Actor Training Debug (Iteration 1204) ===
Q mean: -6.474373
Q std: 5.432497
Actor loss: 6.478360
Action reg: 0.003987
  l1.weight: grad_norm = 0.001118
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001047
Total gradient norm: 0.005167
=== Actor Training Debug (Iteration 1205) ===
Q mean: -5.493902
Q std: 5.189737
Actor loss: 5.497892
Action reg: 0.003990
  l1.weight: grad_norm = 0.006427
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.006307
Total gradient norm: 0.052108
=== Actor Training Debug (Iteration 1206) ===
Q mean: -5.784864
Q std: 5.153379
Actor loss: 5.788836
Action reg: 0.003972
  l1.weight: grad_norm = 0.003732
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.002864
Total gradient norm: 0.015471
=== Actor Training Debug (Iteration 1207) ===
Q mean: -5.248004
Q std: 4.901766
Actor loss: 5.251977
Action reg: 0.003973
  l1.weight: grad_norm = 0.004425
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.003569
Total gradient norm: 0.021140
=== Actor Training Debug (Iteration 1208) ===
Q mean: -5.906053
Q std: 5.999836
Actor loss: 5.910035
Action reg: 0.003982
  l1.weight: grad_norm = 0.007905
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.006411
Total gradient norm: 0.026365
=== Actor Training Debug (Iteration 1209) ===
Q mean: -6.133248
Q std: 5.728855
Actor loss: 6.137235
Action reg: 0.003987
  l1.weight: grad_norm = 0.001593
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.001360
Total gradient norm: 0.006447
=== Actor Training Debug (Iteration 1210) ===
Q mean: -5.407536
Q std: 5.193172
Actor loss: 5.411503
Action reg: 0.003968
  l1.weight: grad_norm = 0.006283
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.005758
Total gradient norm: 0.038127
=== Actor Training Debug (Iteration 1211) ===
Q mean: -5.878139
Q std: 5.462865
Actor loss: 5.882120
Action reg: 0.003981
  l1.weight: grad_norm = 0.006974
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.006999
Total gradient norm: 0.060162
=== Actor Training Debug (Iteration 1212) ===
Q mean: -5.546879
Q std: 4.970214
Actor loss: 5.550870
Action reg: 0.003991
  l1.weight: grad_norm = 0.007401
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.007128
Total gradient norm: 0.050451
=== Actor Training Debug (Iteration 1213) ===
Q mean: -5.563211
Q std: 5.104990
Actor loss: 5.567199
Action reg: 0.003988
  l1.weight: grad_norm = 0.001121
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.001026
Total gradient norm: 0.004086
=== Actor Training Debug (Iteration 1214) ===
Q mean: -6.289750
Q std: 5.663034
Actor loss: 6.293726
Action reg: 0.003976
  l1.weight: grad_norm = 0.004784
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.004484
Total gradient norm: 0.035519
=== Actor Training Debug (Iteration 1215) ===
Q mean: -5.715524
Q std: 5.229395
Actor loss: 5.719500
Action reg: 0.003976
  l1.weight: grad_norm = 0.001540
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.001436
Total gradient norm: 0.007483
=== Actor Training Debug (Iteration 1216) ===
Q mean: -5.280807
Q std: 4.996627
Actor loss: 5.284800
Action reg: 0.003993
  l1.weight: grad_norm = 0.005121
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.004252
Total gradient norm: 0.029513
=== Actor Training Debug (Iteration 1217) ===
Q mean: -6.108545
Q std: 5.976369
Actor loss: 6.112527
Action reg: 0.003982
  l1.weight: grad_norm = 0.011541
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.008702
Total gradient norm: 0.031801
=== Actor Training Debug (Iteration 1218) ===
Q mean: -5.410365
Q std: 5.490082
Actor loss: 5.414340
Action reg: 0.003975
  l1.weight: grad_norm = 0.005784
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.005370
Total gradient norm: 0.023186
=== Actor Training Debug (Iteration 1219) ===
Q mean: -4.714476
Q std: 4.755681
Actor loss: 4.718446
Action reg: 0.003971
  l1.weight: grad_norm = 0.009857
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.008339
Total gradient norm: 0.050209
=== Actor Training Debug (Iteration 1220) ===
Q mean: -5.576865
Q std: 4.967306
Actor loss: 5.580846
Action reg: 0.003981
  l1.weight: grad_norm = 0.020022
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.019092
Total gradient norm: 0.165078
=== Actor Training Debug (Iteration 1221) ===
Q mean: -5.841160
Q std: 5.779119
Actor loss: 5.845135
Action reg: 0.003975
  l1.weight: grad_norm = 0.001832
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.001853
Total gradient norm: 0.009685
=== Actor Training Debug (Iteration 1222) ===
Q mean: -4.741568
Q std: 4.873764
Actor loss: 4.745554
Action reg: 0.003987
  l1.weight: grad_norm = 0.008330
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.006678
Total gradient norm: 0.037170
=== Actor Training Debug (Iteration 1223) ===
Q mean: -5.378411
Q std: 5.389979
Actor loss: 5.382378
Action reg: 0.003966
  l1.weight: grad_norm = 0.008714
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.007867
Total gradient norm: 0.051780
=== Actor Training Debug (Iteration 1224) ===
Q mean: -5.281323
Q std: 4.952632
Actor loss: 5.285311
Action reg: 0.003988
  l1.weight: grad_norm = 0.004766
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.003939
Total gradient norm: 0.014474
=== Actor Training Debug (Iteration 1225) ===
Q mean: -5.454305
Q std: 5.004492
Actor loss: 5.458296
Action reg: 0.003991
  l1.weight: grad_norm = 0.004146
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.003346
Total gradient norm: 0.013526
=== Actor Training Debug (Iteration 1226) ===
Q mean: -5.521750
Q std: 5.472345
Actor loss: 5.525726
Action reg: 0.003975
  l1.weight: grad_norm = 0.004396
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.004082
Total gradient norm: 0.030528
=== Actor Training Debug (Iteration 1227) ===
Q mean: -5.987294
Q std: 5.415840
Actor loss: 5.991277
Action reg: 0.003983
  l1.weight: grad_norm = 0.001413
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001352
Total gradient norm: 0.009371
=== Actor Training Debug (Iteration 1228) ===
Q mean: -5.771085
Q std: 5.860369
Actor loss: 5.775064
Action reg: 0.003979
  l1.weight: grad_norm = 0.011311
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.010050
Total gradient norm: 0.048874
=== Actor Training Debug (Iteration 1229) ===
Q mean: -5.478441
Q std: 5.470417
Actor loss: 5.482429
Action reg: 0.003988
  l1.weight: grad_norm = 0.005107
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004147
Total gradient norm: 0.015995
=== Actor Training Debug (Iteration 1230) ===
Q mean: -5.297143
Q std: 5.568538
Actor loss: 5.301118
Action reg: 0.003976
  l1.weight: grad_norm = 0.000805
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.000710
Total gradient norm: 0.006221
=== Actor Training Debug (Iteration 1231) ===
Q mean: -5.524723
Q std: 5.475492
Actor loss: 5.528703
Action reg: 0.003980
  l1.weight: grad_norm = 0.006390
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.005624
Total gradient norm: 0.039592
=== Actor Training Debug (Iteration 1232) ===
Q mean: -5.766604
Q std: 5.311134
Actor loss: 5.770582
Action reg: 0.003978
  l1.weight: grad_norm = 0.003447
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.003565
Total gradient norm: 0.027676
=== Actor Training Debug (Iteration 1233) ===
Q mean: -5.857719
Q std: 5.555459
Actor loss: 5.861713
Action reg: 0.003994
  l1.weight: grad_norm = 0.005242
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004735
Total gradient norm: 0.026962
=== Actor Training Debug (Iteration 1234) ===
Q mean: -5.814521
Q std: 5.346962
Actor loss: 5.818513
Action reg: 0.003992
  l1.weight: grad_norm = 0.005905
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005441
Total gradient norm: 0.032207
=== Actor Training Debug (Iteration 1235) ===
Q mean: -5.895713
Q std: 5.428904
Actor loss: 5.899704
Action reg: 0.003992
  l1.weight: grad_norm = 0.014365
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.012061
Total gradient norm: 0.052451
=== Actor Training Debug (Iteration 1236) ===
Q mean: -5.938227
Q std: 5.530914
Actor loss: 5.942217
Action reg: 0.003990
  l1.weight: grad_norm = 0.003873
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003441
Total gradient norm: 0.018355
=== Actor Training Debug (Iteration 1237) ===
Q mean: -5.286314
Q std: 5.128530
Actor loss: 5.290297
Action reg: 0.003983
  l1.weight: grad_norm = 0.013246
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.011073
Total gradient norm: 0.051122
=== Actor Training Debug (Iteration 1238) ===
Q mean: -5.543526
Q std: 4.887417
Actor loss: 5.547506
Action reg: 0.003980
  l1.weight: grad_norm = 0.008677
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.008185
Total gradient norm: 0.062441
=== Actor Training Debug (Iteration 1239) ===
Q mean: -6.697922
Q std: 6.040528
Actor loss: 6.701918
Action reg: 0.003996
  l1.weight: grad_norm = 0.003505
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.003243
Total gradient norm: 0.026696
=== Actor Training Debug (Iteration 1240) ===
Q mean: -5.459425
Q std: 5.145432
Actor loss: 5.463414
Action reg: 0.003989
  l1.weight: grad_norm = 0.008000
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.007140
Total gradient norm: 0.051721
=== Actor Training Debug (Iteration 1241) ===
Q mean: -6.028877
Q std: 5.748174
Actor loss: 6.032859
Action reg: 0.003983
  l1.weight: grad_norm = 0.015106
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.014424
Total gradient norm: 0.124243
=== Actor Training Debug (Iteration 1242) ===
Q mean: -5.883470
Q std: 5.959483
Actor loss: 5.887455
Action reg: 0.003985
  l1.weight: grad_norm = 0.005415
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.004243
Total gradient norm: 0.019214
=== Actor Training Debug (Iteration 1243) ===
Q mean: -5.477865
Q std: 5.539564
Actor loss: 5.481854
Action reg: 0.003989
  l1.weight: grad_norm = 0.005745
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.004406
Total gradient norm: 0.025697
=== Actor Training Debug (Iteration 1244) ===
Q mean: -5.163805
Q std: 4.956916
Actor loss: 5.167784
Action reg: 0.003978
  l1.weight: grad_norm = 0.001300
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.001049
Total gradient norm: 0.004432
=== Actor Training Debug (Iteration 1245) ===
Q mean: -5.775277
Q std: 5.477424
Actor loss: 5.779266
Action reg: 0.003989
  l1.weight: grad_norm = 0.008798
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.006516
Total gradient norm: 0.039702
=== Actor Training Debug (Iteration 1246) ===
Q mean: -5.427184
Q std: 5.400717
Actor loss: 5.431166
Action reg: 0.003983
  l1.weight: grad_norm = 0.005130
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.004404
Total gradient norm: 0.018382
=== Actor Training Debug (Iteration 1247) ===
Q mean: -5.830375
Q std: 5.696359
Actor loss: 5.834352
Action reg: 0.003977
  l1.weight: grad_norm = 0.003936
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.003358
Total gradient norm: 0.022402
=== Actor Training Debug (Iteration 1248) ===
Q mean: -5.787946
Q std: 5.718152
Actor loss: 5.791924
Action reg: 0.003979
  l1.weight: grad_norm = 0.020273
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.015765
Total gradient norm: 0.072090
=== Actor Training Debug (Iteration 1249) ===
Q mean: -5.684502
Q std: 5.630267
Actor loss: 5.688488
Action reg: 0.003987
  l1.weight: grad_norm = 0.006504
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005503
Total gradient norm: 0.040456
=== Actor Training Debug (Iteration 1250) ===
Q mean: -6.407187
Q std: 5.837614
Actor loss: 6.411171
Action reg: 0.003984
  l1.weight: grad_norm = 0.010277
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.007700
Total gradient norm: 0.033205
=== Actor Training Debug (Iteration 1251) ===
Q mean: -5.374188
Q std: 4.838536
Actor loss: 5.378169
Action reg: 0.003981
  l1.weight: grad_norm = 0.007903
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.005965
Total gradient norm: 0.024494
=== Actor Training Debug (Iteration 1252) ===
Q mean: -4.923827
Q std: 5.283448
Actor loss: 4.927804
Action reg: 0.003977
  l1.weight: grad_norm = 0.002304
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.001941
Total gradient norm: 0.010784
=== Actor Training Debug (Iteration 1253) ===
Q mean: -5.240721
Q std: 5.576611
Actor loss: 5.244708
Action reg: 0.003986
  l1.weight: grad_norm = 0.002250
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.001860
Total gradient norm: 0.011595
=== Actor Training Debug (Iteration 1254) ===
Q mean: -5.347809
Q std: 5.177408
Actor loss: 5.351791
Action reg: 0.003982
  l1.weight: grad_norm = 0.005174
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.004670
Total gradient norm: 0.031094
=== Actor Training Debug (Iteration 1255) ===
Q mean: -5.618863
Q std: 5.474911
Actor loss: 5.622861
Action reg: 0.003998
  l1.weight: grad_norm = 0.007514
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.007008
Total gradient norm: 0.040166
=== Actor Training Debug (Iteration 1256) ===
Q mean: -5.689550
Q std: 5.760021
Actor loss: 5.693530
Action reg: 0.003980
  l1.weight: grad_norm = 0.005685
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.004457
Total gradient norm: 0.030044
=== Actor Training Debug (Iteration 1257) ===
Q mean: -5.996739
Q std: 5.705452
Actor loss: 6.000723
Action reg: 0.003984
  l1.weight: grad_norm = 0.003154
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.002641
Total gradient norm: 0.017298
=== Actor Training Debug (Iteration 1258) ===
Q mean: -5.848262
Q std: 5.416615
Actor loss: 5.852252
Action reg: 0.003989
  l1.weight: grad_norm = 0.003418
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.003093
Total gradient norm: 0.009689
=== Actor Training Debug (Iteration 1259) ===
Q mean: -5.777394
Q std: 5.453094
Actor loss: 5.781368
Action reg: 0.003974
  l1.weight: grad_norm = 0.007925
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.005320
Total gradient norm: 0.018674
=== Actor Training Debug (Iteration 1260) ===
Q mean: -6.029722
Q std: 5.739498
Actor loss: 6.033704
Action reg: 0.003982
  l1.weight: grad_norm = 0.006230
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.005253
Total gradient norm: 0.027374
=== Actor Training Debug (Iteration 1261) ===
Q mean: -5.613839
Q std: 5.403727
Actor loss: 5.617831
Action reg: 0.003992
  l1.weight: grad_norm = 0.002037
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001754
Total gradient norm: 0.008603
=== Actor Training Debug (Iteration 1262) ===
Q mean: -5.649523
Q std: 5.784861
Actor loss: 5.653509
Action reg: 0.003986
  l1.weight: grad_norm = 0.004375
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.004364
Total gradient norm: 0.038671
=== Actor Training Debug (Iteration 1263) ===
Q mean: -5.662416
Q std: 5.889434
Actor loss: 5.666401
Action reg: 0.003986
  l1.weight: grad_norm = 0.005331
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.005169
Total gradient norm: 0.046369
=== Actor Training Debug (Iteration 1264) ===
Q mean: -5.447214
Q std: 5.695685
Actor loss: 5.451197
Action reg: 0.003982
  l1.weight: grad_norm = 0.003364
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.003102
Total gradient norm: 0.017246
=== Actor Training Debug (Iteration 1265) ===
Q mean: -5.855539
Q std: 5.755223
Actor loss: 5.859532
Action reg: 0.003993
  l1.weight: grad_norm = 0.000681
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.000689
Total gradient norm: 0.005507
=== Actor Training Debug (Iteration 1266) ===
Q mean: -5.706207
Q std: 5.569841
Actor loss: 5.710195
Action reg: 0.003988
  l1.weight: grad_norm = 0.010018
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.009685
Total gradient norm: 0.075041
=== Actor Training Debug (Iteration 1267) ===
Q mean: -5.200632
Q std: 5.737794
Actor loss: 5.204628
Action reg: 0.003996
  l1.weight: grad_norm = 0.000559
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.000553
Total gradient norm: 0.004306
=== Actor Training Debug (Iteration 1268) ===
Q mean: -5.807147
Q std: 5.405990
Actor loss: 5.811132
Action reg: 0.003985
  l1.weight: grad_norm = 0.007920
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.006982
Total gradient norm: 0.029838
=== Actor Training Debug (Iteration 1269) ===
Q mean: -5.866511
Q std: 5.514711
Actor loss: 5.870496
Action reg: 0.003985
  l1.weight: grad_norm = 0.002694
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.002510
Total gradient norm: 0.012435
=== Actor Training Debug (Iteration 1270) ===
Q mean: -5.495193
Q std: 5.750843
Actor loss: 5.499169
Action reg: 0.003977
  l1.weight: grad_norm = 0.005838
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.005652
Total gradient norm: 0.029573
=== Actor Training Debug (Iteration 1271) ===
Q mean: -5.339523
Q std: 5.662438
Actor loss: 5.343512
Action reg: 0.003988
  l1.weight: grad_norm = 0.002028
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.001993
Total gradient norm: 0.014538
=== Actor Training Debug (Iteration 1272) ===
Q mean: -5.723890
Q std: 5.757772
Actor loss: 5.727877
Action reg: 0.003987
  l1.weight: grad_norm = 0.006211
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005252
Total gradient norm: 0.028346
=== Actor Training Debug (Iteration 1273) ===
Q mean: -5.403433
Q std: 5.136889
Actor loss: 5.407414
Action reg: 0.003981
  l1.weight: grad_norm = 0.006705
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.005675
Total gradient norm: 0.037465
=== Actor Training Debug (Iteration 1274) ===
Q mean: -6.225451
Q std: 5.762754
Actor loss: 6.229444
Action reg: 0.003992
  l1.weight: grad_norm = 0.011323
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.011558
Total gradient norm: 0.093443
=== Actor Training Debug (Iteration 1275) ===
Q mean: -6.477471
Q std: 5.849395
Actor loss: 6.481451
Action reg: 0.003979
  l1.weight: grad_norm = 0.004592
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004500
Total gradient norm: 0.038549
=== Actor Training Debug (Iteration 1276) ===
Q mean: -5.016160
Q std: 4.853619
Actor loss: 5.020140
Action reg: 0.003980
  l1.weight: grad_norm = 0.002229
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.002359
Total gradient norm: 0.014130
=== Actor Training Debug (Iteration 1277) ===
Q mean: -5.571848
Q std: 5.309060
Actor loss: 5.575825
Action reg: 0.003977
  l1.weight: grad_norm = 0.011810
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.010138
Total gradient norm: 0.072995
=== Actor Training Debug (Iteration 1278) ===
Q mean: -5.864317
Q std: 6.096493
Actor loss: 5.868287
Action reg: 0.003970
  l1.weight: grad_norm = 0.003943
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.003812
Total gradient norm: 0.022947
=== Actor Training Debug (Iteration 1279) ===
Q mean: -6.127069
Q std: 6.270785
Actor loss: 6.131049
Action reg: 0.003980
  l1.weight: grad_norm = 0.006929
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.005751
Total gradient norm: 0.026726
=== Actor Training Debug (Iteration 1280) ===
Q mean: -5.967161
Q std: 5.709872
Actor loss: 5.971157
Action reg: 0.003996
  l1.weight: grad_norm = 0.004161
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003642
Total gradient norm: 0.017285
=== Actor Training Debug (Iteration 1281) ===
Q mean: -5.133406
Q std: 5.652892
Actor loss: 5.137389
Action reg: 0.003983
  l1.weight: grad_norm = 0.006053
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.005236
Total gradient norm: 0.032831
=== Actor Training Debug (Iteration 1282) ===
Q mean: -5.885003
Q std: 5.471283
Actor loss: 5.888975
Action reg: 0.003972
  l1.weight: grad_norm = 0.015680
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.016708
Total gradient norm: 0.093155
=== Actor Training Debug (Iteration 1283) ===
Q mean: -5.828415
Q std: 5.293696
Actor loss: 5.832401
Action reg: 0.003986
  l1.weight: grad_norm = 0.010197
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.009631
Total gradient norm: 0.055243
=== Actor Training Debug (Iteration 1284) ===
Q mean: -5.950207
Q std: 5.607051
Actor loss: 5.954193
Action reg: 0.003986
  l1.weight: grad_norm = 0.003436
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.003128
Total gradient norm: 0.020428
=== Actor Training Debug (Iteration 1285) ===
Q mean: -6.031647
Q std: 5.421107
Actor loss: 6.035622
Action reg: 0.003974
  l1.weight: grad_norm = 0.008942
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.008142
Total gradient norm: 0.058525
=== Actor Training Debug (Iteration 1286) ===
Q mean: -5.904886
Q std: 5.785395
Actor loss: 5.908877
Action reg: 0.003991
  l1.weight: grad_norm = 0.001896
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001517
Total gradient norm: 0.007328
=== Actor Training Debug (Iteration 1287) ===
Q mean: -5.536652
Q std: 5.166195
Actor loss: 5.540625
Action reg: 0.003973
  l1.weight: grad_norm = 0.007118
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.005937
Total gradient norm: 0.026366
=== Actor Training Debug (Iteration 1288) ===
Q mean: -5.981953
Q std: 5.657755
Actor loss: 5.985940
Action reg: 0.003987
  l1.weight: grad_norm = 0.004677
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.004743
Total gradient norm: 0.033284
=== Actor Training Debug (Iteration 1289) ===
Q mean: -6.326912
Q std: 6.138587
Actor loss: 6.330874
Action reg: 0.003962
  l1.weight: grad_norm = 0.007237
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.005547
Total gradient norm: 0.019665
=== Actor Training Debug (Iteration 1290) ===
Q mean: -5.678821
Q std: 5.718384
Actor loss: 5.682805
Action reg: 0.003984
  l1.weight: grad_norm = 0.008825
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.007184
Total gradient norm: 0.037497
=== Actor Training Debug (Iteration 1291) ===
Q mean: -4.995220
Q std: 5.033465
Actor loss: 4.999203
Action reg: 0.003982
  l1.weight: grad_norm = 0.006911
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.007293
Total gradient norm: 0.066655
=== Actor Training Debug (Iteration 1292) ===
Q mean: -5.540269
Q std: 5.664742
Actor loss: 5.544253
Action reg: 0.003985
  l1.weight: grad_norm = 0.007063
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.007260
Total gradient norm: 0.057080
=== Actor Training Debug (Iteration 1293) ===
Q mean: -5.695110
Q std: 5.693705
Actor loss: 5.699099
Action reg: 0.003989
  l1.weight: grad_norm = 0.003152
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.002852
Total gradient norm: 0.016101
=== Actor Training Debug (Iteration 1294) ===
Q mean: -5.805365
Q std: 5.711220
Actor loss: 5.809354
Action reg: 0.003989
  l1.weight: grad_norm = 0.002975
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002431
Total gradient norm: 0.008444
=== Actor Training Debug (Iteration 1295) ===
Q mean: -5.493441
Q std: 5.679975
Actor loss: 5.497425
Action reg: 0.003984
  l1.weight: grad_norm = 0.003820
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003688
Total gradient norm: 0.030814
=== Actor Training Debug (Iteration 1296) ===
Q mean: -5.600487
Q std: 5.030679
Actor loss: 5.604468
Action reg: 0.003981
  l1.weight: grad_norm = 0.012240
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.011455
Total gradient norm: 0.075728
=== Actor Training Debug (Iteration 1297) ===
Q mean: -6.351454
Q std: 5.537714
Actor loss: 6.355444
Action reg: 0.003990
  l1.weight: grad_norm = 0.015258
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.014446
Total gradient norm: 0.053503
=== Actor Training Debug (Iteration 1298) ===
Q mean: -5.487654
Q std: 5.333800
Actor loss: 5.491636
Action reg: 0.003983
  l1.weight: grad_norm = 0.002087
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.002273
Total gradient norm: 0.016778
=== Actor Training Debug (Iteration 1299) ===
Q mean: -4.976296
Q std: 5.605721
Actor loss: 4.980278
Action reg: 0.003982
  l1.weight: grad_norm = 0.005491
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.004780
Total gradient norm: 0.023396
=== Actor Training Debug (Iteration 1300) ===
Q mean: -5.928624
Q std: 5.933353
Actor loss: 5.932605
Action reg: 0.003981
  l1.weight: grad_norm = 0.005083
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.004544
Total gradient norm: 0.025530
=== Actor Training Debug (Iteration 1301) ===
Q mean: -5.388553
Q std: 5.701265
Actor loss: 5.392533
Action reg: 0.003981
  l1.weight: grad_norm = 0.008184
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.006265
Total gradient norm: 0.032515
=== Actor Training Debug (Iteration 1302) ===
Q mean: -6.295042
Q std: 6.242290
Actor loss: 6.299026
Action reg: 0.003984
  l1.weight: grad_norm = 0.009246
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.007792
Total gradient norm: 0.054464
=== Actor Training Debug (Iteration 1303) ===
Q mean: -5.302945
Q std: 5.444111
Actor loss: 5.306910
Action reg: 0.003965
  l1.weight: grad_norm = 0.007320
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.006865
Total gradient norm: 0.025353
=== Actor Training Debug (Iteration 1304) ===
Q mean: -5.336820
Q std: 5.692879
Actor loss: 5.340805
Action reg: 0.003985
  l1.weight: grad_norm = 0.011595
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.011586
Total gradient norm: 0.071636
=== Actor Training Debug (Iteration 1305) ===
Q mean: -6.059906
Q std: 5.882455
Actor loss: 6.063893
Action reg: 0.003987
  l1.weight: grad_norm = 0.003678
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.003212
Total gradient norm: 0.017133
=== Actor Training Debug (Iteration 1306) ===
Q mean: -5.766162
Q std: 5.745033
Actor loss: 5.770142
Action reg: 0.003980
  l1.weight: grad_norm = 0.003142
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.002983
Total gradient norm: 0.018818
=== Actor Training Debug (Iteration 1307) ===
Q mean: -5.117852
Q std: 5.332041
Actor loss: 5.121834
Action reg: 0.003982
  l1.weight: grad_norm = 0.001985
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.001977
Total gradient norm: 0.014968
=== Actor Training Debug (Iteration 1308) ===
Q mean: -5.771584
Q std: 6.105142
Actor loss: 5.775570
Action reg: 0.003986
  l1.weight: grad_norm = 0.004636
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003951
Total gradient norm: 0.019371
=== Actor Training Debug (Iteration 1309) ===
Q mean: -5.889497
Q std: 5.645396
Actor loss: 5.893470
Action reg: 0.003974
  l1.weight: grad_norm = 0.009826
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.009598
Total gradient norm: 0.069593
=== Actor Training Debug (Iteration 1310) ===
Q mean: -5.462690
Q std: 5.770408
Actor loss: 5.466658
Action reg: 0.003968
  l1.weight: grad_norm = 0.007180
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.006427
Total gradient norm: 0.045874
=== Actor Training Debug (Iteration 1311) ===
Q mean: -5.786433
Q std: 5.303376
Actor loss: 5.790407
Action reg: 0.003974
  l1.weight: grad_norm = 0.003812
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.002947
Total gradient norm: 0.012863
=== Actor Training Debug (Iteration 1312) ===
Q mean: -5.410003
Q std: 5.812685
Actor loss: 5.413994
Action reg: 0.003992
  l1.weight: grad_norm = 0.009112
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.008001
Total gradient norm: 0.032701
=== Actor Training Debug (Iteration 1313) ===
Q mean: -6.134441
Q std: 6.238260
Actor loss: 6.138433
Action reg: 0.003991
  l1.weight: grad_norm = 0.003134
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.002456
Total gradient norm: 0.014263
=== Actor Training Debug (Iteration 1314) ===
Q mean: -6.487976
Q std: 6.131150
Actor loss: 6.491954
Action reg: 0.003978
  l1.weight: grad_norm = 0.009771
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.008431
Total gradient norm: 0.060169
=== Actor Training Debug (Iteration 1315) ===
Q mean: -5.488916
Q std: 5.147012
Actor loss: 5.492903
Action reg: 0.003987
  l1.weight: grad_norm = 0.004175
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.004138
Total gradient norm: 0.034581
=== Actor Training Debug (Iteration 1316) ===
Q mean: -5.448466
Q std: 5.892755
Actor loss: 5.452437
Action reg: 0.003971
  l1.weight: grad_norm = 0.012373
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.009047
Total gradient norm: 0.046594
=== Actor Training Debug (Iteration 1317) ===
Q mean: -5.901206
Q std: 5.950675
Actor loss: 5.905202
Action reg: 0.003996
  l1.weight: grad_norm = 0.001302
  l1.bias: grad_norm = 0.000000
  l2.weight: grad_norm = 0.001406
Total gradient norm: 0.010376
=== Actor Training Debug (Iteration 1318) ===
Q mean: -5.906808
Q std: 5.483831
Actor loss: 5.910797
Action reg: 0.003989
  l1.weight: grad_norm = 0.003141
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.002599
Total gradient norm: 0.009825
=== Actor Training Debug (Iteration 1319) ===
Q mean: -5.546668
Q std: 5.698399
Actor loss: 5.550639
Action reg: 0.003971
  l1.weight: grad_norm = 0.009286
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.008077
Total gradient norm: 0.061220
=== Actor Training Debug (Iteration 1320) ===
Q mean: -5.758085
Q std: 6.031258
Actor loss: 5.762075
Action reg: 0.003990
  l1.weight: grad_norm = 0.011031
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.009593
Total gradient norm: 0.056478
=== Actor Training Debug (Iteration 1321) ===
Q mean: -5.590423
Q std: 6.017331
Actor loss: 5.594393
Action reg: 0.003970
  l1.weight: grad_norm = 0.004559
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.004219
Total gradient norm: 0.027599
=== Actor Training Debug (Iteration 1322) ===
Q mean: -6.019114
Q std: 5.586642
Actor loss: 6.023097
Action reg: 0.003983
  l1.weight: grad_norm = 0.001673
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.001468
Total gradient norm: 0.005274
=== Actor Training Debug (Iteration 1323) ===
Q mean: -5.784184
Q std: 5.888353
Actor loss: 5.788169
Action reg: 0.003985
  l1.weight: grad_norm = 0.007994
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.007520
Total gradient norm: 0.065676
=== Actor Training Debug (Iteration 1324) ===
Q mean: -5.278273
Q std: 5.663904
Actor loss: 5.282259
Action reg: 0.003986
  l1.weight: grad_norm = 0.010984
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.008717
Total gradient norm: 0.034810
=== Actor Training Debug (Iteration 1325) ===
Q mean: -6.001072
Q std: 5.934419
Actor loss: 6.005052
Action reg: 0.003979
  l1.weight: grad_norm = 0.007010
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.005395
Total gradient norm: 0.026485
=== Actor Training Debug (Iteration 1326) ===
Q mean: -5.868110
Q std: 6.192555
Actor loss: 5.872094
Action reg: 0.003984
  l1.weight: grad_norm = 0.007546
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.006040
Total gradient norm: 0.029404
=== Actor Training Debug (Iteration 1327) ===
Q mean: -5.519279
Q std: 5.360343
Actor loss: 5.523270
Action reg: 0.003990
  l1.weight: grad_norm = 0.004055
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.004076
Total gradient norm: 0.018636
=== Actor Training Debug (Iteration 1328) ===
Q mean: -5.606736
Q std: 5.864870
Actor loss: 5.610724
Action reg: 0.003988
  l1.weight: grad_norm = 0.004694
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.004448
Total gradient norm: 0.034945
=== Actor Training Debug (Iteration 1329) ===
Q mean: -5.616204
Q std: 5.583054
Actor loss: 5.620192
Action reg: 0.003988
  l1.weight: grad_norm = 0.010199
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.009791
Total gradient norm: 0.079864
=== Actor Training Debug (Iteration 1330) ===
Q mean: -5.997507
Q std: 5.700309
Actor loss: 6.001486
Action reg: 0.003980
  l1.weight: grad_norm = 0.009229
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.007479
Total gradient norm: 0.029884
=== Actor Training Debug (Iteration 1331) ===
Q mean: -6.223309
Q std: 5.776627
Actor loss: 6.227302
Action reg: 0.003992
  l1.weight: grad_norm = 0.003098
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002210
Total gradient norm: 0.007975
=== Actor Training Debug (Iteration 1332) ===
Q mean: -5.954740
Q std: 6.016504
Actor loss: 5.958727
Action reg: 0.003987
  l1.weight: grad_norm = 0.002016
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.001915
Total gradient norm: 0.013974
=== Actor Training Debug (Iteration 1333) ===
Q mean: -5.077291
Q std: 5.795909
Actor loss: 5.081278
Action reg: 0.003987
  l1.weight: grad_norm = 0.001547
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.001389
Total gradient norm: 0.007217
=== Actor Training Debug (Iteration 1334) ===
Q mean: -5.743537
Q std: 5.838583
Actor loss: 5.747519
Action reg: 0.003983
  l1.weight: grad_norm = 0.004849
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004993
Total gradient norm: 0.036074
=== Actor Training Debug (Iteration 1335) ===
Q mean: -6.368949
Q std: 5.731246
Actor loss: 6.372936
Action reg: 0.003987
  l1.weight: grad_norm = 0.002685
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.002486
Total gradient norm: 0.017806
=== Actor Training Debug (Iteration 1336) ===
Q mean: -5.863698
Q std: 5.851368
Actor loss: 5.867672
Action reg: 0.003974
  l1.weight: grad_norm = 0.008148
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.007592
Total gradient norm: 0.057995
=== Actor Training Debug (Iteration 1337) ===
Q mean: -5.408705
Q std: 5.227746
Actor loss: 5.412698
Action reg: 0.003993
  l1.weight: grad_norm = 0.003171
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.002914
Total gradient norm: 0.013090
=== Actor Training Debug (Iteration 1338) ===
Q mean: -5.528081
Q std: 5.563710
Actor loss: 5.532051
Action reg: 0.003970
  l1.weight: grad_norm = 0.010092
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.007713
Total gradient norm: 0.042561
=== Actor Training Debug (Iteration 1339) ===
Q mean: -6.326316
Q std: 5.907681
Actor loss: 6.330303
Action reg: 0.003986
  l1.weight: grad_norm = 0.005957
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.005160
Total gradient norm: 0.033467
=== Actor Training Debug (Iteration 1340) ===
Q mean: -5.685457
Q std: 5.735475
Actor loss: 5.689456
Action reg: 0.003999
  l1.weight: grad_norm = 0.004781
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003666
Total gradient norm: 0.024820
=== Actor Training Debug (Iteration 1341) ===
Q mean: -5.874874
Q std: 5.977621
Actor loss: 5.878860
Action reg: 0.003986
  l1.weight: grad_norm = 0.005711
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005930
Total gradient norm: 0.056339
=== Actor Training Debug (Iteration 1342) ===
Q mean: -5.439854
Q std: 5.112327
Actor loss: 5.443841
Action reg: 0.003988
  l1.weight: grad_norm = 0.005397
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.004578
Total gradient norm: 0.018973
=== Actor Training Debug (Iteration 1343) ===
Q mean: -4.848841
Q std: 5.053528
Actor loss: 4.852827
Action reg: 0.003985
  l1.weight: grad_norm = 0.003736
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.003490
Total gradient norm: 0.025806
=== Actor Training Debug (Iteration 1344) ===
Q mean: -5.871688
Q std: 6.122256
Actor loss: 5.875669
Action reg: 0.003980
  l1.weight: grad_norm = 0.007125
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.006334
Total gradient norm: 0.046170
=== Actor Training Debug (Iteration 1345) ===
Q mean: -5.502033
Q std: 5.399323
Actor loss: 5.506011
Action reg: 0.003977
  l1.weight: grad_norm = 0.006950
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.006346
Total gradient norm: 0.034517
=== Actor Training Debug (Iteration 1346) ===
Q mean: -6.023628
Q std: 6.167262
Actor loss: 6.027619
Action reg: 0.003991
  l1.weight: grad_norm = 0.010799
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.008788
Total gradient norm: 0.043195
=== Actor Training Debug (Iteration 1347) ===
Q mean: -5.407382
Q std: 5.738740
Actor loss: 5.411372
Action reg: 0.003990
  l1.weight: grad_norm = 0.004452
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004285
Total gradient norm: 0.030071
=== Actor Training Debug (Iteration 1348) ===
Q mean: -6.064649
Q std: 6.291940
Actor loss: 6.068635
Action reg: 0.003987
  l1.weight: grad_norm = 0.002486
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.002134
Total gradient norm: 0.011324
=== Actor Training Debug (Iteration 1349) ===
Q mean: -5.933324
Q std: 6.144357
Actor loss: 5.937299
Action reg: 0.003975
  l1.weight: grad_norm = 0.001812
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.001693
Total gradient norm: 0.009696
=== Actor Training Debug (Iteration 1350) ===
Q mean: -5.909310
Q std: 5.538779
Actor loss: 5.913303
Action reg: 0.003993
  l1.weight: grad_norm = 0.005247
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.004124
Total gradient norm: 0.024839
=== Actor Training Debug (Iteration 1351) ===
Q mean: -5.450016
Q std: 5.789247
Actor loss: 5.453999
Action reg: 0.003983
  l1.weight: grad_norm = 0.004241
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004043
Total gradient norm: 0.025002
=== Actor Training Debug (Iteration 1352) ===
Q mean: -5.214469
Q std: 5.290868
Actor loss: 5.218462
Action reg: 0.003992
  l1.weight: grad_norm = 0.002667
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.002321
Total gradient norm: 0.013454
=== Actor Training Debug (Iteration 1353) ===
Q mean: -5.876406
Q std: 5.478953
Actor loss: 5.880377
Action reg: 0.003972
  l1.weight: grad_norm = 0.006395
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.004366
Total gradient norm: 0.018428
=== Actor Training Debug (Iteration 1354) ===
Q mean: -6.853882
Q std: 6.166860
Actor loss: 6.857875
Action reg: 0.003993
  l1.weight: grad_norm = 0.004479
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.005100
Total gradient norm: 0.036700
=== Actor Training Debug (Iteration 1355) ===
Q mean: -5.854701
Q std: 6.202193
Actor loss: 5.858680
Action reg: 0.003979
  l1.weight: grad_norm = 0.007403
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.006415
Total gradient norm: 0.028194
=== Actor Training Debug (Iteration 1356) ===
Q mean: -5.894073
Q std: 6.579093
Actor loss: 5.898064
Action reg: 0.003991
  l1.weight: grad_norm = 0.009621
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.008728
Total gradient norm: 0.064991
=== Actor Training Debug (Iteration 1357) ===
Q mean: -6.242635
Q std: 6.082828
Actor loss: 6.246616
Action reg: 0.003982
  l1.weight: grad_norm = 0.006702
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.005451
Total gradient norm: 0.033627
=== Actor Training Debug (Iteration 1358) ===
Q mean: -6.087521
Q std: 5.934302
Actor loss: 6.091509
Action reg: 0.003988
  l1.weight: grad_norm = 0.012289
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.011262
Total gradient norm: 0.081587
=== Actor Training Debug (Iteration 1359) ===
Q mean: -6.638919
Q std: 6.043290
Actor loss: 6.642906
Action reg: 0.003987
  l1.weight: grad_norm = 0.002887
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.002503
Total gradient norm: 0.010250
=== Actor Training Debug (Iteration 1360) ===
Q mean: -5.896998
Q std: 5.743634
Actor loss: 5.900994
Action reg: 0.003995
  l1.weight: grad_norm = 0.002301
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001944
Total gradient norm: 0.007589
=== Actor Training Debug (Iteration 1361) ===
Q mean: -6.494169
Q std: 6.290274
Actor loss: 6.498166
Action reg: 0.003996
  l1.weight: grad_norm = 0.001509
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.001339
Total gradient norm: 0.009910
=== Actor Training Debug (Iteration 1362) ===
Q mean: -6.083131
Q std: 5.772273
Actor loss: 6.087126
Action reg: 0.003995
  l1.weight: grad_norm = 0.001318
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001085
Total gradient norm: 0.005989
=== Actor Training Debug (Iteration 1363) ===
Q mean: -5.856912
Q std: 5.858521
Actor loss: 5.860900
Action reg: 0.003988
  l1.weight: grad_norm = 0.003899
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.003119
Total gradient norm: 0.011285
=== Actor Training Debug (Iteration 1364) ===
Q mean: -5.656669
Q std: 5.643527
Actor loss: 5.660652
Action reg: 0.003983
  l1.weight: grad_norm = 0.005268
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.004544
Total gradient norm: 0.018884
=== Actor Training Debug (Iteration 1365) ===
Q mean: -5.748192
Q std: 6.270856
Actor loss: 5.752170
Action reg: 0.003978
  l1.weight: grad_norm = 0.019961
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.017901
Total gradient norm: 0.112991
=== Actor Training Debug (Iteration 1366) ===
Q mean: -5.734954
Q std: 5.978550
Actor loss: 5.738933
Action reg: 0.003978
  l1.weight: grad_norm = 0.017996
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.019061
Total gradient norm: 0.151195
=== Actor Training Debug (Iteration 1367) ===
Q mean: -6.026258
Q std: 5.972697
Actor loss: 6.030251
Action reg: 0.003992
  l1.weight: grad_norm = 0.001303
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.001201
Total gradient norm: 0.006078
=== Actor Training Debug (Iteration 1368) ===
Q mean: -6.430532
Q std: 6.122694
Actor loss: 6.434509
Action reg: 0.003976
  l1.weight: grad_norm = 0.015410
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.013464
Total gradient norm: 0.083244
=== Actor Training Debug (Iteration 1369) ===
Q mean: -6.181777
Q std: 6.387118
Actor loss: 6.185752
Action reg: 0.003974
  l1.weight: grad_norm = 0.004876
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.004661
Total gradient norm: 0.038682
=== Actor Training Debug (Iteration 1370) ===
Q mean: -5.846549
Q std: 6.111250
Actor loss: 5.850531
Action reg: 0.003982
  l1.weight: grad_norm = 0.013041
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.010804
Total gradient norm: 0.064141
=== Actor Training Debug (Iteration 1371) ===
Q mean: -6.686668
Q std: 6.619357
Actor loss: 6.690655
Action reg: 0.003987
  l1.weight: grad_norm = 0.002952
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.002780
Total gradient norm: 0.021649
=== Actor Training Debug (Iteration 1372) ===
Q mean: -5.368909
Q std: 5.731977
Actor loss: 5.372891
Action reg: 0.003982
  l1.weight: grad_norm = 0.009207
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.008411
Total gradient norm: 0.067454
=== Actor Training Debug (Iteration 1373) ===
Q mean: -5.426370
Q std: 5.670479
Actor loss: 5.430357
Action reg: 0.003988
  l1.weight: grad_norm = 0.003196
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.003250
Total gradient norm: 0.017827
=== Actor Training Debug (Iteration 1374) ===
Q mean: -6.630779
Q std: 6.393908
Actor loss: 6.634770
Action reg: 0.003990
  l1.weight: grad_norm = 0.013326
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.011287
Total gradient norm: 0.053222
=== Actor Training Debug (Iteration 1375) ===
Q mean: -6.047272
Q std: 6.296158
Actor loss: 6.051260
Action reg: 0.003988
  l1.weight: grad_norm = 0.003821
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.003483
Total gradient norm: 0.025094
=== Actor Training Debug (Iteration 1376) ===
Q mean: -5.867140
Q std: 6.106131
Actor loss: 5.871129
Action reg: 0.003989
  l1.weight: grad_norm = 0.014738
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.011891
Total gradient norm: 0.069181
=== Actor Training Debug (Iteration 1377) ===
Q mean: -6.544168
Q std: 6.133262
Actor loss: 6.548145
Action reg: 0.003977
  l1.weight: grad_norm = 0.003182
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.002902
Total gradient norm: 0.008356
=== Actor Training Debug (Iteration 1378) ===
Q mean: -6.363194
Q std: 5.666343
Actor loss: 6.367177
Action reg: 0.003984
  l1.weight: grad_norm = 0.003996
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003214
Total gradient norm: 0.015292
=== Actor Training Debug (Iteration 1379) ===
Q mean: -5.789400
Q std: 5.482592
Actor loss: 5.793381
Action reg: 0.003980
  l1.weight: grad_norm = 0.005553
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.004359
Total gradient norm: 0.016676
=== Actor Training Debug (Iteration 1380) ===
Q mean: -5.779979
Q std: 5.851396
Actor loss: 5.783968
Action reg: 0.003989
  l1.weight: grad_norm = 0.009664
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.008079
Total gradient norm: 0.037980
=== Actor Training Debug (Iteration 1381) ===
Q mean: -5.550194
Q std: 6.094133
Actor loss: 5.554189
Action reg: 0.003995
  l1.weight: grad_norm = 0.009121
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.008981
Total gradient norm: 0.059351
=== Actor Training Debug (Iteration 1382) ===
Q mean: -6.139747
Q std: 6.161879
Actor loss: 6.143713
Action reg: 0.003966
  l1.weight: grad_norm = 0.010046
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.008428
Total gradient norm: 0.036908
=== Actor Training Debug (Iteration 1383) ===
Q mean: -6.145039
Q std: 6.253237
Actor loss: 6.149014
Action reg: 0.003975
  l1.weight: grad_norm = 0.014429
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.012522
Total gradient norm: 0.078535
=== Actor Training Debug (Iteration 1384) ===
Q mean: -5.540339
Q std: 5.846324
Actor loss: 5.544323
Action reg: 0.003984
  l1.weight: grad_norm = 0.000370
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.000344
Total gradient norm: 0.002479
=== Actor Training Debug (Iteration 1385) ===
Q mean: -5.893453
Q std: 5.739841
Actor loss: 5.897439
Action reg: 0.003987
  l1.weight: grad_norm = 0.005022
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004423
Total gradient norm: 0.025734
=== Actor Training Debug (Iteration 1386) ===
Q mean: -5.918437
Q std: 6.060217
Actor loss: 5.922420
Action reg: 0.003983
  l1.weight: grad_norm = 0.013066
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.011136
Total gradient norm: 0.070834
=== Actor Training Debug (Iteration 1387) ===
Q mean: -5.945204
Q std: 5.481575
Actor loss: 5.949185
Action reg: 0.003982
  l1.weight: grad_norm = 0.003357
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003196
Total gradient norm: 0.020041
=== Actor Training Debug (Iteration 1388) ===
Q mean: -5.830535
Q std: 5.649356
Actor loss: 5.834524
Action reg: 0.003989
  l1.weight: grad_norm = 0.001027
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.000944
Total gradient norm: 0.007403
=== Actor Training Debug (Iteration 1389) ===
Q mean: -5.801613
Q std: 5.654020
Actor loss: 5.805588
Action reg: 0.003975
  l1.weight: grad_norm = 0.004607
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.003915
Total gradient norm: 0.024969
=== Actor Training Debug (Iteration 1390) ===
Q mean: -6.014106
Q std: 6.141302
Actor loss: 6.018094
Action reg: 0.003988
  l1.weight: grad_norm = 0.003731
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.003594
Total gradient norm: 0.026797
=== Actor Training Debug (Iteration 1391) ===
Q mean: -5.848492
Q std: 6.218285
Actor loss: 5.852481
Action reg: 0.003989
  l1.weight: grad_norm = 0.004908
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004583
Total gradient norm: 0.025160
=== Actor Training Debug (Iteration 1392) ===
Q mean: -6.102194
Q std: 6.113595
Actor loss: 6.106179
Action reg: 0.003985
  l1.weight: grad_norm = 0.001936
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.001683
Total gradient norm: 0.007992
=== Actor Training Debug (Iteration 1393) ===
Q mean: -7.281024
Q std: 6.712638
Actor loss: 7.285017
Action reg: 0.003993
  l1.weight: grad_norm = 0.005442
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.004770
Total gradient norm: 0.029145
=== Actor Training Debug (Iteration 1394) ===
Q mean: -5.995085
Q std: 6.735787
Actor loss: 5.999084
Action reg: 0.003999
  l1.weight: grad_norm = 0.002577
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002598
Total gradient norm: 0.013813
=== Actor Training Debug (Iteration 1395) ===
Q mean: -6.326532
Q std: 5.739253
Actor loss: 6.330515
Action reg: 0.003982
  l1.weight: grad_norm = 0.008864
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.007296
Total gradient norm: 0.027546
=== Actor Training Debug (Iteration 1396) ===
Q mean: -6.170364
Q std: 6.322601
Actor loss: 6.174348
Action reg: 0.003984
  l1.weight: grad_norm = 0.003211
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.003399
Total gradient norm: 0.029344
=== Actor Training Debug (Iteration 1397) ===
Q mean: -6.051965
Q std: 5.842917
Actor loss: 6.055957
Action reg: 0.003992
  l1.weight: grad_norm = 0.013018
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.011764
Total gradient norm: 0.083168
=== Actor Training Debug (Iteration 1398) ===
Q mean: -6.003153
Q std: 6.146266
Actor loss: 6.007138
Action reg: 0.003985
  l1.weight: grad_norm = 0.006166
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.005535
Total gradient norm: 0.028355
=== Actor Training Debug (Iteration 1399) ===
Q mean: -5.378905
Q std: 5.575217
Actor loss: 5.382893
Action reg: 0.003987
  l1.weight: grad_norm = 0.003512
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.003181
Total gradient norm: 0.016123
=== Actor Training Debug (Iteration 1400) ===
Q mean: -6.545422
Q std: 6.516747
Actor loss: 6.549414
Action reg: 0.003992
  l1.weight: grad_norm = 0.011664
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.011237
Total gradient norm: 0.058874
=== Actor Training Debug (Iteration 1401) ===
Q mean: -5.577664
Q std: 5.923949
Actor loss: 5.581632
Action reg: 0.003967
  l1.weight: grad_norm = 0.006577
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.006115
Total gradient norm: 0.042182
=== Actor Training Debug (Iteration 1402) ===
Q mean: -5.489478
Q std: 5.863681
Actor loss: 5.493472
Action reg: 0.003994
  l1.weight: grad_norm = 0.000257
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.000225
Total gradient norm: 0.001454
=== Actor Training Debug (Iteration 1403) ===
Q mean: -6.313197
Q std: 6.272179
Actor loss: 6.317187
Action reg: 0.003990
  l1.weight: grad_norm = 0.010861
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.008360
Total gradient norm: 0.049617
=== Actor Training Debug (Iteration 1404) ===
Q mean: -6.424461
Q std: 6.183666
Actor loss: 6.428456
Action reg: 0.003995
  l1.weight: grad_norm = 0.005759
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.005620
Total gradient norm: 0.029709
=== Actor Training Debug (Iteration 1405) ===
Q mean: -6.021309
Q std: 6.227872
Actor loss: 6.025297
Action reg: 0.003988
  l1.weight: grad_norm = 0.002371
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.002363
Total gradient norm: 0.013698
=== Actor Training Debug (Iteration 1406) ===
Q mean: -6.693115
Q std: 6.423207
Actor loss: 6.697101
Action reg: 0.003986
  l1.weight: grad_norm = 0.012223
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.010638
Total gradient norm: 0.062291
=== Actor Training Debug (Iteration 1407) ===
Q mean: -6.009510
Q std: 6.032748
Actor loss: 6.013507
Action reg: 0.003998
  l1.weight: grad_norm = 0.001558
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.001471
Total gradient norm: 0.011664
=== Actor Training Debug (Iteration 1408) ===
Q mean: -6.371487
Q std: 6.226100
Actor loss: 6.375478
Action reg: 0.003991
  l1.weight: grad_norm = 0.001482
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.001399
Total gradient norm: 0.008332
=== Actor Training Debug (Iteration 1409) ===
Q mean: -6.386853
Q std: 6.064878
Actor loss: 6.390834
Action reg: 0.003982
  l1.weight: grad_norm = 0.008786
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.008212
Total gradient norm: 0.054064
=== Actor Training Debug (Iteration 1410) ===
Q mean: -5.835190
Q std: 5.821665
Actor loss: 5.839164
Action reg: 0.003973
  l1.weight: grad_norm = 0.009343
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.009090
Total gradient norm: 0.061013
=== Actor Training Debug (Iteration 1411) ===
Q mean: -5.845518
Q std: 6.119728
Actor loss: 5.849497
Action reg: 0.003979
  l1.weight: grad_norm = 0.006157
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.005394
Total gradient norm: 0.016365
=== Actor Training Debug (Iteration 1412) ===
Q mean: -6.102302
Q std: 6.392281
Actor loss: 6.106287
Action reg: 0.003985
  l1.weight: grad_norm = 0.005154
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004202
Total gradient norm: 0.016558
=== Actor Training Debug (Iteration 1413) ===
Q mean: -5.892310
Q std: 6.351195
Actor loss: 5.896283
Action reg: 0.003973
  l1.weight: grad_norm = 0.013313
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.013979
Total gradient norm: 0.075154
=== Actor Training Debug (Iteration 1414) ===
Q mean: -5.179620
Q std: 6.136346
Actor loss: 5.183599
Action reg: 0.003980
  l1.weight: grad_norm = 0.009857
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.007601
Total gradient norm: 0.032431
=== Actor Training Debug (Iteration 1415) ===
Q mean: -6.367794
Q std: 6.223111
Actor loss: 6.371781
Action reg: 0.003987
  l1.weight: grad_norm = 0.003570
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.003384
Total gradient norm: 0.022094
=== Actor Training Debug (Iteration 1416) ===
Q mean: -6.905235
Q std: 6.669495
Actor loss: 6.909217
Action reg: 0.003982
  l1.weight: grad_norm = 0.007886
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.007287
Total gradient norm: 0.025283
=== Actor Training Debug (Iteration 1417) ===
Q mean: -6.250344
Q std: 6.102439
Actor loss: 6.254332
Action reg: 0.003987
  l1.weight: grad_norm = 0.006383
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005674
Total gradient norm: 0.028109
=== Actor Training Debug (Iteration 1418) ===
Q mean: -5.659289
Q std: 6.071769
Actor loss: 5.663278
Action reg: 0.003989
  l1.weight: grad_norm = 0.002983
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.002991
Total gradient norm: 0.019598
=== Actor Training Debug (Iteration 1419) ===
Q mean: -6.381998
Q std: 6.498166
Actor loss: 6.385977
Action reg: 0.003979
  l1.weight: grad_norm = 0.011786
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.011308
Total gradient norm: 0.038013
=== Actor Training Debug (Iteration 1420) ===
Q mean: -7.013171
Q std: 6.616791
Actor loss: 7.017160
Action reg: 0.003989
  l1.weight: grad_norm = 0.001280
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.001336
Total gradient norm: 0.011182
=== Actor Training Debug (Iteration 1421) ===
Q mean: -6.245661
Q std: 6.496024
Actor loss: 6.249643
Action reg: 0.003982
  l1.weight: grad_norm = 0.001166
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.001196
Total gradient norm: 0.006755
=== Actor Training Debug (Iteration 1422) ===
Q mean: -5.622162
Q std: 6.539865
Actor loss: 5.626143
Action reg: 0.003981
  l1.weight: grad_norm = 0.006727
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.006565
Total gradient norm: 0.037566
=== Actor Training Debug (Iteration 1423) ===
Q mean: -5.777077
Q std: 6.474312
Actor loss: 5.781056
Action reg: 0.003979
  l1.weight: grad_norm = 0.021934
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.019121
Total gradient norm: 0.130683
=== Actor Training Debug (Iteration 1424) ===
Q mean: -6.536378
Q std: 6.171149
Actor loss: 6.540362
Action reg: 0.003984
  l1.weight: grad_norm = 0.007880
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.008756
Total gradient norm: 0.056957
=== Actor Training Debug (Iteration 1425) ===
Q mean: -6.080698
Q std: 5.890559
Actor loss: 6.084681
Action reg: 0.003983
  l1.weight: grad_norm = 0.000955
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.000894
Total gradient norm: 0.006707
=== Actor Training Debug (Iteration 1426) ===
Q mean: -6.216088
Q std: 6.219771
Actor loss: 6.220063
Action reg: 0.003975
  l1.weight: grad_norm = 0.012570
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.011889
Total gradient norm: 0.063767
=== Actor Training Debug (Iteration 1427) ===
Q mean: -5.995269
Q std: 6.529696
Actor loss: 5.999251
Action reg: 0.003982
  l1.weight: grad_norm = 0.037415
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.027730
Total gradient norm: 0.128799
=== Actor Training Debug (Iteration 1428) ===
Q mean: -6.226424
Q std: 6.145319
Actor loss: 6.230391
Action reg: 0.003967
  l1.weight: grad_norm = 0.017245
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.015270
Total gradient norm: 0.088875
=== Actor Training Debug (Iteration 1429) ===
Q mean: -6.579297
Q std: 6.262206
Actor loss: 6.583280
Action reg: 0.003983
  l1.weight: grad_norm = 0.006325
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.005741
Total gradient norm: 0.029039
=== Actor Training Debug (Iteration 1430) ===
Q mean: -6.310433
Q std: 6.305858
Actor loss: 6.314421
Action reg: 0.003988
  l1.weight: grad_norm = 0.005479
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005476
Total gradient norm: 0.031638
=== Actor Training Debug (Iteration 1431) ===
Q mean: -6.055442
Q std: 6.462852
Actor loss: 6.059438
Action reg: 0.003996
  l1.weight: grad_norm = 0.007404
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006157
Total gradient norm: 0.032228
=== Actor Training Debug (Iteration 1432) ===
Q mean: -5.595679
Q std: 6.511034
Actor loss: 5.599653
Action reg: 0.003974
  l1.weight: grad_norm = 0.010899
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010557
Total gradient norm: 0.060011
=== Actor Training Debug (Iteration 1433) ===
Q mean: -6.792337
Q std: 6.772573
Actor loss: 6.796319
Action reg: 0.003982
  l1.weight: grad_norm = 0.002961
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.002603
Total gradient norm: 0.018330
=== Actor Training Debug (Iteration 1434) ===
Q mean: -6.630632
Q std: 6.713359
Actor loss: 6.634625
Action reg: 0.003993
  l1.weight: grad_norm = 0.004149
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.004639
Total gradient norm: 0.031753
=== Actor Training Debug (Iteration 1435) ===
Q mean: -6.353369
Q std: 6.007185
Actor loss: 6.357357
Action reg: 0.003988
  l1.weight: grad_norm = 0.002038
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001836
Total gradient norm: 0.009966
=== Actor Training Debug (Iteration 1436) ===
Q mean: -5.189296
Q std: 5.593587
Actor loss: 5.193278
Action reg: 0.003983
  l1.weight: grad_norm = 0.006465
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.006286
Total gradient norm: 0.045757
=== Actor Training Debug (Iteration 1437) ===
Q mean: -6.061490
Q std: 5.922366
Actor loss: 6.065474
Action reg: 0.003984
  l1.weight: grad_norm = 0.004813
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004937
Total gradient norm: 0.015447
=== Actor Training Debug (Iteration 1438) ===
Q mean: -6.290275
Q std: 6.334809
Actor loss: 6.294265
Action reg: 0.003990
  l1.weight: grad_norm = 0.002465
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.002511
Total gradient norm: 0.012439
=== Actor Training Debug (Iteration 1439) ===
Q mean: -6.889437
Q std: 6.726733
Actor loss: 6.893409
Action reg: 0.003971
  l1.weight: grad_norm = 0.007663
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.007964
Total gradient norm: 0.066331
=== Actor Training Debug (Iteration 1440) ===
Q mean: -6.188721
Q std: 6.357724
Actor loss: 6.192706
Action reg: 0.003985
  l1.weight: grad_norm = 0.003368
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.003120
Total gradient norm: 0.012108
=== Actor Training Debug (Iteration 1441) ===
Q mean: -5.560677
Q std: 6.265217
Actor loss: 5.564663
Action reg: 0.003986
  l1.weight: grad_norm = 0.003199
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.002456
Total gradient norm: 0.008761
=== Actor Training Debug (Iteration 1442) ===
Q mean: -6.505090
Q std: 6.825975
Actor loss: 6.509052
Action reg: 0.003962
  l1.weight: grad_norm = 0.015799
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.015497
Total gradient norm: 0.063975
=== Actor Training Debug (Iteration 1443) ===
Q mean: -6.548024
Q std: 6.449044
Actor loss: 6.552009
Action reg: 0.003985
  l1.weight: grad_norm = 0.006100
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.006837
Total gradient norm: 0.053259
=== Actor Training Debug (Iteration 1444) ===
Q mean: -5.911995
Q std: 5.994909
Actor loss: 5.915972
Action reg: 0.003977
  l1.weight: grad_norm = 0.011458
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.012636
Total gradient norm: 0.071625
=== Actor Training Debug (Iteration 1445) ===
Q mean: -6.631781
Q std: 6.638306
Actor loss: 6.635766
Action reg: 0.003985
  l1.weight: grad_norm = 0.011305
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.010485
Total gradient norm: 0.066282
=== Actor Training Debug (Iteration 1446) ===
Q mean: -5.263528
Q std: 5.817539
Actor loss: 5.267497
Action reg: 0.003969
  l1.weight: grad_norm = 0.006196
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.005934
Total gradient norm: 0.037393
=== Actor Training Debug (Iteration 1447) ===
Q mean: -5.707593
Q std: 6.242119
Actor loss: 5.711584
Action reg: 0.003990
  l1.weight: grad_norm = 0.004694
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.004311
Total gradient norm: 0.033002
=== Actor Training Debug (Iteration 1448) ===
Q mean: -6.145792
Q std: 5.987063
Actor loss: 6.149750
Action reg: 0.003958
  l1.weight: grad_norm = 0.006500
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.005036
Total gradient norm: 0.019910
=== Actor Training Debug (Iteration 1449) ===
Q mean: -6.349957
Q std: 6.699270
Actor loss: 6.353945
Action reg: 0.003988
  l1.weight: grad_norm = 0.010912
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.010153
Total gradient norm: 0.075424
=== Actor Training Debug (Iteration 1450) ===
Q mean: -6.608236
Q std: 6.667397
Actor loss: 6.612214
Action reg: 0.003978
  l1.weight: grad_norm = 0.014368
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.014765
Total gradient norm: 0.097418
=== Actor Training Debug (Iteration 1451) ===
Q mean: -5.482282
Q std: 6.004517
Actor loss: 5.486246
Action reg: 0.003964
  l1.weight: grad_norm = 0.005332
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.004495
Total gradient norm: 0.022620
=== Actor Training Debug (Iteration 1452) ===
Q mean: -5.733250
Q std: 6.034893
Actor loss: 5.737216
Action reg: 0.003966
  l1.weight: grad_norm = 0.017306
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.016934
Total gradient norm: 0.103048
=== Actor Training Debug (Iteration 1453) ===
Q mean: -6.515830
Q std: 6.244669
Actor loss: 6.519807
Action reg: 0.003978
  l1.weight: grad_norm = 0.010474
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.011541
Total gradient norm: 0.074203
=== Actor Training Debug (Iteration 1454) ===
Q mean: -5.958165
Q std: 6.089455
Actor loss: 5.962127
Action reg: 0.003962
  l1.weight: grad_norm = 0.014550
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.015049
Total gradient norm: 0.091564
=== Actor Training Debug (Iteration 1455) ===
Q mean: -6.074670
Q std: 6.387786
Actor loss: 6.078657
Action reg: 0.003987
  l1.weight: grad_norm = 0.001463
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.001402
Total gradient norm: 0.008813
=== Actor Training Debug (Iteration 1456) ===
Q mean: -6.697074
Q std: 6.894262
Actor loss: 6.701062
Action reg: 0.003988
  l1.weight: grad_norm = 0.005315
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.005413
Total gradient norm: 0.043032
=== Actor Training Debug (Iteration 1457) ===
Q mean: -6.171550
Q std: 6.792410
Actor loss: 6.175536
Action reg: 0.003986
  l1.weight: grad_norm = 0.011756
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.010850
Total gradient norm: 0.069644
=== Actor Training Debug (Iteration 1458) ===
Q mean: -6.789459
Q std: 6.525279
Actor loss: 6.793440
Action reg: 0.003981
  l1.weight: grad_norm = 0.009993
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.008607
Total gradient norm: 0.045345
=== Actor Training Debug (Iteration 1459) ===
Q mean: -6.671041
Q std: 6.368279
Actor loss: 6.675035
Action reg: 0.003994
  l1.weight: grad_norm = 0.004426
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004105
Total gradient norm: 0.026460
=== Actor Training Debug (Iteration 1460) ===
Q mean: -7.323856
Q std: 6.878925
Actor loss: 7.327828
Action reg: 0.003972
  l1.weight: grad_norm = 0.009558
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.008727
Total gradient norm: 0.032961
=== Actor Training Debug (Iteration 1461) ===
Q mean: -7.142671
Q std: 6.890580
Actor loss: 7.146645
Action reg: 0.003975
  l1.weight: grad_norm = 0.006108
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.004463
Total gradient norm: 0.023266
=== Actor Training Debug (Iteration 1462) ===
Q mean: -5.657271
Q std: 6.215491
Actor loss: 5.661246
Action reg: 0.003975
  l1.weight: grad_norm = 0.001534
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.001608
Total gradient norm: 0.010923
=== Actor Training Debug (Iteration 1463) ===
Q mean: -5.963299
Q std: 6.222983
Actor loss: 5.967279
Action reg: 0.003980
  l1.weight: grad_norm = 0.015797
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.011847
Total gradient norm: 0.046201
=== Actor Training Debug (Iteration 1464) ===
Q mean: -6.115353
Q std: 6.691091
Actor loss: 6.119331
Action reg: 0.003978
  l1.weight: grad_norm = 0.006429
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.005142
Total gradient norm: 0.019423
=== Actor Training Debug (Iteration 1465) ===
Q mean: -6.730535
Q std: 6.584487
Actor loss: 6.734516
Action reg: 0.003982
  l1.weight: grad_norm = 0.003004
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002786
Total gradient norm: 0.014361
=== Actor Training Debug (Iteration 1466) ===
Q mean: -6.702627
Q std: 6.303646
Actor loss: 6.706623
Action reg: 0.003996
  l1.weight: grad_norm = 0.002201
  l1.bias: grad_norm = 0.000001
  l2.weight: grad_norm = 0.002119
Total gradient norm: 0.015432
=== Actor Training Debug (Iteration 1467) ===
Q mean: -6.170283
Q std: 6.515142
Actor loss: 6.174255
Action reg: 0.003972
  l1.weight: grad_norm = 0.003605
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.003563
Total gradient norm: 0.014634
=== Actor Training Debug (Iteration 1468) ===
Q mean: -5.783917
Q std: 6.667253
Actor loss: 5.787891
Action reg: 0.003974
  l1.weight: grad_norm = 0.002805
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.002773
Total gradient norm: 0.013419
=== Actor Training Debug (Iteration 1469) ===
Q mean: -6.041652
Q std: 6.321861
Actor loss: 6.045618
Action reg: 0.003967
  l1.weight: grad_norm = 0.011827
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.010869
Total gradient norm: 0.047072
=== Actor Training Debug (Iteration 1470) ===
Q mean: -6.096836
Q std: 6.472704
Actor loss: 6.100797
Action reg: 0.003962
  l1.weight: grad_norm = 0.002541
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.002051
Total gradient norm: 0.009076
=== Actor Training Debug (Iteration 1471) ===
Q mean: -6.678336
Q std: 6.312256
Actor loss: 6.682309
Action reg: 0.003973
  l1.weight: grad_norm = 0.003410
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.002717
Total gradient norm: 0.009743
=== Actor Training Debug (Iteration 1472) ===
Q mean: -6.432683
Q std: 6.155905
Actor loss: 6.436671
Action reg: 0.003988
  l1.weight: grad_norm = 0.009646
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.009193
Total gradient norm: 0.055424
=== Actor Training Debug (Iteration 1473) ===
Q mean: -5.569026
Q std: 5.667321
Actor loss: 5.572993
Action reg: 0.003966
  l1.weight: grad_norm = 0.011700
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.009943
Total gradient norm: 0.038290
=== Actor Training Debug (Iteration 1474) ===
Q mean: -6.263783
Q std: 6.712462
Actor loss: 6.267769
Action reg: 0.003987
  l1.weight: grad_norm = 0.010200
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.009745
Total gradient norm: 0.039287
=== Actor Training Debug (Iteration 1475) ===
Q mean: -6.388185
Q std: 6.701817
Actor loss: 6.392177
Action reg: 0.003992
  l1.weight: grad_norm = 0.002122
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.001793
Total gradient norm: 0.010000
=== Actor Training Debug (Iteration 1476) ===
Q mean: -6.997532
Q std: 6.414866
Actor loss: 7.001514
Action reg: 0.003982
  l1.weight: grad_norm = 0.006048
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.005008
Total gradient norm: 0.024034
=== Actor Training Debug (Iteration 1477) ===
Q mean: -6.142331
Q std: 6.482695
Actor loss: 6.146320
Action reg: 0.003989
  l1.weight: grad_norm = 0.001747
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.001855
Total gradient norm: 0.008442
=== Actor Training Debug (Iteration 1478) ===
Q mean: -5.968252
Q std: 6.486602
Actor loss: 5.972208
Action reg: 0.003956
  l1.weight: grad_norm = 0.002301
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.002263
Total gradient norm: 0.016517
=== Actor Training Debug (Iteration 1479) ===
Q mean: -5.945474
Q std: 6.211947
Actor loss: 5.949457
Action reg: 0.003983
  l1.weight: grad_norm = 0.009866
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.008971
Total gradient norm: 0.040856
=== Actor Training Debug (Iteration 1480) ===
Q mean: -5.971571
Q std: 6.575968
Actor loss: 5.975552
Action reg: 0.003980
  l1.weight: grad_norm = 0.005487
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.004630
Total gradient norm: 0.015916
=== Actor Training Debug (Iteration 1481) ===
Q mean: -6.558258
Q std: 6.504226
Actor loss: 6.562232
Action reg: 0.003973
  l1.weight: grad_norm = 0.004416
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.004204
Total gradient norm: 0.016684
=== Actor Training Debug (Iteration 1482) ===
Q mean: -6.478369
Q std: 6.253070
Actor loss: 6.482353
Action reg: 0.003984
  l1.weight: grad_norm = 0.004810
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.004611
Total gradient norm: 0.030680
=== Actor Training Debug (Iteration 1483) ===
Q mean: -5.788917
Q std: 6.432680
Actor loss: 5.792900
Action reg: 0.003983
  l1.weight: grad_norm = 0.004449
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.003608
Total gradient norm: 0.012622
=== Actor Training Debug (Iteration 1484) ===
Q mean: -5.490512
Q std: 6.831212
Actor loss: 5.494486
Action reg: 0.003974
  l1.weight: grad_norm = 0.007530
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.006775
Total gradient norm: 0.030621
=== Actor Training Debug (Iteration 1485) ===
Q mean: -6.231242
Q std: 6.586619
Actor loss: 6.235218
Action reg: 0.003975
  l1.weight: grad_norm = 0.020365
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.024706
Total gradient norm: 0.312484
=== Actor Training Debug (Iteration 1486) ===
Q mean: -6.500197
Q std: 6.344668
Actor loss: 6.504186
Action reg: 0.003988
  l1.weight: grad_norm = 0.004991
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.004853
Total gradient norm: 0.017310
=== Actor Training Debug (Iteration 1487) ===
Q mean: -6.445586
Q std: 6.614430
Actor loss: 6.449575
Action reg: 0.003989
  l1.weight: grad_norm = 0.028836
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.024065
Total gradient norm: 0.117298
=== Actor Training Debug (Iteration 1488) ===
Q mean: -6.095635
Q std: 6.499038
Actor loss: 6.099598
Action reg: 0.003964
  l1.weight: grad_norm = 0.002617
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.002250
Total gradient norm: 0.016089
=== Actor Training Debug (Iteration 1489) ===
Q mean: -6.184804
Q std: 6.023947
Actor loss: 6.188777
Action reg: 0.003973
  l1.weight: grad_norm = 0.004511
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.004742
Total gradient norm: 0.023512
=== Actor Training Debug (Iteration 1490) ===
Q mean: -6.423049
Q std: 6.804859
Actor loss: 6.427016
Action reg: 0.003967
  l1.weight: grad_norm = 0.006536
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.005664
Total gradient norm: 0.026257
=== Actor Training Debug (Iteration 1491) ===
Q mean: -6.443677
Q std: 6.412919
Actor loss: 6.447667
Action reg: 0.003990
  l1.weight: grad_norm = 0.007475
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.005673
Total gradient norm: 0.017755
=== Actor Training Debug (Iteration 1492) ===
Q mean: -6.492549
Q std: 6.332501
Actor loss: 6.496502
Action reg: 0.003954
  l1.weight: grad_norm = 0.003495
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.003297
Total gradient norm: 0.013718
=== Actor Training Debug (Iteration 1493) ===
Q mean: -6.388810
Q std: 6.522811
Actor loss: 6.392790
Action reg: 0.003980
  l1.weight: grad_norm = 0.010982
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.009903
Total gradient norm: 0.049105
=== Actor Training Debug (Iteration 1494) ===
Q mean: -5.401816
Q std: 6.512856
Actor loss: 5.405802
Action reg: 0.003985
  l1.weight: grad_norm = 0.010994
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.010172
Total gradient norm: 0.062775
=== Actor Training Debug (Iteration 1495) ===
Q mean: -5.970105
Q std: 6.817683
Actor loss: 5.974090
Action reg: 0.003984
  l1.weight: grad_norm = 0.000493
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.000412
Total gradient norm: 0.001795
=== Actor Training Debug (Iteration 1496) ===
Q mean: -5.208817
Q std: 6.052433
Actor loss: 5.212804
Action reg: 0.003987
  l1.weight: grad_norm = 0.005641
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.005999
Total gradient norm: 0.025005
=== Actor Training Debug (Iteration 1497) ===
Q mean: -6.630610
Q std: 6.240908
Actor loss: 6.634597
Action reg: 0.003987
  l1.weight: grad_norm = 0.013031
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.013496
Total gradient norm: 0.087756
=== Actor Training Debug (Iteration 1498) ===
Q mean: -6.557831
Q std: 6.407663
Actor loss: 6.561796
Action reg: 0.003965
  l1.weight: grad_norm = 0.008540
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.007409
Total gradient norm: 0.037193
=== Actor Training Debug (Iteration 1499) ===
Q mean: -7.589364
Q std: 7.049666
Actor loss: 7.593328
Action reg: 0.003964
  l1.weight: grad_norm = 0.005079
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.004714
Total gradient norm: 0.025019
=== Actor Training Debug (Iteration 1500) ===
Q mean: -6.890643
Q std: 6.882847
Actor loss: 6.894607
Action reg: 0.003964
  l1.weight: grad_norm = 0.005309
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.005121
Total gradient norm: 0.025927
  Average reward: -330.918 | Average length: 100.0
Evaluation at episode 65: -330.918
=== Actor Training Debug (Iteration 1501) ===
Q mean: -6.400082
Q std: 6.923941
Actor loss: 6.404058
Action reg: 0.003976
  l1.weight: grad_norm = 0.002817
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.002405
Total gradient norm: 0.016779
=== Actor Training Debug (Iteration 1502) ===
Q mean: -5.842587
Q std: 6.508927
Actor loss: 5.846558
Action reg: 0.003971
  l1.weight: grad_norm = 0.004423
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.003622
Total gradient norm: 0.015787
=== Actor Training Debug (Iteration 1503) ===
Q mean: -6.396188
Q std: 6.838027
Actor loss: 6.400150
Action reg: 0.003962
  l1.weight: grad_norm = 0.003589
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.003828
Total gradient norm: 0.026100
=== Actor Training Debug (Iteration 1504) ===
Q mean: -6.435963
Q std: 5.672373
Actor loss: 6.439947
Action reg: 0.003984
  l1.weight: grad_norm = 0.012183
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.011104
Total gradient norm: 0.063678
=== Actor Training Debug (Iteration 1505) ===
Q mean: -6.169481
Q std: 6.607841
Actor loss: 6.173467
Action reg: 0.003985
  l1.weight: grad_norm = 0.007655
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.006822
Total gradient norm: 0.039420
=== Actor Training Debug (Iteration 1506) ===
Q mean: -6.326238
Q std: 6.582289
Actor loss: 6.330218
Action reg: 0.003980
  l1.weight: grad_norm = 0.022132
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.017230
Total gradient norm: 0.091120
=== Actor Training Debug (Iteration 1507) ===
Q mean: -7.115053
Q std: 7.149064
Actor loss: 7.119036
Action reg: 0.003983
  l1.weight: grad_norm = 0.002273
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.002328
Total gradient norm: 0.014919
=== Actor Training Debug (Iteration 1508) ===
Q mean: -6.658069
Q std: 6.401268
Actor loss: 6.662051
Action reg: 0.003983
  l1.weight: grad_norm = 0.009228
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.008405
Total gradient norm: 0.059119
=== Actor Training Debug (Iteration 1509) ===
Q mean: -6.165215
Q std: 6.753018
Actor loss: 6.169189
Action reg: 0.003974
  l1.weight: grad_norm = 0.011285
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.010877
Total gradient norm: 0.062995
=== Actor Training Debug (Iteration 1510) ===
Q mean: -6.479105
Q std: 6.557616
Actor loss: 6.483073
Action reg: 0.003968
  l1.weight: grad_norm = 0.021042
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.022237
Total gradient norm: 0.197649
=== Actor Training Debug (Iteration 1511) ===
Q mean: -6.503748
Q std: 6.509309
Actor loss: 6.507721
Action reg: 0.003973
  l1.weight: grad_norm = 0.004050
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.004123
Total gradient norm: 0.034473
=== Actor Training Debug (Iteration 1512) ===
Q mean: -6.434975
Q std: 6.790065
Actor loss: 6.438934
Action reg: 0.003960
  l1.weight: grad_norm = 0.013519
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.012277
Total gradient norm: 0.068275
=== Actor Training Debug (Iteration 1513) ===
Q mean: -6.615268
Q std: 7.036390
Actor loss: 6.619230
Action reg: 0.003963
  l1.weight: grad_norm = 0.059700
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.078000
Total gradient norm: 0.789388
=== Actor Training Debug (Iteration 1514) ===
Q mean: -6.354598
Q std: 6.785870
Actor loss: 6.358569
Action reg: 0.003970
  l1.weight: grad_norm = 0.031605
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.038643
Total gradient norm: 0.404946
=== Actor Training Debug (Iteration 1515) ===
Q mean: -6.298508
Q std: 6.594527
Actor loss: 6.302430
Action reg: 0.003922
  l1.weight: grad_norm = 0.026376
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.030535
Total gradient norm: 0.273358
=== Actor Training Debug (Iteration 1516) ===
Q mean: -6.561568
Q std: 6.609092
Actor loss: 6.565507
Action reg: 0.003939
  l1.weight: grad_norm = 0.011738
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.011462
Total gradient norm: 0.075429
=== Actor Training Debug (Iteration 1517) ===
Q mean: -6.796039
Q std: 6.753769
Actor loss: 6.799990
Action reg: 0.003951
  l1.weight: grad_norm = 0.010823
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.012514
Total gradient norm: 0.124489
=== Actor Training Debug (Iteration 1518) ===
Q mean: -7.087181
Q std: 7.023197
Actor loss: 7.091150
Action reg: 0.003969
  l1.weight: grad_norm = 0.008750
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.009576
Total gradient norm: 0.084636
=== Actor Training Debug (Iteration 1519) ===
Q mean: -6.145260
Q std: 6.437010
Actor loss: 6.149223
Action reg: 0.003963
  l1.weight: grad_norm = 0.015288
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.014944
Total gradient norm: 0.090040
=== Actor Training Debug (Iteration 1520) ===
Q mean: -6.165199
Q std: 6.737331
Actor loss: 6.169165
Action reg: 0.003966
  l1.weight: grad_norm = 0.018956
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.019614
Total gradient norm: 0.162545
=== Actor Training Debug (Iteration 1521) ===
Q mean: -5.874166
Q std: 6.901896
Actor loss: 5.878147
Action reg: 0.003982
  l1.weight: grad_norm = 0.010854
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.011582
Total gradient norm: 0.082776
=== Actor Training Debug (Iteration 1522) ===
Q mean: -6.562387
Q std: 6.820863
Actor loss: 6.566361
Action reg: 0.003974
  l1.weight: grad_norm = 0.004290
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.003892
Total gradient norm: 0.018393
=== Actor Training Debug (Iteration 1523) ===
Q mean: -6.616255
Q std: 6.983942
Actor loss: 6.620229
Action reg: 0.003974
  l1.weight: grad_norm = 0.011362
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.012208
Total gradient norm: 0.092073
=== Actor Training Debug (Iteration 1524) ===
Q mean: -6.837735
Q std: 6.892368
Actor loss: 6.841712
Action reg: 0.003977
  l1.weight: grad_norm = 0.010332
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.010457
Total gradient norm: 0.082757
=== Actor Training Debug (Iteration 1525) ===
Q mean: -6.969240
Q std: 6.978634
Actor loss: 6.973205
Action reg: 0.003964
  l1.weight: grad_norm = 0.007131
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.006934
Total gradient norm: 0.054741
=== Actor Training Debug (Iteration 1526) ===
Q mean: -5.969683
Q std: 6.722976
Actor loss: 5.973641
Action reg: 0.003959
  l1.weight: grad_norm = 0.015107
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.016576
Total gradient norm: 0.141309
=== Actor Training Debug (Iteration 1527) ===
Q mean: -6.139898
Q std: 6.973885
Actor loss: 6.143858
Action reg: 0.003960
  l1.weight: grad_norm = 0.006787
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.007654
Total gradient norm: 0.070295
=== Actor Training Debug (Iteration 1528) ===
Q mean: -6.329144
Q std: 6.690967
Actor loss: 6.333127
Action reg: 0.003983
  l1.weight: grad_norm = 0.009379
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.011798
Total gradient norm: 0.106966
=== Actor Training Debug (Iteration 1529) ===
Q mean: -6.837637
Q std: 6.659981
Actor loss: 6.841587
Action reg: 0.003949
  l1.weight: grad_norm = 0.026153
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.024957
Total gradient norm: 0.207931
=== Actor Training Debug (Iteration 1530) ===
Q mean: -6.540019
Q std: 6.545343
Actor loss: 6.544002
Action reg: 0.003982
  l1.weight: grad_norm = 0.018255
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.019995
Total gradient norm: 0.105810
=== Actor Training Debug (Iteration 1531) ===
Q mean: -6.490993
Q std: 7.331717
Actor loss: 6.494977
Action reg: 0.003984
  l1.weight: grad_norm = 0.016287
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.014207
Total gradient norm: 0.089169
=== Actor Training Debug (Iteration 1532) ===
Q mean: -6.415769
Q std: 7.191448
Actor loss: 6.419755
Action reg: 0.003987
  l1.weight: grad_norm = 0.003063
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.002952
Total gradient norm: 0.023309
=== Actor Training Debug (Iteration 1533) ===
Q mean: -5.372066
Q std: 6.013390
Actor loss: 5.376044
Action reg: 0.003978
  l1.weight: grad_norm = 0.007259
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.006320
Total gradient norm: 0.037087
=== Actor Training Debug (Iteration 1534) ===
Q mean: -6.982266
Q std: 7.127377
Actor loss: 6.986228
Action reg: 0.003961
  l1.weight: grad_norm = 0.008680
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.008687
Total gradient norm: 0.041321
=== Actor Training Debug (Iteration 1535) ===
Q mean: -7.297791
Q std: 6.964596
Actor loss: 7.301762
Action reg: 0.003971
  l1.weight: grad_norm = 0.015730
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.014035
Total gradient norm: 0.074010
=== Actor Training Debug (Iteration 1536) ===
Q mean: -7.102067
Q std: 7.041309
Actor loss: 7.106044
Action reg: 0.003977
  l1.weight: grad_norm = 0.006239
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.005867
Total gradient norm: 0.039684
=== Actor Training Debug (Iteration 1537) ===
Q mean: -6.714040
Q std: 6.957237
Actor loss: 6.718023
Action reg: 0.003983
  l1.weight: grad_norm = 0.001585
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.001584
Total gradient norm: 0.010379
=== Actor Training Debug (Iteration 1538) ===
Q mean: -6.679585
Q std: 6.887448
Actor loss: 6.683573
Action reg: 0.003988
  l1.weight: grad_norm = 0.005174
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.004908
Total gradient norm: 0.032473
=== Actor Training Debug (Iteration 1539) ===
Q mean: -6.351058
Q std: 6.889568
Actor loss: 6.355025
Action reg: 0.003967
  l1.weight: grad_norm = 0.010904
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.010137
Total gradient norm: 0.067065
=== Actor Training Debug (Iteration 1540) ===
Q mean: -6.613298
Q std: 6.661301
Actor loss: 6.617283
Action reg: 0.003985
  l1.weight: grad_norm = 0.013177
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.015198
Total gradient norm: 0.114974
=== Actor Training Debug (Iteration 1541) ===
Q mean: -7.348741
Q std: 7.140247
Actor loss: 7.352718
Action reg: 0.003977
  l1.weight: grad_norm = 0.007692
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.008245
Total gradient norm: 0.067416
=== Actor Training Debug (Iteration 1542) ===
Q mean: -6.031812
Q std: 6.248253
Actor loss: 6.035793
Action reg: 0.003981
  l1.weight: grad_norm = 0.010464
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.009844
Total gradient norm: 0.069401
=== Actor Training Debug (Iteration 1543) ===
Q mean: -6.369569
Q std: 6.431149
Actor loss: 6.373552
Action reg: 0.003983
  l1.weight: grad_norm = 0.017458
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.019422
Total gradient norm: 0.124771
=== Actor Training Debug (Iteration 1544) ===
Q mean: -6.279652
Q std: 6.528193
Actor loss: 6.283625
Action reg: 0.003973
  l1.weight: grad_norm = 0.017150
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.017817
Total gradient norm: 0.155536
=== Actor Training Debug (Iteration 1545) ===
Q mean: -7.330646
Q std: 6.980399
Actor loss: 7.334624
Action reg: 0.003979
  l1.weight: grad_norm = 0.014700
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.014101
Total gradient norm: 0.105794
=== Actor Training Debug (Iteration 1546) ===
Q mean: -6.512507
Q std: 7.442846
Actor loss: 6.516490
Action reg: 0.003983
  l1.weight: grad_norm = 0.008145
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.007824
Total gradient norm: 0.032267
=== Actor Training Debug (Iteration 1547) ===
Q mean: -6.659011
Q std: 7.114912
Actor loss: 6.663002
Action reg: 0.003991
  l1.weight: grad_norm = 0.017940
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.017351
Total gradient norm: 0.110078
=== Actor Training Debug (Iteration 1548) ===
Q mean: -7.383173
Q std: 7.315598
Actor loss: 7.387151
Action reg: 0.003978
  l1.weight: grad_norm = 0.020368
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.018634
Total gradient norm: 0.120453
=== Actor Training Debug (Iteration 1549) ===
Q mean: -5.796827
Q std: 6.497707
Actor loss: 5.800815
Action reg: 0.003988
  l1.weight: grad_norm = 0.011304
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.009705
Total gradient norm: 0.073849
=== Actor Training Debug (Iteration 1550) ===
Q mean: -7.354448
Q std: 7.355005
Actor loss: 7.358424
Action reg: 0.003976
  l1.weight: grad_norm = 0.011770
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.012165
Total gradient norm: 0.063303
=== Actor Training Debug (Iteration 1551) ===
Q mean: -6.605462
Q std: 6.668707
Actor loss: 6.609445
Action reg: 0.003983
  l1.weight: grad_norm = 0.011215
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.012693
Total gradient norm: 0.103734
=== Actor Training Debug (Iteration 1552) ===
Q mean: -6.154584
Q std: 6.947211
Actor loss: 6.158575
Action reg: 0.003991
  l1.weight: grad_norm = 0.029187
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.026047
Total gradient norm: 0.145746
=== Actor Training Debug (Iteration 1553) ===
Q mean: -6.558064
Q std: 7.021576
Actor loss: 6.562034
Action reg: 0.003971
  l1.weight: grad_norm = 0.011399
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.012968
Total gradient norm: 0.093328
=== Actor Training Debug (Iteration 1554) ===
Q mean: -6.802715
Q std: 6.466528
Actor loss: 6.806703
Action reg: 0.003987
  l1.weight: grad_norm = 0.017999
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.017059
Total gradient norm: 0.120498
=== Actor Training Debug (Iteration 1555) ===
Q mean: -7.341314
Q std: 6.461253
Actor loss: 7.345291
Action reg: 0.003977
  l1.weight: grad_norm = 0.003930
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.004164
Total gradient norm: 0.027104
=== Actor Training Debug (Iteration 1556) ===
Q mean: -5.419168
Q std: 6.590563
Actor loss: 5.423139
Action reg: 0.003970
  l1.weight: grad_norm = 0.015115
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.013199
Total gradient norm: 0.072558
=== Actor Training Debug (Iteration 1557) ===
Q mean: -6.939760
Q std: 7.134768
Actor loss: 6.943686
Action reg: 0.003926
  l1.weight: grad_norm = 0.013473
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.011876
Total gradient norm: 0.058443
=== Actor Training Debug (Iteration 1558) ===
Q mean: -6.775216
Q std: 7.219034
Actor loss: 6.779189
Action reg: 0.003973
  l1.weight: grad_norm = 0.028570
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.033772
Total gradient norm: 0.394705
=== Actor Training Debug (Iteration 1559) ===
Q mean: -6.725764
Q std: 6.585901
Actor loss: 6.729729
Action reg: 0.003965
  l1.weight: grad_norm = 0.017709
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.019646
Total gradient norm: 0.179755
=== Actor Training Debug (Iteration 1560) ===
Q mean: -5.888206
Q std: 6.411121
Actor loss: 5.892168
Action reg: 0.003962
  l1.weight: grad_norm = 0.006619
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.006173
Total gradient norm: 0.034912
=== Actor Training Debug (Iteration 1561) ===
Q mean: -6.468991
Q std: 7.465561
Actor loss: 6.472956
Action reg: 0.003965
  l1.weight: grad_norm = 0.021565
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.022817
Total gradient norm: 0.204550
=== Actor Training Debug (Iteration 1562) ===
Q mean: -6.653004
Q std: 6.790047
Actor loss: 6.656973
Action reg: 0.003969
  l1.weight: grad_norm = 0.015919
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.019140
Total gradient norm: 0.190133
=== Actor Training Debug (Iteration 1563) ===
Q mean: -6.616123
Q std: 6.569955
Actor loss: 6.620085
Action reg: 0.003962
  l1.weight: grad_norm = 0.022543
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.020887
Total gradient norm: 0.110328
=== Actor Training Debug (Iteration 1564) ===
Q mean: -7.301548
Q std: 6.833683
Actor loss: 7.305497
Action reg: 0.003949
  l1.weight: grad_norm = 0.026066
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.027153
Total gradient norm: 0.206237
=== Actor Training Debug (Iteration 1565) ===
Q mean: -6.970250
Q std: 7.245336
Actor loss: 6.974205
Action reg: 0.003954
  l1.weight: grad_norm = 0.022846
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.022430
Total gradient norm: 0.169109
=== Actor Training Debug (Iteration 1566) ===
Q mean: -6.363800
Q std: 6.739118
Actor loss: 6.367780
Action reg: 0.003980
  l1.weight: grad_norm = 0.014380
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.015900
Total gradient norm: 0.131317
=== Actor Training Debug (Iteration 1567) ===
Q mean: -5.844985
Q std: 6.548975
Actor loss: 5.848961
Action reg: 0.003977
  l1.weight: grad_norm = 0.022244
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.020539
Total gradient norm: 0.142944
=== Actor Training Debug (Iteration 1568) ===
Q mean: -6.541403
Q std: 6.363536
Actor loss: 6.545381
Action reg: 0.003978
  l1.weight: grad_norm = 0.015276
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.011906
Total gradient norm: 0.066091
=== Actor Training Debug (Iteration 1569) ===
Q mean: -7.159112
Q std: 7.304303
Actor loss: 7.163077
Action reg: 0.003965
  l1.weight: grad_norm = 0.040821
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.040831
Total gradient norm: 0.307019
=== Actor Training Debug (Iteration 1570) ===
Q mean: -6.512882
Q std: 7.074698
Actor loss: 6.516871
Action reg: 0.003989
  l1.weight: grad_norm = 0.004592
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.005852
Total gradient norm: 0.048080
=== Actor Training Debug (Iteration 1571) ===
Q mean: -6.554766
Q std: 6.845149
Actor loss: 6.558731
Action reg: 0.003965
  l1.weight: grad_norm = 0.024089
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.022030
Total gradient norm: 0.174915
=== Actor Training Debug (Iteration 1572) ===
Q mean: -6.138263
Q std: 6.650621
Actor loss: 6.142229
Action reg: 0.003966
  l1.weight: grad_norm = 0.008185
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.008150
Total gradient norm: 0.043300
=== Actor Training Debug (Iteration 1573) ===
Q mean: -6.069353
Q std: 6.252542
Actor loss: 6.073322
Action reg: 0.003970
  l1.weight: grad_norm = 0.007112
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.007591
Total gradient norm: 0.054620
=== Actor Training Debug (Iteration 1574) ===
Q mean: -6.449183
Q std: 6.665299
Actor loss: 6.453151
Action reg: 0.003969
  l1.weight: grad_norm = 0.016594
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.015551
Total gradient norm: 0.098795
=== Actor Training Debug (Iteration 1575) ===
Q mean: -6.110818
Q std: 6.260574
Actor loss: 6.114763
Action reg: 0.003945
  l1.weight: grad_norm = 0.010297
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.011441
Total gradient norm: 0.078198
=== Actor Training Debug (Iteration 1576) ===
Q mean: -6.469854
Q std: 6.993787
Actor loss: 6.473808
Action reg: 0.003954
  l1.weight: grad_norm = 0.009111
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.008279
Total gradient norm: 0.061439
=== Actor Training Debug (Iteration 1577) ===
Q mean: -6.859011
Q std: 7.583183
Actor loss: 6.862978
Action reg: 0.003967
  l1.weight: grad_norm = 0.017608
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.018175
Total gradient norm: 0.115190
=== Actor Training Debug (Iteration 1578) ===
Q mean: -6.739956
Q std: 6.790498
Actor loss: 6.743941
Action reg: 0.003985
  l1.weight: grad_norm = 0.009244
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.008897
Total gradient norm: 0.062965
=== Actor Training Debug (Iteration 1579) ===
Q mean: -7.093981
Q std: 7.360106
Actor loss: 7.097959
Action reg: 0.003978
  l1.weight: grad_norm = 0.022879
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.020629
Total gradient norm: 0.133622
=== Actor Training Debug (Iteration 1580) ===
Q mean: -6.587199
Q std: 7.185697
Actor loss: 6.591163
Action reg: 0.003964
  l1.weight: grad_norm = 0.017890
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.015478
Total gradient norm: 0.097034
=== Actor Training Debug (Iteration 1581) ===
Q mean: -6.200787
Q std: 6.957627
Actor loss: 6.204755
Action reg: 0.003968
  l1.weight: grad_norm = 0.005911
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.005452
Total gradient norm: 0.038155
=== Actor Training Debug (Iteration 1582) ===
Q mean: -6.666220
Q std: 6.455476
Actor loss: 6.670172
Action reg: 0.003952
  l1.weight: grad_norm = 0.016503
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.016599
Total gradient norm: 0.100601
=== Actor Training Debug (Iteration 1583) ===
Q mean: -6.947556
Q std: 7.284861
Actor loss: 6.951534
Action reg: 0.003979
  l1.weight: grad_norm = 0.007419
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.006469
Total gradient norm: 0.030814
=== Actor Training Debug (Iteration 1584) ===
Q mean: -6.275856
Q std: 6.252403
Actor loss: 6.279834
Action reg: 0.003978
  l1.weight: grad_norm = 0.009633
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.009944
Total gradient norm: 0.075551
=== Actor Training Debug (Iteration 1585) ===
Q mean: -6.891929
Q std: 6.805177
Actor loss: 6.895913
Action reg: 0.003984
  l1.weight: grad_norm = 0.027257
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.023418
Total gradient norm: 0.131387
=== Actor Training Debug (Iteration 1586) ===
Q mean: -6.369261
Q std: 7.085645
Actor loss: 6.373241
Action reg: 0.003980
  l1.weight: grad_norm = 0.005199
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.004354
Total gradient norm: 0.029751
=== Actor Training Debug (Iteration 1587) ===
Q mean: -6.247782
Q std: 6.741481
Actor loss: 6.251740
Action reg: 0.003957
  l1.weight: grad_norm = 0.012843
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.012442
Total gradient norm: 0.077055
=== Actor Training Debug (Iteration 1588) ===
Q mean: -6.873318
Q std: 7.429736
Actor loss: 6.877278
Action reg: 0.003960
  l1.weight: grad_norm = 0.021497
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.021041
Total gradient norm: 0.128859
=== Actor Training Debug (Iteration 1589) ===
Q mean: -6.462402
Q std: 7.573868
Actor loss: 6.466377
Action reg: 0.003975
  l1.weight: grad_norm = 0.010407
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.008732
Total gradient norm: 0.049508
=== Actor Training Debug (Iteration 1590) ===
Q mean: -6.465664
Q std: 6.825791
Actor loss: 6.469641
Action reg: 0.003977
  l1.weight: grad_norm = 0.007115
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.007250
Total gradient norm: 0.042540
=== Actor Training Debug (Iteration 1591) ===
Q mean: -7.472557
Q std: 7.666132
Actor loss: 7.476539
Action reg: 0.003982
  l1.weight: grad_norm = 0.013143
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.013604
Total gradient norm: 0.093077
=== Actor Training Debug (Iteration 1592) ===
Q mean: -7.948842
Q std: 7.135602
Actor loss: 7.952825
Action reg: 0.003983
  l1.weight: grad_norm = 0.017727
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.017012
Total gradient norm: 0.106288
=== Actor Training Debug (Iteration 1593) ===
Q mean: -6.554447
Q std: 6.838641
Actor loss: 6.558423
Action reg: 0.003975
  l1.weight: grad_norm = 0.019381
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.018910
Total gradient norm: 0.108923
=== Actor Training Debug (Iteration 1594) ===
Q mean: -6.525029
Q std: 7.211749
Actor loss: 6.528998
Action reg: 0.003969
  l1.weight: grad_norm = 0.012008
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.012182
Total gradient norm: 0.081291
=== Actor Training Debug (Iteration 1595) ===
Q mean: -6.968013
Q std: 7.562826
Actor loss: 6.971969
Action reg: 0.003956
  l1.weight: grad_norm = 0.021836
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.022709
Total gradient norm: 0.140456
=== Actor Training Debug (Iteration 1596) ===
Q mean: -6.676515
Q std: 7.106205
Actor loss: 6.680487
Action reg: 0.003973
  l1.weight: grad_norm = 0.007009
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.006434
Total gradient norm: 0.039179
=== Actor Training Debug (Iteration 1597) ===
Q mean: -6.283523
Q std: 6.676613
Actor loss: 6.287491
Action reg: 0.003968
  l1.weight: grad_norm = 0.020361
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.018655
Total gradient norm: 0.111897
=== Actor Training Debug (Iteration 1598) ===
Q mean: -6.302273
Q std: 6.887616
Actor loss: 6.306231
Action reg: 0.003957
  l1.weight: grad_norm = 0.010110
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.010389
Total gradient norm: 0.083971
=== Actor Training Debug (Iteration 1599) ===
Q mean: -6.553358
Q std: 6.561033
Actor loss: 6.557327
Action reg: 0.003969
  l1.weight: grad_norm = 0.012034
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.013197
Total gradient norm: 0.084513
=== Actor Training Debug (Iteration 1600) ===
Q mean: -6.995438
Q std: 7.042964
Actor loss: 6.999374
Action reg: 0.003936
  l1.weight: grad_norm = 0.017364
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.016525
Total gradient norm: 0.110627
=== Actor Training Debug (Iteration 1601) ===
Q mean: -6.758883
Q std: 6.320573
Actor loss: 6.762866
Action reg: 0.003982
  l1.weight: grad_norm = 0.014339
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.012369
Total gradient norm: 0.056102
=== Actor Training Debug (Iteration 1602) ===
Q mean: -6.128865
Q std: 6.557933
Actor loss: 6.132809
Action reg: 0.003943
  l1.weight: grad_norm = 0.008909
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.009574
Total gradient norm: 0.074751
=== Actor Training Debug (Iteration 1603) ===
Q mean: -6.780717
Q std: 7.256402
Actor loss: 6.784683
Action reg: 0.003966
  l1.weight: grad_norm = 0.012068
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.010362
Total gradient norm: 0.063731
=== Actor Training Debug (Iteration 1604) ===
Q mean: -7.620270
Q std: 7.571199
Actor loss: 7.624254
Action reg: 0.003984
  l1.weight: grad_norm = 0.016836
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.016013
Total gradient norm: 0.085201
=== Actor Training Debug (Iteration 1605) ===
Q mean: -5.718056
Q std: 6.601106
Actor loss: 5.721995
Action reg: 0.003940
  l1.weight: grad_norm = 0.014720
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.016248
Total gradient norm: 0.124009
=== Actor Training Debug (Iteration 1606) ===
Q mean: -6.129030
Q std: 6.755385
Actor loss: 6.133001
Action reg: 0.003971
  l1.weight: grad_norm = 0.016998
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.016072
Total gradient norm: 0.113805
=== Actor Training Debug (Iteration 1607) ===
Q mean: -5.936497
Q std: 6.913946
Actor loss: 5.940464
Action reg: 0.003967
  l1.weight: grad_norm = 0.015486
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.015995
Total gradient norm: 0.119173
=== Actor Training Debug (Iteration 1608) ===
Q mean: -6.748307
Q std: 6.934710
Actor loss: 6.752280
Action reg: 0.003972
  l1.weight: grad_norm = 0.033252
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.031291
Total gradient norm: 0.204075
=== Actor Training Debug (Iteration 1609) ===
Q mean: -6.500822
Q std: 6.564813
Actor loss: 6.504804
Action reg: 0.003982
  l1.weight: grad_norm = 0.014236
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.014353
Total gradient norm: 0.090565
=== Actor Training Debug (Iteration 1610) ===
Q mean: -6.697713
Q std: 7.328700
Actor loss: 6.701672
Action reg: 0.003959
  l1.weight: grad_norm = 0.009790
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.009496
Total gradient norm: 0.061506
=== Actor Training Debug (Iteration 1611) ===
Q mean: -6.460137
Q std: 6.824288
Actor loss: 6.464118
Action reg: 0.003980
  l1.weight: grad_norm = 0.006944
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.006866
Total gradient norm: 0.048613
=== Actor Training Debug (Iteration 1612) ===
Q mean: -6.847965
Q std: 6.810668
Actor loss: 6.851932
Action reg: 0.003967
  l1.weight: grad_norm = 0.015154
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.013213
Total gradient norm: 0.063056
=== Actor Training Debug (Iteration 1613) ===
Q mean: -6.144022
Q std: 6.571763
Actor loss: 6.147968
Action reg: 0.003946
  l1.weight: grad_norm = 0.017802
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.017228
Total gradient norm: 0.123455
=== Actor Training Debug (Iteration 1614) ===
Q mean: -6.265646
Q std: 6.192182
Actor loss: 6.269619
Action reg: 0.003973
  l1.weight: grad_norm = 0.024411
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.023984
Total gradient norm: 0.165633
=== Actor Training Debug (Iteration 1615) ===
Q mean: -6.574570
Q std: 6.999292
Actor loss: 6.578523
Action reg: 0.003953
  l1.weight: grad_norm = 0.020820
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.020689
Total gradient norm: 0.139368
=== Actor Training Debug (Iteration 1616) ===
Q mean: -7.054794
Q std: 7.218282
Actor loss: 7.058747
Action reg: 0.003953
  l1.weight: grad_norm = 0.011239
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.010224
Total gradient norm: 0.074020
=== Actor Training Debug (Iteration 1617) ===
Q mean: -7.406106
Q std: 7.332829
Actor loss: 7.410093
Action reg: 0.003987
  l1.weight: grad_norm = 0.014294
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.014077
Total gradient norm: 0.093604
=== Actor Training Debug (Iteration 1618) ===
Q mean: -6.442163
Q std: 7.126856
Actor loss: 6.446136
Action reg: 0.003973
  l1.weight: grad_norm = 0.012053
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.011432
Total gradient norm: 0.073964
=== Actor Training Debug (Iteration 1619) ===
Q mean: -6.229414
Q std: 7.315410
Actor loss: 6.233396
Action reg: 0.003982
  l1.weight: grad_norm = 0.001255
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.001111
Total gradient norm: 0.006263
=== Actor Training Debug (Iteration 1620) ===
Q mean: -6.262814
Q std: 6.737649
Actor loss: 6.266781
Action reg: 0.003968
  l1.weight: grad_norm = 0.017360
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.013914
Total gradient norm: 0.077204
=== Actor Training Debug (Iteration 1621) ===
Q mean: -6.489279
Q std: 6.906369
Actor loss: 6.493244
Action reg: 0.003965
  l1.weight: grad_norm = 0.020725
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.018262
Total gradient norm: 0.092278
=== Actor Training Debug (Iteration 1622) ===
Q mean: -6.810031
Q std: 7.214961
Actor loss: 6.813977
Action reg: 0.003946
  l1.weight: grad_norm = 0.032686
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.032894
Total gradient norm: 0.227265
=== Actor Training Debug (Iteration 1623) ===
Q mean: -5.959669
Q std: 6.842839
Actor loss: 5.963620
Action reg: 0.003951
  l1.weight: grad_norm = 0.009781
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.008918
Total gradient norm: 0.048144
=== Actor Training Debug (Iteration 1624) ===
Q mean: -6.567046
Q std: 6.962528
Actor loss: 6.571017
Action reg: 0.003971
  l1.weight: grad_norm = 0.007706
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.006849
Total gradient norm: 0.046446
=== Actor Training Debug (Iteration 1625) ===
Q mean: -7.035141
Q std: 7.587260
Actor loss: 7.039113
Action reg: 0.003972
  l1.weight: grad_norm = 0.010656
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.010574
Total gradient norm: 0.082499
=== Actor Training Debug (Iteration 1626) ===
Q mean: -5.915326
Q std: 6.275628
Actor loss: 5.919303
Action reg: 0.003977
  l1.weight: grad_norm = 0.008386
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.008131
Total gradient norm: 0.052536
=== Actor Training Debug (Iteration 1627) ===
Q mean: -6.622323
Q std: 7.209088
Actor loss: 6.626313
Action reg: 0.003990
  l1.weight: grad_norm = 0.019238
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.018935
Total gradient norm: 0.118509
=== Actor Training Debug (Iteration 1628) ===
Q mean: -6.895516
Q std: 6.961491
Actor loss: 6.899489
Action reg: 0.003974
  l1.weight: grad_norm = 0.004462
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.005165
Total gradient norm: 0.043001
=== Actor Training Debug (Iteration 1629) ===
Q mean: -7.297355
Q std: 7.200742
Actor loss: 7.301328
Action reg: 0.003973
  l1.weight: grad_norm = 0.009532
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.009530
Total gradient norm: 0.057795
=== Actor Training Debug (Iteration 1630) ===
Q mean: -7.502967
Q std: 7.203740
Actor loss: 7.506956
Action reg: 0.003989
  l1.weight: grad_norm = 0.007328
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.008005
Total gradient norm: 0.059427
=== Actor Training Debug (Iteration 1631) ===
Q mean: -7.045853
Q std: 6.903362
Actor loss: 7.049814
Action reg: 0.003961
  l1.weight: grad_norm = 0.004593
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.003803
Total gradient norm: 0.026627
=== Actor Training Debug (Iteration 1632) ===
Q mean: -7.572723
Q std: 7.287805
Actor loss: 7.576716
Action reg: 0.003993
  l1.weight: grad_norm = 0.013779
  l1.bias: grad_norm = 0.000005
  l2.weight: grad_norm = 0.012967
Total gradient norm: 0.091629
=== Actor Training Debug (Iteration 1633) ===
Q mean: -6.796133
Q std: 7.332095
Actor loss: 6.800110
Action reg: 0.003977
  l1.weight: grad_norm = 0.018946
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.022150
Total gradient norm: 0.160451
=== Actor Training Debug (Iteration 1634) ===
Q mean: -6.682640
Q std: 6.986088
Actor loss: 6.686612
Action reg: 0.003972
  l1.weight: grad_norm = 0.012598
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.010608
Total gradient norm: 0.052375
=== Actor Training Debug (Iteration 1635) ===
Q mean: -6.629198
Q std: 7.594096
Actor loss: 6.633176
Action reg: 0.003978
  l1.weight: grad_norm = 0.010402
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.008443
Total gradient norm: 0.048491
=== Actor Training Debug (Iteration 1636) ===
Q mean: -5.834273
Q std: 6.766531
Actor loss: 5.838259
Action reg: 0.003986
  l1.weight: grad_norm = 0.013345
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.012606
Total gradient norm: 0.085428
=== Actor Training Debug (Iteration 1637) ===
Q mean: -6.896291
Q std: 7.823795
Actor loss: 6.900242
Action reg: 0.003951
  l1.weight: grad_norm = 0.014085
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.012904
Total gradient norm: 0.075839
=== Actor Training Debug (Iteration 1638) ===
Q mean: -6.983586
Q std: 6.810476
Actor loss: 6.987552
Action reg: 0.003966
  l1.weight: grad_norm = 0.012467
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.011129
Total gradient norm: 0.058068
=== Actor Training Debug (Iteration 1639) ===
Q mean: -7.108853
Q std: 7.191976
Actor loss: 7.112824
Action reg: 0.003971
  l1.weight: grad_norm = 0.007750
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.007177
Total gradient norm: 0.049002
=== Actor Training Debug (Iteration 1640) ===
Q mean: -6.840499
Q std: 6.620523
Actor loss: 6.844463
Action reg: 0.003964
  l1.weight: grad_norm = 0.009708
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.008455
Total gradient norm: 0.043743
=== Actor Training Debug (Iteration 1641) ===
Q mean: -7.392481
Q std: 7.241251
Actor loss: 7.396456
Action reg: 0.003975
  l1.weight: grad_norm = 0.026931
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.025599
Total gradient norm: 0.157234
=== Actor Training Debug (Iteration 1642) ===
Q mean: -7.565232
Q std: 7.431905
Actor loss: 7.569213
Action reg: 0.003981
  l1.weight: grad_norm = 0.024400
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.024399
Total gradient norm: 0.144698
=== Actor Training Debug (Iteration 1643) ===
Q mean: -7.032122
Q std: 7.414643
Actor loss: 7.036086
Action reg: 0.003964
  l1.weight: grad_norm = 0.012314
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.013510
Total gradient norm: 0.107348
=== Actor Training Debug (Iteration 1644) ===
Q mean: -6.161337
Q std: 6.728245
Actor loss: 6.165276
Action reg: 0.003939
  l1.weight: grad_norm = 0.008265
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.008583
Total gradient norm: 0.041220
=== Actor Training Debug (Iteration 1645) ===
Q mean: -6.742175
Q std: 7.028059
Actor loss: 6.746165
Action reg: 0.003990
  l1.weight: grad_norm = 0.014863
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.013932
Total gradient norm: 0.089942
=== Actor Training Debug (Iteration 1646) ===
Q mean: -7.020104
Q std: 7.652236
Actor loss: 7.024082
Action reg: 0.003978
  l1.weight: grad_norm = 0.008207
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.006990
Total gradient norm: 0.053751
=== Actor Training Debug (Iteration 1647) ===
Q mean: -7.157795
Q std: 7.704451
Actor loss: 7.161759
Action reg: 0.003964
  l1.weight: grad_norm = 0.019386
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.018015
Total gradient norm: 0.108458
=== Actor Training Debug (Iteration 1648) ===
Q mean: -6.310613
Q std: 6.988745
Actor loss: 6.314592
Action reg: 0.003980
  l1.weight: grad_norm = 0.018277
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.017205
Total gradient norm: 0.099678
=== Actor Training Debug (Iteration 1649) ===
Q mean: -6.879642
Q std: 6.659824
Actor loss: 6.883602
Action reg: 0.003960
  l1.weight: grad_norm = 0.024513
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.023012
Total gradient norm: 0.170071
=== Actor Training Debug (Iteration 1650) ===
Q mean: -6.656370
Q std: 7.611990
Actor loss: 6.660322
Action reg: 0.003952
  l1.weight: grad_norm = 0.041023
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.034417
Total gradient norm: 0.189817
=== Actor Training Debug (Iteration 1651) ===
Q mean: -7.677358
Q std: 8.148396
Actor loss: 7.681352
Action reg: 0.003994
  l1.weight: grad_norm = 0.005753
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004914
Total gradient norm: 0.023908
=== Actor Training Debug (Iteration 1652) ===
Q mean: -6.467175
Q std: 6.605356
Actor loss: 6.471160
Action reg: 0.003986
  l1.weight: grad_norm = 0.018900
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.015678
Total gradient norm: 0.085116
=== Actor Training Debug (Iteration 1653) ===
Q mean: -7.467428
Q std: 7.332862
Actor loss: 7.471397
Action reg: 0.003969
  l1.weight: grad_norm = 0.012787
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.013220
Total gradient norm: 0.085611
=== Actor Training Debug (Iteration 1654) ===
Q mean: -7.592424
Q std: 7.727486
Actor loss: 7.596410
Action reg: 0.003986
  l1.weight: grad_norm = 0.021710
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.021201
Total gradient norm: 0.140625
=== Actor Training Debug (Iteration 1655) ===
Q mean: -6.923881
Q std: 7.747298
Actor loss: 6.927852
Action reg: 0.003971
  l1.weight: grad_norm = 0.011586
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.010767
Total gradient norm: 0.063999
=== Actor Training Debug (Iteration 1656) ===
Q mean: -6.718293
Q std: 7.584733
Actor loss: 6.722260
Action reg: 0.003967
  l1.weight: grad_norm = 0.007696
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.006866
Total gradient norm: 0.037872
=== Actor Training Debug (Iteration 1657) ===
Q mean: -6.691981
Q std: 7.182628
Actor loss: 6.695939
Action reg: 0.003958
  l1.weight: grad_norm = 0.056196
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.051188
Total gradient norm: 0.344704
=== Actor Training Debug (Iteration 1658) ===
Q mean: -6.338596
Q std: 7.150524
Actor loss: 6.342574
Action reg: 0.003977
  l1.weight: grad_norm = 0.031594
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.030101
Total gradient norm: 0.225002
=== Actor Training Debug (Iteration 1659) ===
Q mean: -6.298726
Q std: 7.123216
Actor loss: 6.302706
Action reg: 0.003980
  l1.weight: grad_norm = 0.021020
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.018823
Total gradient norm: 0.112168
=== Actor Training Debug (Iteration 1660) ===
Q mean: -7.101594
Q std: 7.296033
Actor loss: 7.105558
Action reg: 0.003965
  l1.weight: grad_norm = 0.016692
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.014455
Total gradient norm: 0.064648
=== Actor Training Debug (Iteration 1661) ===
Q mean: -7.344783
Q std: 7.802508
Actor loss: 7.348742
Action reg: 0.003959
  l1.weight: grad_norm = 0.008822
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.008446
Total gradient norm: 0.050881
=== Actor Training Debug (Iteration 1662) ===
Q mean: -6.740848
Q std: 7.250422
Actor loss: 6.744825
Action reg: 0.003977
  l1.weight: grad_norm = 0.007284
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.006495
Total gradient norm: 0.033933
=== Actor Training Debug (Iteration 1663) ===
Q mean: -6.368182
Q std: 7.065756
Actor loss: 6.372119
Action reg: 0.003937
  l1.weight: grad_norm = 0.025084
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.025328
Total gradient norm: 0.175264
=== Actor Training Debug (Iteration 1664) ===
Q mean: -6.313185
Q std: 6.859896
Actor loss: 6.317153
Action reg: 0.003968
  l1.weight: grad_norm = 0.012356
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.011527
Total gradient norm: 0.075621
=== Actor Training Debug (Iteration 1665) ===
Q mean: -6.896826
Q std: 7.527147
Actor loss: 6.900782
Action reg: 0.003956
  l1.weight: grad_norm = 0.008292
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.008633
Total gradient norm: 0.055647
=== Actor Training Debug (Iteration 1666) ===
Q mean: -7.491831
Q std: 7.646586
Actor loss: 7.495809
Action reg: 0.003978
  l1.weight: grad_norm = 0.012929
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.013860
Total gradient norm: 0.088949
=== Actor Training Debug (Iteration 1667) ===
Q mean: -7.174814
Q std: 7.214256
Actor loss: 7.178792
Action reg: 0.003978
  l1.weight: grad_norm = 0.017032
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.017466
Total gradient norm: 0.120143
=== Actor Training Debug (Iteration 1668) ===
Q mean: -7.271821
Q std: 7.554153
Actor loss: 7.275800
Action reg: 0.003980
  l1.weight: grad_norm = 0.011906
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.009554
Total gradient norm: 0.037913
=== Actor Training Debug (Iteration 1669) ===
Q mean: -6.717024
Q std: 7.426465
Actor loss: 6.720997
Action reg: 0.003973
  l1.weight: grad_norm = 0.014848
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.014995
Total gradient norm: 0.098347
=== Actor Training Debug (Iteration 1670) ===
Q mean: -6.655364
Q std: 7.215052
Actor loss: 6.659343
Action reg: 0.003979
  l1.weight: grad_norm = 0.007079
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.007348
Total gradient norm: 0.057531
=== Actor Training Debug (Iteration 1671) ===
Q mean: -6.371260
Q std: 7.085035
Actor loss: 6.375234
Action reg: 0.003975
  l1.weight: grad_norm = 0.011569
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.010529
Total gradient norm: 0.066836
=== Actor Training Debug (Iteration 1672) ===
Q mean: -6.949655
Q std: 7.428624
Actor loss: 6.953630
Action reg: 0.003975
  l1.weight: grad_norm = 0.022227
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.020315
Total gradient norm: 0.139652
=== Actor Training Debug (Iteration 1673) ===
Q mean: -6.874184
Q std: 7.286213
Actor loss: 6.878170
Action reg: 0.003986
  l1.weight: grad_norm = 0.005487
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.005879
Total gradient norm: 0.038154
=== Actor Training Debug (Iteration 1674) ===
Q mean: -6.580235
Q std: 6.948394
Actor loss: 6.584199
Action reg: 0.003965
  l1.weight: grad_norm = 0.012391
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.011684
Total gradient norm: 0.073078
=== Actor Training Debug (Iteration 1675) ===
Q mean: -7.347753
Q std: 7.181077
Actor loss: 7.351734
Action reg: 0.003982
  l1.weight: grad_norm = 0.010981
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.010741
Total gradient norm: 0.061367
=== Actor Training Debug (Iteration 1676) ===
Q mean: -7.242499
Q std: 7.435005
Actor loss: 7.246476
Action reg: 0.003977
  l1.weight: grad_norm = 0.006014
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.006350
Total gradient norm: 0.039555
=== Actor Training Debug (Iteration 1677) ===
Q mean: -6.834274
Q std: 6.718488
Actor loss: 6.838260
Action reg: 0.003986
  l1.weight: grad_norm = 0.018110
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.019076
Total gradient norm: 0.158280
=== Actor Training Debug (Iteration 1678) ===
Q mean: -7.275002
Q std: 7.574926
Actor loss: 7.278986
Action reg: 0.003984
  l1.weight: grad_norm = 0.014979
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.015092
Total gradient norm: 0.122379
=== Actor Training Debug (Iteration 1679) ===
Q mean: -7.487867
Q std: 7.664999
Actor loss: 7.491840
Action reg: 0.003973
  l1.weight: grad_norm = 0.012444
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.011968
Total gradient norm: 0.092492
=== Actor Training Debug (Iteration 1680) ===
Q mean: -6.805305
Q std: 7.104569
Actor loss: 6.809278
Action reg: 0.003972
  l1.weight: grad_norm = 0.013256
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.011670
Total gradient norm: 0.069484
=== Actor Training Debug (Iteration 1681) ===
Q mean: -7.926587
Q std: 7.580462
Actor loss: 7.930559
Action reg: 0.003971
  l1.weight: grad_norm = 0.008266
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.008527
Total gradient norm: 0.066581
=== Actor Training Debug (Iteration 1682) ===
Q mean: -6.268820
Q std: 6.863819
Actor loss: 6.272759
Action reg: 0.003939
  l1.weight: grad_norm = 0.030394
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.028874
Total gradient norm: 0.208805
=== Actor Training Debug (Iteration 1683) ===
Q mean: -6.265868
Q std: 7.188786
Actor loss: 6.269817
Action reg: 0.003949
  l1.weight: grad_norm = 0.027870
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.023566
Total gradient norm: 0.105386
=== Actor Training Debug (Iteration 1684) ===
Q mean: -6.557754
Q std: 6.896020
Actor loss: 6.561729
Action reg: 0.003976
  l1.weight: grad_norm = 0.010076
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.009970
Total gradient norm: 0.053860
=== Actor Training Debug (Iteration 1685) ===
Q mean: -6.416211
Q std: 6.906678
Actor loss: 6.420184
Action reg: 0.003973
  l1.weight: grad_norm = 0.014342
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.011936
Total gradient norm: 0.057882
=== Actor Training Debug (Iteration 1686) ===
Q mean: -6.357971
Q std: 7.219127
Actor loss: 6.361939
Action reg: 0.003968
  l1.weight: grad_norm = 0.014776
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.013366
Total gradient norm: 0.052842
=== Actor Training Debug (Iteration 1687) ===
Q mean: -6.035554
Q std: 7.210586
Actor loss: 6.039505
Action reg: 0.003951
  l1.weight: grad_norm = 0.008521
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.007336
Total gradient norm: 0.030188
=== Actor Training Debug (Iteration 1688) ===
Q mean: -7.466681
Q std: 7.816176
Actor loss: 7.470646
Action reg: 0.003965
  l1.weight: grad_norm = 0.017549
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.014322
Total gradient norm: 0.076515
=== Actor Training Debug (Iteration 1689) ===
Q mean: -7.724855
Q std: 7.004083
Actor loss: 7.728830
Action reg: 0.003975
  l1.weight: grad_norm = 0.046551
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.042129
Total gradient norm: 0.291948
=== Actor Training Debug (Iteration 1690) ===
Q mean: -7.549165
Q std: 7.821005
Actor loss: 7.553148
Action reg: 0.003983
  l1.weight: grad_norm = 0.011535
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.010462
Total gradient norm: 0.049746
=== Actor Training Debug (Iteration 1691) ===
Q mean: -5.982149
Q std: 6.743482
Actor loss: 5.986115
Action reg: 0.003966
  l1.weight: grad_norm = 0.023969
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.020669
Total gradient norm: 0.107717
=== Actor Training Debug (Iteration 1692) ===
Q mean: -7.968290
Q std: 7.909934
Actor loss: 7.972275
Action reg: 0.003985
  l1.weight: grad_norm = 0.011087
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.011110
Total gradient norm: 0.079108
=== Actor Training Debug (Iteration 1693) ===
Q mean: -7.346053
Q std: 7.591475
Actor loss: 7.350019
Action reg: 0.003965
  l1.weight: grad_norm = 0.019139
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.015305
Total gradient norm: 0.075518
=== Actor Training Debug (Iteration 1694) ===
Q mean: -7.307307
Q std: 8.108558
Actor loss: 7.311268
Action reg: 0.003960
  l1.weight: grad_norm = 0.014495
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.015311
Total gradient norm: 0.105500
=== Actor Training Debug (Iteration 1695) ===
Q mean: -6.233296
Q std: 7.598367
Actor loss: 6.237270
Action reg: 0.003974
  l1.weight: grad_norm = 0.007105
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.007147
Total gradient norm: 0.058296
=== Actor Training Debug (Iteration 1696) ===
Q mean: -6.634751
Q std: 7.268132
Actor loss: 6.638723
Action reg: 0.003972
  l1.weight: grad_norm = 0.024703
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.022199
Total gradient norm: 0.138054
=== Actor Training Debug (Iteration 1697) ===
Q mean: -6.427591
Q std: 6.775183
Actor loss: 6.431567
Action reg: 0.003976
  l1.weight: grad_norm = 0.012613
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.012083
Total gradient norm: 0.070653
=== Actor Training Debug (Iteration 1698) ===
Q mean: -7.579839
Q std: 7.802846
Actor loss: 7.583790
Action reg: 0.003951
  l1.weight: grad_norm = 0.031176
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.027440
Total gradient norm: 0.166675
=== Actor Training Debug (Iteration 1699) ===
Q mean: -7.786505
Q std: 7.704206
Actor loss: 7.790483
Action reg: 0.003978
  l1.weight: grad_norm = 0.015833
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.014881
Total gradient norm: 0.093594
=== Actor Training Debug (Iteration 1700) ===
Q mean: -7.171971
Q std: 7.647927
Actor loss: 7.175949
Action reg: 0.003978
  l1.weight: grad_norm = 0.032273
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.027295
Total gradient norm: 0.146043
=== Actor Training Debug (Iteration 1701) ===
Q mean: -6.493605
Q std: 7.600129
Actor loss: 6.497576
Action reg: 0.003972
  l1.weight: grad_norm = 0.011680
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.009627
Total gradient norm: 0.049187
=== Actor Training Debug (Iteration 1702) ===
Q mean: -7.012607
Q std: 6.742663
Actor loss: 7.016581
Action reg: 0.003974
  l1.weight: grad_norm = 0.017911
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.015153
Total gradient norm: 0.066108
=== Actor Training Debug (Iteration 1703) ===
Q mean: -6.217386
Q std: 6.469011
Actor loss: 6.221349
Action reg: 0.003962
  l1.weight: grad_norm = 0.017020
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.016765
Total gradient norm: 0.107185
=== Actor Training Debug (Iteration 1704) ===
Q mean: -7.382277
Q std: 7.981393
Actor loss: 7.386238
Action reg: 0.003962
  l1.weight: grad_norm = 0.008820
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.008776
Total gradient norm: 0.064229
=== Actor Training Debug (Iteration 1705) ===
Q mean: -6.671535
Q std: 7.226340
Actor loss: 6.675511
Action reg: 0.003976
  l1.weight: grad_norm = 0.012801
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.011675
Total gradient norm: 0.069844
=== Actor Training Debug (Iteration 1706) ===
Q mean: -6.830730
Q std: 7.901808
Actor loss: 6.834705
Action reg: 0.003975
  l1.weight: grad_norm = 0.016933
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.017336
Total gradient norm: 0.114191
=== Actor Training Debug (Iteration 1707) ===
Q mean: -7.150986
Q std: 7.652905
Actor loss: 7.154951
Action reg: 0.003965
  l1.weight: grad_norm = 0.020475
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.018188
Total gradient norm: 0.085759
=== Actor Training Debug (Iteration 1708) ===
Q mean: -7.094584
Q std: 7.444673
Actor loss: 7.098557
Action reg: 0.003973
  l1.weight: grad_norm = 0.024765
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.021551
Total gradient norm: 0.144029
=== Actor Training Debug (Iteration 1709) ===
Q mean: -7.651845
Q std: 7.815862
Actor loss: 7.655838
Action reg: 0.003994
  l1.weight: grad_norm = 0.011950
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.011164
Total gradient norm: 0.062937
=== Actor Training Debug (Iteration 1710) ===
Q mean: -6.600431
Q std: 7.362092
Actor loss: 6.604409
Action reg: 0.003978
  l1.weight: grad_norm = 0.024923
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.020675
Total gradient norm: 0.110194
=== Actor Training Debug (Iteration 1711) ===
Q mean: -6.276600
Q std: 6.471786
Actor loss: 6.280594
Action reg: 0.003994
  l1.weight: grad_norm = 0.006690
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.006605
Total gradient norm: 0.042102
=== Actor Training Debug (Iteration 1712) ===
Q mean: -7.441433
Q std: 7.659232
Actor loss: 7.445409
Action reg: 0.003976
  l1.weight: grad_norm = 0.017604
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.014879
Total gradient norm: 0.083163
=== Actor Training Debug (Iteration 1713) ===
Q mean: -7.470681
Q std: 7.654891
Actor loss: 7.474668
Action reg: 0.003987
  l1.weight: grad_norm = 0.018178
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.016591
Total gradient norm: 0.114079
=== Actor Training Debug (Iteration 1714) ===
Q mean: -6.857661
Q std: 7.256558
Actor loss: 6.861633
Action reg: 0.003972
  l1.weight: grad_norm = 0.013633
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.013473
Total gradient norm: 0.089665
=== Actor Training Debug (Iteration 1715) ===
Q mean: -6.068490
Q std: 7.274333
Actor loss: 6.072464
Action reg: 0.003974
  l1.weight: grad_norm = 0.012517
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.009893
Total gradient norm: 0.050741
=== Actor Training Debug (Iteration 1716) ===
Q mean: -6.362380
Q std: 7.753393
Actor loss: 6.366373
Action reg: 0.003993
  l1.weight: grad_norm = 0.004894
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004228
Total gradient norm: 0.020995
=== Actor Training Debug (Iteration 1717) ===
Q mean: -6.795815
Q std: 7.308397
Actor loss: 6.799788
Action reg: 0.003972
  l1.weight: grad_norm = 0.008343
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.007204
Total gradient norm: 0.036085
=== Actor Training Debug (Iteration 1718) ===
Q mean: -7.218056
Q std: 7.226417
Actor loss: 7.222028
Action reg: 0.003972
  l1.weight: grad_norm = 0.006149
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.005490
Total gradient norm: 0.032732
=== Actor Training Debug (Iteration 1719) ===
Q mean: -7.666443
Q std: 7.792771
Actor loss: 7.670436
Action reg: 0.003992
  l1.weight: grad_norm = 0.004791
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004266
Total gradient norm: 0.018628
=== Actor Training Debug (Iteration 1720) ===
Q mean: -7.094432
Q std: 7.746768
Actor loss: 7.098420
Action reg: 0.003988
  l1.weight: grad_norm = 0.016497
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.015255
Total gradient norm: 0.090884
=== Actor Training Debug (Iteration 1721) ===
Q mean: -6.906341
Q std: 7.051736
Actor loss: 6.910319
Action reg: 0.003978
  l1.weight: grad_norm = 0.011478
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.011148
Total gradient norm: 0.049364
=== Actor Training Debug (Iteration 1722) ===
Q mean: -6.052289
Q std: 6.855709
Actor loss: 6.056245
Action reg: 0.003956
  l1.weight: grad_norm = 0.009501
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.008694
Total gradient norm: 0.057666
=== Actor Training Debug (Iteration 1723) ===
Q mean: -6.753926
Q std: 7.339010
Actor loss: 6.757889
Action reg: 0.003963
  l1.weight: grad_norm = 0.014371
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.013274
Total gradient norm: 0.090402
=== Actor Training Debug (Iteration 1724) ===
Q mean: -6.765343
Q std: 7.221697
Actor loss: 6.769309
Action reg: 0.003966
  l1.weight: grad_norm = 0.012690
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.013422
Total gradient norm: 0.109983
=== Actor Training Debug (Iteration 1725) ===
Q mean: -5.623095
Q std: 6.831842
Actor loss: 5.627079
Action reg: 0.003985
  l1.weight: grad_norm = 0.002523
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.002784
Total gradient norm: 0.020539
=== Actor Training Debug (Iteration 1726) ===
Q mean: -7.079420
Q std: 7.822436
Actor loss: 7.083391
Action reg: 0.003971
  l1.weight: grad_norm = 0.010851
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.009031
Total gradient norm: 0.040644
=== Actor Training Debug (Iteration 1727) ===
Q mean: -6.998405
Q std: 8.066329
Actor loss: 7.002391
Action reg: 0.003985
  l1.weight: grad_norm = 0.024840
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.021407
Total gradient norm: 0.136773
=== Actor Training Debug (Iteration 1728) ===
Q mean: -7.006222
Q std: 7.696720
Actor loss: 7.010202
Action reg: 0.003980
  l1.weight: grad_norm = 0.006825
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.006322
Total gradient norm: 0.034090
=== Actor Training Debug (Iteration 1729) ===
Q mean: -7.126250
Q std: 7.800092
Actor loss: 7.130213
Action reg: 0.003963
  l1.weight: grad_norm = 0.011320
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.011550
Total gradient norm: 0.078051
=== Actor Training Debug (Iteration 1730) ===
Q mean: -7.226844
Q std: 7.536374
Actor loss: 7.230811
Action reg: 0.003967
  l1.weight: grad_norm = 0.036267
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.034944
Total gradient norm: 0.246882
=== Actor Training Debug (Iteration 1731) ===
Q mean: -6.594276
Q std: 7.801390
Actor loss: 6.598237
Action reg: 0.003960
  l1.weight: grad_norm = 0.023509
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.023731
Total gradient norm: 0.158181
=== Actor Training Debug (Iteration 1732) ===
Q mean: -6.225042
Q std: 6.992849
Actor loss: 6.229013
Action reg: 0.003971
  l1.weight: grad_norm = 0.010727
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.010403
Total gradient norm: 0.066373
=== Actor Training Debug (Iteration 1733) ===
Q mean: -6.308407
Q std: 6.970126
Actor loss: 6.312361
Action reg: 0.003954
  l1.weight: grad_norm = 0.014993
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.014239
Total gradient norm: 0.088635
=== Actor Training Debug (Iteration 1734) ===
Q mean: -7.851055
Q std: 7.565603
Actor loss: 7.854985
Action reg: 0.003929
  l1.weight: grad_norm = 0.059335
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.066292
Total gradient norm: 0.461420
=== Actor Training Debug (Iteration 1735) ===
Q mean: -7.172638
Q std: 7.358165
Actor loss: 7.176620
Action reg: 0.003981
  l1.weight: grad_norm = 0.007682
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.007410
Total gradient norm: 0.040491
=== Actor Training Debug (Iteration 1736) ===
Q mean: -6.992766
Q std: 7.755978
Actor loss: 6.996744
Action reg: 0.003978
  l1.weight: grad_norm = 0.022201
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.021477
Total gradient norm: 0.124706
=== Actor Training Debug (Iteration 1737) ===
Q mean: -7.206370
Q std: 7.743415
Actor loss: 7.210355
Action reg: 0.003985
  l1.weight: grad_norm = 0.007548
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.006921
Total gradient norm: 0.038275
=== Actor Training Debug (Iteration 1738) ===
Q mean: -7.616674
Q std: 7.558783
Actor loss: 7.620656
Action reg: 0.003981
  l1.weight: grad_norm = 0.011898
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.012532
Total gradient norm: 0.090764
=== Actor Training Debug (Iteration 1739) ===
Q mean: -7.424042
Q std: 7.544697
Actor loss: 7.428020
Action reg: 0.003978
  l1.weight: grad_norm = 0.019879
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.017889
Total gradient norm: 0.089163
=== Actor Training Debug (Iteration 1740) ===
Q mean: -6.262728
Q std: 7.524700
Actor loss: 6.266695
Action reg: 0.003967
  l1.weight: grad_norm = 0.019540
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.020380
Total gradient norm: 0.139089
=== Actor Training Debug (Iteration 1741) ===
Q mean: -6.855618
Q std: 7.352664
Actor loss: 6.859586
Action reg: 0.003968
  l1.weight: grad_norm = 0.099416
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.105747
Total gradient norm: 0.740965
=== Actor Training Debug (Iteration 1742) ===
Q mean: -6.650078
Q std: 7.548405
Actor loss: 6.654039
Action reg: 0.003961
  l1.weight: grad_norm = 0.027140
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.025752
Total gradient norm: 0.128201
=== Actor Training Debug (Iteration 1743) ===
Q mean: -7.323302
Q std: 7.740438
Actor loss: 7.327273
Action reg: 0.003971
  l1.weight: grad_norm = 0.053088
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.051128
Total gradient norm: 0.298933
=== Actor Training Debug (Iteration 1744) ===
Q mean: -7.435802
Q std: 7.876095
Actor loss: 7.439775
Action reg: 0.003973
  l1.weight: grad_norm = 0.034954
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.037464
Total gradient norm: 0.258978
=== Actor Training Debug (Iteration 1745) ===
Q mean: -6.479508
Q std: 7.126341
Actor loss: 6.483461
Action reg: 0.003953
  l1.weight: grad_norm = 0.045691
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.053962
Total gradient norm: 0.346655
=== Actor Training Debug (Iteration 1746) ===
Q mean: -6.405933
Q std: 7.446231
Actor loss: 6.409897
Action reg: 0.003964
  l1.weight: grad_norm = 0.016549
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.017472
Total gradient norm: 0.112606
=== Actor Training Debug (Iteration 1747) ===
Q mean: -6.422744
Q std: 7.308176
Actor loss: 6.426714
Action reg: 0.003970
  l1.weight: grad_norm = 0.030317
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.028594
Total gradient norm: 0.163500
=== Actor Training Debug (Iteration 1748) ===
Q mean: -7.130502
Q std: 8.017365
Actor loss: 7.134447
Action reg: 0.003945
  l1.weight: grad_norm = 0.017710
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.018925
Total gradient norm: 0.132869
=== Actor Training Debug (Iteration 1749) ===
Q mean: -7.220085
Q std: 8.009645
Actor loss: 7.224061
Action reg: 0.003975
  l1.weight: grad_norm = 0.059458
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.064010
Total gradient norm: 0.425260
=== Actor Training Debug (Iteration 1750) ===
Q mean: -6.995711
Q std: 7.495479
Actor loss: 6.999681
Action reg: 0.003970
  l1.weight: grad_norm = 0.031226
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.032290
Total gradient norm: 0.199288
=== Actor Training Debug (Iteration 1751) ===
Q mean: -7.482329
Q std: 7.932406
Actor loss: 7.486304
Action reg: 0.003975
  l1.weight: grad_norm = 0.015607
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.016030
Total gradient norm: 0.100498
=== Actor Training Debug (Iteration 1752) ===
Q mean: -7.164101
Q std: 7.675222
Actor loss: 7.168077
Action reg: 0.003976
  l1.weight: grad_norm = 0.014238
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.015343
Total gradient norm: 0.105061
=== Actor Training Debug (Iteration 1753) ===
Q mean: -6.949922
Q std: 8.159152
Actor loss: 6.953881
Action reg: 0.003959
  l1.weight: grad_norm = 0.055236
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.063226
Total gradient norm: 0.360581
=== Actor Training Debug (Iteration 1754) ===
Q mean: -7.501594
Q std: 7.792814
Actor loss: 7.505578
Action reg: 0.003984
  l1.weight: grad_norm = 0.010287
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.009571
Total gradient norm: 0.046447
=== Actor Training Debug (Iteration 1755) ===
Q mean: -7.727383
Q std: 8.204296
Actor loss: 7.731348
Action reg: 0.003965
  l1.weight: grad_norm = 0.015152
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.016629
Total gradient norm: 0.088172
=== Actor Training Debug (Iteration 1756) ===
Q mean: -6.035834
Q std: 7.092247
Actor loss: 6.039783
Action reg: 0.003949
  l1.weight: grad_norm = 0.030212
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.025050
Total gradient norm: 0.091754
=== Actor Training Debug (Iteration 1757) ===
Q mean: -6.455733
Q std: 7.429871
Actor loss: 6.459711
Action reg: 0.003977
  l1.weight: grad_norm = 0.026691
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.023740
Total gradient norm: 0.128429
=== Actor Training Debug (Iteration 1758) ===
Q mean: -6.490041
Q std: 7.486536
Actor loss: 6.493993
Action reg: 0.003952
  l1.weight: grad_norm = 0.032353
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.028171
Total gradient norm: 0.168219
=== Actor Training Debug (Iteration 1759) ===
Q mean: -7.143679
Q std: 8.223325
Actor loss: 7.147657
Action reg: 0.003979
  l1.weight: grad_norm = 0.024317
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.019029
Total gradient norm: 0.084232
=== Actor Training Debug (Iteration 1760) ===
Q mean: -6.401861
Q std: 6.859996
Actor loss: 6.405843
Action reg: 0.003982
  l1.weight: grad_norm = 0.023275
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.022964
Total gradient norm: 0.122365
=== Actor Training Debug (Iteration 1761) ===
Q mean: -7.353405
Q std: 8.378798
Actor loss: 7.357378
Action reg: 0.003973
  l1.weight: grad_norm = 0.031063
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.027218
Total gradient norm: 0.177584
=== Actor Training Debug (Iteration 1762) ===
Q mean: -7.213610
Q std: 7.496293
Actor loss: 7.217574
Action reg: 0.003964
  l1.weight: grad_norm = 0.014691
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.012381
Total gradient norm: 0.065344
=== Actor Training Debug (Iteration 1763) ===
Q mean: -6.634581
Q std: 7.515823
Actor loss: 6.638542
Action reg: 0.003961
  l1.weight: grad_norm = 0.038780
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.038427
Total gradient norm: 0.245544
=== Actor Training Debug (Iteration 1764) ===
Q mean: -6.986169
Q std: 7.799805
Actor loss: 6.990127
Action reg: 0.003958
  l1.weight: grad_norm = 0.020230
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.017824
Total gradient norm: 0.100157
=== Actor Training Debug (Iteration 1765) ===
Q mean: -6.388321
Q std: 7.605241
Actor loss: 6.392269
Action reg: 0.003948
  l1.weight: grad_norm = 0.026534
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.023060
Total gradient norm: 0.125135
=== Actor Training Debug (Iteration 1766) ===
Q mean: -6.576060
Q std: 7.219873
Actor loss: 6.580027
Action reg: 0.003966
  l1.weight: grad_norm = 0.044499
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.044022
Total gradient norm: 0.236585
=== Actor Training Debug (Iteration 1767) ===
Q mean: -7.246799
Q std: 7.879595
Actor loss: 7.250762
Action reg: 0.003963
  l1.weight: grad_norm = 0.025586
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.024230
Total gradient norm: 0.140444
=== Actor Training Debug (Iteration 1768) ===
Q mean: -7.228257
Q std: 7.863924
Actor loss: 7.232235
Action reg: 0.003979
  l1.weight: grad_norm = 0.018405
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.017229
Total gradient norm: 0.103140
=== Actor Training Debug (Iteration 1769) ===
Q mean: -5.401629
Q std: 6.439049
Actor loss: 5.405578
Action reg: 0.003949
  l1.weight: grad_norm = 0.010931
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.010242
Total gradient norm: 0.061418
=== Actor Training Debug (Iteration 1770) ===
Q mean: -7.190463
Q std: 7.696445
Actor loss: 7.194434
Action reg: 0.003971
  l1.weight: grad_norm = 0.033860
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.026897
Total gradient norm: 0.164927
=== Actor Training Debug (Iteration 1771) ===
Q mean: -7.631353
Q std: 7.924994
Actor loss: 7.635295
Action reg: 0.003942
  l1.weight: grad_norm = 0.023481
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.022303
Total gradient norm: 0.120428
=== Actor Training Debug (Iteration 1772) ===
Q mean: -7.450872
Q std: 7.765946
Actor loss: 7.454847
Action reg: 0.003975
  l1.weight: grad_norm = 0.023378
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.020750
Total gradient norm: 0.079377
=== Actor Training Debug (Iteration 1773) ===
Q mean: -7.754395
Q std: 7.967408
Actor loss: 7.758368
Action reg: 0.003973
  l1.weight: grad_norm = 0.031018
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.025914
Total gradient norm: 0.135538
=== Actor Training Debug (Iteration 1774) ===
Q mean: -6.453143
Q std: 7.472473
Actor loss: 6.457118
Action reg: 0.003974
  l1.weight: grad_norm = 0.029572
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.029627
Total gradient norm: 0.160796
=== Actor Training Debug (Iteration 1775) ===
Q mean: -6.929156
Q std: 7.854909
Actor loss: 6.933152
Action reg: 0.003996
  l1.weight: grad_norm = 0.005484
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004753
Total gradient norm: 0.031731
=== Actor Training Debug (Iteration 1776) ===
Q mean: -8.240216
Q std: 8.258289
Actor loss: 8.244202
Action reg: 0.003985
  l1.weight: grad_norm = 0.016586
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.017961
Total gradient norm: 0.108932
=== Actor Training Debug (Iteration 1777) ===
Q mean: -7.781968
Q std: 7.965011
Actor loss: 7.785939
Action reg: 0.003971
  l1.weight: grad_norm = 0.016803
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.015353
Total gradient norm: 0.084307
=== Actor Training Debug (Iteration 1778) ===
Q mean: -7.072915
Q std: 7.833945
Actor loss: 7.076862
Action reg: 0.003947
  l1.weight: grad_norm = 0.042297
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.037136
Total gradient norm: 0.216973
=== Actor Training Debug (Iteration 1779) ===
Q mean: -6.516395
Q std: 7.072030
Actor loss: 6.520360
Action reg: 0.003966
  l1.weight: grad_norm = 0.025000
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.021013
Total gradient norm: 0.114173
=== Actor Training Debug (Iteration 1780) ===
Q mean: -7.648601
Q std: 7.792806
Actor loss: 7.652563
Action reg: 0.003963
  l1.weight: grad_norm = 0.024373
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.023021
Total gradient norm: 0.151395
=== Actor Training Debug (Iteration 1781) ===
Q mean: -8.156674
Q std: 7.964165
Actor loss: 8.160651
Action reg: 0.003976
  l1.weight: grad_norm = 0.020763
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.017261
Total gradient norm: 0.106998
=== Actor Training Debug (Iteration 1782) ===
Q mean: -7.701266
Q std: 7.978775
Actor loss: 7.705247
Action reg: 0.003981
  l1.weight: grad_norm = 0.021792
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.021307
Total gradient norm: 0.108561
=== Actor Training Debug (Iteration 1783) ===
Q mean: -6.478926
Q std: 7.211127
Actor loss: 6.482896
Action reg: 0.003970
  l1.weight: grad_norm = 0.029557
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.027757
Total gradient norm: 0.154434
=== Actor Training Debug (Iteration 1784) ===
Q mean: -6.215687
Q std: 7.465275
Actor loss: 6.219663
Action reg: 0.003976
  l1.weight: grad_norm = 0.019055
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.016968
Total gradient norm: 0.074548
=== Actor Training Debug (Iteration 1785) ===
Q mean: -6.929756
Q std: 7.687492
Actor loss: 6.933736
Action reg: 0.003980
  l1.weight: grad_norm = 0.038587
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.036111
Total gradient norm: 0.197773
=== Actor Training Debug (Iteration 1786) ===
Q mean: -7.745811
Q std: 8.377425
Actor loss: 7.749789
Action reg: 0.003977
  l1.weight: grad_norm = 0.018428
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.018517
Total gradient norm: 0.106547
=== Actor Training Debug (Iteration 1787) ===
Q mean: -7.264660
Q std: 7.462431
Actor loss: 7.268650
Action reg: 0.003990
  l1.weight: grad_norm = 0.008488
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.007828
Total gradient norm: 0.041088
=== Actor Training Debug (Iteration 1788) ===
Q mean: -6.778643
Q std: 7.908747
Actor loss: 6.782602
Action reg: 0.003959
  l1.weight: grad_norm = 0.026867
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.026557
Total gradient norm: 0.152370
=== Actor Training Debug (Iteration 1789) ===
Q mean: -6.222170
Q std: 7.312584
Actor loss: 6.226152
Action reg: 0.003982
  l1.weight: grad_norm = 0.012557
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.011365
Total gradient norm: 0.074830
=== Actor Training Debug (Iteration 1790) ===
Q mean: -6.983562
Q std: 7.820976
Actor loss: 6.987536
Action reg: 0.003974
  l1.weight: grad_norm = 0.026853
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.023943
Total gradient norm: 0.120680
=== Actor Training Debug (Iteration 1791) ===
Q mean: -6.592203
Q std: 7.230934
Actor loss: 6.596178
Action reg: 0.003975
  l1.weight: grad_norm = 0.021972
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.018744
Total gradient norm: 0.087680
=== Actor Training Debug (Iteration 1792) ===
Q mean: -7.625158
Q std: 7.928487
Actor loss: 7.629131
Action reg: 0.003974
  l1.weight: grad_norm = 0.013386
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.011644
Total gradient norm: 0.070942
=== Actor Training Debug (Iteration 1793) ===
Q mean: -7.147519
Q std: 7.533589
Actor loss: 7.151500
Action reg: 0.003981
  l1.weight: grad_norm = 0.019848
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.019084
Total gradient norm: 0.097329
=== Actor Training Debug (Iteration 1794) ===
Q mean: -6.780887
Q std: 7.209785
Actor loss: 6.784853
Action reg: 0.003966
  l1.weight: grad_norm = 0.030620
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.028428
Total gradient norm: 0.167240
=== Actor Training Debug (Iteration 1795) ===
Q mean: -7.202188
Q std: 7.966403
Actor loss: 7.206141
Action reg: 0.003954
  l1.weight: grad_norm = 0.039284
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.039233
Total gradient norm: 0.182297
=== Actor Training Debug (Iteration 1796) ===
Q mean: -6.400069
Q std: 7.249529
Actor loss: 6.404043
Action reg: 0.003974
  l1.weight: grad_norm = 0.044211
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.041140
Total gradient norm: 0.225753
=== Actor Training Debug (Iteration 1797) ===
Q mean: -6.741810
Q std: 7.424708
Actor loss: 6.745773
Action reg: 0.003963
  l1.weight: grad_norm = 0.042082
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.042102
Total gradient norm: 0.225861
=== Actor Training Debug (Iteration 1798) ===
Q mean: -6.508977
Q std: 7.671473
Actor loss: 6.512964
Action reg: 0.003987
  l1.weight: grad_norm = 0.018492
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.016160
Total gradient norm: 0.106473
=== Actor Training Debug (Iteration 1799) ===
Q mean: -6.695601
Q std: 7.748673
Actor loss: 6.699568
Action reg: 0.003967
  l1.weight: grad_norm = 0.023939
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.020175
Total gradient norm: 0.118107
=== Actor Training Debug (Iteration 1800) ===
Q mean: -7.380163
Q std: 8.268293
Actor loss: 7.384127
Action reg: 0.003964
  l1.weight: grad_norm = 0.035297
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.031654
Total gradient norm: 0.179342
=== Actor Training Debug (Iteration 1801) ===
Q mean: -7.130725
Q std: 7.325349
Actor loss: 7.134702
Action reg: 0.003977
  l1.weight: grad_norm = 0.020337
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.016713
Total gradient norm: 0.088358
=== Actor Training Debug (Iteration 1802) ===
Q mean: -7.408943
Q std: 8.357701
Actor loss: 7.412915
Action reg: 0.003973
  l1.weight: grad_norm = 0.021810
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.018780
Total gradient norm: 0.085632
=== Actor Training Debug (Iteration 1803) ===
Q mean: -6.700083
Q std: 6.986241
Actor loss: 6.704056
Action reg: 0.003973
  l1.weight: grad_norm = 0.019641
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.018109
Total gradient norm: 0.096005
=== Actor Training Debug (Iteration 1804) ===
Q mean: -6.535226
Q std: 7.809658
Actor loss: 6.539197
Action reg: 0.003971
  l1.weight: grad_norm = 0.002023
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.001910
Total gradient norm: 0.010866
=== Actor Training Debug (Iteration 1805) ===
Q mean: -6.486355
Q std: 7.935569
Actor loss: 6.490335
Action reg: 0.003980
  l1.weight: grad_norm = 0.023104
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.019460
Total gradient norm: 0.111666
=== Actor Training Debug (Iteration 1806) ===
Q mean: -7.197626
Q std: 7.981882
Actor loss: 7.201597
Action reg: 0.003971
  l1.weight: grad_norm = 0.011295
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.010800
Total gradient norm: 0.060909
=== Actor Training Debug (Iteration 1807) ===
Q mean: -6.397818
Q std: 7.280208
Actor loss: 6.401783
Action reg: 0.003966
  l1.weight: grad_norm = 0.015095
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.012606
Total gradient norm: 0.066628
=== Actor Training Debug (Iteration 1808) ===
Q mean: -6.631213
Q std: 7.870260
Actor loss: 6.635184
Action reg: 0.003971
  l1.weight: grad_norm = 0.037512
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.033282
Total gradient norm: 0.202010
=== Actor Training Debug (Iteration 1809) ===
Q mean: -5.542101
Q std: 7.529606
Actor loss: 5.546076
Action reg: 0.003974
  l1.weight: grad_norm = 0.024600
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.021470
Total gradient norm: 0.127036
=== Actor Training Debug (Iteration 1810) ===
Q mean: -7.001706
Q std: 8.045086
Actor loss: 7.005683
Action reg: 0.003977
  l1.weight: grad_norm = 0.018154
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.017274
Total gradient norm: 0.117024
=== Actor Training Debug (Iteration 1811) ===
Q mean: -6.741431
Q std: 7.552658
Actor loss: 6.745401
Action reg: 0.003970
  l1.weight: grad_norm = 0.017561
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.016333
Total gradient norm: 0.108270
=== Actor Training Debug (Iteration 1812) ===
Q mean: -6.890798
Q std: 7.755643
Actor loss: 6.894760
Action reg: 0.003963
  l1.weight: grad_norm = 0.016955
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.015853
Total gradient norm: 0.101291
=== Actor Training Debug (Iteration 1813) ===
Q mean: -7.347102
Q std: 7.747786
Actor loss: 7.351076
Action reg: 0.003974
  l1.weight: grad_norm = 0.016800
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.016951
Total gradient norm: 0.111636
=== Actor Training Debug (Iteration 1814) ===
Q mean: -7.080586
Q std: 7.515883
Actor loss: 7.084555
Action reg: 0.003969
  l1.weight: grad_norm = 0.018166
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.017469
Total gradient norm: 0.110990
=== Actor Training Debug (Iteration 1815) ===
Q mean: -7.635410
Q std: 8.232245
Actor loss: 7.639378
Action reg: 0.003968
  l1.weight: grad_norm = 0.027575
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.024663
Total gradient norm: 0.114116
=== Actor Training Debug (Iteration 1816) ===
Q mean: -8.061194
Q std: 7.606730
Actor loss: 8.065174
Action reg: 0.003980
  l1.weight: grad_norm = 0.016413
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.014420
Total gradient norm: 0.076759
=== Actor Training Debug (Iteration 1817) ===
Q mean: -6.798923
Q std: 7.970739
Actor loss: 6.802894
Action reg: 0.003971
  l1.weight: grad_norm = 0.014642
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.012602
Total gradient norm: 0.076637
=== Actor Training Debug (Iteration 1818) ===
Q mean: -6.700884
Q std: 7.987675
Actor loss: 6.704852
Action reg: 0.003968
  l1.weight: grad_norm = 0.017056
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.015915
Total gradient norm: 0.098556
=== Actor Training Debug (Iteration 1819) ===
Q mean: -7.124271
Q std: 7.690767
Actor loss: 7.128238
Action reg: 0.003966
  l1.weight: grad_norm = 0.014196
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.012940
Total gradient norm: 0.065764
=== Actor Training Debug (Iteration 1820) ===
Q mean: -7.401326
Q std: 8.246500
Actor loss: 7.405291
Action reg: 0.003965
  l1.weight: grad_norm = 0.014854
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.013633
Total gradient norm: 0.078278
=== Actor Training Debug (Iteration 1821) ===
Q mean: -7.620039
Q std: 8.402342
Actor loss: 7.624023
Action reg: 0.003984
  l1.weight: grad_norm = 0.006725
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.006141
Total gradient norm: 0.040010
=== Actor Training Debug (Iteration 1822) ===
Q mean: -6.571072
Q std: 7.547525
Actor loss: 6.575038
Action reg: 0.003966
  l1.weight: grad_norm = 0.034802
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.031506
Total gradient norm: 0.169896
=== Actor Training Debug (Iteration 1823) ===
Q mean: -7.024784
Q std: 7.560524
Actor loss: 7.028756
Action reg: 0.003972
  l1.weight: grad_norm = 0.017201
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.015737
Total gradient norm: 0.100524
=== Actor Training Debug (Iteration 1824) ===
Q mean: -6.822247
Q std: 7.672450
Actor loss: 6.826221
Action reg: 0.003975
  l1.weight: grad_norm = 0.012075
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.011159
Total gradient norm: 0.073430
=== Actor Training Debug (Iteration 1825) ===
Q mean: -6.813909
Q std: 8.228324
Actor loss: 6.817886
Action reg: 0.003978
  l1.weight: grad_norm = 0.013907
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.012423
Total gradient norm: 0.068564
=== Actor Training Debug (Iteration 1826) ===
Q mean: -7.530805
Q std: 8.140304
Actor loss: 7.534791
Action reg: 0.003985
  l1.weight: grad_norm = 0.019542
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.019406
Total gradient norm: 0.127514
=== Actor Training Debug (Iteration 1827) ===
Q mean: -6.516778
Q std: 7.920383
Actor loss: 6.520758
Action reg: 0.003979
  l1.weight: grad_norm = 0.015091
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.012378
Total gradient norm: 0.077855
=== Actor Training Debug (Iteration 1828) ===
Q mean: -6.741947
Q std: 7.969258
Actor loss: 6.745935
Action reg: 0.003988
  l1.weight: grad_norm = 0.018735
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.017808
Total gradient norm: 0.094056
=== Actor Training Debug (Iteration 1829) ===
Q mean: -7.164296
Q std: 7.985575
Actor loss: 7.168267
Action reg: 0.003971
  l1.weight: grad_norm = 0.019486
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.019051
Total gradient norm: 0.112948
=== Actor Training Debug (Iteration 1830) ===
Q mean: -7.555124
Q std: 7.654933
Actor loss: 7.559113
Action reg: 0.003989
  l1.weight: grad_norm = 0.007607
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.008027
Total gradient norm: 0.048699
=== Actor Training Debug (Iteration 1831) ===
Q mean: -7.335052
Q std: 7.823886
Actor loss: 7.339011
Action reg: 0.003959
  l1.weight: grad_norm = 0.018005
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.019331
Total gradient norm: 0.118173
=== Actor Training Debug (Iteration 1832) ===
Q mean: -6.217557
Q std: 7.857371
Actor loss: 6.221531
Action reg: 0.003973
  l1.weight: grad_norm = 0.014490
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.013431
Total gradient norm: 0.069924
=== Actor Training Debug (Iteration 1833) ===
Q mean: -6.333421
Q std: 8.043375
Actor loss: 6.337377
Action reg: 0.003956
  l1.weight: grad_norm = 0.051061
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.046181
Total gradient norm: 0.258213
=== Actor Training Debug (Iteration 1834) ===
Q mean: -6.903080
Q std: 7.543615
Actor loss: 6.907058
Action reg: 0.003979
  l1.weight: grad_norm = 0.017555
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.018554
Total gradient norm: 0.099986
=== Actor Training Debug (Iteration 1835) ===
Q mean: -7.736072
Q std: 8.132706
Actor loss: 7.740047
Action reg: 0.003974
  l1.weight: grad_norm = 0.022871
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.018137
Total gradient norm: 0.081417
=== Actor Training Debug (Iteration 1836) ===
Q mean: -8.484819
Q std: 8.853164
Actor loss: 8.488789
Action reg: 0.003969
  l1.weight: grad_norm = 0.017850
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.014512
Total gradient norm: 0.060380
=== Actor Training Debug (Iteration 1837) ===
Q mean: -7.846484
Q std: 7.880984
Actor loss: 7.850458
Action reg: 0.003974
  l1.weight: grad_norm = 0.016481
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.015463
Total gradient norm: 0.086230
=== Actor Training Debug (Iteration 1838) ===
Q mean: -6.431425
Q std: 7.361880
Actor loss: 6.435388
Action reg: 0.003964
  l1.weight: grad_norm = 0.021738
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.021015
Total gradient norm: 0.135407
=== Actor Training Debug (Iteration 1839) ===
Q mean: -6.098326
Q std: 7.765924
Actor loss: 6.102299
Action reg: 0.003973
  l1.weight: grad_norm = 0.017871
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.013481
Total gradient norm: 0.061954
=== Actor Training Debug (Iteration 1840) ===
Q mean: -5.880033
Q std: 7.630754
Actor loss: 5.884007
Action reg: 0.003975
  l1.weight: grad_norm = 0.020761
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.020642
Total gradient norm: 0.121202
=== Actor Training Debug (Iteration 1841) ===
Q mean: -6.712424
Q std: 7.429893
Actor loss: 6.716389
Action reg: 0.003965
  l1.weight: grad_norm = 0.024261
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.024460
Total gradient norm: 0.164065
=== Actor Training Debug (Iteration 1842) ===
Q mean: -7.838322
Q std: 7.886442
Actor loss: 7.842287
Action reg: 0.003966
  l1.weight: grad_norm = 0.038695
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.037272
Total gradient norm: 0.230361
=== Actor Training Debug (Iteration 1843) ===
Q mean: -6.893176
Q std: 7.858313
Actor loss: 6.897137
Action reg: 0.003961
  l1.weight: grad_norm = 0.045931
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.044625
Total gradient norm: 0.282681
=== Actor Training Debug (Iteration 1844) ===
Q mean: -7.064218
Q std: 8.148763
Actor loss: 7.068190
Action reg: 0.003971
  l1.weight: grad_norm = 0.020088
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.019076
Total gradient norm: 0.104348
=== Actor Training Debug (Iteration 1845) ===
Q mean: -6.998033
Q std: 8.008989
Actor loss: 7.002003
Action reg: 0.003971
  l1.weight: grad_norm = 0.018116
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.017811
Total gradient norm: 0.076666
=== Actor Training Debug (Iteration 1846) ===
Q mean: -6.703830
Q std: 7.789315
Actor loss: 6.707799
Action reg: 0.003969
  l1.weight: grad_norm = 0.028459
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.024868
Total gradient norm: 0.115938
=== Actor Training Debug (Iteration 1847) ===
Q mean: -6.291428
Q std: 7.544429
Actor loss: 6.295397
Action reg: 0.003970
  l1.weight: grad_norm = 0.014426
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.013672
Total gradient norm: 0.068453
=== Actor Training Debug (Iteration 1848) ===
Q mean: -7.630277
Q std: 8.157462
Actor loss: 7.634238
Action reg: 0.003962
  l1.weight: grad_norm = 0.021880
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.017957
Total gradient norm: 0.081390
=== Actor Training Debug (Iteration 1849) ===
Q mean: -8.522212
Q std: 8.354595
Actor loss: 8.526192
Action reg: 0.003980
  l1.weight: grad_norm = 0.014010
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.011825
Total gradient norm: 0.066321
=== Actor Training Debug (Iteration 1850) ===
Q mean: -8.008456
Q std: 8.379327
Actor loss: 8.012433
Action reg: 0.003976
  l1.weight: grad_norm = 0.033312
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.029760
Total gradient norm: 0.163958
=== Actor Training Debug (Iteration 1851) ===
Q mean: -7.769381
Q std: 8.725159
Actor loss: 7.773368
Action reg: 0.003987
  l1.weight: grad_norm = 0.026520
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.023695
Total gradient norm: 0.136812
=== Actor Training Debug (Iteration 1852) ===
Q mean: -6.164843
Q std: 7.239239
Actor loss: 6.168804
Action reg: 0.003962
  l1.weight: grad_norm = 0.023994
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.019590
Total gradient norm: 0.101422
=== Actor Training Debug (Iteration 1853) ===
Q mean: -6.399468
Q std: 7.586042
Actor loss: 6.403450
Action reg: 0.003982
  l1.weight: grad_norm = 0.013923
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.013060
Total gradient norm: 0.076971
=== Actor Training Debug (Iteration 1854) ===
Q mean: -7.487122
Q std: 7.993255
Actor loss: 7.491092
Action reg: 0.003970
  l1.weight: grad_norm = 0.026097
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.022443
Total gradient norm: 0.110145
=== Actor Training Debug (Iteration 1855) ===
Q mean: -7.553698
Q std: 7.752278
Actor loss: 7.557672
Action reg: 0.003974
  l1.weight: grad_norm = 0.027532
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.027723
Total gradient norm: 0.179857
=== Actor Training Debug (Iteration 1856) ===
Q mean: -7.726179
Q std: 7.923492
Actor loss: 7.730158
Action reg: 0.003979
  l1.weight: grad_norm = 0.023178
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.023046
Total gradient norm: 0.113879
=== Actor Training Debug (Iteration 1857) ===
Q mean: -7.171965
Q std: 8.093389
Actor loss: 7.175937
Action reg: 0.003972
  l1.weight: grad_norm = 0.012421
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.011187
Total gradient norm: 0.054826
=== Actor Training Debug (Iteration 1858) ===
Q mean: -6.127154
Q std: 7.628737
Actor loss: 6.131132
Action reg: 0.003978
  l1.weight: grad_norm = 0.023717
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.018654
Total gradient norm: 0.095124
=== Actor Training Debug (Iteration 1859) ===
Q mean: -6.883455
Q std: 7.770720
Actor loss: 6.887424
Action reg: 0.003969
  l1.weight: grad_norm = 0.026290
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.025008
Total gradient norm: 0.170891
=== Actor Training Debug (Iteration 1860) ===
Q mean: -8.403273
Q std: 8.532021
Actor loss: 8.407260
Action reg: 0.003987
  l1.weight: grad_norm = 0.005748
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.004695
Total gradient norm: 0.023566
=== Actor Training Debug (Iteration 1861) ===
Q mean: -7.925028
Q std: 8.051325
Actor loss: 7.929008
Action reg: 0.003979
  l1.weight: grad_norm = 0.030830
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.028658
Total gradient norm: 0.120394
=== Actor Training Debug (Iteration 1862) ===
Q mean: -8.084548
Q std: 8.467422
Actor loss: 8.088525
Action reg: 0.003977
  l1.weight: grad_norm = 0.011710
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.008983
Total gradient norm: 0.050309
=== Actor Training Debug (Iteration 1863) ===
Q mean: -7.083072
Q std: 7.943672
Actor loss: 7.087053
Action reg: 0.003982
  l1.weight: grad_norm = 0.022250
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.021427
Total gradient norm: 0.126821
=== Actor Training Debug (Iteration 1864) ===
Q mean: -7.014726
Q std: 8.070130
Actor loss: 7.018710
Action reg: 0.003984
  l1.weight: grad_norm = 0.006796
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.006943
Total gradient norm: 0.039396
=== Actor Training Debug (Iteration 1865) ===
Q mean: -7.267288
Q std: 8.451372
Actor loss: 7.271263
Action reg: 0.003975
  l1.weight: grad_norm = 0.025197
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.024899
Total gradient norm: 0.112597
=== Actor Training Debug (Iteration 1866) ===
Q mean: -7.700610
Q std: 8.307144
Actor loss: 7.704581
Action reg: 0.003970
  l1.weight: grad_norm = 0.041376
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.036553
Total gradient norm: 0.171159
=== Actor Training Debug (Iteration 1867) ===
Q mean: -8.531901
Q std: 9.008445
Actor loss: 8.535877
Action reg: 0.003976
  l1.weight: grad_norm = 0.015179
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.015257
Total gradient norm: 0.069405
=== Actor Training Debug (Iteration 1868) ===
Q mean: -7.712041
Q std: 8.212295
Actor loss: 7.715982
Action reg: 0.003941
  l1.weight: grad_norm = 0.022812
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.023565
Total gradient norm: 0.144951
=== Actor Training Debug (Iteration 1869) ===
Q mean: -7.014815
Q std: 7.856876
Actor loss: 7.018790
Action reg: 0.003974
  l1.weight: grad_norm = 0.016715
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.014981
Total gradient norm: 0.077978
=== Actor Training Debug (Iteration 1870) ===
Q mean: -6.051955
Q std: 7.836901
Actor loss: 6.055906
Action reg: 0.003951
  l1.weight: grad_norm = 0.039887
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.032419
Total gradient norm: 0.190359
=== Actor Training Debug (Iteration 1871) ===
Q mean: -6.278502
Q std: 7.383590
Actor loss: 6.282469
Action reg: 0.003966
  l1.weight: grad_norm = 0.040943
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.035905
Total gradient norm: 0.161244
=== Actor Training Debug (Iteration 1872) ===
Q mean: -7.383456
Q std: 8.732350
Actor loss: 7.387420
Action reg: 0.003964
  l1.weight: grad_norm = 0.041274
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.040312
Total gradient norm: 0.216207
=== Actor Training Debug (Iteration 1873) ===
Q mean: -8.735029
Q std: 8.699992
Actor loss: 8.739004
Action reg: 0.003975
  l1.weight: grad_norm = 0.013775
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.012327
Total gradient norm: 0.059398
=== Actor Training Debug (Iteration 1874) ===
Q mean: -7.826943
Q std: 8.055882
Actor loss: 7.830922
Action reg: 0.003978
  l1.weight: grad_norm = 0.020184
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.017046
Total gradient norm: 0.082283
=== Actor Training Debug (Iteration 1875) ===
Q mean: -6.388338
Q std: 8.331697
Actor loss: 6.392293
Action reg: 0.003956
  l1.weight: grad_norm = 0.026220
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.021117
Total gradient norm: 0.105607
=== Actor Training Debug (Iteration 1876) ===
Q mean: -6.137018
Q std: 7.674965
Actor loss: 6.140999
Action reg: 0.003981
  l1.weight: grad_norm = 0.026327
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.023244
Total gradient norm: 0.140767
=== Actor Training Debug (Iteration 1877) ===
Q mean: -6.355631
Q std: 7.298860
Actor loss: 6.359612
Action reg: 0.003981
  l1.weight: grad_norm = 0.015911
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.015564
Total gradient norm: 0.091606
=== Actor Training Debug (Iteration 1878) ===
Q mean: -8.047789
Q std: 8.536161
Actor loss: 8.051776
Action reg: 0.003987
  l1.weight: grad_norm = 0.012348
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.011780
Total gradient norm: 0.073633
=== Actor Training Debug (Iteration 1879) ===
Q mean: -6.699492
Q std: 7.576353
Actor loss: 6.703475
Action reg: 0.003984
  l1.weight: grad_norm = 0.009422
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.008611
Total gradient norm: 0.055875
=== Actor Training Debug (Iteration 1880) ===
Q mean: -8.010761
Q std: 8.268754
Actor loss: 8.014743
Action reg: 0.003982
  l1.weight: grad_norm = 0.016101
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.012927
Total gradient norm: 0.048734
=== Actor Training Debug (Iteration 1881) ===
Q mean: -7.219025
Q std: 7.870930
Actor loss: 7.222990
Action reg: 0.003964
  l1.weight: grad_norm = 0.023216
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.021790
Total gradient norm: 0.153088
=== Actor Training Debug (Iteration 1882) ===
Q mean: -6.535461
Q std: 8.453815
Actor loss: 6.539444
Action reg: 0.003983
  l1.weight: grad_norm = 0.021310
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.018743
Total gradient norm: 0.091757
=== Actor Training Debug (Iteration 1883) ===
Q mean: -7.559392
Q std: 8.585139
Actor loss: 7.563354
Action reg: 0.003961
  l1.weight: grad_norm = 0.023662
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.024398
Total gradient norm: 0.140937
=== Actor Training Debug (Iteration 1884) ===
Q mean: -7.970491
Q std: 8.204417
Actor loss: 7.974458
Action reg: 0.003967
  l1.weight: grad_norm = 0.029215
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.026143
Total gradient norm: 0.187982
=== Actor Training Debug (Iteration 1885) ===
Q mean: -8.375508
Q std: 8.201223
Actor loss: 8.379495
Action reg: 0.003987
  l1.weight: grad_norm = 0.014208
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.013448
Total gradient norm: 0.075065
=== Actor Training Debug (Iteration 1886) ===
Q mean: -6.983254
Q std: 7.878117
Actor loss: 6.987213
Action reg: 0.003959
  l1.weight: grad_norm = 0.008383
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.007732
Total gradient norm: 0.037108
=== Actor Training Debug (Iteration 1887) ===
Q mean: -8.154036
Q std: 8.316523
Actor loss: 8.158010
Action reg: 0.003974
  l1.weight: grad_norm = 0.010570
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.010855
Total gradient norm: 0.053319
=== Actor Training Debug (Iteration 1888) ===
Q mean: -6.526702
Q std: 7.677649
Actor loss: 6.530670
Action reg: 0.003968
  l1.weight: grad_norm = 0.041380
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.040423
Total gradient norm: 0.248589
=== Actor Training Debug (Iteration 1889) ===
Q mean: -7.688705
Q std: 8.611677
Actor loss: 7.692684
Action reg: 0.003979
  l1.weight: grad_norm = 0.002482
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.002512
Total gradient norm: 0.013890
=== Actor Training Debug (Iteration 1890) ===
Q mean: -8.394445
Q std: 7.982213
Actor loss: 8.398433
Action reg: 0.003987
  l1.weight: grad_norm = 0.005837
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.005823
Total gradient norm: 0.031458
=== Actor Training Debug (Iteration 1891) ===
Q mean: -7.200651
Q std: 7.750368
Actor loss: 7.204635
Action reg: 0.003984
  l1.weight: grad_norm = 0.018886
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.016076
Total gradient norm: 0.079922
=== Actor Training Debug (Iteration 1892) ===
Q mean: -7.153238
Q std: 7.961944
Actor loss: 7.157218
Action reg: 0.003980
  l1.weight: grad_norm = 0.007580
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.007574
Total gradient norm: 0.038394
=== Actor Training Debug (Iteration 1893) ===
Q mean: -7.271998
Q std: 8.253617
Actor loss: 7.275997
Action reg: 0.003998
  l1.weight: grad_norm = 0.005448
  l1.bias: grad_norm = 0.000002
  l2.weight: grad_norm = 0.004170
Total gradient norm: 0.017686
=== Actor Training Debug (Iteration 1894) ===
Q mean: -6.657151
Q std: 7.984452
Actor loss: 6.661115
Action reg: 0.003964
  l1.weight: grad_norm = 0.027369
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.025309
Total gradient norm: 0.128549
=== Actor Training Debug (Iteration 1895) ===
Q mean: -7.231428
Q std: 8.155621
Actor loss: 7.235398
Action reg: 0.003970
  l1.weight: grad_norm = 0.016107
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.014049
Total gradient norm: 0.069059
=== Actor Training Debug (Iteration 1896) ===
Q mean: -7.925283
Q std: 8.510014
Actor loss: 7.929268
Action reg: 0.003984
  l1.weight: grad_norm = 0.009909
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.010133
Total gradient norm: 0.060200
=== Actor Training Debug (Iteration 1897) ===
Q mean: -6.221447
Q std: 7.419981
Actor loss: 6.225420
Action reg: 0.003973
  l1.weight: grad_norm = 0.011349
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.012508
Total gradient norm: 0.074752
=== Actor Training Debug (Iteration 1898) ===
Q mean: -6.330945
Q std: 7.948393
Actor loss: 6.334923
Action reg: 0.003978
  l1.weight: grad_norm = 0.013666
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.011923
Total gradient norm: 0.067949
=== Actor Training Debug (Iteration 1899) ===
Q mean: -6.110938
Q std: 7.693225
Actor loss: 6.114921
Action reg: 0.003983
  l1.weight: grad_norm = 0.009946
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.009787
Total gradient norm: 0.058030
=== Actor Training Debug (Iteration 1900) ===
Q mean: -7.176344
Q std: 8.407608
Actor loss: 7.180312
Action reg: 0.003968
  l1.weight: grad_norm = 0.010760
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.010025
Total gradient norm: 0.057887
=== Actor Training Debug (Iteration 1901) ===
Q mean: -6.645785
Q std: 7.809535
Actor loss: 6.649756
Action reg: 0.003971
  l1.weight: grad_norm = 0.010329
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.009302
Total gradient norm: 0.057760
=== Actor Training Debug (Iteration 1902) ===
Q mean: -6.761706
Q std: 8.137705
Actor loss: 6.765677
Action reg: 0.003970
  l1.weight: grad_norm = 0.013677
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.012670
Total gradient norm: 0.067112
=== Actor Training Debug (Iteration 1903) ===
Q mean: -7.461467
Q std: 8.008015
Actor loss: 7.465455
Action reg: 0.003987
  l1.weight: grad_norm = 0.015505
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.015043
Total gradient norm: 0.088379
=== Actor Training Debug (Iteration 1904) ===
Q mean: -7.601436
Q std: 8.291667
Actor loss: 7.605412
Action reg: 0.003976
  l1.weight: grad_norm = 0.025733
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.024402
Total gradient norm: 0.121182
=== Actor Training Debug (Iteration 1905) ===
Q mean: -8.128300
Q std: 8.488508
Actor loss: 8.132283
Action reg: 0.003984
  l1.weight: grad_norm = 0.038168
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.032608
Total gradient norm: 0.151597
=== Actor Training Debug (Iteration 1906) ===
Q mean: -7.249723
Q std: 7.922230
Actor loss: 7.253706
Action reg: 0.003983
  l1.weight: grad_norm = 0.012730
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.012589
Total gradient norm: 0.068709
=== Actor Training Debug (Iteration 1907) ===
Q mean: -7.330750
Q std: 8.531080
Actor loss: 7.334725
Action reg: 0.003976
  l1.weight: grad_norm = 0.013416
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.011305
Total gradient norm: 0.065512
=== Actor Training Debug (Iteration 1908) ===
Q mean: -6.797323
Q std: 7.819870
Actor loss: 6.801305
Action reg: 0.003982
  l1.weight: grad_norm = 0.005761
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.005067
Total gradient norm: 0.032496
=== Actor Training Debug (Iteration 1909) ===
Q mean: -7.447989
Q std: 8.233660
Actor loss: 7.451967
Action reg: 0.003977
  l1.weight: grad_norm = 0.010056
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.009691
Total gradient norm: 0.058692
=== Actor Training Debug (Iteration 1910) ===
Q mean: -6.548009
Q std: 7.824819
Actor loss: 6.552001
Action reg: 0.003992
  l1.weight: grad_norm = 0.022634
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.021986
Total gradient norm: 0.103996
=== Actor Training Debug (Iteration 1911) ===
Q mean: -7.306167
Q std: 8.922627
Actor loss: 7.310153
Action reg: 0.003987
  l1.weight: grad_norm = 0.018845
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.017160
Total gradient norm: 0.079963
=== Actor Training Debug (Iteration 1912) ===
Q mean: -7.908264
Q std: 8.922225
Actor loss: 7.912231
Action reg: 0.003967
  l1.weight: grad_norm = 0.041799
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.035677
Total gradient norm: 0.149964
=== Actor Training Debug (Iteration 1913) ===
Q mean: -6.968345
Q std: 8.606822
Actor loss: 6.972322
Action reg: 0.003977
  l1.weight: grad_norm = 0.010366
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.009450
Total gradient norm: 0.044380
=== Actor Training Debug (Iteration 1914) ===
Q mean: -6.534173
Q std: 7.821670
Actor loss: 6.538150
Action reg: 0.003977
  l1.weight: grad_norm = 0.019803
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.017857
Total gradient norm: 0.096731
=== Actor Training Debug (Iteration 1915) ===
Q mean: -7.747575
Q std: 8.128114
Actor loss: 7.751551
Action reg: 0.003976
  l1.weight: grad_norm = 0.011996
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.010274
Total gradient norm: 0.049924
=== Actor Training Debug (Iteration 1916) ===
Q mean: -7.058041
Q std: 7.870502
Actor loss: 7.062016
Action reg: 0.003975
  l1.weight: grad_norm = 0.027649
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.021654
Total gradient norm: 0.109515
=== Actor Training Debug (Iteration 1917) ===
Q mean: -7.552616
Q std: 8.738743
Actor loss: 7.556602
Action reg: 0.003985
  l1.weight: grad_norm = 0.006428
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.005909
Total gradient norm: 0.030484
=== Actor Training Debug (Iteration 1918) ===
Q mean: -7.027643
Q std: 8.441745
Actor loss: 7.031620
Action reg: 0.003976
  l1.weight: grad_norm = 0.020136
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.017243
Total gradient norm: 0.081175
=== Actor Training Debug (Iteration 1919) ===
Q mean: -8.169722
Q std: 8.908130
Actor loss: 8.173696
Action reg: 0.003974
  l1.weight: grad_norm = 0.021637
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.020991
Total gradient norm: 0.129302
=== Actor Training Debug (Iteration 1920) ===
Q mean: -8.398737
Q std: 8.864902
Actor loss: 8.402713
Action reg: 0.003976
  l1.weight: grad_norm = 0.044218
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.040516
Total gradient norm: 0.171445
=== Actor Training Debug (Iteration 1921) ===
Q mean: -6.070782
Q std: 7.598439
Actor loss: 6.074759
Action reg: 0.003977
  l1.weight: grad_norm = 0.017099
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.016451
Total gradient norm: 0.080644
=== Actor Training Debug (Iteration 1922) ===
Q mean: -7.737158
Q std: 8.722997
Actor loss: 7.741143
Action reg: 0.003985
  l1.weight: grad_norm = 0.014756
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.012630
Total gradient norm: 0.062734
=== Actor Training Debug (Iteration 1923) ===
Q mean: -7.350582
Q std: 8.234695
Actor loss: 7.354548
Action reg: 0.003966
  l1.weight: grad_norm = 0.026623
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.022815
Total gradient norm: 0.105882
=== Actor Training Debug (Iteration 1924) ===
Q mean: -8.280951
Q std: 8.387929
Actor loss: 8.284925
Action reg: 0.003974
  l1.weight: grad_norm = 0.009899
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.010416
Total gradient norm: 0.070149
=== Actor Training Debug (Iteration 1925) ===
Q mean: -7.810541
Q std: 8.050198
Actor loss: 7.814513
Action reg: 0.003971
  l1.weight: grad_norm = 0.032491
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.028620
Total gradient norm: 0.158929
=== Actor Training Debug (Iteration 1926) ===
Q mean: -7.284827
Q std: 8.189140
Actor loss: 7.288816
Action reg: 0.003989
  l1.weight: grad_norm = 0.016610
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.014994
Total gradient norm: 0.074244
=== Actor Training Debug (Iteration 1927) ===
Q mean: -6.813031
Q std: 8.349301
Actor loss: 6.817005
Action reg: 0.003974
  l1.weight: grad_norm = 0.004991
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.004873
Total gradient norm: 0.026416
=== Actor Training Debug (Iteration 1928) ===
Q mean: -6.726899
Q std: 8.547786
Actor loss: 6.730874
Action reg: 0.003974
  l1.weight: grad_norm = 0.030611
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.031794
Total gradient norm: 0.193605
=== Actor Training Debug (Iteration 1929) ===
Q mean: -7.403278
Q std: 9.047487
Actor loss: 7.407240
Action reg: 0.003962
  l1.weight: grad_norm = 0.017627
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.016549
Total gradient norm: 0.085986
=== Actor Training Debug (Iteration 1930) ===
Q mean: -7.613097
Q std: 8.804257
Actor loss: 7.617065
Action reg: 0.003969
  l1.weight: grad_norm = 0.023234
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.019874
Total gradient norm: 0.095450
=== Actor Training Debug (Iteration 1931) ===
Q mean: -6.912405
Q std: 7.599773
Actor loss: 6.916384
Action reg: 0.003979
  l1.weight: grad_norm = 0.023755
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.024462
Total gradient norm: 0.138023
=== Actor Training Debug (Iteration 1932) ===
Q mean: -6.964565
Q std: 8.375681
Actor loss: 6.968548
Action reg: 0.003983
  l1.weight: grad_norm = 0.011077
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.010042
Total gradient norm: 0.051809
=== Actor Training Debug (Iteration 1933) ===
Q mean: -7.663517
Q std: 8.856190
Actor loss: 7.667484
Action reg: 0.003967
  l1.weight: grad_norm = 0.016897
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.017965
Total gradient norm: 0.117052
=== Actor Training Debug (Iteration 1934) ===
Q mean: -7.958117
Q std: 8.655316
Actor loss: 7.962085
Action reg: 0.003968
  l1.weight: grad_norm = 0.025925
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.027386
Total gradient norm: 0.181168
=== Actor Training Debug (Iteration 1935) ===
Q mean: -7.521725
Q std: 8.134115
Actor loss: 7.525701
Action reg: 0.003976
  l1.weight: grad_norm = 0.029444
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.027274
Total gradient norm: 0.182489
=== Actor Training Debug (Iteration 1936) ===
Q mean: -7.282659
Q std: 8.315397
Actor loss: 7.286627
Action reg: 0.003969
  l1.weight: grad_norm = 0.016985
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.017093
Total gradient norm: 0.086197
=== Actor Training Debug (Iteration 1937) ===
Q mean: -7.277432
Q std: 8.810106
Actor loss: 7.281394
Action reg: 0.003961
  l1.weight: grad_norm = 0.019248
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.021028
Total gradient norm: 0.127752
=== Actor Training Debug (Iteration 1938) ===
Q mean: -7.657790
Q std: 8.658051
Actor loss: 7.661759
Action reg: 0.003970
  l1.weight: grad_norm = 0.023087
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.022327
Total gradient norm: 0.154991
=== Actor Training Debug (Iteration 1939) ===
Q mean: -6.490354
Q std: 7.653741
Actor loss: 6.494318
Action reg: 0.003964
  l1.weight: grad_norm = 0.028462
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.027313
Total gradient norm: 0.142731
=== Actor Training Debug (Iteration 1940) ===
Q mean: -6.918996
Q std: 8.005121
Actor loss: 6.922972
Action reg: 0.003976
  l1.weight: grad_norm = 0.011499
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.010156
Total gradient norm: 0.049536
=== Actor Training Debug (Iteration 1941) ===
Q mean: -7.218144
Q std: 7.954428
Actor loss: 7.222109
Action reg: 0.003964
  l1.weight: grad_norm = 0.042448
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.037486
Total gradient norm: 0.163417
=== Actor Training Debug (Iteration 1942) ===
Q mean: -7.314102
Q std: 8.321508
Actor loss: 7.318069
Action reg: 0.003966
  l1.weight: grad_norm = 0.014942
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.014012
Total gradient norm: 0.059528
=== Actor Training Debug (Iteration 1943) ===
Q mean: -8.187212
Q std: 8.544744
Actor loss: 8.191196
Action reg: 0.003984
  l1.weight: grad_norm = 0.013756
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.013373
Total gradient norm: 0.069398
=== Actor Training Debug (Iteration 1944) ===
Q mean: -7.158092
Q std: 8.281572
Actor loss: 7.162059
Action reg: 0.003967
  l1.weight: grad_norm = 0.012768
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.011941
Total gradient norm: 0.066022
=== Actor Training Debug (Iteration 1945) ===
Q mean: -7.047213
Q std: 7.844271
Actor loss: 7.051198
Action reg: 0.003985
  l1.weight: grad_norm = 0.033715
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.031908
Total gradient norm: 0.192816
=== Actor Training Debug (Iteration 1946) ===
Q mean: -8.034039
Q std: 9.168930
Actor loss: 8.038013
Action reg: 0.003975
  l1.weight: grad_norm = 0.009094
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.009178
Total gradient norm: 0.050519
=== Actor Training Debug (Iteration 1947) ===
Q mean: -7.705548
Q std: 8.504428
Actor loss: 7.709522
Action reg: 0.003974
  l1.weight: grad_norm = 0.017117
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.016555
Total gradient norm: 0.083275
=== Actor Training Debug (Iteration 1948) ===
Q mean: -6.946698
Q std: 8.728306
Actor loss: 6.950678
Action reg: 0.003980
  l1.weight: grad_norm = 0.012385
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.011162
Total gradient norm: 0.052899
=== Actor Training Debug (Iteration 1949) ===
Q mean: -8.143487
Q std: 9.162366
Actor loss: 8.147466
Action reg: 0.003979
  l1.weight: grad_norm = 0.023518
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.021151
Total gradient norm: 0.090395
=== Actor Training Debug (Iteration 1950) ===
Q mean: -7.463803
Q std: 8.164649
Actor loss: 7.467791
Action reg: 0.003987
  l1.weight: grad_norm = 0.025559
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.026147
Total gradient norm: 0.147855
=== Actor Training Debug (Iteration 1951) ===
Q mean: -7.043642
Q std: 8.557219
Actor loss: 7.047620
Action reg: 0.003978
  l1.weight: grad_norm = 0.015497
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.016234
Total gradient norm: 0.080620
=== Actor Training Debug (Iteration 1952) ===
Q mean: -6.506716
Q std: 8.072896
Actor loss: 6.510693
Action reg: 0.003977
  l1.weight: grad_norm = 0.002034
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.001825
Total gradient norm: 0.007451
=== Actor Training Debug (Iteration 1953) ===
Q mean: -7.429935
Q std: 8.265859
Actor loss: 7.433919
Action reg: 0.003984
  l1.weight: grad_norm = 0.010566
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.009416
Total gradient norm: 0.052464
=== Actor Training Debug (Iteration 1954) ===
Q mean: -6.526312
Q std: 8.367103
Actor loss: 6.530288
Action reg: 0.003976
  l1.weight: grad_norm = 0.035790
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.027774
Total gradient norm: 0.137273
=== Actor Training Debug (Iteration 1955) ===
Q mean: -7.570904
Q std: 8.960701
Actor loss: 7.574878
Action reg: 0.003974
  l1.weight: grad_norm = 0.019485
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.015562
Total gradient norm: 0.077228
=== Actor Training Debug (Iteration 1956) ===
Q mean: -7.285524
Q std: 8.481381
Actor loss: 7.289479
Action reg: 0.003954
  l1.weight: grad_norm = 0.030097
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.028314
Total gradient norm: 0.150134
=== Actor Training Debug (Iteration 1957) ===
Q mean: -6.985763
Q std: 8.478620
Actor loss: 6.989736
Action reg: 0.003974
  l1.weight: grad_norm = 0.008926
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.008745
Total gradient norm: 0.050901
=== Actor Training Debug (Iteration 1958) ===
Q mean: -7.551235
Q std: 8.736726
Actor loss: 7.555214
Action reg: 0.003979
  l1.weight: grad_norm = 0.021112
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.018950
Total gradient norm: 0.097742
=== Actor Training Debug (Iteration 1959) ===
Q mean: -7.318124
Q std: 8.270591
Actor loss: 7.322104
Action reg: 0.003980
  l1.weight: grad_norm = 0.018468
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.018379
Total gradient norm: 0.087629
=== Actor Training Debug (Iteration 1960) ===
Q mean: -8.679136
Q std: 9.466429
Actor loss: 8.683111
Action reg: 0.003975
  l1.weight: grad_norm = 0.019826
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.017252
Total gradient norm: 0.083166
=== Actor Training Debug (Iteration 1961) ===
Q mean: -7.361795
Q std: 8.540009
Actor loss: 7.365771
Action reg: 0.003975
  l1.weight: grad_norm = 0.028099
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.028507
Total gradient norm: 0.135792
=== Actor Training Debug (Iteration 1962) ===
Q mean: -6.511820
Q std: 8.138267
Actor loss: 6.515799
Action reg: 0.003978
  l1.weight: grad_norm = 0.019163
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.017109
Total gradient norm: 0.112274
=== Actor Training Debug (Iteration 1963) ===
Q mean: -7.042332
Q std: 8.661517
Actor loss: 7.046308
Action reg: 0.003976
  l1.weight: grad_norm = 0.016093
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.016514
Total gradient norm: 0.118855
=== Actor Training Debug (Iteration 1964) ===
Q mean: -6.910779
Q std: 8.028459
Actor loss: 6.914745
Action reg: 0.003966
  l1.weight: grad_norm = 0.034471
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.028261
Total gradient norm: 0.134851
=== Actor Training Debug (Iteration 1965) ===
Q mean: -7.285062
Q std: 8.157596
Actor loss: 7.289042
Action reg: 0.003980
  l1.weight: grad_norm = 0.011399
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.012507
Total gradient norm: 0.072821
=== Actor Training Debug (Iteration 1966) ===
Q mean: -7.004158
Q std: 8.219098
Actor loss: 7.008123
Action reg: 0.003965
  l1.weight: grad_norm = 0.034340
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.029768
Total gradient norm: 0.137709
=== Actor Training Debug (Iteration 1967) ===
Q mean: -6.996760
Q std: 8.186505
Actor loss: 7.000738
Action reg: 0.003978
  l1.weight: grad_norm = 0.015644
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.015082
Total gradient norm: 0.071773
=== Actor Training Debug (Iteration 1968) ===
Q mean: -6.819767
Q std: 8.371952
Actor loss: 6.823740
Action reg: 0.003972
  l1.weight: grad_norm = 0.039083
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.035541
Total gradient norm: 0.206335
=== Actor Training Debug (Iteration 1969) ===
Q mean: -7.382808
Q std: 8.022252
Actor loss: 7.386782
Action reg: 0.003974
  l1.weight: grad_norm = 0.021505
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.021572
Total gradient norm: 0.123227
=== Actor Training Debug (Iteration 1970) ===
Q mean: -6.810890
Q std: 8.049931
Actor loss: 6.814870
Action reg: 0.003980
  l1.weight: grad_norm = 0.010406
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.010651
Total gradient norm: 0.065585
=== Actor Training Debug (Iteration 1971) ===
Q mean: -7.278171
Q std: 8.239118
Actor loss: 7.282152
Action reg: 0.003981
  l1.weight: grad_norm = 0.023191
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.020474
Total gradient norm: 0.110289
=== Actor Training Debug (Iteration 1972) ===
Q mean: -7.189417
Q std: 8.394096
Actor loss: 7.193385
Action reg: 0.003968
  l1.weight: grad_norm = 0.041867
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.040482
Total gradient norm: 0.215865
=== Actor Training Debug (Iteration 1973) ===
Q mean: -7.921598
Q std: 9.233819
Actor loss: 7.925571
Action reg: 0.003973
  l1.weight: grad_norm = 0.014984
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.014120
Total gradient norm: 0.085907
=== Actor Training Debug (Iteration 1974) ===
Q mean: -7.641260
Q std: 8.360358
Actor loss: 7.645220
Action reg: 0.003960
  l1.weight: grad_norm = 0.025621
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.022820
Total gradient norm: 0.093488
=== Actor Training Debug (Iteration 1975) ===
Q mean: -7.676147
Q std: 8.870001
Actor loss: 7.680125
Action reg: 0.003977
  l1.weight: grad_norm = 0.008074
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.006941
Total gradient norm: 0.029365
=== Actor Training Debug (Iteration 1976) ===
Q mean: -7.580782
Q std: 8.272034
Actor loss: 7.584748
Action reg: 0.003965
  l1.weight: grad_norm = 0.014686
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.014433
Total gradient norm: 0.111890
=== Actor Training Debug (Iteration 1977) ===
Q mean: -7.189240
Q std: 8.125238
Actor loss: 7.193206
Action reg: 0.003966
  l1.weight: grad_norm = 0.016425
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.015266
Total gradient norm: 0.077722
=== Actor Training Debug (Iteration 1978) ===
Q mean: -7.753197
Q std: 8.803399
Actor loss: 7.757175
Action reg: 0.003978
  l1.weight: grad_norm = 0.027483
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.025699
Total gradient norm: 0.148370
=== Actor Training Debug (Iteration 1979) ===
Q mean: -7.421796
Q std: 8.628137
Actor loss: 7.425786
Action reg: 0.003989
  l1.weight: grad_norm = 0.003819
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.003320
Total gradient norm: 0.018470
=== Actor Training Debug (Iteration 1980) ===
Q mean: -7.438326
Q std: 8.821714
Actor loss: 7.442314
Action reg: 0.003988
  l1.weight: grad_norm = 0.018351
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.016288
Total gradient norm: 0.094262
=== Actor Training Debug (Iteration 1981) ===
Q mean: -7.691141
Q std: 8.587591
Actor loss: 7.695124
Action reg: 0.003983
  l1.weight: grad_norm = 0.025798
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.022685
Total gradient norm: 0.118163
=== Actor Training Debug (Iteration 1982) ===
Q mean: -7.086321
Q std: 8.719095
Actor loss: 7.090313
Action reg: 0.003992
  l1.weight: grad_norm = 0.018660
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.015842
Total gradient norm: 0.064842
=== Actor Training Debug (Iteration 1983) ===
Q mean: -7.260948
Q std: 8.272927
Actor loss: 7.264917
Action reg: 0.003969
  l1.weight: grad_norm = 0.012972
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.012272
Total gradient norm: 0.066160
=== Actor Training Debug (Iteration 1984) ===
Q mean: -7.002313
Q std: 8.436678
Actor loss: 7.006296
Action reg: 0.003983
  l1.weight: grad_norm = 0.014748
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.013046
Total gradient norm: 0.094079
=== Actor Training Debug (Iteration 1985) ===
Q mean: -7.627659
Q std: 8.222478
Actor loss: 7.631640
Action reg: 0.003981
  l1.weight: grad_norm = 0.019278
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.021381
Total gradient norm: 0.111513
=== Actor Training Debug (Iteration 1986) ===
Q mean: -7.019103
Q std: 8.740842
Actor loss: 7.023068
Action reg: 0.003966
  l1.weight: grad_norm = 0.029525
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.030929
Total gradient norm: 0.135000
=== Actor Training Debug (Iteration 1987) ===
Q mean: -8.522864
Q std: 9.077431
Actor loss: 8.526857
Action reg: 0.003993
  l1.weight: grad_norm = 0.004125
  l1.bias: grad_norm = 0.000003
  l2.weight: grad_norm = 0.004057
Total gradient norm: 0.019332
=== Actor Training Debug (Iteration 1988) ===
Q mean: -6.411058
Q std: 8.182978
Actor loss: 6.415045
Action reg: 0.003988
  l1.weight: grad_norm = 0.010743
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.009882
Total gradient norm: 0.051113
=== Actor Training Debug (Iteration 1989) ===
Q mean: -7.019268
Q std: 8.319132
Actor loss: 7.023249
Action reg: 0.003981
  l1.weight: grad_norm = 0.014915
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.014216
Total gradient norm: 0.079588
=== Actor Training Debug (Iteration 1990) ===
Q mean: -7.329550
Q std: 8.834704
Actor loss: 7.333517
Action reg: 0.003967
  l1.weight: grad_norm = 0.028116
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.025073
Total gradient norm: 0.139283
=== Actor Training Debug (Iteration 1991) ===
Q mean: -7.640790
Q std: 8.953933
Actor loss: 7.644778
Action reg: 0.003988
  l1.weight: grad_norm = 0.013391
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.012877
Total gradient norm: 0.069687
=== Actor Training Debug (Iteration 1992) ===
Q mean: -7.365576
Q std: 8.671253
Actor loss: 7.369551
Action reg: 0.003975
  l1.weight: grad_norm = 0.008700
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.006545
Total gradient norm: 0.025265
=== Actor Training Debug (Iteration 1993) ===
Q mean: -7.236796
Q std: 8.686598
Actor loss: 7.240780
Action reg: 0.003984
  l1.weight: grad_norm = 0.019210
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.017409
Total gradient norm: 0.087329
=== Actor Training Debug (Iteration 1994) ===
Q mean: -7.479327
Q std: 9.484688
Actor loss: 7.483303
Action reg: 0.003976
  l1.weight: grad_norm = 0.026042
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.021826
Total gradient norm: 0.106030
=== Actor Training Debug (Iteration 1995) ===
Q mean: -6.702820
Q std: 7.723382
Actor loss: 6.706793
Action reg: 0.003974
  l1.weight: grad_norm = 0.022592
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.021904
Total gradient norm: 0.116161
=== Actor Training Debug (Iteration 1996) ===
Q mean: -7.026480
Q std: 7.848000
Actor loss: 7.030455
Action reg: 0.003975
  l1.weight: grad_norm = 0.024886
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.022628
Total gradient norm: 0.142419
=== Actor Training Debug (Iteration 1997) ===
Q mean: -7.829791
Q std: 8.692446
Actor loss: 7.833773
Action reg: 0.003982
  l1.weight: grad_norm = 0.002646
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.002756
Total gradient norm: 0.016384
=== Actor Training Debug (Iteration 1998) ===
Q mean: -6.421236
Q std: 8.427259
Actor loss: 6.425214
Action reg: 0.003978
  l1.weight: grad_norm = 0.013309
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.012785
Total gradient norm: 0.061149
=== Actor Training Debug (Iteration 1999) ===
Q mean: -7.130493
Q std: 8.313416
Actor loss: 7.134470
Action reg: 0.003976
  l1.weight: grad_norm = 0.023259
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.019645
Total gradient norm: 0.093235
=== Actor Training Debug (Iteration 2000) ===
Q mean: -8.118961
Q std: 8.487275
Actor loss: 8.122937
Action reg: 0.003976
  l1.weight: grad_norm = 0.029998
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.025483
Total gradient norm: 0.118688
Step 7000: Critic Loss: 0.9338, Actor Loss: 8.1229, Q Value: -8.1190
  Average reward: -333.892 | Average length: 100.0
Evaluation at episode 70: -333.892
=== Actor Training Debug (Iteration 2001) ===
Q mean: -7.146649
Q std: 8.836486
Actor loss: 7.150620
Action reg: 0.003971
  l1.weight: grad_norm = 0.019776
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.017518
Total gradient norm: 0.091640
=== Actor Training Debug (Iteration 2002) ===
Q mean: -6.370436
Q std: 8.228765
Actor loss: 6.374413
Action reg: 0.003978
  l1.weight: grad_norm = 0.013539
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.010433
Total gradient norm: 0.053403
=== Actor Training Debug (Iteration 2003) ===
Q mean: -5.928647
Q std: 7.650709
Actor loss: 5.932607
Action reg: 0.003960
  l1.weight: grad_norm = 0.034951
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.031087
Total gradient norm: 0.151847
=== Actor Training Debug (Iteration 2004) ===
Q mean: -7.231533
Q std: 8.182084
Actor loss: 7.235509
Action reg: 0.003976
  l1.weight: grad_norm = 0.020087
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.020302
Total gradient norm: 0.132961
=== Actor Training Debug (Iteration 2005) ===
Q mean: -7.721280
Q std: 8.856507
Actor loss: 7.725225
Action reg: 0.003945
  l1.weight: grad_norm = 0.073395
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.074461
Total gradient norm: 0.467447
=== Actor Training Debug (Iteration 2006) ===
Q mean: -7.354913
Q std: 8.528756
Actor loss: 7.358878
Action reg: 0.003965
  l1.weight: grad_norm = 0.043435
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.036245
Total gradient norm: 0.151098
=== Actor Training Debug (Iteration 2007) ===
Q mean: -6.236290
Q std: 8.060322
Actor loss: 6.240270
Action reg: 0.003981
  l1.weight: grad_norm = 0.008650
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.009600
Total gradient norm: 0.062781
=== Actor Training Debug (Iteration 2008) ===
Q mean: -7.058947
Q std: 8.452233
Actor loss: 7.062930
Action reg: 0.003983
  l1.weight: grad_norm = 0.028974
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.028504
Total gradient norm: 0.136111
=== Actor Training Debug (Iteration 2009) ===
Q mean: -7.964069
Q std: 9.351269
Actor loss: 7.968048
Action reg: 0.003979
  l1.weight: grad_norm = 0.015488
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.014082
Total gradient norm: 0.060862
=== Actor Training Debug (Iteration 2010) ===
Q mean: -7.752414
Q std: 8.258637
Actor loss: 7.756387
Action reg: 0.003974
  l1.weight: grad_norm = 0.044518
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.040732
Total gradient norm: 0.217321
=== Actor Training Debug (Iteration 2011) ===
Q mean: -8.048961
Q std: 8.792060
Actor loss: 8.052938
Action reg: 0.003977
  l1.weight: grad_norm = 0.022342
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.021216
Total gradient norm: 0.106620
=== Actor Training Debug (Iteration 2012) ===
Q mean: -6.779449
Q std: 8.621201
Actor loss: 6.783424
Action reg: 0.003975
  l1.weight: grad_norm = 0.030549
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.027578
Total gradient norm: 0.158814
=== Actor Training Debug (Iteration 2013) ===
Q mean: -7.607504
Q std: 8.956676
Actor loss: 7.611485
Action reg: 0.003980
  l1.weight: grad_norm = 0.008483
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.008606
Total gradient norm: 0.044730
=== Actor Training Debug (Iteration 2014) ===
Q mean: -6.784151
Q std: 8.521108
Actor loss: 6.788120
Action reg: 0.003970
  l1.weight: grad_norm = 0.024825
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.026348
Total gradient norm: 0.133691
=== Actor Training Debug (Iteration 2015) ===
Q mean: -7.138905
Q std: 8.681351
Actor loss: 7.142870
Action reg: 0.003966
  l1.weight: grad_norm = 0.020999
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.017928
Total gradient norm: 0.083322
=== Actor Training Debug (Iteration 2016) ===
Q mean: -7.818492
Q std: 8.382051
Actor loss: 7.822458
Action reg: 0.003966
  l1.weight: grad_norm = 0.040074
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.042739
Total gradient norm: 0.259046
=== Actor Training Debug (Iteration 2017) ===
Q mean: -7.539900
Q std: 9.216891
Actor loss: 7.543870
Action reg: 0.003970
  l1.weight: grad_norm = 0.021507
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.020994
Total gradient norm: 0.091757
=== Actor Training Debug (Iteration 2018) ===
Q mean: -7.921209
Q std: 8.718597
Actor loss: 7.925188
Action reg: 0.003978
  l1.weight: grad_norm = 0.023254
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.021403
Total gradient norm: 0.090975
=== Actor Training Debug (Iteration 2019) ===
Q mean: -6.817898
Q std: 8.447403
Actor loss: 6.821857
Action reg: 0.003960
  l1.weight: grad_norm = 0.039640
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.032401
Total gradient norm: 0.130754
=== Actor Training Debug (Iteration 2020) ===
Q mean: -7.010310
Q std: 8.614689
Actor loss: 7.014282
Action reg: 0.003972
  l1.weight: grad_norm = 0.033936
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.028675
Total gradient norm: 0.143972
=== Actor Training Debug (Iteration 2021) ===
Q mean: -6.554999
Q std: 8.470715
Actor loss: 6.558972
Action reg: 0.003973
  l1.weight: grad_norm = 0.027151
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.020739
Total gradient norm: 0.087147
=== Actor Training Debug (Iteration 2022) ===
Q mean: -8.152931
Q std: 8.599749
Actor loss: 8.156905
Action reg: 0.003974
  l1.weight: grad_norm = 0.042673
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.041527
Total gradient norm: 0.254770
=== Actor Training Debug (Iteration 2023) ===
Q mean: -8.285947
Q std: 9.009435
Actor loss: 8.289920
Action reg: 0.003973
  l1.weight: grad_norm = 0.032696
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.030335
Total gradient norm: 0.153396
=== Actor Training Debug (Iteration 2024) ===
Q mean: -7.117745
Q std: 8.563495
Actor loss: 7.121716
Action reg: 0.003970
  l1.weight: grad_norm = 0.032556
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.028760
Total gradient norm: 0.116515
=== Actor Training Debug (Iteration 2025) ===
Q mean: -7.082897
Q std: 9.031806
Actor loss: 7.086871
Action reg: 0.003974
  l1.weight: grad_norm = 0.022722
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.022691
Total gradient norm: 0.105779
=== Actor Training Debug (Iteration 2026) ===
Q mean: -6.251370
Q std: 8.450089
Actor loss: 6.255347
Action reg: 0.003977
  l1.weight: grad_norm = 0.005502
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.005401
Total gradient norm: 0.025764
=== Actor Training Debug (Iteration 2027) ===
Q mean: -6.563698
Q std: 8.026274
Actor loss: 6.567675
Action reg: 0.003976
  l1.weight: grad_norm = 0.050498
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.045234
Total gradient norm: 0.262740
=== Actor Training Debug (Iteration 2028) ===
Q mean: -7.058969
Q std: 8.522537
Actor loss: 7.062947
Action reg: 0.003979
  l1.weight: grad_norm = 0.021369
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.020864
Total gradient norm: 0.098625
=== Actor Training Debug (Iteration 2029) ===
Q mean: -7.793582
Q std: 8.582966
Actor loss: 7.797551
Action reg: 0.003969
  l1.weight: grad_norm = 0.056044
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.055101
Total gradient norm: 0.377819
=== Actor Training Debug (Iteration 2030) ===
Q mean: -7.585423
Q std: 7.821571
Actor loss: 7.589389
Action reg: 0.003965
  l1.weight: grad_norm = 0.022548
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.020199
Total gradient norm: 0.110866
=== Actor Training Debug (Iteration 2031) ===
Q mean: -7.567903
Q std: 8.325427
Actor loss: 7.571895
Action reg: 0.003992
  l1.weight: grad_norm = 0.015147
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.014361
Total gradient norm: 0.075072
=== Actor Training Debug (Iteration 2032) ===
Q mean: -7.397541
Q std: 8.715734
Actor loss: 7.401511
Action reg: 0.003970
  l1.weight: grad_norm = 0.030234
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.029683
Total gradient norm: 0.195780
=== Actor Training Debug (Iteration 2033) ===
Q mean: -6.011590
Q std: 8.093150
Actor loss: 6.015565
Action reg: 0.003975
  l1.weight: grad_norm = 0.011066
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.010246
Total gradient norm: 0.045080
=== Actor Training Debug (Iteration 2034) ===
Q mean: -6.977708
Q std: 8.244129
Actor loss: 6.981666
Action reg: 0.003958
  l1.weight: grad_norm = 0.015620
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.015853
Total gradient norm: 0.099501
=== Actor Training Debug (Iteration 2035) ===
Q mean: -7.426246
Q std: 8.299932
Actor loss: 7.430227
Action reg: 0.003980
  l1.weight: grad_norm = 0.043548
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.037594
Total gradient norm: 0.188288
=== Actor Training Debug (Iteration 2036) ===
Q mean: -7.507707
Q std: 8.885211
Actor loss: 7.511678
Action reg: 0.003972
  l1.weight: grad_norm = 0.032476
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.031534
Total gradient norm: 0.223944
=== Actor Training Debug (Iteration 2037) ===
Q mean: -7.300752
Q std: 8.987539
Actor loss: 7.304716
Action reg: 0.003964
  l1.weight: grad_norm = 0.069716
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.060702
Total gradient norm: 0.346913
=== Actor Training Debug (Iteration 2038) ===
Q mean: -7.533840
Q std: 9.396259
Actor loss: 7.537816
Action reg: 0.003975
  l1.weight: grad_norm = 0.022376
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.022245
Total gradient norm: 0.115887
=== Actor Training Debug (Iteration 2039) ===
Q mean: -6.892278
Q std: 8.490266
Actor loss: 6.896249
Action reg: 0.003971
  l1.weight: grad_norm = 0.017689
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.016192
Total gradient norm: 0.090731
=== Actor Training Debug (Iteration 2040) ===
Q mean: -7.990585
Q std: 8.625377
Actor loss: 7.994550
Action reg: 0.003964
  l1.weight: grad_norm = 0.049617
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.044757
Total gradient norm: 0.237536
=== Actor Training Debug (Iteration 2041) ===
Q mean: -7.785899
Q std: 8.847508
Actor loss: 7.789862
Action reg: 0.003963
  l1.weight: grad_norm = 0.051847
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.044652
Total gradient norm: 0.287404
=== Actor Training Debug (Iteration 2042) ===
Q mean: -7.250893
Q std: 8.502466
Actor loss: 7.254881
Action reg: 0.003988
  l1.weight: grad_norm = 0.023751
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.021157
Total gradient norm: 0.094895
=== Actor Training Debug (Iteration 2043) ===
Q mean: -7.268790
Q std: 8.750551
Actor loss: 7.272733
Action reg: 0.003942
  l1.weight: grad_norm = 0.038602
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.036081
Total gradient norm: 0.173416
=== Actor Training Debug (Iteration 2044) ===
Q mean: -7.202471
Q std: 8.548969
Actor loss: 7.206450
Action reg: 0.003979
  l1.weight: grad_norm = 0.043272
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.041835
Total gradient norm: 0.179588
=== Actor Training Debug (Iteration 2045) ===
Q mean: -6.768393
Q std: 8.078250
Actor loss: 6.772358
Action reg: 0.003965
  l1.weight: grad_norm = 0.031845
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.027076
Total gradient norm: 0.113826
=== Actor Training Debug (Iteration 2046) ===
Q mean: -6.999048
Q std: 8.621795
Actor loss: 7.003029
Action reg: 0.003981
  l1.weight: grad_norm = 0.027087
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.024687
Total gradient norm: 0.147116
=== Actor Training Debug (Iteration 2047) ===
Q mean: -7.359488
Q std: 9.001631
Actor loss: 7.363461
Action reg: 0.003972
  l1.weight: grad_norm = 0.032133
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.028332
Total gradient norm: 0.153522
=== Actor Training Debug (Iteration 2048) ===
Q mean: -6.409553
Q std: 8.307854
Actor loss: 6.413522
Action reg: 0.003969
  l1.weight: grad_norm = 0.034855
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.029577
Total gradient norm: 0.143027
=== Actor Training Debug (Iteration 2049) ===
Q mean: -7.312232
Q std: 8.593782
Actor loss: 7.316204
Action reg: 0.003972
  l1.weight: grad_norm = 0.044469
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.044485
Total gradient norm: 0.252388
=== Actor Training Debug (Iteration 2050) ===
Q mean: -6.852430
Q std: 8.808416
Actor loss: 6.856400
Action reg: 0.003970
  l1.weight: grad_norm = 0.063006
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.063824
Total gradient norm: 0.351034
=== Actor Training Debug (Iteration 2051) ===
Q mean: -7.190444
Q std: 8.644123
Actor loss: 7.194420
Action reg: 0.003976
  l1.weight: grad_norm = 0.014114
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.013861
Total gradient norm: 0.067044
=== Actor Training Debug (Iteration 2052) ===
Q mean: -7.632601
Q std: 8.854193
Actor loss: 7.636587
Action reg: 0.003986
  l1.weight: grad_norm = 0.034684
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.031402
Total gradient norm: 0.178246
=== Actor Training Debug (Iteration 2053) ===
Q mean: -7.476105
Q std: 8.904329
Actor loss: 7.480071
Action reg: 0.003966
  l1.weight: grad_norm = 0.018850
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.017667
Total gradient norm: 0.076712
=== Actor Training Debug (Iteration 2054) ===
Q mean: -7.080220
Q std: 8.705746
Actor loss: 7.084195
Action reg: 0.003974
  l1.weight: grad_norm = 0.017932
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.015548
Total gradient norm: 0.070045
=== Actor Training Debug (Iteration 2055) ===
Q mean: -7.780008
Q std: 9.596281
Actor loss: 7.783984
Action reg: 0.003976
  l1.weight: grad_norm = 0.034742
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.029226
Total gradient norm: 0.168280
=== Actor Training Debug (Iteration 2056) ===
Q mean: -7.952973
Q std: 8.960744
Actor loss: 7.956941
Action reg: 0.003968
  l1.weight: grad_norm = 0.038128
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.033939
Total gradient norm: 0.179563
=== Actor Training Debug (Iteration 2057) ===
Q mean: -7.142602
Q std: 9.019443
Actor loss: 7.146580
Action reg: 0.003978
  l1.weight: grad_norm = 0.019745
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.018486
Total gradient norm: 0.091509
=== Actor Training Debug (Iteration 2058) ===
Q mean: -6.436914
Q std: 8.553370
Actor loss: 6.440894
Action reg: 0.003979
  l1.weight: grad_norm = 0.021893
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.020589
Total gradient norm: 0.101273
=== Actor Training Debug (Iteration 2059) ===
Q mean: -6.624049
Q std: 8.684579
Actor loss: 6.628038
Action reg: 0.003989
  l1.weight: grad_norm = 0.030802
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.029971
Total gradient norm: 0.170093
=== Actor Training Debug (Iteration 2060) ===
Q mean: -7.165153
Q std: 8.673441
Actor loss: 7.169135
Action reg: 0.003982
  l1.weight: grad_norm = 0.016817
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.016384
Total gradient norm: 0.086787
=== Actor Training Debug (Iteration 2061) ===
Q mean: -7.067566
Q std: 8.393172
Actor loss: 7.071546
Action reg: 0.003980
  l1.weight: grad_norm = 0.027194
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.027997
Total gradient norm: 0.151825
=== Actor Training Debug (Iteration 2062) ===
Q mean: -8.120288
Q std: 9.480835
Actor loss: 8.124266
Action reg: 0.003978
  l1.weight: grad_norm = 0.021766
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.019629
Total gradient norm: 0.093320
=== Actor Training Debug (Iteration 2063) ===
Q mean: -7.478685
Q std: 8.458794
Actor loss: 7.482661
Action reg: 0.003976
  l1.weight: grad_norm = 0.021639
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.018937
Total gradient norm: 0.108692
=== Actor Training Debug (Iteration 2064) ===
Q mean: -6.760687
Q std: 8.697492
Actor loss: 6.764668
Action reg: 0.003980
  l1.weight: grad_norm = 0.023592
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.019324
Total gradient norm: 0.079348
=== Actor Training Debug (Iteration 2065) ===
Q mean: -6.675413
Q std: 8.743900
Actor loss: 6.679385
Action reg: 0.003972
  l1.weight: grad_norm = 0.027002
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.025200
Total gradient norm: 0.153368
=== Actor Training Debug (Iteration 2066) ===
Q mean: -6.375059
Q std: 8.536716
Actor loss: 6.379043
Action reg: 0.003984
  l1.weight: grad_norm = 0.047041
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.043385
Total gradient norm: 0.230714
=== Actor Training Debug (Iteration 2067) ===
Q mean: -7.813171
Q std: 9.329861
Actor loss: 7.817160
Action reg: 0.003989
  l1.weight: grad_norm = 0.007418
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.006971
Total gradient norm: 0.043503
=== Actor Training Debug (Iteration 2068) ===
Q mean: -6.772661
Q std: 8.672198
Actor loss: 6.776632
Action reg: 0.003971
  l1.weight: grad_norm = 0.055527
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.056007
Total gradient norm: 0.337847
=== Actor Training Debug (Iteration 2069) ===
Q mean: -6.562089
Q std: 8.320877
Actor loss: 6.566062
Action reg: 0.003973
  l1.weight: grad_norm = 0.016759
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.015728
Total gradient norm: 0.082720
=== Actor Training Debug (Iteration 2070) ===
Q mean: -7.839982
Q std: 8.677548
Actor loss: 7.843964
Action reg: 0.003982
  l1.weight: grad_norm = 0.008514
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.009308
Total gradient norm: 0.046727
=== Actor Training Debug (Iteration 2071) ===
Q mean: -7.115428
Q std: 8.065269
Actor loss: 7.119383
Action reg: 0.003955
  l1.weight: grad_norm = 0.041179
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.035656
Total gradient norm: 0.170126
=== Actor Training Debug (Iteration 2072) ===
Q mean: -8.103874
Q std: 9.194282
Actor loss: 8.107834
Action reg: 0.003960
  l1.weight: grad_norm = 0.052874
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.049253
Total gradient norm: 0.278877
=== Actor Training Debug (Iteration 2073) ===
Q mean: -6.857997
Q std: 8.895135
Actor loss: 6.861970
Action reg: 0.003973
  l1.weight: grad_norm = 0.037578
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.037441
Total gradient norm: 0.222491
=== Actor Training Debug (Iteration 2074) ===
Q mean: -6.411228
Q std: 8.508735
Actor loss: 6.415213
Action reg: 0.003985
  l1.weight: grad_norm = 0.014687
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.014873
Total gradient norm: 0.075809
=== Actor Training Debug (Iteration 2075) ===
Q mean: -7.263138
Q std: 8.724689
Actor loss: 7.267111
Action reg: 0.003973
  l1.weight: grad_norm = 0.045423
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.042637
Total gradient norm: 0.248617
=== Actor Training Debug (Iteration 2076) ===
Q mean: -7.831192
Q std: 8.465689
Actor loss: 7.835172
Action reg: 0.003981
  l1.weight: grad_norm = 0.025770
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.024518
Total gradient norm: 0.144907
=== Actor Training Debug (Iteration 2077) ===
Q mean: -7.260990
Q std: 8.442992
Actor loss: 7.264957
Action reg: 0.003967
  l1.weight: grad_norm = 0.065744
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.064575
Total gradient norm: 0.350889
=== Actor Training Debug (Iteration 2078) ===
Q mean: -7.463752
Q std: 8.965335
Actor loss: 7.467716
Action reg: 0.003964
  l1.weight: grad_norm = 0.033834
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.030455
Total gradient norm: 0.184882
=== Actor Training Debug (Iteration 2079) ===
Q mean: -7.620243
Q std: 8.832186
Actor loss: 7.624218
Action reg: 0.003975
  l1.weight: grad_norm = 0.025425
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.026946
Total gradient norm: 0.195799
=== Actor Training Debug (Iteration 2080) ===
Q mean: -8.196123
Q std: 9.054728
Actor loss: 8.200089
Action reg: 0.003966
  l1.weight: grad_norm = 0.024100
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.022054
Total gradient norm: 0.097542
=== Actor Training Debug (Iteration 2081) ===
Q mean: -7.101051
Q std: 8.665436
Actor loss: 7.105016
Action reg: 0.003965
  l1.weight: grad_norm = 0.034264
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.029366
Total gradient norm: 0.154854
=== Actor Training Debug (Iteration 2082) ===
Q mean: -6.917239
Q std: 9.271179
Actor loss: 6.921220
Action reg: 0.003981
  l1.weight: grad_norm = 0.016561
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.016412
Total gradient norm: 0.095621
=== Actor Training Debug (Iteration 2083) ===
Q mean: -7.100596
Q std: 8.654486
Actor loss: 7.104581
Action reg: 0.003985
  l1.weight: grad_norm = 0.045211
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.042192
Total gradient norm: 0.249581
=== Actor Training Debug (Iteration 2084) ===
Q mean: -7.089328
Q std: 8.754972
Actor loss: 7.093299
Action reg: 0.003972
  l1.weight: grad_norm = 0.012078
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.011171
Total gradient norm: 0.057353
=== Actor Training Debug (Iteration 2085) ===
Q mean: -6.862132
Q std: 8.722295
Actor loss: 6.866083
Action reg: 0.003951
  l1.weight: grad_norm = 0.026458
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.022657
Total gradient norm: 0.098848
=== Actor Training Debug (Iteration 2086) ===
Q mean: -6.884385
Q std: 8.829538
Actor loss: 6.888354
Action reg: 0.003969
  l1.weight: grad_norm = 0.026989
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.025340
Total gradient norm: 0.144130
=== Actor Training Debug (Iteration 2087) ===
Q mean: -7.509673
Q std: 8.843431
Actor loss: 7.513651
Action reg: 0.003978
  l1.weight: grad_norm = 0.007153
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.006072
Total gradient norm: 0.023832
=== Actor Training Debug (Iteration 2088) ===
Q mean: -7.168326
Q std: 9.360982
Actor loss: 7.172304
Action reg: 0.003978
  l1.weight: grad_norm = 0.017370
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.015307
Total gradient norm: 0.077177
=== Actor Training Debug (Iteration 2089) ===
Q mean: -7.709670
Q std: 8.258114
Actor loss: 7.713647
Action reg: 0.003977
  l1.weight: grad_norm = 0.046905
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.045988
Total gradient norm: 0.227534
=== Actor Training Debug (Iteration 2090) ===
Q mean: -7.215896
Q std: 8.747250
Actor loss: 7.219861
Action reg: 0.003965
  l1.weight: grad_norm = 0.035596
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.030987
Total gradient norm: 0.123772
=== Actor Training Debug (Iteration 2091) ===
Q mean: -7.250911
Q std: 9.093852
Actor loss: 7.254896
Action reg: 0.003985
  l1.weight: grad_norm = 0.027156
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.024868
Total gradient norm: 0.117246
=== Actor Training Debug (Iteration 2092) ===
Q mean: -6.566240
Q std: 7.985106
Actor loss: 6.570217
Action reg: 0.003977
  l1.weight: grad_norm = 0.020530
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.019276
Total gradient norm: 0.088158
=== Actor Training Debug (Iteration 2093) ===
Q mean: -8.099257
Q std: 9.315827
Actor loss: 8.103229
Action reg: 0.003972
  l1.weight: grad_norm = 0.015335
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.015617
Total gradient norm: 0.062720
=== Actor Training Debug (Iteration 2094) ===
Q mean: -6.496562
Q std: 8.439962
Actor loss: 6.500540
Action reg: 0.003978
  l1.weight: grad_norm = 0.019274
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.016670
Total gradient norm: 0.097372
=== Actor Training Debug (Iteration 2095) ===
Q mean: -6.266173
Q std: 8.224916
Actor loss: 6.270155
Action reg: 0.003981
  l1.weight: grad_norm = 0.015754
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.013759
Total gradient norm: 0.082712
=== Actor Training Debug (Iteration 2096) ===
Q mean: -6.280015
Q std: 8.400132
Actor loss: 6.283993
Action reg: 0.003978
  l1.weight: grad_norm = 0.017864
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.016881
Total gradient norm: 0.112299
=== Actor Training Debug (Iteration 2097) ===
Q mean: -8.096319
Q std: 9.230844
Actor loss: 8.100293
Action reg: 0.003974
  l1.weight: grad_norm = 0.036192
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.030131
Total gradient norm: 0.118418
=== Actor Training Debug (Iteration 2098) ===
Q mean: -6.940535
Q std: 8.680680
Actor loss: 6.944505
Action reg: 0.003971
  l1.weight: grad_norm = 0.020122
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.018838
Total gradient norm: 0.090781
=== Actor Training Debug (Iteration 2099) ===
Q mean: -6.792781
Q std: 8.961946
Actor loss: 6.796744
Action reg: 0.003963
  l1.weight: grad_norm = 0.052330
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.045216
Total gradient norm: 0.206320
=== Actor Training Debug (Iteration 2100) ===
Q mean: -6.749148
Q std: 8.590591
Actor loss: 6.753129
Action reg: 0.003981
  l1.weight: grad_norm = 0.020507
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.017034
Total gradient norm: 0.076516
Episode 71: Steps=100, Reward=-276.764, Buffer_size=7100
=== Actor Training Debug (Iteration 2101) ===
Q mean: -8.189793
Q std: 9.430904
Actor loss: 8.193770
Action reg: 0.003978
  l1.weight: grad_norm = 0.015370
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.013645
Total gradient norm: 0.056562
=== Actor Training Debug (Iteration 2102) ===
Q mean: -9.013074
Q std: 10.032358
Actor loss: 9.017057
Action reg: 0.003983
  l1.weight: grad_norm = 0.019603
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.020821
Total gradient norm: 0.106748
=== Actor Training Debug (Iteration 2103) ===
Q mean: -7.979644
Q std: 9.631699
Actor loss: 7.983622
Action reg: 0.003978
  l1.weight: grad_norm = 0.026334
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.027134
Total gradient norm: 0.145972
=== Actor Training Debug (Iteration 2104) ===
Q mean: -7.160964
Q std: 9.343679
Actor loss: 7.164934
Action reg: 0.003970
  l1.weight: grad_norm = 0.016565
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.015581
Total gradient norm: 0.094667
=== Actor Training Debug (Iteration 2105) ===
Q mean: -6.768647
Q std: 8.547996
Actor loss: 6.772632
Action reg: 0.003985
  l1.weight: grad_norm = 0.025884
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.023506
Total gradient norm: 0.101495
=== Actor Training Debug (Iteration 2106) ===
Q mean: -7.173245
Q std: 8.923295
Actor loss: 7.177226
Action reg: 0.003980
  l1.weight: grad_norm = 0.008264
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.007217
Total gradient norm: 0.030214
=== Actor Training Debug (Iteration 2107) ===
Q mean: -7.883568
Q std: 9.009301
Actor loss: 7.887546
Action reg: 0.003978
  l1.weight: grad_norm = 0.054343
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.047191
Total gradient norm: 0.309756
=== Actor Training Debug (Iteration 2108) ===
Q mean: -7.989159
Q std: 8.916703
Actor loss: 7.993139
Action reg: 0.003981
  l1.weight: grad_norm = 0.025314
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.021530
Total gradient norm: 0.101154
=== Actor Training Debug (Iteration 2109) ===
Q mean: -7.059399
Q std: 9.154740
Actor loss: 7.063377
Action reg: 0.003979
  l1.weight: grad_norm = 0.045673
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.039429
Total gradient norm: 0.214994
=== Actor Training Debug (Iteration 2110) ===
Q mean: -7.027841
Q std: 8.652718
Actor loss: 7.031815
Action reg: 0.003974
  l1.weight: grad_norm = 0.034568
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.032748
Total gradient norm: 0.177347
=== Actor Training Debug (Iteration 2111) ===
Q mean: -6.904868
Q std: 8.643539
Actor loss: 6.908844
Action reg: 0.003976
  l1.weight: grad_norm = 0.017998
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.016881
Total gradient norm: 0.080847
=== Actor Training Debug (Iteration 2112) ===
Q mean: -6.930128
Q std: 8.167171
Actor loss: 6.934105
Action reg: 0.003978
  l1.weight: grad_norm = 0.049150
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.044601
Total gradient norm: 0.301979
=== Actor Training Debug (Iteration 2113) ===
Q mean: -6.366961
Q std: 8.631775
Actor loss: 6.370925
Action reg: 0.003965
  l1.weight: grad_norm = 0.041799
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.040052
Total gradient norm: 0.240297
=== Actor Training Debug (Iteration 2114) ===
Q mean: -7.550154
Q std: 8.603814
Actor loss: 7.554126
Action reg: 0.003972
  l1.weight: grad_norm = 0.035651
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.033513
Total gradient norm: 0.164011
=== Actor Training Debug (Iteration 2115) ===
Q mean: -7.629250
Q std: 9.022632
Actor loss: 7.633211
Action reg: 0.003961
  l1.weight: grad_norm = 0.043538
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.042365
Total gradient norm: 0.253670
=== Actor Training Debug (Iteration 2116) ===
Q mean: -6.980348
Q std: 9.193380
Actor loss: 6.984324
Action reg: 0.003976
  l1.weight: grad_norm = 0.032328
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.030335
Total gradient norm: 0.126740
=== Actor Training Debug (Iteration 2117) ===
Q mean: -6.415730
Q std: 8.179713
Actor loss: 6.419711
Action reg: 0.003982
  l1.weight: grad_norm = 0.026316
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.022355
Total gradient norm: 0.102312
=== Actor Training Debug (Iteration 2118) ===
Q mean: -8.124479
Q std: 9.548499
Actor loss: 8.128458
Action reg: 0.003978
  l1.weight: grad_norm = 0.033007
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.036537
Total gradient norm: 0.216327
=== Actor Training Debug (Iteration 2119) ===
Q mean: -7.574070
Q std: 8.850061
Actor loss: 7.578045
Action reg: 0.003975
  l1.weight: grad_norm = 0.029360
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.025294
Total gradient norm: 0.111782
=== Actor Training Debug (Iteration 2120) ===
Q mean: -7.591265
Q std: 8.609537
Actor loss: 7.595231
Action reg: 0.003966
  l1.weight: grad_norm = 0.036270
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.030460
Total gradient norm: 0.146191
=== Actor Training Debug (Iteration 2121) ===
Q mean: -7.362077
Q std: 9.426306
Actor loss: 7.366060
Action reg: 0.003983
  l1.weight: grad_norm = 0.032012
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.028752
Total gradient norm: 0.106447
=== Actor Training Debug (Iteration 2122) ===
Q mean: -7.578670
Q std: 9.058808
Actor loss: 7.582654
Action reg: 0.003984
  l1.weight: grad_norm = 0.021869
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.020157
Total gradient norm: 0.094753
=== Actor Training Debug (Iteration 2123) ===
Q mean: -6.784952
Q std: 8.695282
Actor loss: 6.788934
Action reg: 0.003982
  l1.weight: grad_norm = 0.009994
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.009103
Total gradient norm: 0.057175
=== Actor Training Debug (Iteration 2124) ===
Q mean: -7.534986
Q std: 9.469796
Actor loss: 7.538972
Action reg: 0.003986
  l1.weight: grad_norm = 0.041537
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.035042
Total gradient norm: 0.185320
=== Actor Training Debug (Iteration 2125) ===
Q mean: -7.401504
Q std: 8.268126
Actor loss: 7.405484
Action reg: 0.003980
  l1.weight: grad_norm = 0.022222
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.020273
Total gradient norm: 0.080238
=== Actor Training Debug (Iteration 2126) ===
Q mean: -9.038308
Q std: 9.911313
Actor loss: 9.042292
Action reg: 0.003983
  l1.weight: grad_norm = 0.021791
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.019199
Total gradient norm: 0.114074
=== Actor Training Debug (Iteration 2127) ===
Q mean: -7.386866
Q std: 8.348570
Actor loss: 7.390838
Action reg: 0.003972
  l1.weight: grad_norm = 0.013707
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.013208
Total gradient norm: 0.073299
=== Actor Training Debug (Iteration 2128) ===
Q mean: -7.131888
Q std: 8.754225
Actor loss: 7.135873
Action reg: 0.003985
  l1.weight: grad_norm = 0.058771
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.056583
Total gradient norm: 0.385540
=== Actor Training Debug (Iteration 2129) ===
Q mean: -7.010973
Q std: 8.648055
Actor loss: 7.014955
Action reg: 0.003982
  l1.weight: grad_norm = 0.012264
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.010930
Total gradient norm: 0.055490
=== Actor Training Debug (Iteration 2130) ===
Q mean: -8.168146
Q std: 9.530805
Actor loss: 8.172119
Action reg: 0.003973
  l1.weight: grad_norm = 0.022600
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.022199
Total gradient norm: 0.115889
=== Actor Training Debug (Iteration 2131) ===
Q mean: -6.807298
Q std: 8.622705
Actor loss: 6.811289
Action reg: 0.003991
  l1.weight: grad_norm = 0.019198
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.016307
Total gradient norm: 0.097801
=== Actor Training Debug (Iteration 2132) ===
Q mean: -7.604760
Q std: 9.665825
Actor loss: 7.608742
Action reg: 0.003982
  l1.weight: grad_norm = 0.024785
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.022294
Total gradient norm: 0.111449
=== Actor Training Debug (Iteration 2133) ===
Q mean: -7.873472
Q std: 8.990906
Actor loss: 7.877453
Action reg: 0.003980
  l1.weight: grad_norm = 0.013376
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.011930
Total gradient norm: 0.052914
=== Actor Training Debug (Iteration 2134) ===
Q mean: -6.714997
Q std: 8.461692
Actor loss: 6.718984
Action reg: 0.003986
  l1.weight: grad_norm = 0.021715
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.020124
Total gradient norm: 0.076543
=== Actor Training Debug (Iteration 2135) ===
Q mean: -6.436974
Q std: 9.163116
Actor loss: 6.440958
Action reg: 0.003984
  l1.weight: grad_norm = 0.036621
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.036868
Total gradient norm: 0.186933
=== Actor Training Debug (Iteration 2136) ===
Q mean: -6.526323
Q std: 8.733501
Actor loss: 6.530302
Action reg: 0.003978
  l1.weight: grad_norm = 0.022258
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.018561
Total gradient norm: 0.078894
=== Actor Training Debug (Iteration 2137) ===
Q mean: -7.567268
Q std: 9.267776
Actor loss: 7.571251
Action reg: 0.003983
  l1.weight: grad_norm = 0.055561
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.045684
Total gradient norm: 0.332219
=== Actor Training Debug (Iteration 2138) ===
Q mean: -7.615542
Q std: 9.283059
Actor loss: 7.619535
Action reg: 0.003993
  l1.weight: grad_norm = 0.027576
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.022143
Total gradient norm: 0.091332
=== Actor Training Debug (Iteration 2139) ===
Q mean: -6.171335
Q std: 8.836223
Actor loss: 6.175314
Action reg: 0.003979
  l1.weight: grad_norm = 0.024811
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.022583
Total gradient norm: 0.101413
=== Actor Training Debug (Iteration 2140) ===
Q mean: -7.914067
Q std: 9.939188
Actor loss: 7.918053
Action reg: 0.003986
  l1.weight: grad_norm = 0.005579
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.004580
Total gradient norm: 0.021954
=== Actor Training Debug (Iteration 2141) ===
Q mean: -7.317406
Q std: 9.109022
Actor loss: 7.321365
Action reg: 0.003959
  l1.weight: grad_norm = 0.030403
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.022985
Total gradient norm: 0.125287
=== Actor Training Debug (Iteration 2142) ===
Q mean: -7.856867
Q std: 9.284878
Actor loss: 7.860847
Action reg: 0.003979
  l1.weight: grad_norm = 0.028132
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.028469
Total gradient norm: 0.134201
=== Actor Training Debug (Iteration 2143) ===
Q mean: -7.554654
Q std: 9.700541
Actor loss: 7.558638
Action reg: 0.003984
  l1.weight: grad_norm = 0.020728
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.019101
Total gradient norm: 0.127010
=== Actor Training Debug (Iteration 2144) ===
Q mean: -7.061416
Q std: 8.654248
Actor loss: 7.065398
Action reg: 0.003982
  l1.weight: grad_norm = 0.011750
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.010699
Total gradient norm: 0.050682
=== Actor Training Debug (Iteration 2145) ===
Q mean: -7.426951
Q std: 8.885608
Actor loss: 7.430929
Action reg: 0.003977
  l1.weight: grad_norm = 0.011303
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.009470
Total gradient norm: 0.048733
=== Actor Training Debug (Iteration 2146) ===
Q mean: -7.207252
Q std: 9.720311
Actor loss: 7.211238
Action reg: 0.003987
  l1.weight: grad_norm = 0.009525
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.008069
Total gradient norm: 0.037236
=== Actor Training Debug (Iteration 2147) ===
Q mean: -6.475242
Q std: 8.595961
Actor loss: 6.479231
Action reg: 0.003989
  l1.weight: grad_norm = 0.017426
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.017100
Total gradient norm: 0.093302
=== Actor Training Debug (Iteration 2148) ===
Q mean: -7.045031
Q std: 9.318003
Actor loss: 7.049020
Action reg: 0.003989
  l1.weight: grad_norm = 0.006681
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.006305
Total gradient norm: 0.025342
=== Actor Training Debug (Iteration 2149) ===
Q mean: -6.970198
Q std: 8.596154
Actor loss: 6.974178
Action reg: 0.003980
  l1.weight: grad_norm = 0.026984
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.025656
Total gradient norm: 0.094225
=== Actor Training Debug (Iteration 2150) ===
Q mean: -6.529027
Q std: 8.753067
Actor loss: 6.533016
Action reg: 0.003989
  l1.weight: grad_norm = 0.022415
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.020167
Total gradient norm: 0.088818
=== Actor Training Debug (Iteration 2151) ===
Q mean: -7.537147
Q std: 9.349707
Actor loss: 7.541128
Action reg: 0.003981
  l1.weight: grad_norm = 0.021303
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.017412
Total gradient norm: 0.085223
=== Actor Training Debug (Iteration 2152) ===
Q mean: -7.634201
Q std: 8.965103
Actor loss: 7.638182
Action reg: 0.003981
  l1.weight: grad_norm = 0.032347
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.032462
Total gradient norm: 0.171519
=== Actor Training Debug (Iteration 2153) ===
Q mean: -7.700231
Q std: 9.376073
Actor loss: 7.704210
Action reg: 0.003980
  l1.weight: grad_norm = 0.015947
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.013810
Total gradient norm: 0.065104
=== Actor Training Debug (Iteration 2154) ===
Q mean: -8.660643
Q std: 9.687551
Actor loss: 8.664620
Action reg: 0.003978
  l1.weight: grad_norm = 0.027907
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.025634
Total gradient norm: 0.127390
=== Actor Training Debug (Iteration 2155) ===
Q mean: -8.130024
Q std: 10.189622
Actor loss: 8.134006
Action reg: 0.003982
  l1.weight: grad_norm = 0.025574
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.023207
Total gradient norm: 0.088554
=== Actor Training Debug (Iteration 2156) ===
Q mean: -6.761100
Q std: 9.102299
Actor loss: 6.765068
Action reg: 0.003968
  l1.weight: grad_norm = 0.012531
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.012537
Total gradient norm: 0.066479
=== Actor Training Debug (Iteration 2157) ===
Q mean: -6.915382
Q std: 8.879876
Actor loss: 6.919354
Action reg: 0.003972
  l1.weight: grad_norm = 0.040146
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.035778
Total gradient norm: 0.161758
=== Actor Training Debug (Iteration 2158) ===
Q mean: -6.953704
Q std: 8.472119
Actor loss: 6.957688
Action reg: 0.003985
  l1.weight: grad_norm = 0.032355
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.027646
Total gradient norm: 0.123587
=== Actor Training Debug (Iteration 2159) ===
Q mean: -7.261442
Q std: 8.573262
Actor loss: 7.265418
Action reg: 0.003976
  l1.weight: grad_norm = 0.024425
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.020984
Total gradient norm: 0.094483
=== Actor Training Debug (Iteration 2160) ===
Q mean: -6.942069
Q std: 8.764984
Actor loss: 6.946056
Action reg: 0.003987
  l1.weight: grad_norm = 0.028408
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.025746
Total gradient norm: 0.110329
=== Actor Training Debug (Iteration 2161) ===
Q mean: -7.085763
Q std: 9.318702
Actor loss: 7.089749
Action reg: 0.003987
  l1.weight: grad_norm = 0.009536
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.008340
Total gradient norm: 0.040848
=== Actor Training Debug (Iteration 2162) ===
Q mean: -7.002244
Q std: 9.410185
Actor loss: 7.006222
Action reg: 0.003978
  l1.weight: grad_norm = 0.024219
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.023019
Total gradient norm: 0.109207
=== Actor Training Debug (Iteration 2163) ===
Q mean: -7.293303
Q std: 8.799066
Actor loss: 7.297285
Action reg: 0.003982
  l1.weight: grad_norm = 0.024145
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.022676
Total gradient norm: 0.110763
=== Actor Training Debug (Iteration 2164) ===
Q mean: -7.614766
Q std: 9.033700
Actor loss: 7.618743
Action reg: 0.003978
  l1.weight: grad_norm = 0.033640
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.033969
Total gradient norm: 0.175300
=== Actor Training Debug (Iteration 2165) ===
Q mean: -6.978566
Q std: 8.886841
Actor loss: 6.982538
Action reg: 0.003972
  l1.weight: grad_norm = 0.027109
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.022050
Total gradient norm: 0.099849
=== Actor Training Debug (Iteration 2166) ===
Q mean: -7.085389
Q std: 9.321790
Actor loss: 7.089365
Action reg: 0.003977
  l1.weight: grad_norm = 0.007845
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.007354
Total gradient norm: 0.039005
=== Actor Training Debug (Iteration 2167) ===
Q mean: -6.966990
Q std: 9.260472
Actor loss: 6.970962
Action reg: 0.003972
  l1.weight: grad_norm = 0.027504
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.023162
Total gradient norm: 0.131293
=== Actor Training Debug (Iteration 2168) ===
Q mean: -7.964322
Q std: 8.827562
Actor loss: 7.968295
Action reg: 0.003972
  l1.weight: grad_norm = 0.068012
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.051284
Total gradient norm: 0.176052
=== Actor Training Debug (Iteration 2169) ===
Q mean: -7.662267
Q std: 9.159998
Actor loss: 7.666245
Action reg: 0.003977
  l1.weight: grad_norm = 0.028485
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.026463
Total gradient norm: 0.140419
=== Actor Training Debug (Iteration 2170) ===
Q mean: -7.257640
Q std: 9.281168
Actor loss: 7.261625
Action reg: 0.003985
  l1.weight: grad_norm = 0.009425
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.008993
Total gradient norm: 0.053000
=== Actor Training Debug (Iteration 2171) ===
Q mean: -7.588448
Q std: 9.300848
Actor loss: 7.592433
Action reg: 0.003985
  l1.weight: grad_norm = 0.013484
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.011960
Total gradient norm: 0.068031
=== Actor Training Debug (Iteration 2172) ===
Q mean: -7.371181
Q std: 8.907242
Actor loss: 7.375160
Action reg: 0.003980
  l1.weight: grad_norm = 0.022888
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.020753
Total gradient norm: 0.120001
=== Actor Training Debug (Iteration 2173) ===
Q mean: -6.845892
Q std: 8.557768
Actor loss: 6.849860
Action reg: 0.003968
  l1.weight: grad_norm = 0.032966
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.025976
Total gradient norm: 0.107106
=== Actor Training Debug (Iteration 2174) ===
Q mean: -6.868875
Q std: 8.608893
Actor loss: 6.872857
Action reg: 0.003983
  l1.weight: grad_norm = 0.030314
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.027328
Total gradient norm: 0.119273
=== Actor Training Debug (Iteration 2175) ===
Q mean: -6.996797
Q std: 8.613815
Actor loss: 7.000775
Action reg: 0.003979
  l1.weight: grad_norm = 0.028465
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.024103
Total gradient norm: 0.104852
=== Actor Training Debug (Iteration 2176) ===
Q mean: -7.762149
Q std: 9.557122
Actor loss: 7.766129
Action reg: 0.003981
  l1.weight: grad_norm = 0.017170
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.015953
Total gradient norm: 0.118076
=== Actor Training Debug (Iteration 2177) ===
Q mean: -7.586975
Q std: 9.668753
Actor loss: 7.590956
Action reg: 0.003981
  l1.weight: grad_norm = 0.032607
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.027137
Total gradient norm: 0.154667
=== Actor Training Debug (Iteration 2178) ===
Q mean: -7.564466
Q std: 9.023545
Actor loss: 7.568427
Action reg: 0.003960
  l1.weight: grad_norm = 0.019799
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.017905
Total gradient norm: 0.096109
=== Actor Training Debug (Iteration 2179) ===
Q mean: -7.465202
Q std: 8.866148
Actor loss: 7.469172
Action reg: 0.003970
  l1.weight: grad_norm = 0.028083
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.022154
Total gradient norm: 0.105580
=== Actor Training Debug (Iteration 2180) ===
Q mean: -6.515387
Q std: 8.651261
Actor loss: 6.519353
Action reg: 0.003967
  l1.weight: grad_norm = 0.035869
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.037217
Total gradient norm: 0.157244
=== Actor Training Debug (Iteration 2181) ===
Q mean: -7.800208
Q std: 8.681208
Actor loss: 7.804185
Action reg: 0.003978
  l1.weight: grad_norm = 0.014130
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.014185
Total gradient norm: 0.080443
=== Actor Training Debug (Iteration 2182) ===
Q mean: -7.123939
Q std: 9.420304
Actor loss: 7.127908
Action reg: 0.003970
  l1.weight: grad_norm = 0.035852
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.031306
Total gradient norm: 0.136958
=== Actor Training Debug (Iteration 2183) ===
Q mean: -7.653931
Q std: 9.186438
Actor loss: 7.657916
Action reg: 0.003985
  l1.weight: grad_norm = 0.009954
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.009759
Total gradient norm: 0.059401
=== Actor Training Debug (Iteration 2184) ===
Q mean: -6.819902
Q std: 9.223342
Actor loss: 6.823885
Action reg: 0.003983
  l1.weight: grad_norm = 0.032693
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.026908
Total gradient norm: 0.165478
=== Actor Training Debug (Iteration 2185) ===
Q mean: -7.208301
Q std: 9.146368
Actor loss: 7.212285
Action reg: 0.003984
  l1.weight: grad_norm = 0.024750
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.023125
Total gradient norm: 0.098452
=== Actor Training Debug (Iteration 2186) ===
Q mean: -8.261616
Q std: 9.577581
Actor loss: 8.265585
Action reg: 0.003969
  l1.weight: grad_norm = 0.029553
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.026264
Total gradient norm: 0.134204
=== Actor Training Debug (Iteration 2187) ===
Q mean: -6.866465
Q std: 8.798889
Actor loss: 6.870421
Action reg: 0.003957
  l1.weight: grad_norm = 0.115595
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.096545
Total gradient norm: 0.379918
=== Actor Training Debug (Iteration 2188) ===
Q mean: -6.799849
Q std: 9.093409
Actor loss: 6.803819
Action reg: 0.003970
  l1.weight: grad_norm = 0.069555
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.065952
Total gradient norm: 0.364257
=== Actor Training Debug (Iteration 2189) ===
Q mean: -6.749399
Q std: 9.282422
Actor loss: 6.753372
Action reg: 0.003973
  l1.weight: grad_norm = 0.011509
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.011633
Total gradient norm: 0.093656
=== Actor Training Debug (Iteration 2190) ===
Q mean: -8.049987
Q std: 9.389078
Actor loss: 8.053963
Action reg: 0.003976
  l1.weight: grad_norm = 0.036477
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.030730
Total gradient norm: 0.145425
=== Actor Training Debug (Iteration 2191) ===
Q mean: -7.961370
Q std: 9.653940
Actor loss: 7.965348
Action reg: 0.003978
  l1.weight: grad_norm = 0.027898
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.024762
Total gradient norm: 0.090491
=== Actor Training Debug (Iteration 2192) ===
Q mean: -8.304539
Q std: 9.636510
Actor loss: 8.308511
Action reg: 0.003972
  l1.weight: grad_norm = 0.033259
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.027784
Total gradient norm: 0.150074
=== Actor Training Debug (Iteration 2193) ===
Q mean: -7.424376
Q std: 8.783349
Actor loss: 7.428360
Action reg: 0.003983
  l1.weight: grad_norm = 0.013135
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.011430
Total gradient norm: 0.047070
=== Actor Training Debug (Iteration 2194) ===
Q mean: -8.045271
Q std: 9.949344
Actor loss: 8.049250
Action reg: 0.003979
  l1.weight: grad_norm = 0.046536
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.037398
Total gradient norm: 0.212130
=== Actor Training Debug (Iteration 2195) ===
Q mean: -8.103443
Q std: 9.654456
Actor loss: 8.107412
Action reg: 0.003969
  l1.weight: grad_norm = 0.045213
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.040150
Total gradient norm: 0.223282
=== Actor Training Debug (Iteration 2196) ===
Q mean: -8.073364
Q std: 9.425793
Actor loss: 8.077334
Action reg: 0.003970
  l1.weight: grad_norm = 0.049368
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.040977
Total gradient norm: 0.202967
=== Actor Training Debug (Iteration 2197) ===
Q mean: -7.945952
Q std: 9.848215
Actor loss: 7.949939
Action reg: 0.003987
  l1.weight: grad_norm = 0.016650
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.014536
Total gradient norm: 0.079013
=== Actor Training Debug (Iteration 2198) ===
Q mean: -7.052549
Q std: 9.091345
Actor loss: 7.056518
Action reg: 0.003969
  l1.weight: grad_norm = 0.042767
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.039161
Total gradient norm: 0.224304
=== Actor Training Debug (Iteration 2199) ===
Q mean: -7.134778
Q std: 8.546193
Actor loss: 7.138761
Action reg: 0.003983
  l1.weight: grad_norm = 0.022509
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.021444
Total gradient norm: 0.101648
=== Actor Training Debug (Iteration 2200) ===
Q mean: -6.159046
Q std: 8.930784
Actor loss: 6.163014
Action reg: 0.003968
  l1.weight: grad_norm = 0.019193
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.017028
Total gradient norm: 0.083596
=== Actor Training Debug (Iteration 2201) ===
Q mean: -8.021621
Q std: 9.907845
Actor loss: 8.025595
Action reg: 0.003974
  l1.weight: grad_norm = 0.030247
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.027265
Total gradient norm: 0.115329
=== Actor Training Debug (Iteration 2202) ===
Q mean: -8.290314
Q std: 9.941810
Actor loss: 8.294291
Action reg: 0.003976
  l1.weight: grad_norm = 0.017690
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.016003
Total gradient norm: 0.073893
=== Actor Training Debug (Iteration 2203) ===
Q mean: -6.473647
Q std: 8.205032
Actor loss: 6.477623
Action reg: 0.003976
  l1.weight: grad_norm = 0.018702
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.014536
Total gradient norm: 0.077470
=== Actor Training Debug (Iteration 2204) ===
Q mean: -6.851598
Q std: 8.372187
Actor loss: 6.855571
Action reg: 0.003973
  l1.weight: grad_norm = 0.067168
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.057165
Total gradient norm: 0.241599
=== Actor Training Debug (Iteration 2205) ===
Q mean: -7.078381
Q std: 9.286697
Actor loss: 7.082362
Action reg: 0.003981
  l1.weight: grad_norm = 0.034223
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.031622
Total gradient norm: 0.174602
=== Actor Training Debug (Iteration 2206) ===
Q mean: -7.441438
Q std: 9.872561
Actor loss: 7.445414
Action reg: 0.003976
  l1.weight: grad_norm = 0.033212
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.030221
Total gradient norm: 0.150463
=== Actor Training Debug (Iteration 2207) ===
Q mean: -8.969439
Q std: 10.246666
Actor loss: 8.973434
Action reg: 0.003996
  l1.weight: grad_norm = 0.009236
  l1.bias: grad_norm = 0.000004
  l2.weight: grad_norm = 0.009012
Total gradient norm: 0.045620
=== Actor Training Debug (Iteration 2208) ===
Q mean: -6.827027
Q std: 8.802895
Actor loss: 6.831007
Action reg: 0.003980
  l1.weight: grad_norm = 0.042954
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.043669
Total gradient norm: 0.268859
=== Actor Training Debug (Iteration 2209) ===
Q mean: -6.952446
Q std: 9.082832
Actor loss: 6.956427
Action reg: 0.003980
  l1.weight: grad_norm = 0.037363
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.033670
Total gradient norm: 0.176990
=== Actor Training Debug (Iteration 2210) ===
Q mean: -7.490833
Q std: 8.437878
Actor loss: 7.494817
Action reg: 0.003984
  l1.weight: grad_norm = 0.034703
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.028270
Total gradient norm: 0.112510
=== Actor Training Debug (Iteration 2211) ===
Q mean: -6.602293
Q std: 8.477835
Actor loss: 6.606280
Action reg: 0.003987
  l1.weight: grad_norm = 0.016654
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.015243
Total gradient norm: 0.083753
=== Actor Training Debug (Iteration 2212) ===
Q mean: -7.703348
Q std: 9.278400
Actor loss: 7.707317
Action reg: 0.003969
  l1.weight: grad_norm = 0.022410
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.020409
Total gradient norm: 0.130538
=== Actor Training Debug (Iteration 2213) ===
Q mean: -7.751261
Q std: 9.524942
Actor loss: 7.755243
Action reg: 0.003982
  l1.weight: grad_norm = 0.019260
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.017857
Total gradient norm: 0.080090
=== Actor Training Debug (Iteration 2214) ===
Q mean: -7.491325
Q std: 9.278284
Actor loss: 7.495309
Action reg: 0.003984
  l1.weight: grad_norm = 0.010353
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.010887
Total gradient norm: 0.057743
=== Actor Training Debug (Iteration 2215) ===
Q mean: -7.761912
Q std: 9.325911
Actor loss: 7.765893
Action reg: 0.003980
  l1.weight: grad_norm = 0.022189
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.022133
Total gradient norm: 0.124537
=== Actor Training Debug (Iteration 2216) ===
Q mean: -7.649779
Q std: 9.695203
Actor loss: 7.653759
Action reg: 0.003980
  l1.weight: grad_norm = 0.033620
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.030164
Total gradient norm: 0.165800
=== Actor Training Debug (Iteration 2217) ===
Q mean: -6.840431
Q std: 8.928268
Actor loss: 6.844405
Action reg: 0.003974
  l1.weight: grad_norm = 0.023130
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.019666
Total gradient norm: 0.086136
=== Actor Training Debug (Iteration 2218) ===
Q mean: -7.719908
Q std: 9.252892
Actor loss: 7.723883
Action reg: 0.003975
  l1.weight: grad_norm = 0.024875
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.021349
Total gradient norm: 0.089064
=== Actor Training Debug (Iteration 2219) ===
Q mean: -8.215553
Q std: 9.201047
Actor loss: 8.219536
Action reg: 0.003982
  l1.weight: grad_norm = 0.024091
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.019284
Total gradient norm: 0.079549
=== Actor Training Debug (Iteration 2220) ===
Q mean: -6.970978
Q std: 9.143108
Actor loss: 6.974952
Action reg: 0.003974
  l1.weight: grad_norm = 0.025606
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.023145
Total gradient norm: 0.111093
=== Actor Training Debug (Iteration 2221) ===
Q mean: -6.407544
Q std: 9.111629
Actor loss: 6.411518
Action reg: 0.003974
  l1.weight: grad_norm = 0.021761
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.017171
Total gradient norm: 0.077989
=== Actor Training Debug (Iteration 2222) ===
Q mean: -7.858964
Q std: 9.923078
Actor loss: 7.862947
Action reg: 0.003983
  l1.weight: grad_norm = 0.009557
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.008643
Total gradient norm: 0.040557
=== Actor Training Debug (Iteration 2223) ===
Q mean: -7.527018
Q std: 8.566977
Actor loss: 7.531006
Action reg: 0.003989
  l1.weight: grad_norm = 0.012228
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.012712
Total gradient norm: 0.076013
=== Actor Training Debug (Iteration 2224) ===
Q mean: -7.979206
Q std: 9.358373
Actor loss: 7.983191
Action reg: 0.003985
  l1.weight: grad_norm = 0.025773
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.023524
Total gradient norm: 0.111419
=== Actor Training Debug (Iteration 2225) ===
Q mean: -8.196727
Q std: 9.991241
Actor loss: 8.200708
Action reg: 0.003982
  l1.weight: grad_norm = 0.039447
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.037366
Total gradient norm: 0.225363
=== Actor Training Debug (Iteration 2226) ===
Q mean: -6.901389
Q std: 9.419957
Actor loss: 6.905370
Action reg: 0.003981
  l1.weight: grad_norm = 0.033873
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.029495
Total gradient norm: 0.151011
=== Actor Training Debug (Iteration 2227) ===
Q mean: -6.842040
Q std: 9.503011
Actor loss: 6.846021
Action reg: 0.003981
  l1.weight: grad_norm = 0.030301
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.030469
Total gradient norm: 0.166423
=== Actor Training Debug (Iteration 2228) ===
Q mean: -7.349467
Q std: 9.929514
Actor loss: 7.353442
Action reg: 0.003976
  l1.weight: grad_norm = 0.027746
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.025787
Total gradient norm: 0.104297
=== Actor Training Debug (Iteration 2229) ===
Q mean: -7.060057
Q std: 8.415130
Actor loss: 7.064017
Action reg: 0.003961
  l1.weight: grad_norm = 0.048615
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.048039
Total gradient norm: 0.254063
=== Actor Training Debug (Iteration 2230) ===
Q mean: -8.426495
Q std: 9.178196
Actor loss: 8.430471
Action reg: 0.003977
  l1.weight: grad_norm = 0.033620
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.026850
Total gradient norm: 0.144108
=== Actor Training Debug (Iteration 2231) ===
Q mean: -8.431684
Q std: 9.430831
Actor loss: 8.435672
Action reg: 0.003989
  l1.weight: grad_norm = 0.007738
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.008512
Total gradient norm: 0.045071
=== Actor Training Debug (Iteration 2232) ===
Q mean: -7.427074
Q std: 9.040950
Actor loss: 7.431052
Action reg: 0.003977
  l1.weight: grad_norm = 0.019562
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.018356
Total gradient norm: 0.099576
=== Actor Training Debug (Iteration 2233) ===
Q mean: -6.893008
Q std: 9.067322
Actor loss: 6.896985
Action reg: 0.003976
  l1.weight: grad_norm = 0.060724
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.059780
Total gradient norm: 0.410683
=== Actor Training Debug (Iteration 2234) ===
Q mean: -7.823791
Q std: 9.663791
Actor loss: 7.827770
Action reg: 0.003979
  l1.weight: grad_norm = 0.032780
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.030911
Total gradient norm: 0.159375
=== Actor Training Debug (Iteration 2235) ===
Q mean: -7.627273
Q std: 10.088345
Actor loss: 7.631249
Action reg: 0.003977
  l1.weight: grad_norm = 0.009900
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.009608
Total gradient norm: 0.045011
=== Actor Training Debug (Iteration 2236) ===
Q mean: -7.209612
Q std: 9.677920
Actor loss: 7.213592
Action reg: 0.003980
  l1.weight: grad_norm = 0.014341
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.011377
Total gradient norm: 0.050796
=== Actor Training Debug (Iteration 2237) ===
Q mean: -7.816466
Q std: 9.794766
Actor loss: 7.820435
Action reg: 0.003969
  l1.weight: grad_norm = 0.041047
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.038051
Total gradient norm: 0.220499
=== Actor Training Debug (Iteration 2238) ===
Q mean: -7.395646
Q std: 9.246847
Actor loss: 7.399628
Action reg: 0.003983
  l1.weight: grad_norm = 0.016320
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.015585
Total gradient norm: 0.074642
=== Actor Training Debug (Iteration 2239) ===
Q mean: -8.016202
Q std: 10.244979
Actor loss: 8.020183
Action reg: 0.003980
  l1.weight: grad_norm = 0.020729
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.017281
Total gradient norm: 0.064840
=== Actor Training Debug (Iteration 2240) ===
Q mean: -7.376582
Q std: 9.109914
Actor loss: 7.380558
Action reg: 0.003975
  l1.weight: grad_norm = 0.032533
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.030874
Total gradient norm: 0.197908
=== Actor Training Debug (Iteration 2241) ===
Q mean: -7.549669
Q std: 9.693495
Actor loss: 7.553627
Action reg: 0.003958
  l1.weight: grad_norm = 0.045795
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.040911
Total gradient norm: 0.232655
=== Actor Training Debug (Iteration 2242) ===
Q mean: -8.892956
Q std: 10.236457
Actor loss: 8.896936
Action reg: 0.003981
  l1.weight: grad_norm = 0.013767
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.013501
Total gradient norm: 0.061485
=== Actor Training Debug (Iteration 2243) ===
Q mean: -6.781378
Q std: 8.973629
Actor loss: 6.785361
Action reg: 0.003983
  l1.weight: grad_norm = 0.019350
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.016909
Total gradient norm: 0.086561
=== Actor Training Debug (Iteration 2244) ===
Q mean: -7.763858
Q std: 9.561689
Actor loss: 7.767844
Action reg: 0.003985
  l1.weight: grad_norm = 0.026412
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.023537
Total gradient norm: 0.108945
=== Actor Training Debug (Iteration 2245) ===
Q mean: -6.944440
Q std: 9.472209
Actor loss: 6.948429
Action reg: 0.003989
  l1.weight: grad_norm = 0.012569
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.011687
Total gradient norm: 0.049273
=== Actor Training Debug (Iteration 2246) ===
Q mean: -7.179905
Q std: 9.225850
Actor loss: 7.183877
Action reg: 0.003972
  l1.weight: grad_norm = 0.035301
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.031113
Total gradient norm: 0.153495
=== Actor Training Debug (Iteration 2247) ===
Q mean: -8.140057
Q std: 9.858616
Actor loss: 8.144023
Action reg: 0.003966
  l1.weight: grad_norm = 0.129335
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.115805
Total gradient norm: 0.600832
=== Actor Training Debug (Iteration 2248) ===
Q mean: -8.395700
Q std: 9.208956
Actor loss: 8.399679
Action reg: 0.003979
  l1.weight: grad_norm = 0.018456
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.016657
Total gradient norm: 0.079990
=== Actor Training Debug (Iteration 2249) ===
Q mean: -7.789854
Q std: 9.781104
Actor loss: 7.793833
Action reg: 0.003979
  l1.weight: grad_norm = 0.027607
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.023531
Total gradient norm: 0.114238
=== Actor Training Debug (Iteration 2250) ===
Q mean: -7.009064
Q std: 9.570739
Actor loss: 7.013048
Action reg: 0.003984
  l1.weight: grad_norm = 0.020824
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.019663
Total gradient norm: 0.110359
=== Actor Training Debug (Iteration 2251) ===
Q mean: -7.298686
Q std: 9.303916
Actor loss: 7.302663
Action reg: 0.003978
  l1.weight: grad_norm = 0.031532
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.027594
Total gradient norm: 0.115407
=== Actor Training Debug (Iteration 2252) ===
Q mean: -7.322898
Q std: 8.959903
Actor loss: 7.326879
Action reg: 0.003981
  l1.weight: grad_norm = 0.011218
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.010991
Total gradient norm: 0.055974
=== Actor Training Debug (Iteration 2253) ===
Q mean: -7.673240
Q std: 9.246193
Actor loss: 7.677220
Action reg: 0.003980
  l1.weight: grad_norm = 0.025538
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.021821
Total gradient norm: 0.092201
=== Actor Training Debug (Iteration 2254) ===
Q mean: -8.192185
Q std: 9.794885
Actor loss: 8.196163
Action reg: 0.003978
  l1.weight: grad_norm = 0.033373
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.030902
Total gradient norm: 0.142772
=== Actor Training Debug (Iteration 2255) ===
Q mean: -6.870001
Q std: 8.912795
Actor loss: 6.873974
Action reg: 0.003973
  l1.weight: grad_norm = 0.022123
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.022333
Total gradient norm: 0.116124
=== Actor Training Debug (Iteration 2256) ===
Q mean: -7.137134
Q std: 8.995659
Actor loss: 7.141120
Action reg: 0.003987
  l1.weight: grad_norm = 0.072318
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.047296
Total gradient norm: 0.207654
=== Actor Training Debug (Iteration 2257) ===
Q mean: -7.924558
Q std: 9.819086
Actor loss: 7.928539
Action reg: 0.003981
  l1.weight: grad_norm = 0.024476
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.022673
Total gradient norm: 0.119381
=== Actor Training Debug (Iteration 2258) ===
Q mean: -8.012041
Q std: 10.063573
Actor loss: 8.016020
Action reg: 0.003978
  l1.weight: grad_norm = 0.061184
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.052987
Total gradient norm: 0.235362
=== Actor Training Debug (Iteration 2259) ===
Q mean: -7.671912
Q std: 9.041675
Actor loss: 7.675902
Action reg: 0.003990
  l1.weight: grad_norm = 0.038072
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.035223
Total gradient norm: 0.208717
=== Actor Training Debug (Iteration 2260) ===
Q mean: -8.223516
Q std: 9.715398
Actor loss: 8.227502
Action reg: 0.003987
  l1.weight: grad_norm = 0.026917
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.025105
Total gradient norm: 0.130734
=== Actor Training Debug (Iteration 2261) ===
Q mean: -6.711793
Q std: 9.533716
Actor loss: 6.715768
Action reg: 0.003975
  l1.weight: grad_norm = 0.033116
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.026549
Total gradient norm: 0.139877
=== Actor Training Debug (Iteration 2262) ===
Q mean: -6.963795
Q std: 9.456995
Actor loss: 6.967779
Action reg: 0.003984
  l1.weight: grad_norm = 0.012467
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.011171
Total gradient norm: 0.049603
=== Actor Training Debug (Iteration 2263) ===
Q mean: -7.735282
Q std: 10.202583
Actor loss: 7.739268
Action reg: 0.003986
  l1.weight: grad_norm = 0.057028
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.050613
Total gradient norm: 0.273127
=== Actor Training Debug (Iteration 2264) ===
Q mean: -6.382043
Q std: 8.014467
Actor loss: 6.386029
Action reg: 0.003986
  l1.weight: grad_norm = 0.037129
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.033821
Total gradient norm: 0.161778
=== Actor Training Debug (Iteration 2265) ===
Q mean: -7.441738
Q std: 9.423327
Actor loss: 7.445722
Action reg: 0.003984
  l1.weight: grad_norm = 0.038335
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.031864
Total gradient norm: 0.147591
=== Actor Training Debug (Iteration 2266) ===
Q mean: -7.410429
Q std: 9.203690
Actor loss: 7.414411
Action reg: 0.003981
  l1.weight: grad_norm = 0.015473
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.013878
Total gradient norm: 0.059824
=== Actor Training Debug (Iteration 2267) ===
Q mean: -8.508277
Q std: 10.667716
Actor loss: 8.512252
Action reg: 0.003975
  l1.weight: grad_norm = 0.038216
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.036340
Total gradient norm: 0.188642
=== Actor Training Debug (Iteration 2268) ===
Q mean: -8.224716
Q std: 9.357477
Actor loss: 8.228707
Action reg: 0.003991
  l1.weight: grad_norm = 0.027409
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.019745
Total gradient norm: 0.084621
=== Actor Training Debug (Iteration 2269) ===
Q mean: -7.556951
Q std: 9.412278
Actor loss: 7.560926
Action reg: 0.003976
  l1.weight: grad_norm = 0.037577
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.033146
Total gradient norm: 0.176792
=== Actor Training Debug (Iteration 2270) ===
Q mean: -7.303938
Q std: 10.020732
Actor loss: 7.307920
Action reg: 0.003982
  l1.weight: grad_norm = 0.022158
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.020902
Total gradient norm: 0.110682
=== Actor Training Debug (Iteration 2271) ===
Q mean: -8.470685
Q std: 10.169560
Actor loss: 8.474661
Action reg: 0.003976
  l1.weight: grad_norm = 0.047775
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.041726
Total gradient norm: 0.198733
=== Actor Training Debug (Iteration 2272) ===
Q mean: -8.438413
Q std: 9.596638
Actor loss: 8.442394
Action reg: 0.003981
  l1.weight: grad_norm = 0.030562
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.025794
Total gradient norm: 0.128379
=== Actor Training Debug (Iteration 2273) ===
Q mean: -8.188059
Q std: 9.888655
Actor loss: 8.192038
Action reg: 0.003978
  l1.weight: grad_norm = 0.025118
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.023483
Total gradient norm: 0.110273
=== Actor Training Debug (Iteration 2274) ===
Q mean: -7.172056
Q std: 9.699426
Actor loss: 7.176029
Action reg: 0.003973
  l1.weight: grad_norm = 0.069787
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.072466
Total gradient norm: 0.288103
=== Actor Training Debug (Iteration 2275) ===
Q mean: -7.653017
Q std: 9.346854
Actor loss: 7.656997
Action reg: 0.003981
  l1.weight: grad_norm = 0.018755
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.015440
Total gradient norm: 0.080104
=== Actor Training Debug (Iteration 2276) ===
Q mean: -7.875092
Q std: 9.307323
Actor loss: 7.879076
Action reg: 0.003984
  l1.weight: grad_norm = 0.046776
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.037329
Total gradient norm: 0.143169
=== Actor Training Debug (Iteration 2277) ===
Q mean: -7.304106
Q std: 8.885131
Actor loss: 7.308089
Action reg: 0.003983
  l1.weight: grad_norm = 0.038666
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.032335
Total gradient norm: 0.152300
=== Actor Training Debug (Iteration 2278) ===
Q mean: -7.300469
Q std: 9.476310
Actor loss: 7.304445
Action reg: 0.003976
  l1.weight: grad_norm = 0.019949
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.018468
Total gradient norm: 0.078276
=== Actor Training Debug (Iteration 2279) ===
Q mean: -7.975183
Q std: 9.882290
Actor loss: 7.979163
Action reg: 0.003980
  l1.weight: grad_norm = 0.031363
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.027279
Total gradient norm: 0.109206
=== Actor Training Debug (Iteration 2280) ===
Q mean: -6.855609
Q std: 9.708658
Actor loss: 6.859586
Action reg: 0.003977
  l1.weight: grad_norm = 0.025751
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.022366
Total gradient norm: 0.097667
=== Actor Training Debug (Iteration 2281) ===
Q mean: -7.609876
Q std: 9.985776
Actor loss: 7.613868
Action reg: 0.003992
  l1.weight: grad_norm = 0.002829
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.002525
Total gradient norm: 0.012027
=== Actor Training Debug (Iteration 2282) ===
Q mean: -7.909650
Q std: 9.109434
Actor loss: 7.913636
Action reg: 0.003986
  l1.weight: grad_norm = 0.018946
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.015191
Total gradient norm: 0.062530
=== Actor Training Debug (Iteration 2283) ===
Q mean: -8.394176
Q std: 9.991024
Actor loss: 8.398165
Action reg: 0.003988
  l1.weight: grad_norm = 0.014450
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.012829
Total gradient norm: 0.070207
=== Actor Training Debug (Iteration 2284) ===
Q mean: -7.103006
Q std: 9.963616
Actor loss: 7.106991
Action reg: 0.003985
  l1.weight: grad_norm = 0.015621
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.014971
Total gradient norm: 0.075355
=== Actor Training Debug (Iteration 2285) ===
Q mean: -6.580419
Q std: 8.268185
Actor loss: 6.584412
Action reg: 0.003993
  l1.weight: grad_norm = 0.011490
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.010344
Total gradient norm: 0.053012
=== Actor Training Debug (Iteration 2286) ===
Q mean: -7.636821
Q std: 9.949352
Actor loss: 7.640800
Action reg: 0.003980
  l1.weight: grad_norm = 0.031071
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.026918
Total gradient norm: 0.102408
=== Actor Training Debug (Iteration 2287) ===
Q mean: -8.548099
Q std: 9.810552
Actor loss: 8.552084
Action reg: 0.003985
  l1.weight: grad_norm = 0.053468
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.050583
Total gradient norm: 0.246043
=== Actor Training Debug (Iteration 2288) ===
Q mean: -7.409154
Q std: 9.412659
Actor loss: 7.413143
Action reg: 0.003989
  l1.weight: grad_norm = 0.014689
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.012527
Total gradient norm: 0.050335
=== Actor Training Debug (Iteration 2289) ===
Q mean: -7.943030
Q std: 9.469663
Actor loss: 7.947018
Action reg: 0.003988
  l1.weight: grad_norm = 0.045846
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.041104
Total gradient norm: 0.226348
=== Actor Training Debug (Iteration 2290) ===
Q mean: -8.281462
Q std: 10.285492
Actor loss: 8.285442
Action reg: 0.003981
  l1.weight: grad_norm = 0.033377
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.030465
Total gradient norm: 0.129611
=== Actor Training Debug (Iteration 2291) ===
Q mean: -7.378215
Q std: 9.545674
Actor loss: 7.382195
Action reg: 0.003981
  l1.weight: grad_norm = 0.039425
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.035167
Total gradient norm: 0.146864
=== Actor Training Debug (Iteration 2292) ===
Q mean: -7.090713
Q std: 8.554665
Actor loss: 7.094697
Action reg: 0.003984
  l1.weight: grad_norm = 0.020962
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.016494
Total gradient norm: 0.082943
=== Actor Training Debug (Iteration 2293) ===
Q mean: -8.021780
Q std: 9.633430
Actor loss: 8.025763
Action reg: 0.003983
  l1.weight: grad_norm = 0.077211
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.064154
Total gradient norm: 0.298302
=== Actor Training Debug (Iteration 2294) ===
Q mean: -6.363890
Q std: 8.603042
Actor loss: 6.367878
Action reg: 0.003988
  l1.weight: grad_norm = 0.018935
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.016483
Total gradient norm: 0.078032
=== Actor Training Debug (Iteration 2295) ===
Q mean: -7.601127
Q std: 9.889668
Actor loss: 7.605106
Action reg: 0.003979
  l1.weight: grad_norm = 0.036693
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.036182
Total gradient norm: 0.158809
=== Actor Training Debug (Iteration 2296) ===
Q mean: -8.359708
Q std: 10.276841
Actor loss: 8.363690
Action reg: 0.003982
  l1.weight: grad_norm = 0.044564
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.038023
Total gradient norm: 0.174136
=== Actor Training Debug (Iteration 2297) ===
Q mean: -8.094890
Q std: 9.760293
Actor loss: 8.098877
Action reg: 0.003987
  l1.weight: grad_norm = 0.024714
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.023930
Total gradient norm: 0.107554
=== Actor Training Debug (Iteration 2298) ===
Q mean: -6.573295
Q std: 8.721354
Actor loss: 6.577272
Action reg: 0.003978
  l1.weight: grad_norm = 0.018331
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.015034
Total gradient norm: 0.058333
=== Actor Training Debug (Iteration 2299) ===
Q mean: -7.346354
Q std: 9.738605
Actor loss: 7.350333
Action reg: 0.003978
  l1.weight: grad_norm = 0.024858
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.021850
Total gradient norm: 0.132416
=== Actor Training Debug (Iteration 2300) ===
Q mean: -7.239526
Q std: 9.617350
Actor loss: 7.243507
Action reg: 0.003982
  l1.weight: grad_norm = 0.046029
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.032258
Total gradient norm: 0.129397
=== Actor Training Debug (Iteration 2301) ===
Q mean: -7.425478
Q std: 9.511636
Actor loss: 7.429464
Action reg: 0.003987
  l1.weight: grad_norm = 0.041069
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.034757
Total gradient norm: 0.225168
=== Actor Training Debug (Iteration 2302) ===
Q mean: -7.773415
Q std: 9.941983
Actor loss: 7.777404
Action reg: 0.003989
  l1.weight: grad_norm = 0.041195
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.035859
Total gradient norm: 0.159914
=== Actor Training Debug (Iteration 2303) ===
Q mean: -6.082561
Q std: 8.775652
Actor loss: 6.086546
Action reg: 0.003985
  l1.weight: grad_norm = 0.080253
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.061506
Total gradient norm: 0.258448
=== Actor Training Debug (Iteration 2304) ===
Q mean: -7.512663
Q std: 9.725896
Actor loss: 7.516647
Action reg: 0.003984
  l1.weight: grad_norm = 0.016875
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.016696
Total gradient norm: 0.107876
=== Actor Training Debug (Iteration 2305) ===
Q mean: -7.645928
Q std: 9.786775
Actor loss: 7.649913
Action reg: 0.003984
  l1.weight: grad_norm = 0.063876
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.055535
Total gradient norm: 0.293637
=== Actor Training Debug (Iteration 2306) ===
Q mean: -7.998667
Q std: 10.205930
Actor loss: 8.002650
Action reg: 0.003983
  l1.weight: grad_norm = 0.041899
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.036908
Total gradient norm: 0.142996
=== Actor Training Debug (Iteration 2307) ===
Q mean: -7.987887
Q std: 9.585058
Actor loss: 7.991865
Action reg: 0.003977
  l1.weight: grad_norm = 0.049789
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.042651
Total gradient norm: 0.203599
=== Actor Training Debug (Iteration 2308) ===
Q mean: -6.892571
Q std: 9.532812
Actor loss: 6.896552
Action reg: 0.003980
  l1.weight: grad_norm = 0.012905
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.011452
Total gradient norm: 0.060568
=== Actor Training Debug (Iteration 2309) ===
Q mean: -7.170874
Q std: 9.983512
Actor loss: 7.174867
Action reg: 0.003993
  l1.weight: grad_norm = 0.010693
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.008541
Total gradient norm: 0.036010
=== Actor Training Debug (Iteration 2310) ===
Q mean: -7.306992
Q std: 9.646851
Actor loss: 7.310975
Action reg: 0.003983
  l1.weight: grad_norm = 0.049216
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.039339
Total gradient norm: 0.152857
=== Actor Training Debug (Iteration 2311) ===
Q mean: -8.060381
Q std: 9.768271
Actor loss: 8.064363
Action reg: 0.003981
  l1.weight: grad_norm = 0.031402
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.028209
Total gradient norm: 0.165814
=== Actor Training Debug (Iteration 2312) ===
Q mean: -7.662562
Q std: 9.506481
Actor loss: 7.666531
Action reg: 0.003969
  l1.weight: grad_norm = 0.029436
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.025486
Total gradient norm: 0.114729
=== Actor Training Debug (Iteration 2313) ===
Q mean: -8.674405
Q std: 10.272378
Actor loss: 8.678396
Action reg: 0.003991
  l1.weight: grad_norm = 0.005498
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.005230
Total gradient norm: 0.023676
=== Actor Training Debug (Iteration 2314) ===
Q mean: -7.115303
Q std: 9.323319
Actor loss: 7.119294
Action reg: 0.003991
  l1.weight: grad_norm = 0.016416
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.015993
Total gradient norm: 0.090090
=== Actor Training Debug (Iteration 2315) ===
Q mean: -7.725321
Q std: 10.556423
Actor loss: 7.729301
Action reg: 0.003981
  l1.weight: grad_norm = 0.035345
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.029413
Total gradient norm: 0.140530
=== Actor Training Debug (Iteration 2316) ===
Q mean: -7.437424
Q std: 8.891974
Actor loss: 7.441406
Action reg: 0.003982
  l1.weight: grad_norm = 0.026573
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.021598
Total gradient norm: 0.112351
=== Actor Training Debug (Iteration 2317) ===
Q mean: -7.863595
Q std: 9.798984
Actor loss: 7.867578
Action reg: 0.003983
  l1.weight: grad_norm = 0.039382
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.030762
Total gradient norm: 0.151675
=== Actor Training Debug (Iteration 2318) ===
Q mean: -7.715722
Q std: 9.150756
Actor loss: 7.719701
Action reg: 0.003980
  l1.weight: grad_norm = 0.061274
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.053932
Total gradient norm: 0.227528
=== Actor Training Debug (Iteration 2319) ===
Q mean: -7.445651
Q std: 9.741627
Actor loss: 7.449640
Action reg: 0.003989
  l1.weight: grad_norm = 0.045865
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.037657
Total gradient norm: 0.167719
=== Actor Training Debug (Iteration 2320) ===
Q mean: -7.362014
Q std: 9.747719
Actor loss: 7.365991
Action reg: 0.003977
  l1.weight: grad_norm = 0.027341
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.024326
Total gradient norm: 0.110382
=== Actor Training Debug (Iteration 2321) ===
Q mean: -7.896674
Q std: 9.370676
Actor loss: 7.900657
Action reg: 0.003983
  l1.weight: grad_norm = 0.038401
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.031828
Total gradient norm: 0.156476
=== Actor Training Debug (Iteration 2322) ===
Q mean: -7.497265
Q std: 9.599997
Actor loss: 7.501239
Action reg: 0.003974
  l1.weight: grad_norm = 0.044290
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.038084
Total gradient norm: 0.163389
=== Actor Training Debug (Iteration 2323) ===
Q mean: -6.937485
Q std: 8.476401
Actor loss: 6.941459
Action reg: 0.003974
  l1.weight: grad_norm = 0.062912
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.055013
Total gradient norm: 0.222614
=== Actor Training Debug (Iteration 2324) ===
Q mean: -6.929106
Q std: 9.220705
Actor loss: 6.933088
Action reg: 0.003982
  l1.weight: grad_norm = 0.039921
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.035985
Total gradient norm: 0.164679
=== Actor Training Debug (Iteration 2325) ===
Q mean: -7.463933
Q std: 9.680900
Actor loss: 7.467916
Action reg: 0.003983
  l1.weight: grad_norm = 0.062304
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.054582
Total gradient norm: 0.237982
=== Actor Training Debug (Iteration 2326) ===
Q mean: -6.564286
Q std: 9.157367
Actor loss: 6.568262
Action reg: 0.003976
  l1.weight: grad_norm = 0.040822
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.036959
Total gradient norm: 0.159127
=== Actor Training Debug (Iteration 2327) ===
Q mean: -8.012220
Q std: 10.255465
Actor loss: 8.016207
Action reg: 0.003986
  l1.weight: grad_norm = 0.018243
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.015573
Total gradient norm: 0.058206
=== Actor Training Debug (Iteration 2328) ===
Q mean: -7.614675
Q std: 9.822122
Actor loss: 7.618652
Action reg: 0.003978
  l1.weight: grad_norm = 0.042683
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.038057
Total gradient norm: 0.152447
=== Actor Training Debug (Iteration 2329) ===
Q mean: -7.424370
Q std: 9.785532
Actor loss: 7.428350
Action reg: 0.003980
  l1.weight: grad_norm = 0.087241
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.069302
Total gradient norm: 0.266824
=== Actor Training Debug (Iteration 2330) ===
Q mean: -7.497485
Q std: 9.905892
Actor loss: 7.501467
Action reg: 0.003982
  l1.weight: grad_norm = 0.061821
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.046786
Total gradient norm: 0.194942
=== Actor Training Debug (Iteration 2331) ===
Q mean: -6.181701
Q std: 8.470791
Actor loss: 6.185678
Action reg: 0.003977
  l1.weight: grad_norm = 0.044686
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.039157
Total gradient norm: 0.156596
=== Actor Training Debug (Iteration 2332) ===
Q mean: -7.321124
Q std: 10.090052
Actor loss: 7.325112
Action reg: 0.003988
  l1.weight: grad_norm = 0.028152
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.022030
Total gradient norm: 0.121686
=== Actor Training Debug (Iteration 2333) ===
Q mean: -7.005875
Q std: 9.367389
Actor loss: 7.009862
Action reg: 0.003987
  l1.weight: grad_norm = 0.034904
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.027681
Total gradient norm: 0.152417
=== Actor Training Debug (Iteration 2334) ===
Q mean: -8.668043
Q std: 9.903632
Actor loss: 8.672028
Action reg: 0.003984
  l1.weight: grad_norm = 0.045870
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.035366
Total gradient norm: 0.153553
=== Actor Training Debug (Iteration 2335) ===
Q mean: -9.427034
Q std: 9.919490
Actor loss: 9.431028
Action reg: 0.003994
  l1.weight: grad_norm = 0.036822
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.023809
Total gradient norm: 0.088377
=== Actor Training Debug (Iteration 2336) ===
Q mean: -7.429255
Q std: 9.123501
Actor loss: 7.433248
Action reg: 0.003994
  l1.weight: grad_norm = 0.053110
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.045668
Total gradient norm: 0.248283
=== Actor Training Debug (Iteration 2337) ===
Q mean: -6.105012
Q std: 8.606961
Actor loss: 6.108990
Action reg: 0.003978
  l1.weight: grad_norm = 0.063780
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.053449
Total gradient norm: 0.213799
=== Actor Training Debug (Iteration 2338) ===
Q mean: -7.035311
Q std: 9.523332
Actor loss: 7.039290
Action reg: 0.003979
  l1.weight: grad_norm = 0.025418
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.021446
Total gradient norm: 0.094560
=== Actor Training Debug (Iteration 2339) ===
Q mean: -7.059823
Q std: 9.580036
Actor loss: 7.063803
Action reg: 0.003980
  l1.weight: grad_norm = 0.031556
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.027814
Total gradient norm: 0.139448
=== Actor Training Debug (Iteration 2340) ===
Q mean: -8.227783
Q std: 9.955128
Actor loss: 8.231774
Action reg: 0.003991
  l1.weight: grad_norm = 0.013794
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.013594
Total gradient norm: 0.068951
=== Actor Training Debug (Iteration 2341) ===
Q mean: -6.675759
Q std: 9.835215
Actor loss: 6.679744
Action reg: 0.003985
  l1.weight: grad_norm = 0.040048
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.038728
Total gradient norm: 0.187025
=== Actor Training Debug (Iteration 2342) ===
Q mean: -8.285207
Q std: 10.462187
Actor loss: 8.289195
Action reg: 0.003988
  l1.weight: grad_norm = 0.010238
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.009406
Total gradient norm: 0.039523
=== Actor Training Debug (Iteration 2343) ===
Q mean: -6.200714
Q std: 8.706683
Actor loss: 6.204691
Action reg: 0.003977
  l1.weight: grad_norm = 0.030579
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.027191
Total gradient norm: 0.120705
=== Actor Training Debug (Iteration 2344) ===
Q mean: -7.427908
Q std: 9.401320
Actor loss: 7.431878
Action reg: 0.003970
  l1.weight: grad_norm = 0.060922
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.049865
Total gradient norm: 0.195948
=== Actor Training Debug (Iteration 2345) ===
Q mean: -7.507391
Q std: 8.896709
Actor loss: 7.511377
Action reg: 0.003986
  l1.weight: grad_norm = 0.017697
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.015373
Total gradient norm: 0.075119
=== Actor Training Debug (Iteration 2346) ===
Q mean: -7.351838
Q std: 9.256896
Actor loss: 7.355825
Action reg: 0.003987
  l1.weight: grad_norm = 0.042178
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.037797
Total gradient norm: 0.177674
=== Actor Training Debug (Iteration 2347) ===
Q mean: -6.818656
Q std: 9.365460
Actor loss: 6.822643
Action reg: 0.003987
  l1.weight: grad_norm = 0.025552
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.021361
Total gradient norm: 0.104253
=== Actor Training Debug (Iteration 2348) ===
Q mean: -7.976810
Q std: 10.034225
Actor loss: 7.980800
Action reg: 0.003989
  l1.weight: grad_norm = 0.025554
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.023297
Total gradient norm: 0.113076
=== Actor Training Debug (Iteration 2349) ===
Q mean: -7.814487
Q std: 10.302666
Actor loss: 7.818474
Action reg: 0.003988
  l1.weight: grad_norm = 0.017019
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.014226
Total gradient norm: 0.073470
=== Actor Training Debug (Iteration 2350) ===
Q mean: -7.511423
Q std: 9.647113
Actor loss: 7.515406
Action reg: 0.003983
  l1.weight: grad_norm = 0.026040
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.025129
Total gradient norm: 0.164507
=== Actor Training Debug (Iteration 2351) ===
Q mean: -7.450681
Q std: 9.608238
Actor loss: 7.454667
Action reg: 0.003986
  l1.weight: grad_norm = 0.057144
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.051502
Total gradient norm: 0.232963
=== Actor Training Debug (Iteration 2352) ===
Q mean: -7.148551
Q std: 10.054349
Actor loss: 7.152535
Action reg: 0.003984
  l1.weight: grad_norm = 0.020930
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.020012
Total gradient norm: 0.120396
=== Actor Training Debug (Iteration 2353) ===
Q mean: -7.393857
Q std: 8.955185
Actor loss: 7.397837
Action reg: 0.003980
  l1.weight: grad_norm = 0.051559
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.042360
Total gradient norm: 0.175974
=== Actor Training Debug (Iteration 2354) ===
Q mean: -8.759769
Q std: 10.031244
Actor loss: 8.763764
Action reg: 0.003995
  l1.weight: grad_norm = 0.018597
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.014496
Total gradient norm: 0.060373
=== Actor Training Debug (Iteration 2355) ===
Q mean: -9.129161
Q std: 10.459761
Actor loss: 9.133143
Action reg: 0.003982
  l1.weight: grad_norm = 0.050907
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.042383
Total gradient norm: 0.182978
=== Actor Training Debug (Iteration 2356) ===
Q mean: -7.693384
Q std: 9.430168
Actor loss: 7.697369
Action reg: 0.003985
  l1.weight: grad_norm = 0.010309
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.009007
Total gradient norm: 0.054950
=== Actor Training Debug (Iteration 2357) ===
Q mean: -7.111253
Q std: 9.345535
Actor loss: 7.115237
Action reg: 0.003984
  l1.weight: grad_norm = 0.042211
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.040453
Total gradient norm: 0.174158
=== Actor Training Debug (Iteration 2358) ===
Q mean: -7.127365
Q std: 9.050137
Actor loss: 7.131355
Action reg: 0.003990
  l1.weight: grad_norm = 0.023161
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.017034
Total gradient norm: 0.085000
=== Actor Training Debug (Iteration 2359) ===
Q mean: -7.417619
Q std: 9.378829
Actor loss: 7.421605
Action reg: 0.003987
  l1.weight: grad_norm = 0.027199
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.024642
Total gradient norm: 0.145889
=== Actor Training Debug (Iteration 2360) ===
Q mean: -8.755285
Q std: 10.631183
Actor loss: 8.759273
Action reg: 0.003988
  l1.weight: grad_norm = 0.042020
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.036713
Total gradient norm: 0.153974
=== Actor Training Debug (Iteration 2361) ===
Q mean: -7.578056
Q std: 9.531780
Actor loss: 7.582044
Action reg: 0.003987
  l1.weight: grad_norm = 0.047587
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.037953
Total gradient norm: 0.172344
=== Actor Training Debug (Iteration 2362) ===
Q mean: -7.546404
Q std: 9.979348
Actor loss: 7.550387
Action reg: 0.003983
  l1.weight: grad_norm = 0.028602
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.022139
Total gradient norm: 0.094960
=== Actor Training Debug (Iteration 2363) ===
Q mean: -6.772741
Q std: 9.580237
Actor loss: 6.776715
Action reg: 0.003973
  l1.weight: grad_norm = 0.042577
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.037675
Total gradient norm: 0.193888
=== Actor Training Debug (Iteration 2364) ===
Q mean: -9.679455
Q std: 10.691182
Actor loss: 9.683442
Action reg: 0.003987
  l1.weight: grad_norm = 0.039738
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.036251
Total gradient norm: 0.152132
=== Actor Training Debug (Iteration 2365) ===
Q mean: -7.652915
Q std: 9.400117
Actor loss: 7.656893
Action reg: 0.003978
  l1.weight: grad_norm = 0.064973
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.056342
Total gradient norm: 0.253216
=== Actor Training Debug (Iteration 2366) ===
Q mean: -6.547206
Q std: 9.490403
Actor loss: 6.551189
Action reg: 0.003983
  l1.weight: grad_norm = 0.018821
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.019691
Total gradient norm: 0.119401
=== Actor Training Debug (Iteration 2367) ===
Q mean: -7.757304
Q std: 9.846375
Actor loss: 7.761293
Action reg: 0.003989
  l1.weight: grad_norm = 0.010341
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.010129
Total gradient norm: 0.057269
=== Actor Training Debug (Iteration 2368) ===
Q mean: -7.236139
Q std: 9.339429
Actor loss: 7.240121
Action reg: 0.003982
  l1.weight: grad_norm = 0.027342
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.024738
Total gradient norm: 0.112193
=== Actor Training Debug (Iteration 2369) ===
Q mean: -8.725804
Q std: 10.150824
Actor loss: 8.729789
Action reg: 0.003985
  l1.weight: grad_norm = 0.040214
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.031288
Total gradient norm: 0.136823
=== Actor Training Debug (Iteration 2370) ===
Q mean: -8.509830
Q std: 10.618039
Actor loss: 8.513813
Action reg: 0.003983
  l1.weight: grad_norm = 0.037124
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.032108
Total gradient norm: 0.140322
=== Actor Training Debug (Iteration 2371) ===
Q mean: -7.536494
Q std: 10.008977
Actor loss: 7.540478
Action reg: 0.003983
  l1.weight: grad_norm = 0.034248
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.029092
Total gradient norm: 0.136083
=== Actor Training Debug (Iteration 2372) ===
Q mean: -8.064442
Q std: 10.150834
Actor loss: 8.068431
Action reg: 0.003989
  l1.weight: grad_norm = 0.027279
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.023262
Total gradient norm: 0.082780
=== Actor Training Debug (Iteration 2373) ===
Q mean: -6.538321
Q std: 9.602762
Actor loss: 6.542307
Action reg: 0.003986
  l1.weight: grad_norm = 0.023393
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.020784
Total gradient norm: 0.087863
=== Actor Training Debug (Iteration 2374) ===
Q mean: -8.327516
Q std: 10.049220
Actor loss: 8.331491
Action reg: 0.003976
  l1.weight: grad_norm = 0.068859
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.061142
Total gradient norm: 0.313207
=== Actor Training Debug (Iteration 2375) ===
Q mean: -7.830617
Q std: 10.128341
Actor loss: 7.834600
Action reg: 0.003983
  l1.weight: grad_norm = 0.037985
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.029392
Total gradient norm: 0.123542
=== Actor Training Debug (Iteration 2376) ===
Q mean: -7.811542
Q std: 10.086857
Actor loss: 7.815521
Action reg: 0.003980
  l1.weight: grad_norm = 0.035538
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.030345
Total gradient norm: 0.159376
=== Actor Training Debug (Iteration 2377) ===
Q mean: -7.728933
Q std: 9.649602
Actor loss: 7.732916
Action reg: 0.003982
  l1.weight: grad_norm = 0.060044
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.046491
Total gradient norm: 0.199098
=== Actor Training Debug (Iteration 2378) ===
Q mean: -7.888898
Q std: 9.857988
Actor loss: 7.892885
Action reg: 0.003987
  l1.weight: grad_norm = 0.086661
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.076956
Total gradient norm: 0.392426
=== Actor Training Debug (Iteration 2379) ===
Q mean: -8.357395
Q std: 10.382072
Actor loss: 8.361378
Action reg: 0.003983
  l1.weight: grad_norm = 0.057446
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.047975
Total gradient norm: 0.194099
=== Actor Training Debug (Iteration 2380) ===
Q mean: -7.635549
Q std: 9.789420
Actor loss: 7.639536
Action reg: 0.003988
  l1.weight: grad_norm = 0.031250
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.031622
Total gradient norm: 0.126137
=== Actor Training Debug (Iteration 2381) ===
Q mean: -7.756355
Q std: 10.113194
Actor loss: 7.760339
Action reg: 0.003984
  l1.weight: grad_norm = 0.035991
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.029544
Total gradient norm: 0.125907
=== Actor Training Debug (Iteration 2382) ===
Q mean: -6.958585
Q std: 10.275542
Actor loss: 6.962568
Action reg: 0.003983
  l1.weight: grad_norm = 0.031728
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.030865
Total gradient norm: 0.141957
=== Actor Training Debug (Iteration 2383) ===
Q mean: -9.210322
Q std: 10.572136
Actor loss: 9.214311
Action reg: 0.003988
  l1.weight: grad_norm = 0.010627
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.009859
Total gradient norm: 0.044576
=== Actor Training Debug (Iteration 2384) ===
Q mean: -7.567447
Q std: 9.620004
Actor loss: 7.571430
Action reg: 0.003983
  l1.weight: grad_norm = 0.025233
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.025248
Total gradient norm: 0.131253
=== Actor Training Debug (Iteration 2385) ===
Q mean: -8.092072
Q std: 9.968632
Actor loss: 8.096059
Action reg: 0.003987
  l1.weight: grad_norm = 0.046014
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.038369
Total gradient norm: 0.141845
=== Actor Training Debug (Iteration 2386) ===
Q mean: -8.689746
Q std: 10.693800
Actor loss: 8.693729
Action reg: 0.003984
  l1.weight: grad_norm = 0.038251
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.033974
Total gradient norm: 0.140318
=== Actor Training Debug (Iteration 2387) ===
Q mean: -8.259599
Q std: 9.969555
Actor loss: 8.263581
Action reg: 0.003983
  l1.weight: grad_norm = 0.037593
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.031820
Total gradient norm: 0.125908
=== Actor Training Debug (Iteration 2388) ===
Q mean: -7.837155
Q std: 9.618196
Actor loss: 7.841141
Action reg: 0.003986
  l1.weight: grad_norm = 0.014815
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.014649
Total gradient norm: 0.075530
=== Actor Training Debug (Iteration 2389) ===
Q mean: -8.133615
Q std: 9.578438
Actor loss: 8.137606
Action reg: 0.003990
  l1.weight: grad_norm = 0.010867
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.009240
Total gradient norm: 0.040025
=== Actor Training Debug (Iteration 2390) ===
Q mean: -7.686853
Q std: 10.447906
Actor loss: 7.690832
Action reg: 0.003979
  l1.weight: grad_norm = 0.027042
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.023058
Total gradient norm: 0.082994
=== Actor Training Debug (Iteration 2391) ===
Q mean: -7.429525
Q std: 10.079769
Actor loss: 7.433502
Action reg: 0.003977
  l1.weight: grad_norm = 0.041419
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.035543
Total gradient norm: 0.170090
=== Actor Training Debug (Iteration 2392) ===
Q mean: -8.606615
Q std: 9.987894
Actor loss: 8.610600
Action reg: 0.003986
  l1.weight: grad_norm = 0.019051
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.016015
Total gradient norm: 0.073832
=== Actor Training Debug (Iteration 2393) ===
Q mean: -8.493711
Q std: 10.091026
Actor loss: 8.497696
Action reg: 0.003984
  l1.weight: grad_norm = 0.033462
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.030892
Total gradient norm: 0.141360
=== Actor Training Debug (Iteration 2394) ===
Q mean: -7.378445
Q std: 9.484240
Actor loss: 7.382423
Action reg: 0.003978
  l1.weight: grad_norm = 0.024023
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.020304
Total gradient norm: 0.077702
=== Actor Training Debug (Iteration 2395) ===
Q mean: -8.858993
Q std: 10.387484
Actor loss: 8.862977
Action reg: 0.003985
  l1.weight: grad_norm = 0.020023
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.017897
Total gradient norm: 0.080535
=== Actor Training Debug (Iteration 2396) ===
Q mean: -7.141866
Q std: 9.282292
Actor loss: 7.145846
Action reg: 0.003981
  l1.weight: grad_norm = 0.018505
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.017423
Total gradient norm: 0.096421
=== Actor Training Debug (Iteration 2397) ===
Q mean: -7.315845
Q std: 9.882192
Actor loss: 7.319825
Action reg: 0.003980
  l1.weight: grad_norm = 0.050626
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.043167
Total gradient norm: 0.200476
=== Actor Training Debug (Iteration 2398) ===
Q mean: -7.628987
Q std: 9.597692
Actor loss: 7.632971
Action reg: 0.003984
  l1.weight: grad_norm = 0.030303
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.028972
Total gradient norm: 0.155065
=== Actor Training Debug (Iteration 2399) ===
Q mean: -7.961385
Q std: 9.936074
Actor loss: 7.965372
Action reg: 0.003987
  l1.weight: grad_norm = 0.026546
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.021752
Total gradient norm: 0.093215
=== Actor Training Debug (Iteration 2400) ===
Q mean: -8.626624
Q std: 10.452731
Actor loss: 8.630611
Action reg: 0.003987
  l1.weight: grad_norm = 0.023961
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.018455
Total gradient norm: 0.074805
=== Actor Training Debug (Iteration 2401) ===
Q mean: -7.343558
Q std: 9.928148
Actor loss: 7.347541
Action reg: 0.003983
  l1.weight: grad_norm = 0.023941
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.021267
Total gradient norm: 0.096632
=== Actor Training Debug (Iteration 2402) ===
Q mean: -6.988394
Q std: 8.789234
Actor loss: 6.992376
Action reg: 0.003982
  l1.weight: grad_norm = 0.029475
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.024530
Total gradient norm: 0.146333
=== Actor Training Debug (Iteration 2403) ===
Q mean: -7.903429
Q std: 9.992173
Actor loss: 7.907413
Action reg: 0.003985
  l1.weight: grad_norm = 0.038594
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.030993
Total gradient norm: 0.132614
=== Actor Training Debug (Iteration 2404) ===
Q mean: -6.674802
Q std: 9.079575
Actor loss: 6.678791
Action reg: 0.003989
  l1.weight: grad_norm = 0.013854
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.011425
Total gradient norm: 0.062003
=== Actor Training Debug (Iteration 2405) ===
Q mean: -7.601008
Q std: 9.781539
Actor loss: 7.604984
Action reg: 0.003976
  l1.weight: grad_norm = 0.043483
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.039848
Total gradient norm: 0.200133
=== Actor Training Debug (Iteration 2406) ===
Q mean: -6.764977
Q std: 9.025697
Actor loss: 6.768959
Action reg: 0.003983
  l1.weight: grad_norm = 0.050199
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.043424
Total gradient norm: 0.225978
=== Actor Training Debug (Iteration 2407) ===
Q mean: -8.069639
Q std: 10.541722
Actor loss: 8.073623
Action reg: 0.003984
  l1.weight: grad_norm = 0.034303
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.034000
Total gradient norm: 0.135580
=== Actor Training Debug (Iteration 2408) ===
Q mean: -6.321014
Q std: 9.530912
Actor loss: 6.325003
Action reg: 0.003988
  l1.weight: grad_norm = 0.024637
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.022231
Total gradient norm: 0.098638
=== Actor Training Debug (Iteration 2409) ===
Q mean: -8.116867
Q std: 9.714643
Actor loss: 8.120854
Action reg: 0.003987
  l1.weight: grad_norm = 0.038162
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.030465
Total gradient norm: 0.169666
=== Actor Training Debug (Iteration 2410) ===
Q mean: -7.186084
Q std: 8.839847
Actor loss: 7.190066
Action reg: 0.003982
  l1.weight: grad_norm = 0.018944
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.017418
Total gradient norm: 0.081021
=== Actor Training Debug (Iteration 2411) ===
Q mean: -7.297637
Q std: 9.342011
Actor loss: 7.301619
Action reg: 0.003981
  l1.weight: grad_norm = 0.022803
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.018069
Total gradient norm: 0.074173
=== Actor Training Debug (Iteration 2412) ===
Q mean: -7.705404
Q std: 9.771041
Actor loss: 7.709384
Action reg: 0.003980
  l1.weight: grad_norm = 0.064928
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.054395
Total gradient norm: 0.263830
=== Actor Training Debug (Iteration 2413) ===
Q mean: -7.680076
Q std: 9.601663
Actor loss: 7.684048
Action reg: 0.003972
  l1.weight: grad_norm = 0.046898
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.045977
Total gradient norm: 0.239717
=== Actor Training Debug (Iteration 2414) ===
Q mean: -8.137676
Q std: 9.839788
Actor loss: 8.141667
Action reg: 0.003991
  l1.weight: grad_norm = 0.016866
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.015743
Total gradient norm: 0.082848
=== Actor Training Debug (Iteration 2415) ===
Q mean: -7.565683
Q std: 9.810608
Actor loss: 7.569672
Action reg: 0.003989
  l1.weight: grad_norm = 0.051132
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.047481
Total gradient norm: 0.203877
=== Actor Training Debug (Iteration 2416) ===
Q mean: -7.774634
Q std: 10.205226
Actor loss: 7.778622
Action reg: 0.003987
  l1.weight: grad_norm = 0.006426
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.005167
Total gradient norm: 0.023240
=== Actor Training Debug (Iteration 2417) ===
Q mean: -7.903392
Q std: 9.853825
Actor loss: 7.907376
Action reg: 0.003984
  l1.weight: grad_norm = 0.036530
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.037480
Total gradient norm: 0.216424
=== Actor Training Debug (Iteration 2418) ===
Q mean: -6.998132
Q std: 8.751700
Actor loss: 7.002119
Action reg: 0.003987
  l1.weight: grad_norm = 0.063192
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.057516
Total gradient norm: 0.270811
=== Actor Training Debug (Iteration 2419) ===
Q mean: -7.583763
Q std: 10.016858
Actor loss: 7.587749
Action reg: 0.003986
  l1.weight: grad_norm = 0.028243
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.022967
Total gradient norm: 0.092515
=== Actor Training Debug (Iteration 2420) ===
Q mean: -7.423979
Q std: 9.756416
Actor loss: 7.427965
Action reg: 0.003986
  l1.weight: grad_norm = 0.039395
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.033269
Total gradient norm: 0.133350
=== Actor Training Debug (Iteration 2421) ===
Q mean: -8.282370
Q std: 10.660846
Actor loss: 8.286354
Action reg: 0.003984
  l1.weight: grad_norm = 0.040684
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.033558
Total gradient norm: 0.156436
=== Actor Training Debug (Iteration 2422) ===
Q mean: -7.703817
Q std: 9.906049
Actor loss: 7.707800
Action reg: 0.003983
  l1.weight: grad_norm = 0.022137
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.020368
Total gradient norm: 0.090029
=== Actor Training Debug (Iteration 2423) ===
Q mean: -8.412424
Q std: 10.788312
Actor loss: 8.416412
Action reg: 0.003988
  l1.weight: grad_norm = 0.091763
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.070869
Total gradient norm: 0.290800
=== Actor Training Debug (Iteration 2424) ===
Q mean: -7.028835
Q std: 9.001975
Actor loss: 7.032816
Action reg: 0.003981
  l1.weight: grad_norm = 0.056847
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.053802
Total gradient norm: 0.251568
=== Actor Training Debug (Iteration 2425) ===
Q mean: -8.368460
Q std: 10.732276
Actor loss: 8.372446
Action reg: 0.003986
  l1.weight: grad_norm = 0.036846
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.035443
Total gradient norm: 0.179833
=== Actor Training Debug (Iteration 2426) ===
Q mean: -7.495631
Q std: 9.703492
Actor loss: 7.499615
Action reg: 0.003984
  l1.weight: grad_norm = 0.053421
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.036865
Total gradient norm: 0.137478
=== Actor Training Debug (Iteration 2427) ===
Q mean: -8.311882
Q std: 9.985989
Actor loss: 8.315866
Action reg: 0.003985
  l1.weight: grad_norm = 0.029158
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.025521
Total gradient norm: 0.099973
=== Actor Training Debug (Iteration 2428) ===
Q mean: -8.252337
Q std: 9.610167
Actor loss: 8.256320
Action reg: 0.003982
  l1.weight: grad_norm = 0.042136
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.035309
Total gradient norm: 0.128926
=== Actor Training Debug (Iteration 2429) ===
Q mean: -8.209059
Q std: 9.897789
Actor loss: 8.213041
Action reg: 0.003983
  l1.weight: grad_norm = 0.054591
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.052277
Total gradient norm: 0.213286
=== Actor Training Debug (Iteration 2430) ===
Q mean: -7.729807
Q std: 9.528232
Actor loss: 7.733799
Action reg: 0.003992
  l1.weight: grad_norm = 0.009459
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.008011
Total gradient norm: 0.044603
=== Actor Training Debug (Iteration 2431) ===
Q mean: -7.667506
Q std: 10.042468
Actor loss: 7.671490
Action reg: 0.003984
  l1.weight: grad_norm = 0.050071
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.044158
Total gradient norm: 0.215083
=== Actor Training Debug (Iteration 2432) ===
Q mean: -8.000690
Q std: 10.782686
Actor loss: 8.004673
Action reg: 0.003983
  l1.weight: grad_norm = 0.030632
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.026537
Total gradient norm: 0.113198
=== Actor Training Debug (Iteration 2433) ===
Q mean: -8.129262
Q std: 10.355399
Actor loss: 8.133250
Action reg: 0.003989
  l1.weight: grad_norm = 0.071770
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.053170
Total gradient norm: 0.210730
=== Actor Training Debug (Iteration 2434) ===
Q mean: -7.503168
Q std: 9.878951
Actor loss: 7.507155
Action reg: 0.003987
  l1.weight: grad_norm = 0.041997
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.031413
Total gradient norm: 0.127475
=== Actor Training Debug (Iteration 2435) ===
Q mean: -8.580922
Q std: 10.430980
Actor loss: 8.584909
Action reg: 0.003988
  l1.weight: grad_norm = 0.037798
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.035050
Total gradient norm: 0.172362
=== Actor Training Debug (Iteration 2436) ===
Q mean: -8.041994
Q std: 9.874705
Actor loss: 8.045979
Action reg: 0.003984
  l1.weight: grad_norm = 0.055103
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.042494
Total gradient norm: 0.208281
=== Actor Training Debug (Iteration 2437) ===
Q mean: -7.324708
Q std: 9.260623
Actor loss: 7.328691
Action reg: 0.003983
  l1.weight: grad_norm = 0.030937
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.028380
Total gradient norm: 0.142753
=== Actor Training Debug (Iteration 2438) ===
Q mean: -8.132476
Q std: 10.075279
Actor loss: 8.136460
Action reg: 0.003985
  l1.weight: grad_norm = 0.030591
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.029918
Total gradient norm: 0.144633
=== Actor Training Debug (Iteration 2439) ===
Q mean: -7.893857
Q std: 10.167618
Actor loss: 7.897841
Action reg: 0.003985
  l1.weight: grad_norm = 0.062694
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.054492
Total gradient norm: 0.185584
=== Actor Training Debug (Iteration 2440) ===
Q mean: -8.134673
Q std: 10.587724
Actor loss: 8.138648
Action reg: 0.003975
  l1.weight: grad_norm = 0.038914
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.032858
Total gradient norm: 0.160027
=== Actor Training Debug (Iteration 2441) ===
Q mean: -8.021090
Q std: 10.364046
Actor loss: 8.025081
Action reg: 0.003991
  l1.weight: grad_norm = 0.042739
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.035265
Total gradient norm: 0.142891
=== Actor Training Debug (Iteration 2442) ===
Q mean: -8.248489
Q std: 10.240776
Actor loss: 8.252483
Action reg: 0.003994
  l1.weight: grad_norm = 0.017222
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.016559
Total gradient norm: 0.078055
=== Actor Training Debug (Iteration 2443) ===
Q mean: -7.476401
Q std: 9.790183
Actor loss: 7.480394
Action reg: 0.003993
  l1.weight: grad_norm = 0.018501
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.016902
Total gradient norm: 0.076910
=== Actor Training Debug (Iteration 2444) ===
Q mean: -7.398422
Q std: 9.896503
Actor loss: 7.402408
Action reg: 0.003986
  l1.weight: grad_norm = 0.027449
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.024680
Total gradient norm: 0.129440
=== Actor Training Debug (Iteration 2445) ===
Q mean: -7.490516
Q std: 9.705707
Actor loss: 7.494503
Action reg: 0.003987
  l1.weight: grad_norm = 0.044075
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.040267
Total gradient norm: 0.198510
=== Actor Training Debug (Iteration 2446) ===
Q mean: -7.817103
Q std: 10.641486
Actor loss: 7.821091
Action reg: 0.003988
  l1.weight: grad_norm = 0.028472
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.026922
Total gradient norm: 0.156665
=== Actor Training Debug (Iteration 2447) ===
Q mean: -6.428547
Q std: 8.653763
Actor loss: 6.432531
Action reg: 0.003984
  l1.weight: grad_norm = 0.032650
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.030061
Total gradient norm: 0.122951
=== Actor Training Debug (Iteration 2448) ===
Q mean: -8.028411
Q std: 10.066598
Actor loss: 8.032401
Action reg: 0.003990
  l1.weight: grad_norm = 0.032148
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.029046
Total gradient norm: 0.153144
=== Actor Training Debug (Iteration 2449) ===
Q mean: -7.340189
Q std: 9.204852
Actor loss: 7.344182
Action reg: 0.003993
  l1.weight: grad_norm = 0.008483
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.007310
Total gradient norm: 0.027329
=== Actor Training Debug (Iteration 2450) ===
Q mean: -7.297099
Q std: 9.160414
Actor loss: 7.301086
Action reg: 0.003987
  l1.weight: grad_norm = 0.050797
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.048910
Total gradient norm: 0.243150
=== Actor Training Debug (Iteration 2451) ===
Q mean: -8.148691
Q std: 10.449653
Actor loss: 8.152679
Action reg: 0.003988
  l1.weight: grad_norm = 0.061543
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.053092
Total gradient norm: 0.210459
=== Actor Training Debug (Iteration 2452) ===
Q mean: -7.711869
Q std: 10.005050
Actor loss: 7.715855
Action reg: 0.003986
  l1.weight: grad_norm = 0.050271
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.043204
Total gradient norm: 0.179794
=== Actor Training Debug (Iteration 2453) ===
Q mean: -8.366188
Q std: 10.389997
Actor loss: 8.370172
Action reg: 0.003984
  l1.weight: grad_norm = 0.053107
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.041172
Total gradient norm: 0.187573
=== Actor Training Debug (Iteration 2454) ===
Q mean: -6.814370
Q std: 9.502048
Actor loss: 6.818351
Action reg: 0.003981
  l1.weight: grad_norm = 0.032537
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.028773
Total gradient norm: 0.111461
=== Actor Training Debug (Iteration 2455) ===
Q mean: -7.806672
Q std: 10.038123
Actor loss: 7.810653
Action reg: 0.003981
  l1.weight: grad_norm = 0.088686
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.067539
Total gradient norm: 0.326844
=== Actor Training Debug (Iteration 2456) ===
Q mean: -8.373022
Q std: 10.074420
Actor loss: 8.377013
Action reg: 0.003991
  l1.weight: grad_norm = 0.013444
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.009272
Total gradient norm: 0.037726
=== Actor Training Debug (Iteration 2457) ===
Q mean: -7.621427
Q std: 10.020100
Actor loss: 7.625417
Action reg: 0.003989
  l1.weight: grad_norm = 0.015228
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.012856
Total gradient norm: 0.047517
=== Actor Training Debug (Iteration 2458) ===
Q mean: -7.763523
Q std: 9.866879
Actor loss: 7.767506
Action reg: 0.003983
  l1.weight: grad_norm = 0.025099
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.024398
Total gradient norm: 0.120742
=== Actor Training Debug (Iteration 2459) ===
Q mean: -8.452330
Q std: 10.128116
Actor loss: 8.456316
Action reg: 0.003986
  l1.weight: grad_norm = 0.029775
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.025428
Total gradient norm: 0.121220
=== Actor Training Debug (Iteration 2460) ===
Q mean: -7.125991
Q std: 10.082381
Actor loss: 7.129973
Action reg: 0.003983
  l1.weight: grad_norm = 0.046346
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.043119
Total gradient norm: 0.205236
=== Actor Training Debug (Iteration 2461) ===
Q mean: -6.907863
Q std: 9.274505
Actor loss: 6.911851
Action reg: 0.003988
  l1.weight: grad_norm = 0.040795
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.039329
Total gradient norm: 0.196470
=== Actor Training Debug (Iteration 2462) ===
Q mean: -7.501346
Q std: 9.637500
Actor loss: 7.505325
Action reg: 0.003979
  l1.weight: grad_norm = 0.042270
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.038697
Total gradient norm: 0.157751
=== Actor Training Debug (Iteration 2463) ===
Q mean: -8.156029
Q std: 10.073950
Actor loss: 8.160009
Action reg: 0.003981
  l1.weight: grad_norm = 0.040707
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.033616
Total gradient norm: 0.153100
=== Actor Training Debug (Iteration 2464) ===
Q mean: -7.568718
Q std: 10.098793
Actor loss: 7.572702
Action reg: 0.003984
  l1.weight: grad_norm = 0.025591
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.021021
Total gradient norm: 0.086786
=== Actor Training Debug (Iteration 2465) ===
Q mean: -7.534071
Q std: 9.825856
Actor loss: 7.538062
Action reg: 0.003990
  l1.weight: grad_norm = 0.058807
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.047193
Total gradient norm: 0.200756
=== Actor Training Debug (Iteration 2466) ===
Q mean: -8.187397
Q std: 10.171752
Actor loss: 8.191383
Action reg: 0.003986
  l1.weight: grad_norm = 0.014618
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.014546
Total gradient norm: 0.084990
=== Actor Training Debug (Iteration 2467) ===
Q mean: -7.501830
Q std: 9.609078
Actor loss: 7.505820
Action reg: 0.003990
  l1.weight: grad_norm = 0.007310
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.007150
Total gradient norm: 0.037774
=== Actor Training Debug (Iteration 2468) ===
Q mean: -7.992409
Q std: 9.858826
Actor loss: 7.996394
Action reg: 0.003985
  l1.weight: grad_norm = 0.040893
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.040918
Total gradient norm: 0.242729
=== Actor Training Debug (Iteration 2469) ===
Q mean: -6.633739
Q std: 9.096944
Actor loss: 6.637728
Action reg: 0.003989
  l1.weight: grad_norm = 0.021763
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.016875
Total gradient norm: 0.066062
=== Actor Training Debug (Iteration 2470) ===
Q mean: -7.641033
Q std: 10.020246
Actor loss: 7.645013
Action reg: 0.003980
  l1.weight: grad_norm = 0.043382
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.037921
Total gradient norm: 0.144196
=== Actor Training Debug (Iteration 2471) ===
Q mean: -6.809747
Q std: 9.415609
Actor loss: 6.813725
Action reg: 0.003979
  l1.weight: grad_norm = 0.047691
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.038560
Total gradient norm: 0.129188
=== Actor Training Debug (Iteration 2472) ===
Q mean: -8.355099
Q std: 10.626634
Actor loss: 8.359087
Action reg: 0.003989
  l1.weight: grad_norm = 0.027372
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.026056
Total gradient norm: 0.127612
=== Actor Training Debug (Iteration 2473) ===
Q mean: -9.823667
Q std: 11.545050
Actor loss: 9.827654
Action reg: 0.003988
  l1.weight: grad_norm = 0.027456
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.021846
Total gradient norm: 0.100632
=== Actor Training Debug (Iteration 2474) ===
Q mean: -7.104519
Q std: 9.358212
Actor loss: 7.108510
Action reg: 0.003991
  l1.weight: grad_norm = 0.022293
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.019841
Total gradient norm: 0.103209
=== Actor Training Debug (Iteration 2475) ===
Q mean: -8.061646
Q std: 10.635198
Actor loss: 8.065638
Action reg: 0.003992
  l1.weight: grad_norm = 0.006240
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.005499
Total gradient norm: 0.016874
=== Actor Training Debug (Iteration 2476) ===
Q mean: -8.784218
Q std: 10.244613
Actor loss: 8.788209
Action reg: 0.003991
  l1.weight: grad_norm = 0.027866
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.025662
Total gradient norm: 0.134267
=== Actor Training Debug (Iteration 2477) ===
Q mean: -7.587128
Q std: 9.937513
Actor loss: 7.591117
Action reg: 0.003989
  l1.weight: grad_norm = 0.019966
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.018244
Total gradient norm: 0.082778
=== Actor Training Debug (Iteration 2478) ===
Q mean: -8.439887
Q std: 9.859782
Actor loss: 8.443871
Action reg: 0.003984
  l1.weight: grad_norm = 0.035220
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.030772
Total gradient norm: 0.130898
=== Actor Training Debug (Iteration 2479) ===
Q mean: -7.487174
Q std: 9.425885
Actor loss: 7.491165
Action reg: 0.003991
  l1.weight: grad_norm = 0.021311
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.021223
Total gradient norm: 0.100369
=== Actor Training Debug (Iteration 2480) ===
Q mean: -8.153076
Q std: 10.372528
Actor loss: 8.157059
Action reg: 0.003983
  l1.weight: grad_norm = 0.054112
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.050610
Total gradient norm: 0.257996
=== Actor Training Debug (Iteration 2481) ===
Q mean: -7.524383
Q std: 10.052215
Actor loss: 7.528368
Action reg: 0.003986
  l1.weight: grad_norm = 0.031708
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.028924
Total gradient norm: 0.140154
=== Actor Training Debug (Iteration 2482) ===
Q mean: -6.462014
Q std: 9.225009
Actor loss: 6.465999
Action reg: 0.003985
  l1.weight: grad_norm = 0.026955
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.023821
Total gradient norm: 0.112972
=== Actor Training Debug (Iteration 2483) ===
Q mean: -7.820240
Q std: 10.413782
Actor loss: 7.824214
Action reg: 0.003974
  l1.weight: grad_norm = 0.038589
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.029081
Total gradient norm: 0.118989
=== Actor Training Debug (Iteration 2484) ===
Q mean: -8.213321
Q std: 10.160397
Actor loss: 8.217307
Action reg: 0.003986
  l1.weight: grad_norm = 0.023414
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.021029
Total gradient norm: 0.109099
=== Actor Training Debug (Iteration 2485) ===
Q mean: -8.310947
Q std: 10.271629
Actor loss: 8.314939
Action reg: 0.003991
  l1.weight: grad_norm = 0.068170
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.055541
Total gradient norm: 0.251388
=== Actor Training Debug (Iteration 2486) ===
Q mean: -8.154984
Q std: 9.283412
Actor loss: 8.158969
Action reg: 0.003986
  l1.weight: grad_norm = 0.023575
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.023310
Total gradient norm: 0.081047
=== Actor Training Debug (Iteration 2487) ===
Q mean: -7.800609
Q std: 9.276825
Actor loss: 7.804596
Action reg: 0.003987
  l1.weight: grad_norm = 0.047722
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.037953
Total gradient norm: 0.191462
=== Actor Training Debug (Iteration 2488) ===
Q mean: -6.511525
Q std: 8.632760
Actor loss: 6.515512
Action reg: 0.003987
  l1.weight: grad_norm = 0.033127
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.029017
Total gradient norm: 0.143880
=== Actor Training Debug (Iteration 2489) ===
Q mean: -7.397349
Q std: 9.335157
Actor loss: 7.401338
Action reg: 0.003989
  l1.weight: grad_norm = 0.052673
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.048953
Total gradient norm: 0.214107
=== Actor Training Debug (Iteration 2490) ===
Q mean: -7.462640
Q std: 9.955597
Actor loss: 7.466629
Action reg: 0.003989
  l1.weight: grad_norm = 0.018421
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.017732
Total gradient norm: 0.076934
=== Actor Training Debug (Iteration 2491) ===
Q mean: -7.852718
Q std: 9.379045
Actor loss: 7.856698
Action reg: 0.003980
  l1.weight: grad_norm = 0.050051
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.046209
Total gradient norm: 0.217516
=== Actor Training Debug (Iteration 2492) ===
Q mean: -8.151757
Q std: 10.338988
Actor loss: 8.155743
Action reg: 0.003985
  l1.weight: grad_norm = 0.038384
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.031530
Total gradient norm: 0.171124
=== Actor Training Debug (Iteration 2493) ===
Q mean: -6.744998
Q std: 9.758561
Actor loss: 6.748978
Action reg: 0.003980
  l1.weight: grad_norm = 0.030544
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.023274
Total gradient norm: 0.111509
=== Actor Training Debug (Iteration 2494) ===
Q mean: -7.327179
Q std: 9.167125
Actor loss: 7.331168
Action reg: 0.003988
  l1.weight: grad_norm = 0.018930
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.018652
Total gradient norm: 0.093365
=== Actor Training Debug (Iteration 2495) ===
Q mean: -8.371058
Q std: 9.789573
Actor loss: 8.375038
Action reg: 0.003981
  l1.weight: grad_norm = 0.058777
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.049421
Total gradient norm: 0.231144
=== Actor Training Debug (Iteration 2496) ===
Q mean: -8.293421
Q std: 10.063951
Actor loss: 8.297400
Action reg: 0.003979
  l1.weight: grad_norm = 0.038775
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.033020
Total gradient norm: 0.162998
=== Actor Training Debug (Iteration 2497) ===
Q mean: -7.970382
Q std: 9.575304
Actor loss: 7.974367
Action reg: 0.003985
  l1.weight: grad_norm = 0.008476
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.007533
Total gradient norm: 0.030004
=== Actor Training Debug (Iteration 2498) ===
Q mean: -7.430046
Q std: 10.209103
Actor loss: 7.434018
Action reg: 0.003972
  l1.weight: grad_norm = 0.065390
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.061629
Total gradient norm: 0.257430
=== Actor Training Debug (Iteration 2499) ===
Q mean: -7.219220
Q std: 9.853361
Actor loss: 7.223196
Action reg: 0.003975
  l1.weight: grad_norm = 0.026355
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.022315
Total gradient norm: 0.105899
=== Actor Training Debug (Iteration 2500) ===
Q mean: -8.093468
Q std: 10.590588
Actor loss: 8.097432
Action reg: 0.003964
  l1.weight: grad_norm = 0.114057
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.098952
Total gradient norm: 0.481703
  Average reward: -325.352 | Average length: 100.0
Evaluation at episode 75: -325.352
=== Actor Training Debug (Iteration 2501) ===
Q mean: -8.494505
Q std: 10.391753
Actor loss: 8.498489
Action reg: 0.003984
  l1.weight: grad_norm = 0.026957
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.023343
Total gradient norm: 0.098143
=== Actor Training Debug (Iteration 2502) ===
Q mean: -8.024981
Q std: 11.077647
Actor loss: 8.028955
Action reg: 0.003973
  l1.weight: grad_norm = 0.073418
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.057618
Total gradient norm: 0.253161
=== Actor Training Debug (Iteration 2503) ===
Q mean: -7.769818
Q std: 10.002998
Actor loss: 7.773801
Action reg: 0.003983
  l1.weight: grad_norm = 0.017668
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.016127
Total gradient norm: 0.079837
=== Actor Training Debug (Iteration 2504) ===
Q mean: -7.698024
Q std: 10.225533
Actor loss: 7.702006
Action reg: 0.003982
  l1.weight: grad_norm = 0.055397
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.044481
Total gradient norm: 0.205418
=== Actor Training Debug (Iteration 2505) ===
Q mean: -7.927415
Q std: 10.499011
Actor loss: 7.931393
Action reg: 0.003978
  l1.weight: grad_norm = 0.069782
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.047402
Total gradient norm: 0.208520
=== Actor Training Debug (Iteration 2506) ===
Q mean: -7.898891
Q std: 10.215959
Actor loss: 7.902870
Action reg: 0.003979
  l1.weight: grad_norm = 0.157301
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.192474
Total gradient norm: 1.146531
=== Actor Training Debug (Iteration 2507) ===
Q mean: -8.298561
Q std: 10.092407
Actor loss: 8.302541
Action reg: 0.003979
  l1.weight: grad_norm = 0.027837
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.023829
Total gradient norm: 0.114983
=== Actor Training Debug (Iteration 2508) ===
Q mean: -7.483580
Q std: 9.871910
Actor loss: 7.487556
Action reg: 0.003976
  l1.weight: grad_norm = 0.064410
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.061619
Total gradient norm: 0.280211
=== Actor Training Debug (Iteration 2509) ===
Q mean: -7.653546
Q std: 9.920107
Actor loss: 7.657526
Action reg: 0.003980
  l1.weight: grad_norm = 0.055485
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.042446
Total gradient norm: 0.163813
=== Actor Training Debug (Iteration 2510) ===
Q mean: -7.116572
Q std: 9.433160
Actor loss: 7.120548
Action reg: 0.003975
  l1.weight: grad_norm = 0.044742
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.046918
Total gradient norm: 0.254640
=== Actor Training Debug (Iteration 2511) ===
Q mean: -7.774140
Q std: 9.772058
Actor loss: 7.778121
Action reg: 0.003980
  l1.weight: grad_norm = 0.094558
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.075381
Total gradient norm: 0.302807
=== Actor Training Debug (Iteration 2512) ===
Q mean: -8.548425
Q std: 10.292238
Actor loss: 8.552406
Action reg: 0.003982
  l1.weight: grad_norm = 0.049437
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.048144
Total gradient norm: 0.214108
=== Actor Training Debug (Iteration 2513) ===
Q mean: -8.134901
Q std: 10.819282
Actor loss: 8.138876
Action reg: 0.003975
  l1.weight: grad_norm = 0.042084
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.040115
Total gradient norm: 0.171182
=== Actor Training Debug (Iteration 2514) ===
Q mean: -8.307584
Q std: 10.417882
Actor loss: 8.311570
Action reg: 0.003986
  l1.weight: grad_norm = 0.154169
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.135446
Total gradient norm: 0.551047
=== Actor Training Debug (Iteration 2515) ===
Q mean: -8.372225
Q std: 9.804917
Actor loss: 8.376204
Action reg: 0.003979
  l1.weight: grad_norm = 0.036467
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.033085
Total gradient norm: 0.154156
=== Actor Training Debug (Iteration 2516) ===
Q mean: -8.212652
Q std: 9.391822
Actor loss: 8.216635
Action reg: 0.003983
  l1.weight: grad_norm = 0.082280
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.071550
Total gradient norm: 0.334919
=== Actor Training Debug (Iteration 2517) ===
Q mean: -7.887875
Q std: 9.909518
Actor loss: 7.891860
Action reg: 0.003985
  l1.weight: grad_norm = 0.039799
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.033242
Total gradient norm: 0.127607
=== Actor Training Debug (Iteration 2518) ===
Q mean: -7.472345
Q std: 9.682182
Actor loss: 7.476317
Action reg: 0.003973
  l1.weight: grad_norm = 0.067513
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.065500
Total gradient norm: 0.223051
=== Actor Training Debug (Iteration 2519) ===
Q mean: -7.582654
Q std: 9.937498
Actor loss: 7.586637
Action reg: 0.003983
  l1.weight: grad_norm = 0.076214
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.073973
Total gradient norm: 0.366404
=== Actor Training Debug (Iteration 2520) ===
Q mean: -7.257200
Q std: 9.401768
Actor loss: 7.261173
Action reg: 0.003973
  l1.weight: grad_norm = 0.046350
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.043467
Total gradient norm: 0.208738
=== Actor Training Debug (Iteration 2521) ===
Q mean: -8.058260
Q std: 10.254339
Actor loss: 8.062240
Action reg: 0.003980
  l1.weight: grad_norm = 0.064094
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.059509
Total gradient norm: 0.274541
=== Actor Training Debug (Iteration 2522) ===
Q mean: -6.309857
Q std: 9.539846
Actor loss: 6.313843
Action reg: 0.003986
  l1.weight: grad_norm = 0.206259
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.190772
Total gradient norm: 0.878992
=== Actor Training Debug (Iteration 2523) ===
Q mean: -7.160679
Q std: 9.457088
Actor loss: 7.164658
Action reg: 0.003978
  l1.weight: grad_norm = 0.080149
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.083218
Total gradient norm: 0.330974
=== Actor Training Debug (Iteration 2524) ===
Q mean: -8.132874
Q std: 9.997320
Actor loss: 8.136849
Action reg: 0.003975
  l1.weight: grad_norm = 0.045936
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.036750
Total gradient norm: 0.154003
=== Actor Training Debug (Iteration 2525) ===
Q mean: -6.370373
Q std: 8.985506
Actor loss: 6.374354
Action reg: 0.003981
  l1.weight: grad_norm = 0.076250
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.062692
Total gradient norm: 0.268936
=== Actor Training Debug (Iteration 2526) ===
Q mean: -7.399820
Q std: 9.778848
Actor loss: 7.403795
Action reg: 0.003975
  l1.weight: grad_norm = 0.048953
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.044417
Total gradient norm: 0.157854
=== Actor Training Debug (Iteration 2527) ===
Q mean: -7.617569
Q std: 10.127470
Actor loss: 7.621555
Action reg: 0.003986
  l1.weight: grad_norm = 0.016422
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.015625
Total gradient norm: 0.054460
=== Actor Training Debug (Iteration 2528) ===
Q mean: -8.497053
Q std: 10.295894
Actor loss: 8.501029
Action reg: 0.003976
  l1.weight: grad_norm = 0.117300
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.115000
Total gradient norm: 0.528728
=== Actor Training Debug (Iteration 2529) ===
Q mean: -8.409462
Q std: 10.431058
Actor loss: 8.413448
Action reg: 0.003986
  l1.weight: grad_norm = 0.025917
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.023636
Total gradient norm: 0.081033
=== Actor Training Debug (Iteration 2530) ===
Q mean: -7.018008
Q std: 9.972536
Actor loss: 7.021997
Action reg: 0.003989
  l1.weight: grad_norm = 0.078626
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.076250
Total gradient norm: 0.307728
=== Actor Training Debug (Iteration 2531) ===
Q mean: -7.780566
Q std: 9.662207
Actor loss: 7.784552
Action reg: 0.003986
  l1.weight: grad_norm = 0.022245
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.019715
Total gradient norm: 0.096769
=== Actor Training Debug (Iteration 2532) ===
Q mean: -7.718638
Q std: 10.128843
Actor loss: 7.722620
Action reg: 0.003982
  l1.weight: grad_norm = 0.037737
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.034103
Total gradient norm: 0.150089
=== Actor Training Debug (Iteration 2533) ===
Q mean: -8.247222
Q std: 10.743569
Actor loss: 8.251204
Action reg: 0.003981
  l1.weight: grad_norm = 0.059881
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.053687
Total gradient norm: 0.272670
=== Actor Training Debug (Iteration 2534) ===
Q mean: -8.500338
Q std: 11.283762
Actor loss: 8.504322
Action reg: 0.003984
  l1.weight: grad_norm = 0.069049
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.061855
Total gradient norm: 0.286348
=== Actor Training Debug (Iteration 2535) ===
Q mean: -6.733930
Q std: 9.282039
Actor loss: 6.737918
Action reg: 0.003989
  l1.weight: grad_norm = 0.022128
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.019848
Total gradient norm: 0.086567
=== Actor Training Debug (Iteration 2536) ===
Q mean: -7.288281
Q std: 10.243151
Actor loss: 7.292272
Action reg: 0.003990
  l1.weight: grad_norm = 0.016584
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.014258
Total gradient norm: 0.053159
=== Actor Training Debug (Iteration 2537) ===
Q mean: -7.722406
Q std: 10.612196
Actor loss: 7.726390
Action reg: 0.003984
  l1.weight: grad_norm = 0.111951
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.098021
Total gradient norm: 0.322623
=== Actor Training Debug (Iteration 2538) ===
Q mean: -7.477894
Q std: 10.177786
Actor loss: 7.481876
Action reg: 0.003982
  l1.weight: grad_norm = 0.049646
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.042707
Total gradient norm: 0.162559
=== Actor Training Debug (Iteration 2539) ===
Q mean: -7.470344
Q std: 9.895736
Actor loss: 7.474324
Action reg: 0.003980
  l1.weight: grad_norm = 0.019332
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.017321
Total gradient norm: 0.082500
=== Actor Training Debug (Iteration 2540) ===
Q mean: -6.576609
Q std: 9.646633
Actor loss: 6.580575
Action reg: 0.003967
  l1.weight: grad_norm = 0.107880
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.091248
Total gradient norm: 0.345028
=== Actor Training Debug (Iteration 2541) ===
Q mean: -8.502687
Q std: 10.817282
Actor loss: 8.506678
Action reg: 0.003990
  l1.weight: grad_norm = 0.021772
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.020485
Total gradient norm: 0.092333
=== Actor Training Debug (Iteration 2542) ===
Q mean: -9.111532
Q std: 10.499412
Actor loss: 9.115505
Action reg: 0.003973
  l1.weight: grad_norm = 0.076965
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.069581
Total gradient norm: 0.278937
=== Actor Training Debug (Iteration 2543) ===
Q mean: -7.802983
Q std: 9.836173
Actor loss: 7.806973
Action reg: 0.003990
  l1.weight: grad_norm = 0.045597
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.038741
Total gradient norm: 0.160745
=== Actor Training Debug (Iteration 2544) ===
Q mean: -7.224020
Q std: 10.128775
Actor loss: 7.228002
Action reg: 0.003982
  l1.weight: grad_norm = 0.045109
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.042802
Total gradient norm: 0.224272
=== Actor Training Debug (Iteration 2545) ===
Q mean: -7.667399
Q std: 9.681975
Actor loss: 7.671382
Action reg: 0.003983
  l1.weight: grad_norm = 0.058118
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.048029
Total gradient norm: 0.193144
=== Actor Training Debug (Iteration 2546) ===
Q mean: -6.846543
Q std: 9.858474
Actor loss: 6.850523
Action reg: 0.003980
  l1.weight: grad_norm = 0.041712
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.036368
Total gradient norm: 0.165801
=== Actor Training Debug (Iteration 2547) ===
Q mean: -8.035179
Q std: 10.735198
Actor loss: 8.039165
Action reg: 0.003986
  l1.weight: grad_norm = 0.029836
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.027677
Total gradient norm: 0.142183
=== Actor Training Debug (Iteration 2548) ===
Q mean: -7.717339
Q std: 10.162223
Actor loss: 7.721314
Action reg: 0.003975
  l1.weight: grad_norm = 0.097138
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.080304
Total gradient norm: 0.305488
=== Actor Training Debug (Iteration 2549) ===
Q mean: -7.823818
Q std: 9.878393
Actor loss: 7.827808
Action reg: 0.003990
  l1.weight: grad_norm = 0.019625
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.017619
Total gradient norm: 0.074805
=== Actor Training Debug (Iteration 2550) ===
Q mean: -6.837423
Q std: 8.980976
Actor loss: 6.841406
Action reg: 0.003984
  l1.weight: grad_norm = 0.048340
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.040986
Total gradient norm: 0.175773
=== Actor Training Debug (Iteration 2551) ===
Q mean: -8.386109
Q std: 10.994174
Actor loss: 8.390093
Action reg: 0.003983
  l1.weight: grad_norm = 0.056600
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.044938
Total gradient norm: 0.173142
=== Actor Training Debug (Iteration 2552) ===
Q mean: -7.464994
Q std: 10.316350
Actor loss: 7.468966
Action reg: 0.003972
  l1.weight: grad_norm = 0.063726
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.052893
Total gradient norm: 0.237063
=== Actor Training Debug (Iteration 2553) ===
Q mean: -8.458361
Q std: 11.124674
Actor loss: 8.462349
Action reg: 0.003988
  l1.weight: grad_norm = 0.024024
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.024145
Total gradient norm: 0.105170
=== Actor Training Debug (Iteration 2554) ===
Q mean: -8.201364
Q std: 10.774793
Actor loss: 8.205345
Action reg: 0.003982
  l1.weight: grad_norm = 0.012218
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.011545
Total gradient norm: 0.052785
=== Actor Training Debug (Iteration 2555) ===
Q mean: -8.813993
Q std: 10.938490
Actor loss: 8.817979
Action reg: 0.003986
  l1.weight: grad_norm = 0.048921
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.042732
Total gradient norm: 0.167681
=== Actor Training Debug (Iteration 2556) ===
Q mean: -9.018829
Q std: 10.976196
Actor loss: 9.022813
Action reg: 0.003983
  l1.weight: grad_norm = 0.037226
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.037189
Total gradient norm: 0.181211
=== Actor Training Debug (Iteration 2557) ===
Q mean: -8.295109
Q std: 10.449933
Actor loss: 8.299081
Action reg: 0.003972
  l1.weight: grad_norm = 0.044092
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.045201
Total gradient norm: 0.228499
=== Actor Training Debug (Iteration 2558) ===
Q mean: -8.641800
Q std: 10.442199
Actor loss: 8.645782
Action reg: 0.003983
  l1.weight: grad_norm = 0.076012
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.061777
Total gradient norm: 0.299632
=== Actor Training Debug (Iteration 2559) ===
Q mean: -7.603255
Q std: 10.073873
Actor loss: 7.607244
Action reg: 0.003989
  l1.weight: grad_norm = 0.020201
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.018747
Total gradient norm: 0.097953
=== Actor Training Debug (Iteration 2560) ===
Q mean: -8.098940
Q std: 10.276854
Actor loss: 8.102914
Action reg: 0.003973
  l1.weight: grad_norm = 0.042494
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.037566
Total gradient norm: 0.153586
=== Actor Training Debug (Iteration 2561) ===
Q mean: -8.059994
Q std: 11.135340
Actor loss: 8.063972
Action reg: 0.003977
  l1.weight: grad_norm = 0.042605
  l1.bias: grad_norm = 0.000670
  l2.weight: grad_norm = 0.034953
Total gradient norm: 0.124547
=== Actor Training Debug (Iteration 2562) ===
Q mean: -8.213282
Q std: 11.347400
Actor loss: 8.217264
Action reg: 0.003983
  l1.weight: grad_norm = 0.065116
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.062328
Total gradient norm: 0.240856
=== Actor Training Debug (Iteration 2563) ===
Q mean: -8.007180
Q std: 10.241680
Actor loss: 8.011160
Action reg: 0.003980
  l1.weight: grad_norm = 0.040363
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.034368
Total gradient norm: 0.165176
=== Actor Training Debug (Iteration 2564) ===
Q mean: -7.870623
Q std: 9.521394
Actor loss: 7.874588
Action reg: 0.003965
  l1.weight: grad_norm = 0.030704
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.030629
Total gradient norm: 0.107067
=== Actor Training Debug (Iteration 2565) ===
Q mean: -7.296441
Q std: 10.400608
Actor loss: 7.300424
Action reg: 0.003983
  l1.weight: grad_norm = 0.036929
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.032993
Total gradient norm: 0.110210
=== Actor Training Debug (Iteration 2566) ===
Q mean: -8.310808
Q std: 10.535291
Actor loss: 8.314795
Action reg: 0.003986
  l1.weight: grad_norm = 0.023237
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.021209
Total gradient norm: 0.086355
=== Actor Training Debug (Iteration 2567) ===
Q mean: -8.786133
Q std: 11.292798
Actor loss: 8.790102
Action reg: 0.003969
  l1.weight: grad_norm = 0.109982
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.093510
Total gradient norm: 0.460731
=== Actor Training Debug (Iteration 2568) ===
Q mean: -7.650857
Q std: 9.548089
Actor loss: 7.654838
Action reg: 0.003981
  l1.weight: grad_norm = 0.039335
  l1.bias: grad_norm = 0.000525
  l2.weight: grad_norm = 0.032689
Total gradient norm: 0.124434
=== Actor Training Debug (Iteration 2569) ===
Q mean: -8.162751
Q std: 10.528987
Actor loss: 8.166738
Action reg: 0.003986
  l1.weight: grad_norm = 0.033904
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.028620
Total gradient norm: 0.123336
=== Actor Training Debug (Iteration 2570) ===
Q mean: -8.241668
Q std: 10.466690
Actor loss: 8.245650
Action reg: 0.003983
  l1.weight: grad_norm = 0.038103
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.032461
Total gradient norm: 0.140488
=== Actor Training Debug (Iteration 2571) ===
Q mean: -7.804500
Q std: 9.738527
Actor loss: 7.808477
Action reg: 0.003978
  l1.weight: grad_norm = 0.078445
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.058554
Total gradient norm: 0.232616
=== Actor Training Debug (Iteration 2572) ===
Q mean: -8.895874
Q std: 11.349943
Actor loss: 8.899853
Action reg: 0.003978
  l1.weight: grad_norm = 0.035477
  l1.bias: grad_norm = 0.000658
  l2.weight: grad_norm = 0.028123
Total gradient norm: 0.105952
=== Actor Training Debug (Iteration 2573) ===
Q mean: -8.689513
Q std: 11.369467
Actor loss: 8.693501
Action reg: 0.003988
  l1.weight: grad_norm = 0.014570
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.013429
Total gradient norm: 0.058203
=== Actor Training Debug (Iteration 2574) ===
Q mean: -7.525109
Q std: 9.807751
Actor loss: 7.529082
Action reg: 0.003973
  l1.weight: grad_norm = 0.050505
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.045345
Total gradient norm: 0.186857
=== Actor Training Debug (Iteration 2575) ===
Q mean: -8.927003
Q std: 10.164267
Actor loss: 8.930985
Action reg: 0.003983
  l1.weight: grad_norm = 0.112833
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.082109
Total gradient norm: 0.299015
=== Actor Training Debug (Iteration 2576) ===
Q mean: -8.496519
Q std: 11.311565
Actor loss: 8.500481
Action reg: 0.003962
  l1.weight: grad_norm = 0.051258
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.048063
Total gradient norm: 0.224147
=== Actor Training Debug (Iteration 2577) ===
Q mean: -7.742215
Q std: 10.086916
Actor loss: 7.746185
Action reg: 0.003970
  l1.weight: grad_norm = 0.039937
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.033710
Total gradient norm: 0.163828
=== Actor Training Debug (Iteration 2578) ===
Q mean: -8.649069
Q std: 10.369899
Actor loss: 8.653049
Action reg: 0.003979
  l1.weight: grad_norm = 0.055138
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.049865
Total gradient norm: 0.239907
=== Actor Training Debug (Iteration 2579) ===
Q mean: -8.851620
Q std: 11.184752
Actor loss: 8.855598
Action reg: 0.003979
  l1.weight: grad_norm = 0.037227
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.030837
Total gradient norm: 0.128379
=== Actor Training Debug (Iteration 2580) ===
Q mean: -7.894081
Q std: 9.969504
Actor loss: 7.898058
Action reg: 0.003977
  l1.weight: grad_norm = 0.055296
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.048657
Total gradient norm: 0.240978
=== Actor Training Debug (Iteration 2581) ===
Q mean: -8.351351
Q std: 10.716670
Actor loss: 8.355335
Action reg: 0.003985
  l1.weight: grad_norm = 0.008008
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.007491
Total gradient norm: 0.031794
=== Actor Training Debug (Iteration 2582) ===
Q mean: -7.725003
Q std: 10.380684
Actor loss: 7.728979
Action reg: 0.003976
  l1.weight: grad_norm = 0.047872
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.043585
Total gradient norm: 0.208031
=== Actor Training Debug (Iteration 2583) ===
Q mean: -7.156365
Q std: 9.833324
Actor loss: 7.160341
Action reg: 0.003976
  l1.weight: grad_norm = 0.021242
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.019675
Total gradient norm: 0.116014
=== Actor Training Debug (Iteration 2584) ===
Q mean: -8.352999
Q std: 10.700539
Actor loss: 8.356980
Action reg: 0.003982
  l1.weight: grad_norm = 0.054890
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.051816
Total gradient norm: 0.255087
=== Actor Training Debug (Iteration 2585) ===
Q mean: -8.582001
Q std: 10.966027
Actor loss: 8.585978
Action reg: 0.003977
  l1.weight: grad_norm = 0.057475
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.052312
Total gradient norm: 0.215922
=== Actor Training Debug (Iteration 2586) ===
Q mean: -8.038389
Q std: 10.240685
Actor loss: 8.042371
Action reg: 0.003981
  l1.weight: grad_norm = 0.038812
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.035722
Total gradient norm: 0.125309
=== Actor Training Debug (Iteration 2587) ===
Q mean: -7.312773
Q std: 9.873534
Actor loss: 7.316756
Action reg: 0.003983
  l1.weight: grad_norm = 0.065807
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.053885
Total gradient norm: 0.234049
=== Actor Training Debug (Iteration 2588) ===
Q mean: -8.139580
Q std: 10.929265
Actor loss: 8.143567
Action reg: 0.003987
  l1.weight: grad_norm = 0.040280
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.035313
Total gradient norm: 0.168543
=== Actor Training Debug (Iteration 2589) ===
Q mean: -7.492234
Q std: 9.695987
Actor loss: 7.496203
Action reg: 0.003969
  l1.weight: grad_norm = 0.028584
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.023991
Total gradient norm: 0.091408
=== Actor Training Debug (Iteration 2590) ===
Q mean: -6.951649
Q std: 9.216017
Actor loss: 6.955626
Action reg: 0.003977
  l1.weight: grad_norm = 0.035851
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.034994
Total gradient norm: 0.157373
=== Actor Training Debug (Iteration 2591) ===
Q mean: -8.506805
Q std: 10.742745
Actor loss: 8.510776
Action reg: 0.003970
  l1.weight: grad_norm = 0.028004
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.026402
Total gradient norm: 0.116111
=== Actor Training Debug (Iteration 2592) ===
Q mean: -6.924484
Q std: 9.468569
Actor loss: 6.928474
Action reg: 0.003990
  l1.weight: grad_norm = 0.016508
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.014301
Total gradient norm: 0.063797
=== Actor Training Debug (Iteration 2593) ===
Q mean: -8.985874
Q std: 11.619105
Actor loss: 8.989852
Action reg: 0.003978
  l1.weight: grad_norm = 0.045982
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.044263
Total gradient norm: 0.179291
=== Actor Training Debug (Iteration 2594) ===
Q mean: -8.019814
Q std: 11.182483
Actor loss: 8.023793
Action reg: 0.003980
  l1.weight: grad_norm = 0.019505
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.017054
Total gradient norm: 0.082776
=== Actor Training Debug (Iteration 2595) ===
Q mean: -7.878139
Q std: 9.906177
Actor loss: 7.882109
Action reg: 0.003970
  l1.weight: grad_norm = 0.054050
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.048167
Total gradient norm: 0.263513
=== Actor Training Debug (Iteration 2596) ===
Q mean: -7.949912
Q std: 10.720911
Actor loss: 7.953882
Action reg: 0.003969
  l1.weight: grad_norm = 0.047023
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.041492
Total gradient norm: 0.209720
=== Actor Training Debug (Iteration 2597) ===
Q mean: -8.473930
Q std: 10.649899
Actor loss: 8.477915
Action reg: 0.003985
  l1.weight: grad_norm = 0.064578
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.054407
Total gradient norm: 0.196044
=== Actor Training Debug (Iteration 2598) ===
Q mean: -7.378260
Q std: 10.259188
Actor loss: 7.382240
Action reg: 0.003981
  l1.weight: grad_norm = 0.075147
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.060714
Total gradient norm: 0.282224
=== Actor Training Debug (Iteration 2599) ===
Q mean: -8.714622
Q std: 10.901489
Actor loss: 8.718594
Action reg: 0.003971
  l1.weight: grad_norm = 0.109932
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.099582
Total gradient norm: 0.345968
=== Actor Training Debug (Iteration 2600) ===
Q mean: -7.828514
Q std: 10.032361
Actor loss: 7.832500
Action reg: 0.003986
  l1.weight: grad_norm = 0.023155
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.022533
Total gradient norm: 0.116071
=== Actor Training Debug (Iteration 2601) ===
Q mean: -8.605893
Q std: 10.416829
Actor loss: 8.609881
Action reg: 0.003988
  l1.weight: grad_norm = 0.063197
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.045301
Total gradient norm: 0.176643
=== Actor Training Debug (Iteration 2602) ===
Q mean: -7.674883
Q std: 10.839058
Actor loss: 7.678864
Action reg: 0.003981
  l1.weight: grad_norm = 0.028866
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.026627
Total gradient norm: 0.120890
=== Actor Training Debug (Iteration 2603) ===
Q mean: -9.384734
Q std: 11.471085
Actor loss: 9.388712
Action reg: 0.003978
  l1.weight: grad_norm = 0.034446
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.026791
Total gradient norm: 0.111442
=== Actor Training Debug (Iteration 2604) ===
Q mean: -8.282244
Q std: 10.531871
Actor loss: 8.286218
Action reg: 0.003974
  l1.weight: grad_norm = 0.057969
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.044383
Total gradient norm: 0.190994
=== Actor Training Debug (Iteration 2605) ===
Q mean: -8.380068
Q std: 11.042151
Actor loss: 8.384041
Action reg: 0.003973
  l1.weight: grad_norm = 0.026579
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.022456
Total gradient norm: 0.082098
=== Actor Training Debug (Iteration 2606) ===
Q mean: -7.595940
Q std: 10.104287
Actor loss: 7.599921
Action reg: 0.003982
  l1.weight: grad_norm = 0.042174
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.039216
Total gradient norm: 0.160750
=== Actor Training Debug (Iteration 2607) ===
Q mean: -7.291324
Q std: 9.970259
Actor loss: 7.295299
Action reg: 0.003975
  l1.weight: grad_norm = 0.027270
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.024277
Total gradient norm: 0.106868
=== Actor Training Debug (Iteration 2608) ===
Q mean: -7.793360
Q std: 10.007568
Actor loss: 7.797336
Action reg: 0.003976
  l1.weight: grad_norm = 0.041901
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.037724
Total gradient norm: 0.169838
=== Actor Training Debug (Iteration 2609) ===
Q mean: -7.004485
Q std: 9.877309
Actor loss: 7.008462
Action reg: 0.003977
  l1.weight: grad_norm = 0.025455
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.022126
Total gradient norm: 0.097144
=== Actor Training Debug (Iteration 2610) ===
Q mean: -7.120804
Q std: 9.635054
Actor loss: 7.124782
Action reg: 0.003978
  l1.weight: grad_norm = 0.071208
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.058231
Total gradient norm: 0.245483
=== Actor Training Debug (Iteration 2611) ===
Q mean: -7.165582
Q std: 10.267838
Actor loss: 7.169564
Action reg: 0.003982
  l1.weight: grad_norm = 0.063330
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.041794
Total gradient norm: 0.159359
=== Actor Training Debug (Iteration 2612) ===
Q mean: -7.059500
Q std: 9.668880
Actor loss: 7.063474
Action reg: 0.003974
  l1.weight: grad_norm = 0.028939
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.025237
Total gradient norm: 0.114886
=== Actor Training Debug (Iteration 2613) ===
Q mean: -8.158539
Q std: 10.367221
Actor loss: 8.162520
Action reg: 0.003982
  l1.weight: grad_norm = 0.017237
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.015798
Total gradient norm: 0.077741
=== Actor Training Debug (Iteration 2614) ===
Q mean: -8.560100
Q std: 10.939470
Actor loss: 8.564067
Action reg: 0.003967
  l1.weight: grad_norm = 0.064988
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.059680
Total gradient norm: 0.299950
=== Actor Training Debug (Iteration 2615) ===
Q mean: -8.869658
Q std: 10.772646
Actor loss: 8.873642
Action reg: 0.003984
  l1.weight: grad_norm = 0.115165
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.075283
Total gradient norm: 0.282393
=== Actor Training Debug (Iteration 2616) ===
Q mean: -9.013998
Q std: 11.559329
Actor loss: 9.017980
Action reg: 0.003981
  l1.weight: grad_norm = 0.031481
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.026220
Total gradient norm: 0.100675
=== Actor Training Debug (Iteration 2617) ===
Q mean: -7.903973
Q std: 9.837296
Actor loss: 7.907941
Action reg: 0.003969
  l1.weight: grad_norm = 0.041986
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.036459
Total gradient norm: 0.160033
=== Actor Training Debug (Iteration 2618) ===
Q mean: -8.297017
Q std: 10.070678
Actor loss: 8.300994
Action reg: 0.003977
  l1.weight: grad_norm = 0.052158
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.041367
Total gradient norm: 0.196347
=== Actor Training Debug (Iteration 2619) ===
Q mean: -7.651946
Q std: 9.845723
Actor loss: 7.655922
Action reg: 0.003976
  l1.weight: grad_norm = 0.020562
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.018207
Total gradient norm: 0.086692
=== Actor Training Debug (Iteration 2620) ===
Q mean: -8.546154
Q std: 10.331193
Actor loss: 8.550142
Action reg: 0.003988
  l1.weight: grad_norm = 0.025618
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.022375
Total gradient norm: 0.090735
=== Actor Training Debug (Iteration 2621) ===
Q mean: -6.258243
Q std: 9.214421
Actor loss: 6.262207
Action reg: 0.003965
  l1.weight: grad_norm = 0.123149
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.095067
Total gradient norm: 0.333878
=== Actor Training Debug (Iteration 2622) ===
Q mean: -7.746253
Q std: 10.210502
Actor loss: 7.750222
Action reg: 0.003969
  l1.weight: grad_norm = 0.037431
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.032755
Total gradient norm: 0.142067
=== Actor Training Debug (Iteration 2623) ===
Q mean: -8.230617
Q std: 10.234652
Actor loss: 8.234592
Action reg: 0.003976
  l1.weight: grad_norm = 0.042958
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.038274
Total gradient norm: 0.192946
=== Actor Training Debug (Iteration 2624) ===
Q mean: -8.077295
Q std: 10.287174
Actor loss: 8.081269
Action reg: 0.003974
  l1.weight: grad_norm = 0.038013
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.032602
Total gradient norm: 0.155494
=== Actor Training Debug (Iteration 2625) ===
Q mean: -7.649832
Q std: 10.452289
Actor loss: 7.653813
Action reg: 0.003981
  l1.weight: grad_norm = 0.086076
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.067626
Total gradient norm: 0.222112
=== Actor Training Debug (Iteration 2626) ===
Q mean: -7.752596
Q std: 9.689769
Actor loss: 7.756580
Action reg: 0.003984
  l1.weight: grad_norm = 0.037248
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.037629
Total gradient norm: 0.162193
=== Actor Training Debug (Iteration 2627) ===
Q mean: -8.336506
Q std: 10.679466
Actor loss: 8.340490
Action reg: 0.003984
  l1.weight: grad_norm = 0.120508
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.096911
Total gradient norm: 0.335900
=== Actor Training Debug (Iteration 2628) ===
Q mean: -7.503080
Q std: 10.158890
Actor loss: 7.507054
Action reg: 0.003974
  l1.weight: grad_norm = 0.043579
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.036111
Total gradient norm: 0.150684
=== Actor Training Debug (Iteration 2629) ===
Q mean: -8.546610
Q std: 11.047527
Actor loss: 8.550595
Action reg: 0.003986
  l1.weight: grad_norm = 0.048672
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.045228
Total gradient norm: 0.209506
=== Actor Training Debug (Iteration 2630) ===
Q mean: -9.193780
Q std: 11.096302
Actor loss: 9.197762
Action reg: 0.003983
  l1.weight: grad_norm = 0.052615
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.049852
Total gradient norm: 0.173631
=== Actor Training Debug (Iteration 2631) ===
Q mean: -6.660406
Q std: 9.881217
Actor loss: 6.664370
Action reg: 0.003964
  l1.weight: grad_norm = 0.163451
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.147215
Total gradient norm: 0.629039
=== Actor Training Debug (Iteration 2632) ===
Q mean: -7.482525
Q std: 9.926151
Actor loss: 7.486499
Action reg: 0.003974
  l1.weight: grad_norm = 0.044998
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.038261
Total gradient norm: 0.146422
=== Actor Training Debug (Iteration 2633) ===
Q mean: -9.114880
Q std: 11.089344
Actor loss: 9.118855
Action reg: 0.003976
  l1.weight: grad_norm = 0.026122
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.025499
Total gradient norm: 0.110065
=== Actor Training Debug (Iteration 2634) ===
Q mean: -7.840550
Q std: 10.051864
Actor loss: 7.844530
Action reg: 0.003980
  l1.weight: grad_norm = 0.081482
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.073135
Total gradient norm: 0.214697
=== Actor Training Debug (Iteration 2635) ===
Q mean: -7.132465
Q std: 9.629812
Actor loss: 7.136448
Action reg: 0.003983
  l1.weight: grad_norm = 0.045566
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.034783
Total gradient norm: 0.140319
=== Actor Training Debug (Iteration 2636) ===
Q mean: -8.060652
Q std: 10.110552
Actor loss: 8.064620
Action reg: 0.003969
  l1.weight: grad_norm = 0.121704
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.088290
Total gradient norm: 0.307413
=== Actor Training Debug (Iteration 2637) ===
Q mean: -7.232535
Q std: 9.332625
Actor loss: 7.236505
Action reg: 0.003970
  l1.weight: grad_norm = 0.135929
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.106257
Total gradient norm: 0.376593
=== Actor Training Debug (Iteration 2638) ===
Q mean: -7.197612
Q std: 10.099007
Actor loss: 7.201591
Action reg: 0.003979
  l1.weight: grad_norm = 0.046127
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.037688
Total gradient norm: 0.172008
=== Actor Training Debug (Iteration 2639) ===
Q mean: -8.133759
Q std: 10.756374
Actor loss: 8.137730
Action reg: 0.003970
  l1.weight: grad_norm = 0.040957
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.039150
Total gradient norm: 0.139020
=== Actor Training Debug (Iteration 2640) ===
Q mean: -7.982961
Q std: 10.609953
Actor loss: 7.986938
Action reg: 0.003977
  l1.weight: grad_norm = 0.045396
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.035423
Total gradient norm: 0.135373
=== Actor Training Debug (Iteration 2641) ===
Q mean: -7.804100
Q std: 10.475721
Actor loss: 7.808085
Action reg: 0.003986
  l1.weight: grad_norm = 0.044010
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.029694
Total gradient norm: 0.093817
=== Actor Training Debug (Iteration 2642) ===
Q mean: -7.907638
Q std: 10.641671
Actor loss: 7.911617
Action reg: 0.003980
  l1.weight: grad_norm = 0.052649
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.041987
Total gradient norm: 0.200081
=== Actor Training Debug (Iteration 2643) ===
Q mean: -8.647702
Q std: 10.719182
Actor loss: 8.651684
Action reg: 0.003981
  l1.weight: grad_norm = 0.044810
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.040763
Total gradient norm: 0.167716
=== Actor Training Debug (Iteration 2644) ===
Q mean: -7.667599
Q std: 10.598939
Actor loss: 7.671579
Action reg: 0.003980
  l1.weight: grad_norm = 0.048895
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.047942
Total gradient norm: 0.223626
=== Actor Training Debug (Iteration 2645) ===
Q mean: -7.742379
Q std: 9.622935
Actor loss: 7.746349
Action reg: 0.003970
  l1.weight: grad_norm = 0.034810
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.032310
Total gradient norm: 0.169900
=== Actor Training Debug (Iteration 2646) ===
Q mean: -8.372110
Q std: 10.422014
Actor loss: 8.376096
Action reg: 0.003985
  l1.weight: grad_norm = 0.026369
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.023434
Total gradient norm: 0.108393
=== Actor Training Debug (Iteration 2647) ===
Q mean: -8.439993
Q std: 11.503537
Actor loss: 8.443983
Action reg: 0.003990
  l1.weight: grad_norm = 0.018988
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.014261
Total gradient norm: 0.061360
=== Actor Training Debug (Iteration 2648) ===
Q mean: -7.939934
Q std: 10.684047
Actor loss: 7.943907
Action reg: 0.003973
  l1.weight: grad_norm = 0.046177
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.036184
Total gradient norm: 0.130698
=== Actor Training Debug (Iteration 2649) ===
Q mean: -7.645404
Q std: 9.487654
Actor loss: 7.649385
Action reg: 0.003982
  l1.weight: grad_norm = 0.046259
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.042181
Total gradient norm: 0.151389
=== Actor Training Debug (Iteration 2650) ===
Q mean: -9.225600
Q std: 11.503412
Actor loss: 9.229578
Action reg: 0.003978
  l1.weight: grad_norm = 0.027346
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.022223
Total gradient norm: 0.075983
=== Actor Training Debug (Iteration 2651) ===
Q mean: -6.251503
Q std: 9.839564
Actor loss: 6.255493
Action reg: 0.003990
  l1.weight: grad_norm = 0.009815
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.009350
Total gradient norm: 0.037543
=== Actor Training Debug (Iteration 2652) ===
Q mean: -6.698509
Q std: 9.954072
Actor loss: 6.702485
Action reg: 0.003977
  l1.weight: grad_norm = 0.044359
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.038373
Total gradient norm: 0.183210
=== Actor Training Debug (Iteration 2653) ===
Q mean: -8.738845
Q std: 10.860007
Actor loss: 8.742818
Action reg: 0.003973
  l1.weight: grad_norm = 0.063792
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.051976
Total gradient norm: 0.175714
=== Actor Training Debug (Iteration 2654) ===
Q mean: -7.561504
Q std: 10.055893
Actor loss: 7.565490
Action reg: 0.003986
  l1.weight: grad_norm = 0.027055
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.020992
Total gradient norm: 0.079535
=== Actor Training Debug (Iteration 2655) ===
Q mean: -7.848129
Q std: 10.491728
Actor loss: 7.852098
Action reg: 0.003969
  l1.weight: grad_norm = 0.123573
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.118527
Total gradient norm: 0.667947
=== Actor Training Debug (Iteration 2656) ===
Q mean: -7.402461
Q std: 9.486277
Actor loss: 7.406440
Action reg: 0.003979
  l1.weight: grad_norm = 0.048731
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.048327
Total gradient norm: 0.262157
=== Actor Training Debug (Iteration 2657) ===
Q mean: -8.058040
Q std: 10.811804
Actor loss: 8.062017
Action reg: 0.003978
  l1.weight: grad_norm = 0.104876
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.074522
Total gradient norm: 0.243430
=== Actor Training Debug (Iteration 2658) ===
Q mean: -7.977021
Q std: 10.530989
Actor loss: 7.980991
Action reg: 0.003970
  l1.weight: grad_norm = 0.050425
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.045884
Total gradient norm: 0.177525
=== Actor Training Debug (Iteration 2659) ===
Q mean: -7.299468
Q std: 9.765988
Actor loss: 7.303446
Action reg: 0.003979
  l1.weight: grad_norm = 0.078784
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.062993
Total gradient norm: 0.222747
=== Actor Training Debug (Iteration 2660) ===
Q mean: -7.531964
Q std: 10.354787
Actor loss: 7.535953
Action reg: 0.003988
  l1.weight: grad_norm = 0.016714
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.014542
Total gradient norm: 0.071688
=== Actor Training Debug (Iteration 2661) ===
Q mean: -7.485421
Q std: 10.145675
Actor loss: 7.489388
Action reg: 0.003967
  l1.weight: grad_norm = 0.065140
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.058154
Total gradient norm: 0.270611
=== Actor Training Debug (Iteration 2662) ===
Q mean: -8.221806
Q std: 10.259278
Actor loss: 8.225783
Action reg: 0.003978
  l1.weight: grad_norm = 0.036278
  l1.bias: grad_norm = 0.000024
  l2.weight: grad_norm = 0.031460
Total gradient norm: 0.149500
=== Actor Training Debug (Iteration 2663) ===
Q mean: -8.074607
Q std: 10.945832
Actor loss: 8.078581
Action reg: 0.003974
  l1.weight: grad_norm = 0.067542
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.067542
Total gradient norm: 0.344775
=== Actor Training Debug (Iteration 2664) ===
Q mean: -7.396236
Q std: 10.239669
Actor loss: 7.400208
Action reg: 0.003972
  l1.weight: grad_norm = 0.034564
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.032917
Total gradient norm: 0.163116
=== Actor Training Debug (Iteration 2665) ===
Q mean: -7.910951
Q std: 9.496061
Actor loss: 7.914928
Action reg: 0.003977
  l1.weight: grad_norm = 0.038037
  l1.bias: grad_norm = 0.000061
Total gradient norm: 0.167790
=== Actor Training Debug (Iteration 2672) ===
Q mean: -8.714437
Q std: 10.615109
Actor loss: 8.718413
Action reg: 0.003975
  l1.weight: grad_norm = 0.057131
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.052873
Total gradient norm: 0.214463
=== Actor Training Debug (Iteration 2673) ===
Q mean: -7.862741
Q std: 9.952948
Actor loss: 7.866715
Action reg: 0.003975
  l1.weight: grad_norm = 0.049695
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.040057
Total gradient norm: 0.150279
=== Actor Training Debug (Iteration 2674) ===
Q mean: -8.981670
Q std: 11.573520
Actor loss: 8.985641
Action reg: 0.003971
  l1.weight: grad_norm = 0.021400
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.018042
Total gradient norm: 0.075413
=== Actor Training Debug (Iteration 2675) ===
Q mean: -8.138579
Q std: 10.755644
Actor loss: 8.142557
Action reg: 0.003978
  l1.weight: grad_norm = 0.044104
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.044234
Total gradient norm: 0.251677
=== Actor Training Debug (Iteration 2676) ===
Q mean: -7.455192
Q std: 9.938181
Actor loss: 7.459174
Action reg: 0.003982
  l1.weight: grad_norm = 0.012962
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.011594
Total gradient norm: 0.040058
=== Actor Training Debug (Iteration 2677) ===
Q mean: -7.276989
Q std: 9.847468
Actor loss: 7.280965
Action reg: 0.003976
  l1.weight: grad_norm = 0.041663
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.042624
Total gradient norm: 0.197631
=== Actor Training Debug (Iteration 2678) ===
Q mean: -7.833898
Q std: 10.583461
Actor loss: 7.837860
Action reg: 0.003961
  l1.weight: grad_norm = 0.052924
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.042077
Total gradient norm: 0.194003
=== Actor Training Debug (Iteration 2679) ===
Q mean: -9.375870
Q std: 11.432049
Actor loss: 9.379856
Action reg: 0.003987
  l1.weight: grad_norm = 0.027064
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.022226
Total gradient norm: 0.118474
=== Actor Training Debug (Iteration 2680) ===
Q mean: -7.851552
Q std: 10.693880
Actor loss: 7.855545
Action reg: 0.003992
  l1.weight: grad_norm = 0.040908
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.037191
Total gradient norm: 0.147117
=== Actor Training Debug (Iteration 2681) ===
Q mean: -8.154719
Q std: 10.826263
Actor loss: 8.158702
Action reg: 0.003982
  l1.weight: grad_norm = 0.059353
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.050144
Total gradient norm: 0.222591
=== Actor Training Debug (Iteration 2682) ===
Q mean: -7.242007
Q std: 10.209638
Actor loss: 7.245987
Action reg: 0.003980
  l1.weight: grad_norm = 0.021998
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.019094
Total gradient norm: 0.090394
=== Actor Training Debug (Iteration 2683) ===
Q mean: -8.369654
Q std: 10.854859
Actor loss: 8.373630
Action reg: 0.003976
  l1.weight: grad_norm = 0.047908
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.043686
Total gradient norm: 0.180850
=== Actor Training Debug (Iteration 2684) ===
Q mean: -8.368923
Q std: 10.701172
Actor loss: 8.372893
Action reg: 0.003970
  l1.weight: grad_norm = 0.048226
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.046442
Total gradient norm: 0.198228
=== Actor Training Debug (Iteration 2685) ===
Q mean: -8.975340
Q std: 10.688581
Actor loss: 8.979304
Action reg: 0.003965
  l1.weight: grad_norm = 0.151720
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.147653
Total gradient norm: 0.608558
=== Actor Training Debug (Iteration 2686) ===
Q mean: -7.696648
Q std: 10.372711
Actor loss: 7.700635
Action reg: 0.003987
  l1.weight: grad_norm = 0.030818
  l1.bias: grad_norm = 0.000011
  l2.weight: grad_norm = 0.030503
Total gradient norm: 0.141764
=== Actor Training Debug (Iteration 2687) ===
Q mean: -8.270057
Q std: 10.981732
Actor loss: 8.274037
Action reg: 0.003981
  l1.weight: grad_norm = 0.020317
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.016797
Total gradient norm: 0.072342
=== Actor Training Debug (Iteration 2688) ===
Q mean: -7.833364
Q std: 10.362968
Actor loss: 7.837353
Action reg: 0.003989
  l1.weight: grad_norm = 0.019782
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.015766
Total gradient norm: 0.065679
=== Actor Training Debug (Iteration 2689) ===
Q mean: -7.197161
Q std: 10.064042
Actor loss: 7.201138
Action reg: 0.003976
  l1.weight: grad_norm = 0.034802
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.027650
Total gradient norm: 0.106720
=== Actor Training Debug (Iteration 2690) ===
Q mean: -7.844176
Q std: 9.851653
Actor loss: 7.848164
Action reg: 0.003987
  l1.weight: grad_norm = 0.017804
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.015424
Total gradient norm: 0.067891
=== Actor Training Debug (Iteration 2691) ===
Q mean: -7.545792
Q std: 10.581106
Actor loss: 7.549773
Action reg: 0.003982
  l1.weight: grad_norm = 0.030745
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.030635
Total gradient norm: 0.153746
=== Actor Training Debug (Iteration 2692) ===
Q mean: -7.128002
Q std: 10.381075
Actor loss: 7.131990
Action reg: 0.003988
  l1.weight: grad_norm = 0.048007
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.040771
Total gradient norm: 0.128092
=== Actor Training Debug (Iteration 2693) ===
Q mean: -9.075487
Q std: 11.250082
Actor loss: 9.079478
Action reg: 0.003991
  l1.weight: grad_norm = 0.012354
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.011783
Total gradient norm: 0.047113
=== Actor Training Debug (Iteration 2694) ===
Q mean: -8.653423
Q std: 10.522852
Actor loss: 8.657407
Action reg: 0.003983
  l1.weight: grad_norm = 0.025509
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.021261
Total gradient norm: 0.109311
=== Actor Training Debug (Iteration 2695) ===
Q mean: -9.278437
Q std: 11.227805
Actor loss: 9.282413
Action reg: 0.003976
  l1.weight: grad_norm = 0.073577
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.060093
Total gradient norm: 0.235029
=== Actor Training Debug (Iteration 2696) ===
Q mean: -9.111238
Q std: 10.906625
Actor loss: 9.115216
Action reg: 0.003978
  l1.weight: grad_norm = 0.112532
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.097632
Total gradient norm: 0.374042
=== Actor Training Debug (Iteration 2697) ===
Q mean: -8.413528
Q std: 10.909575
Actor loss: 8.417513
Action reg: 0.003985
  l1.weight: grad_norm = 0.028739
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.027267
Total gradient norm: 0.121735
=== Actor Training Debug (Iteration 2698) ===
Q mean: -7.526089
Q std: 9.745817
Actor loss: 7.530062
Action reg: 0.003973
  l1.weight: grad_norm = 0.080001
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.053021
Total gradient norm: 0.216410
=== Actor Training Debug (Iteration 2699) ===
Q mean: -8.694893
Q std: 10.723286
Actor loss: 8.698874
Action reg: 0.003982
  l1.weight: grad_norm = 0.023632
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.021385
Total gradient norm: 0.081778
=== Actor Training Debug (Iteration 2700) ===
Q mean: -8.396245
Q std: 10.734754
Actor loss: 8.400231
Action reg: 0.003986
  l1.weight: grad_norm = 0.052175
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.044845
Total gradient norm: 0.165829
=== Actor Training Debug (Iteration 2701) ===
Q mean: -7.186989
Q std: 10.435991
Actor loss: 7.190977
Action reg: 0.003987
  l1.weight: grad_norm = 0.049232
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.038965
Total gradient norm: 0.132569
=== Actor Training Debug (Iteration 2702) ===
Q mean: -7.399968
Q std: 10.612558
Actor loss: 7.403947
Action reg: 0.003979
  l1.weight: grad_norm = 0.063215
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.052240
Total gradient norm: 0.186151
=== Actor Training Debug (Iteration 2703) ===
Q mean: -9.008705
Q std: 11.443118
Actor loss: 9.012692
Action reg: 0.003988
  l1.weight: grad_norm = 0.027238
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.026434
Total gradient norm: 0.113150
=== Actor Training Debug (Iteration 2704) ===
Q mean: -8.019219
Q std: 10.861850
Actor loss: 8.023186
Action reg: 0.003967
  l1.weight: grad_norm = 0.059640
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.053905
Total gradient norm: 0.212840
=== Actor Training Debug (Iteration 2705) ===
Q mean: -8.200041
Q std: 11.032110
Actor loss: 8.204024
Action reg: 0.003983
  l1.weight: grad_norm = 0.021711
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.019759
Total gradient norm: 0.093262
=== Actor Training Debug (Iteration 2706) ===
Q mean: -8.533947
Q std: 11.283481
Actor loss: 8.537920
Action reg: 0.003973
  l1.weight: grad_norm = 0.033604
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.033002
Total gradient norm: 0.157546
=== Actor Training Debug (Iteration 2707) ===
Q mean: -8.686171
Q std: 10.986560
Actor loss: 8.690150
Action reg: 0.003979
  l1.weight: grad_norm = 0.129651
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.110710
Total gradient norm: 0.389311
=== Actor Training Debug (Iteration 2708) ===
Q mean: -7.990009
Q std: 10.511141
Actor loss: 7.993995
Action reg: 0.003985
  l1.weight: grad_norm = 0.037120
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.037896
Total gradient norm: 0.169246
=== Actor Training Debug (Iteration 2709) ===
Q mean: -8.575209
Q std: 10.820992
Actor loss: 8.579191
Action reg: 0.003983
  l1.weight: grad_norm = 0.020445
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.017223
Total gradient norm: 0.066224
=== Actor Training Debug (Iteration 2710) ===
Q mean: -7.635437
Q std: 10.095488
Actor loss: 7.639414
Action reg: 0.003976
  l1.weight: grad_norm = 0.035369
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.027217
Total gradient norm: 0.105423
=== Actor Training Debug (Iteration 2711) ===
Q mean: -9.055302
Q std: 11.516863
Actor loss: 9.059281
Action reg: 0.003980
  l1.weight: grad_norm = 0.035306
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.028490
Total gradient norm: 0.104758
=== Actor Training Debug (Iteration 2712) ===
Q mean: -8.834909
Q std: 10.536868
Actor loss: 8.838883
Action reg: 0.003974
  l1.weight: grad_norm = 0.039972
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.038546
Total gradient norm: 0.183595
=== Actor Training Debug (Iteration 2713) ===
Q mean: -9.011308
Q std: 11.170290
Actor loss: 9.015299
Action reg: 0.003991
  l1.weight: grad_norm = 0.029707
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.028070
Total gradient norm: 0.124707
=== Actor Training Debug (Iteration 2714) ===
Q mean: -8.050209
Q std: 10.137660
Actor loss: 8.054195
Action reg: 0.003986
  l1.weight: grad_norm = 0.077684
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.056934
Total gradient norm: 0.281730
=== Actor Training Debug (Iteration 2715) ===
Q mean: -7.282325
Q std: 10.634264
Actor loss: 7.286304
Action reg: 0.003978
  l1.weight: grad_norm = 0.035976
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.032006
Total gradient norm: 0.126728
=== Actor Training Debug (Iteration 2716) ===
Q mean: -7.652726
Q std: 10.273817
Actor loss: 7.656704
Action reg: 0.003978
  l1.weight: grad_norm = 0.063113
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.051981
Total gradient norm: 0.175769
=== Actor Training Debug (Iteration 2717) ===
Q mean: -8.424965
Q std: 10.965514
Actor loss: 8.428945
Action reg: 0.003980
  l1.weight: grad_norm = 0.072406
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.057404
Total gradient norm: 0.173724
=== Actor Training Debug (Iteration 2718) ===
Q mean: -8.338805
Q std: 10.354206
Actor loss: 8.342777
Action reg: 0.003972
  l1.weight: grad_norm = 0.042385
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.040413
Total gradient norm: 0.203478
=== Actor Training Debug (Iteration 2719) ===
Q mean: -8.920620
Q std: 11.180321
Actor loss: 8.924600
Action reg: 0.003980
  l1.weight: grad_norm = 0.041070
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.036311
Total gradient norm: 0.146347
=== Actor Training Debug (Iteration 2720) ===
Q mean: -8.628553
Q std: 10.786256
Actor loss: 8.632535
Action reg: 0.003981
  l1.weight: grad_norm = 0.041113
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.032411
Total gradient norm: 0.107064
=== Actor Training Debug (Iteration 2721) ===
Q mean: -7.795667
Q std: 10.901681
Actor loss: 7.799644
Action reg: 0.003977
  l1.weight: grad_norm = 0.037385
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.034594
Total gradient norm: 0.152890
=== Actor Training Debug (Iteration 2722) ===
Q mean: -7.919178
Q std: 9.968655
Actor loss: 7.923145
Action reg: 0.003967
  l1.weight: grad_norm = 0.053342
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.045701
Total gradient norm: 0.147866
=== Actor Training Debug (Iteration 2723) ===
Q mean: -8.027660
Q std: 10.260823
Actor loss: 8.031636
Action reg: 0.003976
  l1.weight: grad_norm = 0.079512
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.069792
Total gradient norm: 0.261252
=== Actor Training Debug (Iteration 2724) ===
Q mean: -8.871794
Q std: 11.338196
Actor loss: 8.875769
Action reg: 0.003975
  l1.weight: grad_norm = 0.070457
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.065861
Total gradient norm: 0.275359
=== Actor Training Debug (Iteration 2725) ===
Q mean: -7.571207
Q std: 10.049159
Actor loss: 7.575183
Action reg: 0.003976
  l1.weight: grad_norm = 0.029819
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.030038
Total gradient norm: 0.165313
=== Actor Training Debug (Iteration 2726) ===
Q mean: -7.526265
Q std: 9.893183
Actor loss: 7.530225
Action reg: 0.003959
  l1.weight: grad_norm = 0.073130
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.068164
Total gradient norm: 0.268242
=== Actor Training Debug (Iteration 2727) ===
Q mean: -8.953905
Q std: 10.567359
Actor loss: 8.957884
Action reg: 0.003978
  l1.weight: grad_norm = 0.019330
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.016226
Total gradient norm: 0.078028
=== Actor Training Debug (Iteration 2728) ===
Q mean: -7.139900
Q std: 10.321760
Actor loss: 7.143871
Action reg: 0.003971
  l1.weight: grad_norm = 0.045962
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.039622
Total gradient norm: 0.147716
=== Actor Training Debug (Iteration 2729) ===
Q mean: -7.080087
Q std: 9.804110
Actor loss: 7.084071
Action reg: 0.003984
  l1.weight: grad_norm = 0.076054
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.057602
Total gradient norm: 0.254924
=== Actor Training Debug (Iteration 2730) ===
Q mean: -8.240675
Q std: 10.980376
Actor loss: 8.244662
Action reg: 0.003988
  l1.weight: grad_norm = 0.042960
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.037435
Total gradient norm: 0.186027
=== Actor Training Debug (Iteration 2731) ===
Q mean: -6.726604
Q std: 9.875391
Actor loss: 6.730578
Action reg: 0.003973
  l1.weight: grad_norm = 0.062090
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.057769
Total gradient norm: 0.253838
=== Actor Training Debug (Iteration 2732) ===
Q mean: -7.708660
Q std: 10.945218
Actor loss: 7.712623
Action reg: 0.003963
  l1.weight: grad_norm = 0.056617
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.044735
Total gradient norm: 0.181018
=== Actor Training Debug (Iteration 2733) ===
Q mean: -8.343909
Q std: 11.045628
Actor loss: 8.347884
Action reg: 0.003975
  l1.weight: grad_norm = 0.044464
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.036394
Total gradient norm: 0.121776
=== Actor Training Debug (Iteration 2734) ===
Q mean: -8.182844
Q std: 10.372401
Actor loss: 8.186824
Action reg: 0.003979
  l1.weight: grad_norm = 0.049075
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.046716
Total gradient norm: 0.229440
=== Actor Training Debug (Iteration 2735) ===
Q mean: -7.063732
Q std: 9.837469
Actor loss: 7.067709
Action reg: 0.003977
  l1.weight: grad_norm = 0.098800
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.093821
Total gradient norm: 0.312752
=== Actor Training Debug (Iteration 2736) ===
Q mean: -8.216974
Q std: 11.051520
Actor loss: 8.220949
Action reg: 0.003974
  l1.weight: grad_norm = 0.027507
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.026465
Total gradient norm: 0.128233
=== Actor Training Debug (Iteration 2737) ===
Q mean: -8.584793
Q std: 10.784350
Actor loss: 8.588781
Action reg: 0.003989
  l1.weight: grad_norm = 0.025956
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.023563
Total gradient norm: 0.088157
=== Actor Training Debug (Iteration 2738) ===
Q mean: -8.653451
Q std: 10.765252
Actor loss: 8.657438
Action reg: 0.003988
  l1.weight: grad_norm = 0.089443
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.071674
Total gradient norm: 0.235266
=== Actor Training Debug (Iteration 2739) ===
Q mean: -8.620384
Q std: 11.263453
Actor loss: 8.624359
Action reg: 0.003975
  l1.weight: grad_norm = 0.086319
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.073655
Total gradient norm: 0.234298
=== Actor Training Debug (Iteration 2740) ===
Q mean: -7.423971
Q std: 11.052321
Actor loss: 7.427959
Action reg: 0.003988
  l1.weight: grad_norm = 0.053077
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.046669
Total gradient norm: 0.197636
=== Actor Training Debug (Iteration 2741) ===
Q mean: -7.939765
Q std: 10.843667
Actor loss: 7.943740
Action reg: 0.003975
  l1.weight: grad_norm = 0.054371
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.045010
Total gradient norm: 0.214070
=== Actor Training Debug (Iteration 2742) ===
Q mean: -7.855926
Q std: 10.425187
Actor loss: 7.859907
Action reg: 0.003981
  l1.weight: grad_norm = 0.030614
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.025496
Total gradient norm: 0.090376
=== Actor Training Debug (Iteration 2743) ===
Q mean: -9.322906
Q std: 11.441212
Actor loss: 9.326899
Action reg: 0.003992
  l1.weight: grad_norm = 0.043692
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.040670
Total gradient norm: 0.170992
=== Actor Training Debug (Iteration 2744) ===
Q mean: -8.430357
Q std: 10.555505
Actor loss: 8.434342
Action reg: 0.003985
  l1.weight: grad_norm = 0.049207
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.047115
Total gradient norm: 0.216010
=== Actor Training Debug (Iteration 2745) ===
Q mean: -8.251835
Q std: 11.207618
Actor loss: 8.255816
Action reg: 0.003982
  l1.weight: grad_norm = 0.106176
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.100494
Total gradient norm: 0.311904
=== Actor Training Debug (Iteration 2746) ===
Q mean: -8.661574
Q std: 11.019932
Actor loss: 8.665548
Action reg: 0.003974
  l1.weight: grad_norm = 0.034379
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.032833
Total gradient norm: 0.125229
=== Actor Training Debug (Iteration 2747) ===
Q mean: -8.131104
Q std: 10.147798
Actor loss: 8.135085
Action reg: 0.003981
  l1.weight: grad_norm = 0.066860
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.062307
Total gradient norm: 0.304102
=== Actor Training Debug (Iteration 2748) ===
Q mean: -8.896139
Q std: 10.895212
Actor loss: 8.900118
Action reg: 0.003979
  l1.weight: grad_norm = 0.060806
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.049879
Total gradient norm: 0.244420
=== Actor Training Debug (Iteration 2749) ===
Q mean: -8.110970
Q std: 10.323629
Actor loss: 8.114944
Action reg: 0.003975
  l1.weight: grad_norm = 0.080584
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.072837
Total gradient norm: 0.230965
=== Actor Training Debug (Iteration 2750) ===
Q mean: -9.150286
Q std: 11.183607
Actor loss: 9.154262
Action reg: 0.003976
  l1.weight: grad_norm = 0.026554
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.023592
Total gradient norm: 0.093791
=== Actor Training Debug (Iteration 2751) ===
Q mean: -8.210497
Q std: 10.918585
Actor loss: 8.214471
Action reg: 0.003974
  l1.weight: grad_norm = 0.042427
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.035508
Total gradient norm: 0.133956
=== Actor Training Debug (Iteration 2752) ===
Q mean: -7.639372
Q std: 9.787129
Actor loss: 7.643340
Action reg: 0.003968
  l1.weight: grad_norm = 0.022900
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.022954
Total gradient norm: 0.107973
=== Actor Training Debug (Iteration 2753) ===
Q mean: -8.933454
Q std: 11.014898
Actor loss: 8.937427
Action reg: 0.003973
  l1.weight: grad_norm = 0.038420
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.037440
Total gradient norm: 0.191799
=== Actor Training Debug (Iteration 2754) ===
Q mean: -8.613615
Q std: 11.465822
Actor loss: 8.617587
Action reg: 0.003973
  l1.weight: grad_norm = 0.065988
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.059403
Total gradient norm: 0.293379
=== Actor Training Debug (Iteration 2755) ===
Q mean: -8.800447
Q std: 10.843800
Actor loss: 8.804418
Action reg: 0.003971
  l1.weight: grad_norm = 0.086610
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.084587
Total gradient norm: 0.366933
=== Actor Training Debug (Iteration 2756) ===
Q mean: -9.699886
Q std: 11.654400
Actor loss: 9.703871
Action reg: 0.003985
  l1.weight: grad_norm = 0.018653
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.016210
Total gradient norm: 0.062305
=== Actor Training Debug (Iteration 2757) ===
Q mean: -7.826327
Q std: 10.486850
Actor loss: 7.830309
Action reg: 0.003983
  l1.weight: grad_norm = 0.037182
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.030825
Total gradient norm: 0.116329
=== Actor Training Debug (Iteration 2758) ===
Q mean: -8.613237
Q std: 11.184364
Actor loss: 8.617211
Action reg: 0.003974
  l1.weight: grad_norm = 0.034744
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.031805
Total gradient norm: 0.126289
=== Actor Training Debug (Iteration 2759) ===
Q mean: -6.583343
Q std: 10.367766
Actor loss: 6.587321
Action reg: 0.003978
  l1.weight: grad_norm = 0.040909
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.030068
Total gradient norm: 0.111977
=== Actor Training Debug (Iteration 2760) ===
Q mean: -7.961707
Q std: 10.470525
Actor loss: 7.965673
Action reg: 0.003966
  l1.weight: grad_norm = 0.057741
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.050373
Total gradient norm: 0.166914
=== Actor Training Debug (Iteration 2761) ===
Q mean: -9.116685
Q std: 10.766540
Actor loss: 9.120663
Action reg: 0.003978
  l1.weight: grad_norm = 0.058169
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.053269
Total gradient norm: 0.242906
=== Actor Training Debug (Iteration 2762) ===
Q mean: -8.930202
Q std: 11.867799
Actor loss: 8.934184
Action reg: 0.003982
  l1.weight: grad_norm = 0.040598
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.037311
Total gradient norm: 0.116776
=== Actor Training Debug (Iteration 2763) ===
Q mean: -8.450028
Q std: 11.281078
Actor loss: 8.454007
Action reg: 0.003979
  l1.weight: grad_norm = 0.054519
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.046224
Total gradient norm: 0.178468
=== Actor Training Debug (Iteration 2764) ===
Q mean: -7.654100
Q std: 9.860661
Actor loss: 7.658081
Action reg: 0.003980
  l1.weight: grad_norm = 0.030785
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.024833
Total gradient norm: 0.098211
=== Actor Training Debug (Iteration 2765) ===
Q mean: -7.809092
Q std: 10.344762
Actor loss: 7.813066
Action reg: 0.003975
  l1.weight: grad_norm = 0.044439
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.035007
Total gradient norm: 0.135523
=== Actor Training Debug (Iteration 2766) ===
Q mean: -7.878213
Q std: 10.043290
Actor loss: 7.882195
Action reg: 0.003982
  l1.weight: grad_norm = 0.036162
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.032028
Total gradient norm: 0.116952
=== Actor Training Debug (Iteration 2767) ===
Q mean: -8.759535
Q std: 10.426853
Actor loss: 8.763514
Action reg: 0.003978
  l1.weight: grad_norm = 0.071963
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.056756
Total gradient norm: 0.207551
=== Actor Training Debug (Iteration 2768) ===
Q mean: -8.661049
Q std: 11.284669
Actor loss: 8.665029
Action reg: 0.003980
  l1.weight: grad_norm = 0.058387
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.057221
Total gradient norm: 0.228932
=== Actor Training Debug (Iteration 2769) ===
Q mean: -8.316011
Q std: 10.790059
Actor loss: 8.319995
Action reg: 0.003984
  l1.weight: grad_norm = 0.056065
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.048668
Total gradient norm: 0.170723
=== Actor Training Debug (Iteration 2770) ===
Q mean: -8.771494
Q std: 11.710457
Actor loss: 8.775467
Action reg: 0.003973
  l1.weight: grad_norm = 0.078922
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.073210
Total gradient norm: 0.309434
=== Actor Training Debug (Iteration 2771) ===
Q mean: -8.730890
Q std: 11.735639
Actor loss: 8.734881
Action reg: 0.003991
  l1.weight: grad_norm = 0.020610
  l1.bias: grad_norm = 0.000007
  l2.weight: grad_norm = 0.018891
Total gradient norm: 0.064439
=== Actor Training Debug (Iteration 2772) ===
Q mean: -8.634882
Q std: 11.232148
Actor loss: 8.638872
Action reg: 0.003990
  l1.weight: grad_norm = 0.015519
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.014584
Total gradient norm: 0.065728
=== Actor Training Debug (Iteration 2773) ===
Q mean: -8.414773
Q std: 11.204848
Actor loss: 8.418759
Action reg: 0.003986
  l1.weight: grad_norm = 0.014379
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.014204
Total gradient norm: 0.070788
=== Actor Training Debug (Iteration 2774) ===
Q mean: -8.214407
Q std: 10.595343
Actor loss: 8.218388
Action reg: 0.003981
  l1.weight: grad_norm = 0.061629
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.041333
Total gradient norm: 0.150294
=== Actor Training Debug (Iteration 2775) ===
Q mean: -8.413118
Q std: 10.220313
Actor loss: 8.417104
Action reg: 0.003985
  l1.weight: grad_norm = 0.027798
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.026106
Total gradient norm: 0.110066
=== Actor Training Debug (Iteration 2776) ===
Q mean: -9.378483
Q std: 12.228435
Actor loss: 9.382458
Action reg: 0.003975
  l1.weight: grad_norm = 0.029082
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.027492
Total gradient norm: 0.102701
=== Actor Training Debug (Iteration 2777) ===
Q mean: -7.383662
Q std: 11.107933
Actor loss: 7.387645
Action reg: 0.003983
  l1.weight: grad_norm = 0.011054
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.010300
Total gradient norm: 0.040146
=== Actor Training Debug (Iteration 2778) ===
Q mean: -7.666943
Q std: 10.479690
Actor loss: 7.670907
Action reg: 0.003964
  l1.weight: grad_norm = 0.195676
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.161843
Total gradient norm: 0.487288
=== Actor Training Debug (Iteration 2779) ===
Q mean: -9.057413
Q std: 11.208518
Actor loss: 9.061401
Action reg: 0.003988
  l1.weight: grad_norm = 0.057630
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.052652
Total gradient norm: 0.236841
=== Actor Training Debug (Iteration 2780) ===
Q mean: -9.852648
Q std: 11.867856
Actor loss: 9.856630
Action reg: 0.003982
  l1.weight: grad_norm = 0.034034
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.029452
Total gradient norm: 0.137040
=== Actor Training Debug (Iteration 2781) ===
Q mean: -7.869123
Q std: 10.472741
Actor loss: 7.873108
Action reg: 0.003985
  l1.weight: grad_norm = 0.031647
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.029312
Total gradient norm: 0.089617
=== Actor Training Debug (Iteration 2782) ===
Q mean: -7.904118
Q std: 10.754092
Actor loss: 7.908091
Action reg: 0.003974
  l1.weight: grad_norm = 0.049930
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.036002
Total gradient norm: 0.152116
=== Actor Training Debug (Iteration 2783) ===
Q mean: -7.509951
Q std: 10.031203
Actor loss: 7.513937
Action reg: 0.003986
  l1.weight: grad_norm = 0.042006
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.036016
Total gradient norm: 0.137998
=== Actor Training Debug (Iteration 2784) ===
Q mean: -8.716647
Q std: 10.558518
Actor loss: 8.720623
Action reg: 0.003976
  l1.weight: grad_norm = 0.041488
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.041228
Total gradient norm: 0.188679
=== Actor Training Debug (Iteration 2785) ===
Q mean: -8.524345
Q std: 10.672133
Actor loss: 8.528332
Action reg: 0.003987
  l1.weight: grad_norm = 0.032812
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.026868
Total gradient norm: 0.100720
=== Actor Training Debug (Iteration 2786) ===
Q mean: -7.757758
Q std: 10.559566
Actor loss: 7.761737
Action reg: 0.003979
  l1.weight: grad_norm = 0.034651
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.036510
Total gradient norm: 0.166133
=== Actor Training Debug (Iteration 2787) ===
Q mean: -8.552034
Q std: 11.597422
Actor loss: 8.556014
Action reg: 0.003979
  l1.weight: grad_norm = 0.136609
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.114755
Total gradient norm: 0.419325
=== Actor Training Debug (Iteration 2788) ===
Q mean: -8.822178
Q std: 10.475929
Actor loss: 8.826151
Action reg: 0.003973
  l1.weight: grad_norm = 0.068681
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.059431
Total gradient norm: 0.222160
=== Actor Training Debug (Iteration 2789) ===
Q mean: -8.947248
Q std: 11.327745
Actor loss: 8.951230
Action reg: 0.003981
  l1.weight: grad_norm = 0.036923
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.028747
Total gradient norm: 0.096124
=== Actor Training Debug (Iteration 2790) ===
Q mean: -7.657257
Q std: 10.227757
Actor loss: 7.661231
Action reg: 0.003974
  l1.weight: grad_norm = 0.051378
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.043474
Total gradient norm: 0.149900
=== Actor Training Debug (Iteration 2791) ===
Q mean: -6.306006
Q std: 10.142856
Actor loss: 6.309968
Action reg: 0.003962
  l1.weight: grad_norm = 0.088087
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.081417
Total gradient norm: 0.312177
=== Actor Training Debug (Iteration 2792) ===
Q mean: -8.024066
Q std: 10.494050
Actor loss: 8.028056
Action reg: 0.003990
  l1.weight: grad_norm = 0.035230
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.029939
Total gradient norm: 0.147836
=== Actor Training Debug (Iteration 2793) ===
Q mean: -8.496456
Q std: 10.794591
Actor loss: 8.500424
Action reg: 0.003968
  l1.weight: grad_norm = 0.044456
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.043021
Total gradient norm: 0.154147
=== Actor Training Debug (Iteration 2794) ===
Q mean: -9.014946
Q std: 11.149970
Actor loss: 9.018920
Action reg: 0.003974
  l1.weight: grad_norm = 0.055828
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.040068
Total gradient norm: 0.119015
=== Actor Training Debug (Iteration 2795) ===
Q mean: -8.019487
Q std: 10.406039
Actor loss: 8.023448
Action reg: 0.003960
  l1.weight: grad_norm = 0.081188
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.077722
Total gradient norm: 0.355878
=== Actor Training Debug (Iteration 2796) ===
Q mean: -8.164759
Q std: 10.855043
Actor loss: 8.168727
Action reg: 0.003969
  l1.weight: grad_norm = 0.051834
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.045490
Total gradient norm: 0.226395
=== Actor Training Debug (Iteration 2797) ===
Q mean: -7.701528
Q std: 10.484100
Actor loss: 7.705510
Action reg: 0.003982
  l1.weight: grad_norm = 0.050824
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.044788
Total gradient norm: 0.151902
=== Actor Training Debug (Iteration 2798) ===
Q mean: -7.583307
Q std: 9.983481
Actor loss: 7.587290
Action reg: 0.003982
  l1.weight: grad_norm = 0.018111
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.015773
Total gradient norm: 0.062055
=== Actor Training Debug (Iteration 2799) ===
Q mean: -8.129987
Q std: 10.571748
Actor loss: 8.133972
Action reg: 0.003985
  l1.weight: grad_norm = 0.024898
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.022295
Total gradient norm: 0.080641
=== Actor Training Debug (Iteration 2800) ===
Q mean: -8.743196
Q std: 10.032909
Actor loss: 8.747175
Action reg: 0.003979
  l1.weight: grad_norm = 0.050523
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.047927
Total gradient norm: 0.206314
=== Actor Training Debug (Iteration 2801) ===
Q mean: -7.371942
Q std: 10.369602
Actor loss: 7.375923
Action reg: 0.003981
  l1.weight: grad_norm = 0.041993
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.034476
Total gradient norm: 0.132682
=== Actor Training Debug (Iteration 2802) ===
Q mean: -9.027441
Q std: 11.834966
Actor loss: 9.031425
Action reg: 0.003984
  l1.weight: grad_norm = 0.039331
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.036477
Total gradient norm: 0.155847
=== Actor Training Debug (Iteration 2803) ===
Q mean: -8.455899
Q std: 11.303352
Actor loss: 8.459878
Action reg: 0.003979
  l1.weight: grad_norm = 0.044750
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.042170
Total gradient norm: 0.181406
=== Actor Training Debug (Iteration 2804) ===
Q mean: -8.594051
Q std: 10.997499
Actor loss: 8.598025
Action reg: 0.003974
  l1.weight: grad_norm = 0.033456
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.031522
Total gradient norm: 0.131332
=== Actor Training Debug (Iteration 2805) ===
Q mean: -8.001395
Q std: 9.531600
Actor loss: 8.005383
Action reg: 0.003988
  l1.weight: grad_norm = 0.035385
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.030972
Total gradient norm: 0.141430
=== Actor Training Debug (Iteration 2806) ===
Q mean: -9.125954
Q std: 11.313218
Actor loss: 9.129936
Action reg: 0.003982
  l1.weight: grad_norm = 0.051146
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.044592
Total gradient norm: 0.183559
=== Actor Training Debug (Iteration 2807) ===
Q mean: -8.330191
Q std: 10.638309
Actor loss: 8.334169
Action reg: 0.003978
  l1.weight: grad_norm = 0.047724
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.042114
Total gradient norm: 0.142506
=== Actor Training Debug (Iteration 2808) ===
Q mean: -8.260558
Q std: 10.522646
Actor loss: 8.264533
Action reg: 0.003975
  l1.weight: grad_norm = 0.039181
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.029830
Total gradient norm: 0.115959
=== Actor Training Debug (Iteration 2809) ===
Q mean: -8.641358
Q std: 10.950541
Actor loss: 8.645352
Action reg: 0.003994
  l1.weight: grad_norm = 0.016753
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.016834
Total gradient norm: 0.074190
=== Actor Training Debug (Iteration 2810) ===
Q mean: -8.385536
Q std: 10.646568
Actor loss: 8.389521
Action reg: 0.003984
  l1.weight: grad_norm = 0.057064
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.051798
Total gradient norm: 0.230459
=== Actor Training Debug (Iteration 2811) ===
Q mean: -8.510950
Q std: 11.786295
Actor loss: 8.514919
Action reg: 0.003969
  l1.weight: grad_norm = 0.038170
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.032123
Total gradient norm: 0.123890
=== Actor Training Debug (Iteration 2812) ===
Q mean: -8.382028
Q std: 10.948202
Actor loss: 8.386016
Action reg: 0.003989
  l1.weight: grad_norm = 0.018165
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.016836
Total gradient norm: 0.061372
=== Actor Training Debug (Iteration 2813) ===
Q mean: -8.377253
Q std: 10.547653
Actor loss: 8.381227
Action reg: 0.003974
  l1.weight: grad_norm = 0.042987
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.034050
Total gradient norm: 0.122147
=== Actor Training Debug (Iteration 2814) ===
Q mean: -10.276013
Q std: 12.130167
Actor loss: 10.279995
Action reg: 0.003981
  l1.weight: grad_norm = 0.016342
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.014332
Total gradient norm: 0.073653
=== Actor Training Debug (Iteration 2815) ===
Q mean: -8.558803
Q std: 10.997996
Actor loss: 8.562787
Action reg: 0.003984
  l1.weight: grad_norm = 0.058966
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.046900
Total gradient norm: 0.184879
=== Actor Training Debug (Iteration 2816) ===
Q mean: -8.079298
Q std: 10.317131
Actor loss: 8.083285
Action reg: 0.003988
  l1.weight: grad_norm = 0.028645
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.028604
Total gradient norm: 0.140214
=== Actor Training Debug (Iteration 2817) ===
Q mean: -8.194710
Q std: 10.772318
Actor loss: 8.198689
Action reg: 0.003979
  l1.weight: grad_norm = 0.055347
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.048175
Total gradient norm: 0.190614
=== Actor Training Debug (Iteration 2818) ===
Q mean: -9.765859
Q std: 11.788786
Actor loss: 9.769830
Action reg: 0.003971
  l1.weight: grad_norm = 0.039207
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.033300
Total gradient norm: 0.099039
=== Actor Training Debug (Iteration 2819) ===
Q mean: -9.374208
Q std: 11.842869
Actor loss: 9.378202
Action reg: 0.003994
  l1.weight: grad_norm = 0.040240
  l1.bias: grad_norm = 0.000016
  l2.weight: grad_norm = 0.038494
Total gradient norm: 0.144090
=== Actor Training Debug (Iteration 2820) ===
Q mean: -6.869842
Q std: 9.885441
Actor loss: 6.873816
Action reg: 0.003975
  l1.weight: grad_norm = 0.047412
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.041801
Total gradient norm: 0.153020
=== Actor Training Debug (Iteration 2821) ===
Q mean: -8.917208
Q std: 11.445687
Actor loss: 8.921187
Action reg: 0.003979
  l1.weight: grad_norm = 0.061394
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.053181
Total gradient norm: 0.211512
=== Actor Training Debug (Iteration 2822) ===
Q mean: -8.607445
Q std: 11.201547
Actor loss: 8.611424
Action reg: 0.003979
  l1.weight: grad_norm = 0.042917
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.036570
Total gradient norm: 0.108474
=== Actor Training Debug (Iteration 2823) ===
Q mean: -8.181423
Q std: 11.560659
Actor loss: 8.185409
Action reg: 0.003985
  l1.weight: grad_norm = 0.037923
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.031757
Total gradient norm: 0.118458
=== Actor Training Debug (Iteration 2824) ===
Q mean: -7.633086
Q std: 9.651571
Actor loss: 7.637067
Action reg: 0.003981
  l1.weight: grad_norm = 0.061186
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.052752
Total gradient norm: 0.225552
=== Actor Training Debug (Iteration 2825) ===
Q mean: -8.233274
Q std: 10.423810
Actor loss: 8.237245
Action reg: 0.003972
  l1.weight: grad_norm = 0.023650
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.022666
Total gradient norm: 0.098974
=== Actor Training Debug (Iteration 2826) ===
Q mean: -9.385981
Q std: 11.955875
Actor loss: 9.389962
Action reg: 0.003982
  l1.weight: grad_norm = 0.019530
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.015222
Total gradient norm: 0.062202
=== Actor Training Debug (Iteration 2827) ===
Q mean: -8.113022
Q std: 10.767985
Actor loss: 8.116999
Action reg: 0.003977
  l1.weight: grad_norm = 0.082890
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.063725
Total gradient norm: 0.211288
=== Actor Training Debug (Iteration 2828) ===
Q mean: -10.582882
Q std: 11.645957
Actor loss: 10.586867
Action reg: 0.003986
  l1.weight: grad_norm = 0.072652
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.066776
Total gradient norm: 0.224137
=== Actor Training Debug (Iteration 2829) ===
Q mean: -7.821152
Q std: 11.353712
Actor loss: 7.825133
Action reg: 0.003981
  l1.weight: grad_norm = 0.059354
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.049623
Total gradient norm: 0.215478
=== Actor Training Debug (Iteration 2830) ===
Q mean: -6.956442
Q std: 9.799644
Actor loss: 6.960413
Action reg: 0.003971
  l1.weight: grad_norm = 0.028524
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.028180
Total gradient norm: 0.112111
=== Actor Training Debug (Iteration 2831) ===
Q mean: -8.536623
Q std: 10.617976
Actor loss: 8.540604
Action reg: 0.003981
  l1.weight: grad_norm = 0.039127
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.040237
Total gradient norm: 0.190473
=== Actor Training Debug (Iteration 2832) ===
Q mean: -6.995477
Q std: 10.484962
Actor loss: 6.999452
Action reg: 0.003975
  l1.weight: grad_norm = 0.052032
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.049392
Total gradient norm: 0.164434
=== Actor Training Debug (Iteration 2833) ===
Q mean: -8.105143
Q std: 10.713070
Actor loss: 8.109129
Action reg: 0.003986
  l1.weight: grad_norm = 0.067333
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.052716
Total gradient norm: 0.151013
=== Actor Training Debug (Iteration 2834) ===
Q mean: -7.464317
Q std: 10.977518
Actor loss: 7.468299
Action reg: 0.003982
  l1.weight: grad_norm = 0.037748
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.037108
Total gradient norm: 0.127408
=== Actor Training Debug (Iteration 2835) ===
Q mean: -9.018330
Q std: 11.742645
Actor loss: 9.022311
Action reg: 0.003981
  l1.weight: grad_norm = 0.064231
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.056618
Total gradient norm: 0.163009
=== Actor Training Debug (Iteration 2836) ===
Q mean: -8.717587
Q std: 11.309503
Actor loss: 8.721568
Action reg: 0.003982
  l1.weight: grad_norm = 0.033594
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.032043
Total gradient norm: 0.142632
=== Actor Training Debug (Iteration 2837) ===
Q mean: -9.474134
Q std: 11.531186
Actor loss: 9.478101
Action reg: 0.003966
  l1.weight: grad_norm = 0.066629
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.044612
Total gradient norm: 0.157424
=== Actor Training Debug (Iteration 2838) ===
Q mean: -10.169760
Q std: 12.590402
Actor loss: 10.173731
Action reg: 0.003971
  l1.weight: grad_norm = 0.048790
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.040148
Total gradient norm: 0.156601
=== Actor Training Debug (Iteration 2839) ===
Q mean: -9.774721
Q std: 12.322018
Actor loss: 9.778696
Action reg: 0.003975
  l1.weight: grad_norm = 0.060142
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.052564
Total gradient norm: 0.218486
=== Actor Training Debug (Iteration 2840) ===
Q mean: -8.895607
Q std: 11.082808
Actor loss: 8.899589
Action reg: 0.003982
  l1.weight: grad_norm = 0.093944
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.070346
Total gradient norm: 0.234810
=== Actor Training Debug (Iteration 2841) ===
Q mean: -8.103008
Q std: 9.898574
Actor loss: 8.106990
Action reg: 0.003982
  l1.weight: grad_norm = 0.053747
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.045506
Total gradient norm: 0.170729
=== Actor Training Debug (Iteration 2842) ===
Q mean: -8.481799
Q std: 11.063138
Actor loss: 8.485768
Action reg: 0.003969
  l1.weight: grad_norm = 0.065118
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.056289
Total gradient norm: 0.229274
=== Actor Training Debug (Iteration 2843) ===
Q mean: -8.392708
Q std: 10.644033
Actor loss: 8.396697
Action reg: 0.003989
  l1.weight: grad_norm = 0.027010
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.028203
Total gradient norm: 0.119899
=== Actor Training Debug (Iteration 2844) ===
Q mean: -7.670182
Q std: 10.646900
Actor loss: 7.674169
Action reg: 0.003987
  l1.weight: grad_norm = 0.082803
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.076430
Total gradient norm: 0.274956
=== Actor Training Debug (Iteration 2845) ===
Q mean: -7.966953
Q std: 10.229310
Actor loss: 7.970931
Action reg: 0.003978
  l1.weight: grad_norm = 0.060404
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.055171
Total gradient norm: 0.188005
=== Actor Training Debug (Iteration 2846) ===
Q mean: -8.333990
Q std: 10.809718
Actor loss: 8.337949
Action reg: 0.003959
  l1.weight: grad_norm = 0.046153
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.043892
Total gradient norm: 0.162998
=== Actor Training Debug (Iteration 2847) ===
Q mean: -8.101698
Q std: 11.432041
Actor loss: 8.105684
Action reg: 0.003986
  l1.weight: grad_norm = 0.029720
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.027252
Total gradient norm: 0.097063
=== Actor Training Debug (Iteration 2848) ===
Q mean: -8.677160
Q std: 11.456251
Actor loss: 8.681127
Action reg: 0.003967
  l1.weight: grad_norm = 0.140022
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.162283
Total gradient norm: 0.663184
=== Actor Training Debug (Iteration 2849) ===
Q mean: -9.023212
Q std: 11.748805
Actor loss: 9.027193
Action reg: 0.003980
  l1.weight: grad_norm = 0.020958
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.017756
Total gradient norm: 0.063208
=== Actor Training Debug (Iteration 2850) ===
Q mean: -8.213608
Q std: 10.592578
Actor loss: 8.217581
Action reg: 0.003973
  l1.weight: grad_norm = 0.066919
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.066511
Total gradient norm: 0.209611
=== Actor Training Debug (Iteration 2851) ===
Q mean: -7.512884
Q std: 10.425164
Actor loss: 7.516869
Action reg: 0.003984
  l1.weight: grad_norm = 0.035024
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.028624
Total gradient norm: 0.101850
=== Actor Training Debug (Iteration 2852) ===
Q mean: -8.627558
Q std: 11.181538
Actor loss: 8.631543
Action reg: 0.003986
  l1.weight: grad_norm = 0.084035
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.062295
Total gradient norm: 0.291261
=== Actor Training Debug (Iteration 2853) ===
Q mean: -7.287244
Q std: 10.470926
Actor loss: 7.291222
Action reg: 0.003978
  l1.weight: grad_norm = 0.093148
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.081101
Total gradient norm: 0.288970
=== Actor Training Debug (Iteration 2854) ===
Q mean: -8.242609
Q std: 11.100181
Actor loss: 8.246593
Action reg: 0.003983
  l1.weight: grad_norm = 0.024818
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.022676
Total gradient norm: 0.088507
=== Actor Training Debug (Iteration 2855) ===
Q mean: -9.078312
Q std: 10.535902
Actor loss: 9.082292
Action reg: 0.003980
  l1.weight: grad_norm = 0.060443
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.053632
Total gradient norm: 0.206190
=== Actor Training Debug (Iteration 2856) ===
Q mean: -8.740840
Q std: 11.789971
Actor loss: 8.744822
Action reg: 0.003981
  l1.weight: grad_norm = 0.091441
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.081415
Total gradient norm: 0.252786
=== Actor Training Debug (Iteration 2857) ===
Q mean: -6.715320
Q std: 10.092460
Actor loss: 6.719300
Action reg: 0.003981
  l1.weight: grad_norm = 0.068627
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.053574
Total gradient norm: 0.211957
=== Actor Training Debug (Iteration 2858) ===
Q mean: -6.654820
Q std: 10.214843
Actor loss: 6.658797
Action reg: 0.003976
  l1.weight: grad_norm = 0.054456
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.044774
Total gradient norm: 0.179985
=== Actor Training Debug (Iteration 2859) ===
Q mean: -8.957664
Q std: 11.923691
Actor loss: 8.961647
Action reg: 0.003984
  l1.weight: grad_norm = 0.022799
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.019917
Total gradient norm: 0.078516
=== Actor Training Debug (Iteration 2860) ===
Q mean: -9.571904
Q std: 11.776682
Actor loss: 9.575890
Action reg: 0.003986
  l1.weight: grad_norm = 0.042812
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.039038
Total gradient norm: 0.160613
=== Actor Training Debug (Iteration 2861) ===
Q mean: -8.800647
Q std: 10.863144
Actor loss: 8.804622
Action reg: 0.003975
  l1.weight: grad_norm = 0.048677
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.045571
Total gradient norm: 0.202020
=== Actor Training Debug (Iteration 2862) ===
Q mean: -8.023106
Q std: 10.779193
Actor loss: 8.027083
Action reg: 0.003978
  l1.weight: grad_norm = 0.059250
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.049745
Total gradient norm: 0.195426
=== Actor Training Debug (Iteration 2863) ===
Q mean: -7.555112
Q std: 10.077730
Actor loss: 7.559090
Action reg: 0.003977
  l1.weight: grad_norm = 0.048413
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.039828
Total gradient norm: 0.161803
=== Actor Training Debug (Iteration 2864) ===
Q mean: -8.460047
Q std: 11.039848
Actor loss: 8.464025
Action reg: 0.003978
  l1.weight: grad_norm = 0.051068
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.044286
Total gradient norm: 0.175147
=== Actor Training Debug (Iteration 2865) ===
Q mean: -9.086079
Q std: 11.912956
Actor loss: 9.090065
Action reg: 0.003986
  l1.weight: grad_norm = 0.059628
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.064002
Total gradient norm: 0.344994
=== Actor Training Debug (Iteration 2866) ===
Q mean: -7.894378
Q std: 10.771719
Actor loss: 7.898361
Action reg: 0.003984
  l1.weight: grad_norm = 0.022664
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.025033
Total gradient norm: 0.118043
=== Actor Training Debug (Iteration 2867) ===
Q mean: -7.944302
Q std: 9.391736
Actor loss: 7.948291
Action reg: 0.003990
  l1.weight: grad_norm = 0.022232
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.020308
Total gradient norm: 0.081446
=== Actor Training Debug (Iteration 2868) ===
Q mean: -8.781071
Q std: 11.518222
Actor loss: 8.785050
Action reg: 0.003979
  l1.weight: grad_norm = 0.058472
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.062039
Total gradient norm: 0.254764
=== Actor Training Debug (Iteration 2869) ===
Q mean: -8.056694
Q std: 11.231246
Actor loss: 8.060675
Action reg: 0.003980
  l1.weight: grad_norm = 0.053915
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.044301
Total gradient norm: 0.173312
=== Actor Training Debug (Iteration 2870) ===
Q mean: -8.751613
Q std: 10.200304
Actor loss: 8.755589
Action reg: 0.003976
  l1.weight: grad_norm = 0.077120
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.071429
Total gradient norm: 0.268366
=== Actor Training Debug (Iteration 2871) ===
Q mean: -8.373775
Q std: 11.899756
Actor loss: 8.377746
Action reg: 0.003970
  l1.weight: grad_norm = 0.037019
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.034303
Total gradient norm: 0.147568
=== Actor Training Debug (Iteration 2872) ===
Q mean: -7.819565
Q std: 11.026702
Actor loss: 7.823538
Action reg: 0.003973
  l1.weight: grad_norm = 0.107690
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.093162
Total gradient norm: 0.379707
=== Actor Training Debug (Iteration 2873) ===
Q mean: -8.999914
Q std: 11.585107
Actor loss: 9.003897
Action reg: 0.003983
  l1.weight: grad_norm = 0.032469
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.031085
Total gradient norm: 0.119668
=== Actor Training Debug (Iteration 2874) ===
Q mean: -8.477417
Q std: 11.680817
Actor loss: 8.481395
Action reg: 0.003978
  l1.weight: grad_norm = 0.039913
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.038778
Total gradient norm: 0.142477
=== Actor Training Debug (Iteration 2875) ===
Q mean: -7.489825
Q std: 9.590652
Actor loss: 7.493800
Action reg: 0.003974
  l1.weight: grad_norm = 0.072303
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.065231
Total gradient norm: 0.259311
=== Actor Training Debug (Iteration 2876) ===
Q mean: -7.994370
Q std: 10.462286
Actor loss: 7.998353
Action reg: 0.003983
  l1.weight: grad_norm = 0.072453
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.061394
Total gradient norm: 0.247499
=== Actor Training Debug (Iteration 2877) ===
Q mean: -7.511073
Q std: 10.096945
Actor loss: 7.515049
Action reg: 0.003975
  l1.weight: grad_norm = 0.050170
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.042275
Total gradient norm: 0.140623
=== Actor Training Debug (Iteration 2878) ===
Q mean: -8.805780
Q std: 10.911460
Actor loss: 8.809761
Action reg: 0.003981
  l1.weight: grad_norm = 0.025630
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.025022
Total gradient norm: 0.092302
=== Actor Training Debug (Iteration 2879) ===
Q mean: -8.816019
Q std: 11.305829
Actor loss: 8.820002
Action reg: 0.003982
  l1.weight: grad_norm = 0.057677
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.049280
Total gradient norm: 0.174092
=== Actor Training Debug (Iteration 2880) ===
Q mean: -8.223915
Q std: 10.939663
Actor loss: 8.227862
Action reg: 0.003947
  l1.weight: grad_norm = 0.058956
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.050264
Total gradient norm: 0.257229
=== Actor Training Debug (Iteration 2881) ===
Q mean: -8.358915
Q std: 10.750137
Actor loss: 8.362892
Action reg: 0.003977
  l1.weight: grad_norm = 0.053721
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.042218
Total gradient norm: 0.136320
=== Actor Training Debug (Iteration 2882) ===
Q mean: -8.585582
Q std: 10.764415
Actor loss: 8.589570
Action reg: 0.003988
  l1.weight: grad_norm = 0.029370
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.024240
Total gradient norm: 0.106651
=== Actor Training Debug (Iteration 2883) ===
Q mean: -8.436379
Q std: 11.624032
Actor loss: 8.440364
Action reg: 0.003985
  l1.weight: grad_norm = 0.055506
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.042407
Total gradient norm: 0.162645
=== Actor Training Debug (Iteration 2884) ===
Q mean: -8.011721
Q std: 11.763050
Actor loss: 8.015690
Action reg: 0.003969
  l1.weight: grad_norm = 0.050187
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.042853
Total gradient norm: 0.138380
=== Actor Training Debug (Iteration 2885) ===
Q mean: -9.876930
Q std: 12.231272
Actor loss: 9.880913
Action reg: 0.003983
  l1.weight: grad_norm = 0.027005
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.022526
Total gradient norm: 0.082391
=== Actor Training Debug (Iteration 2886) ===
Q mean: -9.045732
Q std: 11.558311
Actor loss: 9.049702
Action reg: 0.003969
  l1.weight: grad_norm = 0.034283
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.032501
Total gradient norm: 0.127525
=== Actor Training Debug (Iteration 2887) ===
Q mean: -8.960800
Q std: 11.283138
Actor loss: 8.964781
Action reg: 0.003981
  l1.weight: grad_norm = 0.014906
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.011416
Total gradient norm: 0.034737
=== Actor Training Debug (Iteration 2888) ===
Q mean: -8.758524
Q std: 11.314342
Actor loss: 8.762492
Action reg: 0.003969
  l1.weight: grad_norm = 0.057927
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.050471
Total gradient norm: 0.219228
=== Actor Training Debug (Iteration 2889) ===
Q mean: -8.681202
Q std: 11.416408
Actor loss: 8.685177
Action reg: 0.003975
  l1.weight: grad_norm = 0.040126
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.035445
Total gradient norm: 0.123634
=== Actor Training Debug (Iteration 2890) ===
Q mean: -7.658719
Q std: 10.318086
Actor loss: 7.662692
Action reg: 0.003973
  l1.weight: grad_norm = 0.055866
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.054941
Total gradient norm: 0.204980
=== Actor Training Debug (Iteration 2891) ===
Q mean: -7.907424
Q std: 9.744460
Actor loss: 7.911400
Action reg: 0.003976
  l1.weight: grad_norm = 0.058378
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.053636
Total gradient norm: 0.155984
=== Actor Training Debug (Iteration 2892) ===
Q mean: -8.371101
Q std: 10.703416
Actor loss: 8.375082
Action reg: 0.003981
  l1.weight: grad_norm = 0.022301
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.019673
Total gradient norm: 0.066226
=== Actor Training Debug (Iteration 2893) ===
Q mean: -7.520758
Q std: 10.168452
Actor loss: 7.524730
Action reg: 0.003972
  l1.weight: grad_norm = 0.033828
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.028417
Total gradient norm: 0.115010
=== Actor Training Debug (Iteration 2894) ===
Q mean: -7.936885
Q std: 11.057910
Actor loss: 7.940869
Action reg: 0.003984
  l1.weight: grad_norm = 0.039061
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.030506
Total gradient norm: 0.121191
=== Actor Training Debug (Iteration 2895) ===
Q mean: -7.049381
Q std: 9.241524
Actor loss: 7.053352
Action reg: 0.003971
  l1.weight: grad_norm = 0.027044
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.024290
Total gradient norm: 0.079587
=== Actor Training Debug (Iteration 2896) ===
Q mean: -7.812263
Q std: 10.277754
Actor loss: 7.816245
Action reg: 0.003982
  l1.weight: grad_norm = 0.067049
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.063379
Total gradient norm: 0.282002
=== Actor Training Debug (Iteration 2897) ===
Q mean: -9.189472
Q std: 11.472294
Actor loss: 9.193457
Action reg: 0.003985
  l1.weight: grad_norm = 0.048210
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.041570
Total gradient norm: 0.186550
=== Actor Training Debug (Iteration 2898) ===
Q mean: -8.219567
Q std: 10.903408
Actor loss: 8.223550
Action reg: 0.003983
  l1.weight: grad_norm = 0.017846
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.015988
Total gradient norm: 0.055428
=== Actor Training Debug (Iteration 2899) ===
Q mean: -8.390184
Q std: 10.784390
Actor loss: 8.394159
Action reg: 0.003975
  l1.weight: grad_norm = 0.045827
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.039446
Total gradient norm: 0.145329
=== Actor Training Debug (Iteration 2900) ===
Q mean: -9.454218
Q std: 11.928508
Actor loss: 9.458194
Action reg: 0.003976
  l1.weight: grad_norm = 0.072756
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.062588
Total gradient norm: 0.304062
=== Actor Training Debug (Iteration 2901) ===
Q mean: -9.136546
Q std: 11.713882
Actor loss: 9.140531
Action reg: 0.003984
  l1.weight: grad_norm = 0.112012
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.084453
Total gradient norm: 0.317900
=== Actor Training Debug (Iteration 2902) ===
Q mean: -7.475378
Q std: 10.397365
Actor loss: 7.479345
Action reg: 0.003967
  l1.weight: grad_norm = 0.042821
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.036182
Total gradient norm: 0.125033
=== Actor Training Debug (Iteration 2903) ===
Q mean: -8.000387
Q std: 10.749416
Actor loss: 8.004364
Action reg: 0.003977
  l1.weight: grad_norm = 0.035565
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.031798
Total gradient norm: 0.142125
=== Actor Training Debug (Iteration 2904) ===
Q mean: -9.247749
Q std: 11.877493
Actor loss: 9.251727
Action reg: 0.003978
  l1.weight: grad_norm = 0.046355
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.031715
Total gradient norm: 0.109688
=== Actor Training Debug (Iteration 2905) ===
Q mean: -7.433481
Q std: 10.323537
Actor loss: 7.437463
Action reg: 0.003982
  l1.weight: grad_norm = 0.035744
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.028880
Total gradient norm: 0.127318
=== Actor Training Debug (Iteration 2906) ===
Q mean: -8.047440
Q std: 11.242993
Actor loss: 8.051423
Action reg: 0.003984
  l1.weight: grad_norm = 0.026755
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.022740
Total gradient norm: 0.078425
=== Actor Training Debug (Iteration 2907) ===
Q mean: -8.261017
Q std: 10.581974
Actor loss: 8.264997
Action reg: 0.003980
  l1.weight: grad_norm = 0.067006
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.062311
Total gradient norm: 0.302305
=== Actor Training Debug (Iteration 2908) ===
Q mean: -8.415852
Q std: 10.995262
Actor loss: 8.419829
Action reg: 0.003978
  l1.weight: grad_norm = 0.038338
  l1.bias: grad_norm = 0.000453
  l2.weight: grad_norm = 0.032144
Total gradient norm: 0.115918
=== Actor Training Debug (Iteration 2909) ===
Q mean: -9.258018
Q std: 11.809290
Actor loss: 9.262006
Action reg: 0.003987
  l1.weight: grad_norm = 0.015641
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.014039
Total gradient norm: 0.056387
=== Actor Training Debug (Iteration 2910) ===
Q mean: -8.970355
Q std: 11.579934
Actor loss: 8.974330
Action reg: 0.003975
  l1.weight: grad_norm = 0.059393
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.053970
Total gradient norm: 0.212880
=== Actor Training Debug (Iteration 2911) ===
Q mean: -7.866172
Q std: 10.884069
Actor loss: 7.870161
Action reg: 0.003989
  l1.weight: grad_norm = 0.021598
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.017860
Total gradient norm: 0.061014
=== Actor Training Debug (Iteration 2912) ===
Q mean: -8.299546
Q std: 10.908131
Actor loss: 8.303517
Action reg: 0.003971
  l1.weight: grad_norm = 0.025732
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.024772
Total gradient norm: 0.135319
=== Actor Training Debug (Iteration 2913) ===
Q mean: -9.514795
Q std: 11.927102
Actor loss: 9.518773
Action reg: 0.003978
  l1.weight: grad_norm = 0.056169
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.051983
Total gradient norm: 0.237929
=== Actor Training Debug (Iteration 2914) ===
Q mean: -8.366026
Q std: 10.870927
Actor loss: 8.370005
Action reg: 0.003978
  l1.weight: grad_norm = 0.029673
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.025656
Total gradient norm: 0.101169
=== Actor Training Debug (Iteration 2915) ===
Q mean: -8.403105
Q std: 10.468815
Actor loss: 8.407084
Action reg: 0.003980
  l1.weight: grad_norm = 0.042820
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.036114
Total gradient norm: 0.135351
=== Actor Training Debug (Iteration 2916) ===
Q mean: -7.365859
Q std: 10.044749
Actor loss: 7.369831
Action reg: 0.003972
  l1.weight: grad_norm = 0.060531
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.051651
Total gradient norm: 0.161392
=== Actor Training Debug (Iteration 2917) ===
Q mean: -7.297755
Q std: 10.220340
Actor loss: 7.301733
Action reg: 0.003978
  l1.weight: grad_norm = 0.037791
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.037814
Total gradient norm: 0.138406
=== Actor Training Debug (Iteration 2918) ===
Q mean: -9.014708
Q std: 11.660888
Actor loss: 9.018685
Action reg: 0.003977
  l1.weight: grad_norm = 0.110487
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.101574
Total gradient norm: 0.439643
=== Actor Training Debug (Iteration 2919) ===
Q mean: -8.900270
Q std: 11.645902
Actor loss: 8.904257
Action reg: 0.003988
  l1.weight: grad_norm = 0.014670
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.013112
Total gradient norm: 0.060787
=== Actor Training Debug (Iteration 2920) ===
Q mean: -7.700585
Q std: 11.007827
Actor loss: 7.704551
Action reg: 0.003965
  l1.weight: grad_norm = 0.082662
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.081585
Total gradient norm: 0.295807
=== Actor Training Debug (Iteration 2921) ===
Q mean: -7.074657
Q std: 10.375355
Actor loss: 7.078644
Action reg: 0.003987
  l1.weight: grad_norm = 0.027826
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.024885
Total gradient norm: 0.102451
=== Actor Training Debug (Iteration 2922) ===
Q mean: -8.241284
Q std: 11.026874
Actor loss: 8.245259
Action reg: 0.003975
  l1.weight: grad_norm = 0.034484
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.031820
Total gradient norm: 0.144225
=== Actor Training Debug (Iteration 2923) ===
Q mean: -9.190187
Q std: 11.116667
Actor loss: 9.194166
Action reg: 0.003979
  l1.weight: grad_norm = 0.026520
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.022739
Total gradient norm: 0.099272
=== Actor Training Debug (Iteration 2924) ===
Q mean: -9.266781
Q std: 12.301143
Actor loss: 9.270766
Action reg: 0.003985
  l1.weight: grad_norm = 0.030114
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.025175
Total gradient norm: 0.093534
=== Actor Training Debug (Iteration 2925) ===
Q mean: -7.776994
Q std: 11.129696
Actor loss: 7.780975
Action reg: 0.003981
  l1.weight: grad_norm = 0.056934
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.043814
Total gradient norm: 0.157269
=== Actor Training Debug (Iteration 2926) ===
Q mean: -8.252836
Q std: 10.768490
Actor loss: 8.256824
Action reg: 0.003987
  l1.weight: grad_norm = 0.022251
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.021189
Total gradient norm: 0.086524
=== Actor Training Debug (Iteration 2927) ===
Q mean: -8.847254
Q std: 11.168753
Actor loss: 8.851235
Action reg: 0.003982
  l1.weight: grad_norm = 0.071226
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.063084
Total gradient norm: 0.269686
=== Actor Training Debug (Iteration 2928) ===
Q mean: -7.772452
Q std: 10.896131
Actor loss: 7.776431
Action reg: 0.003979
  l1.weight: grad_norm = 0.035247
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.035360
Total gradient norm: 0.153443
=== Actor Training Debug (Iteration 2929) ===
Q mean: -8.768055
Q std: 11.344988
Actor loss: 8.772031
Action reg: 0.003976
  l1.weight: grad_norm = 0.140964
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.109555
Total gradient norm: 0.471815
=== Actor Training Debug (Iteration 2930) ===
Q mean: -9.095501
Q std: 11.295867
Actor loss: 9.099482
Action reg: 0.003981
  l1.weight: grad_norm = 0.044171
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.039256
Total gradient norm: 0.143894
=== Actor Training Debug (Iteration 2931) ===
Q mean: -8.485949
Q std: 11.030059
Actor loss: 8.489926
Action reg: 0.003977
  l1.weight: grad_norm = 0.121564
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.099295
Total gradient norm: 0.319249
=== Actor Training Debug (Iteration 2932) ===
Q mean: -9.732925
Q std: 12.072815
Actor loss: 9.736913
Action reg: 0.003987
  l1.weight: grad_norm = 0.034456
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.031667
Total gradient norm: 0.149810
=== Actor Training Debug (Iteration 2933) ===
Q mean: -8.968092
Q std: 11.376770
Actor loss: 8.972078
Action reg: 0.003986
  l1.weight: grad_norm = 0.030607
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.026707
Total gradient norm: 0.111533
=== Actor Training Debug (Iteration 2934) ===
Q mean: -8.432665
Q std: 11.773484
Actor loss: 8.436645
Action reg: 0.003979
  l1.weight: grad_norm = 0.055867
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.043930
Total gradient norm: 0.219177
=== Actor Training Debug (Iteration 2935) ===
Q mean: -8.786973
Q std: 11.713373
Actor loss: 8.790951
Action reg: 0.003978
  l1.weight: grad_norm = 0.043374
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.036438
Total gradient norm: 0.172825
=== Actor Training Debug (Iteration 2936) ===
Q mean: -8.654675
Q std: 10.574200
Actor loss: 8.658655
Action reg: 0.003981
  l1.weight: grad_norm = 0.062770
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.057320
Total gradient norm: 0.239069
=== Actor Training Debug (Iteration 2937) ===
Q mean: -8.672055
Q std: 11.647860
Actor loss: 8.676032
Action reg: 0.003976
  l1.weight: grad_norm = 0.063334
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.053364
Total gradient norm: 0.160890
=== Actor Training Debug (Iteration 2938) ===
Q mean: -9.236761
Q std: 10.794540
Actor loss: 9.240737
Action reg: 0.003975
  l1.weight: grad_norm = 0.082060
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.063394
Total gradient norm: 0.239005
=== Actor Training Debug (Iteration 2939) ===
Q mean: -8.189013
Q std: 10.308953
Actor loss: 8.192994
Action reg: 0.003981
  l1.weight: grad_norm = 0.112183
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.089988
Total gradient norm: 0.308188
=== Actor Training Debug (Iteration 2940) ===
Q mean: -9.063008
Q std: 11.594762
Actor loss: 9.066998
Action reg: 0.003990
  l1.weight: grad_norm = 0.035798
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.031288
Total gradient norm: 0.141464
=== Actor Training Debug (Iteration 2941) ===
Q mean: -8.019038
Q std: 11.115869
Actor loss: 8.023028
Action reg: 0.003990
  l1.weight: grad_norm = 0.147205
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.099795
Total gradient norm: 0.326135
=== Actor Training Debug (Iteration 2942) ===
Q mean: -8.771868
Q std: 12.511141
Actor loss: 8.775843
Action reg: 0.003975
  l1.weight: grad_norm = 0.085298
  l1.bias: grad_norm = 0.000492
  l2.weight: grad_norm = 0.080601
Total gradient norm: 0.313928
=== Actor Training Debug (Iteration 2943) ===
Q mean: -9.638405
Q std: 12.000171
Actor loss: 9.642388
Action reg: 0.003983
  l1.weight: grad_norm = 0.038206
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.039451
Total gradient norm: 0.158990
=== Actor Training Debug (Iteration 2944) ===
Q mean: -8.620571
Q std: 11.969909
Actor loss: 8.624541
Action reg: 0.003970
  l1.weight: grad_norm = 0.073258
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.051456
Total gradient norm: 0.161247
=== Actor Training Debug (Iteration 2945) ===
Q mean: -8.996188
Q std: 10.900841
Actor loss: 9.000176
Action reg: 0.003988
  l1.weight: grad_norm = 0.037577
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.036689
Total gradient norm: 0.134071
=== Actor Training Debug (Iteration 2946) ===
Q mean: -7.544600
Q std: 10.605387
Actor loss: 7.548585
Action reg: 0.003984
  l1.weight: grad_norm = 0.056809
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.040912
Total gradient norm: 0.151923
=== Actor Training Debug (Iteration 2947) ===
Q mean: -6.674400
Q std: 9.059358
Actor loss: 6.678365
Action reg: 0.003965
  l1.weight: grad_norm = 0.079191
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.068352
Total gradient norm: 0.217847
=== Actor Training Debug (Iteration 2948) ===
Q mean: -8.904531
Q std: 10.915257
Actor loss: 8.908507
Action reg: 0.003976
  l1.weight: grad_norm = 0.053507
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.052098
Total gradient norm: 0.247713
=== Actor Training Debug (Iteration 2949) ===
Q mean: -8.245382
Q std: 10.600796
Actor loss: 8.249364
Action reg: 0.003982
  l1.weight: grad_norm = 0.033922
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.028722
Total gradient norm: 0.102794
=== Actor Training Debug (Iteration 2950) ===
Q mean: -7.950436
Q std: 10.486481
Actor loss: 7.954415
Action reg: 0.003980
  l1.weight: grad_norm = 0.114586
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.102635
Total gradient norm: 0.397899
=== Actor Training Debug (Iteration 2951) ===
Q mean: -8.442230
Q std: 11.141403
Actor loss: 8.446215
Action reg: 0.003984
  l1.weight: grad_norm = 0.025332
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.019149
Total gradient norm: 0.078899
=== Actor Training Debug (Iteration 2952) ===
Q mean: -9.559006
Q std: 11.927290
Actor loss: 9.562992
Action reg: 0.003986
  l1.weight: grad_norm = 0.016394
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.014937
Total gradient norm: 0.055714
=== Actor Training Debug (Iteration 2953) ===
Q mean: -8.727610
Q std: 12.341444
Actor loss: 8.731582
Action reg: 0.003972
  l1.weight: grad_norm = 0.069553
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.063136
Total gradient norm: 0.266066
=== Actor Training Debug (Iteration 2954) ===
Q mean: -10.308486
Q std: 12.256711
Actor loss: 10.312465
Action reg: 0.003979
  l1.weight: grad_norm = 0.086289
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.062743
Total gradient norm: 0.234941
=== Actor Training Debug (Iteration 2955) ===
Q mean: -6.566251
Q std: 9.873199
Actor loss: 6.570238
Action reg: 0.003986
  l1.weight: grad_norm = 0.025500
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.021546
Total gradient norm: 0.078780
=== Actor Training Debug (Iteration 2956) ===
Q mean: -8.778160
Q std: 11.408936
Actor loss: 8.782138
Action reg: 0.003977
  l1.weight: grad_norm = 0.170567
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.139243
Total gradient norm: 0.429101
=== Actor Training Debug (Iteration 2957) ===
Q mean: -10.464458
Q std: 12.095782
Actor loss: 10.468431
Action reg: 0.003973
  l1.weight: grad_norm = 0.037701
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.033294
Total gradient norm: 0.135960
=== Actor Training Debug (Iteration 2958) ===
Q mean: -9.080673
Q std: 11.988647
Actor loss: 9.084654
Action reg: 0.003981
  l1.weight: grad_norm = 0.076255
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.066840
Total gradient norm: 0.309898
=== Actor Training Debug (Iteration 2959) ===
Q mean: -8.594954
Q std: 10.990540
Actor loss: 8.598933
Action reg: 0.003979
  l1.weight: grad_norm = 0.058809
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.053108
Total gradient norm: 0.218811
=== Actor Training Debug (Iteration 2960) ===
Q mean: -7.136366
Q std: 10.343149
Actor loss: 7.140342
Action reg: 0.003976
  l1.weight: grad_norm = 0.054276
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.049009
Total gradient norm: 0.224786
=== Actor Training Debug (Iteration 2961) ===
Q mean: -9.313995
Q std: 10.905085
Actor loss: 9.317973
Action reg: 0.003978
  l1.weight: grad_norm = 0.064836
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.067996
Total gradient norm: 0.347253
=== Actor Training Debug (Iteration 2962) ===
Q mean: -9.483316
Q std: 11.975459
Actor loss: 9.487292
Action reg: 0.003976
  l1.weight: grad_norm = 0.070273
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.057588
Total gradient norm: 0.224350
=== Actor Training Debug (Iteration 2963) ===
Q mean: -8.758918
Q std: 11.845304
Actor loss: 8.762906
Action reg: 0.003988
  l1.weight: grad_norm = 0.030413
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.028948
Total gradient norm: 0.100475
=== Actor Training Debug (Iteration 2964) ===
Q mean: -9.490295
Q std: 12.286536
Actor loss: 9.494282
Action reg: 0.003986
  l1.weight: grad_norm = 0.076253
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.060783
Total gradient norm: 0.230365
=== Actor Training Debug (Iteration 2965) ===
Q mean: -8.375107
Q std: 11.721733
Actor loss: 8.379088
Action reg: 0.003982
  l1.weight: grad_norm = 0.066416
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.049787
Total gradient norm: 0.184338
=== Actor Training Debug (Iteration 2966) ===
Q mean: -8.611860
Q std: 11.456344
Actor loss: 8.615845
Action reg: 0.003984
  l1.weight: grad_norm = 0.059680
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.052402
Total gradient norm: 0.199699
=== Actor Training Debug (Iteration 2967) ===
Q mean: -8.988070
Q std: 11.289119
Actor loss: 8.992053
Action reg: 0.003983
  l1.weight: grad_norm = 0.023870
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.022078
Total gradient norm: 0.096027
=== Actor Training Debug (Iteration 2968) ===
Q mean: -8.600050
Q std: 11.827795
Actor loss: 8.604033
Action reg: 0.003983
  l1.weight: grad_norm = 0.017749
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.014185
Total gradient norm: 0.052199
=== Actor Training Debug (Iteration 2969) ===
Q mean: -9.268982
Q std: 11.058092
Actor loss: 9.272965
Action reg: 0.003984
  l1.weight: grad_norm = 0.019169
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.015430
Total gradient norm: 0.065438
=== Actor Training Debug (Iteration 2970) ===
Q mean: -8.562995
Q std: 11.297601
Actor loss: 8.566967
Action reg: 0.003972
  l1.weight: grad_norm = 0.037217
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.035228
Total gradient norm: 0.152300
=== Actor Training Debug (Iteration 2971) ===
Q mean: -7.478175
Q std: 10.345480
Actor loss: 7.482151
Action reg: 0.003975
  l1.weight: grad_norm = 0.044075
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.040705
Total gradient norm: 0.156352
=== Actor Training Debug (Iteration 2972) ===
Q mean: -9.377535
Q std: 12.192233
Actor loss: 9.381509
Action reg: 0.003974
  l1.weight: grad_norm = 0.103112
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.092743
Total gradient norm: 0.313753
=== Actor Training Debug (Iteration 2973) ===
Q mean: -7.928921
Q std: 11.452244
Actor loss: 7.932901
Action reg: 0.003980
  l1.weight: grad_norm = 0.037434
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.024905
Total gradient norm: 0.090637
=== Actor Training Debug (Iteration 2974) ===
Q mean: -8.604280
Q std: 11.525042
Actor loss: 8.608263
Action reg: 0.003983
  l1.weight: grad_norm = 0.062415
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.060576
Total gradient norm: 0.186307
=== Actor Training Debug (Iteration 2975) ===
Q mean: -9.974798
Q std: 11.567853
Actor loss: 9.978784
Action reg: 0.003985
  l1.weight: grad_norm = 0.038748
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.036756
Total gradient norm: 0.127424
=== Actor Training Debug (Iteration 2976) ===
Q mean: -8.328005
Q std: 11.224920
Actor loss: 8.331986
Action reg: 0.003981
  l1.weight: grad_norm = 0.025431
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.020648
Total gradient norm: 0.076039
=== Actor Training Debug (Iteration 2977) ===
Q mean: -10.092822
Q std: 12.873337
Actor loss: 10.096810
Action reg: 0.003988
  l1.weight: grad_norm = 0.007302
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.006995
Total gradient norm: 0.031600
=== Actor Training Debug (Iteration 2978) ===
Q mean: -7.633482
Q std: 10.556307
Actor loss: 7.637456
Action reg: 0.003974
  l1.weight: grad_norm = 0.055884
  l1.bias: grad_norm = 0.000579
  l2.weight: grad_norm = 0.043936
Total gradient norm: 0.125264
=== Actor Training Debug (Iteration 2979) ===
Q mean: -8.935225
Q std: 10.855028
Actor loss: 8.939204
Action reg: 0.003979
  l1.weight: grad_norm = 0.028819
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.025429
Total gradient norm: 0.096327
=== Actor Training Debug (Iteration 2980) ===
Q mean: -8.997700
Q std: 11.627488
Actor loss: 9.001683
Action reg: 0.003984
  l1.weight: grad_norm = 0.092195
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.078692
Total gradient norm: 0.288714
=== Actor Training Debug (Iteration 2981) ===
Q mean: -9.743603
Q std: 11.225183
Actor loss: 9.747582
Action reg: 0.003980
  l1.weight: grad_norm = 0.026493
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.021531
Total gradient norm: 0.090300
=== Actor Training Debug (Iteration 2982) ===
Q mean: -9.101665
Q std: 12.049947
Actor loss: 9.105655
Action reg: 0.003989
  l1.weight: grad_norm = 0.069785
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.051576
Total gradient norm: 0.206752
=== Actor Training Debug (Iteration 2983) ===
Q mean: -7.662807
Q std: 10.581598
Actor loss: 7.666777
Action reg: 0.003970
  l1.weight: grad_norm = 0.070524
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.052152
Total gradient norm: 0.162945
=== Actor Training Debug (Iteration 2984) ===
Q mean: -7.995214
Q std: 11.115686
Actor loss: 7.999187
Action reg: 0.003973
  l1.weight: grad_norm = 0.055941
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.053843
Total gradient norm: 0.222263
=== Actor Training Debug (Iteration 2985) ===
Q mean: -8.811516
Q std: 11.738505
Actor loss: 8.815503
Action reg: 0.003987
  l1.weight: grad_norm = 0.031136
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.028839
Total gradient norm: 0.134387
=== Actor Training Debug (Iteration 2986) ===
Q mean: -8.480868
Q std: 10.734200
Actor loss: 8.484829
Action reg: 0.003961
  l1.weight: grad_norm = 0.080427
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.070758
Total gradient norm: 0.278721
=== Actor Training Debug (Iteration 2987) ===
Q mean: -9.777298
Q std: 11.965193
Actor loss: 9.781281
Action reg: 0.003982
  l1.weight: grad_norm = 0.021528
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.021019
Total gradient norm: 0.080938
=== Actor Training Debug (Iteration 2988) ===
Q mean: -9.989933
Q std: 12.235075
Actor loss: 9.993914
Action reg: 0.003981
  l1.weight: grad_norm = 0.040842
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.037810
Total gradient norm: 0.169131
=== Actor Training Debug (Iteration 2989) ===
Q mean: -9.968604
Q std: 11.649621
Actor loss: 9.972588
Action reg: 0.003984
  l1.weight: grad_norm = 0.017240
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.018318
Total gradient norm: 0.094719
=== Actor Training Debug (Iteration 2990) ===
Q mean: -9.268183
Q std: 11.371783
Actor loss: 9.272170
Action reg: 0.003987
  l1.weight: grad_norm = 0.041774
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.036352
Total gradient norm: 0.143089
=== Actor Training Debug (Iteration 2991) ===
Q mean: -8.752002
Q std: 10.751827
Actor loss: 8.755977
Action reg: 0.003975
  l1.weight: grad_norm = 0.047660
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.037309
Total gradient norm: 0.156035
=== Actor Training Debug (Iteration 2992) ===
Q mean: -8.381283
Q std: 10.900144
Actor loss: 8.385259
Action reg: 0.003976
  l1.weight: grad_norm = 0.100750
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.072852
Total gradient norm: 0.283584
=== Actor Training Debug (Iteration 2993) ===
Q mean: -8.831582
Q std: 12.177314
Actor loss: 8.835560
Action reg: 0.003977
  l1.weight: grad_norm = 0.049403
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.046954
Total gradient norm: 0.205080
=== Actor Training Debug (Iteration 2994) ===
Q mean: -7.741996
Q std: 10.351742
Actor loss: 7.745979
Action reg: 0.003984
  l1.weight: grad_norm = 0.033044
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.030111
Total gradient norm: 0.144316
=== Actor Training Debug (Iteration 2995) ===
Q mean: -7.822879
Q std: 10.730529
Actor loss: 7.826852
Action reg: 0.003973
  l1.weight: grad_norm = 0.060628
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.048219
Total gradient norm: 0.161151
=== Actor Training Debug (Iteration 2996) ===
Q mean: -9.038161
Q std: 11.416769
Actor loss: 9.042144
Action reg: 0.003983
  l1.weight: grad_norm = 0.041980
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.033712
Total gradient norm: 0.132529
=== Actor Training Debug (Iteration 2997) ===
Q mean: -8.467389
Q std: 10.976214
Actor loss: 8.471361
Action reg: 0.003972
  l1.weight: grad_norm = 0.042930
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.035480
Total gradient norm: 0.136750
=== Actor Training Debug (Iteration 2998) ===
Q mean: -8.964137
Q std: 11.440860
Actor loss: 8.968089
Action reg: 0.003952
  l1.weight: grad_norm = 0.141631
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.093442
Total gradient norm: 0.346038
=== Actor Training Debug (Iteration 2999) ===
Q mean: -9.355185
Q std: 11.664111
Actor loss: 9.359154
Action reg: 0.003969
  l1.weight: grad_norm = 0.060848
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.058238
Total gradient norm: 0.218858
=== Actor Training Debug (Iteration 3000) ===
Q mean: -9.106466
Q std: 11.724088
Actor loss: 9.110444
Action reg: 0.003978
  l1.weight: grad_norm = 0.058090
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.053590
Total gradient norm: 0.161932
Step 8000: Critic Loss: 1.0639, Actor Loss: 9.1104, Q Value: -9.1065
  Average reward: -330.304 | Average length: 100.0
Evaluation at episode 80: -330.304
=== Actor Training Debug (Iteration 3001) ===
Q mean: -9.364473
Q std: 11.376467
Actor loss: 9.368451
Action reg: 0.003978
  l1.weight: grad_norm = 0.059919
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.052535
Total gradient norm: 0.186650
=== Actor Training Debug (Iteration 3002) ===
Q mean: -9.077439
Q std: 11.783067
Actor loss: 9.081419
Action reg: 0.003980
  l1.weight: grad_norm = 0.055860
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.049318
Total gradient norm: 0.175458
=== Actor Training Debug (Iteration 3003) ===
Q mean: -8.158464
Q std: 10.364027
Actor loss: 8.162441
Action reg: 0.003977
  l1.weight: grad_norm = 0.045060
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.037178
Total gradient norm: 0.120419
=== Actor Training Debug (Iteration 3004) ===
Q mean: -9.377048
Q std: 11.689677
Actor loss: 9.381020
Action reg: 0.003971
  l1.weight: grad_norm = 0.073688
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.059939
Total gradient norm: 0.292559
=== Actor Training Debug (Iteration 3005) ===
Q mean: -7.834802
Q std: 10.350009
Actor loss: 7.838781
Action reg: 0.003979
  l1.weight: grad_norm = 0.052176
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.041693
Total gradient norm: 0.152095
=== Actor Training Debug (Iteration 3006) ===
Q mean: -8.029884
Q std: 10.620251
Actor loss: 8.033846
Action reg: 0.003961
  l1.weight: grad_norm = 0.122612
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.087231
Total gradient norm: 0.292232
=== Actor Training Debug (Iteration 3007) ===
Q mean: -6.861478
Q std: 9.787354
Actor loss: 6.865459
Action reg: 0.003981
  l1.weight: grad_norm = 0.032214
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.028743
Total gradient norm: 0.104526
=== Actor Training Debug (Iteration 3008) ===
Q mean: -9.383268
Q std: 11.503685
Actor loss: 9.387239
Action reg: 0.003971
  l1.weight: grad_norm = 0.067072
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.061329
Total gradient norm: 0.219471
=== Actor Training Debug (Iteration 3009) ===
Q mean: -8.439202
Q std: 11.284105
Actor loss: 8.443174
Action reg: 0.003972
  l1.weight: grad_norm = 0.063763
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.060917
Total gradient norm: 0.242923
=== Actor Training Debug (Iteration 3010) ===
Q mean: -9.571636
Q std: 12.405220
Actor loss: 9.575611
Action reg: 0.003975
  l1.weight: grad_norm = 0.040181
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.040277
Total gradient norm: 0.174295
=== Actor Training Debug (Iteration 3011) ===
Q mean: -9.062279
Q std: 11.126290
Actor loss: 9.066257
Action reg: 0.003978
  l1.weight: grad_norm = 0.080761
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.071054
Total gradient norm: 0.208037
=== Actor Training Debug (Iteration 3012) ===
Q mean: -9.640376
Q std: 11.952308
Actor loss: 9.644357
Action reg: 0.003980
  l1.weight: grad_norm = 0.109479
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.105372
Total gradient norm: 0.361551
=== Actor Training Debug (Iteration 3013) ===
Q mean: -9.189706
Q std: 11.858749
Actor loss: 9.193687
Action reg: 0.003981
  l1.weight: grad_norm = 0.066155
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.052087
Total gradient norm: 0.175440
=== Actor Training Debug (Iteration 3014) ===
Q mean: -8.614336
Q std: 12.212660
Actor loss: 8.618326
Action reg: 0.003990
  l1.weight: grad_norm = 0.032917
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.028478
Total gradient norm: 0.100714
=== Actor Training Debug (Iteration 3015) ===
Q mean: -9.331929
Q std: 11.290884
Actor loss: 9.335916
Action reg: 0.003987
  l1.weight: grad_norm = 0.048364
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.035495
Total gradient norm: 0.136603
=== Actor Training Debug (Iteration 3016) ===
Q mean: -8.379656
Q std: 10.488502
Actor loss: 8.383643
Action reg: 0.003987
  l1.weight: grad_norm = 0.022336
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.020470
Total gradient norm: 0.101778
=== Actor Training Debug (Iteration 3017) ===
Q mean: -8.491167
Q std: 11.150221
Actor loss: 8.495134
Action reg: 0.003967
  l1.weight: grad_norm = 0.024292
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.020143
Total gradient norm: 0.065563
=== Actor Training Debug (Iteration 3018) ===
Q mean: -8.436360
Q std: 10.946342
Actor loss: 8.440328
Action reg: 0.003968
  l1.weight: grad_norm = 0.040330
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.033764
Total gradient norm: 0.118586
=== Actor Training Debug (Iteration 3019) ===
Q mean: -8.270763
Q std: 11.185559
Actor loss: 8.274723
Action reg: 0.003959
  l1.weight: grad_norm = 0.054194
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.048474
Total gradient norm: 0.173069
=== Actor Training Debug (Iteration 3020) ===
Q mean: -8.073906
Q std: 10.383975
Actor loss: 8.077891
Action reg: 0.003986
  l1.weight: grad_norm = 0.014038
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.014560
Total gradient norm: 0.076819
=== Actor Training Debug (Iteration 3021) ===
Q mean: -10.042088
Q std: 12.472986
Actor loss: 10.046072
Action reg: 0.003985
  l1.weight: grad_norm = 0.052449
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.048423
Total gradient norm: 0.277379
=== Actor Training Debug (Iteration 3022) ===
Q mean: -8.683153
Q std: 9.704990
Actor loss: 8.687138
Action reg: 0.003984
  l1.weight: grad_norm = 0.033294
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.029323
Total gradient norm: 0.095812
=== Actor Training Debug (Iteration 3023) ===
Q mean: -7.575500
Q std: 11.219175
Actor loss: 7.579475
Action reg: 0.003976
  l1.weight: grad_norm = 0.161489
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.119320
Total gradient norm: 0.522337
=== Actor Training Debug (Iteration 3024) ===
Q mean: -9.222000
Q std: 11.725890
Actor loss: 9.225981
Action reg: 0.003981
  l1.weight: grad_norm = 0.049503
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.042965
Total gradient norm: 0.165648
=== Actor Training Debug (Iteration 3025) ===
Q mean: -7.832123
Q std: 10.874119
Actor loss: 7.836105
Action reg: 0.003982
  l1.weight: grad_norm = 0.012115
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.010670
Total gradient norm: 0.044876
=== Actor Training Debug (Iteration 3026) ===
Q mean: -8.543429
Q std: 11.172483
Actor loss: 8.547407
Action reg: 0.003977
  l1.weight: grad_norm = 0.028804
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.026389
Total gradient norm: 0.127997
=== Actor Training Debug (Iteration 3027) ===
Q mean: -8.175607
Q std: 9.938543
Actor loss: 8.179588
Action reg: 0.003982
  l1.weight: grad_norm = 0.035949
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.034989
Total gradient norm: 0.116649
=== Actor Training Debug (Iteration 3028) ===
Q mean: -9.714840
Q std: 12.412373
Actor loss: 9.718818
Action reg: 0.003978
  l1.weight: grad_norm = 0.064583
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.058910
Total gradient norm: 0.231608
=== Actor Training Debug (Iteration 3029) ===
Q mean: -9.544873
Q std: 11.956162
Actor loss: 9.548861
Action reg: 0.003988
  l1.weight: grad_norm = 0.015490
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.016203
Total gradient norm: 0.064404
=== Actor Training Debug (Iteration 3030) ===
Q mean: -9.923155
Q std: 12.105545
Actor loss: 9.927131
Action reg: 0.003976
  l1.weight: grad_norm = 0.080590
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.065568
Total gradient norm: 0.188263
=== Actor Training Debug (Iteration 3031) ===
Q mean: -7.985140
Q std: 11.040557
Actor loss: 7.989125
Action reg: 0.003985
  l1.weight: grad_norm = 0.036028
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.033334
Total gradient norm: 0.109853
=== Actor Training Debug (Iteration 3032) ===
Q mean: -9.490784
Q std: 12.351476
Actor loss: 9.494763
Action reg: 0.003979
  l1.weight: grad_norm = 0.031155
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.033010
Total gradient norm: 0.128579
=== Actor Training Debug (Iteration 3033) ===
Q mean: -9.041964
Q std: 10.647139
Actor loss: 9.045945
Action reg: 0.003982
  l1.weight: grad_norm = 0.045522
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.035681
Total gradient norm: 0.126689
=== Actor Training Debug (Iteration 3034) ===
Q mean: -8.701269
Q std: 12.421595
Actor loss: 8.705258
Action reg: 0.003990
  l1.weight: grad_norm = 0.035323
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.028996
Total gradient norm: 0.100541
=== Actor Training Debug (Iteration 3035) ===
Q mean: -9.739679
Q std: 11.605998
Actor loss: 9.743659
Action reg: 0.003980
  l1.weight: grad_norm = 0.032156
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.031952
Total gradient norm: 0.150963
=== Actor Training Debug (Iteration 3036) ===
Q mean: -7.614176
Q std: 11.176671
Actor loss: 7.618160
Action reg: 0.003985
  l1.weight: grad_norm = 0.056172
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.052390
Total gradient norm: 0.166628
=== Actor Training Debug (Iteration 3037) ===
Q mean: -9.332683
Q std: 11.611665
Actor loss: 9.336649
Action reg: 0.003966
  l1.weight: grad_norm = 0.047919
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.037855
Total gradient norm: 0.133868
=== Actor Training Debug (Iteration 3038) ===
Q mean: -8.674848
Q std: 11.275378
Actor loss: 8.678819
Action reg: 0.003971
  l1.weight: grad_norm = 0.102897
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.065345
Total gradient norm: 0.217630
=== Actor Training Debug (Iteration 3039) ===
Q mean: -8.788472
Q std: 11.356661
Actor loss: 8.792444
Action reg: 0.003972
  l1.weight: grad_norm = 0.113379
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.112523
Total gradient norm: 0.446300
=== Actor Training Debug (Iteration 3040) ===
Q mean: -8.387114
Q std: 10.748586
Actor loss: 8.391095
Action reg: 0.003982
  l1.weight: grad_norm = 0.056978
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.041005
Total gradient norm: 0.137054
=== Actor Training Debug (Iteration 3041) ===
Q mean: -7.353243
Q std: 10.483373
Actor loss: 7.357223
Action reg: 0.003980
  l1.weight: grad_norm = 0.122035
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.104859
Total gradient norm: 0.431229
=== Actor Training Debug (Iteration 3042) ===
Q mean: -8.904117
Q std: 11.663588
Actor loss: 8.908099
Action reg: 0.003983
  l1.weight: grad_norm = 0.041900
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.038281
Total gradient norm: 0.162589
=== Actor Training Debug (Iteration 3043) ===
Q mean: -6.863699
Q std: 9.954728
Actor loss: 6.867666
Action reg: 0.003967
  l1.weight: grad_norm = 0.079643
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.062875
Total gradient norm: 0.272864
=== Actor Training Debug (Iteration 3044) ===
Q mean: -8.633249
Q std: 11.671209
Actor loss: 8.637234
Action reg: 0.003984
  l1.weight: grad_norm = 0.045819
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.048877
Total gradient norm: 0.180865
=== Actor Training Debug (Iteration 3045) ===
Q mean: -8.377376
Q std: 11.048618
Actor loss: 8.381354
Action reg: 0.003979
  l1.weight: grad_norm = 0.026545
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.024446
Total gradient norm: 0.094467
=== Actor Training Debug (Iteration 3046) ===
Q mean: -8.459390
Q std: 11.392582
Actor loss: 8.463370
Action reg: 0.003981
  l1.weight: grad_norm = 0.191207
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.164625
Total gradient norm: 0.600022
=== Actor Training Debug (Iteration 3047) ===
Q mean: -9.499437
Q std: 12.267916
Actor loss: 9.503411
Action reg: 0.003974
  l1.weight: grad_norm = 0.068744
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.057320
Total gradient norm: 0.200266
=== Actor Training Debug (Iteration 3048) ===
Q mean: -8.399871
Q std: 11.352783
Actor loss: 8.403852
Action reg: 0.003982
  l1.weight: grad_norm = 0.058557
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.053670
Total gradient norm: 0.280205
=== Actor Training Debug (Iteration 3049) ===
Q mean: -8.648750
Q std: 10.967874
Actor loss: 8.652729
Action reg: 0.003979
  l1.weight: grad_norm = 0.057816
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.053290
Total gradient norm: 0.209808
=== Actor Training Debug (Iteration 3050) ===
Q mean: -8.704062
Q std: 10.905517
Actor loss: 8.708041
Action reg: 0.003979
  l1.weight: grad_norm = 0.069320
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.067323
Total gradient norm: 0.228332
=== Actor Training Debug (Iteration 3051) ===
Q mean: -9.067747
Q std: 11.444534
Actor loss: 9.071733
Action reg: 0.003987
  l1.weight: grad_norm = 0.021239
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.020073
Total gradient norm: 0.077888
=== Actor Training Debug (Iteration 3052) ===
Q mean: -9.527972
Q std: 11.754663
Actor loss: 9.531952
Action reg: 0.003979
  l1.weight: grad_norm = 0.033209
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.028738
Total gradient norm: 0.120983
=== Actor Training Debug (Iteration 3053) ===
Q mean: -8.646297
Q std: 11.423251
Actor loss: 8.650256
Action reg: 0.003958
  l1.weight: grad_norm = 0.108427
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.088191
Total gradient norm: 0.373042
=== Actor Training Debug (Iteration 3054) ===
Q mean: -10.568226
Q std: 12.578733
Actor loss: 10.572205
Action reg: 0.003979
  l1.weight: grad_norm = 0.368012
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.262486
Total gradient norm: 0.792752
=== Actor Training Debug (Iteration 3055) ===
Q mean: -7.522982
Q std: 10.425864
Actor loss: 7.526969
Action reg: 0.003987
  l1.weight: grad_norm = 0.017773
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.017061
Total gradient norm: 0.057426
=== Actor Training Debug (Iteration 3056) ===
Q mean: -7.577882
Q std: 11.163966
Actor loss: 7.581857
Action reg: 0.003975
  l1.weight: grad_norm = 0.052741
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.054082
Total gradient norm: 0.247275
=== Actor Training Debug (Iteration 3057) ===
Q mean: -7.972440
Q std: 10.475361
Actor loss: 7.976418
Action reg: 0.003978
  l1.weight: grad_norm = 0.057859
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.059963
Total gradient norm: 0.248644
=== Actor Training Debug (Iteration 3058) ===
Q mean: -9.037788
Q std: 11.822733
Actor loss: 9.041768
Action reg: 0.003980
  l1.weight: grad_norm = 0.035793
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.032073
Total gradient norm: 0.133973
=== Actor Training Debug (Iteration 3059) ===
Q mean: -9.421427
Q std: 12.455897
Actor loss: 9.425415
Action reg: 0.003989
  l1.weight: grad_norm = 0.035091
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.031280
Total gradient norm: 0.109168
=== Actor Training Debug (Iteration 3060) ===
Q mean: -8.580378
Q std: 11.498590
Actor loss: 8.584355
Action reg: 0.003978
  l1.weight: grad_norm = 0.035563
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.032499
Total gradient norm: 0.139526
=== Actor Training Debug (Iteration 3061) ===
Q mean: -9.261837
Q std: 11.160587
Actor loss: 9.265823
Action reg: 0.003987
  l1.weight: grad_norm = 0.073090
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.063833
Total gradient norm: 0.195907
=== Actor Training Debug (Iteration 3062) ===
Q mean: -9.119995
Q std: 11.537222
Actor loss: 9.123973
Action reg: 0.003978
  l1.weight: grad_norm = 0.045354
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.038475
Total gradient norm: 0.144666
=== Actor Training Debug (Iteration 3063) ===
Q mean: -10.288274
Q std: 12.520553
Actor loss: 10.292249
Action reg: 0.003975
  l1.weight: grad_norm = 0.088049
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.072260
Total gradient norm: 0.256200
=== Actor Training Debug (Iteration 3064) ===
Q mean: -7.793903
Q std: 11.251117
Actor loss: 7.797882
Action reg: 0.003978
  l1.weight: grad_norm = 0.032805
  l1.bias: grad_norm = 0.000411
  l2.weight: grad_norm = 0.030672
Total gradient norm: 0.110779
=== Actor Training Debug (Iteration 3065) ===
Q mean: -8.675331
Q std: 11.783094
Actor loss: 8.679314
Action reg: 0.003983
  l1.weight: grad_norm = 0.043337
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.031859
Total gradient norm: 0.106254
=== Actor Training Debug (Iteration 3066) ===
Q mean: -8.511703
Q std: 10.528592
Actor loss: 8.515676
Action reg: 0.003972
  l1.weight: grad_norm = 0.063647
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.053775
Total gradient norm: 0.195345
=== Actor Training Debug (Iteration 3067) ===
Q mean: -8.094281
Q std: 10.659999
Actor loss: 8.098262
Action reg: 0.003981
  l1.weight: grad_norm = 0.077724
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.068758
Total gradient norm: 0.276190
=== Actor Training Debug (Iteration 3068) ===
Q mean: -9.211979
Q std: 12.460667
Actor loss: 9.215954
Action reg: 0.003975
  l1.weight: grad_norm = 0.046483
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.036058
Total gradient norm: 0.132342
=== Actor Training Debug (Iteration 3069) ===
Q mean: -8.268490
Q std: 11.602062
Actor loss: 8.272477
Action reg: 0.003988
  l1.weight: grad_norm = 0.046747
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.038514
Total gradient norm: 0.161573
=== Actor Training Debug (Iteration 3070) ===
Q mean: -9.520609
Q std: 11.569921
Actor loss: 9.524592
Action reg: 0.003984
  l1.weight: grad_norm = 0.029731
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.026242
Total gradient norm: 0.138539
=== Actor Training Debug (Iteration 3071) ===
Q mean: -9.537805
Q std: 11.392533
Actor loss: 9.541784
Action reg: 0.003980
  l1.weight: grad_norm = 0.030806
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.032066
Total gradient norm: 0.101225
=== Actor Training Debug (Iteration 3072) ===
Q mean: -8.874929
Q std: 11.604385
Actor loss: 8.878910
Action reg: 0.003980
  l1.weight: grad_norm = 0.021600
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.019444
Total gradient norm: 0.084465
=== Actor Training Debug (Iteration 3073) ===
Q mean: -9.131222
Q std: 11.305297
Actor loss: 9.135205
Action reg: 0.003983
  l1.weight: grad_norm = 0.023031
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.021127
Total gradient norm: 0.080336
=== Actor Training Debug (Iteration 3074) ===
Q mean: -7.785109
Q std: 10.590689
Actor loss: 7.789083
Action reg: 0.003974
  l1.weight: grad_norm = 0.060059
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.052162
Total gradient norm: 0.172760
=== Actor Training Debug (Iteration 3075) ===
Q mean: -8.261789
Q std: 11.243325
Actor loss: 8.265775
Action reg: 0.003985
  l1.weight: grad_norm = 0.050654
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.047213
Total gradient norm: 0.165138
=== Actor Training Debug (Iteration 3076) ===
Q mean: -9.375139
Q std: 12.491352
Actor loss: 9.379114
Action reg: 0.003975
  l1.weight: grad_norm = 0.062513
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.054188
Total gradient norm: 0.227898
=== Actor Training Debug (Iteration 3077) ===
Q mean: -8.479212
Q std: 10.560400
Actor loss: 8.483192
Action reg: 0.003981
  l1.weight: grad_norm = 0.071693
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.066323
Total gradient norm: 0.249201
=== Actor Training Debug (Iteration 3078) ===
Q mean: -8.243701
Q std: 11.299115
Actor loss: 8.247681
Action reg: 0.003979
  l1.weight: grad_norm = 0.019668
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.017548
Total gradient norm: 0.075492
=== Actor Training Debug (Iteration 3079) ===
Q mean: -8.937364
Q std: 11.450802
Actor loss: 8.941348
Action reg: 0.003984
  l1.weight: grad_norm = 0.068540
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.060965
Total gradient norm: 0.218406
=== Actor Training Debug (Iteration 3080) ===
Q mean: -9.743694
Q std: 11.692817
Actor loss: 9.747677
Action reg: 0.003983
  l1.weight: grad_norm = 0.019989
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.017933
Total gradient norm: 0.076997
=== Actor Training Debug (Iteration 3081) ===
Q mean: -8.760690
Q std: 10.150775
Actor loss: 8.764678
Action reg: 0.003988
  l1.weight: grad_norm = 0.039650
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.035670
Total gradient norm: 0.117328
=== Actor Training Debug (Iteration 3082) ===
Q mean: -8.924670
Q std: 12.627287
Actor loss: 8.928653
Action reg: 0.003982
  l1.weight: grad_norm = 0.103204
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.090475
Total gradient norm: 0.351120
=== Actor Training Debug (Iteration 3083) ===
Q mean: -8.408800
Q std: 11.281329
Actor loss: 8.412783
Action reg: 0.003982
  l1.weight: grad_norm = 0.032922
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.030046
Total gradient norm: 0.118892
=== Actor Training Debug (Iteration 3084) ===
Q mean: -8.840811
Q std: 12.202815
Actor loss: 8.844784
Action reg: 0.003973
  l1.weight: grad_norm = 0.118438
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.091986
Total gradient norm: 0.292879
=== Actor Training Debug (Iteration 3085) ===
Q mean: -7.709600
Q std: 10.951420
Actor loss: 7.713573
Action reg: 0.003973
  l1.weight: grad_norm = 0.120144
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.074486
Total gradient norm: 0.282251
=== Actor Training Debug (Iteration 3086) ===
Q mean: -9.056884
Q std: 12.104639
Actor loss: 9.060863
Action reg: 0.003980
  l1.weight: grad_norm = 0.042033
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.037688
Total gradient norm: 0.140655
=== Actor Training Debug (Iteration 3087) ===
Q mean: -8.927131
Q std: 11.505100
Actor loss: 8.931108
Action reg: 0.003976
  l1.weight: grad_norm = 0.103099
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.094828
Total gradient norm: 0.397559
=== Actor Training Debug (Iteration 3088) ===
Q mean: -9.902277
Q std: 11.448133
Actor loss: 9.906251
Action reg: 0.003974
  l1.weight: grad_norm = 0.055060
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.044617
Total gradient norm: 0.163543
=== Actor Training Debug (Iteration 3089) ===
Q mean: -8.778787
Q std: 10.783998
Actor loss: 8.782745
Action reg: 0.003958
  l1.weight: grad_norm = 0.104538
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.073517
Total gradient norm: 0.253729
=== Actor Training Debug (Iteration 3090) ===
Q mean: -8.664553
Q std: 11.225447
Actor loss: 8.668523
Action reg: 0.003970
  l1.weight: grad_norm = 0.046535
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.040687
Total gradient norm: 0.194331
=== Actor Training Debug (Iteration 3091) ===
Q mean: -9.674328
Q std: 12.265524
Actor loss: 9.678294
Action reg: 0.003967
  l1.weight: grad_norm = 0.078784
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.073630
Total gradient norm: 0.283951
=== Actor Training Debug (Iteration 3092) ===
Q mean: -9.881163
Q std: 12.594887
Actor loss: 9.885144
Action reg: 0.003981
  l1.weight: grad_norm = 0.078609
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.062703
Total gradient norm: 0.255688
=== Actor Training Debug (Iteration 3093) ===
Q mean: -8.570369
Q std: 11.002658
Actor loss: 8.574350
Action reg: 0.003981
  l1.weight: grad_norm = 0.082129
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.068116
Total gradient norm: 0.272225
=== Actor Training Debug (Iteration 3094) ===
Q mean: -7.847768
Q std: 11.274457
Actor loss: 7.851754
Action reg: 0.003986
  l1.weight: grad_norm = 0.030771
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.027290
Total gradient norm: 0.113839
=== Actor Training Debug (Iteration 3095) ===
Q mean: -8.781593
Q std: 11.208533
Actor loss: 8.785571
Action reg: 0.003978
  l1.weight: grad_norm = 0.088630
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.083513
Total gradient norm: 0.393383
=== Actor Training Debug (Iteration 3096) ===
Q mean: -8.118026
Q std: 11.643475
Actor loss: 8.122009
Action reg: 0.003983
  l1.weight: grad_norm = 0.021824
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.018189
Total gradient norm: 0.064873
=== Actor Training Debug (Iteration 3097) ===
Q mean: -9.240862
Q std: 11.919564
Actor loss: 9.244833
Action reg: 0.003971
  l1.weight: grad_norm = 0.235756
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.188570
Total gradient norm: 0.751152
=== Actor Training Debug (Iteration 3098) ===
Q mean: -8.473824
Q std: 10.740134
Actor loss: 8.477808
Action reg: 0.003985
  l1.weight: grad_norm = 0.029806
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.025008
Total gradient norm: 0.101437
=== Actor Training Debug (Iteration 3099) ===
Q mean: -9.572281
Q std: 12.115031
Actor loss: 9.576254
Action reg: 0.003973
  l1.weight: grad_norm = 0.055746
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.050113
Total gradient norm: 0.176625
=== Actor Training Debug (Iteration 3100) ===
Q mean: -9.036490
Q std: 11.418760
Actor loss: 9.040469
Action reg: 0.003979
  l1.weight: grad_norm = 0.057761
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.056672
Total gradient norm: 0.222251
Episode 81: Steps=100, Reward=-271.194, Buffer_size=8100
=== Actor Training Debug (Iteration 3101) ===
Q mean: -9.168978
Q std: 12.091182
Actor loss: 9.172940
Action reg: 0.003963
  l1.weight: grad_norm = 0.154661
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.118218
Total gradient norm: 0.431493
=== Actor Training Debug (Iteration 3102) ===
Q mean: -9.082277
Q std: 11.803823
Actor loss: 9.086256
Action reg: 0.003979
  l1.weight: grad_norm = 0.057125
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.055666
Total gradient norm: 0.283699
=== Actor Training Debug (Iteration 3103) ===
Q mean: -9.463373
Q std: 11.799423
Actor loss: 9.467345
Action reg: 0.003972
  l1.weight: grad_norm = 0.117792
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.102901
Total gradient norm: 0.405356
=== Actor Training Debug (Iteration 3104) ===
Q mean: -8.259686
Q std: 10.907723
Actor loss: 8.263657
Action reg: 0.003971
  l1.weight: grad_norm = 0.042554
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.032642
Total gradient norm: 0.100634
=== Actor Training Debug (Iteration 3105) ===
Q mean: -8.514702
Q std: 10.710299
Actor loss: 8.518674
Action reg: 0.003972
  l1.weight: grad_norm = 0.067572
  l1.bias: grad_norm = 0.000716
  l2.weight: grad_norm = 0.064162
Total gradient norm: 0.275922
=== Actor Training Debug (Iteration 3106) ===
Q mean: -8.274340
Q std: 10.948955
Actor loss: 8.278323
Action reg: 0.003984
  l1.weight: grad_norm = 0.058646
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.055712
Total gradient norm: 0.269510
=== Actor Training Debug (Iteration 3107) ===
Q mean: -8.196009
Q std: 10.648320
Actor loss: 8.199997
Action reg: 0.003988
  l1.weight: grad_norm = 0.038404
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.036340
Total gradient norm: 0.166223
=== Actor Training Debug (Iteration 3108) ===
Q mean: -9.748552
Q std: 12.338269
Actor loss: 9.752527
Action reg: 0.003975
  l1.weight: grad_norm = 0.053769
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.037928
Total gradient norm: 0.129210
=== Actor Training Debug (Iteration 3109) ===
Q mean: -9.943109
Q std: 12.070172
Actor loss: 9.947087
Action reg: 0.003979
  l1.weight: grad_norm = 0.071901
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.054389
Total gradient norm: 0.186931
=== Actor Training Debug (Iteration 3110) ===
Q mean: -9.110656
Q std: 10.990655
Actor loss: 9.114627
Action reg: 0.003971
  l1.weight: grad_norm = 0.053293
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.047931
Total gradient norm: 0.204044
=== Actor Training Debug (Iteration 3111) ===
Q mean: -7.914647
Q std: 12.046666
Actor loss: 7.918630
Action reg: 0.003984
  l1.weight: grad_norm = 0.076193
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.061248
Total gradient norm: 0.261250
=== Actor Training Debug (Iteration 3112) ===
Q mean: -9.302710
Q std: 11.618534
Actor loss: 9.306697
Action reg: 0.003988
  l1.weight: grad_norm = 0.051628
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.043658
Total gradient norm: 0.132778
=== Actor Training Debug (Iteration 3113) ===
Q mean: -9.085199
Q std: 12.066818
Actor loss: 9.089163
Action reg: 0.003964
  l1.weight: grad_norm = 0.173327
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.139203
Total gradient norm: 0.547968
=== Actor Training Debug (Iteration 3114) ===
Q mean: -8.231921
Q std: 10.464953
Actor loss: 8.235905
Action reg: 0.003984
  l1.weight: grad_norm = 0.034145
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.029594
Total gradient norm: 0.104412
=== Actor Training Debug (Iteration 3115) ===
Q mean: -10.449575
Q std: 11.632659
Actor loss: 10.453558
Action reg: 0.003983
  l1.weight: grad_norm = 0.046408
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.039674
Total gradient norm: 0.156588
=== Actor Training Debug (Iteration 3116) ===
Q mean: -8.867575
Q std: 11.826883
Actor loss: 8.871544
Action reg: 0.003970
  l1.weight: grad_norm = 0.068768
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.058538
Total gradient norm: 0.249445
=== Actor Training Debug (Iteration 3117) ===
Q mean: -7.835468
Q std: 10.672073
Actor loss: 7.839446
Action reg: 0.003978
  l1.weight: grad_norm = 0.047126
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.044784
Total gradient norm: 0.165966
=== Actor Training Debug (Iteration 3118) ===
Q mean: -9.844876
Q std: 11.615087
Actor loss: 9.848861
Action reg: 0.003984
  l1.weight: grad_norm = 0.057025
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.049922
Total gradient norm: 0.239324
=== Actor Training Debug (Iteration 3119) ===
Q mean: -8.105062
Q std: 11.714071
Actor loss: 8.109054
Action reg: 0.003991
  l1.weight: grad_norm = 0.020824
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.018857
Total gradient norm: 0.067990
=== Actor Training Debug (Iteration 3120) ===
Q mean: -8.973557
Q std: 11.671155
Actor loss: 8.977534
Action reg: 0.003978
  l1.weight: grad_norm = 0.053661
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.048749
Total gradient norm: 0.177569
=== Actor Training Debug (Iteration 3121) ===
Q mean: -10.121360
Q std: 12.403811
Actor loss: 10.125341
Action reg: 0.003982
  l1.weight: grad_norm = 0.085944
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.071782
Total gradient norm: 0.219714
=== Actor Training Debug (Iteration 3122) ===
Q mean: -9.043503
Q std: 12.261862
Actor loss: 9.047483
Action reg: 0.003981
  l1.weight: grad_norm = 0.020354
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.016519
Total gradient norm: 0.062288
=== Actor Training Debug (Iteration 3123) ===
Q mean: -8.206403
Q std: 10.576966
Actor loss: 8.210375
Action reg: 0.003972
  l1.weight: grad_norm = 0.083070
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.060404
Total gradient norm: 0.205585
=== Actor Training Debug (Iteration 3124) ===
Q mean: -9.313953
Q std: 12.287696
Actor loss: 9.317938
Action reg: 0.003985
  l1.weight: grad_norm = 0.094316
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.079994
Total gradient norm: 0.442549
=== Actor Training Debug (Iteration 3125) ===
Q mean: -9.044503
Q std: 10.991925
Actor loss: 9.048482
Action reg: 0.003978
  l1.weight: grad_norm = 0.069395
  l1.bias: grad_norm = 0.000642
  l2.weight: grad_norm = 0.055350
Total gradient norm: 0.204816
=== Actor Training Debug (Iteration 3126) ===
Q mean: -8.976507
Q std: 11.656499
Actor loss: 8.980487
Action reg: 0.003980
  l1.weight: grad_norm = 0.042574
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.034455
Total gradient norm: 0.113302
=== Actor Training Debug (Iteration 3127) ===
Q mean: -8.976966
Q std: 11.820675
Actor loss: 8.980940
Action reg: 0.003974
  l1.weight: grad_norm = 0.087457
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.082474
Total gradient norm: 0.383806
=== Actor Training Debug (Iteration 3128) ===
Q mean: -9.034855
Q std: 11.594744
Actor loss: 9.038842
Action reg: 0.003988
  l1.weight: grad_norm = 0.044685
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.042147
Total gradient norm: 0.216692
=== Actor Training Debug (Iteration 3129) ===
Q mean: -8.072877
Q std: 11.856865
Actor loss: 8.076840
Action reg: 0.003964
  l1.weight: grad_norm = 0.058552
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.051550
Total gradient norm: 0.239359
=== Actor Training Debug (Iteration 3130) ===
Q mean: -7.875294
Q std: 10.934699
Actor loss: 7.879268
Action reg: 0.003974
  l1.weight: grad_norm = 0.065958
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.061936
Total gradient norm: 0.274033
=== Actor Training Debug (Iteration 3131) ===
Q mean: -7.730770
Q std: 11.330342
Actor loss: 7.734757
Action reg: 0.003987
  l1.weight: grad_norm = 0.052101
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.044297
Total gradient norm: 0.147896
=== Actor Training Debug (Iteration 3132) ===
Q mean: -9.888812
Q std: 11.218364
Actor loss: 9.892795
Action reg: 0.003983
  l1.weight: grad_norm = 0.026405
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.022794
Total gradient norm: 0.086573
=== Actor Training Debug (Iteration 3133) ===
Q mean: -9.157567
Q std: 11.588295
Actor loss: 9.161543
Action reg: 0.003976
  l1.weight: grad_norm = 0.057290
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.047692
Total gradient norm: 0.189860
=== Actor Training Debug (Iteration 3134) ===
Q mean: -9.404766
Q std: 11.653270
Actor loss: 9.408732
Action reg: 0.003966
  l1.weight: grad_norm = 0.195999
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.144875
Total gradient norm: 0.482419
=== Actor Training Debug (Iteration 3135) ===
Q mean: -7.988070
Q std: 11.154272
Actor loss: 7.992057
Action reg: 0.003987
  l1.weight: grad_norm = 0.040003
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.035281
Total gradient norm: 0.176484
=== Actor Training Debug (Iteration 3136) ===
Q mean: -8.038820
Q std: 11.515730
Actor loss: 8.042792
Action reg: 0.003972
  l1.weight: grad_norm = 0.042040
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.035214
Total gradient norm: 0.115857
=== Actor Training Debug (Iteration 3137) ===
Q mean: -9.157112
Q std: 11.254752
Actor loss: 9.161096
Action reg: 0.003984
  l1.weight: grad_norm = 0.050239
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.050192
Total gradient norm: 0.184400
=== Actor Training Debug (Iteration 3138) ===
Q mean: -9.548450
Q std: 11.407270
Actor loss: 9.552421
Action reg: 0.003970
  l1.weight: grad_norm = 0.134509
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.109131
Total gradient norm: 0.478851
=== Actor Training Debug (Iteration 3139) ===
Q mean: -10.344554
Q std: 12.154900
Actor loss: 10.348533
Action reg: 0.003978
  l1.weight: grad_norm = 0.077130
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.069187
Total gradient norm: 0.271018
=== Actor Training Debug (Iteration 3140) ===
Q mean: -7.840738
Q std: 10.002458
Actor loss: 7.844711
Action reg: 0.003973
  l1.weight: grad_norm = 0.110647
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.098703
Total gradient norm: 0.303578
=== Actor Training Debug (Iteration 3141) ===
Q mean: -7.673325
Q std: 11.027859
Actor loss: 7.677298
Action reg: 0.003973
  l1.weight: grad_norm = 0.082170
  l1.bias: grad_norm = 0.000438
  l2.weight: grad_norm = 0.059138
Total gradient norm: 0.219528
=== Actor Training Debug (Iteration 3142) ===
Q mean: -9.114170
Q std: 11.737591
Actor loss: 9.118145
Action reg: 0.003975
  l1.weight: grad_norm = 0.071714
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.063385
Total gradient norm: 0.248748
=== Actor Training Debug (Iteration 3143) ===
Q mean: -7.546251
Q std: 10.431699
Actor loss: 7.550226
Action reg: 0.003975
  l1.weight: grad_norm = 0.102354
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.095175
Total gradient norm: 0.375201
=== Actor Training Debug (Iteration 3144) ===
Q mean: -11.018269
Q std: 12.883971
Actor loss: 11.022246
Action reg: 0.003978
  l1.weight: grad_norm = 0.045106
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.038473
Total gradient norm: 0.130749
=== Actor Training Debug (Iteration 3145) ===
Q mean: -7.583368
Q std: 10.605147
Actor loss: 7.587340
Action reg: 0.003972
  l1.weight: grad_norm = 0.066926
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.055945
Total gradient norm: 0.237100
=== Actor Training Debug (Iteration 3146) ===
Q mean: -9.423230
Q std: 11.766115
Actor loss: 9.427210
Action reg: 0.003980
  l1.weight: grad_norm = 0.034883
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.028901
Total gradient norm: 0.099436
=== Actor Training Debug (Iteration 3147) ===
Q mean: -8.635738
Q std: 11.056554
Actor loss: 8.639718
Action reg: 0.003979
  l1.weight: grad_norm = 0.096586
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.085940
Total gradient norm: 0.368980
=== Actor Training Debug (Iteration 3148) ===
Q mean: -10.367886
Q std: 12.667061
Actor loss: 10.371862
Action reg: 0.003977
  l1.weight: grad_norm = 0.053731
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.049739
Total gradient norm: 0.182064
=== Actor Training Debug (Iteration 3149) ===
Q mean: -7.632449
Q std: 11.066076
Actor loss: 7.636426
Action reg: 0.003977
  l1.weight: grad_norm = 0.132984
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.105274
Total gradient norm: 0.358553
=== Actor Training Debug (Iteration 3150) ===
Q mean: -9.014889
Q std: 10.814348
Actor loss: 9.018850
Action reg: 0.003962
  l1.weight: grad_norm = 0.042919
  l1.bias: grad_norm = 0.000599
  l2.weight: grad_norm = 0.040221
Total gradient norm: 0.140674
=== Actor Training Debug (Iteration 3151) ===
Q mean: -7.881600
Q std: 10.520920
Actor loss: 7.885575
Action reg: 0.003974
  l1.weight: grad_norm = 0.053379
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.054537
Total gradient norm: 0.227706
=== Actor Training Debug (Iteration 3152) ===
Q mean: -8.260894
Q std: 11.133808
Actor loss: 8.264885
Action reg: 0.003991
  l1.weight: grad_norm = 0.047475
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.041990
Total gradient norm: 0.188268
=== Actor Training Debug (Iteration 3153) ===
Q mean: -8.723982
Q std: 11.662475
Actor loss: 8.727962
Action reg: 0.003981
  l1.weight: grad_norm = 0.080900
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.064577
Total gradient norm: 0.247587
=== Actor Training Debug (Iteration 3154) ===
Q mean: -9.022930
Q std: 11.364972
Actor loss: 9.026913
Action reg: 0.003983
  l1.weight: grad_norm = 0.073429
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.063349
Total gradient norm: 0.230344
=== Actor Training Debug (Iteration 3155) ===
Q mean: -8.747467
Q std: 11.223041
Actor loss: 8.751454
Action reg: 0.003988
  l1.weight: grad_norm = 0.052190
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.047029
Total gradient norm: 0.193302
=== Actor Training Debug (Iteration 3156) ===
Q mean: -8.482937
Q std: 11.436169
Actor loss: 8.486923
Action reg: 0.003987
  l1.weight: grad_norm = 0.049282
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.049402
Total gradient norm: 0.216994
=== Actor Training Debug (Iteration 3157) ===
Q mean: -8.115395
Q std: 11.814095
Actor loss: 8.119368
Action reg: 0.003973
  l1.weight: grad_norm = 0.071429
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.058286
Total gradient norm: 0.234523
=== Actor Training Debug (Iteration 3158) ===
Q mean: -7.690534
Q std: 10.263796
Actor loss: 7.694515
Action reg: 0.003981
  l1.weight: grad_norm = 0.075265
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.056838
Total gradient norm: 0.221315
=== Actor Training Debug (Iteration 3159) ===
Q mean: -9.616848
Q std: 12.961649
Actor loss: 9.620830
Action reg: 0.003981
  l1.weight: grad_norm = 0.047629
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.040964
Total gradient norm: 0.157453
=== Actor Training Debug (Iteration 3160) ===
Q mean: -8.463222
Q std: 10.847399
Actor loss: 8.467199
Action reg: 0.003977
  l1.weight: grad_norm = 0.080815
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.072036
Total gradient norm: 0.237803
=== Actor Training Debug (Iteration 3161) ===
Q mean: -8.034768
Q std: 9.827147
Actor loss: 8.038756
Action reg: 0.003988
  l1.weight: grad_norm = 0.093564
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.079329
Total gradient norm: 0.238060
=== Actor Training Debug (Iteration 3162) ===
Q mean: -9.206728
Q std: 11.457302
Actor loss: 9.210712
Action reg: 0.003985
  l1.weight: grad_norm = 0.060712
  l1.bias: grad_norm = 0.000365
  l2.weight: grad_norm = 0.046065
Total gradient norm: 0.163509
=== Actor Training Debug (Iteration 3163) ===
Q mean: -9.666697
Q std: 12.067199
Actor loss: 9.670680
Action reg: 0.003984
  l1.weight: grad_norm = 0.024419
  l1.bias: grad_norm = 0.000674
  l2.weight: grad_norm = 0.022989
Total gradient norm: 0.107930
=== Actor Training Debug (Iteration 3164) ===
Q mean: -10.146823
Q std: 13.194270
Actor loss: 10.150803
Action reg: 0.003980
  l1.weight: grad_norm = 0.081077
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.072265
Total gradient norm: 0.316420
=== Actor Training Debug (Iteration 3165) ===
Q mean: -9.538639
Q std: 12.324418
Actor loss: 9.542621
Action reg: 0.003982
  l1.weight: grad_norm = 0.018654
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.015076
Total gradient norm: 0.059504
=== Actor Training Debug (Iteration 3166) ===
Q mean: -8.172633
Q std: 11.929424
Actor loss: 8.176612
Action reg: 0.003979
  l1.weight: grad_norm = 0.036082
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.032025
Total gradient norm: 0.109045
=== Actor Training Debug (Iteration 3167) ===
Q mean: -9.365941
Q std: 11.981835
Actor loss: 9.369913
Action reg: 0.003972
  l1.weight: grad_norm = 0.047255
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.040027
Total gradient norm: 0.131004
=== Actor Training Debug (Iteration 3168) ===
Q mean: -9.494432
Q std: 11.415517
Actor loss: 9.498417
Action reg: 0.003985
  l1.weight: grad_norm = 0.077742
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.058634
Total gradient norm: 0.210717
=== Actor Training Debug (Iteration 3169) ===
Q mean: -9.494129
Q std: 12.435090
Actor loss: 9.498096
Action reg: 0.003966
  l1.weight: grad_norm = 0.034966
  l1.bias: grad_norm = 0.000917
  l2.weight: grad_norm = 0.033798
Total gradient norm: 0.164817
=== Actor Training Debug (Iteration 3170) ===
Q mean: -8.331788
Q std: 11.844988
Actor loss: 8.335750
Action reg: 0.003961
  l1.weight: grad_norm = 0.067211
  l1.bias: grad_norm = 0.000505
  l2.weight: grad_norm = 0.057787
Total gradient norm: 0.240950
=== Actor Training Debug (Iteration 3171) ===
Q mean: -9.056084
Q std: 12.715205
Actor loss: 9.060066
Action reg: 0.003983
  l1.weight: grad_norm = 0.016000
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.013552
Total gradient norm: 0.046847
=== Actor Training Debug (Iteration 3172) ===
Q mean: -9.699838
Q std: 11.961179
Actor loss: 9.703803
Action reg: 0.003966
  l1.weight: grad_norm = 0.137139
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.100954
Total gradient norm: 0.321277
=== Actor Training Debug (Iteration 3173) ===
Q mean: -9.295138
Q std: 12.208418
Actor loss: 9.299109
Action reg: 0.003970
  l1.weight: grad_norm = 0.089249
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.074161
Total gradient norm: 0.328300
=== Actor Training Debug (Iteration 3174) ===
Q mean: -10.858636
Q std: 13.009811
Actor loss: 10.862614
Action reg: 0.003978
  l1.weight: grad_norm = 0.044048
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.034150
Total gradient norm: 0.128992
=== Actor Training Debug (Iteration 3175) ===
Q mean: -8.416065
Q std: 11.531659
Actor loss: 8.420046
Action reg: 0.003981
  l1.weight: grad_norm = 0.060077
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.050811
Total gradient norm: 0.166474
=== Actor Training Debug (Iteration 3176) ===
Q mean: -7.724639
Q std: 10.982690
Actor loss: 7.728616
Action reg: 0.003977
  l1.weight: grad_norm = 0.095545
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.085082
Total gradient norm: 0.397037
=== Actor Training Debug (Iteration 3177) ===
Q mean: -8.997418
Q std: 11.179933
Actor loss: 9.001395
Action reg: 0.003977
  l1.weight: grad_norm = 0.068688
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.057234
Total gradient norm: 0.298231
=== Actor Training Debug (Iteration 3178) ===
Q mean: -11.026405
Q std: 12.720592
Actor loss: 11.030375
Action reg: 0.003970
  l1.weight: grad_norm = 0.123325
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.114102
Total gradient norm: 0.389083
=== Actor Training Debug (Iteration 3179) ===
Q mean: -9.336258
Q std: 12.155102
Actor loss: 9.340232
Action reg: 0.003974
  l1.weight: grad_norm = 0.069908
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.070116
Total gradient norm: 0.367573
=== Actor Training Debug (Iteration 3180) ===
Q mean: -10.323231
Q std: 13.139758
Actor loss: 10.327222
Action reg: 0.003991
  l1.weight: grad_norm = 0.032124
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.030132
Total gradient norm: 0.145376
=== Actor Training Debug (Iteration 3181) ===
Q mean: -9.303770
Q std: 12.962966
Actor loss: 9.307755
Action reg: 0.003985
  l1.weight: grad_norm = 0.017538
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.015043
Total gradient norm: 0.067582
=== Actor Training Debug (Iteration 3182) ===
Q mean: -8.806765
Q std: 10.885159
Actor loss: 8.810752
Action reg: 0.003987
  l1.weight: grad_norm = 0.024699
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.020969
Total gradient norm: 0.083762
=== Actor Training Debug (Iteration 3183) ===
Q mean: -10.971323
Q std: 12.734778
Actor loss: 10.975293
Action reg: 0.003970
  l1.weight: grad_norm = 0.084437
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.074240
Total gradient norm: 0.320436
=== Actor Training Debug (Iteration 3184) ===
Q mean: -8.840154
Q std: 11.265718
Actor loss: 8.844133
Action reg: 0.003979
  l1.weight: grad_norm = 0.071058
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.066380
Total gradient norm: 0.245677
=== Actor Training Debug (Iteration 3185) ===
Q mean: -6.615915
Q std: 10.529922
Actor loss: 6.619901
Action reg: 0.003986
  l1.weight: grad_norm = 0.072427
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.069578
Total gradient norm: 0.254588
=== Actor Training Debug (Iteration 3186) ===
Q mean: -9.805369
Q std: 11.369078
Actor loss: 9.809355
Action reg: 0.003986
  l1.weight: grad_norm = 0.087081
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.076419
Total gradient norm: 0.299244
=== Actor Training Debug (Iteration 3187) ===
Q mean: -10.289249
Q std: 12.556435
Actor loss: 10.293236
Action reg: 0.003986
  l1.weight: grad_norm = 0.035216
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.031261
Total gradient norm: 0.108070
=== Actor Training Debug (Iteration 3188) ===
Q mean: -9.776182
Q std: 12.494482
Actor loss: 9.780162
Action reg: 0.003979
  l1.weight: grad_norm = 0.105217
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.079781
Total gradient norm: 0.336428
=== Actor Training Debug (Iteration 3189) ===
Q mean: -9.580291
Q std: 11.870931
Actor loss: 9.584277
Action reg: 0.003986
  l1.weight: grad_norm = 0.114278
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.085508
Total gradient norm: 0.266461
=== Actor Training Debug (Iteration 3190) ===
Q mean: -9.318687
Q std: 12.318999
Actor loss: 9.322650
Action reg: 0.003963
  l1.weight: grad_norm = 0.063714
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.058648
Total gradient norm: 0.223213
=== Actor Training Debug (Iteration 3191) ===
Q mean: -9.140633
Q std: 12.236017
Actor loss: 9.144604
Action reg: 0.003971
  l1.weight: grad_norm = 0.070383
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.055196
Total gradient norm: 0.205951
=== Actor Training Debug (Iteration 3192) ===
Q mean: -9.142035
Q std: 12.354467
Actor loss: 9.146012
Action reg: 0.003977
  l1.weight: grad_norm = 0.018624
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.017938
Total gradient norm: 0.070814
=== Actor Training Debug (Iteration 3193) ===
Q mean: -9.957279
Q std: 12.789357
Actor loss: 9.961262
Action reg: 0.003983
  l1.weight: grad_norm = 0.027846
  l1.bias: grad_norm = 0.000557
  l2.weight: grad_norm = 0.022916
Total gradient norm: 0.085098
=== Actor Training Debug (Iteration 3194) ===
Q mean: -9.253572
Q std: 11.783668
Actor loss: 9.257560
Action reg: 0.003987
  l1.weight: grad_norm = 0.044165
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.038499
Total gradient norm: 0.131495
=== Actor Training Debug (Iteration 3195) ===
Q mean: -8.513839
Q std: 11.301107
Actor loss: 8.517832
Action reg: 0.003993
  l1.weight: grad_norm = 0.038555
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.034685
Total gradient norm: 0.118306
=== Actor Training Debug (Iteration 3196) ===
Q mean: -8.843048
Q std: 11.809616
Actor loss: 8.847023
Action reg: 0.003975
  l1.weight: grad_norm = 0.039595
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.038689
Total gradient norm: 0.149879
=== Actor Training Debug (Iteration 3197) ===
Q mean: -9.071303
Q std: 12.418628
Actor loss: 9.075290
Action reg: 0.003986
  l1.weight: grad_norm = 0.010858
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.009156
Total gradient norm: 0.031924
=== Actor Training Debug (Iteration 3198) ===
Q mean: -9.582489
Q std: 12.758221
Actor loss: 9.586467
Action reg: 0.003977
  l1.weight: grad_norm = 0.021680
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.021613
Total gradient norm: 0.100882
=== Actor Training Debug (Iteration 3199) ===
Q mean: -7.898438
Q std: 10.997211
Actor loss: 7.902411
Action reg: 0.003973
  l1.weight: grad_norm = 0.061632
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.063039
Total gradient norm: 0.246227
=== Actor Training Debug (Iteration 3200) ===
Q mean: -9.837841
Q std: 11.665997
Actor loss: 9.841818
Action reg: 0.003976
  l1.weight: grad_norm = 0.070754
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.062184
Total gradient norm: 0.216378
=== Actor Training Debug (Iteration 3201) ===
Q mean: -9.760241
Q std: 12.370630
Actor loss: 9.764211
Action reg: 0.003970
  l1.weight: grad_norm = 0.104665
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.096761
Total gradient norm: 0.329245
=== Actor Training Debug (Iteration 3202) ===
Q mean: -7.614449
Q std: 9.999374
Actor loss: 7.618431
Action reg: 0.003982
  l1.weight: grad_norm = 0.114456
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.099307
Total gradient norm: 0.323510
=== Actor Training Debug (Iteration 3203) ===
Q mean: -9.631405
Q std: 12.966749
Actor loss: 9.635397
Action reg: 0.003992
  l1.weight: grad_norm = 0.026303
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.022182
Total gradient norm: 0.087613
=== Actor Training Debug (Iteration 3204) ===
Q mean: -8.359890
Q std: 11.397875
Actor loss: 8.363873
Action reg: 0.003984
  l1.weight: grad_norm = 0.085410
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.071584
Total gradient norm: 0.282949
=== Actor Training Debug (Iteration 3205) ===
Q mean: -8.898367
Q std: 12.103580
Actor loss: 8.902346
Action reg: 0.003978
  l1.weight: grad_norm = 0.172074
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.122868
Total gradient norm: 0.403961
=== Actor Training Debug (Iteration 3206) ===
Q mean: -8.591646
Q std: 10.682735
Actor loss: 8.595638
Action reg: 0.003992
  l1.weight: grad_norm = 0.027822
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.025781
Total gradient norm: 0.097111
=== Actor Training Debug (Iteration 3207) ===
Q mean: -8.802152
Q std: 11.719348
Actor loss: 8.806118
Action reg: 0.003966
  l1.weight: grad_norm = 0.091539
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.074088
Total gradient norm: 0.265384
=== Actor Training Debug (Iteration 3208) ===
Q mean: -7.849360
Q std: 10.325727
Actor loss: 7.853339
Action reg: 0.003979
  l1.weight: grad_norm = 0.081578
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.077645
Total gradient norm: 0.295345
=== Actor Training Debug (Iteration 3209) ===
Q mean: -9.207998
Q std: 11.229933
Actor loss: 9.211978
Action reg: 0.003979
  l1.weight: grad_norm = 0.043435
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.037432
Total gradient norm: 0.137024
=== Actor Training Debug (Iteration 3210) ===
Q mean: -8.662354
Q std: 11.210068
Actor loss: 8.666328
Action reg: 0.003975
  l1.weight: grad_norm = 0.046399
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.051008
Total gradient norm: 0.229081
=== Actor Training Debug (Iteration 3211) ===
Q mean: -8.592756
Q std: 10.947556
Actor loss: 8.596744
Action reg: 0.003987
  l1.weight: grad_norm = 0.039146
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.031656
Total gradient norm: 0.099659
=== Actor Training Debug (Iteration 3212) ===
Q mean: -8.912991
Q std: 11.559414
Actor loss: 8.916978
Action reg: 0.003987
  l1.weight: grad_norm = 0.047143
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.039311
Total gradient norm: 0.119727
=== Actor Training Debug (Iteration 3213) ===
Q mean: -9.418272
Q std: 11.855420
Actor loss: 9.422251
Action reg: 0.003979
  l1.weight: grad_norm = 0.050380
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.043798
Total gradient norm: 0.158069
=== Actor Training Debug (Iteration 3214) ===
Q mean: -9.648083
Q std: 12.028069
Actor loss: 9.652064
Action reg: 0.003982
  l1.weight: grad_norm = 0.040454
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.030241
Total gradient norm: 0.099622
=== Actor Training Debug (Iteration 3215) ===
Q mean: -9.735057
Q std: 12.663014
Actor loss: 9.739037
Action reg: 0.003980
  l1.weight: grad_norm = 0.052440
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.048918
Total gradient norm: 0.169067
=== Actor Training Debug (Iteration 3216) ===
Q mean: -9.762987
Q std: 11.320020
Actor loss: 9.766964
Action reg: 0.003976
  l1.weight: grad_norm = 0.123760
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.099523
Total gradient norm: 0.339297
=== Actor Training Debug (Iteration 3217) ===
Q mean: -9.375206
Q std: 10.742949
Actor loss: 9.379189
Action reg: 0.003982
  l1.weight: grad_norm = 0.035371
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.029968
Total gradient norm: 0.107224
=== Actor Training Debug (Iteration 3218) ===
Q mean: -9.480473
Q std: 11.819445
Actor loss: 9.484441
Action reg: 0.003968
  l1.weight: grad_norm = 0.122623
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.090375
Total gradient norm: 0.310086
=== Actor Training Debug (Iteration 3219) ===
Q mean: -9.152048
Q std: 12.134606
Actor loss: 9.156032
Action reg: 0.003983
  l1.weight: grad_norm = 0.099051
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.083731
Total gradient norm: 0.301855
=== Actor Training Debug (Iteration 3220) ===
Q mean: -10.146793
Q std: 12.888117
Actor loss: 10.150772
Action reg: 0.003978
  l1.weight: grad_norm = 0.032522
  l1.bias: grad_norm = 0.000449
  l2.weight: grad_norm = 0.027010
Total gradient norm: 0.108895
=== Actor Training Debug (Iteration 3221) ===
Q mean: -9.982960
Q std: 12.496573
Actor loss: 9.986934
Action reg: 0.003974
  l1.weight: grad_norm = 0.074060
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.072962
Total gradient norm: 0.286777
=== Actor Training Debug (Iteration 3222) ===
Q mean: -7.899457
Q std: 11.514838
Actor loss: 7.903434
Action reg: 0.003976
  l1.weight: grad_norm = 0.113416
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.099912
Total gradient norm: 0.468343
=== Actor Training Debug (Iteration 3223) ===
Q mean: -9.703445
Q std: 11.919103
Actor loss: 9.707433
Action reg: 0.003988
  l1.weight: grad_norm = 0.031995
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.028808
Total gradient norm: 0.095936
=== Actor Training Debug (Iteration 3224) ===
Q mean: -8.304678
Q std: 11.456608
Actor loss: 8.308648
Action reg: 0.003970
  l1.weight: grad_norm = 0.065466
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.056056
Total gradient norm: 0.154417
=== Actor Training Debug (Iteration 3225) ===
Q mean: -8.877740
Q std: 10.874406
Actor loss: 8.881721
Action reg: 0.003981
  l1.weight: grad_norm = 0.065285
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.059547
Total gradient norm: 0.235365
=== Actor Training Debug (Iteration 3226) ===
Q mean: -8.307737
Q std: 11.090178
Actor loss: 8.311720
Action reg: 0.003982
  l1.weight: grad_norm = 0.065137
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.058571
Total gradient norm: 0.172528
=== Actor Training Debug (Iteration 3227) ===
Q mean: -8.990438
Q std: 11.627141
Actor loss: 8.994407
Action reg: 0.003969
  l1.weight: grad_norm = 0.062890
  l1.bias: grad_norm = 0.000546
  l2.weight: grad_norm = 0.055883
Total gradient norm: 0.217198
=== Actor Training Debug (Iteration 3228) ===
Q mean: -8.052046
Q std: 11.660082
Actor loss: 8.056029
Action reg: 0.003983
  l1.weight: grad_norm = 0.066305
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.062708
Total gradient norm: 0.221518
=== Actor Training Debug (Iteration 3229) ===
Q mean: -8.772127
Q std: 11.537391
Actor loss: 8.776107
Action reg: 0.003980
  l1.weight: grad_norm = 0.034030
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.029858
Total gradient norm: 0.105761
=== Actor Training Debug (Iteration 3230) ===
Q mean: -8.670102
Q std: 11.678419
Actor loss: 8.674076
Action reg: 0.003974
  l1.weight: grad_norm = 0.037455
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.032831
Total gradient norm: 0.134235
=== Actor Training Debug (Iteration 3231) ===
Q mean: -8.905897
Q std: 11.478749
Actor loss: 8.909878
Action reg: 0.003980
  l1.weight: grad_norm = 0.051030
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.053932
Total gradient norm: 0.183988
=== Actor Training Debug (Iteration 3232) ===
Q mean: -9.400988
Q std: 12.065783
Actor loss: 9.404970
Action reg: 0.003983
  l1.weight: grad_norm = 0.057260
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.047565
Total gradient norm: 0.144475
=== Actor Training Debug (Iteration 3233) ===
Q mean: -8.955776
Q std: 12.207171
Actor loss: 8.959754
Action reg: 0.003977
  l1.weight: grad_norm = 0.070639
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.063635
Total gradient norm: 0.262404
=== Actor Training Debug (Iteration 3234) ===
Q mean: -8.756846
Q std: 11.582816
Actor loss: 8.760819
Action reg: 0.003973
  l1.weight: grad_norm = 0.023555
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.020548
Total gradient norm: 0.083688
=== Actor Training Debug (Iteration 3235) ===
Q mean: -8.102859
Q std: 10.146815
Actor loss: 8.106831
Action reg: 0.003971
  l1.weight: grad_norm = 0.095378
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.078276
Total gradient norm: 0.285955
=== Actor Training Debug (Iteration 3236) ===
Q mean: -8.696990
Q std: 11.410013
Actor loss: 8.700966
Action reg: 0.003976
  l1.weight: grad_norm = 0.082002
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.069256
Total gradient norm: 0.275104
=== Actor Training Debug (Iteration 3237) ===
Q mean: -9.597629
Q std: 12.537997
Actor loss: 9.601618
Action reg: 0.003989
  l1.weight: grad_norm = 0.033277
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.027930
Total gradient norm: 0.091353
=== Actor Training Debug (Iteration 3238) ===
Q mean: -9.052646
Q std: 11.833386
Actor loss: 9.056623
Action reg: 0.003977
  l1.weight: grad_norm = 0.090425
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.078350
Total gradient norm: 0.305132
=== Actor Training Debug (Iteration 3239) ===
Q mean: -9.212502
Q std: 11.286844
Actor loss: 9.216487
Action reg: 0.003985
  l1.weight: grad_norm = 0.066088
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.050658
Total gradient norm: 0.199184
=== Actor Training Debug (Iteration 3240) ===
Q mean: -8.325089
Q std: 10.941758
Actor loss: 8.329060
Action reg: 0.003971
  l1.weight: grad_norm = 0.037709
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.031453
Total gradient norm: 0.117388
=== Actor Training Debug (Iteration 3241) ===
Q mean: -10.844700
Q std: 12.589509
Actor loss: 10.848685
Action reg: 0.003985
  l1.weight: grad_norm = 0.072773
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.063301
Total gradient norm: 0.227727
=== Actor Training Debug (Iteration 3242) ===
Q mean: -7.968585
Q std: 10.875545
Actor loss: 7.972562
Action reg: 0.003977
  l1.weight: grad_norm = 0.033319
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.028661
Total gradient norm: 0.103390
=== Actor Training Debug (Iteration 3243) ===
Q mean: -10.081476
Q std: 13.003319
Actor loss: 10.085462
Action reg: 0.003985
  l1.weight: grad_norm = 0.050904
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.043769
Total gradient norm: 0.158893
=== Actor Training Debug (Iteration 3244) ===
Q mean: -10.020321
Q std: 12.825008
Actor loss: 10.024312
Action reg: 0.003991
  l1.weight: grad_norm = 0.057500
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.050598
Total gradient norm: 0.220365
=== Actor Training Debug (Iteration 3245) ===
Q mean: -10.241137
Q std: 12.699694
Actor loss: 10.245114
Action reg: 0.003978
  l1.weight: grad_norm = 0.045079
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.037600
Total gradient norm: 0.126421
=== Actor Training Debug (Iteration 3246) ===
Q mean: -9.685761
Q std: 12.612547
Actor loss: 9.689742
Action reg: 0.003980
  l1.weight: grad_norm = 0.053465
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.039015
Total gradient norm: 0.136744
=== Actor Training Debug (Iteration 3247) ===
Q mean: -8.987965
Q std: 11.231585
Actor loss: 8.991942
Action reg: 0.003977
  l1.weight: grad_norm = 0.121972
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.108886
Total gradient norm: 0.366385
=== Actor Training Debug (Iteration 3248) ===
Q mean: -9.807344
Q std: 12.432943
Actor loss: 9.811322
Action reg: 0.003978
  l1.weight: grad_norm = 0.083524
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.066579
Total gradient norm: 0.272526
=== Actor Training Debug (Iteration 3249) ===
Q mean: -8.740490
Q std: 10.835944
Actor loss: 8.744463
Action reg: 0.003973
  l1.weight: grad_norm = 0.051923
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.046879
Total gradient norm: 0.178304
=== Actor Training Debug (Iteration 3250) ===
Q mean: -8.499533
Q std: 11.878336
Actor loss: 8.503513
Action reg: 0.003981
  l1.weight: grad_norm = 0.032784
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.028839
Total gradient norm: 0.119146
=== Actor Training Debug (Iteration 3251) ===
Q mean: -9.868457
Q std: 13.179496
Actor loss: 9.872436
Action reg: 0.003979
  l1.weight: grad_norm = 0.071912
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.060558
Total gradient norm: 0.229332
=== Actor Training Debug (Iteration 3252) ===
Q mean: -9.067258
Q std: 12.117826
Actor loss: 9.071242
Action reg: 0.003984
  l1.weight: grad_norm = 0.034204
  l1.bias: grad_norm = 0.000020
  l2.weight: grad_norm = 0.028873
Total gradient norm: 0.108081
=== Actor Training Debug (Iteration 3253) ===
Q mean: -9.500784
Q std: 11.357849
Actor loss: 9.504756
Action reg: 0.003972
  l1.weight: grad_norm = 0.038836
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.032285
Total gradient norm: 0.131183
=== Actor Training Debug (Iteration 3254) ===
Q mean: -9.414352
Q std: 11.585749
Actor loss: 9.418340
Action reg: 0.003987
  l1.weight: grad_norm = 0.054623
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.048630
Total gradient norm: 0.215531
=== Actor Training Debug (Iteration 3255) ===
Q mean: -10.064480
Q std: 11.896467
Actor loss: 10.068457
Action reg: 0.003977
  l1.weight: grad_norm = 0.036119
  l1.bias: grad_norm = 0.000513
  l2.weight: grad_norm = 0.030820
Total gradient norm: 0.110934
=== Actor Training Debug (Iteration 3256) ===
Q mean: -7.680512
Q std: 11.089688
Actor loss: 7.684492
Action reg: 0.003979
  l1.weight: grad_norm = 0.082430
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.065971
Total gradient norm: 0.209250
=== Actor Training Debug (Iteration 3257) ===
Q mean: -9.539494
Q std: 11.702437
Actor loss: 9.543481
Action reg: 0.003987
  l1.weight: grad_norm = 0.153992
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.112242
Total gradient norm: 0.301797
=== Actor Training Debug (Iteration 3258) ===
Q mean: -8.927445
Q std: 11.998313
Actor loss: 8.931417
Action reg: 0.003972
  l1.weight: grad_norm = 0.064340
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.063774
Total gradient norm: 0.246675
=== Actor Training Debug (Iteration 3259) ===
Q mean: -9.992556
Q std: 11.835513
Actor loss: 9.996532
Action reg: 0.003977
  l1.weight: grad_norm = 0.055959
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.045150
Total gradient norm: 0.160579
=== Actor Training Debug (Iteration 3260) ===
Q mean: -9.682182
Q std: 12.173398
Actor loss: 9.686172
Action reg: 0.003990
  l1.weight: grad_norm = 0.056086
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.050077
Total gradient norm: 0.193316
=== Actor Training Debug (Iteration 3261) ===
Q mean: -9.718245
Q std: 12.213254
Actor loss: 9.722232
Action reg: 0.003988
  l1.weight: grad_norm = 0.030823
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.025003
Total gradient norm: 0.090372
=== Actor Training Debug (Iteration 3262) ===
Q mean: -8.594366
Q std: 11.415437
Actor loss: 8.598351
Action reg: 0.003985
  l1.weight: grad_norm = 0.133666
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.105108
Total gradient norm: 0.372442
=== Actor Training Debug (Iteration 3263) ===
Q mean: -8.376772
Q std: 10.651869
Actor loss: 8.380746
Action reg: 0.003974
  l1.weight: grad_norm = 0.080980
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.054572
Total gradient norm: 0.162516
=== Actor Training Debug (Iteration 3264) ===
Q mean: -10.155035
Q std: 12.652810
Actor loss: 10.159022
Action reg: 0.003987
  l1.weight: grad_norm = 0.069597
  l1.bias: grad_norm = 0.000035
  l2.weight: grad_norm = 0.059426
Total gradient norm: 0.244584
=== Actor Training Debug (Iteration 3265) ===
Q mean: -8.984454
Q std: 12.438969
Actor loss: 8.988432
Action reg: 0.003978
  l1.weight: grad_norm = 0.068361
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.057551
Total gradient norm: 0.198201
=== Actor Training Debug (Iteration 3266) ===
Q mean: -8.891695
Q std: 11.216296
Actor loss: 8.895676
Action reg: 0.003980
  l1.weight: grad_norm = 0.106464
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.086714
Total gradient norm: 0.325371
=== Actor Training Debug (Iteration 3267) ===
Q mean: -8.432507
Q std: 11.884532
Actor loss: 8.436482
Action reg: 0.003975
  l1.weight: grad_norm = 0.070988
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.053513
Total gradient norm: 0.208775
=== Actor Training Debug (Iteration 3268) ===
Q mean: -9.023124
Q std: 12.557355
Actor loss: 9.027118
Action reg: 0.003994
  l1.weight: grad_norm = 0.034931
  l1.bias: grad_norm = 0.000012
  l2.weight: grad_norm = 0.033952
Total gradient norm: 0.138850
=== Actor Training Debug (Iteration 3269) ===
Q mean: -9.790106
Q std: 12.800498
Actor loss: 9.794093
Action reg: 0.003987
  l1.weight: grad_norm = 0.045269
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.032164
Total gradient norm: 0.103698
=== Actor Training Debug (Iteration 3270) ===
Q mean: -9.630323
Q std: 11.783017
Actor loss: 9.634308
Action reg: 0.003985
  l1.weight: grad_norm = 0.062499
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.049196
Total gradient norm: 0.169912
=== Actor Training Debug (Iteration 3271) ===
Q mean: -10.404312
Q std: 12.737030
Actor loss: 10.408285
Action reg: 0.003973
  l1.weight: grad_norm = 0.041869
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.037709
Total gradient norm: 0.156019
=== Actor Training Debug (Iteration 3272) ===
Q mean: -9.959138
Q std: 12.125255
Actor loss: 9.963127
Action reg: 0.003989
  l1.weight: grad_norm = 0.031278
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.028533
Total gradient norm: 0.102660
=== Actor Training Debug (Iteration 3273) ===
Q mean: -8.333859
Q std: 11.484181
Actor loss: 8.337834
Action reg: 0.003975
  l1.weight: grad_norm = 0.041035
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.030365
Total gradient norm: 0.099865
=== Actor Training Debug (Iteration 3274) ===
Q mean: -9.296802
Q std: 12.865947
Actor loss: 9.300777
Action reg: 0.003976
  l1.weight: grad_norm = 0.042732
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.037513
Total gradient norm: 0.132636
=== Actor Training Debug (Iteration 3275) ===
Q mean: -9.071998
Q std: 11.911982
Actor loss: 9.075983
Action reg: 0.003985
  l1.weight: grad_norm = 0.034424
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.026911
Total gradient norm: 0.094133
=== Actor Training Debug (Iteration 3276) ===
Q mean: -8.988173
Q std: 12.267528
Actor loss: 8.992153
Action reg: 0.003981
  l1.weight: grad_norm = 0.057749
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.054075
Total gradient norm: 0.246058
=== Actor Training Debug (Iteration 3277) ===
Q mean: -8.770351
Q std: 12.025554
Actor loss: 8.774326
Action reg: 0.003975
  l1.weight: grad_norm = 0.047879
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.043290
Total gradient norm: 0.141440
=== Actor Training Debug (Iteration 3278) ===
Q mean: -9.776409
Q std: 11.780549
Actor loss: 9.780383
Action reg: 0.003974
  l1.weight: grad_norm = 0.031899
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.024354
Total gradient norm: 0.099161
=== Actor Training Debug (Iteration 3279) ===
Q mean: -8.247549
Q std: 10.194210
Actor loss: 8.251532
Action reg: 0.003983
  l1.weight: grad_norm = 0.048793
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.045915
Total gradient norm: 0.175628
=== Actor Training Debug (Iteration 3280) ===
Q mean: -9.109310
Q std: 11.761312
Actor loss: 9.113285
Action reg: 0.003975
  l1.weight: grad_norm = 0.031144
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.026936
Total gradient norm: 0.098965
=== Actor Training Debug (Iteration 3281) ===
Q mean: -8.638372
Q std: 11.180912
Actor loss: 8.642356
Action reg: 0.003983
  l1.weight: grad_norm = 0.048745
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.040807
Total gradient norm: 0.146206
=== Actor Training Debug (Iteration 3282) ===
Q mean: -8.272298
Q std: 10.836294
Actor loss: 8.276261
Action reg: 0.003963
  l1.weight: grad_norm = 0.080111
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.065156
Total gradient norm: 0.245841
=== Actor Training Debug (Iteration 3283) ===
Q mean: -10.426582
Q std: 12.683047
Actor loss: 10.430565
Action reg: 0.003982
  l1.weight: grad_norm = 0.015878
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.014855
Total gradient norm: 0.056765
=== Actor Training Debug (Iteration 3284) ===
Q mean: -9.058094
Q std: 11.417799
Actor loss: 9.062071
Action reg: 0.003977
  l1.weight: grad_norm = 0.058392
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.054204
Total gradient norm: 0.222060
=== Actor Training Debug (Iteration 3285) ===
Q mean: -8.365540
Q std: 11.581061
Actor loss: 8.369512
Action reg: 0.003972
  l1.weight: grad_norm = 0.044045
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.041597
Total gradient norm: 0.159689
=== Actor Training Debug (Iteration 3286) ===
Q mean: -9.212820
Q std: 11.418762
Actor loss: 9.216787
Action reg: 0.003968
  l1.weight: grad_norm = 0.091017
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.083060
Total gradient norm: 0.338540
=== Actor Training Debug (Iteration 3287) ===
Q mean: -9.864206
Q std: 12.839589
Actor loss: 9.868181
Action reg: 0.003975
  l1.weight: grad_norm = 0.063500
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.059201
Total gradient norm: 0.243945
=== Actor Training Debug (Iteration 3288) ===
Q mean: -11.027039
Q std: 13.395143
Actor loss: 11.031013
Action reg: 0.003974
  l1.weight: grad_norm = 0.050383
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.035301
Total gradient norm: 0.117598
=== Actor Training Debug (Iteration 3289) ===
Q mean: -8.858974
Q std: 11.346915
Actor loss: 8.862953
Action reg: 0.003979
  l1.weight: grad_norm = 0.055725
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.041574
Total gradient norm: 0.174664
=== Actor Training Debug (Iteration 3290) ===
Q mean: -10.622434
Q std: 13.127189
Actor loss: 10.626411
Action reg: 0.003978
  l1.weight: grad_norm = 0.084931
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.071500
Total gradient norm: 0.271982
=== Actor Training Debug (Iteration 3291) ===
Q mean: -9.544727
Q std: 10.977619
Actor loss: 9.548699
Action reg: 0.003972
  l1.weight: grad_norm = 0.057882
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.043779
Total gradient norm: 0.148009
=== Actor Training Debug (Iteration 3292) ===
Q mean: -9.750711
Q std: 11.705897
Actor loss: 9.754694
Action reg: 0.003982
  l1.weight: grad_norm = 0.063824
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.047545
Total gradient norm: 0.179262
=== Actor Training Debug (Iteration 3293) ===
Q mean: -9.719185
Q std: 12.870173
Actor loss: 9.723169
Action reg: 0.003984
  l1.weight: grad_norm = 0.043176
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.035487
Total gradient norm: 0.129689
=== Actor Training Debug (Iteration 3294) ===
Q mean: -11.597340
Q std: 12.582492
Actor loss: 11.601322
Action reg: 0.003983
  l1.weight: grad_norm = 0.056201
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.045587
Total gradient norm: 0.184415
=== Actor Training Debug (Iteration 3295) ===
Q mean: -10.623035
Q std: 12.709266
Actor loss: 10.627025
Action reg: 0.003990
  l1.weight: grad_norm = 0.077206
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.059857
Total gradient norm: 0.225626
=== Actor Training Debug (Iteration 3296) ===
Q mean: -8.310317
Q std: 12.026875
Actor loss: 8.314304
Action reg: 0.003987
  l1.weight: grad_norm = 0.020912
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.015636
Total gradient norm: 0.052778
=== Actor Training Debug (Iteration 3297) ===
Q mean: -10.128445
Q std: 12.763930
Actor loss: 10.132408
Action reg: 0.003964
  l1.weight: grad_norm = 0.037382
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.037940
Total gradient norm: 0.158374
=== Actor Training Debug (Iteration 3298) ===
Q mean: -9.710420
Q std: 12.757495
Actor loss: 9.714405
Action reg: 0.003986
  l1.weight: grad_norm = 0.019264
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.014292
Total gradient norm: 0.048070
=== Actor Training Debug (Iteration 3299) ===
Q mean: -8.277193
Q std: 11.781855
Actor loss: 8.281166
Action reg: 0.003973
  l1.weight: grad_norm = 0.043916
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.040118
Total gradient norm: 0.162369
=== Actor Training Debug (Iteration 3300) ===
Q mean: -8.779243
Q std: 11.113470
Actor loss: 8.783213
Action reg: 0.003969
  l1.weight: grad_norm = 0.063188
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.055653
Total gradient norm: 0.231837
=== Actor Training Debug (Iteration 3301) ===
Q mean: -11.140898
Q std: 13.025255
Actor loss: 11.144885
Action reg: 0.003987
  l1.weight: grad_norm = 0.012511
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.009521
Total gradient norm: 0.035455
=== Actor Training Debug (Iteration 3302) ===
Q mean: -9.216347
Q std: 12.727258
Actor loss: 9.220334
Action reg: 0.003987
  l1.weight: grad_norm = 0.078343
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.062552
Total gradient norm: 0.253742
=== Actor Training Debug (Iteration 3303) ===
Q mean: -8.635529
Q std: 11.986379
Actor loss: 8.639505
Action reg: 0.003977
  l1.weight: grad_norm = 0.094527
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.086390
Total gradient norm: 0.285970
=== Actor Training Debug (Iteration 3304) ===
Q mean: -10.277854
Q std: 12.762729
Actor loss: 10.281825
Action reg: 0.003971
  l1.weight: grad_norm = 0.124761
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.090113
Total gradient norm: 0.328186
=== Actor Training Debug (Iteration 3305) ===
Q mean: -8.760776
Q std: 11.119705
Actor loss: 8.764755
Action reg: 0.003979
  l1.weight: grad_norm = 0.041274
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.039934
Total gradient norm: 0.144149
=== Actor Training Debug (Iteration 3306) ===
Q mean: -9.363371
Q std: 11.368192
Actor loss: 9.367347
Action reg: 0.003976
  l1.weight: grad_norm = 0.050747
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.045752
Total gradient norm: 0.161788
=== Actor Training Debug (Iteration 3307) ===
Q mean: -10.089293
Q std: 12.721432
Actor loss: 10.093279
Action reg: 0.003986
  l1.weight: grad_norm = 0.081472
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.076661
Total gradient norm: 0.370114
=== Actor Training Debug (Iteration 3308) ===
Q mean: -8.965158
Q std: 12.617734
Actor loss: 8.969131
Action reg: 0.003974
  l1.weight: grad_norm = 0.048136
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.042777
Total gradient norm: 0.114771
=== Actor Training Debug (Iteration 3309) ===
Q mean: -8.668638
Q std: 12.212538
Actor loss: 8.672622
Action reg: 0.003984
  l1.weight: grad_norm = 0.050864
  l1.bias: grad_norm = 0.000043
  l2.weight: grad_norm = 0.045970
Total gradient norm: 0.183303
=== Actor Training Debug (Iteration 3310) ===
Q mean: -9.741364
Q std: 12.226653
Actor loss: 9.745348
Action reg: 0.003984
  l1.weight: grad_norm = 0.012696
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.011020
Total gradient norm: 0.047582
=== Actor Training Debug (Iteration 3311) ===
Q mean: -8.811241
Q std: 11.778797
Actor loss: 8.815218
Action reg: 0.003977
  l1.weight: grad_norm = 0.083695
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.071033
Total gradient norm: 0.290853
=== Actor Training Debug (Iteration 3312) ===
Q mean: -9.217014
Q std: 12.223435
Actor loss: 9.220996
Action reg: 0.003981
  l1.weight: grad_norm = 0.066641
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.067840
Total gradient norm: 0.218189
=== Actor Training Debug (Iteration 3313) ===
Q mean: -8.492901
Q std: 12.265694
Actor loss: 8.496879
Action reg: 0.003977
  l1.weight: grad_norm = 0.091332
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.081293
Total gradient norm: 0.308242
=== Actor Training Debug (Iteration 3314) ===
Q mean: -8.026293
Q std: 11.609282
Actor loss: 8.030259
Action reg: 0.003966
  l1.weight: grad_norm = 0.077622
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.069211
Total gradient norm: 0.231454
=== Actor Training Debug (Iteration 3315) ===
Q mean: -9.265345
Q std: 12.165183
Actor loss: 9.269322
Action reg: 0.003978
  l1.weight: grad_norm = 0.089126
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.070908
Total gradient norm: 0.237017
=== Actor Training Debug (Iteration 3316) ===
Q mean: -10.053609
Q std: 13.030742
Actor loss: 10.057590
Action reg: 0.003981
  l1.weight: grad_norm = 0.078868
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.068842
Total gradient norm: 0.301759
=== Actor Training Debug (Iteration 3317) ===
Q mean: -9.266600
Q std: 11.366075
Actor loss: 9.270582
Action reg: 0.003983
  l1.weight: grad_norm = 0.067372
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.051318
Total gradient norm: 0.164009
=== Actor Training Debug (Iteration 3318) ===
Q mean: -8.853377
Q std: 11.676932
Actor loss: 8.857347
Action reg: 0.003970
  l1.weight: grad_norm = 0.035781
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.032647
Total gradient norm: 0.120496
=== Actor Training Debug (Iteration 3319) ===
Q mean: -9.935985
Q std: 11.902685
Actor loss: 9.939963
Action reg: 0.003979
  l1.weight: grad_norm = 0.048743
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.048582
Total gradient norm: 0.187422
=== Actor Training Debug (Iteration 3320) ===
Q mean: -9.459312
Q std: 12.816341
Actor loss: 9.463306
Action reg: 0.003994
  l1.weight: grad_norm = 0.015803
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.013436
Total gradient norm: 0.040166
=== Actor Training Debug (Iteration 3321) ===
Q mean: -9.803481
Q std: 11.891473
Actor loss: 9.807464
Action reg: 0.003982
  l1.weight: grad_norm = 0.038552
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.030003
Total gradient norm: 0.086431
=== Actor Training Debug (Iteration 3322) ===
Q mean: -8.910988
Q std: 11.169305
Actor loss: 8.914969
Action reg: 0.003982
  l1.weight: grad_norm = 0.094555
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.085032
Total gradient norm: 0.328079
=== Actor Training Debug (Iteration 3323) ===
Q mean: -9.534312
Q std: 12.324975
Actor loss: 9.538298
Action reg: 0.003985
  l1.weight: grad_norm = 0.035667
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.033340
Total gradient norm: 0.124301
=== Actor Training Debug (Iteration 3324) ===
Q mean: -9.723397
Q std: 12.137892
Actor loss: 9.727371
Action reg: 0.003974
  l1.weight: grad_norm = 0.052656
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.049980
Total gradient norm: 0.176761
=== Actor Training Debug (Iteration 3325) ===
Q mean: -9.585821
Q std: 12.569773
Actor loss: 9.589796
Action reg: 0.003975
  l1.weight: grad_norm = 0.048029
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.043699
Total gradient norm: 0.160806
=== Actor Training Debug (Iteration 3326) ===
Q mean: -9.083143
Q std: 11.944229
Actor loss: 9.087114
Action reg: 0.003971
  l1.weight: grad_norm = 0.050299
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.035666
Total gradient norm: 0.113827
=== Actor Training Debug (Iteration 3327) ===
Q mean: -10.422081
Q std: 12.813914
Actor loss: 10.426064
Action reg: 0.003983
  l1.weight: grad_norm = 0.051448
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.043679
Total gradient norm: 0.159078
=== Actor Training Debug (Iteration 3328) ===
Q mean: -8.655449
Q std: 11.233644
Actor loss: 8.659423
Action reg: 0.003974
  l1.weight: grad_norm = 0.064792
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.065668
Total gradient norm: 0.253000
=== Actor Training Debug (Iteration 3329) ===
Q mean: -10.292742
Q std: 12.500339
Actor loss: 10.296709
Action reg: 0.003967
  l1.weight: grad_norm = 0.066378
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.053198
Total gradient norm: 0.194957
=== Actor Training Debug (Iteration 3330) ===
Q mean: -9.644569
Q std: 11.734599
Actor loss: 9.648553
Action reg: 0.003983
  l1.weight: grad_norm = 0.031603
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.027155
Total gradient norm: 0.090510
=== Actor Training Debug (Iteration 3331) ===
Q mean: -9.911904
Q std: 12.074750
Actor loss: 9.915895
Action reg: 0.003991
  l1.weight: grad_norm = 0.055289
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.039426
Total gradient norm: 0.159046
=== Actor Training Debug (Iteration 3332) ===
Q mean: -9.384676
Q std: 12.182914
Actor loss: 9.388657
Action reg: 0.003981
  l1.weight: grad_norm = 0.031653
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.028097
Total gradient norm: 0.096452
=== Actor Training Debug (Iteration 3333) ===
Q mean: -8.487458
Q std: 11.152395
Actor loss: 8.491447
Action reg: 0.003990
  l1.weight: grad_norm = 0.018706
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.016886
Total gradient norm: 0.071528
=== Actor Training Debug (Iteration 3334) ===
Q mean: -9.163921
Q std: 10.989165
Actor loss: 9.167897
Action reg: 0.003976
  l1.weight: grad_norm = 0.173079
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.134883
Total gradient norm: 0.530819
=== Actor Training Debug (Iteration 3335) ===
Q mean: -10.016123
Q std: 12.004776
Actor loss: 10.020106
Action reg: 0.003984
  l1.weight: grad_norm = 0.038973
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.034032
Total gradient norm: 0.124785
=== Actor Training Debug (Iteration 3336) ===
Q mean: -9.443161
Q std: 12.044249
Actor loss: 9.447139
Action reg: 0.003977
  l1.weight: grad_norm = 0.042810
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.036338
Total gradient norm: 0.159528
=== Actor Training Debug (Iteration 3337) ===
Q mean: -9.527008
Q std: 12.164762
Actor loss: 9.530983
Action reg: 0.003975
  l1.weight: grad_norm = 0.022478
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.021416
Total gradient norm: 0.066131
=== Actor Training Debug (Iteration 3338) ===
Q mean: -10.169911
Q std: 12.988847
Actor loss: 10.173882
Action reg: 0.003970
  l1.weight: grad_norm = 0.045282
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.043180
Total gradient norm: 0.174457
=== Actor Training Debug (Iteration 3339) ===
Q mean: -8.864297
Q std: 11.719234
Actor loss: 8.868246
Action reg: 0.003950
  l1.weight: grad_norm = 0.091820
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.079299
Total gradient norm: 0.301912
=== Actor Training Debug (Iteration 3340) ===
Q mean: -11.178329
Q std: 13.611315
Actor loss: 11.182300
Action reg: 0.003971
  l1.weight: grad_norm = 0.108620
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.089342
Total gradient norm: 0.252774
=== Actor Training Debug (Iteration 3341) ===
Q mean: -9.399645
Q std: 11.911732
Actor loss: 9.403627
Action reg: 0.003982
  l1.weight: grad_norm = 0.075988
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.053914
Total gradient norm: 0.161195
=== Actor Training Debug (Iteration 3342) ===
Q mean: -10.209653
Q std: 12.474462
Actor loss: 10.213627
Action reg: 0.003974
  l1.weight: grad_norm = 0.067718
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.061984
Total gradient norm: 0.242720
=== Actor Training Debug (Iteration 3343) ===
Q mean: -9.584251
Q std: 12.702737
Actor loss: 9.588216
Action reg: 0.003964
  l1.weight: grad_norm = 0.113267
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.099822
Total gradient norm: 0.294754
=== Actor Training Debug (Iteration 3344) ===
Q mean: -9.567347
Q std: 12.007463
Actor loss: 9.571328
Action reg: 0.003982
  l1.weight: grad_norm = 0.066777
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.073799
Total gradient norm: 0.326867
=== Actor Training Debug (Iteration 3345) ===
Q mean: -10.192455
Q std: 13.251789
Actor loss: 10.196427
Action reg: 0.003972
  l1.weight: grad_norm = 0.099271
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.083882
Total gradient norm: 0.328341
=== Actor Training Debug (Iteration 3346) ===
Q mean: -8.622786
Q std: 11.823819
Actor loss: 8.626757
Action reg: 0.003971
  l1.weight: grad_norm = 0.047623
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.047255
Total gradient norm: 0.208408
=== Actor Training Debug (Iteration 3347) ===
Q mean: -9.438540
Q std: 12.160731
Actor loss: 9.442503
Action reg: 0.003964
  l1.weight: grad_norm = 0.064801
  l1.bias: grad_norm = 0.000754
  l2.weight: grad_norm = 0.051858
Total gradient norm: 0.172455
=== Actor Training Debug (Iteration 3348) ===
Q mean: -9.293846
Q std: 11.841248
Actor loss: 9.297816
Action reg: 0.003970
  l1.weight: grad_norm = 0.045621
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.044812
Total gradient norm: 0.193136
=== Actor Training Debug (Iteration 3349) ===
Q mean: -9.007339
Q std: 11.911105
Actor loss: 9.011326
Action reg: 0.003987
  l1.weight: grad_norm = 0.079228
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.071148
Total gradient norm: 0.295590
=== Actor Training Debug (Iteration 3350) ===
Q mean: -7.579117
Q std: 11.089853
Actor loss: 7.583083
Action reg: 0.003967
  l1.weight: grad_norm = 0.088698
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.069563
Total gradient norm: 0.232853
=== Actor Training Debug (Iteration 3351) ===
Q mean: -9.757475
Q std: 12.651700
Actor loss: 9.761447
Action reg: 0.003972
  l1.weight: grad_norm = 0.125873
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.099067
Total gradient norm: 0.319771
=== Actor Training Debug (Iteration 3352) ===
Q mean: -9.412626
Q std: 12.889281
Actor loss: 9.416599
Action reg: 0.003973
  l1.weight: grad_norm = 0.094130
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.070889
Total gradient norm: 0.254563
=== Actor Training Debug (Iteration 3353) ===
Q mean: -9.782991
Q std: 11.718997
Actor loss: 9.786962
Action reg: 0.003970
  l1.weight: grad_norm = 0.106650
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.089601
Total gradient norm: 0.334779
=== Actor Training Debug (Iteration 3354) ===
Q mean: -9.389428
Q std: 13.237864
Actor loss: 9.393381
Action reg: 0.003953
  l1.weight: grad_norm = 0.107977
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.081988
Total gradient norm: 0.241700
=== Actor Training Debug (Iteration 3355) ===
Q mean: -9.046932
Q std: 11.985360
Actor loss: 9.050908
Action reg: 0.003976
  l1.weight: grad_norm = 0.072792
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.059731
Total gradient norm: 0.209716
=== Actor Training Debug (Iteration 3356) ===
Q mean: -9.613092
Q std: 12.190221
Actor loss: 9.617071
Action reg: 0.003978
  l1.weight: grad_norm = 0.031326
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.023899
Total gradient norm: 0.079158
=== Actor Training Debug (Iteration 3357) ===
Q mean: -9.632446
Q std: 11.854657
Actor loss: 9.636417
Action reg: 0.003971
  l1.weight: grad_norm = 0.069779
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.064082
Total gradient norm: 0.255836
=== Actor Training Debug (Iteration 3358) ===
Q mean: -10.116678
Q std: 12.531189
Actor loss: 10.120657
Action reg: 0.003978
  l1.weight: grad_norm = 0.063986
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.058892
Total gradient norm: 0.251841
=== Actor Training Debug (Iteration 3359) ===
Q mean: -9.296295
Q std: 12.866505
Actor loss: 9.300268
Action reg: 0.003973
  l1.weight: grad_norm = 0.135081
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.111806
Total gradient norm: 0.403641
=== Actor Training Debug (Iteration 3360) ===
Q mean: -7.207685
Q std: 10.835835
Actor loss: 7.211669
Action reg: 0.003984
  l1.weight: grad_norm = 0.064335
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.049586
Total gradient norm: 0.196307
=== Actor Training Debug (Iteration 3361) ===
Q mean: -10.097942
Q std: 12.359262
Actor loss: 10.101928
Action reg: 0.003986
  l1.weight: grad_norm = 0.059189
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.046276
Total gradient norm: 0.136423
=== Actor Training Debug (Iteration 3362) ===
Q mean: -8.254700
Q std: 10.687021
Actor loss: 8.258671
Action reg: 0.003971
  l1.weight: grad_norm = 0.081020
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.078079
Total gradient norm: 0.369213
=== Actor Training Debug (Iteration 3363) ===
Q mean: -10.896677
Q std: 13.332232
Actor loss: 10.900649
Action reg: 0.003972
  l1.weight: grad_norm = 0.089891
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.066474
Total gradient norm: 0.233989
=== Actor Training Debug (Iteration 3364) ===
Q mean: -10.560083
Q std: 12.905403
Actor loss: 10.564071
Action reg: 0.003987
  l1.weight: grad_norm = 0.061508
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.053726
Total gradient norm: 0.199826
=== Actor Training Debug (Iteration 3365) ===
Q mean: -7.861941
Q std: 11.631723
Actor loss: 7.865920
Action reg: 0.003979
  l1.weight: grad_norm = 0.053035
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.050372
Total gradient norm: 0.225321
=== Actor Training Debug (Iteration 3366) ===
Q mean: -8.356852
Q std: 11.506927
Actor loss: 8.360810
Action reg: 0.003959
  l1.weight: grad_norm = 0.110156
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.085791
Total gradient norm: 0.321298
=== Actor Training Debug (Iteration 3367) ===
Q mean: -9.075548
Q std: 12.145943
Actor loss: 9.079529
Action reg: 0.003981
  l1.weight: grad_norm = 0.020734
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.017715
Total gradient norm: 0.060510
=== Actor Training Debug (Iteration 3368) ===
Q mean: -10.500025
Q std: 13.045476
Actor loss: 10.503999
Action reg: 0.003974
  l1.weight: grad_norm = 0.038043
  l1.bias: grad_norm = 0.000699
  l2.weight: grad_norm = 0.035021
Total gradient norm: 0.134480
=== Actor Training Debug (Iteration 3369) ===
Q mean: -9.547359
Q std: 11.948947
Actor loss: 9.551344
Action reg: 0.003984
  l1.weight: grad_norm = 0.067459
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.067978
Total gradient norm: 0.291699
=== Actor Training Debug (Iteration 3370) ===
Q mean: -9.674265
Q std: 12.606005
Actor loss: 9.678240
Action reg: 0.003975
  l1.weight: grad_norm = 0.052202
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.044185
Total gradient norm: 0.182652
=== Actor Training Debug (Iteration 3371) ===
Q mean: -9.452688
Q std: 12.104353
Actor loss: 9.456673
Action reg: 0.003984
  l1.weight: grad_norm = 0.027278
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.022078
Total gradient norm: 0.079774
=== Actor Training Debug (Iteration 3372) ===
Q mean: -8.998557
Q std: 11.809613
Actor loss: 9.002530
Action reg: 0.003973
  l1.weight: grad_norm = 0.090173
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.065486
Total gradient norm: 0.184361
=== Actor Training Debug (Iteration 3373) ===
Q mean: -8.928223
Q std: 10.855425
Actor loss: 8.932217
Action reg: 0.003994
  l1.weight: grad_norm = 0.014959
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.013959
Total gradient norm: 0.044747
=== Actor Training Debug (Iteration 3374) ===
Q mean: -10.547091
Q std: 12.906066
Actor loss: 10.551068
Action reg: 0.003977
  l1.weight: grad_norm = 0.106850
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.075680
Total gradient norm: 0.269687
=== Actor Training Debug (Iteration 3375) ===
Q mean: -10.434231
Q std: 12.550347
Actor loss: 10.438190
Action reg: 0.003959
  l1.weight: grad_norm = 0.056258
  l1.bias: grad_norm = 0.000815
  l2.weight: grad_norm = 0.053268
Total gradient norm: 0.183744
=== Actor Training Debug (Iteration 3376) ===
Q mean: -9.251751
Q std: 11.774220
Actor loss: 9.255732
Action reg: 0.003981
  l1.weight: grad_norm = 0.027824
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.024843
Total gradient norm: 0.102897
=== Actor Training Debug (Iteration 3377) ===
Q mean: -8.579691
Q std: 11.969539
Actor loss: 8.583664
Action reg: 0.003973
  l1.weight: grad_norm = 0.095087
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.084092
Total gradient norm: 0.319812
=== Actor Training Debug (Iteration 3378) ===
Q mean: -8.992781
Q std: 12.030962
Actor loss: 8.996746
Action reg: 0.003965
  l1.weight: grad_norm = 0.050572
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.048198
Total gradient norm: 0.197866
=== Actor Training Debug (Iteration 3379) ===
Q mean: -8.826221
Q std: 11.188007
Actor loss: 8.830189
Action reg: 0.003967
  l1.weight: grad_norm = 0.057200
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.051557
Total gradient norm: 0.218366
=== Actor Training Debug (Iteration 3380) ===
Q mean: -7.949508
Q std: 11.442148
Actor loss: 7.953484
Action reg: 0.003976
  l1.weight: grad_norm = 0.072060
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.065119
Total gradient norm: 0.268107
=== Actor Training Debug (Iteration 3381) ===
Q mean: -9.235067
Q std: 10.971119
Actor loss: 9.239039
Action reg: 0.003972
  l1.weight: grad_norm = 0.110397
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.091195
Total gradient norm: 0.302542
=== Actor Training Debug (Iteration 3382) ===
Q mean: -11.211917
Q std: 13.271538
Actor loss: 11.215889
Action reg: 0.003972
  l1.weight: grad_norm = 0.062075
  l1.bias: grad_norm = 0.000822
  l2.weight: grad_norm = 0.064421
Total gradient norm: 0.215414
=== Actor Training Debug (Iteration 3383) ===
Q mean: -8.529686
Q std: 11.543072
Actor loss: 8.533658
Action reg: 0.003972
  l1.weight: grad_norm = 0.080854
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.073152
Total gradient norm: 0.266879
=== Actor Training Debug (Iteration 3384) ===
Q mean: -9.026663
Q std: 11.436166
Actor loss: 9.030634
Action reg: 0.003971
  l1.weight: grad_norm = 0.079786
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.065264
Total gradient norm: 0.218913
=== Actor Training Debug (Iteration 3385) ===
Q mean: -11.262980
Q std: 14.308791
Actor loss: 11.266953
Action reg: 0.003973
  l1.weight: grad_norm = 0.149193
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.135929
Total gradient norm: 0.395019
=== Actor Training Debug (Iteration 3386) ===
Q mean: -10.077435
Q std: 12.536785
Actor loss: 10.081410
Action reg: 0.003975
  l1.weight: grad_norm = 0.061829
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.051329
Total gradient norm: 0.157831
=== Actor Training Debug (Iteration 3387) ===
Q mean: -8.663920
Q std: 11.327448
Actor loss: 8.667892
Action reg: 0.003971
  l1.weight: grad_norm = 0.069182
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.049074
Total gradient norm: 0.174673
=== Actor Training Debug (Iteration 3388) ===
Q mean: -9.714417
Q std: 12.407726
Actor loss: 9.718404
Action reg: 0.003987
  l1.weight: grad_norm = 0.047935
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.041419
Total gradient norm: 0.140974
=== Actor Training Debug (Iteration 3389) ===
Q mean: -9.667471
Q std: 13.182704
Actor loss: 9.671453
Action reg: 0.003981
  l1.weight: grad_norm = 0.047074
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.040592
Total gradient norm: 0.152231
=== Actor Training Debug (Iteration 3390) ===
Q mean: -10.198400
Q std: 11.817112
Actor loss: 10.202374
Action reg: 0.003974
  l1.weight: grad_norm = 0.061450
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.046573
Total gradient norm: 0.160785
=== Actor Training Debug (Iteration 3391) ===
Q mean: -8.296253
Q std: 12.025249
Actor loss: 8.300235
Action reg: 0.003981
  l1.weight: grad_norm = 0.183286
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.155949
Total gradient norm: 0.522229
=== Actor Training Debug (Iteration 3392) ===
Q mean: -9.335316
Q std: 12.604569
Actor loss: 9.339296
Action reg: 0.003981
  l1.weight: grad_norm = 0.089778
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.062454
Total gradient norm: 0.206960
=== Actor Training Debug (Iteration 3393) ===
Q mean: -9.713852
Q std: 12.965986
Actor loss: 9.717830
Action reg: 0.003978
  l1.weight: grad_norm = 0.075315
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.067187
Total gradient norm: 0.287573
=== Actor Training Debug (Iteration 3394) ===
Q mean: -9.238396
Q std: 12.481724
Actor loss: 9.242373
Action reg: 0.003977
  l1.weight: grad_norm = 0.078276
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.080352
Total gradient norm: 0.335519
=== Actor Training Debug (Iteration 3395) ===
Q mean: -9.140337
Q std: 11.722682
Actor loss: 9.144301
Action reg: 0.003965
  l1.weight: grad_norm = 0.243379
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.216437
Total gradient norm: 0.588499
=== Actor Training Debug (Iteration 3396) ===
Q mean: -8.979436
Q std: 11.954181
Actor loss: 8.983413
Action reg: 0.003976
  l1.weight: grad_norm = 0.048692
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.046091
Total gradient norm: 0.171139
=== Actor Training Debug (Iteration 3397) ===
Q mean: -9.559383
Q std: 11.723704
Actor loss: 9.563359
Action reg: 0.003976
  l1.weight: grad_norm = 0.046263
  l1.bias: grad_norm = 0.000647
  l2.weight: grad_norm = 0.046461
Total gradient norm: 0.182135
=== Actor Training Debug (Iteration 3398) ===
Q mean: -8.858772
Q std: 11.393525
Actor loss: 8.862761
Action reg: 0.003988
  l1.weight: grad_norm = 0.043795
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.039195
Total gradient norm: 0.134879
=== Actor Training Debug (Iteration 3399) ===
Q mean: -10.248627
Q std: 12.855899
Actor loss: 10.252615
Action reg: 0.003989
  l1.weight: grad_norm = 0.054609
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.041195
Total gradient norm: 0.119753
=== Actor Training Debug (Iteration 3400) ===
Q mean: -11.245481
Q std: 13.366621
Actor loss: 11.249464
Action reg: 0.003983
  l1.weight: grad_norm = 0.061410
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.064079
Total gradient norm: 0.245223
=== Actor Training Debug (Iteration 3401) ===
Q mean: -10.005997
Q std: 12.432057
Actor loss: 10.009982
Action reg: 0.003985
  l1.weight: grad_norm = 0.076453
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.063208
Total gradient norm: 0.229617
=== Actor Training Debug (Iteration 3402) ===
Q mean: -9.227970
Q std: 11.530791
Actor loss: 9.231934
Action reg: 0.003964
  l1.weight: grad_norm = 0.140998
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.106780
Total gradient norm: 0.414311
=== Actor Training Debug (Iteration 3403) ===
Q mean: -10.793549
Q std: 13.289711
Actor loss: 10.797533
Action reg: 0.003984
  l1.weight: grad_norm = 0.087298
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.076243
Total gradient norm: 0.275152
=== Actor Training Debug (Iteration 3404) ===
Q mean: -9.935541
Q std: 12.096186
Actor loss: 9.939529
Action reg: 0.003989
  l1.weight: grad_norm = 0.060404
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.050933
Total gradient norm: 0.154084
=== Actor Training Debug (Iteration 3405) ===
Q mean: -8.695494
Q std: 11.721990
Actor loss: 8.699455
Action reg: 0.003961
  l1.weight: grad_norm = 0.074950
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.068094
Total gradient norm: 0.221385
=== Actor Training Debug (Iteration 3406) ===
Q mean: -8.655792
Q std: 10.974119
Actor loss: 8.659769
Action reg: 0.003976
  l1.weight: grad_norm = 0.066093
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.063483
Total gradient norm: 0.225135
=== Actor Training Debug (Iteration 3407) ===
Q mean: -8.957888
Q std: 12.049085
Actor loss: 8.961861
Action reg: 0.003973
  l1.weight: grad_norm = 0.092630
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.084949
Total gradient norm: 0.312741
=== Actor Training Debug (Iteration 3408) ===
Q mean: -10.852907
Q std: 13.421827
Actor loss: 10.856890
Action reg: 0.003982
  l1.weight: grad_norm = 0.058194
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.048197
Total gradient norm: 0.170391
=== Actor Training Debug (Iteration 3409) ===
Q mean: -11.197496
Q std: 13.156188
Actor loss: 11.201466
Action reg: 0.003969
  l1.weight: grad_norm = 0.067106
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.060505
Total gradient norm: 0.218530
=== Actor Training Debug (Iteration 3410) ===
Q mean: -9.696748
Q std: 12.062346
Actor loss: 9.700721
Action reg: 0.003973
  l1.weight: grad_norm = 0.049878
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.044949
Total gradient norm: 0.164456
=== Actor Training Debug (Iteration 3411) ===
Q mean: -9.548063
Q std: 11.876215
Actor loss: 9.552040
Action reg: 0.003977
  l1.weight: grad_norm = 0.064687
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.050583
Total gradient norm: 0.164247
=== Actor Training Debug (Iteration 3412) ===
Q mean: -10.152825
Q std: 12.848166
Actor loss: 10.156806
Action reg: 0.003980
  l1.weight: grad_norm = 0.039011
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.037658
Total gradient norm: 0.158270
=== Actor Training Debug (Iteration 3413) ===
Q mean: -10.312704
Q std: 11.707024
Actor loss: 10.316681
Action reg: 0.003976
  l1.weight: grad_norm = 0.040273
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.034567
Total gradient norm: 0.118029
=== Actor Training Debug (Iteration 3414) ===
Q mean: -8.909765
Q std: 12.550262
Actor loss: 8.913724
Action reg: 0.003959
  l1.weight: grad_norm = 0.120494
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.104227
Total gradient norm: 0.334830
=== Actor Training Debug (Iteration 3415) ===
Q mean: -7.581061
Q std: 10.535924
Actor loss: 7.585037
Action reg: 0.003975
  l1.weight: grad_norm = 0.079355
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.076140
Total gradient norm: 0.258851
=== Actor Training Debug (Iteration 3416) ===
Q mean: -8.768676
Q std: 11.247271
Actor loss: 8.772644
Action reg: 0.003969
  l1.weight: grad_norm = 0.064307
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.061155
Total gradient norm: 0.234038
=== Actor Training Debug (Iteration 3417) ===
Q mean: -10.310518
Q std: 12.851762
Actor loss: 10.314493
Action reg: 0.003975
  l1.weight: grad_norm = 0.124041
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.108819
Total gradient norm: 0.382368
=== Actor Training Debug (Iteration 3418) ===
Q mean: -10.433392
Q std: 13.964777
Actor loss: 10.437370
Action reg: 0.003979
  l1.weight: grad_norm = 0.033727
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.035562
Total gradient norm: 0.128463
=== Actor Training Debug (Iteration 3419) ===
Q mean: -9.953762
Q std: 13.155628
Actor loss: 9.957737
Action reg: 0.003975
  l1.weight: grad_norm = 0.131543
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.102035
Total gradient norm: 0.372784
=== Actor Training Debug (Iteration 3420) ===
Q mean: -9.539414
Q std: 11.872876
Actor loss: 9.543386
Action reg: 0.003972
  l1.weight: grad_norm = 0.077618
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.068335
Total gradient norm: 0.213367
=== Actor Training Debug (Iteration 3421) ===
Q mean: -8.854950
Q std: 11.557611
Actor loss: 8.858921
Action reg: 0.003971
  l1.weight: grad_norm = 0.061971
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.055686
Total gradient norm: 0.182949
=== Actor Training Debug (Iteration 3422) ===
Q mean: -8.630888
Q std: 12.070080
Actor loss: 8.634854
Action reg: 0.003966
  l1.weight: grad_norm = 0.293039
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.258191
Total gradient norm: 0.917142
=== Actor Training Debug (Iteration 3423) ===
Q mean: -10.875015
Q std: 12.930929
Actor loss: 10.878991
Action reg: 0.003976
  l1.weight: grad_norm = 0.016018
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.015887
Total gradient norm: 0.059730
=== Actor Training Debug (Iteration 3424) ===
Q mean: -9.593649
Q std: 11.508758
Actor loss: 9.597622
Action reg: 0.003973
  l1.weight: grad_norm = 0.099307
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.079026
Total gradient norm: 0.251564
=== Actor Training Debug (Iteration 3425) ===
Q mean: -9.890694
Q std: 12.376589
Actor loss: 9.894661
Action reg: 0.003967
  l1.weight: grad_norm = 0.119319
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.090333
Total gradient norm: 0.371257
=== Actor Training Debug (Iteration 3426) ===
Q mean: -10.720327
Q std: 13.012679
Actor loss: 10.724305
Action reg: 0.003978
  l1.weight: grad_norm = 0.052437
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.042237
Total gradient norm: 0.130777
=== Actor Training Debug (Iteration 3427) ===
Q mean: -9.515945
Q std: 12.700921
Actor loss: 9.519928
Action reg: 0.003982
  l1.weight: grad_norm = 0.051182
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.041204
Total gradient norm: 0.146092
=== Actor Training Debug (Iteration 3428) ===
Q mean: -9.273689
Q std: 12.811666
Actor loss: 9.277665
Action reg: 0.003976
  l1.weight: grad_norm = 0.087052
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.073934
Total gradient norm: 0.240072
=== Actor Training Debug (Iteration 3429) ===
Q mean: -9.653431
Q std: 11.946420
Actor loss: 9.657413
Action reg: 0.003982
  l1.weight: grad_norm = 0.100283
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.088520
Total gradient norm: 0.332543
=== Actor Training Debug (Iteration 3430) ===
Q mean: -9.107244
Q std: 11.936022
Actor loss: 9.111219
Action reg: 0.003975
  l1.weight: grad_norm = 0.063645
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.048980
Total gradient norm: 0.155627
=== Actor Training Debug (Iteration 3431) ===
Q mean: -9.588351
Q std: 12.876989
Actor loss: 9.592314
Action reg: 0.003962
  l1.weight: grad_norm = 0.059925
  l1.bias: grad_norm = 0.000694
  l2.weight: grad_norm = 0.053439
Total gradient norm: 0.189470
=== Actor Training Debug (Iteration 3432) ===
Q mean: -9.727354
Q std: 11.608769
Actor loss: 9.731321
Action reg: 0.003967
  l1.weight: grad_norm = 0.062129
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.050097
Total gradient norm: 0.179601
=== Actor Training Debug (Iteration 3433) ===
Q mean: -9.026190
Q std: 11.900352
Actor loss: 9.030158
Action reg: 0.003968
  l1.weight: grad_norm = 0.031766
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.028798
Total gradient norm: 0.096739
=== Actor Training Debug (Iteration 3434) ===
Q mean: -9.317878
Q std: 11.514901
Actor loss: 9.321856
Action reg: 0.003979
  l1.weight: grad_norm = 0.064577
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.051338
Total gradient norm: 0.181989
=== Actor Training Debug (Iteration 3435) ===
Q mean: -9.117167
Q std: 12.513998
Actor loss: 9.121139
Action reg: 0.003972
  l1.weight: grad_norm = 0.077108
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.064170
Total gradient norm: 0.190952
=== Actor Training Debug (Iteration 3436) ===
Q mean: -8.575114
Q std: 11.665207
Actor loss: 8.579091
Action reg: 0.003977
  l1.weight: grad_norm = 0.125835
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.087522
Total gradient norm: 0.297371
=== Actor Training Debug (Iteration 3437) ===
Q mean: -10.155631
Q std: 12.872078
Actor loss: 10.159592
Action reg: 0.003961
  l1.weight: grad_norm = 0.105745
  l1.bias: grad_norm = 0.000435
  l2.weight: grad_norm = 0.079260
Total gradient norm: 0.267423
=== Actor Training Debug (Iteration 3438) ===
Q mean: -10.588413
Q std: 12.471436
Actor loss: 10.592400
Action reg: 0.003987
  l1.weight: grad_norm = 0.022142
  l1.bias: grad_norm = 0.000010
  l2.weight: grad_norm = 0.020186
Total gradient norm: 0.064688
=== Actor Training Debug (Iteration 3439) ===
Q mean: -10.481911
Q std: 13.116392
Actor loss: 10.485888
Action reg: 0.003977
  l1.weight: grad_norm = 0.079647
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.063118
Total gradient norm: 0.232612
=== Actor Training Debug (Iteration 3440) ===
Q mean: -9.217693
Q std: 11.963182
Actor loss: 9.221653
Action reg: 0.003960
  l1.weight: grad_norm = 0.063988
  l1.bias: grad_norm = 0.000756
  l2.weight: grad_norm = 0.057729
Total gradient norm: 0.212233
=== Actor Training Debug (Iteration 3441) ===
Q mean: -10.254696
Q std: 12.243136
Actor loss: 10.258680
Action reg: 0.003984
  l1.weight: grad_norm = 0.041545
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.038087
Total gradient norm: 0.165158
=== Actor Training Debug (Iteration 3442) ===
Q mean: -8.831400
Q std: 11.417620
Actor loss: 8.835377
Action reg: 0.003976
  l1.weight: grad_norm = 0.030297
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.028884
Total gradient norm: 0.138388
=== Actor Training Debug (Iteration 3443) ===
Q mean: -10.487270
Q std: 12.645326
Actor loss: 10.491242
Action reg: 0.003973
  l1.weight: grad_norm = 0.037234
  l1.bias: grad_norm = 0.000807
  l2.weight: grad_norm = 0.035117
Total gradient norm: 0.148273
=== Actor Training Debug (Iteration 3444) ===
Q mean: -9.385736
Q std: 11.828874
Actor loss: 9.389710
Action reg: 0.003974
  l1.weight: grad_norm = 0.062236
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.050988
Total gradient norm: 0.180104
=== Actor Training Debug (Iteration 3445) ===
Q mean: -9.368683
Q std: 12.336107
Actor loss: 9.372656
Action reg: 0.003973
  l1.weight: grad_norm = 0.043710
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.035417
Total gradient norm: 0.119314
=== Actor Training Debug (Iteration 3446) ===
Q mean: -8.946920
Q std: 11.080968
Actor loss: 8.950904
Action reg: 0.003983
  l1.weight: grad_norm = 0.038759
  l1.bias: grad_norm = 0.000014
  l2.weight: grad_norm = 0.034661
Total gradient norm: 0.128957
=== Actor Training Debug (Iteration 3447) ===
Q mean: -9.716930
Q std: 12.136538
Actor loss: 9.720897
Action reg: 0.003966
  l1.weight: grad_norm = 0.048877
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.038040
Total gradient norm: 0.129134
=== Actor Training Debug (Iteration 3448) ===
Q mean: -8.642305
Q std: 11.464392
Actor loss: 8.646277
Action reg: 0.003972
  l1.weight: grad_norm = 0.036792
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.034656
Total gradient norm: 0.112287
=== Actor Training Debug (Iteration 3449) ===
Q mean: -9.608430
Q std: 12.050313
Actor loss: 9.612405
Action reg: 0.003975
  l1.weight: grad_norm = 0.026198
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.022028
Total gradient norm: 0.075681
=== Actor Training Debug (Iteration 3450) ===
Q mean: -10.983168
Q std: 13.413339
Actor loss: 10.987133
Action reg: 0.003966
  l1.weight: grad_norm = 0.068182
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.048513
Total gradient norm: 0.164262
=== Actor Training Debug (Iteration 3451) ===
Q mean: -9.658857
Q std: 11.868145
Actor loss: 9.662838
Action reg: 0.003981
  l1.weight: grad_norm = 0.035725
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.028635
Total gradient norm: 0.091002
=== Actor Training Debug (Iteration 3452) ===
Q mean: -11.275867
Q std: 13.885064
Actor loss: 11.279848
Action reg: 0.003980
  l1.weight: grad_norm = 0.059089
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.052353
Total gradient norm: 0.199516
=== Actor Training Debug (Iteration 3453) ===
Q mean: -9.876043
Q std: 12.040261
Actor loss: 9.880027
Action reg: 0.003984
  l1.weight: grad_norm = 0.079452
  l1.bias: grad_norm = 0.000028
  l2.weight: grad_norm = 0.057080
Total gradient norm: 0.170862
=== Actor Training Debug (Iteration 3454) ===
Q mean: -8.855087
Q std: 11.678247
Actor loss: 8.859063
Action reg: 0.003976
  l1.weight: grad_norm = 0.061365
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.056915
Total gradient norm: 0.212293
=== Actor Training Debug (Iteration 3455) ===
Q mean: -8.650990
Q std: 11.609381
Actor loss: 8.654974
Action reg: 0.003985
  l1.weight: grad_norm = 0.033380
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.027799
Total gradient norm: 0.093222
=== Actor Training Debug (Iteration 3456) ===
Q mean: -10.059367
Q std: 11.490112
Actor loss: 10.063343
Action reg: 0.003976
  l1.weight: grad_norm = 0.143274
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.109672
Total gradient norm: 0.323402
=== Actor Training Debug (Iteration 3457) ===
Q mean: -9.694545
Q std: 12.874407
Actor loss: 9.698534
Action reg: 0.003989
  l1.weight: grad_norm = 0.054440
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.049146
Total gradient norm: 0.203587
=== Actor Training Debug (Iteration 3458) ===
Q mean: -9.339758
Q std: 12.075013
Actor loss: 9.343726
Action reg: 0.003968
  l1.weight: grad_norm = 0.103240
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.098815
Total gradient norm: 0.297487
=== Actor Training Debug (Iteration 3459) ===
Q mean: -9.563230
Q std: 12.786817
Actor loss: 9.567186
Action reg: 0.003957
  l1.weight: grad_norm = 0.064314
  l1.bias: grad_norm = 0.001157
  l2.weight: grad_norm = 0.050873
Total gradient norm: 0.190500
=== Actor Training Debug (Iteration 3460) ===
Q mean: -11.184123
Q std: 12.768016
Actor loss: 11.188096
Action reg: 0.003973
  l1.weight: grad_norm = 0.034124
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.024678
Total gradient norm: 0.074202
=== Actor Training Debug (Iteration 3461) ===
Q mean: -9.370979
Q std: 11.443331
Actor loss: 9.374956
Action reg: 0.003977
  l1.weight: grad_norm = 0.034680
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.026906
Total gradient norm: 0.117898
=== Actor Training Debug (Iteration 3462) ===
Q mean: -8.940553
Q std: 10.928625
Actor loss: 8.944545
Action reg: 0.003992
  l1.weight: grad_norm = 0.127923
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.101172
Total gradient norm: 0.311013
=== Actor Training Debug (Iteration 3463) ===
Q mean: -9.345587
Q std: 11.756215
Actor loss: 9.349563
Action reg: 0.003976
  l1.weight: grad_norm = 0.120414
  l1.bias: grad_norm = 0.000638
  l2.weight: grad_norm = 0.108546
Total gradient norm: 0.374909
=== Actor Training Debug (Iteration 3464) ===
Q mean: -9.577226
Q std: 11.964092
Actor loss: 9.581215
Action reg: 0.003990
  l1.weight: grad_norm = 0.038241
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.031654
Total gradient norm: 0.115977
=== Actor Training Debug (Iteration 3465) ===
Q mean: -8.324537
Q std: 11.499698
Actor loss: 8.328515
Action reg: 0.003978
  l1.weight: grad_norm = 0.097037
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.077839
Total gradient norm: 0.313091
=== Actor Training Debug (Iteration 3466) ===
Q mean: -10.645661
Q std: 12.300103
Actor loss: 10.649640
Action reg: 0.003978
  l1.weight: grad_norm = 0.037187
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.031229
Total gradient norm: 0.103490
=== Actor Training Debug (Iteration 3467) ===
Q mean: -10.287496
Q std: 12.691991
Actor loss: 10.291471
Action reg: 0.003976
  l1.weight: grad_norm = 0.072587
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.064621
Total gradient norm: 0.199447
=== Actor Training Debug (Iteration 3468) ===
Q mean: -10.616570
Q std: 13.499592
Actor loss: 10.620553
Action reg: 0.003984
  l1.weight: grad_norm = 0.033090
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.030768
Total gradient norm: 0.098309
=== Actor Training Debug (Iteration 3469) ===
Q mean: -9.814201
Q std: 12.662134
Actor loss: 9.818171
Action reg: 0.003969
  l1.weight: grad_norm = 0.070761
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.055478
Total gradient norm: 0.190182
=== Actor Training Debug (Iteration 3470) ===
Q mean: -10.018330
Q std: 12.486469
Actor loss: 10.022314
Action reg: 0.003984
  l1.weight: grad_norm = 0.015708
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.014772
Total gradient norm: 0.058850
=== Actor Training Debug (Iteration 3471) ===
Q mean: -9.757298
Q std: 12.171781
Actor loss: 9.761271
Action reg: 0.003973
  l1.weight: grad_norm = 0.079648
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.060047
Total gradient norm: 0.203980
=== Actor Training Debug (Iteration 3472) ===
Q mean: -10.873882
Q std: 13.779048
Actor loss: 10.877872
Action reg: 0.003989
  l1.weight: grad_norm = 0.046594
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.041258
Total gradient norm: 0.131315
=== Actor Training Debug (Iteration 3473) ===
Q mean: -7.999862
Q std: 11.666433
Actor loss: 8.003828
Action reg: 0.003967
  l1.weight: grad_norm = 0.085527
  l1.bias: grad_norm = 0.000504
  l2.weight: grad_norm = 0.079009
Total gradient norm: 0.272745
=== Actor Training Debug (Iteration 3474) ===
Q mean: -10.353127
Q std: 12.431133
Actor loss: 10.357100
Action reg: 0.003973
  l1.weight: grad_norm = 0.099374
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.084654
Total gradient norm: 0.274852
=== Actor Training Debug (Iteration 3475) ===
Q mean: -9.912415
Q std: 11.831582
Actor loss: 9.916400
Action reg: 0.003985
  l1.weight: grad_norm = 0.014799
  l1.bias: grad_norm = 0.000009
  l2.weight: grad_norm = 0.012977
Total gradient norm: 0.041671
=== Actor Training Debug (Iteration 3476) ===
Q mean: -9.412805
Q std: 11.283332
Actor loss: 9.416794
Action reg: 0.003989
  l1.weight: grad_norm = 0.017602
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.017170
Total gradient norm: 0.064685
=== Actor Training Debug (Iteration 3477) ===
Q mean: -9.515430
Q std: 12.383197
Actor loss: 9.519414
Action reg: 0.003983
  l1.weight: grad_norm = 0.070964
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.060837
Total gradient norm: 0.200571
=== Actor Training Debug (Iteration 3478) ===
Q mean: -8.677997
Q std: 11.108130
Actor loss: 8.681979
Action reg: 0.003982
  l1.weight: grad_norm = 0.035183
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.029321
Total gradient norm: 0.107344
=== Actor Training Debug (Iteration 3479) ===
Q mean: -10.297747
Q std: 12.643875
Actor loss: 10.301722
Action reg: 0.003975
  l1.weight: grad_norm = 0.108956
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.091945
Total gradient norm: 0.310891
=== Actor Training Debug (Iteration 3480) ===
Q mean: -9.344784
Q std: 11.910763
Actor loss: 9.348756
Action reg: 0.003972
  l1.weight: grad_norm = 0.061903
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.047081
Total gradient norm: 0.151466
=== Actor Training Debug (Iteration 3481) ===
Q mean: -10.422493
Q std: 13.147778
Actor loss: 10.426468
Action reg: 0.003975
  l1.weight: grad_norm = 0.040389
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.033196
Total gradient norm: 0.114324
=== Actor Training Debug (Iteration 3482) ===
Q mean: -9.492787
Q std: 12.816357
Actor loss: 9.496755
Action reg: 0.003968
  l1.weight: grad_norm = 0.171244
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.127372
Total gradient norm: 0.385483
=== Actor Training Debug (Iteration 3483) ===
Q mean: -8.749290
Q std: 12.856120
Actor loss: 8.753276
Action reg: 0.003985
  l1.weight: grad_norm = 0.021103
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.019812
Total gradient norm: 0.067527
=== Actor Training Debug (Iteration 3484) ===
Q mean: -7.987767
Q std: 11.003052
Actor loss: 7.991749
Action reg: 0.003982
  l1.weight: grad_norm = 0.064608
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.050898
Total gradient norm: 0.167287
=== Actor Training Debug (Iteration 3485) ===
Q mean: -10.466312
Q std: 13.065240
Actor loss: 10.470290
Action reg: 0.003977
  l1.weight: grad_norm = 0.069919
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.062291
Total gradient norm: 0.191526
=== Actor Training Debug (Iteration 3486) ===
Q mean: -10.219990
Q std: 12.167443
Actor loss: 10.223960
Action reg: 0.003970
  l1.weight: grad_norm = 0.093414
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.077352
Total gradient norm: 0.243874
=== Actor Training Debug (Iteration 3487) ===
Q mean: -10.091652
Q std: 12.744831
Actor loss: 10.095631
Action reg: 0.003978
  l1.weight: grad_norm = 0.023203
  l1.bias: grad_norm = 0.000668
  l2.weight: grad_norm = 0.016837
Total gradient norm: 0.068840
=== Actor Training Debug (Iteration 3488) ===
Q mean: -9.241451
Q std: 13.195815
Actor loss: 9.245428
Action reg: 0.003977
  l1.weight: grad_norm = 0.133026
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.097927
Total gradient norm: 0.313699
=== Actor Training Debug (Iteration 3489) ===
Q mean: -8.045115
Q std: 11.213511
Actor loss: 8.049090
Action reg: 0.003976
  l1.weight: grad_norm = 0.028859
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.025063
Total gradient norm: 0.077520
=== Actor Training Debug (Iteration 3490) ===
Q mean: -10.638868
Q std: 12.006825
Actor loss: 10.642853
Action reg: 0.003984
  l1.weight: grad_norm = 0.040979
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.036475
Total gradient norm: 0.120544
=== Actor Training Debug (Iteration 3491) ===
Q mean: -9.109414
Q std: 11.902587
Actor loss: 9.113383
Action reg: 0.003969
  l1.weight: grad_norm = 0.131331
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.113183
Total gradient norm: 0.359600
=== Actor Training Debug (Iteration 3492) ===
Q mean: -8.134857
Q std: 10.528341
Actor loss: 8.138830
Action reg: 0.003973
  l1.weight: grad_norm = 0.029321
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.027789
Total gradient norm: 0.093049
=== Actor Training Debug (Iteration 3493) ===
Q mean: -10.225389
Q std: 13.503717
Actor loss: 10.229362
Action reg: 0.003973
  l1.weight: grad_norm = 0.046156
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.039230
Total gradient norm: 0.150707
=== Actor Training Debug (Iteration 3494) ===
Q mean: -10.731850
Q std: 13.361738
Actor loss: 10.735832
Action reg: 0.003983
  l1.weight: grad_norm = 0.086152
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.062436
Total gradient norm: 0.179543
=== Actor Training Debug (Iteration 3495) ===
Q mean: -10.672455
Q std: 13.333160
Actor loss: 10.676430
Action reg: 0.003975
  l1.weight: grad_norm = 0.069485
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.061460
Total gradient norm: 0.234899
=== Actor Training Debug (Iteration 3496) ===
Q mean: -9.420210
Q std: 12.603620
Actor loss: 9.424194
Action reg: 0.003984
  l1.weight: grad_norm = 0.069052
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.066522
Total gradient norm: 0.236705
=== Actor Training Debug (Iteration 3497) ===
Q mean: -10.012896
Q std: 12.884912
Actor loss: 10.016875
Action reg: 0.003980
  l1.weight: grad_norm = 0.033510
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.030034
Total gradient norm: 0.131100
=== Actor Training Debug (Iteration 3498) ===
Q mean: -8.934513
Q std: 11.967384
Actor loss: 8.938484
Action reg: 0.003971
  l1.weight: grad_norm = 0.037793
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.033182
Total gradient norm: 0.106564
=== Actor Training Debug (Iteration 3499) ===
Q mean: -9.415812
Q std: 12.100389
Actor loss: 9.419785
Action reg: 0.003973
  l1.weight: grad_norm = 0.167213
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.134041
Total gradient norm: 0.413310
=== Actor Training Debug (Iteration 3500) ===
Q mean: -9.922517
Q std: 12.359651
Actor loss: 9.926500
Action reg: 0.003983
  l1.weight: grad_norm = 0.140067
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.125970
Total gradient norm: 0.512594
  Average reward: -327.843 | Average length: 100.0
Evaluation at episode 85: -327.843
=== Actor Training Debug (Iteration 3501) ===
Q mean: -9.754534
Q std: 13.155783
Actor loss: 9.758508
Action reg: 0.003974
  l1.weight: grad_norm = 0.022214
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.023974
Total gradient norm: 0.116820
=== Actor Training Debug (Iteration 3502) ===
Q mean: -8.576668
Q std: 11.128201
Actor loss: 8.580638
Action reg: 0.003971
  l1.weight: grad_norm = 0.124259
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.094705
Total gradient norm: 0.335117
=== Actor Training Debug (Iteration 3503) ===
Q mean: -12.058890
Q std: 14.455494
Actor loss: 12.062870
Action reg: 0.003980
  l1.weight: grad_norm = 0.049975
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.044968
Total gradient norm: 0.145331
=== Actor Training Debug (Iteration 3504) ===
Q mean: -9.896217
Q std: 12.130058
Actor loss: 9.900188
Action reg: 0.003971
  l1.weight: grad_norm = 0.046731
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.037343
Total gradient norm: 0.109640
=== Actor Training Debug (Iteration 3505) ===
Q mean: -9.373931
Q std: 12.269979
Actor loss: 9.377912
Action reg: 0.003980
  l1.weight: grad_norm = 0.056481
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.058421
Total gradient norm: 0.265628
=== Actor Training Debug (Iteration 3506) ===
Q mean: -10.908161
Q std: 13.327654
Actor loss: 10.912144
Action reg: 0.003983
  l1.weight: grad_norm = 0.115499
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.118522
Total gradient norm: 0.513732
=== Actor Training Debug (Iteration 3507) ===
Q mean: -9.277370
Q std: 12.747021
Actor loss: 9.281350
Action reg: 0.003980
  l1.weight: grad_norm = 0.075552
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.070775
Total gradient norm: 0.263095
=== Actor Training Debug (Iteration 3508) ===
Q mean: -10.346802
Q std: 12.477551
Actor loss: 10.350784
Action reg: 0.003983
  l1.weight: grad_norm = 0.033380
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.030669
Total gradient norm: 0.140191
=== Actor Training Debug (Iteration 3509) ===
Q mean: -10.083878
Q std: 11.783471
Actor loss: 10.087860
Action reg: 0.003982
  l1.weight: grad_norm = 0.080235
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.079267
Total gradient norm: 0.291296
=== Actor Training Debug (Iteration 3510) ===
Q mean: -10.118187
Q std: 11.645057
Actor loss: 10.122171
Action reg: 0.003985
  l1.weight: grad_norm = 0.063687
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.052120
Total gradient norm: 0.146349
=== Actor Training Debug (Iteration 3511) ===
Q mean: -10.935224
Q std: 12.855890
Actor loss: 10.939188
Action reg: 0.003965
  l1.weight: grad_norm = 0.066006
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.059068
Total gradient norm: 0.206410
=== Actor Training Debug (Iteration 3512) ===
Q mean: -9.696067
Q std: 12.344503
Actor loss: 9.700052
Action reg: 0.003985
  l1.weight: grad_norm = 0.083802
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.064287
Total gradient norm: 0.177185
=== Actor Training Debug (Iteration 3513) ===
Q mean: -10.792042
Q std: 13.291369
Actor loss: 10.796028
Action reg: 0.003986
  l1.weight: grad_norm = 0.019181
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.018519
Total gradient norm: 0.064839
=== Actor Training Debug (Iteration 3514) ===
Q mean: -9.577745
Q std: 12.829317
Actor loss: 9.581721
Action reg: 0.003976
  l1.weight: grad_norm = 0.051388
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.042718
Total gradient norm: 0.143646
=== Actor Training Debug (Iteration 3515) ===
Q mean: -10.455384
Q std: 12.293927
Actor loss: 10.459365
Action reg: 0.003981
  l1.weight: grad_norm = 0.045397
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.035706
Total gradient norm: 0.102380
=== Actor Training Debug (Iteration 3516) ===
Q mean: -10.261989
Q std: 12.251957
Actor loss: 10.265964
Action reg: 0.003975
  l1.weight: grad_norm = 0.055841
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.057693
Total gradient norm: 0.219872
=== Actor Training Debug (Iteration 3517) ===
Q mean: -9.420528
Q std: 12.557817
Actor loss: 9.424498
Action reg: 0.003969
  l1.weight: grad_norm = 0.080233
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.074971
Total gradient norm: 0.239206
=== Actor Training Debug (Iteration 3518) ===
Q mean: -9.236345
Q std: 12.187111
Actor loss: 9.240318
Action reg: 0.003973
  l1.weight: grad_norm = 0.080303
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.065194
Total gradient norm: 0.204197
=== Actor Training Debug (Iteration 3519) ===
Q mean: -9.288180
Q std: 11.603341
Actor loss: 9.292139
Action reg: 0.003959
  l1.weight: grad_norm = 0.106865
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.094284
Total gradient norm: 0.358710
=== Actor Training Debug (Iteration 3520) ===
Q mean: -9.113618
Q std: 11.262506
Actor loss: 9.117589
Action reg: 0.003971
  l1.weight: grad_norm = 0.028925
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.025465
Total gradient norm: 0.080717
=== Actor Training Debug (Iteration 3521) ===
Q mean: -10.221603
Q std: 12.865455
Actor loss: 10.225582
Action reg: 0.003979
  l1.weight: grad_norm = 0.050428
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.045286
Total gradient norm: 0.146028
=== Actor Training Debug (Iteration 3522) ===
Q mean: -8.459913
Q std: 12.051633
Actor loss: 8.463896
Action reg: 0.003982
  l1.weight: grad_norm = 0.053679
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.043850
Total gradient norm: 0.148907
=== Actor Training Debug (Iteration 3523) ===
Q mean: -10.089508
Q std: 12.589437
Actor loss: 10.093484
Action reg: 0.003976
  l1.weight: grad_norm = 0.075541
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.066461
Total gradient norm: 0.275342
=== Actor Training Debug (Iteration 3524) ===
Q mean: -9.724645
Q std: 12.655022
Actor loss: 9.728627
Action reg: 0.003983
  l1.weight: grad_norm = 0.037934
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.034865
Total gradient norm: 0.130143
=== Actor Training Debug (Iteration 3525) ===
Q mean: -9.814585
Q std: 12.977902
Actor loss: 9.818554
Action reg: 0.003969
  l1.weight: grad_norm = 0.123604
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.087744
Total gradient norm: 0.244549
=== Actor Training Debug (Iteration 3526) ===
Q mean: -9.406314
Q std: 11.979891
Actor loss: 9.410294
Action reg: 0.003980
  l1.weight: grad_norm = 0.076555
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.073013
Total gradient norm: 0.284955
=== Actor Training Debug (Iteration 3527) ===
Q mean: -11.307308
Q std: 13.982305
Actor loss: 11.311288
Action reg: 0.003979
  l1.weight: grad_norm = 0.051189
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.039234
Total gradient norm: 0.117456
=== Actor Training Debug (Iteration 3528) ===
Q mean: -9.038809
Q std: 11.607993
Actor loss: 9.042784
Action reg: 0.003975
  l1.weight: grad_norm = 0.022001
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.017988
Total gradient norm: 0.067518
=== Actor Training Debug (Iteration 3529) ===
Q mean: -9.259483
Q std: 11.957281
Actor loss: 9.263462
Action reg: 0.003978
  l1.weight: grad_norm = 0.130554
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.089922
Total gradient norm: 0.262214
=== Actor Training Debug (Iteration 3530) ===
Q mean: -10.054531
Q std: 12.828039
Actor loss: 10.058498
Action reg: 0.003968
  l1.weight: grad_norm = 0.127514
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.089274
Total gradient norm: 0.303798
=== Actor Training Debug (Iteration 3531) ===
Q mean: -10.133214
Q std: 12.808119
Actor loss: 10.137197
Action reg: 0.003983
  l1.weight: grad_norm = 0.056051
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.044937
Total gradient norm: 0.127940
=== Actor Training Debug (Iteration 3532) ===
Q mean: -9.875956
Q std: 11.879063
Actor loss: 9.879944
Action reg: 0.003988
  l1.weight: grad_norm = 0.066051
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.051410
Total gradient norm: 0.169353
=== Actor Training Debug (Iteration 3533) ===
Q mean: -9.456060
Q std: 12.380932
Actor loss: 9.460032
Action reg: 0.003972
  l1.weight: grad_norm = 0.101859
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.077676
Total gradient norm: 0.257107
=== Actor Training Debug (Iteration 3534) ===
Q mean: -10.678740
Q std: 12.959270
Actor loss: 10.682715
Action reg: 0.003976
  l1.weight: grad_norm = 0.069969
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.069288
Total gradient norm: 0.265499
=== Actor Training Debug (Iteration 3535) ===
Q mean: -10.169932
Q std: 12.616521
Actor loss: 10.173915
Action reg: 0.003983
  l1.weight: grad_norm = 0.060303
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.049625
Total gradient norm: 0.186223
=== Actor Training Debug (Iteration 3536) ===
Q mean: -10.544476
Q std: 12.683039
Actor loss: 10.548450
Action reg: 0.003974
  l1.weight: grad_norm = 0.134935
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.118312
Total gradient norm: 0.434329
=== Actor Training Debug (Iteration 3537) ===
Q mean: -9.219657
Q std: 11.464676
Actor loss: 9.223627
Action reg: 0.003970
  l1.weight: grad_norm = 0.150136
  l1.bias: grad_norm = 0.000428
  l2.weight: grad_norm = 0.129380
Total gradient norm: 0.408979
=== Actor Training Debug (Iteration 3538) ===
Q mean: -8.435378
Q std: 11.682981
Actor loss: 8.439352
Action reg: 0.003973
  l1.weight: grad_norm = 0.058962
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.054915
Total gradient norm: 0.204322
=== Actor Training Debug (Iteration 3539) ===
Q mean: -10.733311
Q std: 13.557915
Actor loss: 10.737288
Action reg: 0.003977
  l1.weight: grad_norm = 0.054031
  l1.bias: grad_norm = 0.000535
  l2.weight: grad_norm = 0.039769
Total gradient norm: 0.144242
=== Actor Training Debug (Iteration 3540) ===
Q mean: -8.309397
Q std: 11.546384
Actor loss: 8.313370
Action reg: 0.003973
  l1.weight: grad_norm = 0.032625
  l1.bias: grad_norm = 0.000378
  l2.weight: grad_norm = 0.029967
Total gradient norm: 0.114324
=== Actor Training Debug (Iteration 3541) ===
Q mean: -8.623697
Q std: 11.140176
Actor loss: 8.627674
Action reg: 0.003977
  l1.weight: grad_norm = 0.163610
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.120625
Total gradient norm: 0.324530
=== Actor Training Debug (Iteration 3542) ===
Q mean: -9.278074
Q std: 11.206596
Actor loss: 9.282055
Action reg: 0.003980
  l1.weight: grad_norm = 0.066289
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.060198
Total gradient norm: 0.205281
=== Actor Training Debug (Iteration 3543) ===
Q mean: -9.402318
Q std: 11.969582
Actor loss: 9.406304
Action reg: 0.003986
  l1.weight: grad_norm = 0.092171
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.064940
Total gradient norm: 0.230482
=== Actor Training Debug (Iteration 3544) ===
Q mean: -10.461762
Q std: 13.570970
Actor loss: 10.465746
Action reg: 0.003983
  l1.weight: grad_norm = 0.047380
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.036016
Total gradient norm: 0.110191
=== Actor Training Debug (Iteration 3545) ===
Q mean: -8.672700
Q std: 11.425522
Actor loss: 8.676687
Action reg: 0.003987
  l1.weight: grad_norm = 0.024504
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.021975
Total gradient norm: 0.078586
=== Actor Training Debug (Iteration 3546) ===
Q mean: -9.776977
Q std: 12.054420
Actor loss: 9.780956
Action reg: 0.003980
  l1.weight: grad_norm = 0.077480
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.065000
Total gradient norm: 0.216856
=== Actor Training Debug (Iteration 3547) ===
Q mean: -9.578999
Q std: 12.150908
Actor loss: 9.582988
Action reg: 0.003989
  l1.weight: grad_norm = 0.043034
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.031371
Total gradient norm: 0.091981
=== Actor Training Debug (Iteration 3548) ===
Q mean: -8.277748
Q std: 11.252335
Actor loss: 8.281729
Action reg: 0.003980
  l1.weight: grad_norm = 0.113264
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.093297
Total gradient norm: 0.330120
=== Actor Training Debug (Iteration 3549) ===
Q mean: -10.096636
Q std: 12.516486
Actor loss: 10.100612
Action reg: 0.003976
  l1.weight: grad_norm = 0.044134
  l1.bias: grad_norm = 0.000525
  l2.weight: grad_norm = 0.035792
Total gradient norm: 0.109649
=== Actor Training Debug (Iteration 3550) ===
Q mean: -9.052109
Q std: 11.307657
Actor loss: 9.056082
Action reg: 0.003973
  l1.weight: grad_norm = 0.030949
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.028928
Total gradient norm: 0.097186
=== Actor Training Debug (Iteration 3551) ===
Q mean: -10.157488
Q std: 12.451859
Actor loss: 10.161464
Action reg: 0.003976
  l1.weight: grad_norm = 0.044045
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.046057
Total gradient norm: 0.191388
=== Actor Training Debug (Iteration 3552) ===
Q mean: -9.292716
Q std: 11.967068
Actor loss: 9.296687
Action reg: 0.003971
  l1.weight: grad_norm = 0.037358
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.037928
Total gradient norm: 0.140877
=== Actor Training Debug (Iteration 3553) ===
Q mean: -9.577854
Q std: 12.109994
Actor loss: 9.581840
Action reg: 0.003985
  l1.weight: grad_norm = 0.163005
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.135958
Total gradient norm: 0.512280
=== Actor Training Debug (Iteration 3554) ===
Q mean: -9.671489
Q std: 12.459506
Actor loss: 9.675468
Action reg: 0.003980
  l1.weight: grad_norm = 0.039101
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.035283
Total gradient norm: 0.113518
=== Actor Training Debug (Iteration 3555) ===
Q mean: -9.517884
Q std: 11.762953
Actor loss: 9.521862
Action reg: 0.003978
  l1.weight: grad_norm = 0.028265
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.027226
Total gradient norm: 0.094467
=== Actor Training Debug (Iteration 3556) ===
Q mean: -8.433352
Q std: 11.028621
Actor loss: 8.437328
Action reg: 0.003977
  l1.weight: grad_norm = 0.050876
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.047350
Total gradient norm: 0.191665
=== Actor Training Debug (Iteration 3557) ===
Q mean: -10.599132
Q std: 12.443203
Actor loss: 10.603105
Action reg: 0.003973
  l1.weight: grad_norm = 0.046842
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.038187
Total gradient norm: 0.141768
=== Actor Training Debug (Iteration 3558) ===
Q mean: -9.661814
Q std: 13.318786
Actor loss: 9.665794
Action reg: 0.003980
  l1.weight: grad_norm = 0.057018
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.051208
Total gradient norm: 0.175242
=== Actor Training Debug (Iteration 3559) ===
Q mean: -10.020057
Q std: 12.347387
Actor loss: 10.024038
Action reg: 0.003981
  l1.weight: grad_norm = 0.031500
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.031673
Total gradient norm: 0.165829
=== Actor Training Debug (Iteration 3560) ===
Q mean: -9.279131
Q std: 13.195763
Actor loss: 9.283099
Action reg: 0.003968
  l1.weight: grad_norm = 0.158511
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.149075
Total gradient norm: 0.810741
=== Actor Training Debug (Iteration 3561) ===
Q mean: -9.565328
Q std: 12.239785
Actor loss: 9.569302
Action reg: 0.003974
  l1.weight: grad_norm = 0.016801
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.014316
Total gradient norm: 0.052185
=== Actor Training Debug (Iteration 3562) ===
Q mean: -9.889048
Q std: 11.925046
Actor loss: 9.893020
Action reg: 0.003972
  l1.weight: grad_norm = 0.077738
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.070167
Total gradient norm: 0.233259
=== Actor Training Debug (Iteration 3563) ===
Q mean: -9.923452
Q std: 13.515835
Actor loss: 9.927429
Action reg: 0.003977
  l1.weight: grad_norm = 0.092143
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.075824
Total gradient norm: 0.239856
=== Actor Training Debug (Iteration 3564) ===
Q mean: -10.200934
Q std: 13.484366
Actor loss: 10.204913
Action reg: 0.003979
  l1.weight: grad_norm = 0.118118
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.077133
Total gradient norm: 0.222615
=== Actor Training Debug (Iteration 3565) ===
Q mean: -10.018321
Q std: 11.485501
Actor loss: 10.022304
Action reg: 0.003982
  l1.weight: grad_norm = 0.074144
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.067998
Total gradient norm: 0.241090
=== Actor Training Debug (Iteration 3566) ===
Q mean: -10.671405
Q std: 12.071779
Actor loss: 10.675385
Action reg: 0.003981
  l1.weight: grad_norm = 0.060891
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.051986
Total gradient norm: 0.156072
=== Actor Training Debug (Iteration 3567) ===
Q mean: -9.418312
Q std: 12.260292
Actor loss: 9.422299
Action reg: 0.003987
  l1.weight: grad_norm = 0.025096
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.022783
Total gradient norm: 0.089521
=== Actor Training Debug (Iteration 3568) ===
Q mean: -8.978209
Q std: 11.345636
Actor loss: 8.982200
Action reg: 0.003991
  l1.weight: grad_norm = 0.009145
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.008311
Total gradient norm: 0.027876
=== Actor Training Debug (Iteration 3569) ===
Q mean: -10.317820
Q std: 11.952415
Actor loss: 10.321796
Action reg: 0.003977
  l1.weight: grad_norm = 0.032910
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 0.026097
Total gradient norm: 0.098858
=== Actor Training Debug (Iteration 3570) ===
Q mean: -10.372483
Q std: 13.249331
Actor loss: 10.376454
Action reg: 0.003971
  l1.weight: grad_norm = 0.051587
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.049964
Total gradient norm: 0.196769
=== Actor Training Debug (Iteration 3571) ===
Q mean: -9.284740
Q std: 12.304296
Actor loss: 9.288716
Action reg: 0.003976
  l1.weight: grad_norm = 0.092573
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.078405
Total gradient norm: 0.305463
=== Actor Training Debug (Iteration 3572) ===
Q mean: -9.820111
Q std: 12.665524
Actor loss: 9.824094
Action reg: 0.003983
  l1.weight: grad_norm = 0.047397
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.040827
Total gradient norm: 0.139872
=== Actor Training Debug (Iteration 3573) ===
Q mean: -8.545135
Q std: 12.028429
Actor loss: 8.549115
Action reg: 0.003980
  l1.weight: grad_norm = 0.032308
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.024738
Total gradient norm: 0.085206
=== Actor Training Debug (Iteration 3574) ===
Q mean: -10.411693
Q std: 12.440753
Actor loss: 10.415675
Action reg: 0.003983
  l1.weight: grad_norm = 0.069257
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.063597
Total gradient norm: 0.223806
=== Actor Training Debug (Iteration 3575) ===
Q mean: -9.662627
Q std: 11.914635
Actor loss: 9.666612
Action reg: 0.003984
  l1.weight: grad_norm = 0.054001
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.042852
Total gradient norm: 0.182244
=== Actor Training Debug (Iteration 3576) ===
Q mean: -9.941830
Q std: 13.176660
Actor loss: 9.945802
Action reg: 0.003972
  l1.weight: grad_norm = 0.046726
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.041368
Total gradient norm: 0.124035
=== Actor Training Debug (Iteration 3577) ===
Q mean: -8.933337
Q std: 12.138317
Actor loss: 8.937310
Action reg: 0.003973
  l1.weight: grad_norm = 0.085049
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.075596
Total gradient norm: 0.277011
=== Actor Training Debug (Iteration 3578) ===
Q mean: -8.896152
Q std: 11.941355
Actor loss: 8.900133
Action reg: 0.003981
  l1.weight: grad_norm = 0.091603
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.076921
Total gradient norm: 0.271407
=== Actor Training Debug (Iteration 3579) ===
Q mean: -9.046621
Q std: 11.426422
Actor loss: 9.050593
Action reg: 0.003972
  l1.weight: grad_norm = 0.042650
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.034919
Total gradient norm: 0.115094
=== Actor Training Debug (Iteration 3580) ===
Q mean: -9.903231
Q std: 12.026682
Actor loss: 9.907203
Action reg: 0.003972
  l1.weight: grad_norm = 0.089243
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.081886
Total gradient norm: 0.321690
=== Actor Training Debug (Iteration 3581) ===
Q mean: -10.169495
Q std: 12.801398
Actor loss: 10.173475
Action reg: 0.003980
  l1.weight: grad_norm = 0.038974
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.036742
Total gradient norm: 0.154238
=== Actor Training Debug (Iteration 3582) ===
Q mean: -9.522022
Q std: 12.479587
Actor loss: 9.525998
Action reg: 0.003976
  l1.weight: grad_norm = 0.066356
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.051779
Total gradient norm: 0.143371
=== Actor Training Debug (Iteration 3583) ===
Q mean: -9.028866
Q std: 12.591178
Actor loss: 9.032848
Action reg: 0.003983
  l1.weight: grad_norm = 0.046127
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.040914
Total gradient norm: 0.157386
=== Actor Training Debug (Iteration 3584) ===
Q mean: -10.114491
Q std: 12.049603
Actor loss: 10.118472
Action reg: 0.003980
  l1.weight: grad_norm = 0.054368
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.046298
Total gradient norm: 0.135811
=== Actor Training Debug (Iteration 3585) ===
Q mean: -10.576683
Q std: 13.088162
Actor loss: 10.580664
Action reg: 0.003980
  l1.weight: grad_norm = 0.092397
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.070975
Total gradient norm: 0.233300
=== Actor Training Debug (Iteration 3586) ===
Q mean: -9.163616
Q std: 12.446796
Actor loss: 9.167598
Action reg: 0.003981
  l1.weight: grad_norm = 0.038332
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.035209
Total gradient norm: 0.168308
=== Actor Training Debug (Iteration 3587) ===
Q mean: -10.565985
Q std: 12.316492
Actor loss: 10.569973
Action reg: 0.003989
  l1.weight: grad_norm = 0.015948
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.012708
Total gradient norm: 0.034954
=== Actor Training Debug (Iteration 3588) ===
Q mean: -9.493666
Q std: 11.910694
Actor loss: 9.497641
Action reg: 0.003975
  l1.weight: grad_norm = 0.095315
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.079477
Total gradient norm: 0.269007
=== Actor Training Debug (Iteration 3589) ===
Q mean: -10.363600
Q std: 13.101941
Actor loss: 10.367577
Action reg: 0.003977
  l1.weight: grad_norm = 0.062275
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.046363
Total gradient norm: 0.167376
=== Actor Training Debug (Iteration 3590) ===
Q mean: -10.480352
Q std: 12.433328
Actor loss: 10.484327
Action reg: 0.003975
  l1.weight: grad_norm = 0.082094
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.068638
Total gradient norm: 0.255548
=== Actor Training Debug (Iteration 3591) ===
Q mean: -9.509927
Q std: 12.434238
Actor loss: 9.513905
Action reg: 0.003978
  l1.weight: grad_norm = 0.025294
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.024062
Total gradient norm: 0.106873
=== Actor Training Debug (Iteration 3592) ===
Q mean: -11.770520
Q std: 14.096691
Actor loss: 11.774497
Action reg: 0.003977
  l1.weight: grad_norm = 0.048359
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.038193
Total gradient norm: 0.135534
=== Actor Training Debug (Iteration 3593) ===
Q mean: -9.053526
Q std: 12.524615
Actor loss: 9.057504
Action reg: 0.003977
  l1.weight: grad_norm = 0.084816
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.069819
Total gradient norm: 0.286503
=== Actor Training Debug (Iteration 3594) ===
Q mean: -11.429550
Q std: 12.478743
Actor loss: 11.433537
Action reg: 0.003987
  l1.weight: grad_norm = 0.047317
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.041452
Total gradient norm: 0.128105
=== Actor Training Debug (Iteration 3595) ===
Q mean: -8.929257
Q std: 10.611600
Actor loss: 8.933237
Action reg: 0.003979
  l1.weight: grad_norm = 0.058667
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.050781
Total gradient norm: 0.216492
=== Actor Training Debug (Iteration 3596) ===
Q mean: -10.150729
Q std: 13.209836
Actor loss: 10.154716
Action reg: 0.003987
  l1.weight: grad_norm = 0.051409
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.042847
Total gradient norm: 0.164692
=== Actor Training Debug (Iteration 3597) ===
Q mean: -10.822266
Q std: 13.042932
Actor loss: 10.826246
Action reg: 0.003981
  l1.weight: grad_norm = 0.039184
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.037496
Total gradient norm: 0.145827
=== Actor Training Debug (Iteration 3598) ===
Q mean: -10.379005
Q std: 12.162883
Actor loss: 10.382995
Action reg: 0.003989
  l1.weight: grad_norm = 0.025079
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.019524
Total gradient norm: 0.066237
=== Actor Training Debug (Iteration 3599) ===
Q mean: -10.526965
Q std: 13.036889
Actor loss: 10.530946
Action reg: 0.003981
  l1.weight: grad_norm = 0.053523
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.052412
Total gradient norm: 0.253998
=== Actor Training Debug (Iteration 3600) ===
Q mean: -10.735806
Q std: 12.506110
Actor loss: 10.739784
Action reg: 0.003979
  l1.weight: grad_norm = 0.061482
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.049106
Total gradient norm: 0.185071
=== Actor Training Debug (Iteration 3601) ===
Q mean: -11.153123
Q std: 11.694775
Actor loss: 11.157108
Action reg: 0.003985
  l1.weight: grad_norm = 0.049579
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.043151
Total gradient norm: 0.154851
=== Actor Training Debug (Iteration 3602) ===
Q mean: -10.297037
Q std: 12.462308
Actor loss: 10.301016
Action reg: 0.003979
  l1.weight: grad_norm = 0.055504
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.055079
Total gradient norm: 0.245097
=== Actor Training Debug (Iteration 3603) ===
Q mean: -10.726580
Q std: 13.880419
Actor loss: 10.730560
Action reg: 0.003981
  l1.weight: grad_norm = 0.101186
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.085384
Total gradient norm: 0.280682
=== Actor Training Debug (Iteration 3604) ===
Q mean: -9.800004
Q std: 13.081250
Actor loss: 9.803990
Action reg: 0.003986
  l1.weight: grad_norm = 0.058451
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.051395
Total gradient norm: 0.213696
=== Actor Training Debug (Iteration 3605) ===
Q mean: -10.059290
Q std: 12.283217
Actor loss: 10.063260
Action reg: 0.003971
  l1.weight: grad_norm = 0.027949
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.027737
Total gradient norm: 0.113250
=== Actor Training Debug (Iteration 3606) ===
Q mean: -9.186125
Q std: 12.303082
Actor loss: 9.190094
Action reg: 0.003969
  l1.weight: grad_norm = 0.089582
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.080486
Total gradient norm: 0.268865
=== Actor Training Debug (Iteration 3607) ===
Q mean: -9.821030
Q std: 11.238362
Actor loss: 9.825005
Action reg: 0.003975
  l1.weight: grad_norm = 0.032166
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.029920
Total gradient norm: 0.113327
=== Actor Training Debug (Iteration 3608) ===
Q mean: -10.987089
Q std: 12.856878
Actor loss: 10.991063
Action reg: 0.003974
  l1.weight: grad_norm = 0.080001
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.073606
Total gradient norm: 0.314401
=== Actor Training Debug (Iteration 3609) ===
Q mean: -9.627403
Q std: 11.987193
Actor loss: 9.631384
Action reg: 0.003980
  l1.weight: grad_norm = 0.122952
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.099597
Total gradient norm: 0.314273
=== Actor Training Debug (Iteration 3610) ===
Q mean: -10.531211
Q std: 13.233905
Actor loss: 10.535192
Action reg: 0.003982
  l1.weight: grad_norm = 0.056669
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.059654
Total gradient norm: 0.244609
=== Actor Training Debug (Iteration 3611) ===
Q mean: -10.093542
Q std: 12.518099
Actor loss: 10.097511
Action reg: 0.003970
  l1.weight: grad_norm = 0.055242
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.043017
Total gradient norm: 0.152283
=== Actor Training Debug (Iteration 3612) ===
Q mean: -10.466981
Q std: 11.914531
Actor loss: 10.470963
Action reg: 0.003983
  l1.weight: grad_norm = 0.069171
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.061810
Total gradient norm: 0.233175
=== Actor Training Debug (Iteration 3613) ===
Q mean: -10.402479
Q std: 12.936443
Actor loss: 10.406457
Action reg: 0.003977
  l1.weight: grad_norm = 0.161286
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.126134
Total gradient norm: 0.442122
=== Actor Training Debug (Iteration 3614) ===
Q mean: -9.549421
Q std: 12.438773
Actor loss: 9.553394
Action reg: 0.003973
  l1.weight: grad_norm = 0.064917
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.065419
Total gradient norm: 0.256859
=== Actor Training Debug (Iteration 3615) ===
Q mean: -8.743814
Q std: 12.284336
Actor loss: 8.747784
Action reg: 0.003969
  l1.weight: grad_norm = 0.020788
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.019783
Total gradient norm: 0.073191
=== Actor Training Debug (Iteration 3616) ===
Q mean: -10.229080
Q std: 12.620268
Actor loss: 10.233059
Action reg: 0.003979
  l1.weight: grad_norm = 0.061543
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.071858
Total gradient norm: 0.298846
=== Actor Training Debug (Iteration 3617) ===
Q mean: -10.326112
Q std: 12.575851
Actor loss: 10.330068
Action reg: 0.003956
  l1.weight: grad_norm = 0.113152
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.106405
Total gradient norm: 0.436614
=== Actor Training Debug (Iteration 3618) ===
Q mean: -8.592588
Q std: 11.814784
Actor loss: 8.596561
Action reg: 0.003973
  l1.weight: grad_norm = 0.069821
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.055861
Total gradient norm: 0.173830
=== Actor Training Debug (Iteration 3619) ===
Q mean: -9.800709
Q std: 12.667460
Actor loss: 9.804686
Action reg: 0.003977
  l1.weight: grad_norm = 0.168188
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.136728
Total gradient norm: 0.442898
=== Actor Training Debug (Iteration 3620) ===
Q mean: -11.436398
Q std: 12.815177
Actor loss: 11.440364
Action reg: 0.003966
  l1.weight: grad_norm = 0.130980
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.120434
Total gradient norm: 0.480563
=== Actor Training Debug (Iteration 3621) ===
Q mean: -10.139997
Q std: 12.258159
Actor loss: 10.143967
Action reg: 0.003970
  l1.weight: grad_norm = 0.100362
  l1.bias: grad_norm = 0.000608
  l2.weight: grad_norm = 0.092143
Total gradient norm: 0.308048
=== Actor Training Debug (Iteration 3622) ===
Q mean: -9.978533
Q std: 12.718307
Actor loss: 9.982505
Action reg: 0.003972
  l1.weight: grad_norm = 0.059432
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.051762
Total gradient norm: 0.195578
=== Actor Training Debug (Iteration 3623) ===
Q mean: -9.149754
Q std: 11.802518
Actor loss: 9.153730
Action reg: 0.003977
  l1.weight: grad_norm = 0.051959
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.042737
Total gradient norm: 0.128993
=== Actor Training Debug (Iteration 3624) ===
Q mean: -10.288639
Q std: 12.483449
Actor loss: 10.292610
Action reg: 0.003971
  l1.weight: grad_norm = 0.050646
  l1.bias: grad_norm = 0.001005
  l2.weight: grad_norm = 0.044691
Total gradient norm: 0.205243
=== Actor Training Debug (Iteration 3625) ===
Q mean: -10.288978
Q std: 13.740399
Actor loss: 10.292946
Action reg: 0.003968
  l1.weight: grad_norm = 0.110197
  l1.bias: grad_norm = 0.000872
  l2.weight: grad_norm = 0.087044
Total gradient norm: 0.287465
=== Actor Training Debug (Iteration 3626) ===
Q mean: -9.281624
Q std: 12.562652
Actor loss: 9.285601
Action reg: 0.003977
  l1.weight: grad_norm = 0.090150
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.089344
Total gradient norm: 0.294357
=== Actor Training Debug (Iteration 3627) ===
Q mean: -12.311139
Q std: 14.167114
Actor loss: 12.315123
Action reg: 0.003984
  l1.weight: grad_norm = 0.077970
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.070909
Total gradient norm: 0.205080
=== Actor Training Debug (Iteration 3628) ===
Q mean: -10.857139
Q std: 13.299220
Actor loss: 10.861114
Action reg: 0.003975
  l1.weight: grad_norm = 0.066406
  l1.bias: grad_norm = 0.000736
  l2.weight: grad_norm = 0.054175
Total gradient norm: 0.191310
=== Actor Training Debug (Iteration 3629) ===
Q mean: -10.674317
Q std: 12.939590
Actor loss: 10.678301
Action reg: 0.003984
  l1.weight: grad_norm = 0.140229
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.117324
Total gradient norm: 0.349912
=== Actor Training Debug (Iteration 3630) ===
Q mean: -10.687042
Q std: 14.365574
Actor loss: 10.691022
Action reg: 0.003980
  l1.weight: grad_norm = 0.203343
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.162386
Total gradient norm: 0.448607
=== Actor Training Debug (Iteration 3631) ===
Q mean: -10.644793
Q std: 12.974546
Actor loss: 10.648763
Action reg: 0.003970
  l1.weight: grad_norm = 0.065737
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.051255
Total gradient norm: 0.160411
=== Actor Training Debug (Iteration 3632) ===
Q mean: -11.270351
Q std: 14.046597
Actor loss: 11.274331
Action reg: 0.003979
  l1.weight: grad_norm = 0.056539
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.053970
Total gradient norm: 0.208196
=== Actor Training Debug (Iteration 3633) ===
Q mean: -8.238332
Q std: 11.212135
Actor loss: 8.242305
Action reg: 0.003973
  l1.weight: grad_norm = 0.104350
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.081171
Total gradient norm: 0.348052
=== Actor Training Debug (Iteration 3634) ===
Q mean: -9.610527
Q std: 12.239256
Actor loss: 9.614511
Action reg: 0.003984
  l1.weight: grad_norm = 0.045548
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.045064
Total gradient norm: 0.191414
=== Actor Training Debug (Iteration 3635) ===
Q mean: -10.895956
Q std: 12.902921
Actor loss: 10.899937
Action reg: 0.003981
  l1.weight: grad_norm = 0.079041
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.077138
Total gradient norm: 0.233779
=== Actor Training Debug (Iteration 3636) ===
Q mean: -10.716309
Q std: 13.038662
Actor loss: 10.720279
Action reg: 0.003971
  l1.weight: grad_norm = 0.100043
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.077747
Total gradient norm: 0.244141
=== Actor Training Debug (Iteration 3637) ===
Q mean: -9.167120
Q std: 12.460268
Actor loss: 9.171094
Action reg: 0.003974
  l1.weight: grad_norm = 0.107465
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.094161
Total gradient norm: 0.270513
=== Actor Training Debug (Iteration 3638) ===
Q mean: -9.063384
Q std: 11.688127
Actor loss: 9.067368
Action reg: 0.003984
  l1.weight: grad_norm = 0.016901
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.015624
Total gradient norm: 0.053969
=== Actor Training Debug (Iteration 3639) ===
Q mean: -10.005165
Q std: 12.219679
Actor loss: 10.009140
Action reg: 0.003975
  l1.weight: grad_norm = 0.115616
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.085852
Total gradient norm: 0.240729
=== Actor Training Debug (Iteration 3640) ===
Q mean: -10.095535
Q std: 12.744092
Actor loss: 10.099513
Action reg: 0.003978
  l1.weight: grad_norm = 0.025130
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.021514
Total gradient norm: 0.068454
=== Actor Training Debug (Iteration 3641) ===
Q mean: -8.246775
Q std: 11.610566
Actor loss: 8.250751
Action reg: 0.003977
  l1.weight: grad_norm = 0.101612
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.075469
Total gradient norm: 0.225173
=== Actor Training Debug (Iteration 3642) ===
Q mean: -9.925657
Q std: 11.811414
Actor loss: 9.929629
Action reg: 0.003972
  l1.weight: grad_norm = 0.042420
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.036889
Total gradient norm: 0.124881
=== Actor Training Debug (Iteration 3643) ===
Q mean: -8.560837
Q std: 11.975224
Actor loss: 8.564821
Action reg: 0.003984
  l1.weight: grad_norm = 0.037211
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.034224
Total gradient norm: 0.109434
=== Actor Training Debug (Iteration 3644) ===
Q mean: -8.966791
Q std: 11.960570
Actor loss: 8.970770
Action reg: 0.003978
  l1.weight: grad_norm = 0.043694
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.042713
Total gradient norm: 0.133231
=== Actor Training Debug (Iteration 3645) ===
Q mean: -10.007758
Q std: 12.249684
Actor loss: 10.011730
Action reg: 0.003972
  l1.weight: grad_norm = 0.075043
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.068269
Total gradient norm: 0.235085
=== Actor Training Debug (Iteration 3646) ===
Q mean: -9.580072
Q std: 12.567001
Actor loss: 9.584025
Action reg: 0.003953
  l1.weight: grad_norm = 0.083757
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.075350
Total gradient norm: 0.244335
=== Actor Training Debug (Iteration 3647) ===
Q mean: -10.410464
Q std: 13.154975
Actor loss: 10.414438
Action reg: 0.003974
  l1.weight: grad_norm = 0.101010
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.083629
Total gradient norm: 0.255667
=== Actor Training Debug (Iteration 3648) ===
Q mean: -9.387565
Q std: 11.701447
Actor loss: 9.391536
Action reg: 0.003971
  l1.weight: grad_norm = 0.160902
  l1.bias: grad_norm = 0.000519
  l2.weight: grad_norm = 0.233887
Total gradient norm: 1.523864
=== Actor Training Debug (Iteration 3649) ===
Q mean: -9.785915
Q std: 12.247578
Actor loss: 9.789868
Action reg: 0.003953
  l1.weight: grad_norm = 0.899634
  l1.bias: grad_norm = 0.000686
  l2.weight: grad_norm = 1.010861
Total gradient norm: 4.990631
=== Actor Training Debug (Iteration 3650) ===
Q mean: -8.996841
Q std: 10.798078
Actor loss: 9.000821
Action reg: 0.003979
  l1.weight: grad_norm = 0.090704
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.074652
Total gradient norm: 0.220964
=== Actor Training Debug (Iteration 3651) ===
Q mean: -8.359708
Q std: 11.933773
Actor loss: 8.363685
Action reg: 0.003977
  l1.weight: grad_norm = 0.145286
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.141189
Total gradient norm: 0.464391
=== Actor Training Debug (Iteration 3652) ===
Q mean: -10.239224
Q std: 13.438914
Actor loss: 10.243192
Action reg: 0.003967
  l1.weight: grad_norm = 0.146289
  l1.bias: grad_norm = 0.001234
  l2.weight: grad_norm = 0.167250
Total gradient norm: 0.831139
=== Actor Training Debug (Iteration 3653) ===
Q mean: -10.043081
Q std: 11.772676
Actor loss: 10.047051
Action reg: 0.003970
  l1.weight: grad_norm = 0.165196
  l1.bias: grad_norm = 0.000737
  l2.weight: grad_norm = 0.157002
Total gradient norm: 0.538678
=== Actor Training Debug (Iteration 3654) ===
Q mean: -10.738895
Q std: 12.559790
Actor loss: 10.742878
Action reg: 0.003983
  l1.weight: grad_norm = 0.083444
  l1.bias: grad_norm = 0.000025
  l2.weight: grad_norm = 0.072405
Total gradient norm: 0.241620
=== Actor Training Debug (Iteration 3655) ===
Q mean: -9.147959
Q std: 12.288151
Actor loss: 9.151925
Action reg: 0.003966
  l1.weight: grad_norm = 0.069211
  l1.bias: grad_norm = 0.000796
  l2.weight: grad_norm = 0.057140
Total gradient norm: 0.204997
=== Actor Training Debug (Iteration 3656) ===
Q mean: -10.091916
Q std: 11.904026
Actor loss: 10.095885
Action reg: 0.003970
  l1.weight: grad_norm = 0.053106
  l1.bias: grad_norm = 0.001807
  l2.weight: grad_norm = 0.046153
Total gradient norm: 0.145711
=== Actor Training Debug (Iteration 3657) ===
Q mean: -8.625471
Q std: 12.025757
Actor loss: 8.629436
Action reg: 0.003965
  l1.weight: grad_norm = 0.117250
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.115629
Total gradient norm: 0.389287
=== Actor Training Debug (Iteration 3658) ===
Q mean: -9.358114
Q std: 12.556914
Actor loss: 9.362078
Action reg: 0.003964
  l1.weight: grad_norm = 0.130339
  l1.bias: grad_norm = 0.000778
  l2.weight: grad_norm = 0.117494
Total gradient norm: 0.398923
=== Actor Training Debug (Iteration 3659) ===
Q mean: -10.696676
Q std: 12.256027
Actor loss: 10.700633
Action reg: 0.003957
  l1.weight: grad_norm = 0.121170
  l1.bias: grad_norm = 0.001663
  l2.weight: grad_norm = 0.120637
Total gradient norm: 0.384895
=== Actor Training Debug (Iteration 3660) ===
Q mean: -10.368093
Q std: 12.732862
Actor loss: 10.372070
Action reg: 0.003977
  l1.weight: grad_norm = 0.099268
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.113717
Total gradient norm: 0.474113
=== Actor Training Debug (Iteration 3661) ===
Q mean: -10.166469
Q std: 12.967221
Actor loss: 10.170442
Action reg: 0.003973
  l1.weight: grad_norm = 0.096249
  l1.bias: grad_norm = 0.000728
  l2.weight: grad_norm = 0.100080
Total gradient norm: 0.439250
=== Actor Training Debug (Iteration 3662) ===
Q mean: -10.470461
Q std: 13.677545
Actor loss: 10.474428
Action reg: 0.003968
  l1.weight: grad_norm = 0.145264
  l1.bias: grad_norm = 0.000915
  l2.weight: grad_norm = 0.117664
Total gradient norm: 0.464975
=== Actor Training Debug (Iteration 3663) ===
Q mean: -10.682882
Q std: 13.768437
Actor loss: 10.686849
Action reg: 0.003966
  l1.weight: grad_norm = 0.141986
  l1.bias: grad_norm = 0.001139
  l2.weight: grad_norm = 0.106471
Total gradient norm: 0.416491
=== Actor Training Debug (Iteration 3664) ===
Q mean: -9.958885
Q std: 11.964595
Actor loss: 9.962870
Action reg: 0.003984
  l1.weight: grad_norm = 0.055899
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.061102
Total gradient norm: 0.238315
=== Actor Training Debug (Iteration 3665) ===
Q mean: -8.873844
Q std: 12.470929
Actor loss: 8.877821
Action reg: 0.003976
  l1.weight: grad_norm = 0.111678
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.092965
Total gradient norm: 0.261183
=== Actor Training Debug (Iteration 3666) ===
Q mean: -10.311016
Q std: 12.912676
Actor loss: 10.315000
Action reg: 0.003983
  l1.weight: grad_norm = 0.080815
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.065147
Total gradient norm: 0.284212
=== Actor Training Debug (Iteration 3667) ===
Q mean: -11.176800
Q std: 14.072855
Actor loss: 11.180781
Action reg: 0.003981
  l1.weight: grad_norm = 0.055525
  l1.bias: grad_norm = 0.001049
  l2.weight: grad_norm = 0.046723
Total gradient norm: 0.147064
=== Actor Training Debug (Iteration 3668) ===
Q mean: -9.843161
Q std: 13.193748
Actor loss: 9.847131
Action reg: 0.003970
  l1.weight: grad_norm = 0.119575
  l1.bias: grad_norm = 0.001431
  l2.weight: grad_norm = 0.090019
Total gradient norm: 0.254028
=== Actor Training Debug (Iteration 3669) ===
Q mean: -10.427162
Q std: 13.185183
Actor loss: 10.431137
Action reg: 0.003975
  l1.weight: grad_norm = 0.075797
  l1.bias: grad_norm = 0.000839
  l2.weight: grad_norm = 0.067937
Total gradient norm: 0.262994
=== Actor Training Debug (Iteration 3670) ===
Q mean: -10.921238
Q std: 13.617448
Actor loss: 10.925222
Action reg: 0.003984
  l1.weight: grad_norm = 0.048226
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.045661
Total gradient norm: 0.189212
=== Actor Training Debug (Iteration 3671) ===
Q mean: -10.346325
Q std: 12.932241
Actor loss: 10.350305
Action reg: 0.003980
  l1.weight: grad_norm = 0.102448
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.095291
Total gradient norm: 0.315634
=== Actor Training Debug (Iteration 3672) ===
Q mean: -9.677740
Q std: 12.173890
Actor loss: 9.681722
Action reg: 0.003982
  l1.weight: grad_norm = 0.061146
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.052575
Total gradient norm: 0.202634
=== Actor Training Debug (Iteration 3673) ===
Q mean: -10.876062
Q std: 13.889294
Actor loss: 10.880039
Action reg: 0.003977
  l1.weight: grad_norm = 0.074993
  l1.bias: grad_norm = 0.000867
  l2.weight: grad_norm = 0.070195
Total gradient norm: 0.255416
=== Actor Training Debug (Iteration 3674) ===
Q mean: -10.066555
Q std: 12.566323
Actor loss: 10.070531
Action reg: 0.003976
  l1.weight: grad_norm = 0.029097
  l1.bias: grad_norm = 0.001865
  l2.weight: grad_norm = 0.027107
Total gradient norm: 0.105784
=== Actor Training Debug (Iteration 3675) ===
Q mean: -10.431810
Q std: 13.098333
Actor loss: 10.435787
Action reg: 0.003977
  l1.weight: grad_norm = 0.047819
  l1.bias: grad_norm = 0.001074
  l2.weight: grad_norm = 0.046699
Total gradient norm: 0.145882
=== Actor Training Debug (Iteration 3676) ===
Q mean: -9.382674
Q std: 12.172818
Actor loss: 9.386648
Action reg: 0.003974
  l1.weight: grad_norm = 0.060452
  l1.bias: grad_norm = 0.000836
  l2.weight: grad_norm = 0.061104
Total gradient norm: 0.226242
=== Actor Training Debug (Iteration 3677) ===
Q mean: -9.630157
Q std: 11.044089
Actor loss: 9.634136
Action reg: 0.003978
  l1.weight: grad_norm = 0.050703
  l1.bias: grad_norm = 0.000688
  l2.weight: grad_norm = 0.044876
Total gradient norm: 0.178291
=== Actor Training Debug (Iteration 3678) ===
Q mean: -9.441767
Q std: 12.551459
Actor loss: 9.445745
Action reg: 0.003978
  l1.weight: grad_norm = 0.155302
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.170283
Total gradient norm: 0.690211
=== Actor Training Debug (Iteration 3679) ===
Q mean: -10.566002
Q std: 13.097783
Actor loss: 10.569968
Action reg: 0.003966
  l1.weight: grad_norm = 0.133345
  l1.bias: grad_norm = 0.000821
  l2.weight: grad_norm = 0.090922
Total gradient norm: 0.293973
=== Actor Training Debug (Iteration 3680) ===
Q mean: -11.227472
Q std: 14.019552
Actor loss: 11.231448
Action reg: 0.003976
  l1.weight: grad_norm = 0.029683
  l1.bias: grad_norm = 0.000627
  l2.weight: grad_norm = 0.027348
Total gradient norm: 0.101788
=== Actor Training Debug (Iteration 3681) ===
Q mean: -9.310238
Q std: 12.050339
Actor loss: 9.314216
Action reg: 0.003977
  l1.weight: grad_norm = 0.172418
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.128958
Total gradient norm: 0.372907
=== Actor Training Debug (Iteration 3682) ===
Q mean: -9.976517
Q std: 12.149473
Actor loss: 9.980475
Action reg: 0.003959
  l1.weight: grad_norm = 0.123659
  l1.bias: grad_norm = 0.000556
  l2.weight: grad_norm = 0.101693
Total gradient norm: 0.376644
=== Actor Training Debug (Iteration 3683) ===
Q mean: -10.188226
Q std: 13.428223
Actor loss: 10.192202
Action reg: 0.003975
  l1.weight: grad_norm = 0.051814
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.047106
Total gradient norm: 0.178126
=== Actor Training Debug (Iteration 3684) ===
Q mean: -9.108944
Q std: 11.938229
Actor loss: 9.112886
Action reg: 0.003942
  l1.weight: grad_norm = 0.184189
  l1.bias: grad_norm = 0.000811
  l2.weight: grad_norm = 0.136551
Total gradient norm: 0.449015
=== Actor Training Debug (Iteration 3685) ===
Q mean: -9.606475
Q std: 12.441562
Actor loss: 9.610449
Action reg: 0.003974
  l1.weight: grad_norm = 0.039900
  l1.bias: grad_norm = 0.000857
  l2.weight: grad_norm = 0.035125
Total gradient norm: 0.105922
=== Actor Training Debug (Iteration 3686) ===
Q mean: -11.436083
Q std: 13.648965
Actor loss: 11.440063
Action reg: 0.003979
  l1.weight: grad_norm = 0.038858
  l1.bias: grad_norm = 0.000526
  l2.weight: grad_norm = 0.033749
Total gradient norm: 0.124507
=== Actor Training Debug (Iteration 3687) ===
Q mean: -10.225755
Q std: 12.288046
Actor loss: 10.229714
Action reg: 0.003959
  l1.weight: grad_norm = 0.086129
  l1.bias: grad_norm = 0.001028
  l2.weight: grad_norm = 0.081035
Total gradient norm: 0.256297
=== Actor Training Debug (Iteration 3688) ===
Q mean: -10.379987
Q std: 13.105985
Actor loss: 10.383958
Action reg: 0.003971
  l1.weight: grad_norm = 0.092240
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.066612
Total gradient norm: 0.204240
=== Actor Training Debug (Iteration 3689) ===
Q mean: -10.098034
Q std: 13.531466
Actor loss: 10.101993
Action reg: 0.003958
  l1.weight: grad_norm = 0.070889
  l1.bias: grad_norm = 0.002242
  l2.weight: grad_norm = 0.060749
Total gradient norm: 0.224212
=== Actor Training Debug (Iteration 3690) ===
Q mean: -9.556242
Q std: 12.333675
Actor loss: 9.560220
Action reg: 0.003978
  l1.weight: grad_norm = 0.162428
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.128564
Total gradient norm: 0.526175
=== Actor Training Debug (Iteration 3691) ===
Q mean: -10.585632
Q std: 13.353066
Actor loss: 10.589594
Action reg: 0.003962
  l1.weight: grad_norm = 0.362932
  l1.bias: grad_norm = 0.001101
  l2.weight: grad_norm = 0.271092
Total gradient norm: 0.877721
=== Actor Training Debug (Iteration 3692) ===
Q mean: -11.095943
Q std: 12.628906
Actor loss: 11.099913
Action reg: 0.003969
  l1.weight: grad_norm = 0.063564
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.054749
Total gradient norm: 0.176612
=== Actor Training Debug (Iteration 3693) ===
Q mean: -11.504146
Q std: 13.829293
Actor loss: 11.508127
Action reg: 0.003982
  l1.weight: grad_norm = 0.094165
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.089857
Total gradient norm: 0.289823
=== Actor Training Debug (Iteration 3694) ===
Q mean: -11.290057
Q std: 13.363703
Actor loss: 11.294032
Action reg: 0.003974
  l1.weight: grad_norm = 0.073558
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.061802
Total gradient norm: 0.222090
=== Actor Training Debug (Iteration 3695) ===
Q mean: -9.004450
Q std: 12.255804
Actor loss: 9.008419
Action reg: 0.003969
  l1.weight: grad_norm = 0.115272
  l1.bias: grad_norm = 0.000933
  l2.weight: grad_norm = 0.091882
Total gradient norm: 0.367174
=== Actor Training Debug (Iteration 3696) ===
Q mean: -8.125883
Q std: 11.980420
Actor loss: 8.129861
Action reg: 0.003978
  l1.weight: grad_norm = 0.089815
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.078369
Total gradient norm: 0.231102
=== Actor Training Debug (Iteration 3697) ===
Q mean: -10.171449
Q std: 12.320617
Actor loss: 10.175427
Action reg: 0.003979
  l1.weight: grad_norm = 0.107903
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.085444
Total gradient norm: 0.283229
=== Actor Training Debug (Iteration 3698) ===
Q mean: -10.998493
Q std: 12.869886
Actor loss: 11.002472
Action reg: 0.003978
  l1.weight: grad_norm = 0.099648
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.095947
Total gradient norm: 0.345480
=== Actor Training Debug (Iteration 3699) ===
Q mean: -10.127956
Q std: 12.477975
Actor loss: 10.131927
Action reg: 0.003970
  l1.weight: grad_norm = 0.076398
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.075665
Total gradient norm: 0.270150
=== Actor Training Debug (Iteration 3700) ===
Q mean: -10.600981
Q std: 13.330476
Actor loss: 10.604956
Action reg: 0.003975
  l1.weight: grad_norm = 0.048957
  l1.bias: grad_norm = 0.000805
  l2.weight: grad_norm = 0.045763
Total gradient norm: 0.215941
=== Actor Training Debug (Iteration 3701) ===
Q mean: -8.727908
Q std: 12.215736
Actor loss: 8.731882
Action reg: 0.003974
  l1.weight: grad_norm = 0.102791
  l1.bias: grad_norm = 0.000989
  l2.weight: grad_norm = 0.080788
Total gradient norm: 0.301949
=== Actor Training Debug (Iteration 3702) ===
Q mean: -8.715235
Q std: 11.352831
Actor loss: 8.719209
Action reg: 0.003974
  l1.weight: grad_norm = 0.066213
  l1.bias: grad_norm = 0.000892
  l2.weight: grad_norm = 0.061961
Total gradient norm: 0.205294
=== Actor Training Debug (Iteration 3703) ===
Q mean: -11.550692
Q std: 13.158839
Actor loss: 11.554670
Action reg: 0.003979
  l1.weight: grad_norm = 0.075400
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.073620
Total gradient norm: 0.276281
=== Actor Training Debug (Iteration 3704) ===
Q mean: -8.577234
Q std: 12.551994
Actor loss: 8.581195
Action reg: 0.003960
  l1.weight: grad_norm = 0.147088
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.109582
Total gradient norm: 0.348138
=== Actor Training Debug (Iteration 3705) ===
Q mean: -10.367832
Q std: 13.669701
Actor loss: 10.371802
Action reg: 0.003970
  l1.weight: grad_norm = 0.085347
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.074966
Total gradient norm: 0.299217
=== Actor Training Debug (Iteration 3706) ===
Q mean: -10.435658
Q std: 12.954760
Actor loss: 10.439639
Action reg: 0.003981
  l1.weight: grad_norm = 0.033310
  l1.bias: grad_norm = 0.001464
  l2.weight: grad_norm = 0.029144
Total gradient norm: 0.100237
=== Actor Training Debug (Iteration 3707) ===
Q mean: -9.058363
Q std: 11.669373
Actor loss: 9.062341
Action reg: 0.003977
  l1.weight: grad_norm = 0.057246
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.056153
Total gradient norm: 0.164977
=== Actor Training Debug (Iteration 3708) ===
Q mean: -11.201681
Q std: 13.359738
Actor loss: 11.205649
Action reg: 0.003968
  l1.weight: grad_norm = 0.032948
  l1.bias: grad_norm = 0.002404
  l2.weight: grad_norm = 0.031227
Total gradient norm: 0.140266
=== Actor Training Debug (Iteration 3709) ===
Q mean: -9.309893
Q std: 11.993758
Actor loss: 9.313846
Action reg: 0.003953
  l1.weight: grad_norm = 0.128311
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.118552
Total gradient norm: 0.475615
=== Actor Training Debug (Iteration 3710) ===
Q mean: -10.898319
Q std: 14.221297
Actor loss: 10.902284
Action reg: 0.003964
  l1.weight: grad_norm = 0.049361
  l1.bias: grad_norm = 0.000843
  l2.weight: grad_norm = 0.046396
Total gradient norm: 0.172091
=== Actor Training Debug (Iteration 3711) ===
Q mean: -10.333110
Q std: 12.325427
Actor loss: 10.337102
Action reg: 0.003992
  l1.weight: grad_norm = 0.026875
  l1.bias: grad_norm = 0.000015
  l2.weight: grad_norm = 0.025323
Total gradient norm: 0.078054
=== Actor Training Debug (Iteration 3712) ===
Q mean: -10.530334
Q std: 11.785669
Actor loss: 10.534317
Action reg: 0.003983
  l1.weight: grad_norm = 0.024608
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.019251
Total gradient norm: 0.063234
=== Actor Training Debug (Iteration 3713) ===
Q mean: -7.942966
Q std: 11.100025
Actor loss: 7.946944
Action reg: 0.003978
  l1.weight: grad_norm = 0.287649
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.199995
Total gradient norm: 0.838968
=== Actor Training Debug (Iteration 3714) ===
Q mean: -9.852238
Q std: 12.275567
Actor loss: 9.856207
Action reg: 0.003969
  l1.weight: grad_norm = 0.100285
  l1.bias: grad_norm = 0.000666
  l2.weight: grad_norm = 0.087607
Total gradient norm: 0.282049
=== Actor Training Debug (Iteration 3715) ===
Q mean: -9.782832
Q std: 13.471703
Actor loss: 9.786806
Action reg: 0.003974
  l1.weight: grad_norm = 0.163349
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.140868
Total gradient norm: 0.434695
=== Actor Training Debug (Iteration 3716) ===
Q mean: -9.827172
Q std: 11.453835
Actor loss: 9.831143
Action reg: 0.003971
  l1.weight: grad_norm = 0.060020
  l1.bias: grad_norm = 0.000566
  l2.weight: grad_norm = 0.058038
Total gradient norm: 0.177026
=== Actor Training Debug (Iteration 3717) ===
Q mean: -9.387292
Q std: 12.758044
Actor loss: 9.391262
Action reg: 0.003970
  l1.weight: grad_norm = 0.175162
  l1.bias: grad_norm = 0.000484
  l2.weight: grad_norm = 0.141851
Total gradient norm: 0.489399
=== Actor Training Debug (Iteration 3718) ===
Q mean: -9.233672
Q std: 12.873793
Actor loss: 9.237640
Action reg: 0.003968
  l1.weight: grad_norm = 0.106458
  l1.bias: grad_norm = 0.000894
  l2.weight: grad_norm = 0.083224
Total gradient norm: 0.278697
=== Actor Training Debug (Iteration 3719) ===
Q mean: -10.846248
Q std: 13.189305
Actor loss: 10.850216
Action reg: 0.003968
  l1.weight: grad_norm = 0.065663
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.051008
Total gradient norm: 0.165566
=== Actor Training Debug (Iteration 3720) ===
Q mean: -11.187749
Q std: 14.217327
Actor loss: 11.191725
Action reg: 0.003976
  l1.weight: grad_norm = 0.028637
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.022864
Total gradient norm: 0.064682
=== Actor Training Debug (Iteration 3721) ===
Q mean: -10.257345
Q std: 12.900145
Actor loss: 10.261314
Action reg: 0.003969
  l1.weight: grad_norm = 0.154369
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.117905
Total gradient norm: 0.442004
=== Actor Training Debug (Iteration 3722) ===
Q mean: -10.114335
Q std: 12.308959
Actor loss: 10.118308
Action reg: 0.003973
  l1.weight: grad_norm = 0.107851
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.097871
Total gradient norm: 0.332854
=== Actor Training Debug (Iteration 3723) ===
Q mean: -9.072586
Q std: 11.979868
Actor loss: 9.076551
Action reg: 0.003966
  l1.weight: grad_norm = 0.136199
  l1.bias: grad_norm = 0.000650
  l2.weight: grad_norm = 0.125110
Total gradient norm: 0.445879
=== Actor Training Debug (Iteration 3724) ===
Q mean: -9.712095
Q std: 12.276348
Actor loss: 9.716075
Action reg: 0.003979
  l1.weight: grad_norm = 0.132735
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.101715
Total gradient norm: 0.324254
=== Actor Training Debug (Iteration 3725) ===
Q mean: -10.126971
Q std: 12.694658
Actor loss: 10.130949
Action reg: 0.003978
  l1.weight: grad_norm = 0.073459
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.059428
Total gradient norm: 0.211534
=== Actor Training Debug (Iteration 3726) ===
Q mean: -10.413782
Q std: 12.602411
Actor loss: 10.417737
Action reg: 0.003955
  l1.weight: grad_norm = 0.060032
  l1.bias: grad_norm = 0.000435
  l2.weight: grad_norm = 0.053145
Total gradient norm: 0.181912
=== Actor Training Debug (Iteration 3727) ===
Q mean: -9.974502
Q std: 13.308800
Actor loss: 9.978468
Action reg: 0.003966
  l1.weight: grad_norm = 0.134351
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.091264
Total gradient norm: 0.257779
=== Actor Training Debug (Iteration 3728) ===
Q mean: -8.991125
Q std: 11.460742
Actor loss: 8.995098
Action reg: 0.003973
  l1.weight: grad_norm = 0.128821
  l1.bias: grad_norm = 0.000507
  l2.weight: grad_norm = 0.096269
Total gradient norm: 0.298110
=== Actor Training Debug (Iteration 3729) ===
Q mean: -11.305452
Q std: 14.092879
Actor loss: 11.309443
Action reg: 0.003991
  l1.weight: grad_norm = 0.059683
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.048907
Total gradient norm: 0.174896
=== Actor Training Debug (Iteration 3730) ===
Q mean: -10.388077
Q std: 12.785232
Actor loss: 10.392035
Action reg: 0.003958
  l1.weight: grad_norm = 0.072777
  l1.bias: grad_norm = 0.001022
  l2.weight: grad_norm = 0.061685
Total gradient norm: 0.182050
=== Actor Training Debug (Iteration 3731) ===
Q mean: -9.804602
Q std: 13.589113
Actor loss: 9.808582
Action reg: 0.003981
  l1.weight: grad_norm = 0.071514
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.056932
Total gradient norm: 0.172434
=== Actor Training Debug (Iteration 3732) ===
Q mean: -9.503757
Q std: 13.185818
Actor loss: 9.507724
Action reg: 0.003967
  l1.weight: grad_norm = 0.131600
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.110125
Total gradient norm: 0.422640
=== Actor Training Debug (Iteration 3733) ===
Q mean: -9.818661
Q std: 12.254455
Actor loss: 9.822634
Action reg: 0.003973
  l1.weight: grad_norm = 0.105712
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.095161
Total gradient norm: 0.311131
=== Actor Training Debug (Iteration 3734) ===
Q mean: -9.126871
Q std: 11.883191
Actor loss: 9.130842
Action reg: 0.003971
  l1.weight: grad_norm = 0.042671
  l1.bias: grad_norm = 0.000579
  l2.weight: grad_norm = 0.037580
Total gradient norm: 0.111775
=== Actor Training Debug (Iteration 3735) ===
Q mean: -9.099264
Q std: 11.651417
Actor loss: 9.103233
Action reg: 0.003970
  l1.weight: grad_norm = 0.103570
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.087356
Total gradient norm: 0.314249
=== Actor Training Debug (Iteration 3736) ===
Q mean: -10.107298
Q std: 12.527883
Actor loss: 10.111262
Action reg: 0.003964
  l1.weight: grad_norm = 0.101296
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.084602
Total gradient norm: 0.261390
=== Actor Training Debug (Iteration 3737) ===
Q mean: -10.548756
Q std: 12.079131
Actor loss: 10.552737
Action reg: 0.003981
  l1.weight: grad_norm = 0.072332
  l1.bias: grad_norm = 0.000616
  l2.weight: grad_norm = 0.068222
Total gradient norm: 0.212520
=== Actor Training Debug (Iteration 3738) ===
Q mean: -9.615377
Q std: 11.967525
Actor loss: 9.619343
Action reg: 0.003965
  l1.weight: grad_norm = 0.128676
  l1.bias: grad_norm = 0.000803
  l2.weight: grad_norm = 0.110240
Total gradient norm: 0.365150
=== Actor Training Debug (Iteration 3739) ===
Q mean: -9.948918
Q std: 13.641993
Actor loss: 9.952901
Action reg: 0.003982
  l1.weight: grad_norm = 0.028518
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.022666
Total gradient norm: 0.079483
=== Actor Training Debug (Iteration 3740) ===
Q mean: -10.247195
Q std: 13.070635
Actor loss: 10.251164
Action reg: 0.003969
  l1.weight: grad_norm = 0.086418
  l1.bias: grad_norm = 0.000417
  l2.weight: grad_norm = 0.079384
Total gradient norm: 0.287428
=== Actor Training Debug (Iteration 3741) ===
Q mean: -10.147575
Q std: 11.782671
Actor loss: 10.151543
Action reg: 0.003968
  l1.weight: grad_norm = 0.070931
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.059428
Total gradient norm: 0.196499
=== Actor Training Debug (Iteration 3742) ===
Q mean: -9.512403
Q std: 12.663860
Actor loss: 9.516367
Action reg: 0.003963
  l1.weight: grad_norm = 0.096785
  l1.bias: grad_norm = 0.000542
  l2.weight: grad_norm = 0.082324
Total gradient norm: 0.248771
=== Actor Training Debug (Iteration 3743) ===
Q mean: -8.906254
Q std: 11.898340
Actor loss: 8.910234
Action reg: 0.003981
  l1.weight: grad_norm = 0.051486
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.039526
Total gradient norm: 0.113224
=== Actor Training Debug (Iteration 3744) ===
Q mean: -9.997026
Q std: 12.888916
Actor loss: 10.000998
Action reg: 0.003971
  l1.weight: grad_norm = 0.094799
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.080175
Total gradient norm: 0.226527
=== Actor Training Debug (Iteration 3745) ===
Q mean: -11.258072
Q std: 13.012046
Actor loss: 11.262026
Action reg: 0.003954
  l1.weight: grad_norm = 0.071691
  l1.bias: grad_norm = 0.001542
  l2.weight: grad_norm = 0.066132
Total gradient norm: 0.193265
=== Actor Training Debug (Iteration 3746) ===
Q mean: -11.935338
Q std: 14.081601
Actor loss: 11.939316
Action reg: 0.003978
  l1.weight: grad_norm = 0.065549
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.063962
Total gradient norm: 0.237171
=== Actor Training Debug (Iteration 3747) ===
Q mean: -10.792015
Q std: 13.365252
Actor loss: 10.795984
Action reg: 0.003970
  l1.weight: grad_norm = 0.133033
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.115402
Total gradient norm: 0.371662
=== Actor Training Debug (Iteration 3748) ===
Q mean: -9.779469
Q std: 12.550563
Actor loss: 9.783436
Action reg: 0.003966
  l1.weight: grad_norm = 0.108422
  l1.bias: grad_norm = 0.001438
  l2.weight: grad_norm = 0.088022
Total gradient norm: 0.297740
=== Actor Training Debug (Iteration 3749) ===
Q mean: -11.112165
Q std: 13.535784
Actor loss: 11.116138
Action reg: 0.003972
  l1.weight: grad_norm = 0.087026
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.072839
Total gradient norm: 0.209176
=== Actor Training Debug (Iteration 3750) ===
Q mean: -9.692080
Q std: 12.443134
Actor loss: 9.696049
Action reg: 0.003968
  l1.weight: grad_norm = 0.027343
  l1.bias: grad_norm = 0.000823
  l2.weight: grad_norm = 0.026826
Total gradient norm: 0.106583
=== Actor Training Debug (Iteration 3751) ===
Q mean: -11.412525
Q std: 13.389144
Actor loss: 11.416501
Action reg: 0.003976
  l1.weight: grad_norm = 0.124024
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.103474
Total gradient norm: 0.431054
=== Actor Training Debug (Iteration 3752) ===
Q mean: -9.590595
Q std: 12.536311
Actor loss: 9.594574
Action reg: 0.003978
  l1.weight: grad_norm = 0.071786
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.070382
Total gradient norm: 0.216505
=== Actor Training Debug (Iteration 3753) ===
Q mean: -11.131735
Q std: 13.650744
Actor loss: 11.135703
Action reg: 0.003969
  l1.weight: grad_norm = 0.059068
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.047809
Total gradient norm: 0.150666
=== Actor Training Debug (Iteration 3754) ===
Q mean: -8.481275
Q std: 11.086578
Actor loss: 8.485246
Action reg: 0.003971
  l1.weight: grad_norm = 0.106305
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.089936
Total gradient norm: 0.358425
=== Actor Training Debug (Iteration 3755) ===
Q mean: -9.977058
Q std: 12.662535
Actor loss: 9.981034
Action reg: 0.003976
  l1.weight: grad_norm = 0.046884
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.040009
Total gradient norm: 0.132185
=== Actor Training Debug (Iteration 3756) ===
Q mean: -10.785179
Q std: 13.300192
Actor loss: 10.789154
Action reg: 0.003975
  l1.weight: grad_norm = 0.047526
  l1.bias: grad_norm = 0.000826
  l2.weight: grad_norm = 0.045423
Total gradient norm: 0.145183
=== Actor Training Debug (Iteration 3757) ===
Q mean: -11.254659
Q std: 13.035995
Actor loss: 11.258634
Action reg: 0.003975
  l1.weight: grad_norm = 0.084785
  l1.bias: grad_norm = 0.000622
  l2.weight: grad_norm = 0.070770
Total gradient norm: 0.244276
=== Actor Training Debug (Iteration 3758) ===
Q mean: -10.416364
Q std: 13.393994
Actor loss: 10.420341
Action reg: 0.003978
  l1.weight: grad_norm = 0.197135
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.161720
Total gradient norm: 0.472616
=== Actor Training Debug (Iteration 3759) ===
Q mean: -10.740810
Q std: 13.376074
Actor loss: 10.744780
Action reg: 0.003970
  l1.weight: grad_norm = 0.086099
  l1.bias: grad_norm = 0.001419
  l2.weight: grad_norm = 0.083184
Total gradient norm: 0.261079
=== Actor Training Debug (Iteration 3760) ===
Q mean: -9.474361
Q std: 12.175607
Actor loss: 9.478334
Action reg: 0.003973
  l1.weight: grad_norm = 0.037750
  l1.bias: grad_norm = 0.000877
  l2.weight: grad_norm = 0.030901
Total gradient norm: 0.114069
=== Actor Training Debug (Iteration 3761) ===
Q mean: -10.412878
Q std: 12.513017
Actor loss: 10.416869
Action reg: 0.003991
  l1.weight: grad_norm = 0.050059
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.037825
Total gradient norm: 0.119277
=== Actor Training Debug (Iteration 3762) ===
Q mean: -10.712057
Q std: 13.422138
Actor loss: 10.716032
Action reg: 0.003975
  l1.weight: grad_norm = 0.133570
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.102993
Total gradient norm: 0.335842
=== Actor Training Debug (Iteration 3763) ===
Q mean: -9.659945
Q std: 12.884910
Actor loss: 9.663910
Action reg: 0.003965
  l1.weight: grad_norm = 0.122285
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.096485
Total gradient norm: 0.265920
=== Actor Training Debug (Iteration 3764) ===
Q mean: -10.970760
Q std: 13.064106
Actor loss: 10.974735
Action reg: 0.003975
  l1.weight: grad_norm = 0.059636
  l1.bias: grad_norm = 0.000609
  l2.weight: grad_norm = 0.053393
Total gradient norm: 0.173203
=== Actor Training Debug (Iteration 3765) ===
Q mean: -8.745174
Q std: 11.671656
Actor loss: 8.749146
Action reg: 0.003972
  l1.weight: grad_norm = 0.086842
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.085727
Total gradient norm: 0.235718
=== Actor Training Debug (Iteration 3766) ===
Q mean: -9.880801
Q std: 12.943958
Actor loss: 9.884766
Action reg: 0.003964
  l1.weight: grad_norm = 0.097137
  l1.bias: grad_norm = 0.000613
  l2.weight: grad_norm = 0.078090
Total gradient norm: 0.240121
=== Actor Training Debug (Iteration 3767) ===
Q mean: -10.628256
Q std: 12.179961
Actor loss: 10.632236
Action reg: 0.003980
  l1.weight: grad_norm = 0.099783
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.090765
Total gradient norm: 0.303236
=== Actor Training Debug (Iteration 3768) ===
Q mean: -10.305201
Q std: 12.457243
Actor loss: 10.309170
Action reg: 0.003969
  l1.weight: grad_norm = 0.038005
  l1.bias: grad_norm = 0.000654
  l2.weight: grad_norm = 0.040628
Total gradient norm: 0.148378
=== Actor Training Debug (Iteration 3769) ===
Q mean: -9.848145
Q std: 12.882518
Actor loss: 9.852118
Action reg: 0.003973
  l1.weight: grad_norm = 0.088580
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 0.083309
Total gradient norm: 0.289404
=== Actor Training Debug (Iteration 3770) ===
Q mean: -10.889141
Q std: 13.539008
Actor loss: 10.893076
Action reg: 0.003935
  l1.weight: grad_norm = 0.112451
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.083883
Total gradient norm: 0.304114
=== Actor Training Debug (Iteration 3771) ===
Q mean: -10.090426
Q std: 12.694327
Actor loss: 10.094420
Action reg: 0.003994
  l1.weight: grad_norm = 0.029135
  l1.bias: grad_norm = 0.000013
  l2.weight: grad_norm = 0.028766
Total gradient norm: 0.090785
=== Actor Training Debug (Iteration 3772) ===
Q mean: -10.282261
Q std: 13.170353
Actor loss: 10.286226
Action reg: 0.003965
  l1.weight: grad_norm = 0.107006
  l1.bias: grad_norm = 0.000774
  l2.weight: grad_norm = 0.100267
Total gradient norm: 0.421308
=== Actor Training Debug (Iteration 3773) ===
Q mean: -10.322712
Q std: 12.735093
Actor loss: 10.326684
Action reg: 0.003972
  l1.weight: grad_norm = 0.087523
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.065158
Total gradient norm: 0.230419
=== Actor Training Debug (Iteration 3774) ===
Q mean: -11.516371
Q std: 13.976989
Actor loss: 11.520341
Action reg: 0.003970
  l1.weight: grad_norm = 0.070093
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.056398
Total gradient norm: 0.198519
=== Actor Training Debug (Iteration 3775) ===
Q mean: -9.584236
Q std: 13.318683
Actor loss: 9.588223
Action reg: 0.003988
  l1.weight: grad_norm = 0.075503
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.071040
Total gradient norm: 0.220146
=== Actor Training Debug (Iteration 3776) ===
Q mean: -12.234377
Q std: 14.180301
Actor loss: 12.238361
Action reg: 0.003984
  l1.weight: grad_norm = 0.053958
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.050692
Total gradient norm: 0.194226
=== Actor Training Debug (Iteration 3777) ===
Q mean: -10.183760
Q std: 12.072228
Actor loss: 10.187735
Action reg: 0.003975
  l1.weight: grad_norm = 0.106877
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.088280
Total gradient norm: 0.323048
=== Actor Training Debug (Iteration 3778) ===
Q mean: -10.318371
Q std: 12.981647
Actor loss: 10.322343
Action reg: 0.003972
  l1.weight: grad_norm = 0.100640
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.086583
Total gradient norm: 0.295918
=== Actor Training Debug (Iteration 3779) ===
Q mean: -10.649169
Q std: 12.667013
Actor loss: 10.653144
Action reg: 0.003975
  l1.weight: grad_norm = 0.071375
  l1.bias: grad_norm = 0.000768
  l2.weight: grad_norm = 0.066543
Total gradient norm: 0.242075
=== Actor Training Debug (Iteration 3780) ===
Q mean: -10.932272
Q std: 13.837173
Actor loss: 10.936244
Action reg: 0.003972
  l1.weight: grad_norm = 0.090739
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.075259
Total gradient norm: 0.216610
=== Actor Training Debug (Iteration 3781) ===
Q mean: -11.336477
Q std: 13.304663
Actor loss: 11.340443
Action reg: 0.003965
  l1.weight: grad_norm = 0.109137
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.091261
Total gradient norm: 0.282763
=== Actor Training Debug (Iteration 3782) ===
Q mean: -9.746218
Q std: 12.879410
Actor loss: 9.750198
Action reg: 0.003980
  l1.weight: grad_norm = 0.064085
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.057898
Total gradient norm: 0.184875
=== Actor Training Debug (Iteration 3783) ===
Q mean: -9.820869
Q std: 11.994936
Actor loss: 9.824848
Action reg: 0.003978
  l1.weight: grad_norm = 0.075821
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.075823
Total gradient norm: 0.288469
=== Actor Training Debug (Iteration 3784) ===
Q mean: -11.007252
Q std: 13.785439
Actor loss: 11.011224
Action reg: 0.003972
  l1.weight: grad_norm = 0.114452
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.113496
Total gradient norm: 0.337895
=== Actor Training Debug (Iteration 3785) ===
Q mean: -10.499802
Q std: 13.759194
Actor loss: 10.503774
Action reg: 0.003972
  l1.weight: grad_norm = 0.152628
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.142233
Total gradient norm: 0.611160
=== Actor Training Debug (Iteration 3786) ===
Q mean: -10.248306
Q std: 12.534200
Actor loss: 10.252264
Action reg: 0.003958
  l1.weight: grad_norm = 0.094878
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.085806
Total gradient norm: 0.263285
=== Actor Training Debug (Iteration 3787) ===
Q mean: -11.167161
Q std: 13.876204
Actor loss: 11.171138
Action reg: 0.003977
  l1.weight: grad_norm = 0.145347
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.123273
Total gradient norm: 0.380457
=== Actor Training Debug (Iteration 3788) ===
Q mean: -10.663372
Q std: 12.727044
Actor loss: 10.667347
Action reg: 0.003975
  l1.weight: grad_norm = 0.091067
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.075856
Total gradient norm: 0.294072
=== Actor Training Debug (Iteration 3789) ===
Q mean: -9.296954
Q std: 11.976141
Actor loss: 9.300920
Action reg: 0.003966
  l1.weight: grad_norm = 0.159373
  l1.bias: grad_norm = 0.000493
  l2.weight: grad_norm = 0.126770
Total gradient norm: 0.392121
=== Actor Training Debug (Iteration 3790) ===
Q mean: -9.030752
Q std: 11.813804
Actor loss: 9.034706
Action reg: 0.003954
  l1.weight: grad_norm = 0.084301
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.073355
Total gradient norm: 0.218187
=== Actor Training Debug (Iteration 3791) ===
Q mean: -11.514740
Q std: 14.828355
Actor loss: 11.518709
Action reg: 0.003970
  l1.weight: grad_norm = 0.133969
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.120427
Total gradient norm: 0.425622
=== Actor Training Debug (Iteration 3792) ===
Q mean: -11.811137
Q std: 14.322568
Actor loss: 11.815111
Action reg: 0.003974
  l1.weight: grad_norm = 0.092144
  l1.bias: grad_norm = 0.000638
  l2.weight: grad_norm = 0.082231
Total gradient norm: 0.299169
=== Actor Training Debug (Iteration 3793) ===
Q mean: -10.695127
Q std: 13.045926
Actor loss: 10.699099
Action reg: 0.003971
  l1.weight: grad_norm = 0.071074
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.076594
Total gradient norm: 0.246499
=== Actor Training Debug (Iteration 3794) ===
Q mean: -8.828300
Q std: 12.000943
Actor loss: 8.832271
Action reg: 0.003970
  l1.weight: grad_norm = 0.090929
  l1.bias: grad_norm = 0.000544
  l2.weight: grad_norm = 0.083529
Total gradient norm: 0.303922
=== Actor Training Debug (Iteration 3795) ===
Q mean: -11.542200
Q std: 14.224055
Actor loss: 11.546166
Action reg: 0.003966
  l1.weight: grad_norm = 0.168722
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.134511
Total gradient norm: 0.518467
=== Actor Training Debug (Iteration 3796) ===
Q mean: -10.108960
Q std: 12.385859
Actor loss: 10.112928
Action reg: 0.003968
  l1.weight: grad_norm = 0.164676
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.135041
Total gradient norm: 0.427763
=== Actor Training Debug (Iteration 3797) ===
Q mean: -10.362548
Q std: 12.337191
Actor loss: 10.366512
Action reg: 0.003964
  l1.weight: grad_norm = 0.092279
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.073090
Total gradient norm: 0.243893
=== Actor Training Debug (Iteration 3798) ===
Q mean: -10.362648
Q std: 12.608468
Actor loss: 10.366603
Action reg: 0.003955
  l1.weight: grad_norm = 0.058600
  l1.bias: grad_norm = 0.001148
  l2.weight: grad_norm = 0.057191
Total gradient norm: 0.211230
=== Actor Training Debug (Iteration 3799) ===
Q mean: -10.199204
Q std: 12.886306
Actor loss: 10.203180
Action reg: 0.003976
  l1.weight: grad_norm = 0.042782
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.039670
Total gradient norm: 0.126620
=== Actor Training Debug (Iteration 3800) ===
Q mean: -10.314833
Q std: 12.974030
Actor loss: 10.318813
Action reg: 0.003981
  l1.weight: grad_norm = 0.067359
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.056219
Total gradient norm: 0.180835
=== Actor Training Debug (Iteration 3801) ===
Q mean: -9.304792
Q std: 11.208349
Actor loss: 9.308760
Action reg: 0.003968
  l1.weight: grad_norm = 0.127838
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.095686
Total gradient norm: 0.274701
=== Actor Training Debug (Iteration 3802) ===
Q mean: -10.007409
Q std: 12.167084
Actor loss: 10.011391
Action reg: 0.003981
  l1.weight: grad_norm = 0.056061
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.056327
Total gradient norm: 0.187637
=== Actor Training Debug (Iteration 3803) ===
Q mean: -9.520639
Q std: 12.653595
Actor loss: 9.524611
Action reg: 0.003971
  l1.weight: grad_norm = 0.122845
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.098528
Total gradient norm: 0.336912
=== Actor Training Debug (Iteration 3804) ===
Q mean: -9.781764
Q std: 13.681451
Actor loss: 9.785732
Action reg: 0.003969
  l1.weight: grad_norm = 0.143177
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.120843
Total gradient norm: 0.422050
=== Actor Training Debug (Iteration 3805) ===
Q mean: -10.588880
Q std: 14.989434
Actor loss: 10.592844
Action reg: 0.003965
  l1.weight: grad_norm = 0.183006
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.161250
Total gradient norm: 0.451289
=== Actor Training Debug (Iteration 3806) ===
Q mean: -10.372465
Q std: 12.541209
Actor loss: 10.376438
Action reg: 0.003973
  l1.weight: grad_norm = 0.044412
  l1.bias: grad_norm = 0.000746
  l2.weight: grad_norm = 0.041455
Total gradient norm: 0.155898
=== Actor Training Debug (Iteration 3807) ===
Q mean: -11.514860
Q std: 13.250500
Actor loss: 11.518833
Action reg: 0.003973
  l1.weight: grad_norm = 0.042056
  l1.bias: grad_norm = 0.001146
  l2.weight: grad_norm = 0.033416
Total gradient norm: 0.114525
=== Actor Training Debug (Iteration 3808) ===
Q mean: -10.263323
Q std: 13.010865
Actor loss: 10.267306
Action reg: 0.003983
  l1.weight: grad_norm = 0.122300
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.104610
Total gradient norm: 0.344578
=== Actor Training Debug (Iteration 3809) ===
Q mean: -9.473736
Q std: 11.308372
Actor loss: 9.477712
Action reg: 0.003975
  l1.weight: grad_norm = 0.107320
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.088444
Total gradient norm: 0.297748
=== Actor Training Debug (Iteration 3810) ===
Q mean: -9.743257
Q std: 12.123666
Actor loss: 9.747235
Action reg: 0.003979
  l1.weight: grad_norm = 0.042771
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.047059
Total gradient norm: 0.148075
=== Actor Training Debug (Iteration 3811) ===
Q mean: -11.619967
Q std: 14.154057
Actor loss: 11.623937
Action reg: 0.003970
  l1.weight: grad_norm = 0.087049
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.070369
Total gradient norm: 0.224609
=== Actor Training Debug (Iteration 3812) ===
Q mean: -9.015785
Q std: 11.520367
Actor loss: 9.019747
Action reg: 0.003961
  l1.weight: grad_norm = 0.102280
  l1.bias: grad_norm = 0.001137
  l2.weight: grad_norm = 0.094886
Total gradient norm: 0.314495
=== Actor Training Debug (Iteration 3813) ===
Q mean: -10.286577
Q std: 14.121112
Actor loss: 10.290549
Action reg: 0.003972
  l1.weight: grad_norm = 0.033275
  l1.bias: grad_norm = 0.001198
  l2.weight: grad_norm = 0.034127
Total gradient norm: 0.108545
=== Actor Training Debug (Iteration 3814) ===
Q mean: -9.976730
Q std: 12.906336
Actor loss: 9.980702
Action reg: 0.003972
  l1.weight: grad_norm = 0.080593
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.073712
Total gradient norm: 0.268274
=== Actor Training Debug (Iteration 3815) ===
Q mean: -9.003605
Q std: 12.449488
Actor loss: 9.007573
Action reg: 0.003968
  l1.weight: grad_norm = 0.143617
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.138867
Total gradient norm: 0.456741
=== Actor Training Debug (Iteration 3816) ===
Q mean: -8.752783
Q std: 11.225636
Actor loss: 8.756750
Action reg: 0.003967
  l1.weight: grad_norm = 0.066781
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.060452
Total gradient norm: 0.216454
=== Actor Training Debug (Iteration 3817) ===
Q mean: -10.350595
Q std: 13.293486
Actor loss: 10.354571
Action reg: 0.003976
  l1.weight: grad_norm = 0.068369
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.065412
Total gradient norm: 0.203575
=== Actor Training Debug (Iteration 3818) ===
Q mean: -8.927578
Q std: 12.017875
Actor loss: 8.931543
Action reg: 0.003966
  l1.weight: grad_norm = 0.089319
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.079336
Total gradient norm: 0.290062
=== Actor Training Debug (Iteration 3819) ===
Q mean: -10.387631
Q std: 12.944571
Actor loss: 10.391608
Action reg: 0.003977
  l1.weight: grad_norm = 0.167122
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.161511
Total gradient norm: 0.527866
=== Actor Training Debug (Iteration 3820) ===
Q mean: -9.617208
Q std: 12.215548
Actor loss: 9.621183
Action reg: 0.003975
  l1.weight: grad_norm = 0.151670
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.133600
Total gradient norm: 0.453823
=== Actor Training Debug (Iteration 3821) ===
Q mean: -11.730157
Q std: 13.760369
Actor loss: 11.734136
Action reg: 0.003979
  l1.weight: grad_norm = 0.058748
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.052198
Total gradient norm: 0.167194
=== Actor Training Debug (Iteration 3822) ===
Q mean: -9.644406
Q std: 13.923909
Actor loss: 9.648383
Action reg: 0.003977
  l1.weight: grad_norm = 0.091798
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.081874
Total gradient norm: 0.308105
=== Actor Training Debug (Iteration 3823) ===
Q mean: -10.262957
Q std: 13.105370
Actor loss: 10.266932
Action reg: 0.003975
  l1.weight: grad_norm = 0.041250
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.049037
Total gradient norm: 0.183118
=== Actor Training Debug (Iteration 3824) ===
Q mean: -9.656882
Q std: 11.153964
Actor loss: 9.660850
Action reg: 0.003968
  l1.weight: grad_norm = 0.212940
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.207094
Total gradient norm: 0.637931
=== Actor Training Debug (Iteration 3825) ===
Q mean: -10.794490
Q std: 13.380146
Actor loss: 10.798474
Action reg: 0.003985
  l1.weight: grad_norm = 0.068192
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.051298
Total gradient norm: 0.156554
=== Actor Training Debug (Iteration 3841) ===
Q mean: -11.586388
Q std: 13.258493
Actor loss: 11.590352
Action reg: 0.003964
  l1.weight: grad_norm = 0.117310
  l1.bias: grad_norm = 0.001194
  l2.weight: grad_norm = 0.116625
Total gradient norm: 0.355901
=== Actor Training Debug (Iteration 3842) ===
Q mean: -9.264891
Q std: 11.907109
Actor loss: 9.268864
Action reg: 0.003973
  l1.weight: grad_norm = 0.073874
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.060696
Total gradient norm: 0.197046
=== Actor Training Debug (Iteration 3843) ===
Q mean: -8.343056
Q std: 12.053449
Actor loss: 8.347039
Action reg: 0.003984
  l1.weight: grad_norm = 0.064435
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.054711
Total gradient norm: 0.187971
=== Actor Training Debug (Iteration 3844) ===
Q mean: -11.135992
Q std: 12.576309
Actor loss: 11.139971
Action reg: 0.003979
  l1.weight: grad_norm = 0.129037
  l1.bias: grad_norm = 0.000814
  l2.weight: grad_norm = 0.107319
Total gradient norm: 0.324995
=== Actor Training Debug (Iteration 3845) ===
Q mean: -12.212471
Q std: 13.578081
Actor loss: 12.216449
Action reg: 0.003978
  l1.weight: grad_norm = 0.090111
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.082391
Total gradient norm: 0.259851
=== Actor Training Debug (Iteration 3846) ===
Q mean: -9.647774
Q std: 12.194106
Actor loss: 9.651752
Action reg: 0.003978
  l1.weight: grad_norm = 0.098556
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.079372
Total gradient norm: 0.263189
=== Actor Training Debug (Iteration 3847) ===
Q mean: -10.909393
Q std: 12.854264
Actor loss: 10.913383
Action reg: 0.003990
  l1.weight: grad_norm = 0.054981
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.048924
Total gradient norm: 0.143878
=== Actor Training Debug (Iteration 3848) ===
Q mean: -11.105015
Q std: 14.525551
Actor loss: 11.108986
Action reg: 0.003971
  l1.weight: grad_norm = 0.034738
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.030762
Total gradient norm: 0.114410
=== Actor Training Debug (Iteration 3849) ===
Q mean: -10.297211
Q std: 13.429116
Actor loss: 10.301179
Action reg: 0.003969
  l1.weight: grad_norm = 0.062820
  l1.bias: grad_norm = 0.002146
  l2.weight: grad_norm = 0.060626
Total gradient norm: 0.192789
=== Actor Training Debug (Iteration 3850) ===
Q mean: -10.638313
Q std: 12.289255
Actor loss: 10.642302
Action reg: 0.003989
  l1.weight: grad_norm = 0.063019
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.054401
Total gradient norm: 0.172718
=== Actor Training Debug (Iteration 3851) ===
Q mean: -9.076664
Q std: 11.821377
Actor loss: 9.080638
Action reg: 0.003974
  l1.weight: grad_norm = 0.037701
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.033214
Total gradient norm: 0.113824
=== Actor Training Debug (Iteration 3852) ===
Q mean: -9.893504
Q std: 13.448499
Actor loss: 9.897453
Action reg: 0.003949
  l1.weight: grad_norm = 0.174593
  l1.bias: grad_norm = 0.000529
  l2.weight: grad_norm = 0.148388
Total gradient norm: 0.447068
=== Actor Training Debug (Iteration 3853) ===
Q mean: -11.676383
Q std: 14.338705
Actor loss: 11.680357
Action reg: 0.003974
  l1.weight: grad_norm = 0.090809
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.095449
Total gradient norm: 0.309490
=== Actor Training Debug (Iteration 3854) ===
Q mean: -10.108064
Q std: 11.817421
Actor loss: 10.112043
Action reg: 0.003980
  l1.weight: grad_norm = 0.068831
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.057570
Total gradient norm: 0.180081
=== Actor Training Debug (Iteration 3855) ===
Q mean: -10.396906
Q std: 13.816458
Actor loss: 10.400882
Action reg: 0.003976
  l1.weight: grad_norm = 0.047082
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.041816
Total gradient norm: 0.161937
=== Actor Training Debug (Iteration 3856) ===
Q mean: -10.044376
Q std: 13.348362
Actor loss: 10.048335
Action reg: 0.003959
  l1.weight: grad_norm = 0.167333
  l1.bias: grad_norm = 0.000454
  l2.weight: grad_norm = 0.116600
Total gradient norm: 0.327418
=== Actor Training Debug (Iteration 3857) ===
Q mean: -10.194913
Q std: 13.086310
Actor loss: 10.198886
Action reg: 0.003973
  l1.weight: grad_norm = 0.068386
  l1.bias: grad_norm = 0.000618
  l2.weight: grad_norm = 0.062572
Total gradient norm: 0.212565
=== Actor Training Debug (Iteration 3858) ===
Q mean: -10.489014
Q std: 13.229372
Actor loss: 10.492993
Action reg: 0.003980
  l1.weight: grad_norm = 0.116336
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.092033
Total gradient norm: 0.273782
=== Actor Training Debug (Iteration 3859) ===
Q mean: -10.087148
Q std: 12.758216
Actor loss: 10.091124
Action reg: 0.003976
  l1.weight: grad_norm = 0.101466
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.101496
Total gradient norm: 0.367471
=== Actor Training Debug (Iteration 3860) ===
Q mean: -9.919491
Q std: 12.294604
Actor loss: 9.923463
Action reg: 0.003972
  l1.weight: grad_norm = 0.079390
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.067729
Total gradient norm: 0.225810
=== Actor Training Debug (Iteration 3861) ===
Q mean: -9.773931
Q std: 12.272918
Actor loss: 9.777920
Action reg: 0.003990
  l1.weight: grad_norm = 0.029158
  l1.bias: grad_norm = 0.000008
  l2.weight: grad_norm = 0.025721
Total gradient norm: 0.084455
=== Actor Training Debug (Iteration 3862) ===
Q mean: -11.661964
Q std: 13.106884
Actor loss: 11.665926
Action reg: 0.003962
  l1.weight: grad_norm = 0.063552
  l1.bias: grad_norm = 0.001478
  l2.weight: grad_norm = 0.055377
Total gradient norm: 0.170934
=== Actor Training Debug (Iteration 3863) ===
Q mean: -10.612444
Q std: 13.832520
Actor loss: 10.616418
Action reg: 0.003974
  l1.weight: grad_norm = 0.100551
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.077312
Total gradient norm: 0.236396
=== Actor Training Debug (Iteration 3864) ===
Q mean: -9.988222
Q std: 12.542449
Actor loss: 9.992198
Action reg: 0.003976
  l1.weight: grad_norm = 0.083717
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.073117
Total gradient norm: 0.281884
=== Actor Training Debug (Iteration 3865) ===
Q mean: -10.514032
Q std: 12.707056
Actor loss: 10.518011
Action reg: 0.003979
  l1.weight: grad_norm = 0.064170
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.064762
Total gradient norm: 0.217647
=== Actor Training Debug (Iteration 3866) ===
Q mean: -9.395353
Q std: 12.892693
Actor loss: 9.399323
Action reg: 0.003970
  l1.weight: grad_norm = 0.069116
  l1.bias: grad_norm = 0.000444
  l2.weight: grad_norm = 0.064892
Total gradient norm: 0.209139
=== Actor Training Debug (Iteration 3867) ===
Q mean: -10.442847
Q std: 13.662252
Actor loss: 10.446820
Action reg: 0.003973
  l1.weight: grad_norm = 0.084526
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.074553
Total gradient norm: 0.240321
=== Actor Training Debug (Iteration 3868) ===
Q mean: -10.229809
Q std: 12.112310
Actor loss: 10.233779
Action reg: 0.003970
  l1.weight: grad_norm = 0.098495
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.075845
Total gradient norm: 0.197707
=== Actor Training Debug (Iteration 3869) ===
Q mean: -9.715309
Q std: 12.349461
Actor loss: 9.719277
Action reg: 0.003968
  l1.weight: grad_norm = 0.192500
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.152159
Total gradient norm: 0.522283
=== Actor Training Debug (Iteration 3870) ===
Q mean: -8.871986
Q std: 10.873902
Actor loss: 8.875969
Action reg: 0.003982
  l1.weight: grad_norm = 0.051485
  l1.bias: grad_norm = 0.000601
  l2.weight: grad_norm = 0.046779
Total gradient norm: 0.140377
=== Actor Training Debug (Iteration 3871) ===
Q mean: -10.437645
Q std: 13.248945
Actor loss: 10.441612
Action reg: 0.003968
  l1.weight: grad_norm = 0.085068
  l1.bias: grad_norm = 0.000577
  l2.weight: grad_norm = 0.073270
Total gradient norm: 0.243772
=== Actor Training Debug (Iteration 3872) ===
Q mean: -10.722016
Q std: 13.706256
Actor loss: 10.725978
Action reg: 0.003962
  l1.weight: grad_norm = 0.073062
  l1.bias: grad_norm = 0.000783
  l2.weight: grad_norm = 0.062416
Total gradient norm: 0.198197
=== Actor Training Debug (Iteration 3873) ===
Q mean: -8.665568
Q std: 10.987629
Actor loss: 8.669511
Action reg: 0.003942
  l1.weight: grad_norm = 0.238853
  l1.bias: grad_norm = 0.000452
  l2.weight: grad_norm = 0.186490
Total gradient norm: 0.577404
=== Actor Training Debug (Iteration 3874) ===
Q mean: -9.616236
Q std: 12.674846
Actor loss: 9.620212
Action reg: 0.003976
  l1.weight: grad_norm = 0.107065
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.085559
Total gradient norm: 0.293875
=== Actor Training Debug (Iteration 3875) ===
Q mean: -9.841835
Q std: 13.033550
Actor loss: 9.845812
Action reg: 0.003977
  l1.weight: grad_norm = 0.090881
  l1.bias: grad_norm = 0.000835
  l2.weight: grad_norm = 0.074665
Total gradient norm: 0.202447
=== Actor Training Debug (Iteration 3876) ===
Q mean: -10.355106
Q std: 13.776381
Actor loss: 10.359087
Action reg: 0.003980
  l1.weight: grad_norm = 0.047756
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.040442
Total gradient norm: 0.137917
=== Actor Training Debug (Iteration 3877) ===
Q mean: -10.351095
Q std: 12.766185
Actor loss: 10.355068
Action reg: 0.003973
  l1.weight: grad_norm = 0.118147
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.106353
Total gradient norm: 0.306070
=== Actor Training Debug (Iteration 3878) ===
Q mean: -11.430565
Q std: 13.804363
Actor loss: 11.434538
Action reg: 0.003973
  l1.weight: grad_norm = 0.155618
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.145849
Total gradient norm: 0.453749
=== Actor Training Debug (Iteration 3879) ===
Q mean: -12.059226
Q std: 13.214248
Actor loss: 12.063191
Action reg: 0.003966
  l1.weight: grad_norm = 0.065730
  l1.bias: grad_norm = 0.000742
  l2.weight: grad_norm = 0.059056
Total gradient norm: 0.224173
=== Actor Training Debug (Iteration 3880) ===
Q mean: -9.319401
Q std: 11.863393
Actor loss: 9.323368
Action reg: 0.003968
  l1.weight: grad_norm = 0.096360
  l1.bias: grad_norm = 0.000686
  l2.weight: grad_norm = 0.080550
Total gradient norm: 0.225643
=== Actor Training Debug (Iteration 3881) ===
Q mean: -10.928747
Q std: 12.863019
Actor loss: 10.932729
Action reg: 0.003982
  l1.weight: grad_norm = 0.040924
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.030261
Total gradient norm: 0.080960
=== Actor Training Debug (Iteration 3882) ===
Q mean: -10.221232
Q std: 12.719400
Actor loss: 10.225211
Action reg: 0.003979
  l1.weight: grad_norm = 0.054313
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.048482
Total gradient norm: 0.149635
=== Actor Training Debug (Iteration 3883) ===
Q mean: -9.041260
Q std: 11.752563
Actor loss: 9.045218
Action reg: 0.003958
  l1.weight: grad_norm = 0.063916
  l1.bias: grad_norm = 0.001396
  l2.weight: grad_norm = 0.053653
Total gradient norm: 0.177274
=== Actor Training Debug (Iteration 3884) ===
Q mean: -11.861446
Q std: 14.124267
Actor loss: 11.865428
Action reg: 0.003982
  l1.weight: grad_norm = 0.155312
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.125977
Total gradient norm: 0.495023
=== Actor Training Debug (Iteration 3885) ===
Q mean: -11.461285
Q std: 12.847208
Actor loss: 11.465251
Action reg: 0.003967
  l1.weight: grad_norm = 0.209420
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.151593
Total gradient norm: 0.455647
=== Actor Training Debug (Iteration 3886) ===
Q mean: -10.145553
Q std: 13.335032
Actor loss: 10.149520
Action reg: 0.003968
  l1.weight: grad_norm = 0.296945
  l1.bias: grad_norm = 0.000940
  l2.weight: grad_norm = 0.249812
Total gradient norm: 0.872630
=== Actor Training Debug (Iteration 3887) ===
Q mean: -10.334938
Q std: 12.767182
Actor loss: 10.338914
Action reg: 0.003976
  l1.weight: grad_norm = 0.034804
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.033049
Total gradient norm: 0.127587
=== Actor Training Debug (Iteration 3888) ===
Q mean: -10.427141
Q std: 13.173223
Actor loss: 10.431124
Action reg: 0.003983
  l1.weight: grad_norm = 0.069473
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.069296
Total gradient norm: 0.252334
=== Actor Training Debug (Iteration 3889) ===
Q mean: -10.402990
Q std: 12.355455
Actor loss: 10.406949
Action reg: 0.003959
  l1.weight: grad_norm = 0.122839
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.106929
Total gradient norm: 0.375323
=== Actor Training Debug (Iteration 3890) ===
Q mean: -9.284023
Q std: 12.788675
Actor loss: 9.287987
Action reg: 0.003963
  l1.weight: grad_norm = 0.111502
  l1.bias: grad_norm = 0.000908
  l2.weight: grad_norm = 0.078964
Total gradient norm: 0.234525
=== Actor Training Debug (Iteration 3891) ===
Q mean: -9.693098
Q std: 12.635226
Actor loss: 9.697076
Action reg: 0.003978
  l1.weight: grad_norm = 0.105219
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.097220
Total gradient norm: 0.284704
=== Actor Training Debug (Iteration 3892) ===
Q mean: -10.786989
Q std: 13.887210
Actor loss: 10.790962
Action reg: 0.003973
  l1.weight: grad_norm = 0.112199
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.096331
Total gradient norm: 0.329262
=== Actor Training Debug (Iteration 3893) ===
Q mean: -9.901509
Q std: 12.443359
Actor loss: 9.905488
Action reg: 0.003978
  l1.weight: grad_norm = 0.308613
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.188781
Total gradient norm: 0.478482
=== Actor Training Debug (Iteration 3894) ===
Q mean: -9.772160
Q std: 12.587812
Actor loss: 9.776142
Action reg: 0.003983
  l1.weight: grad_norm = 0.065347
  l1.bias: grad_norm = 0.000026
  l2.weight: grad_norm = 0.058352
Total gradient norm: 0.189998
=== Actor Training Debug (Iteration 3895) ===
Q mean: -10.365705
Q std: 13.234417
Actor loss: 10.369679
Action reg: 0.003974
  l1.weight: grad_norm = 0.159341
  l1.bias: grad_norm = 0.000977
  l2.weight: grad_norm = 0.132805
Total gradient norm: 0.434050
=== Actor Training Debug (Iteration 3896) ===
Q mean: -10.445239
Q std: 12.601418
Actor loss: 10.449216
Action reg: 0.003977
  l1.weight: grad_norm = 0.132101
  l1.bias: grad_norm = 0.000659
  l2.weight: grad_norm = 0.119362
Total gradient norm: 0.326437
=== Actor Training Debug (Iteration 3897) ===
Q mean: -9.802286
Q std: 13.598571
Actor loss: 9.806270
Action reg: 0.003983
  l1.weight: grad_norm = 0.034821
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.029996
Total gradient norm: 0.100962
=== Actor Training Debug (Iteration 3898) ===
Q mean: -10.517821
Q std: 13.492700
Actor loss: 10.521802
Action reg: 0.003980
  l1.weight: grad_norm = 0.083135
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.068207
Total gradient norm: 0.241988
=== Actor Training Debug (Iteration 3899) ===
Q mean: -9.910982
Q std: 12.350334
Actor loss: 9.914960
Action reg: 0.003978
  l1.weight: grad_norm = 0.048970
  l1.bias: grad_norm = 0.000596
  l2.weight: grad_norm = 0.046095
Total gradient norm: 0.136487
=== Actor Training Debug (Iteration 3900) ===
Q mean: -8.691986
Q std: 11.867309
Actor loss: 8.695956
Action reg: 0.003970
  l1.weight: grad_norm = 0.154139
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.112421
Total gradient norm: 0.364689
=== Actor Training Debug (Iteration 3901) ===
Q mean: -10.817657
Q std: 13.714661
Actor loss: 10.821631
Action reg: 0.003974
  l1.weight: grad_norm = 0.200147
  l1.bias: grad_norm = 0.000709
  l2.weight: grad_norm = 0.148072
Total gradient norm: 0.429995
=== Actor Training Debug (Iteration 3902) ===
Q mean: -10.058105
Q std: 12.985626
Actor loss: 10.062093
Action reg: 0.003987
  l1.weight: grad_norm = 0.079033
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.085198
Total gradient norm: 0.279868
=== Actor Training Debug (Iteration 3903) ===
Q mean: -10.171005
Q std: 13.403111
Actor loss: 10.174992
Action reg: 0.003986
  l1.weight: grad_norm = 0.040012
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.035509
Total gradient norm: 0.123051
=== Actor Training Debug (Iteration 3904) ===
Q mean: -10.057775
Q std: 12.737386
Actor loss: 10.061752
Action reg: 0.003978
  l1.weight: grad_norm = 0.100176
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.086127
Total gradient norm: 0.315930
=== Actor Training Debug (Iteration 3905) ===
Q mean: -9.680434
Q std: 12.041772
Actor loss: 9.684408
Action reg: 0.003974
  l1.weight: grad_norm = 0.049929
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.042553
Total gradient norm: 0.151045
=== Actor Training Debug (Iteration 3906) ===
Q mean: -10.374004
Q std: 12.585553
Actor loss: 10.377983
Action reg: 0.003978
  l1.weight: grad_norm = 0.073069
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.068242
Total gradient norm: 0.220555
=== Actor Training Debug (Iteration 3907) ===
Q mean: -10.152471
Q std: 12.492973
Actor loss: 10.156463
Action reg: 0.003992
  l1.weight: grad_norm = 0.063068
  l1.bias: grad_norm = 0.000030
  l2.weight: grad_norm = 0.049950
Total gradient norm: 0.145008
=== Actor Training Debug (Iteration 3908) ===
Q mean: -9.948466
Q std: 12.605794
Actor loss: 9.952430
Action reg: 0.003963
  l1.weight: grad_norm = 0.107534
  l1.bias: grad_norm = 0.000719
  l2.weight: grad_norm = 0.104086
Total gradient norm: 0.400450
=== Actor Training Debug (Iteration 3909) ===
Q mean: -9.777734
Q std: 12.772359
Actor loss: 9.781714
Action reg: 0.003980
  l1.weight: grad_norm = 0.082726
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.072927
Total gradient norm: 0.226854
=== Actor Training Debug (Iteration 3910) ===
Q mean: -12.123564
Q std: 14.117233
Actor loss: 12.127542
Action reg: 0.003979
  l1.weight: grad_norm = 0.263712
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.202357
Total gradient norm: 0.551931
=== Actor Training Debug (Iteration 3911) ===
Q mean: -10.671535
Q std: 13.452914
Actor loss: 10.675507
Action reg: 0.003971
  l1.weight: grad_norm = 0.138986
  l1.bias: grad_norm = 0.000689
  l2.weight: grad_norm = 0.118419
Total gradient norm: 0.365174
=== Actor Training Debug (Iteration 3912) ===
Q mean: -11.116474
Q std: 13.080663
Actor loss: 11.120455
Action reg: 0.003981
  l1.weight: grad_norm = 0.046632
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.040803
Total gradient norm: 0.148570
=== Actor Training Debug (Iteration 3913) ===
Q mean: -8.649998
Q std: 12.049665
Actor loss: 8.653965
Action reg: 0.003968
  l1.weight: grad_norm = 0.151186
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.132011
Total gradient norm: 0.392383
=== Actor Training Debug (Iteration 3914) ===
Q mean: -8.534264
Q std: 11.465646
Actor loss: 8.538237
Action reg: 0.003973
  l1.weight: grad_norm = 0.121875
  l1.bias: grad_norm = 0.001705
  l2.weight: grad_norm = 0.107502
Total gradient norm: 0.361005
=== Actor Training Debug (Iteration 3915) ===
Q mean: -11.024628
Q std: 13.637619
Actor loss: 11.028607
Action reg: 0.003980
  l1.weight: grad_norm = 0.058505
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.053859
Total gradient norm: 0.174881
=== Actor Training Debug (Iteration 3916) ===
Q mean: -9.502190
Q std: 11.997256
Actor loss: 9.506150
Action reg: 0.003961
  l1.weight: grad_norm = 0.106801
  l1.bias: grad_norm = 0.000634
  l2.weight: grad_norm = 0.105055
Total gradient norm: 0.317149
=== Actor Training Debug (Iteration 3917) ===
Q mean: -11.112730
Q std: 14.869665
Actor loss: 11.116695
Action reg: 0.003965
  l1.weight: grad_norm = 0.044487
  l1.bias: grad_norm = 0.000510
  l2.weight: grad_norm = 0.036261
Total gradient norm: 0.132984
=== Actor Training Debug (Iteration 3918) ===
Q mean: -10.386366
Q std: 13.330238
Actor loss: 10.390341
Action reg: 0.003975
  l1.weight: grad_norm = 0.062804
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.053994
Total gradient norm: 0.175488
=== Actor Training Debug (Iteration 3919) ===
Q mean: -10.955551
Q std: 13.600492
Actor loss: 10.959520
Action reg: 0.003969
  l1.weight: grad_norm = 0.069294
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.065678
Total gradient norm: 0.214121
=== Actor Training Debug (Iteration 3920) ===
Q mean: -9.987873
Q std: 12.302637
Actor loss: 9.991842
Action reg: 0.003969
  l1.weight: grad_norm = 0.046764
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.039724
Total gradient norm: 0.150252
=== Actor Training Debug (Iteration 3921) ===
Q mean: -10.827732
Q std: 12.387515
Actor loss: 10.831683
Action reg: 0.003951
  l1.weight: grad_norm = 0.154097
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.127995
Total gradient norm: 0.391603
=== Actor Training Debug (Iteration 3922) ===
Q mean: -10.378637
Q std: 13.411525
Actor loss: 10.382614
Action reg: 0.003977
  l1.weight: grad_norm = 0.062277
  l1.bias: grad_norm = 0.000575
  l2.weight: grad_norm = 0.058248
Total gradient norm: 0.191770
=== Actor Training Debug (Iteration 3923) ===
Q mean: -10.460736
Q std: 13.504319
Actor loss: 10.464705
Action reg: 0.003969
  l1.weight: grad_norm = 0.100549
  l1.bias: grad_norm = 0.000550
  l2.weight: grad_norm = 0.099892
Total gradient norm: 0.301378
=== Actor Training Debug (Iteration 3924) ===
Q mean: -9.372057
Q std: 12.824647
Actor loss: 9.376034
Action reg: 0.003977
  l1.weight: grad_norm = 0.083236
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.076196
Total gradient norm: 0.246919
=== Actor Training Debug (Iteration 3925) ===
Q mean: -9.856231
Q std: 12.817378
Actor loss: 9.860212
Action reg: 0.003981
  l1.weight: grad_norm = 0.057015
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.056244
Total gradient norm: 0.223123
=== Actor Training Debug (Iteration 3926) ===
Q mean: -9.679495
Q std: 12.040674
Actor loss: 9.683471
Action reg: 0.003976
  l1.weight: grad_norm = 0.117757
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.120600
Total gradient norm: 0.381415
=== Actor Training Debug (Iteration 3927) ===
Q mean: -9.992105
Q std: 13.211713
Actor loss: 9.996072
Action reg: 0.003966
  l1.weight: grad_norm = 0.101475
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.100701
Total gradient norm: 0.310818
=== Actor Training Debug (Iteration 3928) ===
Q mean: -9.869044
Q std: 12.938875
Actor loss: 9.873017
Action reg: 0.003973
  l1.weight: grad_norm = 0.038408
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.029349
Total gradient norm: 0.091643
=== Actor Training Debug (Iteration 3929) ===
Q mean: -11.286065
Q std: 13.611446
Actor loss: 11.290051
Action reg: 0.003986
  l1.weight: grad_norm = 0.064814
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.055926
Total gradient norm: 0.187755
=== Actor Training Debug (Iteration 3930) ===
Q mean: -12.202082
Q std: 14.438753
Actor loss: 12.206060
Action reg: 0.003979
  l1.weight: grad_norm = 0.088556
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.071340
Total gradient norm: 0.235796
=== Actor Training Debug (Iteration 3931) ===
Q mean: -10.401860
Q std: 12.542029
Actor loss: 10.405814
Action reg: 0.003954
  l1.weight: grad_norm = 0.126387
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.118207
Total gradient norm: 0.421250
=== Actor Training Debug (Iteration 3932) ===
Q mean: -11.333255
Q std: 13.148306
Actor loss: 11.337235
Action reg: 0.003981
  l1.weight: grad_norm = 0.147750
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.158147
Total gradient norm: 0.555730
=== Actor Training Debug (Iteration 3933) ===
Q mean: -10.485392
Q std: 13.413076
Actor loss: 10.489374
Action reg: 0.003982
  l1.weight: grad_norm = 0.063082
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.061132
Total gradient norm: 0.192943
=== Actor Training Debug (Iteration 3934) ===
Q mean: -10.878103
Q std: 14.933238
Actor loss: 10.882080
Action reg: 0.003977
  l1.weight: grad_norm = 0.103623
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.097921
Total gradient norm: 0.317994
=== Actor Training Debug (Iteration 3935) ===
Q mean: -11.568200
Q std: 14.670905
Actor loss: 11.572184
Action reg: 0.003983
  l1.weight: grad_norm = 0.058201
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.049386
Total gradient norm: 0.161368
=== Actor Training Debug (Iteration 3936) ===
Q mean: -9.446466
Q std: 13.245950
Actor loss: 9.450446
Action reg: 0.003980
  l1.weight: grad_norm = 0.085299
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.079282
Total gradient norm: 0.268184
=== Actor Training Debug (Iteration 3937) ===
Q mean: -10.310312
Q std: 13.560139
Actor loss: 10.314293
Action reg: 0.003980
  l1.weight: grad_norm = 0.057858
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.053288
Total gradient norm: 0.240732
=== Actor Training Debug (Iteration 3938) ===
Q mean: -10.864830
Q std: 13.663594
Actor loss: 10.868809
Action reg: 0.003978
  l1.weight: grad_norm = 0.096840
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.097619
Total gradient norm: 0.288451
=== Actor Training Debug (Iteration 3939) ===
Q mean: -10.456997
Q std: 12.472914
Actor loss: 10.460971
Action reg: 0.003974
  l1.weight: grad_norm = 0.105139
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.089644
Total gradient norm: 0.307239
=== Actor Training Debug (Iteration 3940) ===
Q mean: -8.656816
Q std: 11.895308
Actor loss: 8.660793
Action reg: 0.003977
  l1.weight: grad_norm = 0.034586
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.031297
Total gradient norm: 0.104833
=== Actor Training Debug (Iteration 3941) ===
Q mean: -11.204956
Q std: 14.118712
Actor loss: 11.208931
Action reg: 0.003975
  l1.weight: grad_norm = 0.053644
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.047785
Total gradient norm: 0.147515
=== Actor Training Debug (Iteration 3942) ===
Q mean: -8.698769
Q std: 12.598048
Actor loss: 8.702739
Action reg: 0.003970
  l1.weight: grad_norm = 0.123542
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.094495
Total gradient norm: 0.267170
=== Actor Training Debug (Iteration 3943) ===
Q mean: -10.840879
Q std: 13.568034
Actor loss: 10.844857
Action reg: 0.003978
  l1.weight: grad_norm = 0.099405
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.075718
Total gradient norm: 0.208954
=== Actor Training Debug (Iteration 3944) ===
Q mean: -10.784339
Q std: 13.210686
Actor loss: 10.788311
Action reg: 0.003972
  l1.weight: grad_norm = 0.055282
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.055456
Total gradient norm: 0.203852
=== Actor Training Debug (Iteration 3945) ===
Q mean: -10.032332
Q std: 13.315042
Actor loss: 10.036303
Action reg: 0.003970
  l1.weight: grad_norm = 0.071055
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.074949
Total gradient norm: 0.291450
=== Actor Training Debug (Iteration 3946) ===
Q mean: -10.587551
Q std: 14.083907
Actor loss: 10.591526
Action reg: 0.003975
  l1.weight: grad_norm = 0.150768
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.161580
Total gradient norm: 0.529340
=== Actor Training Debug (Iteration 3947) ===
Q mean: -10.102894
Q std: 12.713777
Actor loss: 10.106855
Action reg: 0.003961
  l1.weight: grad_norm = 0.088031
  l1.bias: grad_norm = 0.000760
  l2.weight: grad_norm = 0.082826
Total gradient norm: 0.285747
=== Actor Training Debug (Iteration 3948) ===
Q mean: -11.955029
Q std: 13.995643
Actor loss: 11.959011
Action reg: 0.003982
  l1.weight: grad_norm = 0.175031
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.134776
Total gradient norm: 0.411942
=== Actor Training Debug (Iteration 3949) ===
Q mean: -10.516335
Q std: 12.575686
Actor loss: 10.520317
Action reg: 0.003981
  l1.weight: grad_norm = 0.067005
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.054736
Total gradient norm: 0.191908
=== Actor Training Debug (Iteration 3950) ===
Q mean: -9.832359
Q std: 12.324182
Actor loss: 9.836342
Action reg: 0.003983
  l1.weight: grad_norm = 0.092238
  l1.bias: grad_norm = 0.000032
  l2.weight: grad_norm = 0.076908
Total gradient norm: 0.224219
=== Actor Training Debug (Iteration 3951) ===
Q mean: -10.634048
Q std: 14.536656
Actor loss: 10.638016
Action reg: 0.003967
  l1.weight: grad_norm = 0.103040
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.089478
Total gradient norm: 0.282722
=== Actor Training Debug (Iteration 3952) ===
Q mean: -11.210198
Q std: 13.550200
Actor loss: 11.214175
Action reg: 0.003977
  l1.weight: grad_norm = 0.066012
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.058443
Total gradient norm: 0.197075
=== Actor Training Debug (Iteration 3953) ===
Q mean: -9.969429
Q std: 13.553368
Actor loss: 9.973400
Action reg: 0.003971
  l1.weight: grad_norm = 0.074289
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.067816
Total gradient norm: 0.240064
=== Actor Training Debug (Iteration 3954) ===
Q mean: -10.495214
Q std: 14.043919
Actor loss: 10.499179
Action reg: 0.003966
  l1.weight: grad_norm = 0.122089
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.094760
Total gradient norm: 0.307587
=== Actor Training Debug (Iteration 3955) ===
Q mean: -11.700859
Q std: 14.769768
Actor loss: 11.704831
Action reg: 0.003972
  l1.weight: grad_norm = 0.101719
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.095546
Total gradient norm: 0.261893
=== Actor Training Debug (Iteration 3956) ===
Q mean: -11.459820
Q std: 12.960580
Actor loss: 11.463796
Action reg: 0.003976
  l1.weight: grad_norm = 0.113685
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.104735
Total gradient norm: 0.368122
=== Actor Training Debug (Iteration 3957) ===
Q mean: -10.144291
Q std: 13.493942
Actor loss: 10.148270
Action reg: 0.003979
  l1.weight: grad_norm = 0.143023
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.105390
Total gradient norm: 0.303491
=== Actor Training Debug (Iteration 3958) ===
Q mean: -8.477079
Q std: 12.313539
Actor loss: 8.481058
Action reg: 0.003978
  l1.weight: grad_norm = 0.061582
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.050743
Total gradient norm: 0.198010
=== Actor Training Debug (Iteration 3959) ===
Q mean: -9.434991
Q std: 13.410374
Actor loss: 9.438951
Action reg: 0.003960
  l1.weight: grad_norm = 0.177947
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.127938
Total gradient norm: 0.449046
=== Actor Training Debug (Iteration 3960) ===
Q mean: -12.572130
Q std: 14.779584
Actor loss: 12.576112
Action reg: 0.003982
  l1.weight: grad_norm = 0.054638
  l1.bias: grad_norm = 0.000582
  l2.weight: grad_norm = 0.047308
Total gradient norm: 0.157321
=== Actor Training Debug (Iteration 3961) ===
Q mean: -10.400426
Q std: 12.569040
Actor loss: 10.404399
Action reg: 0.003973
  l1.weight: grad_norm = 0.050365
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.043546
Total gradient norm: 0.161308
=== Actor Training Debug (Iteration 3962) ===
Q mean: -10.539412
Q std: 13.677466
Actor loss: 10.543386
Action reg: 0.003975
  l1.weight: grad_norm = 0.058229
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.056086
Total gradient norm: 0.224435
=== Actor Training Debug (Iteration 3963) ===
Q mean: -10.428501
Q std: 13.379174
Actor loss: 10.432482
Action reg: 0.003981
  l1.weight: grad_norm = 0.040631
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.040020
Total gradient norm: 0.152785
=== Actor Training Debug (Iteration 3964) ===
Q mean: -9.957518
Q std: 13.201502
Actor loss: 9.961497
Action reg: 0.003979
  l1.weight: grad_norm = 0.180196
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.136331
Total gradient norm: 0.393908
=== Actor Training Debug (Iteration 3965) ===
Q mean: -10.538436
Q std: 13.310989
Actor loss: 10.542410
Action reg: 0.003974
  l1.weight: grad_norm = 0.106514
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.091961
Total gradient norm: 0.286802
=== Actor Training Debug (Iteration 3966) ===
Q mean: -10.411102
Q std: 13.042387
Actor loss: 10.415075
Action reg: 0.003973
  l1.weight: grad_norm = 0.101914
  l1.bias: grad_norm = 0.000851
  l2.weight: grad_norm = 0.091119
Total gradient norm: 0.272759
=== Actor Training Debug (Iteration 3967) ===
Q mean: -11.868250
Q std: 12.724997
Actor loss: 11.872225
Action reg: 0.003975
  l1.weight: grad_norm = 0.144002
  l1.bias: grad_norm = 0.000917
  l2.weight: grad_norm = 0.121120
Total gradient norm: 0.421811
=== Actor Training Debug (Iteration 3968) ===
Q mean: -10.932804
Q std: 13.269602
Actor loss: 10.936782
Action reg: 0.003978
  l1.weight: grad_norm = 0.068701
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.056774
Total gradient norm: 0.174063
=== Actor Training Debug (Iteration 3969) ===
Q mean: -9.874460
Q std: 12.741101
Actor loss: 9.878426
Action reg: 0.003965
  l1.weight: grad_norm = 0.075635
  l1.bias: grad_norm = 0.000752
  l2.weight: grad_norm = 0.059069
Total gradient norm: 0.183790
=== Actor Training Debug (Iteration 3970) ===
Q mean: -9.480068
Q std: 11.781582
Actor loss: 9.484040
Action reg: 0.003972
  l1.weight: grad_norm = 0.156056
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.121461
Total gradient norm: 0.400052
=== Actor Training Debug (Iteration 3971) ===
Q mean: -10.237641
Q std: 12.766125
Actor loss: 10.241613
Action reg: 0.003972
  l1.weight: grad_norm = 0.092729
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.081614
Total gradient norm: 0.273729
=== Actor Training Debug (Iteration 3972) ===
Q mean: -11.357855
Q std: 13.899719
Actor loss: 11.361824
Action reg: 0.003969
  l1.weight: grad_norm = 0.073000
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.058484
Total gradient norm: 0.150454
=== Actor Training Debug (Iteration 3973) ===
Q mean: -10.618470
Q std: 12.839687
Actor loss: 10.622452
Action reg: 0.003981
  l1.weight: grad_norm = 0.091411
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.087535
Total gradient norm: 0.264282
=== Actor Training Debug (Iteration 3974) ===
Q mean: -10.077927
Q std: 12.211720
Actor loss: 10.081904
Action reg: 0.003978
  l1.weight: grad_norm = 0.078549
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.050675
Total gradient norm: 0.148896
=== Actor Training Debug (Iteration 3975) ===
Q mean: -11.984985
Q std: 14.187385
Actor loss: 11.988961
Action reg: 0.003976
  l1.weight: grad_norm = 0.068576
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.060201
Total gradient norm: 0.203493
=== Actor Training Debug (Iteration 3976) ===
Q mean: -10.296377
Q std: 12.843740
Actor loss: 10.300346
Action reg: 0.003969
  l1.weight: grad_norm = 0.075116
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.071864
Total gradient norm: 0.249114
=== Actor Training Debug (Iteration 3977) ===
Q mean: -9.277780
Q std: 13.309553
Actor loss: 9.281750
Action reg: 0.003970
  l1.weight: grad_norm = 0.118405
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.086770
Total gradient norm: 0.275577
=== Actor Training Debug (Iteration 3978) ===
Q mean: -11.034517
Q std: 13.289726
Actor loss: 11.038500
Action reg: 0.003982
  l1.weight: grad_norm = 0.051969
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.045757
Total gradient norm: 0.136824
=== Actor Training Debug (Iteration 3979) ===
Q mean: -10.005007
Q std: 12.541757
Actor loss: 10.008976
Action reg: 0.003970
  l1.weight: grad_norm = 0.072326
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.071857
Total gradient norm: 0.294680
=== Actor Training Debug (Iteration 3980) ===
Q mean: -10.293741
Q std: 12.425587
Actor loss: 10.297719
Action reg: 0.003977
  l1.weight: grad_norm = 0.058584
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.045247
Total gradient norm: 0.155903
=== Actor Training Debug (Iteration 3981) ===
Q mean: -7.794228
Q std: 11.869522
Actor loss: 7.798205
Action reg: 0.003977
  l1.weight: grad_norm = 0.073282
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.063680
Total gradient norm: 0.207849
=== Actor Training Debug (Iteration 3982) ===
Q mean: -9.430153
Q std: 12.220484
Actor loss: 9.434126
Action reg: 0.003973
  l1.weight: grad_norm = 0.093403
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.079971
Total gradient norm: 0.240813
=== Actor Training Debug (Iteration 3983) ===
Q mean: -10.054226
Q std: 12.219080
Actor loss: 10.058199
Action reg: 0.003973
  l1.weight: grad_norm = 0.044973
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.042070
Total gradient norm: 0.125185
=== Actor Training Debug (Iteration 3984) ===
Q mean: -9.488504
Q std: 12.817996
Actor loss: 9.492477
Action reg: 0.003973
  l1.weight: grad_norm = 0.117561
  l1.bias: grad_norm = 0.000671
  l2.weight: grad_norm = 0.097081
Total gradient norm: 0.273401
=== Actor Training Debug (Iteration 3985) ===
Q mean: -9.504545
Q std: 12.117690
Actor loss: 9.508523
Action reg: 0.003978
  l1.weight: grad_norm = 0.063788
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.058354
Total gradient norm: 0.254377
=== Actor Training Debug (Iteration 3986) ===
Q mean: -10.530878
Q std: 12.506058
Actor loss: 10.534849
Action reg: 0.003971
  l1.weight: grad_norm = 0.093936
  l1.bias: grad_norm = 0.000660
  l2.weight: grad_norm = 0.067535
Total gradient norm: 0.225920
=== Actor Training Debug (Iteration 3987) ===
Q mean: -10.549797
Q std: 13.603233
Actor loss: 10.553760
Action reg: 0.003962
  l1.weight: grad_norm = 0.146726
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.139007
Total gradient norm: 0.405320
=== Actor Training Debug (Iteration 3988) ===
Q mean: -9.995050
Q std: 13.052162
Actor loss: 9.999027
Action reg: 0.003976
  l1.weight: grad_norm = 0.061511
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.050538
Total gradient norm: 0.166303
=== Actor Training Debug (Iteration 3989) ===
Q mean: -12.459690
Q std: 14.514934
Actor loss: 12.463662
Action reg: 0.003972
  l1.weight: grad_norm = 0.155292
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.139022
Total gradient norm: 0.446220
=== Actor Training Debug (Iteration 3990) ===
Q mean: -10.030418
Q std: 13.777321
Actor loss: 10.034394
Action reg: 0.003976
  l1.weight: grad_norm = 0.036850
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.030043
Total gradient norm: 0.098486
=== Actor Training Debug (Iteration 3991) ===
Q mean: -11.531791
Q std: 13.676923
Actor loss: 11.535773
Action reg: 0.003983
  l1.weight: grad_norm = 0.062693
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.055515
Total gradient norm: 0.156047
=== Actor Training Debug (Iteration 3992) ===
Q mean: -9.255102
Q std: 12.358821
Actor loss: 9.259075
Action reg: 0.003973
  l1.weight: grad_norm = 0.120517
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.093278
Total gradient norm: 0.301567
=== Actor Training Debug (Iteration 3993) ===
Q mean: -8.578032
Q std: 12.127286
Actor loss: 8.582007
Action reg: 0.003976
  l1.weight: grad_norm = 0.083914
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.073332
Total gradient norm: 0.239661
=== Actor Training Debug (Iteration 3994) ===
Q mean: -9.318508
Q std: 12.944173
Actor loss: 9.322482
Action reg: 0.003974
  l1.weight: grad_norm = 0.119329
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.115058
Total gradient norm: 0.377384
=== Actor Training Debug (Iteration 3995) ===
Q mean: -10.217737
Q std: 13.049842
Actor loss: 10.221709
Action reg: 0.003972
  l1.weight: grad_norm = 0.134649
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.097095
Total gradient norm: 0.269134
=== Actor Training Debug (Iteration 3996) ===
Q mean: -12.462490
Q std: 14.362329
Actor loss: 12.466470
Action reg: 0.003980
  l1.weight: grad_norm = 0.346656
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.217569
Total gradient norm: 0.682598
=== Actor Training Debug (Iteration 3997) ===
Q mean: -10.814657
Q std: 13.233016
Actor loss: 10.818630
Action reg: 0.003973
  l1.weight: grad_norm = 0.093492
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.077640
Total gradient norm: 0.264864
=== Actor Training Debug (Iteration 3998) ===
Q mean: -10.328253
Q std: 13.676687
Actor loss: 10.332226
Action reg: 0.003973
  l1.weight: grad_norm = 0.093220
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.078915
Total gradient norm: 0.308156
=== Actor Training Debug (Iteration 3999) ===
Q mean: -10.581484
Q std: 12.893672
Actor loss: 10.585446
Action reg: 0.003963
  l1.weight: grad_norm = 0.131626
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.105528
Total gradient norm: 0.312786
=== Actor Training Debug (Iteration 4000) ===
Q mean: -10.723459
Q std: 13.204150
Actor loss: 10.727434
Action reg: 0.003975
  l1.weight: grad_norm = 0.087170
  l1.bias: grad_norm = 0.000655
  l2.weight: grad_norm = 0.081020
Total gradient norm: 0.195736
Step 9000: Critic Loss: 1.2671, Actor Loss: 10.7274, Q Value: -10.7235
  Average reward: -320.722 | Average length: 100.0
Evaluation at episode 90: -320.722
=== Actor Training Debug (Iteration 4001) ===
Q mean: -10.551053
Q std: 13.167275
Actor loss: 10.555039
Action reg: 0.003986
  l1.weight: grad_norm = 0.027921
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.025437
Total gradient norm: 0.083872
=== Actor Training Debug (Iteration 4002) ===
Q mean: -10.335912
Q std: 13.554993
Actor loss: 10.339884
Action reg: 0.003972
  l1.weight: grad_norm = 0.096203
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.089473
Total gradient norm: 0.259993
=== Actor Training Debug (Iteration 4003) ===
Q mean: -10.799262
Q std: 12.398197
Actor loss: 10.803237
Action reg: 0.003975
  l1.weight: grad_norm = 0.145618
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.139964
Total gradient norm: 0.423660
=== Actor Training Debug (Iteration 4004) ===
Q mean: -11.401306
Q std: 13.733560
Actor loss: 11.405278
Action reg: 0.003972
  l1.weight: grad_norm = 0.069607
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.055258
Total gradient norm: 0.177387
=== Actor Training Debug (Iteration 4005) ===
Q mean: -10.388512
Q std: 13.677375
Actor loss: 10.392492
Action reg: 0.003981
  l1.weight: grad_norm = 0.094371
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.094139
Total gradient norm: 0.265446
=== Actor Training Debug (Iteration 4006) ===
Q mean: -11.572672
Q std: 13.833450
Actor loss: 11.576640
Action reg: 0.003968
  l1.weight: grad_norm = 0.166425
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.156094
Total gradient norm: 0.555301
=== Actor Training Debug (Iteration 4007) ===
Q mean: -11.599661
Q std: 13.857576
Actor loss: 11.603634
Action reg: 0.003973
  l1.weight: grad_norm = 0.118251
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.110019
Total gradient norm: 0.311319
=== Actor Training Debug (Iteration 4008) ===
Q mean: -11.165056
Q std: 13.222443
Actor loss: 11.169044
Action reg: 0.003989
  l1.weight: grad_norm = 0.026148
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.021240
Total gradient norm: 0.062747
=== Actor Training Debug (Iteration 4009) ===
Q mean: -9.878325
Q std: 13.499329
Actor loss: 9.882296
Action reg: 0.003970
  l1.weight: grad_norm = 0.130306
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.103563
Total gradient norm: 0.303119
=== Actor Training Debug (Iteration 4010) ===
Q mean: -9.070141
Q std: 13.927579
Actor loss: 9.074111
Action reg: 0.003970
  l1.weight: grad_norm = 0.187750
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.123096
Total gradient norm: 0.402160
=== Actor Training Debug (Iteration 4011) ===
Q mean: -9.818134
Q std: 13.474199
Actor loss: 9.822107
Action reg: 0.003973
  l1.weight: grad_norm = 0.111228
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.088377
Total gradient norm: 0.245272
=== Actor Training Debug (Iteration 4012) ===
Q mean: -12.043158
Q std: 15.020907
Actor loss: 12.047132
Action reg: 0.003974
  l1.weight: grad_norm = 0.051022
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.047192
Total gradient norm: 0.170813
=== Actor Training Debug (Iteration 4013) ===
Q mean: -11.074442
Q std: 14.249671
Actor loss: 11.078423
Action reg: 0.003981
  l1.weight: grad_norm = 0.060462
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.053334
Total gradient norm: 0.193095
=== Actor Training Debug (Iteration 4014) ===
Q mean: -11.049793
Q std: 13.944209
Actor loss: 11.053771
Action reg: 0.003978
  l1.weight: grad_norm = 0.079457
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.071353
Total gradient norm: 0.277695
=== Actor Training Debug (Iteration 4015) ===
Q mean: -10.075338
Q std: 12.446900
Actor loss: 10.079296
Action reg: 0.003957
  l1.weight: grad_norm = 0.067415
  l1.bias: grad_norm = 0.001840
  l2.weight: grad_norm = 0.061918
Total gradient norm: 0.211948
=== Actor Training Debug (Iteration 4016) ===
Q mean: -10.845879
Q std: 13.542169
Actor loss: 10.849858
Action reg: 0.003980
  l1.weight: grad_norm = 0.047389
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.043114
Total gradient norm: 0.142195
=== Actor Training Debug (Iteration 4017) ===
Q mean: -10.446896
Q std: 13.781400
Actor loss: 10.450867
Action reg: 0.003971
  l1.weight: grad_norm = 0.099752
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.078488
Total gradient norm: 0.285066
=== Actor Training Debug (Iteration 4018) ===
Q mean: -9.481140
Q std: 12.490821
Actor loss: 9.485123
Action reg: 0.003983
  l1.weight: grad_norm = 0.263769
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.203387
Total gradient norm: 0.690546
=== Actor Training Debug (Iteration 4019) ===
Q mean: -11.087946
Q std: 12.797191
Actor loss: 11.091906
Action reg: 0.003959
  l1.weight: grad_norm = 0.151484
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.129640
Total gradient norm: 0.388035
=== Actor Training Debug (Iteration 4020) ===
Q mean: -10.570986
Q std: 12.694788
Actor loss: 10.574972
Action reg: 0.003987
  l1.weight: grad_norm = 0.080687
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.075128
Total gradient norm: 0.202793
=== Actor Training Debug (Iteration 4021) ===
Q mean: -10.433324
Q std: 13.389394
Actor loss: 10.437304
Action reg: 0.003981
  l1.weight: grad_norm = 0.106196
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.091245
Total gradient norm: 0.280537
=== Actor Training Debug (Iteration 4022) ===
Q mean: -10.575136
Q std: 13.184564
Actor loss: 10.579113
Action reg: 0.003977
  l1.weight: grad_norm = 0.051562
  l1.bias: grad_norm = 0.000454
  l2.weight: grad_norm = 0.050374
Total gradient norm: 0.144519
=== Actor Training Debug (Iteration 4023) ===
Q mean: -10.957966
Q std: 12.507355
Actor loss: 10.961935
Action reg: 0.003969
  l1.weight: grad_norm = 0.029611
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.027369
Total gradient norm: 0.090483
=== Actor Training Debug (Iteration 4024) ===
Q mean: -10.171740
Q std: 13.188108
Actor loss: 10.175717
Action reg: 0.003978
  l1.weight: grad_norm = 0.037987
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.041604
Total gradient norm: 0.129212
=== Actor Training Debug (Iteration 4025) ===
Q mean: -9.953511
Q std: 11.839252
Actor loss: 9.957502
Action reg: 0.003991
  l1.weight: grad_norm = 0.050847
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.044182
Total gradient norm: 0.122842
=== Actor Training Debug (Iteration 4026) ===
Q mean: -10.918899
Q std: 13.916529
Actor loss: 10.922869
Action reg: 0.003970
  l1.weight: grad_norm = 0.073248
  l1.bias: grad_norm = 0.000498
  l2.weight: grad_norm = 0.065312
Total gradient norm: 0.210076
=== Actor Training Debug (Iteration 4027) ===
Q mean: -11.126972
Q std: 13.716894
Actor loss: 11.130947
Action reg: 0.003974
  l1.weight: grad_norm = 0.080438
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.069025
Total gradient norm: 0.200664
=== Actor Training Debug (Iteration 4028) ===
Q mean: -11.948724
Q std: 14.929853
Actor loss: 11.952692
Action reg: 0.003969
  l1.weight: grad_norm = 0.055776
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.054058
Total gradient norm: 0.164292
=== Actor Training Debug (Iteration 4029) ===
Q mean: -9.565186
Q std: 12.098807
Actor loss: 9.569167
Action reg: 0.003982
  l1.weight: grad_norm = 0.040179
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.030044
Total gradient norm: 0.100801
=== Actor Training Debug (Iteration 4030) ===
Q mean: -10.457026
Q std: 12.883629
Actor loss: 10.460994
Action reg: 0.003968
  l1.weight: grad_norm = 0.072010
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.060968
Total gradient norm: 0.182989
=== Actor Training Debug (Iteration 4031) ===
Q mean: -10.998067
Q std: 13.923214
Actor loss: 11.002045
Action reg: 0.003978
  l1.weight: grad_norm = 0.086155
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.072867
Total gradient norm: 0.251626
=== Actor Training Debug (Iteration 4032) ===
Q mean: -9.926233
Q std: 13.735029
Actor loss: 9.930218
Action reg: 0.003985
  l1.weight: grad_norm = 0.100941
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.077053
Total gradient norm: 0.278267
=== Actor Training Debug (Iteration 4033) ===
Q mean: -11.088215
Q std: 13.981762
Actor loss: 11.092193
Action reg: 0.003977
  l1.weight: grad_norm = 0.093212
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.066855
Total gradient norm: 0.197362
=== Actor Training Debug (Iteration 4034) ===
Q mean: -10.921430
Q std: 14.300249
Actor loss: 10.925395
Action reg: 0.003966
  l1.weight: grad_norm = 0.066092
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.055082
Total gradient norm: 0.178551
=== Actor Training Debug (Iteration 4035) ===
Q mean: -9.694547
Q std: 12.161250
Actor loss: 9.698527
Action reg: 0.003981
  l1.weight: grad_norm = 0.103591
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.081892
Total gradient norm: 0.236270
=== Actor Training Debug (Iteration 4036) ===
Q mean: -10.356173
Q std: 14.461399
Actor loss: 10.360159
Action reg: 0.003986
  l1.weight: grad_norm = 0.071613
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.069379
Total gradient norm: 0.209022
=== Actor Training Debug (Iteration 4037) ===
Q mean: -8.752502
Q std: 11.690802
Actor loss: 8.756484
Action reg: 0.003981
  l1.weight: grad_norm = 0.114437
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.095608
Total gradient norm: 0.277399
=== Actor Training Debug (Iteration 4038) ===
Q mean: -11.294260
Q std: 13.807968
Actor loss: 11.298241
Action reg: 0.003980
  l1.weight: grad_norm = 0.075400
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.071920
Total gradient norm: 0.276542
=== Actor Training Debug (Iteration 4039) ===
Q mean: -10.482483
Q std: 13.201225
Actor loss: 10.486471
Action reg: 0.003988
  l1.weight: grad_norm = 0.035535
  l1.bias: grad_norm = 0.000031
  l2.weight: grad_norm = 0.030641
Total gradient norm: 0.098521
=== Actor Training Debug (Iteration 4040) ===
Q mean: -11.646183
Q std: 15.018298
Actor loss: 11.650165
Action reg: 0.003982
  l1.weight: grad_norm = 0.142046
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.107537
Total gradient norm: 0.353654
=== Actor Training Debug (Iteration 4041) ===
Q mean: -10.055838
Q std: 13.069304
Actor loss: 10.059819
Action reg: 0.003982
  l1.weight: grad_norm = 0.047144
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.043230
Total gradient norm: 0.136536
=== Actor Training Debug (Iteration 4042) ===
Q mean: -11.069038
Q std: 13.559729
Actor loss: 11.073019
Action reg: 0.003980
  l1.weight: grad_norm = 0.089278
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.077095
Total gradient norm: 0.283334
=== Actor Training Debug (Iteration 4043) ===
Q mean: -10.050398
Q std: 13.515179
Actor loss: 10.054380
Action reg: 0.003983
  l1.weight: grad_norm = 0.067652
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.061046
Total gradient norm: 0.224319
=== Actor Training Debug (Iteration 4044) ===
Q mean: -10.932808
Q std: 13.953212
Actor loss: 10.936783
Action reg: 0.003975
  l1.weight: grad_norm = 0.097122
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.077029
Total gradient norm: 0.245074
=== Actor Training Debug (Iteration 4045) ===
Q mean: -11.255112
Q std: 13.597922
Actor loss: 11.259097
Action reg: 0.003985
  l1.weight: grad_norm = 0.291440
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.231011
Total gradient norm: 0.660572
=== Actor Training Debug (Iteration 4046) ===
Q mean: -10.500406
Q std: 14.070901
Actor loss: 10.504392
Action reg: 0.003985
  l1.weight: grad_norm = 0.102703
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.091568
Total gradient norm: 0.262131
=== Actor Training Debug (Iteration 4047) ===
Q mean: -10.238396
Q std: 13.364914
Actor loss: 10.242377
Action reg: 0.003982
  l1.weight: grad_norm = 0.073168
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.057433
Total gradient norm: 0.174194
=== Actor Training Debug (Iteration 4048) ===
Q mean: -10.582063
Q std: 13.099570
Actor loss: 10.586040
Action reg: 0.003977
  l1.weight: grad_norm = 0.110751
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.094538
Total gradient norm: 0.317007
=== Actor Training Debug (Iteration 4049) ===
Q mean: -11.731432
Q std: 13.710847
Actor loss: 11.735415
Action reg: 0.003984
  l1.weight: grad_norm = 0.044114
  l1.bias: grad_norm = 0.000017
  l2.weight: grad_norm = 0.037245
Total gradient norm: 0.119840
=== Actor Training Debug (Iteration 4050) ===
Q mean: -11.108658
Q std: 14.011449
Actor loss: 11.112632
Action reg: 0.003974
  l1.weight: grad_norm = 0.086742
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.077792
Total gradient norm: 0.244052
=== Actor Training Debug (Iteration 4051) ===
Q mean: -10.863329
Q std: 14.326038
Actor loss: 10.867313
Action reg: 0.003985
  l1.weight: grad_norm = 0.066966
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.059416
Total gradient norm: 0.149518
=== Actor Training Debug (Iteration 4052) ===
Q mean: -11.299387
Q std: 13.760316
Actor loss: 11.303370
Action reg: 0.003982
  l1.weight: grad_norm = 0.055249
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.051375
Total gradient norm: 0.161775
=== Actor Training Debug (Iteration 4053) ===
Q mean: -10.584011
Q std: 13.632792
Actor loss: 10.588001
Action reg: 0.003990
  l1.weight: grad_norm = 0.070692
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.059657
Total gradient norm: 0.180736
=== Actor Training Debug (Iteration 4054) ===
Q mean: -11.589590
Q std: 14.722755
Actor loss: 11.593554
Action reg: 0.003964
  l1.weight: grad_norm = 0.092216
  l1.bias: grad_norm = 0.000571
  l2.weight: grad_norm = 0.075955
Total gradient norm: 0.207336
=== Actor Training Debug (Iteration 4055) ===
Q mean: -10.408594
Q std: 13.768696
Actor loss: 10.412563
Action reg: 0.003969
  l1.weight: grad_norm = 0.137556
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.125342
Total gradient norm: 0.371325
=== Actor Training Debug (Iteration 4056) ===
Q mean: -10.697158
Q std: 12.537670
Actor loss: 10.701142
Action reg: 0.003984
  l1.weight: grad_norm = 0.077078
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.073783
Total gradient norm: 0.212686
=== Actor Training Debug (Iteration 4057) ===
Q mean: -11.173214
Q std: 14.755835
Actor loss: 11.177190
Action reg: 0.003976
  l1.weight: grad_norm = 0.269311
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.212637
Total gradient norm: 0.619994
=== Actor Training Debug (Iteration 4058) ===
Q mean: -12.846119
Q std: 14.935402
Actor loss: 12.850097
Action reg: 0.003978
  l1.weight: grad_norm = 0.087222
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.074608
Total gradient norm: 0.246342
=== Actor Training Debug (Iteration 4059) ===
Q mean: -9.646330
Q std: 12.873181
Actor loss: 9.650326
Action reg: 0.003996
  l1.weight: grad_norm = 0.019965
  l1.bias: grad_norm = 0.000006
  l2.weight: grad_norm = 0.018233
Total gradient norm: 0.054294
=== Actor Training Debug (Iteration 4060) ===
Q mean: -11.090727
Q std: 13.631180
Actor loss: 11.094703
Action reg: 0.003976
  l1.weight: grad_norm = 0.104489
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.087622
Total gradient norm: 0.315145
=== Actor Training Debug (Iteration 4061) ===
Q mean: -10.354507
Q std: 12.829987
Actor loss: 10.358499
Action reg: 0.003991
  l1.weight: grad_norm = 0.008013
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.007551
Total gradient norm: 0.027846
=== Actor Training Debug (Iteration 4062) ===
Q mean: -10.613826
Q std: 12.620404
Actor loss: 10.617797
Action reg: 0.003972
  l1.weight: grad_norm = 0.100753
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.090121
Total gradient norm: 0.253012
=== Actor Training Debug (Iteration 4063) ===
Q mean: -10.292724
Q std: 12.099622
Actor loss: 10.296701
Action reg: 0.003978
  l1.weight: grad_norm = 0.154939
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.118902
Total gradient norm: 0.342990
=== Actor Training Debug (Iteration 4064) ===
Q mean: -9.846411
Q std: 13.297338
Actor loss: 9.850392
Action reg: 0.003981
  l1.weight: grad_norm = 0.112832
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.098456
Total gradient norm: 0.272064
=== Actor Training Debug (Iteration 4065) ===
Q mean: -10.909781
Q std: 14.020687
Actor loss: 10.913750
Action reg: 0.003969
  l1.weight: grad_norm = 0.057984
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.048388
Total gradient norm: 0.159328
=== Actor Training Debug (Iteration 4066) ===
Q mean: -9.962324
Q std: 13.424505
Actor loss: 9.966294
Action reg: 0.003970
  l1.weight: grad_norm = 0.132188
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.098273
Total gradient norm: 0.274325
=== Actor Training Debug (Iteration 4067) ===
Q mean: -11.192842
Q std: 13.600520
Actor loss: 11.196816
Action reg: 0.003974
  l1.weight: grad_norm = 0.185855
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.167337
Total gradient norm: 0.564299
=== Actor Training Debug (Iteration 4068) ===
Q mean: -10.105838
Q std: 12.891053
Actor loss: 10.109826
Action reg: 0.003989
  l1.weight: grad_norm = 0.045158
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.037454
Total gradient norm: 0.135913
=== Actor Training Debug (Iteration 4069) ===
Q mean: -10.172749
Q std: 12.679703
Actor loss: 10.176736
Action reg: 0.003988
  l1.weight: grad_norm = 0.024936
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.021877
Total gradient norm: 0.066905
=== Actor Training Debug (Iteration 4070) ===
Q mean: -8.886303
Q std: 12.452077
Actor loss: 8.890281
Action reg: 0.003978
  l1.weight: grad_norm = 0.062759
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.055563
Total gradient norm: 0.211289
=== Actor Training Debug (Iteration 4071) ===
Q mean: -9.964228
Q std: 10.835047
Actor loss: 9.968201
Action reg: 0.003973
  l1.weight: grad_norm = 0.178245
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.137959
Total gradient norm: 0.393634
=== Actor Training Debug (Iteration 4072) ===
Q mean: -9.778623
Q std: 13.337858
Actor loss: 9.782607
Action reg: 0.003984
  l1.weight: grad_norm = 0.056177
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.048898
Total gradient norm: 0.151324
=== Actor Training Debug (Iteration 4073) ===
Q mean: -10.281866
Q std: 13.009480
Actor loss: 10.285838
Action reg: 0.003972
  l1.weight: grad_norm = 0.069696
  l1.bias: grad_norm = 0.000498
  l2.weight: grad_norm = 0.055596
Total gradient norm: 0.153024
=== Actor Training Debug (Iteration 4074) ===
Q mean: -12.459520
Q std: 13.762817
Actor loss: 12.463493
Action reg: 0.003973
  l1.weight: grad_norm = 0.098061
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.085343
Total gradient norm: 0.290097
=== Actor Training Debug (Iteration 4075) ===
Q mean: -11.889826
Q std: 14.438603
Actor loss: 11.893806
Action reg: 0.003981
  l1.weight: grad_norm = 0.099431
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.071333
Total gradient norm: 0.213686
=== Actor Training Debug (Iteration 4076) ===
Q mean: -10.136040
Q std: 12.908793
Actor loss: 10.140016
Action reg: 0.003976
  l1.weight: grad_norm = 0.172869
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.130418
Total gradient norm: 0.426861
=== Actor Training Debug (Iteration 4077) ===
Q mean: -10.366409
Q std: 12.282281
Actor loss: 10.370375
Action reg: 0.003966
  l1.weight: grad_norm = 0.118771
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.107067
Total gradient norm: 0.419476
=== Actor Training Debug (Iteration 4078) ===
Q mean: -11.136391
Q std: 13.850315
Actor loss: 11.140372
Action reg: 0.003981
  l1.weight: grad_norm = 0.033837
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.032699
Total gradient norm: 0.098048
=== Actor Training Debug (Iteration 4079) ===
Q mean: -10.987248
Q std: 14.122777
Actor loss: 10.991220
Action reg: 0.003971
  l1.weight: grad_norm = 0.067315
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.057400
Total gradient norm: 0.189095
=== Actor Training Debug (Iteration 4080) ===
Q mean: -11.860522
Q std: 13.969029
Actor loss: 11.864498
Action reg: 0.003976
  l1.weight: grad_norm = 0.097377
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.078167
Total gradient norm: 0.235885
=== Actor Training Debug (Iteration 4081) ===
Q mean: -10.799503
Q std: 13.192925
Actor loss: 10.803490
Action reg: 0.003986
  l1.weight: grad_norm = 0.038857
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.032653
Total gradient norm: 0.089703
=== Actor Training Debug (Iteration 4082) ===
Q mean: -9.507067
Q std: 12.845360
Actor loss: 9.511030
Action reg: 0.003963
  l1.weight: grad_norm = 0.048359
  l1.bias: grad_norm = 0.000549
  l2.weight: grad_norm = 0.041102
Total gradient norm: 0.118506
=== Actor Training Debug (Iteration 4083) ===
Q mean: -10.711766
Q std: 13.497567
Actor loss: 10.715742
Action reg: 0.003976
  l1.weight: grad_norm = 0.091972
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.087898
Total gradient norm: 0.304914
=== Actor Training Debug (Iteration 4084) ===
Q mean: -9.823605
Q std: 13.371483
Actor loss: 9.827573
Action reg: 0.003969
  l1.weight: grad_norm = 0.045417
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.038583
Total gradient norm: 0.118518
=== Actor Training Debug (Iteration 4085) ===
Q mean: -9.233955
Q std: 12.947113
Actor loss: 9.237932
Action reg: 0.003977
  l1.weight: grad_norm = 0.109941
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.080395
Total gradient norm: 0.239964
=== Actor Training Debug (Iteration 4086) ===
Q mean: -10.752891
Q std: 13.263530
Actor loss: 10.756866
Action reg: 0.003975
  l1.weight: grad_norm = 0.045537
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.036619
Total gradient norm: 0.116885
=== Actor Training Debug (Iteration 4087) ===
Q mean: -9.166502
Q std: 12.232176
Actor loss: 9.170467
Action reg: 0.003966
  l1.weight: grad_norm = 0.077150
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.068048
Total gradient norm: 0.201557
=== Actor Training Debug (Iteration 4088) ===
Q mean: -10.714531
Q std: 13.978095
Actor loss: 10.718511
Action reg: 0.003980
  l1.weight: grad_norm = 0.062530
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.050644
Total gradient norm: 0.164268
=== Actor Training Debug (Iteration 4089) ===
Q mean: -11.227530
Q std: 13.618102
Actor loss: 11.231517
Action reg: 0.003987
  l1.weight: grad_norm = 0.035073
  l1.bias: grad_norm = 0.000018
  l2.weight: grad_norm = 0.030020
Total gradient norm: 0.101798
=== Actor Training Debug (Iteration 4090) ===
Q mean: -11.863325
Q std: 14.272867
Actor loss: 11.867298
Action reg: 0.003973
  l1.weight: grad_norm = 0.098816
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.088706
Total gradient norm: 0.281320
=== Actor Training Debug (Iteration 4091) ===
Q mean: -11.433611
Q std: 13.651045
Actor loss: 11.437577
Action reg: 0.003966
  l1.weight: grad_norm = 0.084500
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.070397
Total gradient norm: 0.227090
=== Actor Training Debug (Iteration 4092) ===
Q mean: -9.454544
Q std: 12.661301
Actor loss: 9.458511
Action reg: 0.003967
  l1.weight: grad_norm = 0.264181
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.220254
Total gradient norm: 0.793531
=== Actor Training Debug (Iteration 4093) ===
Q mean: -11.461554
Q std: 12.958337
Actor loss: 11.465533
Action reg: 0.003980
  l1.weight: grad_norm = 0.112329
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.092306
Total gradient norm: 0.288468
=== Actor Training Debug (Iteration 4094) ===
Q mean: -11.628666
Q std: 13.976190
Actor loss: 11.632627
Action reg: 0.003961
  l1.weight: grad_norm = 0.174518
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.147597
Total gradient norm: 0.416638
=== Actor Training Debug (Iteration 4095) ===
Q mean: -10.925202
Q std: 12.962039
Actor loss: 10.929187
Action reg: 0.003984
  l1.weight: grad_norm = 0.028475
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.023499
Total gradient norm: 0.067489
=== Actor Training Debug (Iteration 4096) ===
Q mean: -11.061995
Q std: 13.989724
Actor loss: 11.065981
Action reg: 0.003986
  l1.weight: grad_norm = 0.050797
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.047454
Total gradient norm: 0.172533
=== Actor Training Debug (Iteration 4097) ===
Q mean: -10.900930
Q std: 13.214335
Actor loss: 10.904901
Action reg: 0.003970
  l1.weight: grad_norm = 0.037701
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.034094
Total gradient norm: 0.109535
=== Actor Training Debug (Iteration 4098) ===
Q mean: -9.881090
Q std: 12.826684
Actor loss: 9.885065
Action reg: 0.003975
  l1.weight: grad_norm = 0.130040
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.110267
Total gradient norm: 0.315403
=== Actor Training Debug (Iteration 4099) ===
Q mean: -9.075185
Q std: 12.324746
Actor loss: 9.079153
Action reg: 0.003969
  l1.weight: grad_norm = 0.078463
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.066212
Total gradient norm: 0.237374
=== Actor Training Debug (Iteration 4100) ===
Q mean: -11.273123
Q std: 13.160880
Actor loss: 11.277108
Action reg: 0.003986
  l1.weight: grad_norm = 0.048281
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.046453
Total gradient norm: 0.157056
Episode 91: Steps=100, Reward=-285.646, Buffer_size=9100
=== Actor Training Debug (Iteration 4101) ===
Q mean: -11.065022
Q std: 14.266532
Actor loss: 11.068996
Action reg: 0.003974
  l1.weight: grad_norm = 0.141402
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.129641
Total gradient norm: 0.416142
=== Actor Training Debug (Iteration 4102) ===
Q mean: -10.410941
Q std: 13.533655
Actor loss: 10.414911
Action reg: 0.003970
  l1.weight: grad_norm = 0.123738
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.089699
Total gradient norm: 0.236877
=== Actor Training Debug (Iteration 4103) ===
Q mean: -11.046146
Q std: 14.712631
Actor loss: 11.050127
Action reg: 0.003980
  l1.weight: grad_norm = 0.252727
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.191708
Total gradient norm: 0.558435
=== Actor Training Debug (Iteration 4104) ===
Q mean: -11.558022
Q std: 14.071366
Actor loss: 11.561997
Action reg: 0.003975
  l1.weight: grad_norm = 0.053408
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.049581
Total gradient norm: 0.161905
=== Actor Training Debug (Iteration 4105) ===
Q mean: -11.858974
Q std: 14.222860
Actor loss: 11.862946
Action reg: 0.003972
  l1.weight: grad_norm = 0.115401
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.092835
Total gradient norm: 0.289301
=== Actor Training Debug (Iteration 4106) ===
Q mean: -11.419716
Q std: 15.040986
Actor loss: 11.423689
Action reg: 0.003973
  l1.weight: grad_norm = 0.089718
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.071932
Total gradient norm: 0.279099
=== Actor Training Debug (Iteration 4107) ===
Q mean: -9.961077
Q std: 13.432154
Actor loss: 9.965055
Action reg: 0.003978
  l1.weight: grad_norm = 0.112944
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.097469
Total gradient norm: 0.317283
=== Actor Training Debug (Iteration 4108) ===
Q mean: -10.758753
Q std: 13.150952
Actor loss: 10.762729
Action reg: 0.003976
  l1.weight: grad_norm = 0.248342
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.159197
Total gradient norm: 0.419516
=== Actor Training Debug (Iteration 4109) ===
Q mean: -11.819887
Q std: 13.940824
Actor loss: 11.823862
Action reg: 0.003975
  l1.weight: grad_norm = 0.073306
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.065973
Total gradient norm: 0.229728
=== Actor Training Debug (Iteration 4110) ===
Q mean: -11.680067
Q std: 13.316774
Actor loss: 11.684047
Action reg: 0.003980
  l1.weight: grad_norm = 0.134764
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.104886
Total gradient norm: 0.312146
=== Actor Training Debug (Iteration 4111) ===
Q mean: -10.198565
Q std: 12.905324
Actor loss: 10.202532
Action reg: 0.003967
  l1.weight: grad_norm = 0.044921
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.037263
Total gradient norm: 0.125204
=== Actor Training Debug (Iteration 4112) ===
Q mean: -10.713798
Q std: 13.450632
Actor loss: 10.717774
Action reg: 0.003977
  l1.weight: grad_norm = 0.134009
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.112647
Total gradient norm: 0.324870
=== Actor Training Debug (Iteration 4113) ===
Q mean: -10.829863
Q std: 13.598518
Actor loss: 10.833831
Action reg: 0.003968
  l1.weight: grad_norm = 0.071260
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.061650
Total gradient norm: 0.183476
=== Actor Training Debug (Iteration 4114) ===
Q mean: -13.282739
Q std: 14.465388
Actor loss: 13.286726
Action reg: 0.003988
  l1.weight: grad_norm = 0.027356
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.024906
Total gradient norm: 0.072407
=== Actor Training Debug (Iteration 4115) ===
Q mean: -10.530396
Q std: 13.343042
Actor loss: 10.534369
Action reg: 0.003973
  l1.weight: grad_norm = 0.132188
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.104892
Total gradient norm: 0.307444
=== Actor Training Debug (Iteration 4116) ===
Q mean: -9.920609
Q std: 13.068215
Actor loss: 9.924591
Action reg: 0.003982
  l1.weight: grad_norm = 0.085229
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.074225
Total gradient norm: 0.270527
=== Actor Training Debug (Iteration 4117) ===
Q mean: -12.196284
Q std: 13.986323
Actor loss: 12.200261
Action reg: 0.003976
  l1.weight: grad_norm = 0.066191
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.051138
Total gradient norm: 0.168135
=== Actor Training Debug (Iteration 4118) ===
Q mean: -10.155118
Q std: 13.094836
Actor loss: 10.159091
Action reg: 0.003973
  l1.weight: grad_norm = 0.152504
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.110694
Total gradient norm: 0.321172
=== Actor Training Debug (Iteration 4119) ===
Q mean: -10.029472
Q std: 12.831729
Actor loss: 10.033452
Action reg: 0.003979
  l1.weight: grad_norm = 0.056427
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.049818
Total gradient norm: 0.168998
=== Actor Training Debug (Iteration 4120) ===
Q mean: -11.608580
Q std: 13.652823
Actor loss: 11.612551
Action reg: 0.003971
  l1.weight: grad_norm = 0.202520
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.154599
Total gradient norm: 0.474945
=== Actor Training Debug (Iteration 4121) ===
Q mean: -11.258169
Q std: 13.336060
Actor loss: 11.262143
Action reg: 0.003974
  l1.weight: grad_norm = 0.054004
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.055126
Total gradient norm: 0.218810
=== Actor Training Debug (Iteration 4122) ===
Q mean: -11.297384
Q std: 14.612159
Actor loss: 11.301365
Action reg: 0.003980
  l1.weight: grad_norm = 0.110501
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.089562
Total gradient norm: 0.314059
=== Actor Training Debug (Iteration 4123) ===
Q mean: -10.582373
Q std: 13.966332
Actor loss: 10.586349
Action reg: 0.003977
  l1.weight: grad_norm = 0.198683
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.175761
Total gradient norm: 0.608826
=== Actor Training Debug (Iteration 4124) ===
Q mean: -11.341954
Q std: 13.723771
Actor loss: 11.345933
Action reg: 0.003978
  l1.weight: grad_norm = 0.136245
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.111658
Total gradient norm: 0.424773
=== Actor Training Debug (Iteration 4125) ===
Q mean: -11.248262
Q std: 13.759462
Actor loss: 11.252233
Action reg: 0.003970
  l1.weight: grad_norm = 0.217067
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.195759
Total gradient norm: 0.606630
=== Actor Training Debug (Iteration 4126) ===
Q mean: -11.364677
Q std: 14.383698
Actor loss: 11.368649
Action reg: 0.003971
  l1.weight: grad_norm = 0.093612
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.081730
Total gradient norm: 0.273839
=== Actor Training Debug (Iteration 4127) ===
Q mean: -10.848497
Q std: 15.201923
Actor loss: 10.852459
Action reg: 0.003962
  l1.weight: grad_norm = 0.163628
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.163599
Total gradient norm: 0.698609
=== Actor Training Debug (Iteration 4128) ===
Q mean: -10.919190
Q std: 13.379854
Actor loss: 10.923142
Action reg: 0.003952
  l1.weight: grad_norm = 0.186642
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.162825
Total gradient norm: 0.701804
=== Actor Training Debug (Iteration 4129) ===
Q mean: -10.599368
Q std: 14.193950
Actor loss: 10.603334
Action reg: 0.003966
  l1.weight: grad_norm = 0.145832
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.146678
Total gradient norm: 0.588220
=== Actor Training Debug (Iteration 4130) ===
Q mean: -11.256037
Q std: 14.785460
Actor loss: 11.260007
Action reg: 0.003970
  l1.weight: grad_norm = 0.144433
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.138165
Total gradient norm: 0.521672
=== Actor Training Debug (Iteration 4131) ===
Q mean: -11.197701
Q std: 14.487547
Actor loss: 11.201672
Action reg: 0.003971
  l1.weight: grad_norm = 0.187882
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.158890
Total gradient norm: 0.504201
=== Actor Training Debug (Iteration 4132) ===
Q mean: -10.752536
Q std: 14.009119
Actor loss: 10.756510
Action reg: 0.003974
  l1.weight: grad_norm = 0.131436
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.122110
Total gradient norm: 0.461251
=== Actor Training Debug (Iteration 4133) ===
Q mean: -10.191194
Q std: 13.394834
Actor loss: 10.195160
Action reg: 0.003966
  l1.weight: grad_norm = 0.096000
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.086473
Total gradient norm: 0.270850
=== Actor Training Debug (Iteration 4134) ===
Q mean: -10.742840
Q std: 14.617917
Actor loss: 10.746802
Action reg: 0.003963
  l1.weight: grad_norm = 0.133466
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.125274
Total gradient norm: 0.473275
=== Actor Training Debug (Iteration 4135) ===
Q mean: -9.617992
Q std: 13.095603
Actor loss: 9.621971
Action reg: 0.003978
  l1.weight: grad_norm = 0.226469
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.214955
Total gradient norm: 0.707593
=== Actor Training Debug (Iteration 4136) ===
Q mean: -10.487686
Q std: 13.356027
Actor loss: 10.491655
Action reg: 0.003969
  l1.weight: grad_norm = 0.165137
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.162077
Total gradient norm: 0.514173
=== Actor Training Debug (Iteration 4137) ===
Q mean: -10.746312
Q std: 13.417161
Actor loss: 10.750290
Action reg: 0.003978
  l1.weight: grad_norm = 0.083777
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.090030
Total gradient norm: 0.272023
=== Actor Training Debug (Iteration 4138) ===
Q mean: -10.000627
Q std: 12.841557
Actor loss: 10.004597
Action reg: 0.003970
  l1.weight: grad_norm = 0.178924
  l1.bias: grad_norm = 0.000799
  l2.weight: grad_norm = 0.158237
Total gradient norm: 0.567858
=== Actor Training Debug (Iteration 4139) ===
Q mean: -10.638772
Q std: 14.251562
Actor loss: 10.642739
Action reg: 0.003968
  l1.weight: grad_norm = 0.073997
  l1.bias: grad_norm = 0.000672
  l2.weight: grad_norm = 0.063980
Total gradient norm: 0.229772
=== Actor Training Debug (Iteration 4140) ===
Q mean: -11.124342
Q std: 13.217919
Actor loss: 11.128305
Action reg: 0.003963
  l1.weight: grad_norm = 0.071354
  l1.bias: grad_norm = 0.001350
  l2.weight: grad_norm = 0.064464
Total gradient norm: 0.224793
=== Actor Training Debug (Iteration 4141) ===
Q mean: -11.007401
Q std: 13.216724
Actor loss: 11.011368
Action reg: 0.003967
  l1.weight: grad_norm = 0.119185
  l1.bias: grad_norm = 0.000967
  l2.weight: grad_norm = 0.113752
Total gradient norm: 0.363587
=== Actor Training Debug (Iteration 4142) ===
Q mean: -12.258887
Q std: 13.212609
Actor loss: 12.262859
Action reg: 0.003972
  l1.weight: grad_norm = 0.105927
  l1.bias: grad_norm = 0.001094
  l2.weight: grad_norm = 0.093156
Total gradient norm: 0.313684
=== Actor Training Debug (Iteration 4143) ===
Q mean: -10.355453
Q std: 12.475564
Actor loss: 10.359412
Action reg: 0.003958
  l1.weight: grad_norm = 0.210046
  l1.bias: grad_norm = 0.001415
  l2.weight: grad_norm = 0.179602
Total gradient norm: 0.579264
=== Actor Training Debug (Iteration 4144) ===
Q mean: -10.924503
Q std: 12.235237
Actor loss: 10.928466
Action reg: 0.003962
  l1.weight: grad_norm = 0.173203
  l1.bias: grad_norm = 0.001334
  l2.weight: grad_norm = 0.127705
Total gradient norm: 0.475564
=== Actor Training Debug (Iteration 4145) ===
Q mean: -10.612783
Q std: 13.745585
Actor loss: 10.616754
Action reg: 0.003971
  l1.weight: grad_norm = 0.135815
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.114252
Total gradient norm: 0.338058
=== Actor Training Debug (Iteration 4146) ===
Q mean: -10.711120
Q std: 13.424430
Actor loss: 10.715086
Action reg: 0.003967
  l1.weight: grad_norm = 0.192914
  l1.bias: grad_norm = 0.000948
  l2.weight: grad_norm = 0.191713
Total gradient norm: 0.598585
=== Actor Training Debug (Iteration 4147) ===
Q mean: -10.503069
Q std: 13.371394
Actor loss: 10.507015
Action reg: 0.003946
  l1.weight: grad_norm = 0.078108
  l1.bias: grad_norm = 0.001398
  l2.weight: grad_norm = 0.068350
Total gradient norm: 0.222775
=== Actor Training Debug (Iteration 4148) ===
Q mean: -11.554993
Q std: 13.428772
Actor loss: 11.558960
Action reg: 0.003968
  l1.weight: grad_norm = 0.175771
  l1.bias: grad_norm = 0.000844
  l2.weight: grad_norm = 0.172477
Total gradient norm: 0.738448
=== Actor Training Debug (Iteration 4149) ===
Q mean: -10.251381
Q std: 13.172174
Actor loss: 10.255347
Action reg: 0.003966
  l1.weight: grad_norm = 0.085038
  l1.bias: grad_norm = 0.002240
  l2.weight: grad_norm = 0.082471
Total gradient norm: 0.266470
=== Actor Training Debug (Iteration 4150) ===
Q mean: -8.532429
Q std: 11.228860
Actor loss: 8.536400
Action reg: 0.003972
  l1.weight: grad_norm = 0.189163
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.142850
Total gradient norm: 0.469680
=== Actor Training Debug (Iteration 4151) ===
Q mean: -9.849396
Q std: 13.442462
Actor loss: 9.853337
Action reg: 0.003942
  l1.weight: grad_norm = 0.137859
  l1.bias: grad_norm = 0.000857
  l2.weight: grad_norm = 0.115281
Total gradient norm: 0.333721
=== Actor Training Debug (Iteration 4152) ===
Q mean: -10.007107
Q std: 13.031559
Actor loss: 10.011071
Action reg: 0.003964
  l1.weight: grad_norm = 0.147632
  l1.bias: grad_norm = 0.000683
  l2.weight: grad_norm = 0.144200
Total gradient norm: 0.451160
=== Actor Training Debug (Iteration 4153) ===
Q mean: -11.512652
Q std: 14.452828
Actor loss: 11.516612
Action reg: 0.003959
  l1.weight: grad_norm = 0.097440
  l1.bias: grad_norm = 0.001511
  l2.weight: grad_norm = 0.082233
Total gradient norm: 0.292134
=== Actor Training Debug (Iteration 4154) ===
Q mean: -9.625175
Q std: 13.233160
Actor loss: 9.629138
Action reg: 0.003963
  l1.weight: grad_norm = 0.147641
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.131485
Total gradient norm: 0.404281
=== Actor Training Debug (Iteration 4155) ===
Q mean: -11.334888
Q std: 13.241139
Actor loss: 11.338857
Action reg: 0.003969
  l1.weight: grad_norm = 0.084992
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.073818
Total gradient norm: 0.258958
=== Actor Training Debug (Iteration 4156) ===
Q mean: -11.390198
Q std: 13.593496
Actor loss: 11.394156
Action reg: 0.003958
  l1.weight: grad_norm = 0.243520
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.199980
Total gradient norm: 0.736549
=== Actor Training Debug (Iteration 4157) ===
Q mean: -12.703959
Q std: 14.577067
Actor loss: 12.707923
Action reg: 0.003964
  l1.weight: grad_norm = 0.173920
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.148624
Total gradient norm: 0.501176
=== Actor Training Debug (Iteration 4158) ===
Q mean: -10.261045
Q std: 13.169913
Actor loss: 10.265016
Action reg: 0.003970
  l1.weight: grad_norm = 0.107002
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.105547
Total gradient norm: 0.288668
=== Actor Training Debug (Iteration 4159) ===
Q mean: -9.838964
Q std: 12.529006
Actor loss: 9.842933
Action reg: 0.003968
  l1.weight: grad_norm = 0.183987
  l1.bias: grad_norm = 0.000827
  l2.weight: grad_norm = 0.146371
Total gradient norm: 0.459804
=== Actor Training Debug (Iteration 4160) ===
Q mean: -9.838219
Q std: 12.935406
Actor loss: 9.842206
Action reg: 0.003987
  l1.weight: grad_norm = 0.121679
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.113499
Total gradient norm: 0.376401
=== Actor Training Debug (Iteration 4161) ===
Q mean: -11.224865
Q std: 14.070107
Actor loss: 11.228834
Action reg: 0.003969
  l1.weight: grad_norm = 0.141346
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.137758
Total gradient norm: 0.394366
=== Actor Training Debug (Iteration 4162) ===
Q mean: -10.794584
Q std: 14.049841
Actor loss: 10.798557
Action reg: 0.003973
  l1.weight: grad_norm = 0.095198
  l1.bias: grad_norm = 0.000727
  l2.weight: grad_norm = 0.071881
Total gradient norm: 0.228179
=== Actor Training Debug (Iteration 4163) ===
Q mean: -10.142065
Q std: 13.126417
Actor loss: 10.146044
Action reg: 0.003978
  l1.weight: grad_norm = 0.190314
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.126050
Total gradient norm: 0.403004
=== Actor Training Debug (Iteration 4164) ===
Q mean: -9.651569
Q std: 12.155154
Actor loss: 9.655546
Action reg: 0.003977
  l1.weight: grad_norm = 0.079221
  l1.bias: grad_norm = 0.000625
  l2.weight: grad_norm = 0.064935
Total gradient norm: 0.213366
=== Actor Training Debug (Iteration 4165) ===
Q mean: -8.708036
Q std: 13.257804
Actor loss: 8.712009
Action reg: 0.003973
  l1.weight: grad_norm = 0.164428
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.130118
Total gradient norm: 0.413560
=== Actor Training Debug (Iteration 4166) ===
Q mean: -11.576004
Q std: 15.381876
Actor loss: 11.579974
Action reg: 0.003970
  l1.weight: grad_norm = 0.163812
  l1.bias: grad_norm = 0.001032
  l2.weight: grad_norm = 0.151635
Total gradient norm: 0.574141
=== Actor Training Debug (Iteration 4167) ===
Q mean: -10.741177
Q std: 13.345361
Actor loss: 10.745151
Action reg: 0.003974
  l1.weight: grad_norm = 0.081027
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.070010
Total gradient norm: 0.262124
=== Actor Training Debug (Iteration 4168) ===
Q mean: -10.718876
Q std: 12.898712
Actor loss: 10.722845
Action reg: 0.003969
  l1.weight: grad_norm = 0.111133
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.096915
Total gradient norm: 0.319995
=== Actor Training Debug (Iteration 4169) ===
Q mean: -10.404514
Q std: 14.447961
Actor loss: 10.408470
Action reg: 0.003956
  l1.weight: grad_norm = 0.128039
  l1.bias: grad_norm = 0.001208
  l2.weight: grad_norm = 0.104303
Total gradient norm: 0.342117
=== Actor Training Debug (Iteration 4170) ===
Q mean: -9.773478
Q std: 12.205857
Actor loss: 9.777446
Action reg: 0.003968
  l1.weight: grad_norm = 0.115293
  l1.bias: grad_norm = 0.000838
  l2.weight: grad_norm = 0.098962
Total gradient norm: 0.311410
=== Actor Training Debug (Iteration 4171) ===
Q mean: -10.230324
Q std: 13.221990
Actor loss: 10.234288
Action reg: 0.003964
  l1.weight: grad_norm = 0.176450
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.169734
Total gradient norm: 0.504442
=== Actor Training Debug (Iteration 4172) ===
Q mean: -11.537184
Q std: 15.141346
Actor loss: 11.541153
Action reg: 0.003969
  l1.weight: grad_norm = 0.134140
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.123667
Total gradient norm: 0.342874
=== Actor Training Debug (Iteration 4173) ===
Q mean: -9.697865
Q std: 13.012994
Actor loss: 9.701838
Action reg: 0.003974
  l1.weight: grad_norm = 0.052363
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.049757
Total gradient norm: 0.171142
=== Actor Training Debug (Iteration 4174) ===
Q mean: -11.065857
Q std: 12.841678
Actor loss: 11.069822
Action reg: 0.003966
  l1.weight: grad_norm = 0.134299
  l1.bias: grad_norm = 0.000647
  l2.weight: grad_norm = 0.105662
Total gradient norm: 0.377050
=== Actor Training Debug (Iteration 4175) ===
Q mean: -11.535165
Q std: 13.323211
Actor loss: 11.539146
Action reg: 0.003982
  l1.weight: grad_norm = 0.054802
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.048645
Total gradient norm: 0.155207
=== Actor Training Debug (Iteration 4176) ===
Q mean: -8.592614
Q std: 12.208532
Actor loss: 8.596583
Action reg: 0.003969
  l1.weight: grad_norm = 0.110900
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.098787
Total gradient norm: 0.320940
=== Actor Training Debug (Iteration 4177) ===
Q mean: -10.047070
Q std: 13.502723
Actor loss: 10.051041
Action reg: 0.003971
  l1.weight: grad_norm = 0.120878
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.111022
Total gradient norm: 0.311653
=== Actor Training Debug (Iteration 4178) ===
Q mean: -11.066298
Q std: 13.302197
Actor loss: 11.070263
Action reg: 0.003965
  l1.weight: grad_norm = 0.161568
  l1.bias: grad_norm = 0.000561
  l2.weight: grad_norm = 0.143717
Total gradient norm: 0.382728
=== Actor Training Debug (Iteration 4179) ===
Q mean: -10.486272
Q std: 13.006804
Actor loss: 10.490241
Action reg: 0.003969
  l1.weight: grad_norm = 0.098375
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.084269
Total gradient norm: 0.236252
=== Actor Training Debug (Iteration 4180) ===
Q mean: -11.123435
Q std: 13.438864
Actor loss: 11.127401
Action reg: 0.003967
  l1.weight: grad_norm = 0.124845
  l1.bias: grad_norm = 0.000694
  l2.weight: grad_norm = 0.105927
Total gradient norm: 0.327086
=== Actor Training Debug (Iteration 4181) ===
Q mean: -11.079707
Q std: 14.107863
Actor loss: 11.083692
Action reg: 0.003984
  l1.weight: grad_norm = 0.084248
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.079868
Total gradient norm: 0.278182
=== Actor Training Debug (Iteration 4182) ===
Q mean: -10.258142
Q std: 11.650843
Actor loss: 10.262105
Action reg: 0.003963
  l1.weight: grad_norm = 0.128123
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.118789
Total gradient norm: 0.339315
=== Actor Training Debug (Iteration 4183) ===
Q mean: -11.953577
Q std: 15.118582
Actor loss: 11.957537
Action reg: 0.003960
  l1.weight: grad_norm = 0.189373
  l1.bias: grad_norm = 0.000906
  l2.weight: grad_norm = 0.158695
Total gradient norm: 0.533258
=== Actor Training Debug (Iteration 4184) ===
Q mean: -9.760555
Q std: 13.308502
Actor loss: 9.764522
Action reg: 0.003966
  l1.weight: grad_norm = 0.175534
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.169147
Total gradient norm: 0.514192
=== Actor Training Debug (Iteration 4185) ===
Q mean: -11.356894
Q std: 14.616186
Actor loss: 11.360837
Action reg: 0.003943
  l1.weight: grad_norm = 0.391080
  l1.bias: grad_norm = 0.001497
  l2.weight: grad_norm = 0.356758
Total gradient norm: 0.931755
=== Actor Training Debug (Iteration 4186) ===
Q mean: -10.749882
Q std: 13.558588
Actor loss: 10.753859
Action reg: 0.003977
  l1.weight: grad_norm = 0.168890
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.163319
Total gradient norm: 0.455084
=== Actor Training Debug (Iteration 4187) ===
Q mean: -9.699159
Q std: 12.591006
Actor loss: 9.703111
Action reg: 0.003952
  l1.weight: grad_norm = 0.113441
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.099221
Total gradient norm: 0.339618
=== Actor Training Debug (Iteration 4188) ===
Q mean: -10.109415
Q std: 12.209173
Actor loss: 10.113396
Action reg: 0.003981
  l1.weight: grad_norm = 0.160171
  l1.bias: grad_norm = 0.000492
  l2.weight: grad_norm = 0.139564
Total gradient norm: 0.403012
=== Actor Training Debug (Iteration 4189) ===
Q mean: -9.968708
Q std: 12.920923
Actor loss: 9.972664
Action reg: 0.003956
  l1.weight: grad_norm = 0.131955
  l1.bias: grad_norm = 0.000595
  l2.weight: grad_norm = 0.114069
Total gradient norm: 0.381759
=== Actor Training Debug (Iteration 4190) ===
Q mean: -11.386510
Q std: 13.648669
Actor loss: 11.390460
Action reg: 0.003950
  l1.weight: grad_norm = 0.309839
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.257358
Total gradient norm: 0.709993
=== Actor Training Debug (Iteration 4191) ===
Q mean: -9.255424
Q std: 12.404540
Actor loss: 9.259385
Action reg: 0.003961
  l1.weight: grad_norm = 0.112190
  l1.bias: grad_norm = 0.000749
  l2.weight: grad_norm = 0.092199
Total gradient norm: 0.274282
=== Actor Training Debug (Iteration 4192) ===
Q mean: -9.267483
Q std: 11.641459
Actor loss: 9.271442
Action reg: 0.003959
  l1.weight: grad_norm = 0.105297
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.096201
Total gradient norm: 0.263110
=== Actor Training Debug (Iteration 4193) ===
Q mean: -11.623504
Q std: 13.877981
Actor loss: 11.627468
Action reg: 0.003965
  l1.weight: grad_norm = 0.072732
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.067372
Total gradient norm: 0.203675
=== Actor Training Debug (Iteration 4194) ===
Q mean: -12.264930
Q std: 14.284168
Actor loss: 12.268908
Action reg: 0.003978
  l1.weight: grad_norm = 0.078676
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.071363
Total gradient norm: 0.225239
=== Actor Training Debug (Iteration 4195) ===
Q mean: -11.721554
Q std: 14.717138
Actor loss: 11.725528
Action reg: 0.003974
  l1.weight: grad_norm = 0.101022
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.093249
Total gradient norm: 0.290410
=== Actor Training Debug (Iteration 4196) ===
Q mean: -9.927031
Q std: 12.351956
Actor loss: 9.931006
Action reg: 0.003976
  l1.weight: grad_norm = 0.211292
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.158278
Total gradient norm: 0.445028
=== Actor Training Debug (Iteration 4197) ===
Q mean: -10.206111
Q std: 12.491152
Actor loss: 10.210080
Action reg: 0.003970
  l1.weight: grad_norm = 0.092051
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.086701
Total gradient norm: 0.271135
=== Actor Training Debug (Iteration 4198) ===
Q mean: -9.496069
Q std: 12.366928
Actor loss: 9.500032
Action reg: 0.003963
  l1.weight: grad_norm = 0.244108
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.209606
Total gradient norm: 0.570514
=== Actor Training Debug (Iteration 4199) ===
Q mean: -10.470263
Q std: 13.761765
Actor loss: 10.474233
Action reg: 0.003970
  l1.weight: grad_norm = 0.245058
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.191132
Total gradient norm: 0.619589
=== Actor Training Debug (Iteration 4200) ===
Q mean: -10.825861
Q std: 12.956253
Actor loss: 10.829847
Action reg: 0.003986
  l1.weight: grad_norm = 0.053907
  l1.bias: grad_norm = 0.000019
  l2.weight: grad_norm = 0.049626
Total gradient norm: 0.143731
=== Actor Training Debug (Iteration 4201) ===
Q mean: -12.218722
Q std: 14.153768
Actor loss: 12.222705
Action reg: 0.003982
  l1.weight: grad_norm = 0.033005
  l1.bias: grad_norm = 0.000619
  l2.weight: grad_norm = 0.028221
Total gradient norm: 0.091325
=== Actor Training Debug (Iteration 4202) ===
Q mean: -9.974836
Q std: 13.056376
Actor loss: 9.978817
Action reg: 0.003980
  l1.weight: grad_norm = 0.073096
  l1.bias: grad_norm = 0.000590
  l2.weight: grad_norm = 0.067909
Total gradient norm: 0.252681
=== Actor Training Debug (Iteration 4203) ===
Q mean: -10.907117
Q std: 12.516470
Actor loss: 10.911098
Action reg: 0.003982
  l1.weight: grad_norm = 0.098582
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.078478
Total gradient norm: 0.235755
=== Actor Training Debug (Iteration 4204) ===
Q mean: -12.165742
Q std: 14.262343
Actor loss: 12.169711
Action reg: 0.003970
  l1.weight: grad_norm = 0.209788
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.205632
Total gradient norm: 0.589160
=== Actor Training Debug (Iteration 4205) ===
Q mean: -10.357341
Q std: 13.004066
Actor loss: 10.361317
Action reg: 0.003976
  l1.weight: grad_norm = 0.243630
  l1.bias: grad_norm = 0.000695
  l2.weight: grad_norm = 0.173016
Total gradient norm: 0.529798
=== Actor Training Debug (Iteration 4206) ===
Q mean: -10.779661
Q std: 13.554411
Actor loss: 10.783628
Action reg: 0.003966
  l1.weight: grad_norm = 0.169338
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.153069
Total gradient norm: 0.452799
=== Actor Training Debug (Iteration 4207) ===
Q mean: -10.585604
Q std: 14.639557
Actor loss: 10.589554
Action reg: 0.003950
  l1.weight: grad_norm = 0.145143
  l1.bias: grad_norm = 0.000583
  l2.weight: grad_norm = 0.131161
Total gradient norm: 0.435352
=== Actor Training Debug (Iteration 4208) ===
Q mean: -11.690142
Q std: 15.124003
Actor loss: 11.694102
Action reg: 0.003960
  l1.weight: grad_norm = 0.119911
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.116526
Total gradient norm: 0.390707
=== Actor Training Debug (Iteration 4209) ===
Q mean: -10.433428
Q std: 13.544514
Actor loss: 10.437386
Action reg: 0.003957
  l1.weight: grad_norm = 0.167361
  l1.bias: grad_norm = 0.000745
  l2.weight: grad_norm = 0.139793
Total gradient norm: 0.436832
=== Actor Training Debug (Iteration 4210) ===
Q mean: -11.502462
Q std: 15.107203
Actor loss: 11.506442
Action reg: 0.003979
  l1.weight: grad_norm = 0.078750
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.068135
Total gradient norm: 0.225218
=== Actor Training Debug (Iteration 4211) ===
Q mean: -10.767513
Q std: 13.778000
Actor loss: 10.771490
Action reg: 0.003977
  l1.weight: grad_norm = 0.060138
  l1.bias: grad_norm = 0.000524
  l2.weight: grad_norm = 0.056630
Total gradient norm: 0.172565
=== Actor Training Debug (Iteration 4212) ===
Q mean: -9.892071
Q std: 12.762092
Actor loss: 9.896035
Action reg: 0.003965
  l1.weight: grad_norm = 0.139794
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.118826
Total gradient norm: 0.445852
=== Actor Training Debug (Iteration 4213) ===
Q mean: -10.391291
Q std: 12.958836
Actor loss: 10.395266
Action reg: 0.003975
  l1.weight: grad_norm = 0.118350
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.103606
Total gradient norm: 0.317038
=== Actor Training Debug (Iteration 4214) ===
Q mean: -9.929781
Q std: 12.403634
Actor loss: 9.933747
Action reg: 0.003966
  l1.weight: grad_norm = 0.114361
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.096478
Total gradient norm: 0.298508
=== Actor Training Debug (Iteration 4215) ===
Q mean: -11.467396
Q std: 13.974979
Actor loss: 11.471354
Action reg: 0.003958
  l1.weight: grad_norm = 0.123812
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.116892
Total gradient norm: 0.339707
=== Actor Training Debug (Iteration 4216) ===
Q mean: -10.812057
Q std: 13.847691
Actor loss: 10.816021
Action reg: 0.003963
  l1.weight: grad_norm = 0.136849
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.107277
Total gradient norm: 0.326496
=== Actor Training Debug (Iteration 4217) ===
Q mean: -11.038261
Q std: 13.759260
Actor loss: 11.042230
Action reg: 0.003968
  l1.weight: grad_norm = 0.121128
  l1.bias: grad_norm = 0.000614
  l2.weight: grad_norm = 0.108043
Total gradient norm: 0.360546
=== Actor Training Debug (Iteration 4218) ===
Q mean: -9.862781
Q std: 12.233744
Actor loss: 9.866753
Action reg: 0.003973
  l1.weight: grad_norm = 0.104058
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.096360
Total gradient norm: 0.301164
=== Actor Training Debug (Iteration 4219) ===
Q mean: -9.943574
Q std: 13.333391
Actor loss: 9.947525
Action reg: 0.003951
  l1.weight: grad_norm = 0.232584
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.211462
Total gradient norm: 0.752280
=== Actor Training Debug (Iteration 4220) ===
Q mean: -10.639599
Q std: 13.764378
Actor loss: 10.643568
Action reg: 0.003970
  l1.weight: grad_norm = 0.088207
  l1.bias: grad_norm = 0.000945
  l2.weight: grad_norm = 0.081659
Total gradient norm: 0.257120
=== Actor Training Debug (Iteration 4221) ===
Q mean: -11.241208
Q std: 13.942728
Actor loss: 11.245169
Action reg: 0.003961
  l1.weight: grad_norm = 0.146461
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.129223
Total gradient norm: 0.375127
=== Actor Training Debug (Iteration 4222) ===
Q mean: -8.753051
Q std: 12.559463
Actor loss: 8.757020
Action reg: 0.003969
  l1.weight: grad_norm = 0.133884
  l1.bias: grad_norm = 0.000667
  l2.weight: grad_norm = 0.118735
Total gradient norm: 0.373300
=== Actor Training Debug (Iteration 4223) ===
Q mean: -11.894794
Q std: 14.707282
Actor loss: 11.898758
Action reg: 0.003964
  l1.weight: grad_norm = 0.096029
  l1.bias: grad_norm = 0.000698
  l2.weight: grad_norm = 0.086952
Total gradient norm: 0.287570
=== Actor Training Debug (Iteration 4224) ===
Q mean: -12.231330
Q std: 13.694203
Actor loss: 12.235306
Action reg: 0.003976
  l1.weight: grad_norm = 0.102497
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.094854
Total gradient norm: 0.273179
=== Actor Training Debug (Iteration 4225) ===
Q mean: -12.682771
Q std: 15.354114
Actor loss: 12.686728
Action reg: 0.003958
  l1.weight: grad_norm = 0.146419
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.131852
Total gradient norm: 0.376426
=== Actor Training Debug (Iteration 4226) ===
Q mean: -10.294497
Q std: 14.766335
Actor loss: 10.298470
Action reg: 0.003974
  l1.weight: grad_norm = 0.079364
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.069020
Total gradient norm: 0.195295
=== Actor Training Debug (Iteration 4227) ===
Q mean: -10.031691
Q std: 13.567966
Actor loss: 10.035672
Action reg: 0.003981
  l1.weight: grad_norm = 0.050774
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.039198
Total gradient norm: 0.113676
=== Actor Training Debug (Iteration 4228) ===
Q mean: -10.362242
Q std: 14.135175
Actor loss: 10.366199
Action reg: 0.003958
  l1.weight: grad_norm = 0.130027
  l1.bias: grad_norm = 0.000586
  l2.weight: grad_norm = 0.125980
Total gradient norm: 0.501569
=== Actor Training Debug (Iteration 4229) ===
Q mean: -10.636280
Q std: 14.302201
Actor loss: 10.640260
Action reg: 0.003980
  l1.weight: grad_norm = 0.058196
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.049061
Total gradient norm: 0.162203
=== Actor Training Debug (Iteration 4230) ===
Q mean: -11.543453
Q std: 13.722505
Actor loss: 11.547421
Action reg: 0.003968
  l1.weight: grad_norm = 0.157746
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.143620
Total gradient norm: 0.414643
=== Actor Training Debug (Iteration 4231) ===
Q mean: -9.896226
Q std: 12.159022
Actor loss: 9.900185
Action reg: 0.003959
  l1.weight: grad_norm = 0.174356
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.170304
Total gradient norm: 0.547318
=== Actor Training Debug (Iteration 4232) ===
Q mean: -10.184885
Q std: 12.687492
Actor loss: 10.188863
Action reg: 0.003978
  l1.weight: grad_norm = 0.060933
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.045577
Total gradient norm: 0.167669
=== Actor Training Debug (Iteration 4233) ===
Q mean: -10.235070
Q std: 11.733974
Actor loss: 10.239041
Action reg: 0.003971
  l1.weight: grad_norm = 0.138411
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.138415
Total gradient norm: 0.457833
=== Actor Training Debug (Iteration 4234) ===
Q mean: -10.016148
Q std: 13.187473
Actor loss: 10.020132
Action reg: 0.003985
  l1.weight: grad_norm = 0.048874
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.048375
Total gradient norm: 0.131966
=== Actor Training Debug (Iteration 4235) ===
Q mean: -11.599379
Q std: 14.302078
Actor loss: 11.603331
Action reg: 0.003952
  l1.weight: grad_norm = 0.136826
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.115374
Total gradient norm: 0.341479
=== Actor Training Debug (Iteration 4236) ===
Q mean: -9.981443
Q std: 13.744647
Actor loss: 9.985420
Action reg: 0.003977
  l1.weight: grad_norm = 0.121622
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.108664
Total gradient norm: 0.303020
=== Actor Training Debug (Iteration 4237) ===
Q mean: -10.155217
Q std: 12.581586
Actor loss: 10.159168
Action reg: 0.003952
  l1.weight: grad_norm = 0.096141
  l1.bias: grad_norm = 0.000899
  l2.weight: grad_norm = 0.091917
Total gradient norm: 0.317348
=== Actor Training Debug (Iteration 4238) ===
Q mean: -10.220991
Q std: 14.420892
Actor loss: 10.224945
Action reg: 0.003954
  l1.weight: grad_norm = 0.139821
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.119375
Total gradient norm: 0.359839
=== Actor Training Debug (Iteration 4239) ===
Q mean: -11.579580
Q std: 14.774055
Actor loss: 11.583536
Action reg: 0.003956
  l1.weight: grad_norm = 0.151026
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.146179
Total gradient norm: 0.431109
=== Actor Training Debug (Iteration 4240) ===
Q mean: -11.324450
Q std: 13.639564
Actor loss: 11.328420
Action reg: 0.003970
  l1.weight: grad_norm = 0.101128
  l1.bias: grad_norm = 0.000794
  l2.weight: grad_norm = 0.076601
Total gradient norm: 0.202248
=== Actor Training Debug (Iteration 4241) ===
Q mean: -10.762033
Q std: 14.802949
Actor loss: 10.765998
Action reg: 0.003964
  l1.weight: grad_norm = 0.305072
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.248863
Total gradient norm: 0.781165
=== Actor Training Debug (Iteration 4242) ===
Q mean: -9.291729
Q std: 12.696809
Actor loss: 9.295675
Action reg: 0.003947
  l1.weight: grad_norm = 0.215412
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.171375
Total gradient norm: 0.492775
=== Actor Training Debug (Iteration 4243) ===
Q mean: -11.162285
Q std: 13.780237
Actor loss: 11.166251
Action reg: 0.003966
  l1.weight: grad_norm = 0.132260
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.114972
Total gradient norm: 0.412819
=== Actor Training Debug (Iteration 4244) ===
Q mean: -10.272598
Q std: 13.944790
Actor loss: 10.276578
Action reg: 0.003980
  l1.weight: grad_norm = 0.096754
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.084589
Total gradient norm: 0.272962
=== Actor Training Debug (Iteration 4245) ===
Q mean: -10.445328
Q std: 12.906116
Actor loss: 10.449306
Action reg: 0.003978
  l1.weight: grad_norm = 0.047778
  l1.bias: grad_norm = 0.000539
  l2.weight: grad_norm = 0.045183
Total gradient norm: 0.140775
=== Actor Training Debug (Iteration 4246) ===
Q mean: -12.035303
Q std: 14.059334
Actor loss: 12.039279
Action reg: 0.003976
  l1.weight: grad_norm = 0.093091
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.084481
Total gradient norm: 0.258666
=== Actor Training Debug (Iteration 4247) ===
Q mean: -11.217636
Q std: 14.544094
Actor loss: 11.221612
Action reg: 0.003976
  l1.weight: grad_norm = 0.118201
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.100122
Total gradient norm: 0.326875
=== Actor Training Debug (Iteration 4248) ===
Q mean: -10.877134
Q std: 13.549432
Actor loss: 10.881103
Action reg: 0.003968
  l1.weight: grad_norm = 0.168083
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.141927
Total gradient norm: 0.515910
=== Actor Training Debug (Iteration 4249) ===
Q mean: -11.959985
Q std: 15.429337
Actor loss: 11.963956
Action reg: 0.003971
  l1.weight: grad_norm = 0.105274
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.108494
Total gradient norm: 0.351291
=== Actor Training Debug (Iteration 4250) ===
Q mean: -10.287144
Q std: 13.931605
Actor loss: 10.291104
Action reg: 0.003961
  l1.weight: grad_norm = 0.162494
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.143045
Total gradient norm: 0.502036
=== Actor Training Debug (Iteration 4251) ===
Q mean: -11.734732
Q std: 14.425033
Actor loss: 11.738693
Action reg: 0.003961
  l1.weight: grad_norm = 0.158345
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.134076
Total gradient norm: 0.378058
=== Actor Training Debug (Iteration 4252) ===
Q mean: -10.431797
Q std: 13.612309
Actor loss: 10.435768
Action reg: 0.003971
  l1.weight: grad_norm = 0.173439
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.142348
Total gradient norm: 0.466898
=== Actor Training Debug (Iteration 4253) ===
Q mean: -10.256586
Q std: 11.486744
Actor loss: 10.260561
Action reg: 0.003975
  l1.weight: grad_norm = 0.147918
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.127515
Total gradient norm: 0.390458
=== Actor Training Debug (Iteration 4254) ===
Q mean: -10.954657
Q std: 13.842254
Actor loss: 10.958631
Action reg: 0.003974
  l1.weight: grad_norm = 0.113641
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.093054
Total gradient norm: 0.266016
=== Actor Training Debug (Iteration 4255) ===
Q mean: -13.003381
Q std: 15.194400
Actor loss: 13.007353
Action reg: 0.003972
  l1.weight: grad_norm = 0.173460
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.146241
Total gradient norm: 0.517924
=== Actor Training Debug (Iteration 4256) ===
Q mean: -9.403859
Q std: 12.624002
Actor loss: 9.407818
Action reg: 0.003959
  l1.weight: grad_norm = 0.128922
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.105257
Total gradient norm: 0.316176
=== Actor Training Debug (Iteration 4257) ===
Q mean: -10.950949
Q std: 13.432339
Actor loss: 10.954912
Action reg: 0.003963
  l1.weight: grad_norm = 0.122487
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.112392
Total gradient norm: 0.338853
=== Actor Training Debug (Iteration 4258) ===
Q mean: -10.615606
Q std: 12.538594
Actor loss: 10.619577
Action reg: 0.003971
  l1.weight: grad_norm = 0.085348
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.069951
Total gradient norm: 0.193937
=== Actor Training Debug (Iteration 4259) ===
Q mean: -11.577211
Q std: 15.110028
Actor loss: 11.581190
Action reg: 0.003979
  l1.weight: grad_norm = 0.297948
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.257576
Total gradient norm: 0.860196
=== Actor Training Debug (Iteration 4260) ===
Q mean: -11.537332
Q std: 14.911216
Actor loss: 11.541300
Action reg: 0.003968
  l1.weight: grad_norm = 0.102601
  l1.bias: grad_norm = 0.000797
  l2.weight: grad_norm = 0.085071
Total gradient norm: 0.240283
=== Actor Training Debug (Iteration 4261) ===
Q mean: -11.010468
Q std: 13.906857
Actor loss: 11.014436
Action reg: 0.003968
  l1.weight: grad_norm = 0.120021
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.104913
Total gradient norm: 0.408548
=== Actor Training Debug (Iteration 4262) ===
Q mean: -12.325520
Q std: 14.789567
Actor loss: 12.329487
Action reg: 0.003967
  l1.weight: grad_norm = 0.177059
  l1.bias: grad_norm = 0.000589
  l2.weight: grad_norm = 0.171693
Total gradient norm: 0.528852
=== Actor Training Debug (Iteration 4263) ===
Q mean: -10.231825
Q std: 12.708327
Actor loss: 10.235800
Action reg: 0.003975
  l1.weight: grad_norm = 0.117548
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.101992
Total gradient norm: 0.288620
=== Actor Training Debug (Iteration 4264) ===
Q mean: -10.467729
Q std: 13.679515
Actor loss: 10.471697
Action reg: 0.003968
  l1.weight: grad_norm = 0.089186
  l1.bias: grad_norm = 0.000627
  l2.weight: grad_norm = 0.073239
Total gradient norm: 0.193715
=== Actor Training Debug (Iteration 4265) ===
Q mean: -10.191287
Q std: 14.362651
Actor loss: 10.195252
Action reg: 0.003965
  l1.weight: grad_norm = 0.166919
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.148513
Total gradient norm: 0.471429
=== Actor Training Debug (Iteration 4266) ===
Q mean: -10.326596
Q std: 14.270980
Actor loss: 10.330570
Action reg: 0.003974
  l1.weight: grad_norm = 0.151657
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.141126
Total gradient norm: 0.423559
=== Actor Training Debug (Iteration 4267) ===
Q mean: -11.528650
Q std: 14.786308
Actor loss: 11.532621
Action reg: 0.003971
  l1.weight: grad_norm = 0.101270
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.102468
Total gradient norm: 0.261899
=== Actor Training Debug (Iteration 4268) ===
Q mean: -11.159895
Q std: 13.195795
Actor loss: 11.163863
Action reg: 0.003968
  l1.weight: grad_norm = 0.227955
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.188259
Total gradient norm: 0.678172
=== Actor Training Debug (Iteration 4269) ===
Q mean: -11.151628
Q std: 14.246592
Actor loss: 11.155610
Action reg: 0.003982
  l1.weight: grad_norm = 0.141345
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.117251
Total gradient norm: 0.357140
=== Actor Training Debug (Iteration 4270) ===
Q mean: -11.360674
Q std: 14.368083
Actor loss: 11.364643
Action reg: 0.003969
  l1.weight: grad_norm = 0.195551
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.138824
Total gradient norm: 0.430047
=== Actor Training Debug (Iteration 4271) ===
Q mean: -11.151652
Q std: 13.767391
Actor loss: 11.155618
Action reg: 0.003966
  l1.weight: grad_norm = 0.137188
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.113288
Total gradient norm: 0.331615
=== Actor Training Debug (Iteration 4272) ===
Q mean: -10.196955
Q std: 14.073219
Actor loss: 10.200920
Action reg: 0.003965
  l1.weight: grad_norm = 0.132634
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.102635
Total gradient norm: 0.324165
=== Actor Training Debug (Iteration 4273) ===
Q mean: -10.644569
Q std: 12.330313
Actor loss: 10.648517
Action reg: 0.003948
  l1.weight: grad_norm = 0.217451
  l1.bias: grad_norm = 0.000561
  l2.weight: grad_norm = 0.169976
Total gradient norm: 0.483650
=== Actor Training Debug (Iteration 4274) ===
Q mean: -11.452732
Q std: 14.446388
Actor loss: 11.456692
Action reg: 0.003960
  l1.weight: grad_norm = 0.078132
  l1.bias: grad_norm = 0.000432
  l2.weight: grad_norm = 0.067911
Total gradient norm: 0.197573
=== Actor Training Debug (Iteration 4275) ===
Q mean: -10.726107
Q std: 13.655836
Actor loss: 10.730075
Action reg: 0.003968
  l1.weight: grad_norm = 0.113262
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.096026
Total gradient norm: 0.271733
=== Actor Training Debug (Iteration 4276) ===
Q mean: -11.395217
Q std: 14.172884
Actor loss: 11.399182
Action reg: 0.003965
  l1.weight: grad_norm = 0.124490
  l1.bias: grad_norm = 0.000899
  l2.weight: grad_norm = 0.104805
Total gradient norm: 0.370590
=== Actor Training Debug (Iteration 4277) ===
Q mean: -10.337851
Q std: 14.249360
Actor loss: 10.341824
Action reg: 0.003973
  l1.weight: grad_norm = 0.233788
  l1.bias: grad_norm = 0.000704
  l2.weight: grad_norm = 0.183311
Total gradient norm: 0.618588
=== Actor Training Debug (Iteration 4278) ===
Q mean: -9.801640
Q std: 12.787151
Actor loss: 9.805610
Action reg: 0.003970
  l1.weight: grad_norm = 0.095187
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.080963
Total gradient norm: 0.252988
=== Actor Training Debug (Iteration 4279) ===
Q mean: -11.946901
Q std: 14.417799
Actor loss: 11.950868
Action reg: 0.003967
  l1.weight: grad_norm = 0.059176
  l1.bias: grad_norm = 0.000809
  l2.weight: grad_norm = 0.049660
Total gradient norm: 0.156622
=== Actor Training Debug (Iteration 4280) ===
Q mean: -12.300066
Q std: 15.261785
Actor loss: 12.304034
Action reg: 0.003969
  l1.weight: grad_norm = 0.072911
  l1.bias: grad_norm = 0.001034
  l2.weight: grad_norm = 0.064586
Total gradient norm: 0.202388
=== Actor Training Debug (Iteration 4281) ===
Q mean: -10.458740
Q std: 13.176287
Actor loss: 10.462710
Action reg: 0.003970
  l1.weight: grad_norm = 0.202976
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.178068
Total gradient norm: 0.574388
=== Actor Training Debug (Iteration 4282) ===
Q mean: -10.864887
Q std: 14.584340
Actor loss: 10.868849
Action reg: 0.003961
  l1.weight: grad_norm = 0.178433
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.175531
Total gradient norm: 0.462190
=== Actor Training Debug (Iteration 4283) ===
Q mean: -11.569569
Q std: 14.207340
Actor loss: 11.573541
Action reg: 0.003972
  l1.weight: grad_norm = 0.143530
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.116770
Total gradient norm: 0.353642
=== Actor Training Debug (Iteration 4284) ===
Q mean: -11.668983
Q std: 14.452888
Actor loss: 11.672962
Action reg: 0.003979
  l1.weight: grad_norm = 0.167984
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.131059
Total gradient norm: 0.358539
=== Actor Training Debug (Iteration 4285) ===
Q mean: -11.409756
Q std: 13.124097
Actor loss: 11.413733
Action reg: 0.003977
  l1.weight: grad_norm = 0.088447
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.080947
Total gradient norm: 0.270153
=== Actor Training Debug (Iteration 4286) ===
Q mean: -10.985407
Q std: 13.438567
Actor loss: 10.989390
Action reg: 0.003983
  l1.weight: grad_norm = 0.079377
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.075828
Total gradient norm: 0.251530
=== Actor Training Debug (Iteration 4287) ===
Q mean: -12.395769
Q std: 14.829118
Actor loss: 12.399739
Action reg: 0.003970
  l1.weight: grad_norm = 0.081330
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.074701
Total gradient norm: 0.219938
=== Actor Training Debug (Iteration 4288) ===
Q mean: -10.668104
Q std: 13.775169
Actor loss: 10.672063
Action reg: 0.003959
  l1.weight: grad_norm = 0.110541
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.097243
Total gradient norm: 0.292414
=== Actor Training Debug (Iteration 4289) ===
Q mean: -9.974867
Q std: 12.415272
Actor loss: 9.978837
Action reg: 0.003970
  l1.weight: grad_norm = 0.189993
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.142097
Total gradient norm: 0.424332
=== Actor Training Debug (Iteration 4290) ===
Q mean: -11.637449
Q std: 14.717448
Actor loss: 11.641419
Action reg: 0.003971
  l1.weight: grad_norm = 0.223602
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.189228
Total gradient norm: 0.544639
=== Actor Training Debug (Iteration 4291) ===
Q mean: -11.134342
Q std: 14.003782
Actor loss: 11.138314
Action reg: 0.003972
  l1.weight: grad_norm = 0.185102
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.166008
Total gradient norm: 0.444952
=== Actor Training Debug (Iteration 4292) ===
Q mean: -11.079676
Q std: 13.293148
Actor loss: 11.083632
Action reg: 0.003956
  l1.weight: grad_norm = 0.135426
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.120206
Total gradient norm: 0.409904
=== Actor Training Debug (Iteration 4293) ===
Q mean: -10.494915
Q std: 11.695615
Actor loss: 10.498889
Action reg: 0.003973
  l1.weight: grad_norm = 0.094910
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.071549
Total gradient norm: 0.209710
=== Actor Training Debug (Iteration 4294) ===
Q mean: -10.453319
Q std: 14.034854
Actor loss: 10.457295
Action reg: 0.003977
  l1.weight: grad_norm = 0.117129
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.103255
Total gradient norm: 0.316769
=== Actor Training Debug (Iteration 4295) ===
Q mean: -10.444244
Q std: 12.506985
Actor loss: 10.448207
Action reg: 0.003962
  l1.weight: grad_norm = 0.127815
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.118908
Total gradient norm: 0.394101
=== Actor Training Debug (Iteration 4296) ===
Q mean: -11.766668
Q std: 14.820306
Actor loss: 11.770637
Action reg: 0.003969
  l1.weight: grad_norm = 0.122368
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.099816
Total gradient norm: 0.334732
=== Actor Training Debug (Iteration 4297) ===
Q mean: -9.768169
Q std: 12.828564
Actor loss: 9.772128
Action reg: 0.003959
  l1.weight: grad_norm = 0.102770
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.086801
Total gradient norm: 0.271820
=== Actor Training Debug (Iteration 4298) ===
Q mean: -12.178451
Q std: 13.920800
Actor loss: 12.182428
Action reg: 0.003978
  l1.weight: grad_norm = 0.121947
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.108406
Total gradient norm: 0.319266
=== Actor Training Debug (Iteration 4299) ===
Q mean: -10.424101
Q std: 14.074582
Actor loss: 10.428072
Action reg: 0.003971
  l1.weight: grad_norm = 0.079073
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.065065
Total gradient norm: 0.215812
=== Actor Training Debug (Iteration 4300) ===
Q mean: -10.348059
Q std: 13.692463
Actor loss: 10.352033
Action reg: 0.003974
  l1.weight: grad_norm = 0.383548
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.311217
Total gradient norm: 0.999845
=== Actor Training Debug (Iteration 4301) ===
Q mean: -11.880127
Q std: 14.151882
Actor loss: 11.884099
Action reg: 0.003972
  l1.weight: grad_norm = 0.119409
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.106180
Total gradient norm: 0.369153
=== Actor Training Debug (Iteration 4302) ===
Q mean: -10.576278
Q std: 13.192295
Actor loss: 10.580241
Action reg: 0.003963
  l1.weight: grad_norm = 0.118881
  l1.bias: grad_norm = 0.001312
  l2.weight: grad_norm = 0.116477
Total gradient norm: 0.309562
=== Actor Training Debug (Iteration 4303) ===
Q mean: -10.012798
Q std: 12.357281
Actor loss: 10.016765
Action reg: 0.003966
  l1.weight: grad_norm = 0.133545
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.096860
Total gradient norm: 0.265660
=== Actor Training Debug (Iteration 4304) ===
Q mean: -10.884899
Q std: 13.920473
Actor loss: 10.888874
Action reg: 0.003975
  l1.weight: grad_norm = 0.072365
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.059317
Total gradient norm: 0.212320
=== Actor Training Debug (Iteration 4305) ===
Q mean: -10.222672
Q std: 13.533970
Actor loss: 10.226645
Action reg: 0.003973
  l1.weight: grad_norm = 0.036640
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.033892
Total gradient norm: 0.098260
=== Actor Training Debug (Iteration 4306) ===
Q mean: -10.757914
Q std: 14.626815
Actor loss: 10.761886
Action reg: 0.003972
  l1.weight: grad_norm = 0.096404
  l1.bias: grad_norm = 0.000804
  l2.weight: grad_norm = 0.098827
Total gradient norm: 0.327478
=== Actor Training Debug (Iteration 4307) ===
Q mean: -10.267296
Q std: 12.965299
Actor loss: 10.271269
Action reg: 0.003973
  l1.weight: grad_norm = 0.127451
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.108433
Total gradient norm: 0.291359
=== Actor Training Debug (Iteration 4308) ===
Q mean: -10.943612
Q std: 13.468029
Actor loss: 10.947580
Action reg: 0.003968
  l1.weight: grad_norm = 0.033386
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.031189
Total gradient norm: 0.098063
=== Actor Training Debug (Iteration 4309) ===
Q mean: -10.178161
Q std: 14.479769
Actor loss: 10.182124
Action reg: 0.003963
  l1.weight: grad_norm = 0.116240
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.098798
Total gradient norm: 0.288160
=== Actor Training Debug (Iteration 4310) ===
Q mean: -10.100689
Q std: 13.048948
Actor loss: 10.104677
Action reg: 0.003988
  l1.weight: grad_norm = 0.085951
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.088270
Total gradient norm: 0.293766
=== Actor Training Debug (Iteration 4311) ===
Q mean: -9.231298
Q std: 12.819400
Actor loss: 9.235246
Action reg: 0.003947
  l1.weight: grad_norm = 0.108610
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.109832
Total gradient norm: 0.362460
=== Actor Training Debug (Iteration 4312) ===
Q mean: -8.822469
Q std: 12.754663
Actor loss: 8.826437
Action reg: 0.003969
  l1.weight: grad_norm = 0.131763
  l1.bias: grad_norm = 0.000558
  l2.weight: grad_norm = 0.114810
Total gradient norm: 0.303429
=== Actor Training Debug (Iteration 4313) ===
Q mean: -10.959616
Q std: 14.076929
Actor loss: 10.963580
Action reg: 0.003965
  l1.weight: grad_norm = 0.112886
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.095092
Total gradient norm: 0.318117
=== Actor Training Debug (Iteration 4314) ===
Q mean: -11.608019
Q std: 13.635277
Actor loss: 11.611990
Action reg: 0.003971
  l1.weight: grad_norm = 0.082141
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.070824
Total gradient norm: 0.250309
=== Actor Training Debug (Iteration 4315) ===
Q mean: -9.455751
Q std: 13.095797
Actor loss: 9.459726
Action reg: 0.003975
  l1.weight: grad_norm = 0.120389
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.102307
Total gradient norm: 0.263301
=== Actor Training Debug (Iteration 4316) ===
Q mean: -10.858012
Q std: 13.928580
Actor loss: 10.861954
Action reg: 0.003942
  l1.weight: grad_norm = 0.114114
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.102490
Total gradient norm: 0.324624
=== Actor Training Debug (Iteration 4317) ===
Q mean: -9.332958
Q std: 13.077487
Actor loss: 9.336939
Action reg: 0.003981
  l1.weight: grad_norm = 0.063439
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.059016
Total gradient norm: 0.172707
=== Actor Training Debug (Iteration 4318) ===
Q mean: -12.282810
Q std: 14.361176
Actor loss: 12.286796
Action reg: 0.003985
  l1.weight: grad_norm = 0.035928
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.035238
Total gradient norm: 0.122022
=== Actor Training Debug (Iteration 4319) ===
Q mean: -11.307592
Q std: 14.726040
Actor loss: 11.311558
Action reg: 0.003965
  l1.weight: grad_norm = 0.336454
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.237309
Total gradient norm: 0.679874
=== Actor Training Debug (Iteration 4320) ===
Q mean: -12.243538
Q std: 15.590910
Actor loss: 12.247519
Action reg: 0.003982
  l1.weight: grad_norm = 0.042790
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.037508
Total gradient norm: 0.120285
=== Actor Training Debug (Iteration 4321) ===
Q mean: -11.219021
Q std: 13.703779
Actor loss: 11.223006
Action reg: 0.003985
  l1.weight: grad_norm = 0.093072
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.097703
Total gradient norm: 0.348967
=== Actor Training Debug (Iteration 4322) ===
Q mean: -11.654161
Q std: 15.072295
Actor loss: 11.658139
Action reg: 0.003978
  l1.weight: grad_norm = 0.129204
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.109461
Total gradient norm: 0.329190
=== Actor Training Debug (Iteration 4323) ===
Q mean: -10.463776
Q std: 13.945464
Actor loss: 10.467750
Action reg: 0.003974
  l1.weight: grad_norm = 0.158772
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.152123
Total gradient norm: 0.467751
=== Actor Training Debug (Iteration 4324) ===
Q mean: -10.375089
Q std: 14.206209
Actor loss: 10.379059
Action reg: 0.003970
  l1.weight: grad_norm = 0.134446
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.099326
Total gradient norm: 0.277085
=== Actor Training Debug (Iteration 4325) ===
Q mean: -9.909239
Q std: 12.528419
Actor loss: 9.913205
Action reg: 0.003966
  l1.weight: grad_norm = 0.131504
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.115385
Total gradient norm: 0.380313
=== Actor Training Debug (Iteration 4326) ===
Q mean: -11.090091
Q std: 13.458184
Actor loss: 11.094047
Action reg: 0.003956
  l1.weight: grad_norm = 0.174593
  l1.bias: grad_norm = 0.000783
  l2.weight: grad_norm = 0.161952
Total gradient norm: 0.508935
=== Actor Training Debug (Iteration 4327) ===
Q mean: -11.211493
Q std: 13.150609
Actor loss: 11.215476
Action reg: 0.003983
  l1.weight: grad_norm = 0.156416
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.126827
Total gradient norm: 0.375334
=== Actor Training Debug (Iteration 4328) ===
Q mean: -9.462394
Q std: 13.803693
Actor loss: 9.466364
Action reg: 0.003970
  l1.weight: grad_norm = 0.084924
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.076757
Total gradient norm: 0.223719
=== Actor Training Debug (Iteration 4329) ===
Q mean: -9.868568
Q std: 13.292664
Actor loss: 9.872538
Action reg: 0.003969
  l1.weight: grad_norm = 0.120888
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.105387
Total gradient norm: 0.307284
=== Actor Training Debug (Iteration 4330) ===
Q mean: -11.297819
Q std: 15.288034
Actor loss: 11.301796
Action reg: 0.003977
  l1.weight: grad_norm = 0.328729
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.247856
Total gradient norm: 0.683901
=== Actor Training Debug (Iteration 4331) ===
Q mean: -9.911606
Q std: 12.444809
Actor loss: 9.915594
Action reg: 0.003988
  l1.weight: grad_norm = 0.065108
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.055807
Total gradient norm: 0.172239
=== Actor Training Debug (Iteration 4332) ===
Q mean: -10.367134
Q std: 14.389567
Actor loss: 10.371104
Action reg: 0.003970
  l1.weight: grad_norm = 0.116674
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.101408
Total gradient norm: 0.315939
=== Actor Training Debug (Iteration 4333) ===
Q mean: -10.087223
Q std: 14.186093
Actor loss: 10.091192
Action reg: 0.003970
  l1.weight: grad_norm = 0.110828
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.096810
Total gradient norm: 0.317757
=== Actor Training Debug (Iteration 4334) ===
Q mean: -10.378708
Q std: 12.541982
Actor loss: 10.382676
Action reg: 0.003969
  l1.weight: grad_norm = 0.106679
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.100143
Total gradient norm: 0.360352
=== Actor Training Debug (Iteration 4335) ===
Q mean: -11.768622
Q std: 13.766597
Actor loss: 11.772583
Action reg: 0.003961
  l1.weight: grad_norm = 0.163672
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.132000
Total gradient norm: 0.515549
=== Actor Training Debug (Iteration 4336) ===
Q mean: -11.232134
Q std: 13.211821
Actor loss: 11.236109
Action reg: 0.003974
  l1.weight: grad_norm = 0.119294
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.111501
Total gradient norm: 0.317598
=== Actor Training Debug (Iteration 4337) ===
Q mean: -11.012522
Q std: 13.416925
Actor loss: 11.016495
Action reg: 0.003973
  l1.weight: grad_norm = 0.128471
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.120144
Total gradient norm: 0.375784
=== Actor Training Debug (Iteration 4338) ===
Q mean: -10.257563
Q std: 14.800878
Actor loss: 10.261525
Action reg: 0.003962
  l1.weight: grad_norm = 0.351220
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.261673
Total gradient norm: 0.737236
=== Actor Training Debug (Iteration 4339) ===
Q mean: -10.299421
Q std: 14.879864
Actor loss: 10.303398
Action reg: 0.003977
  l1.weight: grad_norm = 0.099053
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.075326
Total gradient norm: 0.238504
=== Actor Training Debug (Iteration 4340) ===
Q mean: -12.067159
Q std: 15.158375
Actor loss: 12.071148
Action reg: 0.003989
  l1.weight: grad_norm = 0.050829
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.048157
Total gradient norm: 0.149087
=== Actor Training Debug (Iteration 4341) ===
Q mean: -10.639891
Q std: 14.011494
Actor loss: 10.643865
Action reg: 0.003974
  l1.weight: grad_norm = 0.151716
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.127316
Total gradient norm: 0.431875
=== Actor Training Debug (Iteration 4342) ===
Q mean: -10.734669
Q std: 13.326218
Actor loss: 10.738642
Action reg: 0.003973
  l1.weight: grad_norm = 0.188774
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.153776
Total gradient norm: 0.479118
=== Actor Training Debug (Iteration 4343) ===
Q mean: -10.808566
Q std: 14.886216
Actor loss: 10.812535
Action reg: 0.003969
  l1.weight: grad_norm = 0.108672
  l1.bias: grad_norm = 0.000784
  l2.weight: grad_norm = 0.094235
Total gradient norm: 0.317632
=== Actor Training Debug (Iteration 4344) ===
Q mean: -10.620943
Q std: 14.623877
Actor loss: 10.624922
Action reg: 0.003979
  l1.weight: grad_norm = 0.143509
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.113709
Total gradient norm: 0.335481
=== Actor Training Debug (Iteration 4345) ===
Q mean: -11.195648
Q std: 14.136383
Actor loss: 11.199618
Action reg: 0.003970
  l1.weight: grad_norm = 0.118654
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.102444
Total gradient norm: 0.320836
=== Actor Training Debug (Iteration 4346) ===
Q mean: -10.977528
Q std: 15.138970
Actor loss: 10.981484
Action reg: 0.003957
  l1.weight: grad_norm = 0.180737
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.162760
Total gradient norm: 0.498031
=== Actor Training Debug (Iteration 4347) ===
Q mean: -9.725638
Q std: 12.862717
Actor loss: 9.729611
Action reg: 0.003973
  l1.weight: grad_norm = 0.088711
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.083199
Total gradient norm: 0.235747
=== Actor Training Debug (Iteration 4348) ===
Q mean: -10.685352
Q std: 15.427994
Actor loss: 10.689331
Action reg: 0.003979
  l1.weight: grad_norm = 0.137325
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.111757
Total gradient norm: 0.327413
=== Actor Training Debug (Iteration 4349) ===
Q mean: -10.844371
Q std: 15.505699
Actor loss: 10.848341
Action reg: 0.003970
  l1.weight: grad_norm = 0.091698
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.083973
Total gradient norm: 0.267275
=== Actor Training Debug (Iteration 4350) ===
Q mean: -11.514555
Q std: 14.507492
Actor loss: 11.518526
Action reg: 0.003971
  l1.weight: grad_norm = 0.168443
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.137759
Total gradient norm: 0.382524
=== Actor Training Debug (Iteration 4351) ===
Q mean: -11.194046
Q std: 12.646197
Actor loss: 11.198018
Action reg: 0.003972
  l1.weight: grad_norm = 0.122660
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.108719
Total gradient norm: 0.322427
=== Actor Training Debug (Iteration 4352) ===
Q mean: -12.445249
Q std: 14.531109
Actor loss: 12.449231
Action reg: 0.003982
  l1.weight: grad_norm = 0.111636
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.090746
Total gradient norm: 0.274253
=== Actor Training Debug (Iteration 4353) ===
Q mean: -11.700349
Q std: 14.911954
Actor loss: 11.704334
Action reg: 0.003985
  l1.weight: grad_norm = 0.073642
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.062013
Total gradient norm: 0.190318
=== Actor Training Debug (Iteration 4354) ===
Q mean: -9.574750
Q std: 12.236683
Actor loss: 9.578733
Action reg: 0.003983
  l1.weight: grad_norm = 0.194752
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.151110
Total gradient norm: 0.498282
=== Actor Training Debug (Iteration 4355) ===
Q mean: -8.990877
Q std: 13.782183
Actor loss: 8.994847
Action reg: 0.003971
  l1.weight: grad_norm = 0.164011
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.129898
Total gradient norm: 0.393921
=== Actor Training Debug (Iteration 4356) ===
Q mean: -10.864155
Q std: 14.673427
Actor loss: 10.868129
Action reg: 0.003974
  l1.weight: grad_norm = 0.185292
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.144925
Total gradient norm: 0.459107
=== Actor Training Debug (Iteration 4357) ===
Q mean: -10.091148
Q std: 13.159221
Actor loss: 10.095117
Action reg: 0.003968
  l1.weight: grad_norm = 0.116398
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.093441
Total gradient norm: 0.259744
=== Actor Training Debug (Iteration 4358) ===
Q mean: -10.759815
Q std: 13.432887
Actor loss: 10.763785
Action reg: 0.003971
  l1.weight: grad_norm = 0.111251
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.081471
Total gradient norm: 0.243478
=== Actor Training Debug (Iteration 4359) ===
Q mean: -10.022234
Q std: 13.261837
Actor loss: 10.026194
Action reg: 0.003959
  l1.weight: grad_norm = 0.144325
  l1.bias: grad_norm = 0.000634
  l2.weight: grad_norm = 0.132318
Total gradient norm: 0.424688
=== Actor Training Debug (Iteration 4360) ===
Q mean: -10.693067
Q std: 13.704172
Actor loss: 10.697025
Action reg: 0.003959
  l1.weight: grad_norm = 0.071238
  l1.bias: grad_norm = 0.000964
  l2.weight: grad_norm = 0.059437
Total gradient norm: 0.196080
=== Actor Training Debug (Iteration 4361) ===
Q mean: -10.416376
Q std: 15.161335
Actor loss: 10.420352
Action reg: 0.003976
  l1.weight: grad_norm = 0.068267
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.059768
Total gradient norm: 0.204686
=== Actor Training Debug (Iteration 4362) ===
Q mean: -9.500920
Q std: 12.819479
Actor loss: 9.504896
Action reg: 0.003975
  l1.weight: grad_norm = 0.126985
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.109254
Total gradient norm: 0.326450
=== Actor Training Debug (Iteration 4363) ===
Q mean: -10.797815
Q std: 15.111620
Actor loss: 10.801784
Action reg: 0.003968
  l1.weight: grad_norm = 0.205612
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.198288
Total gradient norm: 0.520592
=== Actor Training Debug (Iteration 4364) ===
Q mean: -10.172161
Q std: 13.978190
Actor loss: 10.176126
Action reg: 0.003965
  l1.weight: grad_norm = 0.129439
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.103985
Total gradient norm: 0.319978
=== Actor Training Debug (Iteration 4365) ===
Q mean: -10.926174
Q std: 13.335494
Actor loss: 10.930134
Action reg: 0.003959
  l1.weight: grad_norm = 0.140707
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.115813
Total gradient norm: 0.334727
=== Actor Training Debug (Iteration 4366) ===
Q mean: -10.188524
Q std: 13.285666
Actor loss: 10.192492
Action reg: 0.003967
  l1.weight: grad_norm = 0.133490
  l1.bias: grad_norm = 0.000635
  l2.weight: grad_norm = 0.125455
Total gradient norm: 0.418946
=== Actor Training Debug (Iteration 4367) ===
Q mean: -10.096100
Q std: 14.226308
Actor loss: 10.100069
Action reg: 0.003969
  l1.weight: grad_norm = 0.138091
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.110896
Total gradient norm: 0.362162
=== Actor Training Debug (Iteration 4368) ===
Q mean: -10.798250
Q std: 15.708185
Actor loss: 10.802216
Action reg: 0.003966
  l1.weight: grad_norm = 0.190266
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.150300
Total gradient norm: 0.514706
Actor loss: 12.295699
Action reg: 0.003974
  l1.weight: grad_norm = 0.092688
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.086400
Total gradient norm: 0.263629
=== Actor Training Debug (Iteration 4386) ===
Q mean: -10.386519
Q std: 13.854671
Actor loss: 10.390505
Action reg: 0.003985
  l1.weight: grad_norm = 0.084597
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.071819
Total gradient norm: 0.214417
=== Actor Training Debug (Iteration 4387) ===
Q mean: -10.516811
Q std: 14.726258
Actor loss: 10.520788
Action reg: 0.003977
  l1.weight: grad_norm = 0.146750
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.129029
Total gradient norm: 0.349642
=== Actor Training Debug (Iteration 4388) ===
Q mean: -11.787032
Q std: 14.702220
Actor loss: 11.791002
Action reg: 0.003970
  l1.weight: grad_norm = 0.102903
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.093359
Total gradient norm: 0.294513
=== Actor Training Debug (Iteration 4389) ===
Q mean: -10.286686
Q std: 13.114932
Actor loss: 10.290650
Action reg: 0.003964
  l1.weight: grad_norm = 0.169989
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.150524
Total gradient norm: 0.484261
=== Actor Training Debug (Iteration 4390) ===
Q mean: -10.988407
Q std: 14.547639
Actor loss: 10.992369
Action reg: 0.003961
  l1.weight: grad_norm = 0.082826
  l1.bias: grad_norm = 0.000940
  l2.weight: grad_norm = 0.073337
Total gradient norm: 0.243142
=== Actor Training Debug (Iteration 4391) ===
Q mean: -10.971524
Q std: 14.996027
Actor loss: 10.975493
Action reg: 0.003969
  l1.weight: grad_norm = 0.132088
  l1.bias: grad_norm = 0.000571
  l2.weight: grad_norm = 0.105553
Total gradient norm: 0.335865
=== Actor Training Debug (Iteration 4392) ===
Q mean: -10.337504
Q std: 14.945872
Actor loss: 10.341483
Action reg: 0.003979
  l1.weight: grad_norm = 0.070288
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.061802
Total gradient norm: 0.215212
=== Actor Training Debug (Iteration 4393) ===
Q mean: -9.412409
Q std: 13.158248
Actor loss: 9.416389
Action reg: 0.003980
  l1.weight: grad_norm = 0.058138
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.052195
Total gradient norm: 0.160842
=== Actor Training Debug (Iteration 4394) ===
Q mean: -12.893526
Q std: 16.021725
Actor loss: 12.897501
Action reg: 0.003974
  l1.weight: grad_norm = 0.081093
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.076924
Total gradient norm: 0.217278
=== Actor Training Debug (Iteration 4395) ===
Q mean: -12.002916
Q std: 14.720089
Actor loss: 12.006886
Action reg: 0.003969
  l1.weight: grad_norm = 0.113780
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.103511
Total gradient norm: 0.298576
=== Actor Training Debug (Iteration 4396) ===
Q mean: -10.194646
Q std: 14.431766
Actor loss: 10.198606
Action reg: 0.003959
  l1.weight: grad_norm = 0.173844
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.149150
Total gradient norm: 0.413623
=== Actor Training Debug (Iteration 4397) ===
Q mean: -10.867040
Q std: 14.700795
Actor loss: 10.871008
Action reg: 0.003968
  l1.weight: grad_norm = 0.151885
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.122129
Total gradient norm: 0.317155
=== Actor Training Debug (Iteration 4398) ===
Q mean: -9.886539
Q std: 12.684172
Actor loss: 9.890503
Action reg: 0.003963
  l1.weight: grad_norm = 0.169119
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.146813
Total gradient norm: 0.417456
=== Actor Training Debug (Iteration 4399) ===
Q mean: -10.794806
Q std: 15.321835
Actor loss: 10.798779
Action reg: 0.003973
  l1.weight: grad_norm = 0.107351
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.099744
Total gradient norm: 0.283588
=== Actor Training Debug (Iteration 4400) ===
Q mean: -10.327682
Q std: 13.605238
Actor loss: 10.331647
Action reg: 0.003964
  l1.weight: grad_norm = 0.154535
  l1.bias: grad_norm = 0.000707
  l2.weight: grad_norm = 0.124833
Total gradient norm: 0.340088
=== Actor Training Debug (Iteration 4401) ===
Q mean: -11.194410
Q std: 13.788010
Actor loss: 11.198384
Action reg: 0.003974
  l1.weight: grad_norm = 0.164283
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.142463
Total gradient norm: 0.440406
=== Actor Training Debug (Iteration 4402) ===
Q mean: -11.671785
Q std: 15.154564
Actor loss: 11.675755
Action reg: 0.003969
  l1.weight: grad_norm = 0.151280
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.139194
Total gradient norm: 0.396664
=== Actor Training Debug (Iteration 4403) ===
Q mean: -10.513474
Q std: 13.834518
Actor loss: 10.517434
Action reg: 0.003960
  l1.weight: grad_norm = 0.386014
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.355079
Total gradient norm: 1.090431
=== Actor Training Debug (Iteration 4404) ===
Q mean: -8.964073
Q std: 13.216887
Actor loss: 8.968056
Action reg: 0.003982
  l1.weight: grad_norm = 0.071232
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.060001
Total gradient norm: 0.186514
=== Actor Training Debug (Iteration 4405) ===
Q mean: -9.599636
Q std: 13.929649
Actor loss: 9.603600
Action reg: 0.003964
  l1.weight: grad_norm = 0.157828
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.132241
Total gradient norm: 0.402391
=== Actor Training Debug (Iteration 4406) ===
Q mean: -9.588337
Q std: 14.276658
Actor loss: 9.592311
Action reg: 0.003973
  l1.weight: grad_norm = 0.116790
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.090481
Total gradient norm: 0.263867
=== Actor Training Debug (Iteration 4407) ===
Q mean: -10.506363
Q std: 15.169403
Actor loss: 10.510345
Action reg: 0.003982
  l1.weight: grad_norm = 0.092564
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.078931
Total gradient norm: 0.244348
=== Actor Training Debug (Iteration 4408) ===
Q mean: -10.208769
Q std: 14.288776
Actor loss: 10.212740
Action reg: 0.003971
  l1.weight: grad_norm = 0.134291
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.122493
Total gradient norm: 0.379997
=== Actor Training Debug (Iteration 4409) ===
Q mean: -10.857599
Q std: 12.769916
Actor loss: 10.861578
Action reg: 0.003978
  l1.weight: grad_norm = 0.082228
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.075504
Total gradient norm: 0.245648
=== Actor Training Debug (Iteration 4410) ===
Q mean: -10.018249
Q std: 12.882028
Actor loss: 10.022215
Action reg: 0.003967
  l1.weight: grad_norm = 0.131051
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.115444
Total gradient norm: 0.344136
=== Actor Training Debug (Iteration 4411) ===
Q mean: -9.906662
Q std: 14.118003
Actor loss: 9.910645
Action reg: 0.003983
  l1.weight: grad_norm = 0.147831
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.110806
Total gradient norm: 0.380179
=== Actor Training Debug (Iteration 4412) ===
Q mean: -11.138133
Q std: 14.825806
Actor loss: 11.142106
Action reg: 0.003973
  l1.weight: grad_norm = 0.103751
  l1.bias: grad_norm = 0.000761
  l2.weight: grad_norm = 0.074773
Total gradient norm: 0.247417
=== Actor Training Debug (Iteration 4413) ===
Q mean: -12.067257
Q std: 15.007300
Actor loss: 12.071219
Action reg: 0.003963
  l1.weight: grad_norm = 0.090938
  l1.bias: grad_norm = 0.001077
  l2.weight: grad_norm = 0.076512
Total gradient norm: 0.243681
=== Actor Training Debug (Iteration 4414) ===
Q mean: -12.527206
Q std: 15.682693
Actor loss: 12.531167
Action reg: 0.003961
  l1.weight: grad_norm = 0.184761
  l1.bias: grad_norm = 0.000432
  l2.weight: grad_norm = 0.153752
Total gradient norm: 0.465805
=== Actor Training Debug (Iteration 4415) ===
Q mean: -9.941677
Q std: 14.204249
Actor loss: 9.945654
Action reg: 0.003977
  l1.weight: grad_norm = 0.168943
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.177598
Total gradient norm: 0.614136
=== Actor Training Debug (Iteration 4416) ===
Q mean: -11.256025
Q std: 15.059236
Actor loss: 11.260003
Action reg: 0.003978
  l1.weight: grad_norm = 0.165751
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.183844
Total gradient norm: 0.641691
=== Actor Training Debug (Iteration 4417) ===
Q mean: -10.479408
Q std: 14.695735
Actor loss: 10.483381
Action reg: 0.003973
  l1.weight: grad_norm = 0.171835
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.160431
Total gradient norm: 0.498351
=== Actor Training Debug (Iteration 4418) ===
Q mean: -9.676001
Q std: 13.152325
Actor loss: 9.679973
Action reg: 0.003972
  l1.weight: grad_norm = 0.148577
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.142688
Total gradient norm: 0.453629
=== Actor Training Debug (Iteration 4419) ===
Q mean: -9.416306
Q std: 14.312894
Actor loss: 9.420259
Action reg: 0.003953
  l1.weight: grad_norm = 0.171289
  l1.bias: grad_norm = 0.000694
  l2.weight: grad_norm = 0.148378
Total gradient norm: 0.408023
=== Actor Training Debug (Iteration 4420) ===
Q mean: -11.390776
Q std: 14.704917
Actor loss: 11.394740
Action reg: 0.003964
  l1.weight: grad_norm = 0.137460
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.124018
Total gradient norm: 0.408969
=== Actor Training Debug (Iteration 4421) ===
Q mean: -11.142558
Q std: 14.272232
Actor loss: 11.146534
Action reg: 0.003976
  l1.weight: grad_norm = 0.266880
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.205966
Total gradient norm: 0.592143
=== Actor Training Debug (Iteration 4422) ===
Q mean: -10.013323
Q std: 12.823711
Actor loss: 10.017298
Action reg: 0.003975
  l1.weight: grad_norm = 0.099402
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.087243
Total gradient norm: 0.274139
=== Actor Training Debug (Iteration 4423) ===
Q mean: -10.576408
Q std: 14.452666
Actor loss: 10.580374
Action reg: 0.003966
  l1.weight: grad_norm = 0.065445
  l1.bias: grad_norm = 0.000708
  l2.weight: grad_norm = 0.059837
Total gradient norm: 0.207179
=== Actor Training Debug (Iteration 4424) ===
Q mean: -10.297812
Q std: 14.986788
Actor loss: 10.301786
Action reg: 0.003975
  l1.weight: grad_norm = 0.078931
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.064415
Total gradient norm: 0.217627
=== Actor Training Debug (Iteration 4425) ===
Q mean: -12.181995
Q std: 17.185764
Actor loss: 12.185960
Action reg: 0.003964
  l1.weight: grad_norm = 0.089164
  l1.bias: grad_norm = 0.000555
  l2.weight: grad_norm = 0.084122
Total gradient norm: 0.255765
=== Actor Training Debug (Iteration 4426) ===
Q mean: -11.735737
Q std: 14.343616
Actor loss: 11.739702
Action reg: 0.003966
  l1.weight: grad_norm = 0.108177
  l1.bias: grad_norm = 0.000470
  l2.weight: grad_norm = 0.107393
Total gradient norm: 0.325594
=== Actor Training Debug (Iteration 4427) ===
Q mean: -10.955210
Q std: 12.492681
Actor loss: 10.959191
Action reg: 0.003982
  l1.weight: grad_norm = 0.083199
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.080389
Total gradient norm: 0.252130
=== Actor Training Debug (Iteration 4428) ===
Q mean: -10.411458
Q std: 13.389180
Actor loss: 10.415434
Action reg: 0.003976
  l1.weight: grad_norm = 0.116077
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.096447
Total gradient norm: 0.284921
=== Actor Training Debug (Iteration 4429) ===
Q mean: -10.501933
Q std: 14.527390
Actor loss: 10.505895
Action reg: 0.003961
  l1.weight: grad_norm = 0.230426
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.216792
Total gradient norm: 0.570308
=== Actor Training Debug (Iteration 4430) ===
Q mean: -9.848473
Q std: 14.267490
Actor loss: 9.852449
Action reg: 0.003977
  l1.weight: grad_norm = 0.107451
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.093029
Total gradient norm: 0.318502
=== Actor Training Debug (Iteration 4431) ===
Q mean: -10.097851
Q std: 14.420311
Actor loss: 10.101834
Action reg: 0.003983
  l1.weight: grad_norm = 0.105128
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.102214
Total gradient norm: 0.312860
=== Actor Training Debug (Iteration 4432) ===
Q mean: -12.956513
Q std: 15.823133
Actor loss: 12.960487
Action reg: 0.003974
  l1.weight: grad_norm = 0.062774
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.054290
Total gradient norm: 0.171050
=== Actor Training Debug (Iteration 4433) ===
Q mean: -12.014944
Q std: 14.921206
Actor loss: 12.018909
Action reg: 0.003966
  l1.weight: grad_norm = 0.125202
  l1.bias: grad_norm = 0.001281
  l2.weight: grad_norm = 0.109806
Total gradient norm: 0.321448
=== Actor Training Debug (Iteration 4434) ===
Q mean: -12.313707
Q std: 16.791908
Actor loss: 12.317691
Action reg: 0.003984
  l1.weight: grad_norm = 0.062695
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.053011
Total gradient norm: 0.153228
=== Actor Training Debug (Iteration 4435) ===
Q mean: -9.062036
Q std: 13.619326
Actor loss: 9.066018
Action reg: 0.003983
  l1.weight: grad_norm = 0.168397
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.145266
Total gradient norm: 0.407028
=== Actor Training Debug (Iteration 4436) ===
Q mean: -9.819349
Q std: 14.698892
Actor loss: 9.823318
Action reg: 0.003968
  l1.weight: grad_norm = 0.194585
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.160009
Total gradient norm: 0.438164
=== Actor Training Debug (Iteration 4437) ===
Q mean: -11.319996
Q std: 14.150298
Actor loss: 11.323958
Action reg: 0.003962
  l1.weight: grad_norm = 0.052767
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.048112
Total gradient norm: 0.142788
=== Actor Training Debug (Iteration 4438) ===
Q mean: -10.490286
Q std: 14.436781
Actor loss: 10.494261
Action reg: 0.003975
  l1.weight: grad_norm = 0.376480
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.276387
Total gradient norm: 0.858673
=== Actor Training Debug (Iteration 4439) ===
Q mean: -9.761865
Q std: 13.225229
Actor loss: 9.765829
Action reg: 0.003964
  l1.weight: grad_norm = 0.104252
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.097339
Total gradient norm: 0.321171
=== Actor Training Debug (Iteration 4440) ===
Q mean: -9.831654
Q std: 13.116349
Actor loss: 9.835629
Action reg: 0.003976
  l1.weight: grad_norm = 0.078114
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.070147
Total gradient norm: 0.225611
=== Actor Training Debug (Iteration 4441) ===
Q mean: -12.501417
Q std: 14.831106
Actor loss: 12.505376
Action reg: 0.003959
  l1.weight: grad_norm = 0.162642
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.132365
Total gradient norm: 0.386898
=== Actor Training Debug (Iteration 4442) ===
Q mean: -10.762558
Q std: 14.150855
Actor loss: 10.766526
Action reg: 0.003968
  l1.weight: grad_norm = 0.123256
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.101937
Total gradient norm: 0.296167
=== Actor Training Debug (Iteration 4443) ===
Q mean: -11.310520
Q std: 14.307025
Actor loss: 11.314480
Action reg: 0.003960
  l1.weight: grad_norm = 0.105243
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.090840
Total gradient norm: 0.283603
=== Actor Training Debug (Iteration 4444) ===
Q mean: -9.714535
Q std: 14.000561
Actor loss: 9.718495
Action reg: 0.003960
  l1.weight: grad_norm = 0.185143
  l1.bias: grad_norm = 0.000623
  l2.weight: grad_norm = 0.140371
Total gradient norm: 0.454987
=== Actor Training Debug (Iteration 4445) ===
Q mean: -9.657892
Q std: 13.875128
Actor loss: 9.661845
Action reg: 0.003953
  l1.weight: grad_norm = 0.161786
  l1.bias: grad_norm = 0.000677
  l2.weight: grad_norm = 0.144213
Total gradient norm: 0.521243
=== Actor Training Debug (Iteration 4446) ===
Q mean: -9.658340
Q std: 13.708810
Actor loss: 9.662303
Action reg: 0.003963
  l1.weight: grad_norm = 0.204014
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.188129
Total gradient norm: 0.568460
=== Actor Training Debug (Iteration 4447) ===
Q mean: -9.935936
Q std: 13.435207
Actor loss: 9.939895
Action reg: 0.003959
  l1.weight: grad_norm = 0.122348
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.115503
Total gradient norm: 0.353651
=== Actor Training Debug (Iteration 4448) ===
Q mean: -9.737256
Q std: 13.796158
Actor loss: 9.741227
Action reg: 0.003971
  l1.weight: grad_norm = 0.157376
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.125726
Total gradient norm: 0.390474
=== Actor Training Debug (Iteration 4449) ===
Q mean: -9.999482
Q std: 13.982255
Actor loss: 10.003447
Action reg: 0.003964
  l1.weight: grad_norm = 0.210080
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.160459
Total gradient norm: 0.491530
=== Actor Training Debug (Iteration 4450) ===
Q mean: -11.112186
Q std: 16.003971
Actor loss: 11.116158
Action reg: 0.003972
  l1.weight: grad_norm = 0.115203
  l1.bias: grad_norm = 0.000049
  l2.weight: grad_norm = 0.100899
Total gradient norm: 0.298449
=== Actor Training Debug (Iteration 4451) ===
Q mean: -9.307397
Q std: 12.094762
Actor loss: 9.311365
Action reg: 0.003968
  l1.weight: grad_norm = 0.138394
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.131113
Total gradient norm: 0.389299
=== Actor Training Debug (Iteration 4452) ===
Q mean: -11.144520
Q std: 15.208616
Actor loss: 11.148483
Action reg: 0.003963
  l1.weight: grad_norm = 0.110444
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.085729
Total gradient norm: 0.265296
=== Actor Training Debug (Iteration 4453) ===
Q mean: -12.462453
Q std: 15.685520
Actor loss: 12.466414
Action reg: 0.003962
  l1.weight: grad_norm = 0.307211
  l1.bias: grad_norm = 0.000576
  l2.weight: grad_norm = 0.281932
Total gradient norm: 0.932060
=== Actor Training Debug (Iteration 4454) ===
Q mean: -9.650588
Q std: 13.667002
Actor loss: 9.654558
Action reg: 0.003970
  l1.weight: grad_norm = 0.095488
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.090773
Total gradient norm: 0.278422
=== Actor Training Debug (Iteration 4455) ===
Q mean: -9.966879
Q std: 12.598065
Actor loss: 9.970853
Action reg: 0.003974
  l1.weight: grad_norm = 0.085786
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.067602
Total gradient norm: 0.200134
=== Actor Training Debug (Iteration 4456) ===
Q mean: -10.863781
Q std: 14.404883
Actor loss: 10.867742
Action reg: 0.003961
  l1.weight: grad_norm = 0.103958
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.107574
Total gradient norm: 0.349079
=== Actor Training Debug (Iteration 4457) ===
Q mean: -8.146763
Q std: 13.055899
Actor loss: 8.150723
Action reg: 0.003959
  l1.weight: grad_norm = 0.136537
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.126962
Total gradient norm: 0.358639
=== Actor Training Debug (Iteration 4458) ===
Q mean: -9.433640
Q std: 13.552844
Actor loss: 9.437597
Action reg: 0.003958
  l1.weight: grad_norm = 0.228870
  l1.bias: grad_norm = 0.000533
  l2.weight: grad_norm = 0.177901
Total gradient norm: 0.616262
=== Actor Training Debug (Iteration 4459) ===
Q mean: -10.434916
Q std: 14.545980
Actor loss: 10.438890
Action reg: 0.003974
  l1.weight: grad_norm = 0.052433
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.047812
Total gradient norm: 0.154522
=== Actor Training Debug (Iteration 4460) ===
Q mean: -9.806280
Q std: 14.154164
Actor loss: 9.810232
Action reg: 0.003952
  l1.weight: grad_norm = 0.093680
  l1.bias: grad_norm = 0.000865
  l2.weight: grad_norm = 0.092361
Total gradient norm: 0.262653
=== Actor Training Debug (Iteration 4461) ===
Q mean: -10.230318
Q std: 14.055980
Actor loss: 10.234294
Action reg: 0.003976
  l1.weight: grad_norm = 0.102345
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.078700
Total gradient norm: 0.226790
=== Actor Training Debug (Iteration 4462) ===
Q mean: -10.319756
Q std: 13.830874
Actor loss: 10.323715
Action reg: 0.003960
  l1.weight: grad_norm = 0.145860
  l1.bias: grad_norm = 0.001694
  l2.weight: grad_norm = 0.129304
Total gradient norm: 0.355380
=== Actor Training Debug (Iteration 4463) ===
Q mean: -11.131252
Q std: 13.878353
Actor loss: 11.135228
Action reg: 0.003976
  l1.weight: grad_norm = 0.121558
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.111618
Total gradient norm: 0.308159
=== Actor Training Debug (Iteration 4464) ===
Q mean: -11.834525
Q std: 14.890275
Actor loss: 11.838495
Action reg: 0.003970
  l1.weight: grad_norm = 0.123365
  l1.bias: grad_norm = 0.000677
  l2.weight: grad_norm = 0.115533
Total gradient norm: 0.371346
=== Actor Training Debug (Iteration 4465) ===
Q mean: -10.274763
Q std: 14.268138
Actor loss: 10.278739
Action reg: 0.003976
  l1.weight: grad_norm = 0.184651
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.152878
Total gradient norm: 0.592032
=== Actor Training Debug (Iteration 4466) ===
Q mean: -10.703108
Q std: 14.857615
Actor loss: 10.707065
Action reg: 0.003957
  l1.weight: grad_norm = 0.128061
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.121528
Total gradient norm: 0.323254
=== Actor Training Debug (Iteration 4467) ===
Q mean: -12.709453
Q std: 14.781454
Actor loss: 12.713431
Action reg: 0.003979
  l1.weight: grad_norm = 0.060498
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.053430
Total gradient norm: 0.170693
=== Actor Training Debug (Iteration 4468) ===
Q mean: -11.082603
Q std: 13.425001
Actor loss: 11.086550
Action reg: 0.003947
  l1.weight: grad_norm = 0.109254
  l1.bias: grad_norm = 0.000524
  l2.weight: grad_norm = 0.099954
Total gradient norm: 0.345158
=== Actor Training Debug (Iteration 4469) ===
Q mean: -13.060991
Q std: 15.948578
Actor loss: 13.064956
Action reg: 0.003964
  l1.weight: grad_norm = 0.124086
  l1.bias: grad_norm = 0.000713
  l2.weight: grad_norm = 0.097400
Total gradient norm: 0.296111
=== Actor Training Debug (Iteration 4470) ===
Q mean: -10.318460
Q std: 14.284581
Actor loss: 10.322424
Action reg: 0.003964
  l1.weight: grad_norm = 0.240659
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.189174
Total gradient norm: 0.637001
=== Actor Training Debug (Iteration 4471) ===
Q mean: -9.952694
Q std: 14.935242
Actor loss: 9.956656
Action reg: 0.003962
  l1.weight: grad_norm = 0.114002
  l1.bias: grad_norm = 0.000569
  l2.weight: grad_norm = 0.093921
Total gradient norm: 0.313018
=== Actor Training Debug (Iteration 4472) ===
Q mean: -9.613412
Q std: 13.792006
Actor loss: 9.617376
Action reg: 0.003965
  l1.weight: grad_norm = 0.109012
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.088749
Total gradient norm: 0.312027
=== Actor Training Debug (Iteration 4473) ===
Q mean: -10.451468
Q std: 13.504797
Actor loss: 10.455433
Action reg: 0.003965
  l1.weight: grad_norm = 0.122885
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.115563
Total gradient norm: 0.373808
=== Actor Training Debug (Iteration 4474) ===
Q mean: -10.141235
Q std: 13.584064
Actor loss: 10.145203
Action reg: 0.003967
  l1.weight: grad_norm = 0.162780
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.134209
Total gradient norm: 0.393039
=== Actor Training Debug (Iteration 4475) ===
Q mean: -10.841455
Q std: 15.364547
Actor loss: 10.845419
Action reg: 0.003964
  l1.weight: grad_norm = 0.193998
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.151274
Total gradient norm: 0.443544
=== Actor Training Debug (Iteration 4476) ===
Q mean: -10.624197
Q std: 15.141825
Actor loss: 10.628157
Action reg: 0.003960
  l1.weight: grad_norm = 0.172114
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.141878
Total gradient norm: 0.407455
=== Actor Training Debug (Iteration 4477) ===
Q mean: -10.737119
Q std: 15.011492
Actor loss: 10.741096
Action reg: 0.003977
  l1.weight: grad_norm = 0.054925
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.048565
Total gradient norm: 0.172648
=== Actor Training Debug (Iteration 4478) ===
Q mean: -10.843302
Q std: 14.720819
Actor loss: 10.847276
Action reg: 0.003974
  l1.weight: grad_norm = 0.154586
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.115305
Total gradient norm: 0.340994
=== Actor Training Debug (Iteration 4479) ===
Q mean: -9.805915
Q std: 15.071864
Actor loss: 9.809878
Action reg: 0.003964
  l1.weight: grad_norm = 0.303359
  l1.bias: grad_norm = 0.000625
  l2.weight: grad_norm = 0.241555
Total gradient norm: 0.743364
=== Actor Training Debug (Iteration 4480) ===
Q mean: -9.843466
Q std: 12.994232
Actor loss: 9.847436
Action reg: 0.003971
  l1.weight: grad_norm = 0.196145
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.148972
Total gradient norm: 0.501110
=== Actor Training Debug (Iteration 4481) ===
Q mean: -10.919907
Q std: 15.265194
Actor loss: 10.923882
Action reg: 0.003976
  l1.weight: grad_norm = 0.111587
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.092092
Total gradient norm: 0.271187
=== Actor Training Debug (Iteration 4482) ===
Q mean: -10.109848
Q std: 15.335929
Actor loss: 10.113824
Action reg: 0.003976
  l1.weight: grad_norm = 0.127201
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.116154
Total gradient norm: 0.321703
=== Actor Training Debug (Iteration 4483) ===
Q mean: -8.821943
Q std: 12.626169
Actor loss: 8.825894
Action reg: 0.003951
  l1.weight: grad_norm = 0.148170
  l1.bias: grad_norm = 0.000901
  l2.weight: grad_norm = 0.114066
Total gradient norm: 0.359894
=== Actor Training Debug (Iteration 4484) ===
Q mean: -9.124966
Q std: 13.912414
Actor loss: 9.128946
Action reg: 0.003981
  l1.weight: grad_norm = 0.092109
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.074622
Total gradient norm: 0.242446
=== Actor Training Debug (Iteration 4485) ===
Q mean: -8.455179
Q std: 12.783651
Actor loss: 8.459136
Action reg: 0.003957
  l1.weight: grad_norm = 0.262394
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.218820
Total gradient norm: 0.708437
=== Actor Training Debug (Iteration 4486) ===
Q mean: -9.398906
Q std: 12.335516
Actor loss: 9.402884
Action reg: 0.003978
  l1.weight: grad_norm = 0.084405
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.078798
Total gradient norm: 0.242651
=== Actor Training Debug (Iteration 4487) ===
Q mean: -9.556217
Q std: 14.242275
Actor loss: 9.560174
Action reg: 0.003957
  l1.weight: grad_norm = 0.206669
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.170988
Total gradient norm: 0.512474
=== Actor Training Debug (Iteration 4488) ===
Q mean: -8.751448
Q std: 12.970368
Actor loss: 8.755406
Action reg: 0.003959
  l1.weight: grad_norm = 0.153934
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.131259
Total gradient norm: 0.441207
=== Actor Training Debug (Iteration 4489) ===
Q mean: -10.796484
Q std: 14.490693
Actor loss: 10.800447
Action reg: 0.003963
  l1.weight: grad_norm = 0.134044
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.100605
Total gradient norm: 0.325630
=== Actor Training Debug (Iteration 4490) ===
Q mean: -10.575335
Q std: 15.898226
Actor loss: 10.579284
Action reg: 0.003949
  l1.weight: grad_norm = 0.175366
  l1.bias: grad_norm = 0.001147
  l2.weight: grad_norm = 0.138618
Total gradient norm: 0.399311
=== Actor Training Debug (Iteration 4491) ===
Q mean: -11.987790
Q std: 15.435572
Actor loss: 11.991760
Action reg: 0.003970
  l1.weight: grad_norm = 0.102460
  l1.bias: grad_norm = 0.000597
  l2.weight: grad_norm = 0.082969
Total gradient norm: 0.252701
=== Actor Training Debug (Iteration 4492) ===
Q mean: -11.653219
Q std: 15.343284
Actor loss: 11.657187
Action reg: 0.003968
  l1.weight: grad_norm = 0.143504
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.136821
Total gradient norm: 0.431604
=== Actor Training Debug (Iteration 4493) ===
Q mean: -8.989885
Q std: 13.842532
Actor loss: 8.993839
Action reg: 0.003954
  l1.weight: grad_norm = 0.155849
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.134614
Total gradient norm: 0.456373
=== Actor Training Debug (Iteration 4494) ===
Q mean: -9.363416
Q std: 13.480905
Actor loss: 9.367384
Action reg: 0.003968
  l1.weight: grad_norm = 0.117961
  l1.bias: grad_norm = 0.000699
  l2.weight: grad_norm = 0.099390
Total gradient norm: 0.285502
=== Actor Training Debug (Iteration 4495) ===
Q mean: -11.454741
Q std: 15.245940
Actor loss: 11.458722
Action reg: 0.003981
  l1.weight: grad_norm = 0.099052
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.079863
Total gradient norm: 0.221220
=== Actor Training Debug (Iteration 4496) ===
Q mean: -10.513463
Q std: 14.181314
Actor loss: 10.517438
Action reg: 0.003975
  l1.weight: grad_norm = 0.131609
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.121769
Total gradient norm: 0.367600
=== Actor Training Debug (Iteration 4497) ===
Q mean: -10.741071
Q std: 14.285950
Actor loss: 10.745051
Action reg: 0.003981
  l1.weight: grad_norm = 0.130809
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.112557
Total gradient norm: 0.382901
=== Actor Training Debug (Iteration 4498) ===
Q mean: -12.110086
Q std: 16.409838
Actor loss: 12.114053
Action reg: 0.003966
  l1.weight: grad_norm = 0.099988
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.089602
Total gradient norm: 0.301139
=== Actor Training Debug (Iteration 4499) ===
Q mean: -9.376882
Q std: 14.173330
Actor loss: 9.380857
Action reg: 0.003975
  l1.weight: grad_norm = 0.191163
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.157710
Total gradient norm: 0.475905
=== Actor Training Debug (Iteration 4500) ===
Q mean: -10.919415
Q std: 14.614313
Actor loss: 10.923380
Action reg: 0.003965
  l1.weight: grad_norm = 0.160844
  l1.bias: grad_norm = 0.000649
  l2.weight: grad_norm = 0.130693
Total gradient norm: 0.402550
  Average reward: -318.365 | Average length: 100.0
Evaluation at episode 95: -318.365
=== Actor Training Debug (Iteration 4501) ===
Q mean: -9.996231
Q std: 15.042965
Actor loss: 10.000202
Action reg: 0.003971
  l1.weight: grad_norm = 0.081329
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.066531
Total gradient norm: 0.223490
=== Actor Training Debug (Iteration 4502) ===
Q mean: -11.995643
Q std: 15.103234
Actor loss: 11.999608
Action reg: 0.003966
  l1.weight: grad_norm = 0.103990
  l1.bias: grad_norm = 0.001633
  l2.weight: grad_norm = 0.086006
Total gradient norm: 0.291178
=== Actor Training Debug (Iteration 4503) ===
Q mean: -11.509961
Q std: 15.444514
Actor loss: 11.513920
Action reg: 0.003959
  l1.weight: grad_norm = 0.167881
  l1.bias: grad_norm = 0.001367
  l2.weight: grad_norm = 0.134873
Total gradient norm: 0.424405
=== Actor Training Debug (Iteration 4504) ===
Q mean: -10.994808
Q std: 13.782855
Actor loss: 10.998780
Action reg: 0.003972
  l1.weight: grad_norm = 0.094099
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.082493
Total gradient norm: 0.242830
=== Actor Training Debug (Iteration 4505) ===
Q mean: -10.177465
Q std: 13.999778
Actor loss: 10.181422
Action reg: 0.003957
  l1.weight: grad_norm = 0.117446
  l1.bias: grad_norm = 0.000548
  l2.weight: grad_norm = 0.101780
Total gradient norm: 0.356215
=== Actor Training Debug (Iteration 4506) ===
Q mean: -10.367777
Q std: 13.236098
Actor loss: 10.371740
Action reg: 0.003963
  l1.weight: grad_norm = 0.092381
  l1.bias: grad_norm = 0.000758
  l2.weight: grad_norm = 0.086112
Total gradient norm: 0.271297
=== Actor Training Debug (Iteration 4507) ===
Q mean: -10.884158
Q std: 15.365519
Actor loss: 10.888122
Action reg: 0.003964
  l1.weight: grad_norm = 0.097297
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.085552
Total gradient norm: 0.302773
=== Actor Training Debug (Iteration 4508) ===
Q mean: -10.336559
Q std: 14.258196
Actor loss: 10.340537
Action reg: 0.003978
  l1.weight: grad_norm = 0.062547
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.052834
Total gradient norm: 0.162905
=== Actor Training Debug (Iteration 4509) ===
Q mean: -10.255140
Q std: 14.837439
Actor loss: 10.259111
Action reg: 0.003971
  l1.weight: grad_norm = 0.114471
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.101427
Total gradient norm: 0.338671
=== Actor Training Debug (Iteration 4510) ===
Q mean: -9.981976
Q std: 13.768105
Actor loss: 9.985947
Action reg: 0.003971
  l1.weight: grad_norm = 0.129854
  l1.bias: grad_norm = 0.000764
  l2.weight: grad_norm = 0.110590
Total gradient norm: 0.343652
=== Actor Training Debug (Iteration 4511) ===
Q mean: -10.403338
Q std: 14.086828
Actor loss: 10.407309
Action reg: 0.003970
  l1.weight: grad_norm = 0.167716
  l1.bias: grad_norm = 0.000750
  l2.weight: grad_norm = 0.139520
Total gradient norm: 0.399933
=== Actor Training Debug (Iteration 4512) ===
Q mean: -12.163109
Q std: 15.448542
Actor loss: 12.167076
Action reg: 0.003967
  l1.weight: grad_norm = 0.094671
  l1.bias: grad_norm = 0.000630
  l2.weight: grad_norm = 0.077992
Total gradient norm: 0.243847
=== Actor Training Debug (Iteration 4513) ===
Q mean: -10.925684
Q std: 14.291944
Actor loss: 10.929654
Action reg: 0.003970
  l1.weight: grad_norm = 0.133256
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.130582
Total gradient norm: 0.346247
=== Actor Training Debug (Iteration 4514) ===
Q mean: -8.667124
Q std: 13.667027
Actor loss: 8.671096
Action reg: 0.003972
  l1.weight: grad_norm = 0.167289
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.157721
Total gradient norm: 0.550751
=== Actor Training Debug (Iteration 4515) ===
Q mean: -11.523915
Q std: 15.517982
Actor loss: 11.527858
Action reg: 0.003943
  l1.weight: grad_norm = 0.097473
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.086940
Total gradient norm: 0.313734
=== Actor Training Debug (Iteration 4516) ===
Q mean: -10.458427
Q std: 15.013982
Actor loss: 10.462401
Action reg: 0.003974
  l1.weight: grad_norm = 0.134798
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.114968
Total gradient norm: 0.350471
=== Actor Training Debug (Iteration 4517) ===
Q mean: -11.887186
Q std: 15.196404
Actor loss: 11.891166
Action reg: 0.003980
  l1.weight: grad_norm = 0.137651
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.116257
Total gradient norm: 0.453250
=== Actor Training Debug (Iteration 4518) ===
Q mean: -10.240129
Q std: 14.249393
Actor loss: 10.244099
Action reg: 0.003969
  l1.weight: grad_norm = 0.172448
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.152921
Total gradient norm: 0.434368
=== Actor Training Debug (Iteration 4519) ===
Q mean: -12.303684
Q std: 15.265898
Actor loss: 12.307653
Action reg: 0.003969
  l1.weight: grad_norm = 0.184886
  l1.bias: grad_norm = 0.000635
  l2.weight: grad_norm = 0.162829
Total gradient norm: 0.500019
=== Actor Training Debug (Iteration 4520) ===
Q mean: -10.206422
Q std: 14.606739
Actor loss: 10.210386
Action reg: 0.003964
  l1.weight: grad_norm = 0.058420
  l1.bias: grad_norm = 0.001014
  l2.weight: grad_norm = 0.047639
Total gradient norm: 0.150252
=== Actor Training Debug (Iteration 4521) ===
Q mean: -10.512520
Q std: 15.572291
Actor loss: 10.516496
Action reg: 0.003976
  l1.weight: grad_norm = 0.176957
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.147516
Total gradient norm: 0.447640
=== Actor Training Debug (Iteration 4522) ===
Q mean: -10.642794
Q std: 14.891029
Actor loss: 10.646763
Action reg: 0.003969
  l1.weight: grad_norm = 0.089513
  l1.bias: grad_norm = 0.001024
  l2.weight: grad_norm = 0.077229
Total gradient norm: 0.282151
=== Actor Training Debug (Iteration 4523) ===
Q mean: -10.137686
Q std: 14.140342
Actor loss: 10.141650
Action reg: 0.003964
  l1.weight: grad_norm = 0.080209
  l1.bias: grad_norm = 0.000657
  l2.weight: grad_norm = 0.069750
Total gradient norm: 0.203519
=== Actor Training Debug (Iteration 4524) ===
Q mean: -11.235863
Q std: 14.463151
Actor loss: 11.239830
Action reg: 0.003967
  l1.weight: grad_norm = 0.101386
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.096094
Total gradient norm: 0.278154
=== Actor Training Debug (Iteration 4525) ===
Q mean: -10.846098
Q std: 14.994808
Actor loss: 10.850053
Action reg: 0.003955
  l1.weight: grad_norm = 0.259259
  l1.bias: grad_norm = 0.000472
  l2.weight: grad_norm = 0.243374
Total gradient norm: 0.689920
=== Actor Training Debug (Iteration 4526) ===
Q mean: -10.178119
Q std: 14.085214
Actor loss: 10.182085
Action reg: 0.003967
  l1.weight: grad_norm = 0.066230
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.058052
Total gradient norm: 0.187553
=== Actor Training Debug (Iteration 4527) ===
Q mean: -10.165801
Q std: 14.329185
Actor loss: 10.169753
Action reg: 0.003952
  l1.weight: grad_norm = 0.166998
  l1.bias: grad_norm = 0.000643
  l2.weight: grad_norm = 0.158318
Total gradient norm: 0.474470
=== Actor Training Debug (Iteration 4528) ===
Q mean: -9.782994
Q std: 13.620947
Actor loss: 9.786959
Action reg: 0.003964
  l1.weight: grad_norm = 0.039644
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.032484
Total gradient norm: 0.104672
=== Actor Training Debug (Iteration 4529) ===
Q mean: -10.381520
Q std: 13.898708
Actor loss: 10.385485
Action reg: 0.003964
  l1.weight: grad_norm = 0.153131
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.116012
Total gradient norm: 0.342138
=== Actor Training Debug (Iteration 4530) ===
Q mean: -11.347084
Q std: 13.641801
Actor loss: 11.351065
Action reg: 0.003980
  l1.weight: grad_norm = 0.055824
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.042743
Total gradient norm: 0.123677
=== Actor Training Debug (Iteration 4531) ===
Q mean: -10.884996
Q std: 14.652958
Actor loss: 10.888949
Action reg: 0.003953
  l1.weight: grad_norm = 0.078687
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.069914
Total gradient norm: 0.204979
=== Actor Training Debug (Iteration 4532) ===
Q mean: -11.109478
Q std: 14.873199
Actor loss: 11.113436
Action reg: 0.003958
  l1.weight: grad_norm = 0.098887
  l1.bias: grad_norm = 0.001607
  l2.weight: grad_norm = 0.097991
Total gradient norm: 0.254119
=== Actor Training Debug (Iteration 4533) ===
Q mean: -10.287197
Q std: 13.447636
Actor loss: 10.291160
Action reg: 0.003963
  l1.weight: grad_norm = 0.164276
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.147277
Total gradient norm: 0.456333
=== Actor Training Debug (Iteration 4534) ===
Q mean: -11.964273
Q std: 15.920926
Actor loss: 11.968238
Action reg: 0.003965
  l1.weight: grad_norm = 0.095823
  l1.bias: grad_norm = 0.000828
  l2.weight: grad_norm = 0.078856
Total gradient norm: 0.249662
=== Actor Training Debug (Iteration 4535) ===
Q mean: -11.346766
Q std: 15.627462
Actor loss: 11.350727
Action reg: 0.003961
  l1.weight: grad_norm = 0.102892
  l1.bias: grad_norm = 0.000822
  l2.weight: grad_norm = 0.095462
Total gradient norm: 0.330002
=== Actor Training Debug (Iteration 4536) ===
Q mean: -11.575520
Q std: 14.751877
Actor loss: 11.579490
Action reg: 0.003970
  l1.weight: grad_norm = 0.129951
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.107626
Total gradient norm: 0.363582
=== Actor Training Debug (Iteration 4537) ===
Q mean: -11.590031
Q std: 15.528903
Actor loss: 11.593994
Action reg: 0.003963
  l1.weight: grad_norm = 0.050297
  l1.bias: grad_norm = 0.000876
  l2.weight: grad_norm = 0.047865
Total gradient norm: 0.147832
=== Actor Training Debug (Iteration 4538) ===
Q mean: -10.137188
Q std: 14.137426
Actor loss: 10.141150
Action reg: 0.003963
  l1.weight: grad_norm = 0.091656
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.080517
Total gradient norm: 0.230888
=== Actor Training Debug (Iteration 4539) ===
Q mean: -10.478416
Q std: 13.130077
Actor loss: 10.482389
Action reg: 0.003973
  l1.weight: grad_norm = 0.205388
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.179286
Total gradient norm: 0.579409
=== Actor Training Debug (Iteration 4540) ===
Q mean: -11.646781
Q std: 15.333422
Actor loss: 11.650753
Action reg: 0.003972
  l1.weight: grad_norm = 0.151638
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.136134
Total gradient norm: 0.409732
=== Actor Training Debug (Iteration 4541) ===
Q mean: -10.666396
Q std: 14.306666
Actor loss: 10.670359
Action reg: 0.003963
  l1.weight: grad_norm = 0.139101
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.119313
Total gradient norm: 0.396428
=== Actor Training Debug (Iteration 4542) ===
Q mean: -10.226561
Q std: 14.731120
Actor loss: 10.230535
Action reg: 0.003974
  l1.weight: grad_norm = 0.137692
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.126249
Total gradient norm: 0.387234
=== Actor Training Debug (Iteration 4543) ===
Q mean: -9.916981
Q std: 14.259563
Actor loss: 9.920950
Action reg: 0.003969
  l1.weight: grad_norm = 0.133556
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.130550
Total gradient norm: 0.463158
=== Actor Training Debug (Iteration 4544) ===
Q mean: -10.681735
Q std: 14.547581
Actor loss: 10.685713
Action reg: 0.003978
  l1.weight: grad_norm = 0.075434
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.068957
Total gradient norm: 0.248065
=== Actor Training Debug (Iteration 4545) ===
Q mean: -10.659851
Q std: 14.723776
Actor loss: 10.663817
Action reg: 0.003966
  l1.weight: grad_norm = 0.196497
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.161248
Total gradient norm: 0.492026
=== Actor Training Debug (Iteration 4546) ===
Q mean: -11.909423
Q std: 15.417633
Actor loss: 11.913391
Action reg: 0.003968
  l1.weight: grad_norm = 0.148887
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.132080
Total gradient norm: 0.405873
=== Actor Training Debug (Iteration 4547) ===
Q mean: -10.850388
Q std: 14.395370
Actor loss: 10.854347
Action reg: 0.003960
  l1.weight: grad_norm = 0.147933
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.129161
Total gradient norm: 0.425434
=== Actor Training Debug (Iteration 4548) ===
Q mean: -10.715473
Q std: 14.759650
Actor loss: 10.719442
Action reg: 0.003970
  l1.weight: grad_norm = 0.113611
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.111699
Total gradient norm: 0.353509
=== Actor Training Debug (Iteration 4549) ===
Q mean: -8.622482
Q std: 12.094735
Actor loss: 8.626457
Action reg: 0.003975
  l1.weight: grad_norm = 0.061502
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.051735
Total gradient norm: 0.174865
=== Actor Training Debug (Iteration 4550) ===
Q mean: -10.550135
Q std: 14.957583
Actor loss: 10.554097
Action reg: 0.003962
  l1.weight: grad_norm = 0.203942
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.187249
Total gradient norm: 0.522422
=== Actor Training Debug (Iteration 4551) ===
Q mean: -10.208546
Q std: 15.456841
Actor loss: 10.212502
Action reg: 0.003957
  l1.weight: grad_norm = 0.140452
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.119640
Total gradient norm: 0.332825
=== Actor Training Debug (Iteration 4552) ===
Q mean: -11.813320
Q std: 15.181566
Actor loss: 11.817307
Action reg: 0.003987
  l1.weight: grad_norm = 0.140854
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.115603
Total gradient norm: 0.373640
=== Actor Training Debug (Iteration 4553) ===
Q mean: -8.822886
Q std: 12.946543
Actor loss: 8.826848
Action reg: 0.003962
  l1.weight: grad_norm = 0.141861
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.109435
Total gradient norm: 0.303097
=== Actor Training Debug (Iteration 4554) ===
Q mean: -8.328581
Q std: 12.089573
Actor loss: 8.332533
Action reg: 0.003952
  l1.weight: grad_norm = 0.131167
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.114629
Total gradient norm: 0.390175
=== Actor Training Debug (Iteration 4555) ===
Q mean: -9.380732
Q std: 13.520336
Actor loss: 9.384699
Action reg: 0.003968
  l1.weight: grad_norm = 0.136670
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.116920
Total gradient norm: 0.365465
=== Actor Training Debug (Iteration 4556) ===
Q mean: -11.030502
Q std: 16.092237
Actor loss: 11.034478
Action reg: 0.003975
  l1.weight: grad_norm = 0.087364
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.073343
Total gradient norm: 0.255368
=== Actor Training Debug (Iteration 4557) ===
Q mean: -9.446371
Q std: 13.759274
Actor loss: 9.450329
Action reg: 0.003958
  l1.weight: grad_norm = 0.187851
  l1.bias: grad_norm = 0.000658
  l2.weight: grad_norm = 0.157995
Total gradient norm: 0.479166
=== Actor Training Debug (Iteration 4558) ===
Q mean: -13.080112
Q std: 17.171213
Actor loss: 13.084084
Action reg: 0.003971
  l1.weight: grad_norm = 0.102106
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.089499
Total gradient norm: 0.279344
=== Actor Training Debug (Iteration 4559) ===
Q mean: -10.393070
Q std: 13.748251
Actor loss: 10.397041
Action reg: 0.003972
  l1.weight: grad_norm = 0.183426
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.164052
Total gradient norm: 0.459022
=== Actor Training Debug (Iteration 4560) ===
Q mean: -9.495770
Q std: 14.664825
Actor loss: 9.499743
Action reg: 0.003974
  l1.weight: grad_norm = 0.109211
  l1.bias: grad_norm = 0.000745
  l2.weight: grad_norm = 0.107185
Total gradient norm: 0.309867
=== Actor Training Debug (Iteration 4561) ===
Q mean: -9.998749
Q std: 13.552876
Actor loss: 10.002713
Action reg: 0.003965
  l1.weight: grad_norm = 0.146379
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.117931
Total gradient norm: 0.368991
=== Actor Training Debug (Iteration 4562) ===
Q mean: -10.709503
Q std: 14.911819
Actor loss: 10.713478
Action reg: 0.003975
  l1.weight: grad_norm = 0.281568
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.210728
Total gradient norm: 0.851696
=== Actor Training Debug (Iteration 4563) ===
Q mean: -11.549313
Q std: 14.313771
Actor loss: 11.553266
Action reg: 0.003953
  l1.weight: grad_norm = 0.180203
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.170754
Total gradient norm: 0.515213
=== Actor Training Debug (Iteration 4564) ===
Q mean: -10.339778
Q std: 14.540735
Actor loss: 10.343726
Action reg: 0.003948
  l1.weight: grad_norm = 0.175445
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.162119
Total gradient norm: 0.520046
=== Actor Training Debug (Iteration 4565) ===
Q mean: -9.026550
Q std: 13.362731
Actor loss: 9.030529
Action reg: 0.003979
  l1.weight: grad_norm = 0.192752
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.153405
Total gradient norm: 0.471651
=== Actor Training Debug (Iteration 4566) ===
Q mean: -10.143375
Q std: 13.399965
Actor loss: 10.147329
Action reg: 0.003954
  l1.weight: grad_norm = 0.226647
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.185392
Total gradient norm: 0.578501
=== Actor Training Debug (Iteration 4567) ===
Q mean: -10.393330
Q std: 15.035553
Actor loss: 10.397304
Action reg: 0.003974
  l1.weight: grad_norm = 0.097832
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.099442
Total gradient norm: 0.269640
=== Actor Training Debug (Iteration 4568) ===
Q mean: -10.650405
Q std: 15.267811
Actor loss: 10.654388
Action reg: 0.003983
  l1.weight: grad_norm = 0.107723
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.084907
Total gradient norm: 0.298190
=== Actor Training Debug (Iteration 4569) ===
Q mean: -11.135476
Q std: 14.802576
Actor loss: 11.139445
Action reg: 0.003969
  l1.weight: grad_norm = 0.131760
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.108779
Total gradient norm: 0.299522
=== Actor Training Debug (Iteration 4570) ===
Q mean: -12.219819
Q std: 14.095062
Actor loss: 12.223787
Action reg: 0.003968
  l1.weight: grad_norm = 0.207705
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.163665
Total gradient norm: 0.407963
=== Actor Training Debug (Iteration 4571) ===
Q mean: -11.765318
Q std: 15.596032
Actor loss: 11.769289
Action reg: 0.003971
  l1.weight: grad_norm = 0.160227
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.147423
Total gradient norm: 0.468671
=== Actor Training Debug (Iteration 4572) ===
Q mean: -9.466741
Q std: 13.463742
Actor loss: 9.470702
Action reg: 0.003961
  l1.weight: grad_norm = 0.192758
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.181382
Total gradient norm: 0.516362
=== Actor Training Debug (Iteration 4573) ===
Q mean: -10.883226
Q std: 13.904045
Actor loss: 10.887198
Action reg: 0.003972
  l1.weight: grad_norm = 0.088139
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.079018
Total gradient norm: 0.266623
=== Actor Training Debug (Iteration 4574) ===
Q mean: -9.054892
Q std: 14.195479
Actor loss: 9.058868
Action reg: 0.003977
  l1.weight: grad_norm = 0.062754
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.053771
Total gradient norm: 0.152533
=== Actor Training Debug (Iteration 4575) ===
Q mean: -10.042326
Q std: 16.024834
Actor loss: 10.046303
Action reg: 0.003976
  l1.weight: grad_norm = 0.147641
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.137867
Total gradient norm: 0.441899
=== Actor Training Debug (Iteration 4576) ===
Q mean: -11.175859
Q std: 14.273203
Actor loss: 11.179828
Action reg: 0.003968
  l1.weight: grad_norm = 0.145645
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.119919
Total gradient norm: 0.403201
=== Actor Training Debug (Iteration 4577) ===
Q mean: -10.267162
Q std: 13.945361
Actor loss: 10.271125
Action reg: 0.003962
  l1.weight: grad_norm = 0.151653
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.133204
Total gradient norm: 0.436070
=== Actor Training Debug (Iteration 4578) ===
Q mean: -10.095942
Q std: 14.644546
Actor loss: 10.099913
Action reg: 0.003971
  l1.weight: grad_norm = 0.189943
  l1.bias: grad_norm = 0.000543
  l2.weight: grad_norm = 0.195699
Total gradient norm: 0.584187
=== Actor Training Debug (Iteration 4579) ===
Q mean: -10.330404
Q std: 13.892896
Actor loss: 10.334355
Action reg: 0.003951
  l1.weight: grad_norm = 0.204522
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.189861
Total gradient norm: 0.542019
=== Actor Training Debug (Iteration 4580) ===
Q mean: -12.174506
Q std: 14.574245
Actor loss: 12.178467
Action reg: 0.003961
  l1.weight: grad_norm = 0.238810
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.197339
Total gradient norm: 0.668448
=== Actor Training Debug (Iteration 4581) ===
Q mean: -10.736229
Q std: 14.909125
Actor loss: 10.740197
Action reg: 0.003968
  l1.weight: grad_norm = 0.158881
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.152568
Total gradient norm: 0.392635
=== Actor Training Debug (Iteration 4582) ===
Q mean: -10.820797
Q std: 14.158837
Actor loss: 10.824764
Action reg: 0.003968
  l1.weight: grad_norm = 0.185416
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.168498
Total gradient norm: 0.558250
=== Actor Training Debug (Iteration 4583) ===
Q mean: -10.929728
Q std: 14.469770
Actor loss: 10.933701
Action reg: 0.003973
  l1.weight: grad_norm = 0.221830
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.197356
Total gradient norm: 0.615846
=== Actor Training Debug (Iteration 4584) ===
Q mean: -10.245608
Q std: 14.001312
Actor loss: 10.249578
Action reg: 0.003970
  l1.weight: grad_norm = 0.190643
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.162316
Total gradient norm: 0.447334
=== Actor Training Debug (Iteration 4585) ===
Q mean: -10.576520
Q std: 14.396441
Actor loss: 10.580484
Action reg: 0.003964
  l1.weight: grad_norm = 0.111877
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.085883
Total gradient norm: 0.271814
=== Actor Training Debug (Iteration 4586) ===
Q mean: -11.781978
Q std: 14.713573
Actor loss: 11.785947
Action reg: 0.003969
  l1.weight: grad_norm = 0.150241
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.109289
Total gradient norm: 0.322259
=== Actor Training Debug (Iteration 4587) ===
Q mean: -10.977331
Q std: 14.086981
Actor loss: 10.981305
Action reg: 0.003974
  l1.weight: grad_norm = 0.105846
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.086725
Total gradient norm: 0.277937
=== Actor Training Debug (Iteration 4588) ===
Q mean: -9.870199
Q std: 15.205812
Actor loss: 9.874166
Action reg: 0.003968
  l1.weight: grad_norm = 0.150207
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.138894
Total gradient norm: 0.360164
=== Actor Training Debug (Iteration 4589) ===
Q mean: -9.092827
Q std: 15.759656
Actor loss: 9.096792
Action reg: 0.003966
  l1.weight: grad_norm = 0.108081
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.086058
Total gradient norm: 0.278727
=== Actor Training Debug (Iteration 4590) ===
Q mean: -10.165153
Q std: 15.504044
Actor loss: 10.169106
Action reg: 0.003953
  l1.weight: grad_norm = 0.135076
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.118765
Total gradient norm: 0.399665
=== Actor Training Debug (Iteration 4591) ===
Q mean: -9.986592
Q std: 13.965404
Actor loss: 9.990561
Action reg: 0.003969
  l1.weight: grad_norm = 0.127938
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.112096
Total gradient norm: 0.348222
=== Actor Training Debug (Iteration 4592) ===
Q mean: -10.350004
Q std: 15.103844
Actor loss: 10.353976
Action reg: 0.003972
  l1.weight: grad_norm = 0.084663
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.076306
Total gradient norm: 0.263773
=== Actor Training Debug (Iteration 4593) ===
Q mean: -10.612654
Q std: 14.576656
Actor loss: 10.616624
Action reg: 0.003970
  l1.weight: grad_norm = 0.092268
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.088971
Total gradient norm: 0.231488
=== Actor Training Debug (Iteration 4594) ===
Q mean: -10.438501
Q std: 14.806458
Actor loss: 10.442472
Action reg: 0.003970
  l1.weight: grad_norm = 0.083311
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.075668
Total gradient norm: 0.272360
=== Actor Training Debug (Iteration 4595) ===
Q mean: -10.031504
Q std: 14.401256
Actor loss: 10.035481
Action reg: 0.003978
  l1.weight: grad_norm = 0.056312
  l1.bias: grad_norm = 0.000021
  l2.weight: grad_norm = 0.053349
Total gradient norm: 0.143988
=== Actor Training Debug (Iteration 4596) ===
Q mean: -10.570767
Q std: 13.941686
Actor loss: 10.574748
Action reg: 0.003981
  l1.weight: grad_norm = 0.179030
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.154606
Total gradient norm: 0.449911
=== Actor Training Debug (Iteration 4597) ===
Q mean: -11.087675
Q std: 14.714908
Actor loss: 11.091650
Action reg: 0.003975
  l1.weight: grad_norm = 0.145271
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.136774
Total gradient norm: 0.480108
=== Actor Training Debug (Iteration 4598) ===
Q mean: -9.670774
Q std: 13.962716
Actor loss: 9.674742
Action reg: 0.003968
  l1.weight: grad_norm = 0.123883
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.104084
Total gradient norm: 0.357428
=== Actor Training Debug (Iteration 4599) ===
Q mean: -9.613914
Q std: 14.905664
Actor loss: 9.617878
Action reg: 0.003963
  l1.weight: grad_norm = 0.171544
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.159329
Total gradient norm: 0.486194
=== Actor Training Debug (Iteration 4600) ===
Q mean: -9.963018
Q std: 14.619615
Actor loss: 9.966989
Action reg: 0.003970
  l1.weight: grad_norm = 0.115635
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.093247
Total gradient norm: 0.293647
=== Actor Training Debug (Iteration 4601) ===
Q mean: -11.282348
Q std: 14.985595
Actor loss: 11.286330
Action reg: 0.003982
  l1.weight: grad_norm = 0.056319
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.044728
Total gradient norm: 0.140885
=== Actor Training Debug (Iteration 4602) ===
Q mean: -9.960293
Q std: 15.120321
Actor loss: 9.964262
Action reg: 0.003969
  l1.weight: grad_norm = 0.127079
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.105588
Total gradient norm: 0.302028
=== Actor Training Debug (Iteration 4603) ===
Q mean: -10.954694
Q std: 15.421947
Actor loss: 10.958668
Action reg: 0.003974
  l1.weight: grad_norm = 0.141329
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.115610
Total gradient norm: 0.344991
=== Actor Training Debug (Iteration 4604) ===
Q mean: -9.775936
Q std: 12.718914
Actor loss: 9.779914
Action reg: 0.003978
  l1.weight: grad_norm = 0.097738
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.084948
Total gradient norm: 0.278601
=== Actor Training Debug (Iteration 4605) ===
Q mean: -9.027550
Q std: 13.390479
Actor loss: 9.031523
Action reg: 0.003973
  l1.weight: grad_norm = 0.128878
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.111951
Total gradient norm: 0.328138
=== Actor Training Debug (Iteration 4606) ===
Q mean: -9.484638
Q std: 14.673439
Actor loss: 9.488616
Action reg: 0.003978
  l1.weight: grad_norm = 0.145302
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.123503
Total gradient norm: 0.375319
=== Actor Training Debug (Iteration 4607) ===
Q mean: -10.823664
Q std: 15.000902
Actor loss: 10.827623
Action reg: 0.003960
  l1.weight: grad_norm = 0.127226
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.102444
Total gradient norm: 0.328721
=== Actor Training Debug (Iteration 4608) ===
Q mean: -10.900447
Q std: 14.574638
Actor loss: 10.904424
Action reg: 0.003977
  l1.weight: grad_norm = 0.210723
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.175722
Total gradient norm: 0.495938
=== Actor Training Debug (Iteration 4609) ===
Q mean: -12.299110
Q std: 16.117907
Actor loss: 12.303086
Action reg: 0.003976
  l1.weight: grad_norm = 0.071909
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.067950
Total gradient norm: 0.232889
=== Actor Training Debug (Iteration 4610) ===
Q mean: -10.683365
Q std: 14.756283
Actor loss: 10.687334
Action reg: 0.003970
  l1.weight: grad_norm = 0.087018
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.072938
Total gradient norm: 0.226726
=== Actor Training Debug (Iteration 4611) ===
Q mean: -9.521488
Q std: 14.033031
Actor loss: 9.525471
Action reg: 0.003982
  l1.weight: grad_norm = 0.159806
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.144856
Total gradient norm: 0.448322
=== Actor Training Debug (Iteration 4612) ===
Q mean: -9.639856
Q std: 14.138098
Actor loss: 9.643841
Action reg: 0.003984
  l1.weight: grad_norm = 0.117160
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.099311
Total gradient norm: 0.273566
=== Actor Training Debug (Iteration 4613) ===
Q mean: -9.644886
Q std: 13.708304
Actor loss: 9.648866
Action reg: 0.003980
  l1.weight: grad_norm = 0.135488
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.122111
Total gradient norm: 0.439371
=== Actor Training Debug (Iteration 4614) ===
Q mean: -10.747430
Q std: 14.476974
Actor loss: 10.751399
Action reg: 0.003969
  l1.weight: grad_norm = 0.158230
  l1.bias: grad_norm = 0.000572
  l2.weight: grad_norm = 0.141273
Total gradient norm: 0.368426
=== Actor Training Debug (Iteration 4615) ===
Q mean: -10.024795
Q std: 14.313983
Actor loss: 10.028777
Action reg: 0.003982
  l1.weight: grad_norm = 0.058915
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.054336
Total gradient norm: 0.164840
=== Actor Training Debug (Iteration 4616) ===
Q mean: -10.329668
Q std: 14.568373
Actor loss: 10.333643
Action reg: 0.003975
  l1.weight: grad_norm = 0.090442
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.084317
Total gradient norm: 0.248339
=== Actor Training Debug (Iteration 4617) ===
Q mean: -10.855706
Q std: 15.467376
Actor loss: 10.859668
Action reg: 0.003961
  l1.weight: grad_norm = 0.189785
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.159052
Total gradient norm: 0.554303
=== Actor Training Debug (Iteration 4618) ===
Q mean: -10.963294
Q std: 14.847241
Actor loss: 10.967257
Action reg: 0.003963
  l1.weight: grad_norm = 0.092997
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.086163
Total gradient norm: 0.293834
=== Actor Training Debug (Iteration 4619) ===
Q mean: -11.035278
Q std: 16.069603
Actor loss: 11.039252
Action reg: 0.003974
  l1.weight: grad_norm = 0.078077
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.072400
Total gradient norm: 0.215758
=== Actor Training Debug (Iteration 4620) ===
Q mean: -11.005843
Q std: 15.133955
Actor loss: 11.009822
Action reg: 0.003979
  l1.weight: grad_norm = 0.204142
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.203475
Total gradient norm: 0.563728
=== Actor Training Debug (Iteration 4621) ===
Q mean: -10.452151
Q std: 15.095907
Actor loss: 10.456118
Action reg: 0.003967
  l1.weight: grad_norm = 0.061784
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.057512
Total gradient norm: 0.198129
=== Actor Training Debug (Iteration 4622) ===
Q mean: -10.334671
Q std: 14.617042
Actor loss: 10.338650
Action reg: 0.003979
  l1.weight: grad_norm = 0.061921
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.056172
Total gradient norm: 0.171109
=== Actor Training Debug (Iteration 4623) ===
Q mean: -11.856932
Q std: 15.319059
Actor loss: 11.860896
Action reg: 0.003964
  l1.weight: grad_norm = 0.066501
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.063431
Total gradient norm: 0.178230
=== Actor Training Debug (Iteration 4624) ===
Q mean: -13.169785
Q std: 16.845888
Actor loss: 13.173759
Action reg: 0.003973
  l1.weight: grad_norm = 0.060451
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.055550
Total gradient norm: 0.167386
=== Actor Training Debug (Iteration 4625) ===
Q mean: -9.458243
Q std: 13.145225
Actor loss: 9.462211
Action reg: 0.003968
  l1.weight: grad_norm = 0.104041
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.085363
Total gradient norm: 0.269593
=== Actor Training Debug (Iteration 4626) ===
Q mean: -10.638269
Q std: 14.500360
Actor loss: 10.642243
Action reg: 0.003974
  l1.weight: grad_norm = 0.114358
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.097342
Total gradient norm: 0.320865
=== Actor Training Debug (Iteration 4627) ===
Q mean: -10.009402
Q std: 13.986735
Actor loss: 10.013379
Action reg: 0.003977
  l1.weight: grad_norm = 0.055397
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.051076
Total gradient norm: 0.185358
=== Actor Training Debug (Iteration 4628) ===
Q mean: -11.970469
Q std: 14.801403
Actor loss: 11.974442
Action reg: 0.003974
  l1.weight: grad_norm = 0.079209
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.071089
Total gradient norm: 0.207160
=== Actor Training Debug (Iteration 4629) ===
Q mean: -11.453437
Q std: 16.039669
Actor loss: 11.457417
Action reg: 0.003980
  l1.weight: grad_norm = 0.106919
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.096608
Total gradient norm: 0.247430
=== Actor Training Debug (Iteration 4630) ===
Q mean: -11.843458
Q std: 13.805104
Actor loss: 11.847430
Action reg: 0.003972
  l1.weight: grad_norm = 0.179422
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.139420
Total gradient norm: 0.402045
=== Actor Training Debug (Iteration 4631) ===
Q mean: -10.109555
Q std: 15.578019
Actor loss: 10.113525
Action reg: 0.003970
  l1.weight: grad_norm = 0.191046
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.147821
Total gradient norm: 0.414635
=== Actor Training Debug (Iteration 4632) ===
Q mean: -10.353647
Q std: 13.779339
Actor loss: 10.357608
Action reg: 0.003961
  l1.weight: grad_norm = 0.225241
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.161953
Total gradient norm: 0.408446
=== Actor Training Debug (Iteration 4633) ===
Q mean: -10.449833
Q std: 14.337619
Actor loss: 10.453798
Action reg: 0.003966
  l1.weight: grad_norm = 0.156675
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.144151
Total gradient norm: 0.404107
=== Actor Training Debug (Iteration 4634) ===
Q mean: -11.060789
Q std: 14.357805
Actor loss: 11.064753
Action reg: 0.003963
  l1.weight: grad_norm = 0.125412
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.119367
Total gradient norm: 0.371570
=== Actor Training Debug (Iteration 4635) ===
Q mean: -9.488450
Q std: 13.449971
Actor loss: 9.492435
Action reg: 0.003985
  l1.weight: grad_norm = 0.034214
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.035515
Total gradient norm: 0.101740
=== Actor Training Debug (Iteration 4636) ===
Q mean: -9.275627
Q std: 14.003825
Actor loss: 9.279598
Action reg: 0.003971
  l1.weight: grad_norm = 0.083157
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.078775
Total gradient norm: 0.226506
=== Actor Training Debug (Iteration 4637) ===
Q mean: -12.132985
Q std: 16.065687
Actor loss: 12.136955
Action reg: 0.003970
  l1.weight: grad_norm = 0.191207
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.158462
Total gradient norm: 0.468331
=== Actor Training Debug (Iteration 4638) ===
Q mean: -12.416420
Q std: 15.082394
Actor loss: 12.420400
Action reg: 0.003979
  l1.weight: grad_norm = 0.084144
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.070980
Total gradient norm: 0.226692
=== Actor Training Debug (Iteration 4639) ===
Q mean: -11.622667
Q std: 14.070337
Actor loss: 11.626646
Action reg: 0.003979
  l1.weight: grad_norm = 0.149820
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.142373
Total gradient norm: 0.347959
=== Actor Training Debug (Iteration 4640) ===
Q mean: -9.579773
Q std: 14.403522
Actor loss: 9.583750
Action reg: 0.003977
  l1.weight: grad_norm = 0.034155
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.034321
Total gradient norm: 0.118245
=== Actor Training Debug (Iteration 4641) ===
Q mean: -11.013688
Q std: 14.813300
Actor loss: 11.017665
Action reg: 0.003977
  l1.weight: grad_norm = 0.092070
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.079310
Total gradient norm: 0.229548
=== Actor Training Debug (Iteration 4642) ===
Q mean: -10.206793
Q std: 15.976153
Actor loss: 10.210756
Action reg: 0.003963
  l1.weight: grad_norm = 0.130663
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.110895
Total gradient norm: 0.315653
=== Actor Training Debug (Iteration 4643) ===
Q mean: -10.795633
Q std: 14.108889
Actor loss: 10.799616
Action reg: 0.003983
  l1.weight: grad_norm = 0.118702
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.105817
Total gradient norm: 0.325386
=== Actor Training Debug (Iteration 4644) ===
Q mean: -11.572697
Q std: 15.792815
Actor loss: 11.576658
Action reg: 0.003961
  l1.weight: grad_norm = 0.211872
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.175536
Total gradient norm: 0.531513
=== Actor Training Debug (Iteration 4645) ===
Q mean: -10.845357
Q std: 14.478827
Actor loss: 10.849336
Action reg: 0.003979
  l1.weight: grad_norm = 0.055672
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.055156
Total gradient norm: 0.178007
=== Actor Training Debug (Iteration 4646) ===
Q mean: -11.886941
Q std: 16.561386
Actor loss: 11.890919
Action reg: 0.003977
  l1.weight: grad_norm = 0.145707
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.115657
Total gradient norm: 0.365889
=== Actor Training Debug (Iteration 4647) ===
Q mean: -10.115868
Q std: 14.430349
Actor loss: 10.119833
Action reg: 0.003965
  l1.weight: grad_norm = 0.123881
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.108568
Total gradient norm: 0.343865
=== Actor Training Debug (Iteration 4648) ===
Q mean: -10.177780
Q std: 15.073875
Actor loss: 10.181762
Action reg: 0.003981
  l1.weight: grad_norm = 0.191753
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.168741
Total gradient norm: 0.459128
=== Actor Training Debug (Iteration 4649) ===
Q mean: -9.797266
Q std: 14.180005
Actor loss: 9.801236
Action reg: 0.003970
  l1.weight: grad_norm = 0.129962
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.115741
Total gradient norm: 0.302169
=== Actor Training Debug (Iteration 4650) ===
Q mean: -9.729181
Q std: 13.370245
Actor loss: 9.733153
Action reg: 0.003972
  l1.weight: grad_norm = 0.090852
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.077585
Total gradient norm: 0.250559
=== Actor Training Debug (Iteration 4651) ===
Q mean: -10.553598
Q std: 14.434898
Actor loss: 10.557552
Action reg: 0.003954
  l1.weight: grad_norm = 0.166539
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.154596
Total gradient norm: 0.479984
=== Actor Training Debug (Iteration 4652) ===
Q mean: -9.416707
Q std: 14.541083
Actor loss: 9.420680
Action reg: 0.003973
  l1.weight: grad_norm = 0.159049
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.143117
Total gradient norm: 0.365863
=== Actor Training Debug (Iteration 4653) ===
Q mean: -8.754004
Q std: 13.688911
Actor loss: 8.757974
Action reg: 0.003969
  l1.weight: grad_norm = 0.201148
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.154889
Total gradient norm: 0.558929
=== Actor Training Debug (Iteration 4654) ===
Q mean: -9.892896
Q std: 14.567222
Actor loss: 9.896867
Action reg: 0.003972
  l1.weight: grad_norm = 0.084455
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.076007
Total gradient norm: 0.233234
=== Actor Training Debug (Iteration 4655) ===
Q mean: -9.568437
Q std: 14.298072
Actor loss: 9.572414
Action reg: 0.003978
  l1.weight: grad_norm = 0.103285
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.091692
Total gradient norm: 0.286026
=== Actor Training Debug (Iteration 4656) ===
Q mean: -12.229084
Q std: 16.044577
Actor loss: 12.233054
Action reg: 0.003970
  l1.weight: grad_norm = 0.120103
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.106298
Total gradient norm: 0.338863
=== Actor Training Debug (Iteration 4657) ===
Q mean: -9.929035
Q std: 13.821101
Actor loss: 9.933016
Action reg: 0.003980
  l1.weight: grad_norm = 0.130213
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.111466
Total gradient norm: 0.312657
=== Actor Training Debug (Iteration 4658) ===
Q mean: -8.974868
Q std: 13.449389
Actor loss: 8.978844
Action reg: 0.003975
  l1.weight: grad_norm = 0.160367
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.137467
Total gradient norm: 0.443387
=== Actor Training Debug (Iteration 4659) ===
Q mean: -10.073987
Q std: 13.734818
Actor loss: 10.077950
Action reg: 0.003964
  l1.weight: grad_norm = 0.102155
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.087011
Total gradient norm: 0.314531
=== Actor Training Debug (Iteration 4660) ===
Q mean: -10.179296
Q std: 14.374039
Actor loss: 10.183261
Action reg: 0.003964
  l1.weight: grad_norm = 0.263118
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.193828
Total gradient norm: 0.569850
=== Actor Training Debug (Iteration 4661) ===
Q mean: -10.333023
Q std: 14.306730
Actor loss: 10.337008
Action reg: 0.003984
  l1.weight: grad_norm = 0.059586
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.048618
Total gradient norm: 0.146649
=== Actor Training Debug (Iteration 4662) ===
Q mean: -10.142654
Q std: 14.408058
Actor loss: 10.146637
Action reg: 0.003982
  l1.weight: grad_norm = 0.103504
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.083770
Total gradient norm: 0.257021
=== Actor Training Debug (Iteration 4663) ===
Q mean: -10.807877
Q std: 14.052311
Actor loss: 10.811831
Action reg: 0.003954
  l1.weight: grad_norm = 0.151090
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.129086
Total gradient norm: 0.408195
=== Actor Training Debug (Iteration 4664) ===
Q mean: -10.721006
Q std: 14.592258
Actor loss: 10.724968
Action reg: 0.003961
  l1.weight: grad_norm = 0.325733
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.278966
Total gradient norm: 0.742100
=== Actor Training Debug (Iteration 4665) ===
Q mean: -9.847195
Q std: 15.006503
Actor loss: 9.851171
Action reg: 0.003976
  l1.weight: grad_norm = 0.125722
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.102459
Total gradient norm: 0.323393
=== Actor Training Debug (Iteration 4666) ===
Q mean: -11.067930
Q std: 14.110547
Actor loss: 11.071906
Action reg: 0.003976
  l1.weight: grad_norm = 0.129123
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.113863
Total gradient norm: 0.303461
=== Actor Training Debug (Iteration 4667) ===
Q mean: -10.443123
Q std: 15.217773
Actor loss: 10.447104
Action reg: 0.003981
  l1.weight: grad_norm = 0.062727
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.053705
Total gradient norm: 0.178144
=== Actor Training Debug (Iteration 4668) ===
Q mean: -10.304613
Q std: 14.848400
Actor loss: 10.308581
Action reg: 0.003969
  l1.weight: grad_norm = 0.239139
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.233839
Total gradient norm: 0.768030
=== Actor Training Debug (Iteration 4669) ===
Q mean: -9.712482
Q std: 13.592098
Actor loss: 9.716434
Action reg: 0.003951
  l1.weight: grad_norm = 0.204302
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.174558
Total gradient norm: 0.578307
=== Actor Training Debug (Iteration 4670) ===
Q mean: -8.900993
Q std: 13.908367
Actor loss: 8.904965
Action reg: 0.003972
  l1.weight: grad_norm = 0.121528
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.120757
Total gradient norm: 0.436263
=== Actor Training Debug (Iteration 4671) ===
Q mean: -12.173304
Q std: 16.402596
Actor loss: 12.177279
Action reg: 0.003975
  l1.weight: grad_norm = 0.082246
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.064635
Total gradient norm: 0.190179
=== Actor Training Debug (Iteration 4672) ===
Q mean: -12.176147
Q std: 15.425685
Actor loss: 12.180120
Action reg: 0.003973
  l1.weight: grad_norm = 0.125151
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.103671
Total gradient norm: 0.423633
=== Actor Training Debug (Iteration 4673) ===
Q mean: -10.515030
Q std: 15.944227
Actor loss: 10.519000
Action reg: 0.003970
  l1.weight: grad_norm = 0.141801
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.105457
Total gradient norm: 0.360246
=== Actor Training Debug (Iteration 4674) ===
Q mean: -10.254477
Q std: 15.958289
Actor loss: 10.258445
Action reg: 0.003968
  l1.weight: grad_norm = 0.297440
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.259283
Total gradient norm: 0.694257
=== Actor Training Debug (Iteration 4675) ===
Q mean: -10.030422
Q std: 15.179009
Actor loss: 10.034389
Action reg: 0.003968
  l1.weight: grad_norm = 0.089404
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.078840
Total gradient norm: 0.234600
=== Actor Training Debug (Iteration 4676) ===
Q mean: -10.039932
Q std: 14.435958
Actor loss: 10.043898
Action reg: 0.003965
  l1.weight: grad_norm = 0.120663
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.104253
Total gradient norm: 0.284426
=== Actor Training Debug (Iteration 4677) ===
Q mean: -12.387117
Q std: 16.081030
Actor loss: 12.391099
Action reg: 0.003982
  l1.weight: grad_norm = 0.163923
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.113210
Total gradient norm: 0.360466
=== Actor Training Debug (Iteration 4678) ===
Q mean: -11.520575
Q std: 15.395836
Actor loss: 11.524547
Action reg: 0.003972
  l1.weight: grad_norm = 0.085508
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.076598
Total gradient norm: 0.233819
=== Actor Training Debug (Iteration 4679) ===
Q mean: -11.555378
Q std: 14.887907
Actor loss: 11.559355
Action reg: 0.003977
  l1.weight: grad_norm = 0.122569
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.105765
Total gradient norm: 0.318754
=== Actor Training Debug (Iteration 4680) ===
Q mean: -10.622927
Q std: 13.934699
Actor loss: 10.626897
Action reg: 0.003970
  l1.weight: grad_norm = 0.118194
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.108764
Total gradient norm: 0.334505
=== Actor Training Debug (Iteration 4681) ===
Q mean: -10.040208
Q std: 13.942798
Actor loss: 10.044185
Action reg: 0.003977
  l1.weight: grad_norm = 0.107485
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.092873
Total gradient norm: 0.267642
=== Actor Training Debug (Iteration 4682) ===
Q mean: -8.782607
Q std: 13.562789
Actor loss: 8.786580
Action reg: 0.003973
  l1.weight: grad_norm = 0.202237
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.172066
Total gradient norm: 0.476718
=== Actor Training Debug (Iteration 4683) ===
Q mean: -11.303122
Q std: 16.474892
Actor loss: 11.307098
Action reg: 0.003976
  l1.weight: grad_norm = 0.065430
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.058967
Total gradient norm: 0.174968
=== Actor Training Debug (Iteration 4684) ===
Q mean: -10.316468
Q std: 14.775281
Actor loss: 10.320448
Action reg: 0.003979
  l1.weight: grad_norm = 0.141005
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.123113
Total gradient norm: 0.333713
=== Actor Training Debug (Iteration 4685) ===
Q mean: -9.264889
Q std: 14.552683
Actor loss: 9.268873
Action reg: 0.003984
  l1.weight: grad_norm = 0.115078
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.104557
Total gradient norm: 0.358965
=== Actor Training Debug (Iteration 4686) ===
Q mean: -9.915699
Q std: 15.169148
Actor loss: 9.919654
Action reg: 0.003955
  l1.weight: grad_norm = 0.160554
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.131177
Total gradient norm: 0.379546
=== Actor Training Debug (Iteration 4687) ===
Q mean: -10.440319
Q std: 13.418024
Actor loss: 10.444295
Action reg: 0.003976
  l1.weight: grad_norm = 0.113191
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.112057
Total gradient norm: 0.282228
=== Actor Training Debug (Iteration 4688) ===
Q mean: -9.104870
Q std: 14.009526
Actor loss: 9.108828
Action reg: 0.003958
  l1.weight: grad_norm = 0.109698
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.097350
Total gradient norm: 0.280439
=== Actor Training Debug (Iteration 4689) ===
Q mean: -10.346437
Q std: 13.933031
Actor loss: 10.350393
Action reg: 0.003957
  l1.weight: grad_norm = 0.166269
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.139417
Total gradient norm: 0.382256
=== Actor Training Debug (Iteration 4690) ===
Q mean: -11.826834
Q std: 15.298327
Actor loss: 11.830801
Action reg: 0.003967
  l1.weight: grad_norm = 0.082805
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.075463
Total gradient norm: 0.256269
=== Actor Training Debug (Iteration 4691) ===
Q mean: -10.862780
Q std: 14.342631
Actor loss: 10.866752
Action reg: 0.003972
  l1.weight: grad_norm = 0.116117
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.102409
Total gradient norm: 0.382665
=== Actor Training Debug (Iteration 4692) ===
Q mean: -12.200888
Q std: 16.299904
Actor loss: 12.204859
Action reg: 0.003971
  l1.weight: grad_norm = 0.094827
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.087812
Total gradient norm: 0.265528
=== Actor Training Debug (Iteration 4693) ===
Q mean: -8.938802
Q std: 13.086776
Actor loss: 8.942767
Action reg: 0.003965
  l1.weight: grad_norm = 0.258916
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.209228
Total gradient norm: 0.777118
=== Actor Training Debug (Iteration 4694) ===
Q mean: -11.099487
Q std: 14.744199
Actor loss: 11.103459
Action reg: 0.003972
  l1.weight: grad_norm = 0.117764
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.094583
Total gradient norm: 0.298544
=== Actor Training Debug (Iteration 4695) ===
Q mean: -9.141783
Q std: 14.424560
Actor loss: 9.145755
Action reg: 0.003972
  l1.weight: grad_norm = 0.121435
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.104181
Total gradient norm: 0.343173
=== Actor Training Debug (Iteration 4696) ===
Q mean: -9.738490
Q std: 13.297224
Actor loss: 9.742451
Action reg: 0.003961
  l1.weight: grad_norm = 0.261594
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.251507
Total gradient norm: 0.902184
=== Actor Training Debug (Iteration 4697) ===
Q mean: -10.415960
Q std: 14.698613
Actor loss: 10.419934
Action reg: 0.003974
  l1.weight: grad_norm = 0.147264
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.121316
Total gradient norm: 0.382088
=== Actor Training Debug (Iteration 4698) ===
Q mean: -11.058849
Q std: 14.385592
Actor loss: 11.062824
Action reg: 0.003975
  l1.weight: grad_norm = 0.147971
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.139403
Total gradient norm: 0.390881
=== Actor Training Debug (Iteration 4699) ===
Q mean: -11.874848
Q std: 14.407141
Actor loss: 11.878820
Action reg: 0.003972
  l1.weight: grad_norm = 0.235634
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.196468
Total gradient norm: 0.642552
=== Actor Training Debug (Iteration 4700) ===
Q mean: -10.363949
Q std: 14.100883
Actor loss: 10.367913
Action reg: 0.003964
  l1.weight: grad_norm = 0.206913
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.184093
Total gradient norm: 0.542279
=== Actor Training Debug (Iteration 4701) ===
Q mean: -10.615596
Q std: 16.401749
Actor loss: 10.619561
Action reg: 0.003965
  l1.weight: grad_norm = 0.115500
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.095738
Total gradient norm: 0.271761
=== Actor Training Debug (Iteration 4702) ===
Q mean: -11.709537
Q std: 15.579840
Actor loss: 11.713508
Action reg: 0.003971
  l1.weight: grad_norm = 0.099586
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.089253
Total gradient norm: 0.286278
=== Actor Training Debug (Iteration 4703) ===
Q mean: -9.502522
Q std: 14.154039
Actor loss: 9.506483
Action reg: 0.003961
  l1.weight: grad_norm = 0.158387
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.134067
Total gradient norm: 0.389087
=== Actor Training Debug (Iteration 4704) ===
Q mean: -9.622101
Q std: 14.566235
Actor loss: 9.626068
Action reg: 0.003967
  l1.weight: grad_norm = 0.151660
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.133245
Total gradient norm: 0.398727
=== Actor Training Debug (Iteration 4705) ===
Q mean: -11.490597
Q std: 16.380915
Actor loss: 11.494576
Action reg: 0.003980
  l1.weight: grad_norm = 0.059445
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.056953
Total gradient norm: 0.196524
=== Actor Training Debug (Iteration 4706) ===
Q mean: -11.024174
Q std: 13.808216
Actor loss: 11.028144
Action reg: 0.003970
  l1.weight: grad_norm = 0.150633
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.145283
Total gradient norm: 0.395615
=== Actor Training Debug (Iteration 4707) ===
Q mean: -10.179471
Q std: 14.022990
Actor loss: 10.183445
Action reg: 0.003974
  l1.weight: grad_norm = 0.092103
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.087858
Total gradient norm: 0.256925
=== Actor Training Debug (Iteration 4708) ===
Q mean: -9.669140
Q std: 13.189478
Actor loss: 9.673118
Action reg: 0.003978
  l1.weight: grad_norm = 0.109177
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.098795
Total gradient norm: 0.357991
=== Actor Training Debug (Iteration 4709) ===
Q mean: -12.054224
Q std: 17.180679
Actor loss: 12.058185
Action reg: 0.003961
  l1.weight: grad_norm = 0.234401
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.212122
Total gradient norm: 0.644267
=== Actor Training Debug (Iteration 4710) ===
Q mean: -12.000202
Q std: 16.448154
Actor loss: 12.004164
Action reg: 0.003962
  l1.weight: grad_norm = 0.266583
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.230646
Total gradient norm: 0.646270
=== Actor Training Debug (Iteration 4711) ===
Q mean: -10.832417
Q std: 14.694825
Actor loss: 10.836388
Action reg: 0.003971
  l1.weight: grad_norm = 0.133261
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.121360
Total gradient norm: 0.413661
=== Actor Training Debug (Iteration 4712) ===
Q mean: -11.113581
Q std: 14.984818
Actor loss: 11.117552
Action reg: 0.003971
  l1.weight: grad_norm = 0.110942
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.099311
Total gradient norm: 0.289220
=== Actor Training Debug (Iteration 4713) ===
Q mean: -11.254992
Q std: 15.744156
Actor loss: 11.258967
Action reg: 0.003975
  l1.weight: grad_norm = 0.076233
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.075355
Total gradient norm: 0.240037
=== Actor Training Debug (Iteration 4714) ===
Q mean: -9.834044
Q std: 13.467931
Actor loss: 9.838014
Action reg: 0.003970
  l1.weight: grad_norm = 0.089451
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.074429
Total gradient norm: 0.205318
=== Actor Training Debug (Iteration 4715) ===
Q mean: -10.660662
Q std: 14.392509
Actor loss: 10.664638
Action reg: 0.003976
  l1.weight: grad_norm = 0.083039
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.072016
Total gradient norm: 0.238226
=== Actor Training Debug (Iteration 4716) ===
Q mean: -10.009851
Q std: 14.387955
Actor loss: 10.013804
Action reg: 0.003953
  l1.weight: grad_norm = 0.127351
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.116887
Total gradient norm: 0.345445
=== Actor Training Debug (Iteration 4717) ===
Q mean: -11.342523
Q std: 15.026535
Actor loss: 11.346498
Action reg: 0.003975
  l1.weight: grad_norm = 0.131381
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.132764
Total gradient norm: 0.456765
=== Actor Training Debug (Iteration 4718) ===
Q mean: -10.116920
Q std: 14.302760
Actor loss: 10.120888
Action reg: 0.003967
  l1.weight: grad_norm = 0.131761
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.115537
Total gradient norm: 0.353297
=== Actor Training Debug (Iteration 4719) ===
Q mean: -11.299246
Q std: 14.692690
Actor loss: 11.303211
Action reg: 0.003965
  l1.weight: grad_norm = 0.110498
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.103747
Total gradient norm: 0.363180
=== Actor Training Debug (Iteration 4720) ===
Q mean: -10.122952
Q std: 14.803715
Actor loss: 10.126920
Action reg: 0.003968
  l1.weight: grad_norm = 0.136315
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.123567
Total gradient norm: 0.336225
=== Actor Training Debug (Iteration 4721) ===
Q mean: -11.616205
Q std: 15.459191
Actor loss: 11.620176
Action reg: 0.003971
  l1.weight: grad_norm = 0.160723
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.157711
Total gradient norm: 0.431995
=== Actor Training Debug (Iteration 4722) ===
Q mean: -9.880363
Q std: 15.250552
Actor loss: 9.884341
Action reg: 0.003979
  l1.weight: grad_norm = 0.144980
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.127797
Total gradient norm: 0.536624
=== Actor Training Debug (Iteration 4723) ===
Q mean: -11.057706
Q std: 16.287996
Actor loss: 11.061678
Action reg: 0.003972
  l1.weight: grad_norm = 0.099213
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.074153
Total gradient norm: 0.237019
=== Actor Training Debug (Iteration 4724) ===
Q mean: -10.041310
Q std: 14.212069
Actor loss: 10.045286
Action reg: 0.003976
  l1.weight: grad_norm = 0.121435
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.107164
Total gradient norm: 0.300542
=== Actor Training Debug (Iteration 4725) ===
Q mean: -10.733849
Q std: 15.508308
Actor loss: 10.737831
Action reg: 0.003983
  l1.weight: grad_norm = 0.056157
  l1.bias: grad_norm = 0.000022
  l2.weight: grad_norm = 0.054044
Total gradient norm: 0.186397
=== Actor Training Debug (Iteration 4726) ===
Q mean: -9.737255
Q std: 12.988044
Actor loss: 9.741210
Action reg: 0.003955
  l1.weight: grad_norm = 0.219779
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.186342
Total gradient norm: 0.696038
=== Actor Training Debug (Iteration 4727) ===
Q mean: -10.611122
Q std: 14.294868
Actor loss: 10.615096
Action reg: 0.003974
  l1.weight: grad_norm = 0.070011
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.061494
Total gradient norm: 0.206425
=== Actor Training Debug (Iteration 4728) ===
Q mean: -12.026140
Q std: 15.934457
Actor loss: 12.030120
Action reg: 0.003980
  l1.weight: grad_norm = 0.074727
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.064726
Total gradient norm: 0.176446
=== Actor Training Debug (Iteration 4729) ===
Q mean: -8.588735
Q std: 12.737328
Actor loss: 8.592695
Action reg: 0.003961
  l1.weight: grad_norm = 0.230059
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.190189
Total gradient norm: 0.603354
=== Actor Training Debug (Iteration 4730) ===
Q mean: -10.206364
Q std: 15.048240
Actor loss: 10.210324
Action reg: 0.003960
  l1.weight: grad_norm = 0.166471
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.128603
Total gradient norm: 0.425902
=== Actor Training Debug (Iteration 4731) ===
Q mean: -10.517288
Q std: 15.021811
Actor loss: 10.521259
Action reg: 0.003971
  l1.weight: grad_norm = 0.170905
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.166870
Total gradient norm: 0.506684
=== Actor Training Debug (Iteration 4732) ===
Q mean: -11.119062
Q std: 14.954363
Actor loss: 11.123042
Action reg: 0.003980
  l1.weight: grad_norm = 0.153403
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.131989
Total gradient norm: 0.428426
=== Actor Training Debug (Iteration 4733) ===
Q mean: -10.181877
Q std: 14.504295
Actor loss: 10.185844
Action reg: 0.003967
  l1.weight: grad_norm = 0.066891
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.063800
Total gradient norm: 0.181469
=== Actor Training Debug (Iteration 4734) ===
Q mean: -12.129417
Q std: 15.635669
Actor loss: 12.133397
Action reg: 0.003979
  l1.weight: grad_norm = 0.049198
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.043742
Total gradient norm: 0.153348
=== Actor Training Debug (Iteration 4735) ===
Q mean: -10.667403
Q std: 14.283247
Actor loss: 10.671372
Action reg: 0.003969
  l1.weight: grad_norm = 0.089944
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.067227
Total gradient norm: 0.199927
=== Actor Training Debug (Iteration 4736) ===
Q mean: -10.747789
Q std: 13.482079
Actor loss: 10.751761
Action reg: 0.003972
  l1.weight: grad_norm = 0.062277
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.050471
Total gradient norm: 0.148451
=== Actor Training Debug (Iteration 4737) ===
Q mean: -12.165130
Q std: 14.767550
Actor loss: 12.169101
Action reg: 0.003971
  l1.weight: grad_norm = 0.051708
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.053665
Total gradient norm: 0.205352
=== Actor Training Debug (Iteration 4738) ===
Q mean: -11.262709
Q std: 13.564865
Actor loss: 11.266684
Action reg: 0.003975
  l1.weight: grad_norm = 0.093524
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.078336
Total gradient norm: 0.298816
=== Actor Training Debug (Iteration 4739) ===
Q mean: -9.749558
Q std: 14.154297
Actor loss: 9.753503
Action reg: 0.003945
  l1.weight: grad_norm = 0.198732
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.165947
Total gradient norm: 0.463555
=== Actor Training Debug (Iteration 4740) ===
Q mean: -11.385509
Q std: 14.289503
Actor loss: 11.389487
Action reg: 0.003978
  l1.weight: grad_norm = 0.091215
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.087345
Total gradient norm: 0.267921
=== Actor Training Debug (Iteration 4741) ===
Q mean: -13.020205
Q std: 15.833160
Actor loss: 13.024172
Action reg: 0.003968
  l1.weight: grad_norm = 0.157242
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.144251
Total gradient norm: 0.421283
=== Actor Training Debug (Iteration 4742) ===
Q mean: -10.976315
Q std: 15.587816
Actor loss: 10.980284
Action reg: 0.003968
  l1.weight: grad_norm = 0.147848
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.143268
Total gradient norm: 0.424067
=== Actor Training Debug (Iteration 4743) ===
Q mean: -11.151102
Q std: 15.286412
Actor loss: 11.155081
Action reg: 0.003979
  l1.weight: grad_norm = 0.072284
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.057104
Total gradient norm: 0.149836
=== Actor Training Debug (Iteration 4744) ===
Q mean: -10.175020
Q std: 15.787952
Actor loss: 10.179002
Action reg: 0.003982
  l1.weight: grad_norm = 0.086434
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.070773
Total gradient norm: 0.210906
=== Actor Training Debug (Iteration 4745) ===
Q mean: -10.299427
Q std: 14.057654
Actor loss: 10.303385
Action reg: 0.003958
  l1.weight: grad_norm = 0.073521
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.072852
Total gradient norm: 0.239844
=== Actor Training Debug (Iteration 4746) ===
Q mean: -10.234344
Q std: 14.625310
Actor loss: 10.238306
Action reg: 0.003962
  l1.weight: grad_norm = 0.261075
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.213718
Total gradient norm: 0.621731
=== Actor Training Debug (Iteration 4747) ===
Q mean: -11.635180
Q std: 15.456351
Actor loss: 11.639148
Action reg: 0.003969
  l1.weight: grad_norm = 0.083402
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.066980
Total gradient norm: 0.201223
=== Actor Training Debug (Iteration 4748) ===
Q mean: -10.593890
Q std: 16.494057
Actor loss: 10.597845
Action reg: 0.003955
  l1.weight: grad_norm = 0.233904
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.186374
Total gradient norm: 0.549120
=== Actor Training Debug (Iteration 4749) ===
Q mean: -10.661466
Q std: 13.597527
Actor loss: 10.665442
Action reg: 0.003976
  l1.weight: grad_norm = 0.196674
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.166105
Total gradient norm: 0.470441
=== Actor Training Debug (Iteration 4750) ===
Q mean: -10.285883
Q std: 14.914788
Actor loss: 10.289857
Action reg: 0.003974
  l1.weight: grad_norm = 0.205671
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.168707
Total gradient norm: 0.497770
=== Actor Training Debug (Iteration 4751) ===
Q mean: -9.235006
Q std: 14.866214
Actor loss: 9.238983
Action reg: 0.003977
  l1.weight: grad_norm = 0.218838
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.185497
Total gradient norm: 0.470196
=== Actor Training Debug (Iteration 4752) ===
Q mean: -9.521935
Q std: 14.253242
Actor loss: 9.525908
Action reg: 0.003973
  l1.weight: grad_norm = 0.086044
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.072477
Total gradient norm: 0.212512
=== Actor Training Debug (Iteration 4753) ===
Q mean: -12.305090
Q std: 15.739730
Actor loss: 12.309062
Action reg: 0.003972
  l1.weight: grad_norm = 0.133128
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.123788
Total gradient norm: 0.447128
=== Actor Training Debug (Iteration 4754) ===
Q mean: -9.940780
Q std: 13.901361
Actor loss: 9.944739
Action reg: 0.003960
  l1.weight: grad_norm = 0.219560
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.174669
Total gradient norm: 0.487910
=== Actor Training Debug (Iteration 4755) ===
Q mean: -10.271749
Q std: 14.459835
Actor loss: 10.275724
Action reg: 0.003975
  l1.weight: grad_norm = 0.099239
  l1.bias: grad_norm = 0.000613
  l2.weight: grad_norm = 0.086036
Total gradient norm: 0.250535
=== Actor Training Debug (Iteration 4756) ===
Q mean: -10.664545
Q std: 16.593262
Actor loss: 10.668505
Action reg: 0.003959
  l1.weight: grad_norm = 0.128934
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.104172
Total gradient norm: 0.333646
=== Actor Training Debug (Iteration 4757) ===
Q mean: -10.582737
Q std: 14.043438
Actor loss: 10.586701
Action reg: 0.003964
  l1.weight: grad_norm = 0.101668
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.094091
Total gradient norm: 0.300183
=== Actor Training Debug (Iteration 4758) ===
Q mean: -8.609146
Q std: 13.879616
Actor loss: 8.613111
Action reg: 0.003965
  l1.weight: grad_norm = 0.128752
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.098970
Total gradient norm: 0.323745
=== Actor Training Debug (Iteration 4759) ===
Q mean: -11.065792
Q std: 16.255417
Actor loss: 11.069757
Action reg: 0.003965
  l1.weight: grad_norm = 0.227658
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.194096
Total gradient norm: 0.666960
=== Actor Training Debug (Iteration 4760) ===
Q mean: -9.541412
Q std: 15.289207
Actor loss: 9.545374
Action reg: 0.003961
  l1.weight: grad_norm = 0.051708
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.046266
Total gradient norm: 0.146021
=== Actor Training Debug (Iteration 4761) ===
Q mean: -10.449405
Q std: 15.356034
Actor loss: 10.453386
Action reg: 0.003982
  l1.weight: grad_norm = 0.118572
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.098680
Total gradient norm: 0.276959
=== Actor Training Debug (Iteration 4762) ===
Q mean: -13.074617
Q std: 16.942314
Actor loss: 13.078588
Action reg: 0.003971
  l1.weight: grad_norm = 0.095498
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.090540
Total gradient norm: 0.257521
=== Actor Training Debug (Iteration 4763) ===
Q mean: -9.722858
Q std: 14.896720
Actor loss: 9.726812
Action reg: 0.003954
  l1.weight: grad_norm = 0.167126
  l1.bias: grad_norm = 0.000605
  l2.weight: grad_norm = 0.124627
Total gradient norm: 0.352042
=== Actor Training Debug (Iteration 4764) ===
Q mean: -9.227658
Q std: 13.273260
Actor loss: 9.231625
Action reg: 0.003967
  l1.weight: grad_norm = 0.113646
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.097274
Total gradient norm: 0.311404
=== Actor Training Debug (Iteration 4765) ===
Q mean: -10.608683
Q std: 14.222967
Actor loss: 10.612660
Action reg: 0.003978
  l1.weight: grad_norm = 0.142356
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.125658
Total gradient norm: 0.391984
=== Actor Training Debug (Iteration 4766) ===
Q mean: -11.706367
Q std: 15.053904
Actor loss: 11.710335
Action reg: 0.003968
  l1.weight: grad_norm = 0.143984
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.124892
Total gradient norm: 0.382031
=== Actor Training Debug (Iteration 4767) ===
Q mean: -12.068599
Q std: 15.647799
Actor loss: 12.072565
Action reg: 0.003966
  l1.weight: grad_norm = 0.116527
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.106119
Total gradient norm: 0.288686
=== Actor Training Debug (Iteration 4768) ===
Q mean: -12.636433
Q std: 17.372505
Actor loss: 12.640402
Action reg: 0.003970
  l1.weight: grad_norm = 0.101379
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.089634
Total gradient norm: 0.253174
=== Actor Training Debug (Iteration 4769) ===
Q mean: -9.258976
Q std: 12.784865
Actor loss: 9.262933
Action reg: 0.003957
  l1.weight: grad_norm = 0.135107
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.122234
Total gradient norm: 0.382049
=== Actor Training Debug (Iteration 4770) ===
Q mean: -10.256798
Q std: 15.019414
Actor loss: 10.260773
Action reg: 0.003975
  l1.weight: grad_norm = 0.162520
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.126818
Total gradient norm: 0.433364
=== Actor Training Debug (Iteration 4771) ===
Q mean: -8.792827
Q std: 13.658236
Actor loss: 8.796779
Action reg: 0.003952
  l1.weight: grad_norm = 0.535225
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.405224
Total gradient norm: 1.308902
=== Actor Training Debug (Iteration 4772) ===
Q mean: -11.169125
Q std: 14.543330
Actor loss: 11.173089
Action reg: 0.003964
  l1.weight: grad_norm = 0.075238
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.065000
Total gradient norm: 0.199596
=== Actor Training Debug (Iteration 4773) ===
Q mean: -10.627228
Q std: 15.409196
Actor loss: 10.631207
Action reg: 0.003980
  l1.weight: grad_norm = 0.077936
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.059056
Total gradient norm: 0.180188
=== Actor Training Debug (Iteration 4774) ===
Q mean: -9.143612
Q std: 13.046209
Actor loss: 9.147583
Action reg: 0.003971
  l1.weight: grad_norm = 0.056221
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.050452
Total gradient norm: 0.144813
=== Actor Training Debug (Iteration 4775) ===
Q mean: -12.061846
Q std: 15.657837
Actor loss: 12.065805
Action reg: 0.003960
  l1.weight: grad_norm = 0.100993
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.083508
Total gradient norm: 0.275754
=== Actor Training Debug (Iteration 4776) ===
Q mean: -10.179756
Q std: 13.989261
Actor loss: 10.183724
Action reg: 0.003968
  l1.weight: grad_norm = 0.196669
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.148074
Total gradient norm: 0.468048
=== Actor Training Debug (Iteration 4777) ===
Q mean: -10.560650
Q std: 14.320514
Actor loss: 10.564610
Action reg: 0.003960
  l1.weight: grad_norm = 0.224654
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.191061
Total gradient norm: 0.519385
=== Actor Training Debug (Iteration 4778) ===
Q mean: -11.236225
Q std: 14.426685
Actor loss: 11.240189
Action reg: 0.003964
  l1.weight: grad_norm = 0.271234
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.217475
Total gradient norm: 0.722896
=== Actor Training Debug (Iteration 4779) ===
Q mean: -9.981302
Q std: 14.299382
Actor loss: 9.985263
Action reg: 0.003961
  l1.weight: grad_norm = 0.231123
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.173390
Total gradient norm: 0.573856
=== Actor Training Debug (Iteration 4780) ===
Q mean: -11.717808
Q std: 16.226852
Actor loss: 11.721766
Action reg: 0.003958
  l1.weight: grad_norm = 0.222430
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.173840
Total gradient norm: 0.461638
=== Actor Training Debug (Iteration 4781) ===
Q mean: -10.750838
Q std: 14.177176
Actor loss: 10.754810
Action reg: 0.003972
  l1.weight: grad_norm = 0.120007
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.111713
Total gradient norm: 0.312784
=== Actor Training Debug (Iteration 4782) ===
Q mean: -12.408389
Q std: 15.159531
Actor loss: 12.412364
Action reg: 0.003975
  l1.weight: grad_norm = 0.277177
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.203298
Total gradient norm: 0.640535
=== Actor Training Debug (Iteration 4783) ===
Q mean: -9.491631
Q std: 13.804286
Actor loss: 9.495615
Action reg: 0.003985
  l1.weight: grad_norm = 0.124170
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.110850
Total gradient norm: 0.328518
=== Actor Training Debug (Iteration 4784) ===
Q mean: -9.760155
Q std: 14.607937
Actor loss: 9.764126
Action reg: 0.003971
  l1.weight: grad_norm = 0.154272
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.137237
Total gradient norm: 0.396126
=== Actor Training Debug (Iteration 4785) ===
Q mean: -12.726048
Q std: 15.882185
Actor loss: 12.730022
Action reg: 0.003974
  l1.weight: grad_norm = 0.101712
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.092187
Total gradient norm: 0.259886
=== Actor Training Debug (Iteration 4786) ===
Q mean: -10.081316
Q std: 15.906086
Actor loss: 10.085286
Action reg: 0.003971
  l1.weight: grad_norm = 0.176790
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.157321
Total gradient norm: 0.465510
=== Actor Training Debug (Iteration 4787) ===
Q mean: -9.906399
Q std: 14.474717
Actor loss: 9.910356
Action reg: 0.003957
  l1.weight: grad_norm = 0.111430
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.096348
Total gradient norm: 0.288598
=== Actor Training Debug (Iteration 4788) ===
Q mean: -11.580807
Q std: 16.349390
Actor loss: 11.584774
Action reg: 0.003968
  l1.weight: grad_norm = 0.066538
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.058590
Total gradient norm: 0.162248
=== Actor Training Debug (Iteration 4789) ===
Q mean: -12.817379
Q std: 15.506096
Actor loss: 12.821331
Action reg: 0.003953
  l1.weight: grad_norm = 0.146925
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.131137
Total gradient norm: 0.465089
=== Actor Training Debug (Iteration 4790) ===
Q mean: -8.899868
Q std: 13.852672
Actor loss: 8.903829
Action reg: 0.003961
  l1.weight: grad_norm = 0.229438
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.195697
Total gradient norm: 0.612076
=== Actor Training Debug (Iteration 4791) ===
Q mean: -10.953424
Q std: 17.443518
Actor loss: 10.957395
Action reg: 0.003970
  l1.weight: grad_norm = 0.067917
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.050761
Total gradient norm: 0.167465
=== Actor Training Debug (Iteration 4792) ===
Q mean: -11.828926
Q std: 15.532526
Actor loss: 11.832902
Action reg: 0.003976
  l1.weight: grad_norm = 0.114755
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.101482
Total gradient norm: 0.296299
=== Actor Training Debug (Iteration 4793) ===
Q mean: -11.388584
Q std: 14.925089
Actor loss: 11.392548
Action reg: 0.003964
  l1.weight: grad_norm = 0.160421
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.145526
Total gradient norm: 0.442252
=== Actor Training Debug (Iteration 4794) ===
Q mean: -10.886095
Q std: 15.014503
Actor loss: 10.890066
Action reg: 0.003971
  l1.weight: grad_norm = 0.137301
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.119128
Total gradient norm: 0.367563
=== Actor Training Debug (Iteration 4795) ===
Q mean: -9.884781
Q std: 14.461804
Actor loss: 9.888743
Action reg: 0.003962
  l1.weight: grad_norm = 0.108412
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.096618
Total gradient norm: 0.250227
=== Actor Training Debug (Iteration 4796) ===
Q mean: -11.563292
Q std: 15.654302
Actor loss: 11.567266
Action reg: 0.003974
  l1.weight: grad_norm = 0.098264
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.090333
Total gradient norm: 0.293663
=== Actor Training Debug (Iteration 4797) ===
Q mean: -12.854156
Q std: 15.969123
Actor loss: 12.858130
Action reg: 0.003974
  l1.weight: grad_norm = 0.211035
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.177148
Total gradient norm: 0.558538
=== Actor Training Debug (Iteration 4798) ===
Q mean: -11.307798
Q std: 15.387298
Actor loss: 11.311768
Action reg: 0.003969
  l1.weight: grad_norm = 0.081565
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.070642
Total gradient norm: 0.212881
=== Actor Training Debug (Iteration 4799) ===
Q mean: -12.194378
Q std: 16.801395
Actor loss: 12.198347
Action reg: 0.003970
  l1.weight: grad_norm = 0.078462
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.074908
Total gradient norm: 0.218222
=== Actor Training Debug (Iteration 4800) ===
Q mean: -9.257202
Q std: 14.048078
Actor loss: 9.261176
Action reg: 0.003974
  l1.weight: grad_norm = 0.047809
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.050185
Total gradient norm: 0.154387
=== Actor Training Debug (Iteration 4801) ===
Q mean: -11.838839
Q std: 15.919725
Actor loss: 11.842818
Action reg: 0.003980
  l1.weight: grad_norm = 0.116608
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.101356
Total gradient norm: 0.346124
=== Actor Training Debug (Iteration 4802) ===
Q mean: -11.467798
Q std: 14.871075
Actor loss: 11.471750
Action reg: 0.003952
  l1.weight: grad_norm = 0.123070
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.110810
Total gradient norm: 0.351360
=== Actor Training Debug (Iteration 4803) ===
Q mean: -11.013361
Q std: 14.515799
Actor loss: 11.017321
Action reg: 0.003960
  l1.weight: grad_norm = 0.135022
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.118050
Total gradient norm: 0.381788
=== Actor Training Debug (Iteration 4804) ===
Q mean: -9.392076
Q std: 12.826302
Actor loss: 9.396038
Action reg: 0.003961
  l1.weight: grad_norm = 0.113977
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.105325
Total gradient norm: 0.290404
=== Actor Training Debug (Iteration 4805) ===
Q mean: -10.829609
Q std: 15.723150
Actor loss: 10.833570
Action reg: 0.003961
  l1.weight: grad_norm = 0.221552
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.211006
Total gradient norm: 0.569675
=== Actor Training Debug (Iteration 4806) ===
Q mean: -9.372915
Q std: 14.753799
Actor loss: 9.376883
Action reg: 0.003967
  l1.weight: grad_norm = 0.231888
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.180080
Total gradient norm: 0.499772
=== Actor Training Debug (Iteration 4807) ===
Q mean: -11.174379
Q std: 14.662860
Actor loss: 11.178351
Action reg: 0.003972
  l1.weight: grad_norm = 0.122377
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.095082
Total gradient norm: 0.287594
=== Actor Training Debug (Iteration 4808) ===
Q mean: -10.766363
Q std: 13.817017
Actor loss: 10.770333
Action reg: 0.003971
  l1.weight: grad_norm = 0.112730
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.096065
Total gradient norm: 0.325780
=== Actor Training Debug (Iteration 4809) ===
Q mean: -10.248820
Q std: 15.219658
Actor loss: 10.252784
Action reg: 0.003964
  l1.weight: grad_norm = 0.171553
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.143538
Total gradient norm: 0.438598
=== Actor Training Debug (Iteration 4810) ===
Q mean: -10.221365
Q std: 13.894979
Actor loss: 10.225338
Action reg: 0.003973
  l1.weight: grad_norm = 0.262481
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.193989
Total gradient norm: 0.583897
=== Actor Training Debug (Iteration 4811) ===
Q mean: -9.770017
Q std: 14.624116
Actor loss: 9.773980
Action reg: 0.003963
  l1.weight: grad_norm = 0.160818
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.134101
Total gradient norm: 0.415491
=== Actor Training Debug (Iteration 4812) ===
Q mean: -8.940860
Q std: 13.013000
Actor loss: 8.944832
Action reg: 0.003972
  l1.weight: grad_norm = 0.155181
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.135529
Total gradient norm: 0.377285
=== Actor Training Debug (Iteration 4813) ===
Q mean: -11.886073
Q std: 14.694530
Actor loss: 11.890052
Action reg: 0.003979
  l1.weight: grad_norm = 0.058336
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.056373
Total gradient norm: 0.158066
=== Actor Training Debug (Iteration 4814) ===
Q mean: -11.057123
Q std: 14.552028
Actor loss: 11.061081
Action reg: 0.003958
  l1.weight: grad_norm = 0.220920
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.174881
Total gradient norm: 0.480108
=== Actor Training Debug (Iteration 4815) ===
Q mean: -10.051758
Q std: 14.001347
Actor loss: 10.055723
Action reg: 0.003965
  l1.weight: grad_norm = 0.152350
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.121946
Total gradient norm: 0.369862
=== Actor Training Debug (Iteration 4816) ===
Q mean: -9.919970
Q std: 13.145407
Actor loss: 9.923934
Action reg: 0.003965
  l1.weight: grad_norm = 0.063783
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.055921
Total gradient norm: 0.197240
=== Actor Training Debug (Iteration 4817) ===
Q mean: -10.382957
Q std: 15.066945
Actor loss: 10.386925
Action reg: 0.003967
  l1.weight: grad_norm = 0.165461
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.147036
Total gradient norm: 0.406842
=== Actor Training Debug (Iteration 4818) ===
Q mean: -11.030071
Q std: 15.276428
Actor loss: 11.034051
Action reg: 0.003979
  l1.weight: grad_norm = 0.096497
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.078353
Total gradient norm: 0.248018
=== Actor Training Debug (Iteration 4819) ===
Q mean: -10.223633
Q std: 14.037755
Actor loss: 10.227607
Action reg: 0.003974
  l1.weight: grad_norm = 0.092801
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.083612
Total gradient norm: 0.237833
=== Actor Training Debug (Iteration 4820) ===
Q mean: -10.749884
Q std: 15.534938
Actor loss: 10.753855
Action reg: 0.003971
  l1.weight: grad_norm = 0.140727
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.122572
Total gradient norm: 0.317329
=== Actor Training Debug (Iteration 4821) ===
Q mean: -9.061991
Q std: 13.568585
Actor loss: 9.065948
Action reg: 0.003957
  l1.weight: grad_norm = 0.161808
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.134510
Total gradient norm: 0.408366
=== Actor Training Debug (Iteration 4822) ===
Q mean: -10.842920
Q std: 16.004974
Actor loss: 10.846894
Action reg: 0.003974
  l1.weight: grad_norm = 0.134280
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.115593
Total gradient norm: 0.314895
=== Actor Training Debug (Iteration 4823) ===
Q mean: -11.133179
Q std: 14.940815
Actor loss: 11.137133
Action reg: 0.003954
  l1.weight: grad_norm = 0.148981
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.125029
Total gradient norm: 0.366484
=== Actor Training Debug (Iteration 4824) ===
Q mean: -11.157411
Q std: 15.294543
Actor loss: 11.161374
Action reg: 0.003964
  l1.weight: grad_norm = 0.113059
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.092454
Total gradient norm: 0.275941
=== Actor Training Debug (Iteration 4825) ===
Q mean: -11.360837
Q std: 14.714190
Actor loss: 11.364808
Action reg: 0.003971
  l1.weight: grad_norm = 0.131058
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.106081
Total gradient norm: 0.360039
=== Actor Training Debug (Iteration 4826) ===
Q mean: -10.399654
Q std: 14.807366
Actor loss: 10.403625
Action reg: 0.003971
  l1.weight: grad_norm = 0.140563
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.129340
Total gradient norm: 0.424125
=== Actor Training Debug (Iteration 4827) ===
Q mean: -10.117214
Q std: 15.060123
Actor loss: 10.121167
Action reg: 0.003953
  l1.weight: grad_norm = 0.070996
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.063470
Total gradient norm: 0.206617
=== Actor Training Debug (Iteration 4828) ===
Q mean: -9.451633
Q std: 15.134521
Actor loss: 9.455603
Action reg: 0.003970
  l1.weight: grad_norm = 0.071272
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.058298
Total gradient norm: 0.158706
=== Actor Training Debug (Iteration 4829) ===
Q mean: -10.759037
Q std: 14.358469
Actor loss: 10.763009
Action reg: 0.003973
  l1.weight: grad_norm = 0.135468
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.107533
Total gradient norm: 0.320713
=== Actor Training Debug (Iteration 4830) ===
Q mean: -10.915709
Q std: 15.010398
Actor loss: 10.919673
Action reg: 0.003963
  l1.weight: grad_norm = 0.203506
  l1.bias: grad_norm = 0.000501
  l2.weight: grad_norm = 0.173846
Total gradient norm: 0.445098
=== Actor Training Debug (Iteration 4831) ===
Q mean: -8.402156
Q std: 13.835041
Actor loss: 8.406124
Action reg: 0.003968
  l1.weight: grad_norm = 0.206451
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.177763
Total gradient norm: 0.406859
=== Actor Training Debug (Iteration 4832) ===
Q mean: -8.400174
Q std: 12.940673
Actor loss: 8.404146
Action reg: 0.003972
  l1.weight: grad_norm = 0.098744
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.079488
Total gradient norm: 0.226461
=== Actor Training Debug (Iteration 4833) ===
Q mean: -9.645741
Q std: 14.804895
Actor loss: 9.649710
Action reg: 0.003969
  l1.weight: grad_norm = 0.085341
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.074096
Total gradient norm: 0.238572
=== Actor Training Debug (Iteration 4834) ===
Q mean: -11.174525
Q std: 14.899447
Actor loss: 11.178491
Action reg: 0.003965
  l1.weight: grad_norm = 0.137033
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.113278
Total gradient norm: 0.345804
=== Actor Training Debug (Iteration 4835) ===
Q mean: -10.604697
Q std: 14.452641
Actor loss: 10.608668
Action reg: 0.003971
  l1.weight: grad_norm = 0.159862
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.132048
Total gradient norm: 0.329148
=== Actor Training Debug (Iteration 4836) ===
Q mean: -11.383890
Q std: 15.324358
Actor loss: 11.387847
Action reg: 0.003957
  l1.weight: grad_norm = 0.146103
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.126254
Total gradient norm: 0.368950
=== Actor Training Debug (Iteration 4837) ===
Q mean: -10.713131
Q std: 15.358912
Actor loss: 10.717102
Action reg: 0.003971
  l1.weight: grad_norm = 0.157378
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.123799
Total gradient norm: 0.323829
=== Actor Training Debug (Iteration 4838) ===
Q mean: -9.627331
Q std: 13.699440
Actor loss: 9.631297
Action reg: 0.003966
  l1.weight: grad_norm = 0.214111
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.202810
Total gradient norm: 0.582748
=== Actor Training Debug (Iteration 4839) ===
Q mean: -9.979471
Q std: 15.831146
Actor loss: 9.983431
Action reg: 0.003960
  l1.weight: grad_norm = 0.106125
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.083789
Total gradient norm: 0.232971
=== Actor Training Debug (Iteration 4840) ===
Q mean: -8.837701
Q std: 13.972717
Actor loss: 8.841677
Action reg: 0.003975
  l1.weight: grad_norm = 0.107497
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.085811
Total gradient norm: 0.262139
=== Actor Training Debug (Iteration 4841) ===
Q mean: -10.313352
Q std: 15.325162
Actor loss: 10.317313
Action reg: 0.003962
  l1.weight: grad_norm = 0.167805
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.125968
Total gradient norm: 0.319849
=== Actor Training Debug (Iteration 4842) ===
Q mean: -12.168821
Q std: 15.502287
Actor loss: 12.172787
Action reg: 0.003966
  l1.weight: grad_norm = 0.156233
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.123152
Total gradient norm: 0.317910
=== Actor Training Debug (Iteration 4843) ===
Q mean: -10.487689
Q std: 15.807163
Actor loss: 10.491676
Action reg: 0.003987
  l1.weight: grad_norm = 0.094878
  l1.bias: grad_norm = 0.000037
  l2.weight: grad_norm = 0.076729
Total gradient norm: 0.221823
=== Actor Training Debug (Iteration 4844) ===
Q mean: -9.762425
Q std: 14.009423
Actor loss: 9.766395
Action reg: 0.003970
  l1.weight: grad_norm = 0.147233
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.128076
Total gradient norm: 0.350688
=== Actor Training Debug (Iteration 4845) ===
Q mean: -10.092014
Q std: 14.721115
Actor loss: 10.095992
Action reg: 0.003978
  l1.weight: grad_norm = 0.112819
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.104565
Total gradient norm: 0.294941
=== Actor Training Debug (Iteration 4846) ===
Q mean: -10.640904
Q std: 15.135078
Actor loss: 10.644872
Action reg: 0.003967
  l1.weight: grad_norm = 0.160625
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.143766
Total gradient norm: 0.442975
=== Actor Training Debug (Iteration 4847) ===
Q mean: -9.852058
Q std: 14.868769
Actor loss: 9.856020
Action reg: 0.003962
  l1.weight: grad_norm = 0.132212
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.118604
Total gradient norm: 0.368066
=== Actor Training Debug (Iteration 4848) ===
Q mean: -12.184607
Q std: 15.670668
Actor loss: 12.188585
Action reg: 0.003979
  l1.weight: grad_norm = 0.058396
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.049464
Total gradient norm: 0.159721
=== Actor Training Debug (Iteration 4849) ===
Q mean: -11.143820
Q std: 15.385986
Actor loss: 11.147785
Action reg: 0.003965
  l1.weight: grad_norm = 0.138731
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.128354
Total gradient norm: 0.350896
=== Actor Training Debug (Iteration 4850) ===
Q mean: -9.485533
Q std: 15.493146
Actor loss: 9.489492
Action reg: 0.003959
  l1.weight: grad_norm = 0.344737
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.293154
Total gradient norm: 1.024418
=== Actor Training Debug (Iteration 4851) ===
Q mean: -12.859653
Q std: 16.392229
Actor loss: 12.863634
Action reg: 0.003981
  l1.weight: grad_norm = 0.089023
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.081688
Total gradient norm: 0.211422
=== Actor Training Debug (Iteration 4852) ===
Q mean: -11.657688
Q std: 14.835333
Actor loss: 11.661654
Action reg: 0.003965
  l1.weight: grad_norm = 0.140459
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.137647
Total gradient norm: 0.371245
=== Actor Training Debug (Iteration 4853) ===
Q mean: -11.104250
Q std: 15.269998
Actor loss: 11.108219
Action reg: 0.003969
  l1.weight: grad_norm = 0.242163
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.188518
Total gradient norm: 0.595310
=== Actor Training Debug (Iteration 4854) ===
Q mean: -10.650083
Q std: 16.030094
Actor loss: 10.654057
Action reg: 0.003974
  l1.weight: grad_norm = 0.123583
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.112179
Total gradient norm: 0.311521
=== Actor Training Debug (Iteration 4855) ===
Q mean: -10.333891
Q std: 13.499869
Actor loss: 10.337865
Action reg: 0.003974
  l1.weight: grad_norm = 0.292696
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.248332
Total gradient norm: 0.693143
=== Actor Training Debug (Iteration 4856) ===
Q mean: -12.781496
Q std: 15.749852
Actor loss: 12.785463
Action reg: 0.003967
  l1.weight: grad_norm = 0.218516
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.178799
Total gradient norm: 0.535519
=== Actor Training Debug (Iteration 4857) ===
Q mean: -9.368181
Q std: 14.509991
Actor loss: 9.372145
Action reg: 0.003963
  l1.weight: grad_norm = 0.145362
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.142954
Total gradient norm: 0.434952
=== Actor Training Debug (Iteration 4858) ===
Q mean: -10.244637
Q std: 13.909107
Actor loss: 10.248609
Action reg: 0.003972
  l1.weight: grad_norm = 0.119022
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.100143
Total gradient norm: 0.287083
=== Actor Training Debug (Iteration 4859) ===
Q mean: -9.679886
Q std: 15.203697
Actor loss: 9.683847
Action reg: 0.003961
  l1.weight: grad_norm = 0.123795
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.116350
Total gradient norm: 0.340381
=== Actor Training Debug (Iteration 4860) ===
Q mean: -10.015340
Q std: 14.208559
Actor loss: 10.019305
Action reg: 0.003965
  l1.weight: grad_norm = 0.146157
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.134766
Total gradient norm: 0.383071
=== Actor Training Debug (Iteration 4861) ===
Q mean: -10.237635
Q std: 15.270055
Actor loss: 10.241598
Action reg: 0.003964
  l1.weight: grad_norm = 0.122528
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.101445
Total gradient norm: 0.311905
=== Actor Training Debug (Iteration 4862) ===
Q mean: -10.751049
Q std: 14.360199
Actor loss: 10.755029
Action reg: 0.003980
  l1.weight: grad_norm = 0.183115
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.146690
Total gradient norm: 0.464699
=== Actor Training Debug (Iteration 4863) ===
Q mean: -11.388713
Q std: 14.267625
Actor loss: 11.392668
Action reg: 0.003955
  l1.weight: grad_norm = 0.188007
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.160273
Total gradient norm: 0.473581
=== Actor Training Debug (Iteration 4864) ===
Q mean: -12.108403
Q std: 16.873171
Actor loss: 12.112361
Action reg: 0.003958
  l1.weight: grad_norm = 0.253245
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.227955
Total gradient norm: 0.671750
=== Actor Training Debug (Iteration 4865) ===
Q mean: -10.713977
Q std: 14.778704
Actor loss: 10.717947
Action reg: 0.003970
  l1.weight: grad_norm = 0.133538
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.113789
Total gradient norm: 0.316862
=== Actor Training Debug (Iteration 4866) ===
Q mean: -10.358088
Q std: 15.028829
Actor loss: 10.362065
Action reg: 0.003978
  l1.weight: grad_norm = 0.048330
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.041547
Total gradient norm: 0.123129
=== Actor Training Debug (Iteration 4867) ===
Q mean: -8.976315
Q std: 12.171578
Actor loss: 8.980268
Action reg: 0.003954
  l1.weight: grad_norm = 0.174170
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.141809
Total gradient norm: 0.467946
=== Actor Training Debug (Iteration 4868) ===
Q mean: -9.352728
Q std: 13.203180
Actor loss: 9.356698
Action reg: 0.003970
  l1.weight: grad_norm = 0.101056
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.089118
Total gradient norm: 0.330860
=== Actor Training Debug (Iteration 4869) ===
Q mean: -8.622559
Q std: 13.612805
Actor loss: 8.626532
Action reg: 0.003973
  l1.weight: grad_norm = 0.097538
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.077407
Total gradient norm: 0.243176
=== Actor Training Debug (Iteration 4870) ===
Q mean: -10.368484
Q std: 15.477137
Actor loss: 10.372458
Action reg: 0.003974
  l1.weight: grad_norm = 0.092465
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.080314
Total gradient norm: 0.265868
=== Actor Training Debug (Iteration 4871) ===
Q mean: -10.178709
Q std: 13.531869
Actor loss: 10.182673
Action reg: 0.003963
  l1.weight: grad_norm = 0.147407
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.128785
Total gradient norm: 0.397034
=== Actor Training Debug (Iteration 4872) ===
Q mean: -11.427436
Q std: 15.626745
Actor loss: 11.431413
Action reg: 0.003977
  l1.weight: grad_norm = 0.097816
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.077164
Total gradient norm: 0.223416
=== Actor Training Debug (Iteration 4873) ===
Q mean: -10.575644
Q std: 15.514897
Actor loss: 10.579603
Action reg: 0.003959
  l1.weight: grad_norm = 0.243712
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.226614
Total gradient norm: 0.820462
=== Actor Training Debug (Iteration 4874) ===
Q mean: -11.052974
Q std: 15.860191
Actor loss: 11.056952
Action reg: 0.003977
  l1.weight: grad_norm = 0.165358
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.133593
Total gradient norm: 0.466443
=== Actor Training Debug (Iteration 4875) ===
Q mean: -10.048247
Q std: 13.785287
Actor loss: 10.052216
Action reg: 0.003968
  l1.weight: grad_norm = 0.102171
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.083555
Total gradient norm: 0.243585
=== Actor Training Debug (Iteration 4876) ===
Q mean: -11.454257
Q std: 15.686748
Actor loss: 11.458222
Action reg: 0.003965
  l1.weight: grad_norm = 0.131705
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.116957
Total gradient norm: 0.324814
=== Actor Training Debug (Iteration 4877) ===
Q mean: -12.251881
Q std: 16.815397
Actor loss: 12.255842
Action reg: 0.003962
  l1.weight: grad_norm = 0.163218
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.150083
Total gradient norm: 0.511018
=== Actor Training Debug (Iteration 4878) ===
Q mean: -11.643509
Q std: 15.254095
Actor loss: 11.647472
Action reg: 0.003964
  l1.weight: grad_norm = 0.198236
  l1.bias: grad_norm = 0.000639
  l2.weight: grad_norm = 0.165937
Total gradient norm: 0.490514
=== Actor Training Debug (Iteration 4879) ===
Q mean: -10.658689
Q std: 15.357891
Actor loss: 10.662651
Action reg: 0.003963
  l1.weight: grad_norm = 0.135928
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.119849
Total gradient norm: 0.342968
=== Actor Training Debug (Iteration 4880) ===
Q mean: -8.793577
Q std: 13.322874
Actor loss: 8.797541
Action reg: 0.003964
  l1.weight: grad_norm = 0.115301
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.111476
Total gradient norm: 0.335996
=== Actor Training Debug (Iteration 4881) ===
Q mean: -10.237931
Q std: 14.483545
Actor loss: 10.241903
Action reg: 0.003972
  l1.weight: grad_norm = 0.119271
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.108861
Total gradient norm: 0.385591
=== Actor Training Debug (Iteration 4882) ===
Q mean: -11.246412
Q std: 14.767102
Actor loss: 11.250384
Action reg: 0.003972
  l1.weight: grad_norm = 0.208275
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.175923
Total gradient norm: 0.519417
=== Actor Training Debug (Iteration 4883) ===
Q mean: -11.570385
Q std: 15.734676
Actor loss: 11.574363
Action reg: 0.003978
  l1.weight: grad_norm = 0.114866
  l1.bias: grad_norm = 0.000042
  l2.weight: grad_norm = 0.114785
Total gradient norm: 0.315933
=== Actor Training Debug (Iteration 4884) ===
Q mean: -10.937287
Q std: 16.072945
Actor loss: 10.941238
Action reg: 0.003951
  l1.weight: grad_norm = 0.250619
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.216640
Total gradient norm: 0.636226
=== Actor Training Debug (Iteration 4885) ===
Q mean: -10.353159
Q std: 15.409648
Actor loss: 10.357133
Action reg: 0.003974
  l1.weight: grad_norm = 0.122509
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.103827
Total gradient norm: 0.314266
=== Actor Training Debug (Iteration 4886) ===
Q mean: -10.345518
Q std: 14.647142
Actor loss: 10.349463
Action reg: 0.003945
  l1.weight: grad_norm = 0.166323
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.155748
Total gradient norm: 0.465057
=== Actor Training Debug (Iteration 4887) ===
Q mean: -11.206898
Q std: 16.175371
Actor loss: 11.210877
Action reg: 0.003980
  l1.weight: grad_norm = 0.232209
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.185018
Total gradient norm: 0.549358
=== Actor Training Debug (Iteration 4888) ===
Q mean: -10.253111
Q std: 15.318193
Actor loss: 10.257074
Action reg: 0.003963
  l1.weight: grad_norm = 0.096723
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.090654
Total gradient norm: 0.293016
=== Actor Training Debug (Iteration 4889) ===
Q mean: -11.627030
Q std: 15.133131
Actor loss: 11.630986
Action reg: 0.003956
  l1.weight: grad_norm = 0.113783
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.099047
Total gradient norm: 0.276711
=== Actor Training Debug (Iteration 4890) ===
Q mean: -10.185913
Q std: 14.604959
Actor loss: 10.189894
Action reg: 0.003980
  l1.weight: grad_norm = 0.049771
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.046329
Total gradient norm: 0.133336
=== Actor Training Debug (Iteration 4891) ===
Q mean: -13.270792
Q std: 16.584444
Actor loss: 13.274758
Action reg: 0.003966
  l1.weight: grad_norm = 0.181949
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.174006
Total gradient norm: 0.643430
=== Actor Training Debug (Iteration 4892) ===
Q mean: -10.134535
Q std: 15.426758
Actor loss: 10.138506
Action reg: 0.003971
  l1.weight: grad_norm = 0.107268
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.086414
Total gradient norm: 0.233608
=== Actor Training Debug (Iteration 4893) ===
Q mean: -10.972309
Q std: 15.926742
Actor loss: 10.976279
Action reg: 0.003971
  l1.weight: grad_norm = 0.060326
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.052290
Total gradient norm: 0.170315
=== Actor Training Debug (Iteration 4894) ===
Q mean: -11.343115
Q std: 16.538956
Actor loss: 11.347086
Action reg: 0.003971
  l1.weight: grad_norm = 0.094757
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.092678
Total gradient norm: 0.286611
=== Actor Training Debug (Iteration 4895) ===
Q mean: -10.724322
Q std: 15.608217
Actor loss: 10.728275
Action reg: 0.003953
  l1.weight: grad_norm = 0.166287
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.147698
Total gradient norm: 0.444132
=== Actor Training Debug (Iteration 4896) ===
Q mean: -9.325024
Q std: 14.086266
Actor loss: 9.329005
Action reg: 0.003981
  l1.weight: grad_norm = 0.205911
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.186775
Total gradient norm: 0.513321
=== Actor Training Debug (Iteration 4897) ===
Q mean: -11.586287
Q std: 15.527732
Actor loss: 11.590244
Action reg: 0.003958
  l1.weight: grad_norm = 0.209561
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.177204
Total gradient norm: 0.482628
=== Actor Training Debug (Iteration 4898) ===
Q mean: -11.039654
Q std: 15.673885
Actor loss: 11.043617
Action reg: 0.003963
  l1.weight: grad_norm = 0.118377
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.110516
Total gradient norm: 0.273416
=== Actor Training Debug (Iteration 4899) ===
Q mean: -9.608787
Q std: 14.376889
Actor loss: 9.612758
Action reg: 0.003971
  l1.weight: grad_norm = 0.299769
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.266070
Total gradient norm: 0.748276
=== Actor Training Debug (Iteration 4900) ===
Q mean: -9.673217
Q std: 14.706583
Actor loss: 9.677186
Action reg: 0.003969
  l1.weight: grad_norm = 0.173833
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.167058
Total gradient norm: 0.522183
=== Actor Training Debug (Iteration 4901) ===
Q mean: -10.953691
Q std: 13.975368
Actor loss: 10.957639
Action reg: 0.003947
  l1.weight: grad_norm = 0.226040
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.188256
Total gradient norm: 0.653159
=== Actor Training Debug (Iteration 4902) ===
Q mean: -11.010359
Q std: 15.592866
Actor loss: 11.014318
Action reg: 0.003960
  l1.weight: grad_norm = 0.228234
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.206937
Total gradient norm: 0.792314
=== Actor Training Debug (Iteration 4903) ===
Q mean: -11.809073
Q std: 16.192738
Actor loss: 11.813039
Action reg: 0.003966
  l1.weight: grad_norm = 0.132846
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.105913
Total gradient norm: 0.315612
=== Actor Training Debug (Iteration 4904) ===
Q mean: -10.099803
Q std: 14.829442
Actor loss: 10.103752
Action reg: 0.003949
  l1.weight: grad_norm = 0.183638
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.158019
Total gradient norm: 0.477704
=== Actor Training Debug (Iteration 4905) ===
Q mean: -10.595699
Q std: 15.209558
Actor loss: 10.599679
Action reg: 0.003980
  l1.weight: grad_norm = 0.063234
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.054588
Total gradient norm: 0.158487
=== Actor Training Debug (Iteration 4906) ===
Q mean: -12.051634
Q std: 16.051754
Actor loss: 12.055600
Action reg: 0.003966
  l1.weight: grad_norm = 0.196278
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.183115
Total gradient norm: 0.551022
=== Actor Training Debug (Iteration 4907) ===
Q mean: -11.960478
Q std: 15.803020
Actor loss: 11.964446
Action reg: 0.003968
  l1.weight: grad_norm = 0.191221
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.169876
Total gradient norm: 0.471883
=== Actor Training Debug (Iteration 4908) ===
Q mean: -8.801498
Q std: 13.361532
Actor loss: 8.805465
Action reg: 0.003966
  l1.weight: grad_norm = 0.129970
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.116138
Total gradient norm: 0.372749
=== Actor Training Debug (Iteration 4909) ===
Q mean: -11.508739
Q std: 17.076092
Actor loss: 11.512706
Action reg: 0.003967
  l1.weight: grad_norm = 0.087017
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.079971
Total gradient norm: 0.256094
=== Actor Training Debug (Iteration 4910) ===
Q mean: -11.928730
Q std: 16.744579
Actor loss: 11.932702
Action reg: 0.003972
  l1.weight: grad_norm = 0.334264
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.271119
Total gradient norm: 0.791628
=== Actor Training Debug (Iteration 4911) ===
Q mean: -11.032775
Q std: 14.668471
Actor loss: 11.036751
Action reg: 0.003976
  l1.weight: grad_norm = 0.111842
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.091589
Total gradient norm: 0.258868
=== Actor Training Debug (Iteration 4912) ===
Q mean: -9.880364
Q std: 13.993672
Actor loss: 9.884323
Action reg: 0.003959
  l1.weight: grad_norm = 0.231482
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.205002
Total gradient norm: 0.565336
=== Actor Training Debug (Iteration 4913) ===
Q mean: -9.965905
Q std: 13.810699
Actor loss: 9.969881
Action reg: 0.003976
  l1.weight: grad_norm = 0.109681
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.102809
Total gradient norm: 0.340828
=== Actor Training Debug (Iteration 4914) ===
Q mean: -11.070506
Q std: 14.938595
Actor loss: 11.074481
Action reg: 0.003975
  l1.weight: grad_norm = 0.166506
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.145306
Total gradient norm: 0.433968
=== Actor Training Debug (Iteration 4915) ===
Q mean: -10.265821
Q std: 14.578384
Actor loss: 10.269778
Action reg: 0.003958
  l1.weight: grad_norm = 0.442507
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.335632
Total gradient norm: 0.825753
=== Actor Training Debug (Iteration 4916) ===
Q mean: -9.686246
Q std: 12.733650
Actor loss: 9.690200
Action reg: 0.003954
  l1.weight: grad_norm = 0.228185
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.187816
Total gradient norm: 0.555038
=== Actor Training Debug (Iteration 4917) ===
Q mean: -10.180953
Q std: 14.674011
Actor loss: 10.184931
Action reg: 0.003977
  l1.weight: grad_norm = 0.161269
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.128507
Total gradient norm: 0.411196
=== Actor Training Debug (Iteration 4918) ===
Q mean: -9.895182
Q std: 15.585764
Actor loss: 9.899147
Action reg: 0.003966
  l1.weight: grad_norm = 0.153231
  l1.bias: grad_norm = 0.000516
  l2.weight: grad_norm = 0.160018
Total gradient norm: 0.628147
=== Actor Training Debug (Iteration 4919) ===
Q mean: -10.926675
Q std: 15.332028
Actor loss: 10.930648
Action reg: 0.003973
  l1.weight: grad_norm = 0.138878
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.119407
Total gradient norm: 0.388505
=== Actor Training Debug (Iteration 4920) ===
Q mean: -10.286312
Q std: 14.597147
Actor loss: 10.290269
Action reg: 0.003957
  l1.weight: grad_norm = 0.191860
  l1.bias: grad_norm = 0.000492
  l2.weight: grad_norm = 0.160877
Total gradient norm: 0.488471
=== Actor Training Debug (Iteration 4921) ===
Q mean: -13.052358
Q std: 17.556181
Actor loss: 13.056319
Action reg: 0.003962
  l1.weight: grad_norm = 0.269876
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.219411
Total gradient norm: 0.679881
=== Actor Training Debug (Iteration 4922) ===
Q mean: -9.789076
Q std: 14.524634
Actor loss: 9.793054
Action reg: 0.003978
  l1.weight: grad_norm = 0.126445
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.105148
Total gradient norm: 0.311779
=== Actor Training Debug (Iteration 4923) ===
Q mean: -10.302529
Q std: 12.856402
Actor loss: 10.306508
Action reg: 0.003979
  l1.weight: grad_norm = 0.128379
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.138925
Total gradient norm: 0.408326
=== Actor Training Debug (Iteration 4924) ===
Q mean: -11.052240
Q std: 14.921398
Actor loss: 11.056209
Action reg: 0.003969
  l1.weight: grad_norm = 0.107297
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.090409
Total gradient norm: 0.281412
=== Actor Training Debug (Iteration 4925) ===
Q mean: -10.561824
Q std: 14.733111
Actor loss: 10.565806
Action reg: 0.003982
  l1.weight: grad_norm = 0.114707
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.103796
Total gradient norm: 0.304172
=== Actor Training Debug (Iteration 4926) ===
Q mean: -9.479504
Q std: 15.112246
Actor loss: 9.483474
Action reg: 0.003971
  l1.weight: grad_norm = 0.094491
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.086795
Total gradient norm: 0.247838
=== Actor Training Debug (Iteration 4927) ===
Q mean: -12.553015
Q std: 15.705191
Actor loss: 12.556993
Action reg: 0.003979
  l1.weight: grad_norm = 0.066910
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.054851
Total gradient norm: 0.163293
=== Actor Training Debug (Iteration 4928) ===
Q mean: -9.776722
Q std: 14.294262
Actor loss: 9.780678
Action reg: 0.003956
  l1.weight: grad_norm = 0.177396
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.138691
Total gradient norm: 0.355525
=== Actor Training Debug (Iteration 4929) ===
Q mean: -10.376926
Q std: 15.453477
Actor loss: 10.380886
Action reg: 0.003960
  l1.weight: grad_norm = 0.073965
  l1.bias: grad_norm = 0.000425
  l2.weight: grad_norm = 0.063884
Total gradient norm: 0.211114
=== Actor Training Debug (Iteration 4930) ===
Q mean: -8.648140
Q std: 14.921614
Actor loss: 8.652094
Action reg: 0.003953
  l1.weight: grad_norm = 0.148212
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.128028
Total gradient norm: 0.376932
=== Actor Training Debug (Iteration 4931) ===
Q mean: -11.166188
Q std: 15.879877
Actor loss: 11.170153
Action reg: 0.003965
  l1.weight: grad_norm = 0.160675
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.115906
Total gradient norm: 0.344692
=== Actor Training Debug (Iteration 4932) ===
Q mean: -10.260920
Q std: 14.587656
Actor loss: 10.264893
Action reg: 0.003973
  l1.weight: grad_norm = 0.147735
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.132125
Total gradient norm: 0.372129
=== Actor Training Debug (Iteration 4933) ===
Q mean: -10.812097
Q std: 14.614325
Actor loss: 10.816052
Action reg: 0.003956
  l1.weight: grad_norm = 0.108933
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.090019
Total gradient norm: 0.270169
=== Actor Training Debug (Iteration 4934) ===
Q mean: -10.544012
Q std: 15.516114
Actor loss: 10.547975
Action reg: 0.003963
  l1.weight: grad_norm = 0.146215
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.122838
Total gradient norm: 0.335650
=== Actor Training Debug (Iteration 4935) ===
Q mean: -10.241342
Q std: 15.040283
Actor loss: 10.245307
Action reg: 0.003965
  l1.weight: grad_norm = 0.092894
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.083598
Total gradient norm: 0.299729
=== Actor Training Debug (Iteration 4936) ===
Q mean: -10.053897
Q std: 13.835108
Actor loss: 10.057873
Action reg: 0.003976
  l1.weight: grad_norm = 0.136757
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.105393
Total gradient norm: 0.296473
=== Actor Training Debug (Iteration 4937) ===
Q mean: -11.559934
Q std: 15.071311
Actor loss: 11.563891
Action reg: 0.003957
  l1.weight: grad_norm = 0.201428
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.161478
Total gradient norm: 0.524823
=== Actor Training Debug (Iteration 4938) ===
Q mean: -10.749162
Q std: 14.852932
Actor loss: 10.753115
Action reg: 0.003953
  l1.weight: grad_norm = 0.133707
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.117791
Total gradient norm: 0.356226
=== Actor Training Debug (Iteration 4939) ===
Q mean: -11.784832
Q std: 16.175442
Actor loss: 11.788804
Action reg: 0.003972
  l1.weight: grad_norm = 0.137224
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.112424
Total gradient norm: 0.321970
=== Actor Training Debug (Iteration 4940) ===
Q mean: -11.697313
Q std: 14.845095
Actor loss: 11.701275
Action reg: 0.003962
  l1.weight: grad_norm = 0.158458
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.123515
Total gradient norm: 0.381661
=== Actor Training Debug (Iteration 4941) ===
Q mean: -9.938477
Q std: 14.845060
Actor loss: 9.942436
Action reg: 0.003960
  l1.weight: grad_norm = 0.153508
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.135587
Total gradient norm: 0.444089
=== Actor Training Debug (Iteration 4942) ===
Q mean: -8.997021
Q std: 14.820332
Actor loss: 9.000990
Action reg: 0.003969
  l1.weight: grad_norm = 0.106219
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.083413
Total gradient norm: 0.236709
=== Actor Training Debug (Iteration 4943) ===
Q mean: -12.089201
Q std: 16.820869
Actor loss: 12.093182
Action reg: 0.003981
  l1.weight: grad_norm = 0.275623
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.215532
Total gradient norm: 0.807002
=== Actor Training Debug (Iteration 4944) ===
Q mean: -11.437190
Q std: 15.239206
Actor loss: 11.441166
Action reg: 0.003975
  l1.weight: grad_norm = 0.169706
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.131721
Total gradient norm: 0.442832
=== Actor Training Debug (Iteration 4945) ===
Q mean: -10.027492
Q std: 15.177201
Actor loss: 10.031464
Action reg: 0.003972
  l1.weight: grad_norm = 0.110185
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.092796
Total gradient norm: 0.262749
=== Actor Training Debug (Iteration 4946) ===
Q mean: -10.101634
Q std: 15.950750
Actor loss: 10.105619
Action reg: 0.003985
  l1.weight: grad_norm = 0.058799
  l1.bias: grad_norm = 0.000023
  l2.weight: grad_norm = 0.053101
Total gradient norm: 0.188266
=== Actor Training Debug (Iteration 4947) ===
Q mean: -10.353459
Q std: 14.533222
Actor loss: 10.357433
Action reg: 0.003974
  l1.weight: grad_norm = 0.041758
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.037053
Total gradient norm: 0.129870
=== Actor Training Debug (Iteration 4948) ===
Q mean: -10.803320
Q std: 15.088195
Actor loss: 10.807289
Action reg: 0.003969
  l1.weight: grad_norm = 0.306263
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.274803
Total gradient norm: 0.879421
=== Actor Training Debug (Iteration 4949) ===
Q mean: -10.074492
Q std: 14.052114
Actor loss: 10.078465
Action reg: 0.003974
  l1.weight: grad_norm = 0.133879
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.109099
Total gradient norm: 0.319861
=== Actor Training Debug (Iteration 4950) ===
Q mean: -11.821873
Q std: 15.062636
Actor loss: 11.825830
Action reg: 0.003958
  l1.weight: grad_norm = 0.092855
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.083480
Total gradient norm: 0.209377
=== Actor Training Debug (Iteration 4951) ===
Q mean: -11.473067
Q std: 15.866132
Actor loss: 11.477033
Action reg: 0.003965
  l1.weight: grad_norm = 0.094699
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.072011
Total gradient norm: 0.201601
=== Actor Training Debug (Iteration 4952) ===
Q mean: -10.187044
Q std: 15.011281
Actor loss: 10.191010
Action reg: 0.003965
  l1.weight: grad_norm = 0.154150
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.122229
Total gradient norm: 0.368564
=== Actor Training Debug (Iteration 4953) ===
Q mean: -12.210786
Q std: 17.506599
Actor loss: 12.214760
Action reg: 0.003974
  l1.weight: grad_norm = 0.105316
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.084099
Total gradient norm: 0.285618
=== Actor Training Debug (Iteration 4954) ===
Q mean: -11.914609
Q std: 16.811041
Actor loss: 11.918573
Action reg: 0.003965
  l1.weight: grad_norm = 0.080554
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.068863
Total gradient norm: 0.216462
=== Actor Training Debug (Iteration 4955) ===
Q mean: -12.917480
Q std: 17.106915
Actor loss: 12.921447
Action reg: 0.003966
  l1.weight: grad_norm = 0.123826
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.119573
Total gradient norm: 0.318732
=== Actor Training Debug (Iteration 4956) ===
Q mean: -12.263369
Q std: 15.503626
Actor loss: 12.267334
Action reg: 0.003965
  l1.weight: grad_norm = 0.064490
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.060199
Total gradient norm: 0.177194
=== Actor Training Debug (Iteration 4957) ===
Q mean: -9.878061
Q std: 15.189351
Actor loss: 9.882025
Action reg: 0.003964
  l1.weight: grad_norm = 0.087633
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.081221
Total gradient norm: 0.253505
=== Actor Training Debug (Iteration 4958) ===
Q mean: -10.972830
Q std: 14.905161
Actor loss: 10.976789
Action reg: 0.003960
  l1.weight: grad_norm = 0.168704
  l1.bias: grad_norm = 0.000435
  l2.weight: grad_norm = 0.141551
Total gradient norm: 0.386867
=== Actor Training Debug (Iteration 4959) ===
Q mean: -9.157425
Q std: 13.644448
Actor loss: 9.161394
Action reg: 0.003969
  l1.weight: grad_norm = 0.298231
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.256759
Total gradient norm: 0.706115
=== Actor Training Debug (Iteration 4960) ===
Q mean: -12.497494
Q std: 16.068199
Actor loss: 12.501462
Action reg: 0.003968
  l1.weight: grad_norm = 0.112049
  l1.bias: grad_norm = 0.000659
  l2.weight: grad_norm = 0.093225
Total gradient norm: 0.364402
=== Actor Training Debug (Iteration 4961) ===
Q mean: -10.486313
Q std: 14.675278
Actor loss: 10.490293
Action reg: 0.003980
  l1.weight: grad_norm = 0.148659
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.120341
Total gradient norm: 0.350028
=== Actor Training Debug (Iteration 4962) ===
Q mean: -12.828951
Q std: 15.956786
Actor loss: 12.832920
Action reg: 0.003969
  l1.weight: grad_norm = 0.108546
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.086462
Total gradient norm: 0.270468
=== Actor Training Debug (Iteration 4963) ===
Q mean: -9.973737
Q std: 14.160874
Actor loss: 9.977704
Action reg: 0.003967
  l1.weight: grad_norm = 0.100498
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.085285
Total gradient norm: 0.261214
=== Actor Training Debug (Iteration 4964) ===
Q mean: -9.984877
Q std: 13.201777
Actor loss: 9.988834
Action reg: 0.003958
  l1.weight: grad_norm = 0.313543
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.262343
Total gradient norm: 0.811218
=== Actor Training Debug (Iteration 4965) ===
Q mean: -10.725393
Q std: 13.952856
Actor loss: 10.729366
Action reg: 0.003973
  l1.weight: grad_norm = 0.197633
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.148860
Total gradient norm: 0.404977
=== Actor Training Debug (Iteration 4966) ===
Q mean: -9.798644
Q std: 13.461448
Actor loss: 9.802603
Action reg: 0.003959
  l1.weight: grad_norm = 0.180251
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.168714
Total gradient norm: 0.522245
=== Actor Training Debug (Iteration 4967) ===
Q mean: -10.724938
Q std: 14.150970
Actor loss: 10.728905
Action reg: 0.003967
  l1.weight: grad_norm = 0.161328
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.133279
Total gradient norm: 0.361477
=== Actor Training Debug (Iteration 4968) ===
Q mean: -10.471758
Q std: 15.754850
Actor loss: 10.475708
Action reg: 0.003950
  l1.weight: grad_norm = 0.151604
  l1.bias: grad_norm = 0.000743
  l2.weight: grad_norm = 0.129022
Total gradient norm: 0.400371
=== Actor Training Debug (Iteration 4969) ===
Q mean: -10.797487
Q std: 15.202631
Actor loss: 10.801439
Action reg: 0.003952
  l1.weight: grad_norm = 0.463445
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.396633
Total gradient norm: 1.131866
=== Actor Training Debug (Iteration 4970) ===
Q mean: -9.398478
Q std: 15.363134
Actor loss: 9.402432
Action reg: 0.003955
  l1.weight: grad_norm = 0.178323
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.144595
Total gradient norm: 0.419481
=== Actor Training Debug (Iteration 4971) ===
Q mean: -8.998113
Q std: 13.257873
Actor loss: 9.002065
Action reg: 0.003952
  l1.weight: grad_norm = 0.213726
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.175379
Total gradient norm: 0.541132
=== Actor Training Debug (Iteration 4972) ===
Q mean: -10.176687
Q std: 15.136720
Actor loss: 10.180640
Action reg: 0.003953
  l1.weight: grad_norm = 0.184613
  l1.bias: grad_norm = 0.000513
  l2.weight: grad_norm = 0.129559
Total gradient norm: 0.381567
=== Actor Training Debug (Iteration 4973) ===
Q mean: -11.002399
Q std: 14.869177
Actor loss: 11.006356
Action reg: 0.003957
  l1.weight: grad_norm = 0.154653
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.126560
Total gradient norm: 0.419317
=== Actor Training Debug (Iteration 4974) ===
Q mean: -10.290409
Q std: 14.201052
Actor loss: 10.294384
Action reg: 0.003975
  l1.weight: grad_norm = 0.136649
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.094371
Total gradient norm: 0.299488
=== Actor Training Debug (Iteration 4975) ===
Q mean: -11.461312
Q std: 15.281731
Actor loss: 11.465260
Action reg: 0.003947
  l1.weight: grad_norm = 0.168759
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.161746
Total gradient norm: 0.546647
=== Actor Training Debug (Iteration 4976) ===
Q mean: -10.814969
Q std: 14.073682
Actor loss: 10.818928
Action reg: 0.003959
  l1.weight: grad_norm = 0.162989
  l1.bias: grad_norm = 0.000712
  l2.weight: grad_norm = 0.144747
Total gradient norm: 0.434654
=== Actor Training Debug (Iteration 4977) ===
Q mean: -11.965032
Q std: 15.736080
Actor loss: 11.968982
Action reg: 0.003950
  l1.weight: grad_norm = 0.136372
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.107503
Total gradient norm: 0.336653
=== Actor Training Debug (Iteration 4978) ===
Q mean: -11.162186
Q std: 15.809646
Actor loss: 11.166159
Action reg: 0.003973
  l1.weight: grad_norm = 0.111298
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.096232
Total gradient norm: 0.305256
=== Actor Training Debug (Iteration 4979) ===
Q mean: -9.314607
Q std: 13.650612
Actor loss: 9.318576
Action reg: 0.003969
  l1.weight: grad_norm = 0.114483
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.093208
Total gradient norm: 0.289934
=== Actor Training Debug (Iteration 4980) ===
Q mean: -10.370870
Q std: 15.333108
Actor loss: 10.374833
Action reg: 0.003963
  l1.weight: grad_norm = 0.114627
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.094133
Total gradient norm: 0.275880
=== Actor Training Debug (Iteration 4981) ===
Q mean: -11.587854
Q std: 14.710001
Actor loss: 11.591816
Action reg: 0.003961
  l1.weight: grad_norm = 0.151805
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.118949
Total gradient norm: 0.336490
=== Actor Training Debug (Iteration 4982) ===
Q mean: -11.812720
Q std: 15.464225
Actor loss: 11.816685
Action reg: 0.003964
  l1.weight: grad_norm = 0.115735
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.105994
Total gradient norm: 0.322076
=== Actor Training Debug (Iteration 4983) ===
Q mean: -10.339248
Q std: 15.379353
Actor loss: 10.343211
Action reg: 0.003964
  l1.weight: grad_norm = 0.342211
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.257599
Total gradient norm: 0.789392
=== Actor Training Debug (Iteration 4984) ===
Q mean: -11.060799
Q std: 14.604926
Actor loss: 11.064762
Action reg: 0.003963
  l1.weight: grad_norm = 0.330719
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.291138
Total gradient norm: 0.782842
=== Actor Training Debug (Iteration 4985) ===
Q mean: -11.850561
Q std: 14.867609
Actor loss: 11.854527
Action reg: 0.003966
  l1.weight: grad_norm = 0.223917
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.168879
Total gradient norm: 0.491154
=== Actor Training Debug (Iteration 4986) ===
Q mean: -8.589252
Q std: 14.566559
Actor loss: 8.593219
Action reg: 0.003966
  l1.weight: grad_norm = 0.154694
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.133460
Total gradient norm: 0.413281
=== Actor Training Debug (Iteration 4987) ===
Q mean: -11.339894
Q std: 16.572342
Actor loss: 11.343853
Action reg: 0.003958
  l1.weight: grad_norm = 0.415158
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.359769
Total gradient norm: 0.993027
=== Actor Training Debug (Iteration 4988) ===
Q mean: -10.187769
Q std: 14.307260
Actor loss: 10.191733
Action reg: 0.003964
  l1.weight: grad_norm = 0.202506
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.165506
Total gradient norm: 0.575969
=== Actor Training Debug (Iteration 4989) ===
Q mean: -11.936704
Q std: 16.148914
Actor loss: 11.940674
Action reg: 0.003970
  l1.weight: grad_norm = 0.114066
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.095115
Total gradient norm: 0.320986
=== Actor Training Debug (Iteration 4990) ===
Q mean: -11.218843
Q std: 15.954667
Actor loss: 11.222819
Action reg: 0.003976
  l1.weight: grad_norm = 0.096205
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.074725
Total gradient norm: 0.249705
=== Actor Training Debug (Iteration 4991) ===
Q mean: -10.874889
Q std: 14.486710
Actor loss: 10.878852
Action reg: 0.003963
  l1.weight: grad_norm = 0.264220
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.189832
Total gradient norm: 0.585618
=== Actor Training Debug (Iteration 4992) ===
Q mean: -10.486901
Q std: 13.977942
Actor loss: 10.490878
Action reg: 0.003977
  l1.weight: grad_norm = 0.228376
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.200611
Total gradient norm: 0.531595
=== Actor Training Debug (Iteration 4993) ===
Q mean: -10.642136
Q std: 14.393896
Actor loss: 10.646111
Action reg: 0.003976
  l1.weight: grad_norm = 0.115815
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.112269
Total gradient norm: 0.348346
=== Actor Training Debug (Iteration 4994) ===
Q mean: -11.649506
Q std: 15.465855
Actor loss: 11.653477
Action reg: 0.003971
  l1.weight: grad_norm = 0.152790
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.135594
Total gradient norm: 0.417444
=== Actor Training Debug (Iteration 4995) ===
Q mean: -11.741201
Q std: 15.663483
Actor loss: 11.745152
Action reg: 0.003951
  l1.weight: grad_norm = 0.154744
  l1.bias: grad_norm = 0.000703
  l2.weight: grad_norm = 0.139129
Total gradient norm: 0.426541
=== Actor Training Debug (Iteration 4996) ===
Q mean: -11.192608
Q std: 14.996428
Actor loss: 11.196582
Action reg: 0.003973
  l1.weight: grad_norm = 0.102030
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.090948
Total gradient norm: 0.295153
=== Actor Training Debug (Iteration 4997) ===
Q mean: -11.237977
Q std: 15.827439
Actor loss: 11.241958
Action reg: 0.003981
  l1.weight: grad_norm = 0.089682
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.078281
Total gradient norm: 0.248066
=== Actor Training Debug (Iteration 4998) ===
Q mean: -11.187067
Q std: 14.278877
Actor loss: 11.191048
Action reg: 0.003981
  l1.weight: grad_norm = 0.097918
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.079475
Total gradient norm: 0.264905
=== Actor Training Debug (Iteration 4999) ===
Q mean: -13.056356
Q std: 16.353807
Actor loss: 13.060331
Action reg: 0.003974
  l1.weight: grad_norm = 0.113601
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.091997
Total gradient norm: 0.249419
=== Actor Training Debug (Iteration 5000) ===
Q mean: -10.827296
Q std: 15.506841
Actor loss: 10.831262
Action reg: 0.003966
  l1.weight: grad_norm = 0.148785
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.134384
Total gradient norm: 0.427539
Step 10000: Critic Loss: 1.0673, Actor Loss: 10.8313, Q Value: -10.8273
  Average reward: -320.829 | Average length: 100.0
Evaluation at episode 100: -320.829
=== Actor Training Debug (Iteration 5001) ===
Q mean: -11.999337
Q std: 15.358113
Actor loss: 12.003306
Action reg: 0.003969
  l1.weight: grad_norm = 0.123272
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.090158
Total gradient norm: 0.274133
=== Actor Training Debug (Iteration 5002) ===
Q mean: -12.242229
Q std: 15.503983
Actor loss: 12.246172
Action reg: 0.003943
  l1.weight: grad_norm = 0.246569
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.198825
Total gradient norm: 0.652315
=== Actor Training Debug (Iteration 5003) ===
Q mean: -8.494189
Q std: 13.531445
Actor loss: 8.498144
Action reg: 0.003955
  l1.weight: grad_norm = 0.133694
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.112003
Total gradient norm: 0.343117
=== Actor Training Debug (Iteration 5004) ===
Q mean: -11.518693
Q std: 16.106932
Actor loss: 11.522651
Action reg: 0.003958
  l1.weight: grad_norm = 0.098158
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.089034
Total gradient norm: 0.295542
=== Actor Training Debug (Iteration 5005) ===
Q mean: -9.683704
Q std: 14.967790
Actor loss: 9.687669
Action reg: 0.003964
  l1.weight: grad_norm = 0.147211
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.129022
Total gradient norm: 0.353670
=== Actor Training Debug (Iteration 5006) ===
Q mean: -11.015217
Q std: 14.788065
Actor loss: 11.019173
Action reg: 0.003956
  l1.weight: grad_norm = 0.213426
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.193942
Total gradient norm: 0.547301
=== Actor Training Debug (Iteration 5007) ===
Q mean: -10.070354
Q std: 15.354933
Actor loss: 10.074331
Action reg: 0.003978
  l1.weight: grad_norm = 0.072460
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.062436
Total gradient norm: 0.179317
=== Actor Training Debug (Iteration 5008) ===
Q mean: -12.073721
Q std: 16.119610
Actor loss: 12.077680
Action reg: 0.003959
  l1.weight: grad_norm = 0.110332
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.100277
Total gradient norm: 0.320392
=== Actor Training Debug (Iteration 5009) ===
Q mean: -10.160816
Q std: 14.398740
Actor loss: 10.164781
Action reg: 0.003964
  l1.weight: grad_norm = 0.111360
  l1.bias: grad_norm = 0.000481
  l2.weight: grad_norm = 0.096421
Total gradient norm: 0.277883
=== Actor Training Debug (Iteration 5010) ===
Q mean: -10.590190
Q std: 15.062873
Actor loss: 10.594169
Action reg: 0.003978
  l1.weight: grad_norm = 0.062223
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.066830
Total gradient norm: 0.269240
=== Actor Training Debug (Iteration 5011) ===
Q mean: -12.499731
Q std: 17.094351
Actor loss: 12.503706
Action reg: 0.003975
  l1.weight: grad_norm = 0.090250
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.079651
Total gradient norm: 0.229106
=== Actor Training Debug (Iteration 5012) ===
Q mean: -10.908581
Q std: 14.715476
Actor loss: 10.912549
Action reg: 0.003968
  l1.weight: grad_norm = 0.091569
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.083181
Total gradient norm: 0.268303
=== Actor Training Debug (Iteration 5013) ===
Q mean: -10.746027
Q std: 15.322836
Actor loss: 10.749996
Action reg: 0.003969
  l1.weight: grad_norm = 0.100396
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.086185
Total gradient norm: 0.238046
=== Actor Training Debug (Iteration 5014) ===
Q mean: -11.469574
Q std: 16.342049
Actor loss: 11.473550
Action reg: 0.003976
  l1.weight: grad_norm = 0.069505
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.061232
Total gradient norm: 0.199739
=== Actor Training Debug (Iteration 5015) ===
Q mean: -12.550481
Q std: 17.723581
Actor loss: 12.554432
Action reg: 0.003951
  l1.weight: grad_norm = 0.251947
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.227906
Total gradient norm: 0.635215
=== Actor Training Debug (Iteration 5016) ===
Q mean: -11.792898
Q std: 15.500166
Actor loss: 11.796868
Action reg: 0.003970
  l1.weight: grad_norm = 0.155806
  l1.bias: grad_norm = 0.000527
  l2.weight: grad_norm = 0.138497
Total gradient norm: 0.356319
=== Actor Training Debug (Iteration 5017) ===
Q mean: -11.302101
Q std: 15.319320
Actor loss: 11.306054
Action reg: 0.003953
  l1.weight: grad_norm = 0.140705
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.131059
Total gradient norm: 0.362469
=== Actor Training Debug (Iteration 5018) ===
Q mean: -9.824641
Q std: 13.459410
Actor loss: 9.828607
Action reg: 0.003965
  l1.weight: grad_norm = 0.110617
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.091356
Total gradient norm: 0.262113
=== Actor Training Debug (Iteration 5019) ===
Q mean: -10.253033
Q std: 14.202661
Actor loss: 10.256987
Action reg: 0.003954
  l1.weight: grad_norm = 0.234106
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.193592
Total gradient norm: 0.463706
=== Actor Training Debug (Iteration 5020) ===
Q mean: -11.597485
Q std: 16.125063
Actor loss: 11.601442
Action reg: 0.003958
  l1.weight: grad_norm = 0.151854
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.138067
Total gradient norm: 0.399750
=== Actor Training Debug (Iteration 5021) ===
Q mean: -11.079766
Q std: 16.293320
Actor loss: 11.083722
Action reg: 0.003956
  l1.weight: grad_norm = 0.142741
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.117556
Total gradient norm: 0.395295
=== Actor Training Debug (Iteration 5022) ===
Q mean: -11.237709
Q std: 15.571956
Actor loss: 11.241681
Action reg: 0.003972
  l1.weight: grad_norm = 0.166698
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.139247
Total gradient norm: 0.419077
=== Actor Training Debug (Iteration 5023) ===
Q mean: -11.365918
Q std: 16.050533
Actor loss: 11.369876
Action reg: 0.003958
  l1.weight: grad_norm = 0.198636
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.181328
Total gradient norm: 0.595949
=== Actor Training Debug (Iteration 5024) ===
Q mean: -10.009583
Q std: 15.119730
Actor loss: 10.013534
Action reg: 0.003951
  l1.weight: grad_norm = 0.286832
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.273127
Total gradient norm: 0.790157
=== Actor Training Debug (Iteration 5025) ===
Q mean: -10.384636
Q std: 15.310590
Actor loss: 10.388591
Action reg: 0.003955
  l1.weight: grad_norm = 0.222389
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.195774
Total gradient norm: 0.493685
=== Actor Training Debug (Iteration 5026) ===
Q mean: -11.905827
Q std: 15.819560
Actor loss: 11.909804
Action reg: 0.003978
  l1.weight: grad_norm = 0.174011
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.130782
Total gradient norm: 0.372489
=== Actor Training Debug (Iteration 5027) ===
Q mean: -10.887644
Q std: 14.712872
Actor loss: 10.891594
Action reg: 0.003950
  l1.weight: grad_norm = 0.206776
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.176735
Total gradient norm: 0.495183
=== Actor Training Debug (Iteration 5028) ===
Q mean: -10.355888
Q std: 14.794336
Actor loss: 10.359863
Action reg: 0.003975
  l1.weight: grad_norm = 0.127555
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.111266
Total gradient norm: 0.306959
=== Actor Training Debug (Iteration 5029) ===
Q mean: -9.673161
Q std: 15.283673
Actor loss: 9.677127
Action reg: 0.003967
  l1.weight: grad_norm = 0.177714
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.147271
Total gradient norm: 0.443084
=== Actor Training Debug (Iteration 5030) ===
Q mean: -9.847502
Q std: 16.325890
Actor loss: 9.851482
Action reg: 0.003981
  l1.weight: grad_norm = 0.146671
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.121723
Total gradient norm: 0.381990
=== Actor Training Debug (Iteration 5031) ===
Q mean: -11.816385
Q std: 15.923525
Actor loss: 11.820341
Action reg: 0.003956
  l1.weight: grad_norm = 0.164669
  l1.bias: grad_norm = 0.000726
  l2.weight: grad_norm = 0.153150
Total gradient norm: 0.426235
=== Actor Training Debug (Iteration 5032) ===
Q mean: -10.460690
Q std: 14.157999
Actor loss: 10.464636
Action reg: 0.003947
  l1.weight: grad_norm = 0.126742
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.105514
Total gradient norm: 0.333551
=== Actor Training Debug (Iteration 5033) ===
Q mean: -9.994797
Q std: 15.055475
Actor loss: 9.998758
Action reg: 0.003961
  l1.weight: grad_norm = 0.214194
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.203067
Total gradient norm: 0.559671
=== Actor Training Debug (Iteration 5034) ===
Q mean: -10.631878
Q std: 14.690400
Actor loss: 10.635842
Action reg: 0.003965
  l1.weight: grad_norm = 0.159779
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.138613
Total gradient norm: 0.424021
=== Actor Training Debug (Iteration 5035) ===
Q mean: -9.556515
Q std: 14.899378
Actor loss: 9.560479
Action reg: 0.003964
  l1.weight: grad_norm = 0.117478
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.108573
Total gradient norm: 0.380627
=== Actor Training Debug (Iteration 5036) ===
Q mean: -11.819893
Q std: 15.715662
Actor loss: 11.823853
Action reg: 0.003960
  l1.weight: grad_norm = 0.145227
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.134401
Total gradient norm: 0.430689
=== Actor Training Debug (Iteration 5037) ===
Q mean: -13.993773
Q std: 17.053408
Actor loss: 13.997742
Action reg: 0.003969
  l1.weight: grad_norm = 0.184933
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.152603
Total gradient norm: 0.386448
=== Actor Training Debug (Iteration 5038) ===
Q mean: -11.669897
Q std: 14.725295
Actor loss: 11.673843
Action reg: 0.003946
  l1.weight: grad_norm = 0.123796
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.110706
Total gradient norm: 0.329013
=== Actor Training Debug (Iteration 5039) ===
Q mean: -9.959518
Q std: 14.556834
Actor loss: 9.963471
Action reg: 0.003953
  l1.weight: grad_norm = 0.395516
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.262705
Total gradient norm: 0.687007
=== Actor Training Debug (Iteration 5040) ===
Q mean: -10.215243
Q std: 14.302029
Actor loss: 10.219188
Action reg: 0.003944
  l1.weight: grad_norm = 0.258449
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.211834
Total gradient norm: 0.585528
=== Actor Training Debug (Iteration 5041) ===
Q mean: -9.275739
Q std: 14.919216
Actor loss: 9.279705
Action reg: 0.003966
  l1.weight: grad_norm = 0.114985
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.095084
Total gradient norm: 0.241761
=== Actor Training Debug (Iteration 5042) ===
Q mean: -12.093845
Q std: 14.747536
Actor loss: 12.097809
Action reg: 0.003963
  l1.weight: grad_norm = 0.284007
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.211922
Total gradient norm: 0.627116
=== Actor Training Debug (Iteration 5043) ===
Q mean: -13.081268
Q std: 16.478333
Actor loss: 13.085227
Action reg: 0.003959
  l1.weight: grad_norm = 0.132716
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.109588
Total gradient norm: 0.352186
=== Actor Training Debug (Iteration 5044) ===
Q mean: -9.399062
Q std: 14.192756
Actor loss: 9.403023
Action reg: 0.003961
  l1.weight: grad_norm = 0.197492
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.185059
Total gradient norm: 0.486996
=== Actor Training Debug (Iteration 5045) ===
Q mean: -10.493683
Q std: 15.150172
Actor loss: 10.497634
Action reg: 0.003951
  l1.weight: grad_norm = 0.361563
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.300616
Total gradient norm: 0.816266
=== Actor Training Debug (Iteration 5046) ===
Q mean: -11.766253
Q std: 15.321052
Actor loss: 11.770225
Action reg: 0.003972
  l1.weight: grad_norm = 0.131348
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.106943
Total gradient norm: 0.292148
=== Actor Training Debug (Iteration 5047) ===
Q mean: -10.784552
Q std: 15.667637
Actor loss: 10.788530
Action reg: 0.003979
  l1.weight: grad_norm = 0.056788
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.051632
Total gradient norm: 0.150856
=== Actor Training Debug (Iteration 5048) ===
Q mean: -10.798090
Q std: 14.849419
Actor loss: 10.802059
Action reg: 0.003969
  l1.weight: grad_norm = 0.122716
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.091466
Total gradient norm: 0.355986
=== Actor Training Debug (Iteration 5049) ===
Q mean: -10.599243
Q std: 14.241746
Actor loss: 10.603220
Action reg: 0.003977
  l1.weight: grad_norm = 0.225375
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.169055
Total gradient norm: 0.466642
=== Actor Training Debug (Iteration 5050) ===
Q mean: -10.965499
Q std: 15.561725
Actor loss: 10.969478
Action reg: 0.003979
  l1.weight: grad_norm = 0.067556
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.059847
Total gradient norm: 0.169190
=== Actor Training Debug (Iteration 5051) ===
Q mean: -8.781933
Q std: 13.773930
Actor loss: 8.785893
Action reg: 0.003961
  l1.weight: grad_norm = 0.213520
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.186826
Total gradient norm: 0.535554
=== Actor Training Debug (Iteration 5052) ===
Q mean: -9.547371
Q std: 14.323516
Actor loss: 9.551321
Action reg: 0.003950
  l1.weight: grad_norm = 0.220939
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.174934
Total gradient norm: 0.492627
=== Actor Training Debug (Iteration 5053) ===
Q mean: -12.150627
Q std: 15.623021
Actor loss: 12.154590
Action reg: 0.003963
  l1.weight: grad_norm = 0.118372
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.098719
Total gradient norm: 0.353097
=== Actor Training Debug (Iteration 5054) ===
Q mean: -10.612440
Q std: 14.752427
Actor loss: 10.616404
Action reg: 0.003963
  l1.weight: grad_norm = 0.187544
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.153052
Total gradient norm: 0.487101
=== Actor Training Debug (Iteration 5055) ===
Q mean: -10.798253
Q std: 13.838553
Actor loss: 10.802217
Action reg: 0.003963
  l1.weight: grad_norm = 0.101692
  l1.bias: grad_norm = 0.000747
  l2.weight: grad_norm = 0.091465
Total gradient norm: 0.267198
=== Actor Training Debug (Iteration 5056) ===
Q mean: -12.437972
Q std: 15.478091
Actor loss: 12.441937
Action reg: 0.003965
  l1.weight: grad_norm = 0.127984
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.102318
Total gradient norm: 0.276546
=== Actor Training Debug (Iteration 5057) ===
Q mean: -9.451324
Q std: 15.196171
Actor loss: 9.455298
Action reg: 0.003974
  l1.weight: grad_norm = 0.174224
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.148298
Total gradient norm: 0.461403
=== Actor Training Debug (Iteration 5058) ===
Q mean: -11.292423
Q std: 15.887795
Actor loss: 11.296376
Action reg: 0.003953
  l1.weight: grad_norm = 0.239349
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.183006
Total gradient norm: 0.587941
=== Actor Training Debug (Iteration 5059) ===
Q mean: -8.955189
Q std: 13.685785
Actor loss: 8.959138
Action reg: 0.003950
  l1.weight: grad_norm = 0.294424
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.232998
Total gradient norm: 0.673407
=== Actor Training Debug (Iteration 5060) ===
Q mean: -9.609453
Q std: 15.202923
Actor loss: 9.613430
Action reg: 0.003977
  l1.weight: grad_norm = 0.099403
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.079321
Total gradient norm: 0.230026
=== Actor Training Debug (Iteration 5061) ===
Q mean: -10.120789
Q std: 14.508707
Actor loss: 10.124758
Action reg: 0.003970
  l1.weight: grad_norm = 0.118272
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.109565
Total gradient norm: 0.337453
=== Actor Training Debug (Iteration 5062) ===
Q mean: -13.075233
Q std: 17.644424
Actor loss: 13.079204
Action reg: 0.003971
  l1.weight: grad_norm = 0.300805
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.246026
Total gradient norm: 0.717630
=== Actor Training Debug (Iteration 5063) ===
Q mean: -10.828033
Q std: 15.288136
Actor loss: 10.831974
Action reg: 0.003941
  l1.weight: grad_norm = 0.144931
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.118682
Total gradient norm: 0.357609
=== Actor Training Debug (Iteration 5064) ===
Q mean: -11.791822
Q std: 15.274574
Actor loss: 11.795806
Action reg: 0.003983
  l1.weight: grad_norm = 0.075605
  l1.bias: grad_norm = 0.000029
  l2.weight: grad_norm = 0.070588
Total gradient norm: 0.210711
=== Actor Training Debug (Iteration 5065) ===
Q mean: -10.255567
Q std: 13.573638
Actor loss: 10.259545
Action reg: 0.003978
  l1.weight: grad_norm = 0.198942
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.164905
Total gradient norm: 0.504847
=== Actor Training Debug (Iteration 5066) ===
Q mean: -10.676300
Q std: 14.434519
Actor loss: 10.680270
Action reg: 0.003970
  l1.weight: grad_norm = 0.224879
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.187204
Total gradient norm: 0.581994
=== Actor Training Debug (Iteration 5067) ===
Q mean: -10.061513
Q std: 16.104021
Actor loss: 10.065475
Action reg: 0.003963
  l1.weight: grad_norm = 0.213406
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.174300
Total gradient norm: 0.535421
=== Actor Training Debug (Iteration 5068) ===
Q mean: -10.861732
Q std: 15.505141
Actor loss: 10.865687
Action reg: 0.003955
  l1.weight: grad_norm = 0.127402
  l1.bias: grad_norm = 0.000686
  l2.weight: grad_norm = 0.119825
Total gradient norm: 0.330525
=== Actor Training Debug (Iteration 5069) ===
Q mean: -12.449539
Q std: 16.439049
Actor loss: 12.453520
Action reg: 0.003981
  l1.weight: grad_norm = 0.094923
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.088348
Total gradient norm: 0.286305
=== Actor Training Debug (Iteration 5070) ===
Q mean: -8.365870
Q std: 13.474672
Actor loss: 8.369832
Action reg: 0.003961
  l1.weight: grad_norm = 0.160311
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.140792
Total gradient norm: 0.352395
=== Actor Training Debug (Iteration 5071) ===
Q mean: -9.954545
Q std: 16.279324
Actor loss: 9.958517
Action reg: 0.003972
  l1.weight: grad_norm = 0.157776
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.159623
Total gradient norm: 0.417219
=== Actor Training Debug (Iteration 5072) ===
Q mean: -11.541608
Q std: 15.424788
Actor loss: 11.545568
Action reg: 0.003961
  l1.weight: grad_norm = 0.123780
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.111591
Total gradient norm: 0.304345
=== Actor Training Debug (Iteration 5073) ===
Q mean: -11.424625
Q std: 15.971633
Actor loss: 11.428583
Action reg: 0.003957
  l1.weight: grad_norm = 0.152386
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.135852
Total gradient norm: 0.443632
=== Actor Training Debug (Iteration 5074) ===
Q mean: -11.357533
Q std: 15.162480
Actor loss: 11.361506
Action reg: 0.003973
  l1.weight: grad_norm = 0.205400
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.176570
Total gradient norm: 0.557459
=== Actor Training Debug (Iteration 5075) ===
Q mean: -12.435907
Q std: 16.844542
Actor loss: 12.439875
Action reg: 0.003967
  l1.weight: grad_norm = 0.130013
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.111145
Total gradient norm: 0.318259
=== Actor Training Debug (Iteration 5076) ===
Q mean: -11.927692
Q std: 15.617702
Actor loss: 11.931664
Action reg: 0.003972
  l1.weight: grad_norm = 0.094577
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.090509
Total gradient norm: 0.352586
=== Actor Training Debug (Iteration 5077) ===
Q mean: -9.788616
Q std: 14.024527
Actor loss: 9.792573
Action reg: 0.003957
  l1.weight: grad_norm = 0.196761
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.150577
Total gradient norm: 0.444721
=== Actor Training Debug (Iteration 5078) ===
Q mean: -9.910500
Q std: 14.395975
Actor loss: 9.914460
Action reg: 0.003961
  l1.weight: grad_norm = 0.166764
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.129316
Total gradient norm: 0.476518
=== Actor Training Debug (Iteration 5079) ===
Q mean: -11.516166
Q std: 16.306540
Actor loss: 11.520131
Action reg: 0.003965
  l1.weight: grad_norm = 0.078509
  l1.bias: grad_norm = 0.000571
  l2.weight: grad_norm = 0.068879
Total gradient norm: 0.198331
=== Actor Training Debug (Iteration 5080) ===
Q mean: -11.618547
Q std: 14.943336
Actor loss: 11.622524
Action reg: 0.003976
  l1.weight: grad_norm = 0.052656
  l1.bias: grad_norm = 0.000027
  l2.weight: grad_norm = 0.044393
Total gradient norm: 0.127514
=== Actor Training Debug (Iteration 5081) ===
Q mean: -11.269644
Q std: 15.121032
Actor loss: 11.273606
Action reg: 0.003962
  l1.weight: grad_norm = 0.110227
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.083546
Total gradient norm: 0.240673
=== Actor Training Debug (Iteration 5082) ===
Q mean: -10.221403
Q std: 14.980148
Actor loss: 10.225380
Action reg: 0.003976
  l1.weight: grad_norm = 0.202328
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.174070
Total gradient norm: 0.470062
=== Actor Training Debug (Iteration 5083) ===
Q mean: -11.889002
Q std: 16.117273
Actor loss: 11.892960
Action reg: 0.003957
  l1.weight: grad_norm = 0.210923
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.164781
Total gradient norm: 0.533668
=== Actor Training Debug (Iteration 5084) ===
Q mean: -9.489394
Q std: 14.798958
Actor loss: 9.493332
Action reg: 0.003938
  l1.weight: grad_norm = 0.259457
  l1.bias: grad_norm = 0.000554
  l2.weight: grad_norm = 0.202898
Total gradient norm: 0.553665
=== Actor Training Debug (Iteration 5085) ===
Q mean: -10.046362
Q std: 14.003634
Actor loss: 10.050330
Action reg: 0.003968
  l1.weight: grad_norm = 0.136825
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.133423
Total gradient norm: 0.372522
=== Actor Training Debug (Iteration 5086) ===
Q mean: -11.058542
Q std: 15.430114
Actor loss: 11.062509
Action reg: 0.003966
  l1.weight: grad_norm = 0.083294
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.061857
Total gradient norm: 0.201673
=== Actor Training Debug (Iteration 5087) ===
Q mean: -10.269817
Q std: 14.922419
Actor loss: 10.273766
Action reg: 0.003949
  l1.weight: grad_norm = 0.162228
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.156097
Total gradient norm: 0.388434
=== Actor Training Debug (Iteration 5088) ===
Q mean: -10.873315
Q std: 16.129227
Actor loss: 10.877276
Action reg: 0.003962
  l1.weight: grad_norm = 0.160519
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.140377
Total gradient norm: 0.459425
=== Actor Training Debug (Iteration 5089) ===
Q mean: -10.680537
Q std: 15.833838
Actor loss: 10.684500
Action reg: 0.003962
  l1.weight: grad_norm = 0.152226
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.110482
Total gradient norm: 0.333238
=== Actor Training Debug (Iteration 5090) ===
Q mean: -10.138601
Q std: 14.382319
Actor loss: 10.142573
Action reg: 0.003972
  l1.weight: grad_norm = 0.108119
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.089971
Total gradient norm: 0.264992
=== Actor Training Debug (Iteration 5091) ===
Q mean: -10.151148
Q std: 14.081847
Actor loss: 10.155115
Action reg: 0.003967
  l1.weight: grad_norm = 0.502216
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.450925
Total gradient norm: 1.115666
=== Actor Training Debug (Iteration 5092) ===
Q mean: -10.715968
Q std: 15.845078
Actor loss: 10.719924
Action reg: 0.003956
  l1.weight: grad_norm = 0.214676
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.186350
Total gradient norm: 0.549237
=== Actor Training Debug (Iteration 5093) ===
Q mean: -10.719849
Q std: 16.174454
Actor loss: 10.723811
Action reg: 0.003962
  l1.weight: grad_norm = 0.163987
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.158580
Total gradient norm: 0.476967
=== Actor Training Debug (Iteration 5094) ===
Q mean: -11.794666
Q std: 17.500273
Actor loss: 11.798640
Action reg: 0.003974
  l1.weight: grad_norm = 0.185530
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.136455
Total gradient norm: 0.414534
=== Actor Training Debug (Iteration 5095) ===
Q mean: -10.713132
Q std: 16.253042
Actor loss: 10.717093
Action reg: 0.003961
  l1.weight: grad_norm = 0.182761
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.142482
Total gradient norm: 0.367957
=== Actor Training Debug (Iteration 5096) ===
Q mean: -11.918869
Q std: 15.864971
Actor loss: 11.922842
Action reg: 0.003973
  l1.weight: grad_norm = 0.062084
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.049927
Total gradient norm: 0.141019
=== Actor Training Debug (Iteration 5097) ===
Q mean: -10.177553
Q std: 15.929263
Actor loss: 10.181522
Action reg: 0.003969
  l1.weight: grad_norm = 0.193408
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.177502
Total gradient norm: 0.481945
=== Actor Training Debug (Iteration 5098) ===
Q mean: -12.050891
Q std: 15.536917
Actor loss: 12.054860
Action reg: 0.003969
  l1.weight: grad_norm = 0.153704
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.125665
Total gradient norm: 0.361625
=== Actor Training Debug (Iteration 5099) ===
Q mean: -11.752964
Q std: 15.809011
Actor loss: 11.756917
Action reg: 0.003953
  l1.weight: grad_norm = 0.393025
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.275994
Total gradient norm: 0.975704
=== Actor Training Debug (Iteration 5100) ===
Q mean: -12.316320
Q std: 16.782070
Actor loss: 12.320294
Action reg: 0.003974
  l1.weight: grad_norm = 0.094696
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.079727
Total gradient norm: 0.239251
Episode 101: Steps=100, Reward=-261.578, Buffer_size=10100
=== Actor Training Debug (Iteration 5101) ===
Q mean: -11.179394
Q std: 14.775061
Actor loss: 11.183375
Action reg: 0.003982
  l1.weight: grad_norm = 0.097692
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.076842
Total gradient norm: 0.211204
=== Actor Training Debug (Iteration 5102) ===
Q mean: -10.531643
Q std: 14.677351
Actor loss: 10.535609
Action reg: 0.003966
  l1.weight: grad_norm = 0.129986
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.107432
Total gradient norm: 0.322786
=== Actor Training Debug (Iteration 5103) ===
Q mean: -9.734373
Q std: 14.224992
Actor loss: 9.738304
Action reg: 0.003931
  l1.weight: grad_norm = 0.274757
  l1.bias: grad_norm = 0.000646
  l2.weight: grad_norm = 0.225596
Total gradient norm: 0.661782
=== Actor Training Debug (Iteration 5104) ===
Q mean: -9.071520
Q std: 13.237220
Actor loss: 9.075489
Action reg: 0.003969
  l1.weight: grad_norm = 0.209612
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.148572
Total gradient norm: 0.420664
=== Actor Training Debug (Iteration 5105) ===
Q mean: -12.283051
Q std: 16.336588
Actor loss: 12.287013
Action reg: 0.003962
  l1.weight: grad_norm = 0.113672
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.110959
Total gradient norm: 0.295509
=== Actor Training Debug (Iteration 5106) ===
Q mean: -10.587855
Q std: 15.710415
Actor loss: 10.591818
Action reg: 0.003963
  l1.weight: grad_norm = 0.201211
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.154062
Total gradient norm: 0.431749
=== Actor Training Debug (Iteration 5107) ===
Q mean: -10.824609
Q std: 14.884109
Actor loss: 10.828577
Action reg: 0.003968
  l1.weight: grad_norm = 0.110603
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.091524
Total gradient norm: 0.260730
=== Actor Training Debug (Iteration 5108) ===
Q mean: -11.887914
Q std: 16.413208
Actor loss: 11.891876
Action reg: 0.003962
  l1.weight: grad_norm = 0.182332
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.174545
Total gradient norm: 0.479052
=== Actor Training Debug (Iteration 5109) ===
Q mean: -10.575094
Q std: 14.910582
Actor loss: 10.579062
Action reg: 0.003968
  l1.weight: grad_norm = 0.631565
  l1.bias: grad_norm = 0.000405
  l2.weight: grad_norm = 0.508367
Total gradient norm: 1.778079
=== Actor Training Debug (Iteration 5110) ===
Q mean: -10.747080
Q std: 15.299265
Actor loss: 10.751023
Action reg: 0.003943
  l1.weight: grad_norm = 0.196684
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.135544
Total gradient norm: 0.410078
=== Actor Training Debug (Iteration 5111) ===
Q mean: -9.670462
Q std: 16.258617
Actor loss: 9.674431
Action reg: 0.003969
  l1.weight: grad_norm = 0.269419
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.211515
Total gradient norm: 0.721653
=== Actor Training Debug (Iteration 5112) ===
Q mean: -9.155355
Q std: 14.541725
Actor loss: 9.159319
Action reg: 0.003963
  l1.weight: grad_norm = 0.182075
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.152996
Total gradient norm: 0.448420
=== Actor Training Debug (Iteration 5113) ===
Q mean: -11.953106
Q std: 15.960862
Actor loss: 11.957075
Action reg: 0.003969
  l1.weight: grad_norm = 0.113037
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.117761
Total gradient norm: 0.355054
=== Actor Training Debug (Iteration 5114) ===
Q mean: -10.601220
Q std: 16.572233
Actor loss: 10.605191
Action reg: 0.003971
  l1.weight: grad_norm = 0.183041
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.143837
Total gradient norm: 0.406884
=== Actor Training Debug (Iteration 5115) ===
Q mean: -10.411386
Q std: 15.021814
Actor loss: 10.415360
Action reg: 0.003974
  l1.weight: grad_norm = 0.215420
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.170423
Total gradient norm: 0.431376
=== Actor Training Debug (Iteration 5116) ===
Q mean: -10.919026
Q std: 16.290880
Actor loss: 10.922991
Action reg: 0.003964
  l1.weight: grad_norm = 0.256699
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.216846
Total gradient norm: 0.726642
=== Actor Training Debug (Iteration 5117) ===
Q mean: -10.481466
Q std: 15.076721
Actor loss: 10.485421
Action reg: 0.003955
  l1.weight: grad_norm = 0.279505
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.241460
Total gradient norm: 0.800104
=== Actor Training Debug (Iteration 5118) ===
Q mean: -10.819097
Q std: 16.286732
Actor loss: 10.823052
Action reg: 0.003956
  l1.weight: grad_norm = 0.074665
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.076609
Total gradient norm: 0.266186
=== Actor Training Debug (Iteration 5119) ===
Q mean: -11.885040
Q std: 16.359079
Actor loss: 11.889005
Action reg: 0.003965
  l1.weight: grad_norm = 0.153625
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.139480
Total gradient norm: 0.425646
=== Actor Training Debug (Iteration 5120) ===
Q mean: -10.726954
Q std: 14.365856
Actor loss: 10.730922
Action reg: 0.003968
  l1.weight: grad_norm = 0.185392
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.134613
Total gradient norm: 0.411609
=== Actor Training Debug (Iteration 5121) ===
Q mean: -9.603582
Q std: 14.314102
Actor loss: 9.607571
Action reg: 0.003988
  l1.weight: grad_norm = 0.096878
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.085986
Total gradient norm: 0.243659
=== Actor Training Debug (Iteration 5122) ===
Q mean: -9.054621
Q std: 13.794970
Actor loss: 9.058576
Action reg: 0.003954
  l1.weight: grad_norm = 0.192119
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.149911
Total gradient norm: 0.431867
=== Actor Training Debug (Iteration 5123) ===
Q mean: -9.927817
Q std: 15.774070
Actor loss: 9.931789
Action reg: 0.003972
  l1.weight: grad_norm = 0.259328
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.189413
Total gradient norm: 0.514031
=== Actor Training Debug (Iteration 5124) ===
Q mean: -12.689119
Q std: 15.508956
Actor loss: 12.693091
Action reg: 0.003972
  l1.weight: grad_norm = 0.139970
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.128904
Total gradient norm: 0.382125
=== Actor Training Debug (Iteration 5125) ===
Q mean: -11.835264
Q std: 14.737391
Actor loss: 11.839228
Action reg: 0.003963
  l1.weight: grad_norm = 0.173759
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.148131
Total gradient norm: 0.425647
=== Actor Training Debug (Iteration 5126) ===
Q mean: -11.382064
Q std: 16.157394
Actor loss: 11.386030
Action reg: 0.003967
  l1.weight: grad_norm = 0.135364
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.109560
Total gradient norm: 0.346041
=== Actor Training Debug (Iteration 5127) ===
Q mean: -9.584764
Q std: 14.857486
Actor loss: 9.588718
Action reg: 0.003954
  l1.weight: grad_norm = 0.198394
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.162578
Total gradient norm: 0.415218
=== Actor Training Debug (Iteration 5128) ===
Q mean: -11.975349
Q std: 17.430136
Actor loss: 11.979322
Action reg: 0.003973
  l1.weight: grad_norm = 0.112365
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.095854
Total gradient norm: 0.345997
=== Actor Training Debug (Iteration 5129) ===
Q mean: -11.516463
Q std: 15.505109
Actor loss: 11.520425
Action reg: 0.003961
  l1.weight: grad_norm = 0.115617
  l1.bias: grad_norm = 0.000400
  l2.weight: grad_norm = 0.107061
Total gradient norm: 0.290720
=== Actor Training Debug (Iteration 5130) ===
Q mean: -11.104132
Q std: 15.041980
Actor loss: 11.108101
Action reg: 0.003969
  l1.weight: grad_norm = 0.074355
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.061577
Total gradient norm: 0.167718
=== Actor Training Debug (Iteration 5131) ===
Q mean: -9.625685
Q std: 15.541044
Actor loss: 9.629632
Action reg: 0.003947
  l1.weight: grad_norm = 0.140912
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.113500
Total gradient norm: 0.397017
=== Actor Training Debug (Iteration 5132) ===
Q mean: -10.846840
Q std: 16.309816
Actor loss: 10.850822
Action reg: 0.003982
  l1.weight: grad_norm = 0.097787
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.085353
Total gradient norm: 0.243825
=== Actor Training Debug (Iteration 5133) ===
Q mean: -10.194096
Q std: 15.772921
Actor loss: 10.198061
Action reg: 0.003966
  l1.weight: grad_norm = 0.151689
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.129450
Total gradient norm: 0.418993
=== Actor Training Debug (Iteration 5134) ===
Q mean: -11.481839
Q std: 15.276770
Actor loss: 11.485806
Action reg: 0.003967
  l1.weight: grad_norm = 0.118314
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.094505
Total gradient norm: 0.276821
=== Actor Training Debug (Iteration 5135) ===
Q mean: -10.456497
Q std: 14.366068
Actor loss: 10.460461
Action reg: 0.003963
  l1.weight: grad_norm = 0.093449
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.085064
Total gradient norm: 0.238209
=== Actor Training Debug (Iteration 5136) ===
Q mean: -11.547497
Q std: 14.677086
Actor loss: 11.551460
Action reg: 0.003963
  l1.weight: grad_norm = 0.130506
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.114355
Total gradient norm: 0.350782
=== Actor Training Debug (Iteration 5137) ===
Q mean: -10.392306
Q std: 13.912327
Actor loss: 10.396285
Action reg: 0.003979
  l1.weight: grad_norm = 0.137386
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.116554
Total gradient norm: 0.358378
=== Actor Training Debug (Iteration 5138) ===
Q mean: -10.911989
Q std: 16.433260
Actor loss: 10.915955
Action reg: 0.003966
  l1.weight: grad_norm = 0.199021
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.167554
Total gradient norm: 0.574930
=== Actor Training Debug (Iteration 5139) ===
Q mean: -11.823736
Q std: 16.228380
Actor loss: 11.827712
Action reg: 0.003976
  l1.weight: grad_norm = 0.160930
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.144572
Total gradient norm: 0.423607
=== Actor Training Debug (Iteration 5140) ===
Q mean: -9.602972
Q std: 14.815418
Actor loss: 9.606941
Action reg: 0.003970
  l1.weight: grad_norm = 0.106342
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.091649
Total gradient norm: 0.248154
=== Actor Training Debug (Iteration 5141) ===
Q mean: -9.294830
Q std: 14.774114
Actor loss: 9.298795
Action reg: 0.003964
  l1.weight: grad_norm = 0.146434
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.131153
Total gradient norm: 0.346117
=== Actor Training Debug (Iteration 5142) ===
Q mean: -12.293322
Q std: 16.190960
Actor loss: 12.297303
Action reg: 0.003981
  l1.weight: grad_norm = 0.149185
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.117585
Total gradient norm: 0.327084
=== Actor Training Debug (Iteration 5143) ===
Q mean: -11.307055
Q std: 15.764395
Actor loss: 11.311020
Action reg: 0.003965
  l1.weight: grad_norm = 0.107520
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.095066
Total gradient norm: 0.332976
=== Actor Training Debug (Iteration 5144) ===
Q mean: -10.543778
Q std: 15.571838
Actor loss: 10.547733
Action reg: 0.003955
  l1.weight: grad_norm = 0.090783
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.080526
Total gradient norm: 0.243140
=== Actor Training Debug (Iteration 5145) ===
Q mean: -10.498697
Q std: 15.665947
Actor loss: 10.502664
Action reg: 0.003966
  l1.weight: grad_norm = 0.248704
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.215719
Total gradient norm: 0.563695
=== Actor Training Debug (Iteration 5146) ===
Q mean: -12.505123
Q std: 17.501266
Actor loss: 12.509089
Action reg: 0.003966
  l1.weight: grad_norm = 0.108952
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.093714
Total gradient norm: 0.311250
=== Actor Training Debug (Iteration 5147) ===
Q mean: -11.150015
Q std: 15.482876
Actor loss: 11.153990
Action reg: 0.003975
  l1.weight: grad_norm = 0.109533
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.088619
Total gradient norm: 0.238462
=== Actor Training Debug (Iteration 5148) ===
Q mean: -11.874922
Q std: 16.070095
Actor loss: 11.878901
Action reg: 0.003978
  l1.weight: grad_norm = 0.178476
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.140693
Total gradient norm: 0.435659
=== Actor Training Debug (Iteration 5149) ===
Q mean: -9.773049
Q std: 14.792351
Actor loss: 9.777019
Action reg: 0.003969
  l1.weight: grad_norm = 0.140210
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.122765
Total gradient norm: 0.367918
=== Actor Training Debug (Iteration 5150) ===
Q mean: -10.214567
Q std: 15.534916
Actor loss: 10.218534
Action reg: 0.003966
  l1.weight: grad_norm = 0.218400
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.168808
Total gradient norm: 0.462486
=== Actor Training Debug (Iteration 5151) ===
Q mean: -10.798319
Q std: 14.757633
Actor loss: 10.802279
Action reg: 0.003960
  l1.weight: grad_norm = 0.102151
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.086595
Total gradient norm: 0.226740
=== Actor Training Debug (Iteration 5152) ===
Q mean: -10.912042
Q std: 16.486225
Actor loss: 10.915986
Action reg: 0.003945
  l1.weight: grad_norm = 0.230863
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.247088
Total gradient norm: 0.751761
=== Actor Training Debug (Iteration 5153) ===
Q mean: -9.735868
Q std: 14.720708
Actor loss: 9.739827
Action reg: 0.003959
  l1.weight: grad_norm = 0.204525
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.191680
Total gradient norm: 0.581450
=== Actor Training Debug (Iteration 5154) ===
Q mean: -10.901787
Q std: 15.533253
Actor loss: 10.905754
Action reg: 0.003968
  l1.weight: grad_norm = 0.177645
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.148304
Total gradient norm: 0.498027
=== Actor Training Debug (Iteration 5155) ===
Q mean: -11.735576
Q std: 16.685625
Actor loss: 11.739544
Action reg: 0.003968
  l1.weight: grad_norm = 0.182048
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.152639
Total gradient norm: 0.504813
=== Actor Training Debug (Iteration 5156) ===
Q mean: -10.916231
Q std: 14.356451
Actor loss: 10.920208
Action reg: 0.003977
  l1.weight: grad_norm = 0.083180
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.083303
Total gradient norm: 0.282069
=== Actor Training Debug (Iteration 5157) ===
Q mean: -10.708585
Q std: 13.428902
Actor loss: 10.712553
Action reg: 0.003968
  l1.weight: grad_norm = 0.177462
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.167425
Total gradient norm: 0.453493
=== Actor Training Debug (Iteration 5158) ===
Q mean: -10.512060
Q std: 14.390034
Actor loss: 10.516026
Action reg: 0.003966
  l1.weight: grad_norm = 0.175730
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.163998
Total gradient norm: 0.504755
=== Actor Training Debug (Iteration 5159) ===
Q mean: -10.238447
Q std: 15.062287
Actor loss: 10.242419
Action reg: 0.003972
  l1.weight: grad_norm = 0.084949
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.070857
Total gradient norm: 0.209279
=== Actor Training Debug (Iteration 5160) ===
Q mean: -12.423057
Q std: 16.880600
Actor loss: 12.427029
Action reg: 0.003972
  l1.weight: grad_norm = 0.115621
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.095155
Total gradient norm: 0.279479
=== Actor Training Debug (Iteration 5161) ===
Q mean: -9.598000
Q std: 14.255094
Actor loss: 9.601976
Action reg: 0.003977
  l1.weight: grad_norm = 0.158090
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.131440
Total gradient norm: 0.386983
=== Actor Training Debug (Iteration 5162) ===
Q mean: -9.864183
Q std: 14.689173
Actor loss: 9.868142
Action reg: 0.003959
  l1.weight: grad_norm = 0.155065
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.146621
Total gradient norm: 0.429710
=== Actor Training Debug (Iteration 5163) ===
Q mean: -9.598501
Q std: 14.800022
Actor loss: 9.602461
Action reg: 0.003960
  l1.weight: grad_norm = 0.212965
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.202115
Total gradient norm: 0.509887
=== Actor Training Debug (Iteration 5164) ===
Q mean: -10.170353
Q std: 14.483195
Actor loss: 10.174327
Action reg: 0.003974
  l1.weight: grad_norm = 0.191281
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.150213
Total gradient norm: 0.423152
=== Actor Training Debug (Iteration 5165) ===
Q mean: -9.103868
Q std: 13.416612
Actor loss: 9.107808
Action reg: 0.003940
  l1.weight: grad_norm = 0.106327
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.097851
Total gradient norm: 0.298516
=== Actor Training Debug (Iteration 5166) ===
Q mean: -9.296865
Q std: 14.025984
Actor loss: 9.300839
Action reg: 0.003975
  l1.weight: grad_norm = 0.108166
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.087624
Total gradient norm: 0.268021
=== Actor Training Debug (Iteration 5167) ===
Q mean: -10.101297
Q std: 14.862782
Actor loss: 10.105263
Action reg: 0.003965
  l1.weight: grad_norm = 0.099454
  l1.bias: grad_norm = 0.000528
  l2.weight: grad_norm = 0.083107
Total gradient norm: 0.210738
=== Actor Training Debug (Iteration 5168) ===
Q mean: -12.034229
Q std: 16.230537
Actor loss: 12.038198
Action reg: 0.003968
  l1.weight: grad_norm = 0.143427
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.117945
Total gradient norm: 0.313748
=== Actor Training Debug (Iteration 5169) ===
Q mean: -11.012493
Q std: 16.597567
Actor loss: 11.016464
Action reg: 0.003971
  l1.weight: grad_norm = 0.185312
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.154260
Total gradient norm: 0.440812
=== Actor Training Debug (Iteration 5170) ===
Q mean: -10.135690
Q std: 15.141785
Actor loss: 10.139664
Action reg: 0.003974
  l1.weight: grad_norm = 0.073671
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.067802
Total gradient norm: 0.179564
=== Actor Training Debug (Iteration 5171) ===
Q mean: -11.495262
Q std: 15.760337
Actor loss: 11.499230
Action reg: 0.003969
  l1.weight: grad_norm = 0.110128
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.094329
Total gradient norm: 0.280612
=== Actor Training Debug (Iteration 5172) ===
Q mean: -9.028706
Q std: 14.492583
Actor loss: 9.032665
Action reg: 0.003960
  l1.weight: grad_norm = 0.195043
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.178212
Total gradient norm: 0.489862
=== Actor Training Debug (Iteration 5173) ===
Q mean: -11.551806
Q std: 14.247615
Actor loss: 11.555789
Action reg: 0.003982
  l1.weight: grad_norm = 0.278435
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.260310
Total gradient norm: 0.740033
=== Actor Training Debug (Iteration 5174) ===
Q mean: -11.060529
Q std: 15.157221
Actor loss: 11.064492
Action reg: 0.003964
  l1.weight: grad_norm = 0.273329
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.225925
Total gradient norm: 0.620777
=== Actor Training Debug (Iteration 5175) ===
Q mean: -11.702265
Q std: 15.653015
Actor loss: 11.706228
Action reg: 0.003963
  l1.weight: grad_norm = 0.159498
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.136514
Total gradient norm: 0.442823
=== Actor Training Debug (Iteration 5176) ===
Q mean: -9.030514
Q std: 13.797112
Actor loss: 9.034469
Action reg: 0.003954
  l1.weight: grad_norm = 0.095813
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.081511
Total gradient norm: 0.255660
=== Actor Training Debug (Iteration 5177) ===
Q mean: -12.118891
Q std: 16.281073
Actor loss: 12.122872
Action reg: 0.003981
  l1.weight: grad_norm = 0.146032
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.113187
Total gradient norm: 0.299547
=== Actor Training Debug (Iteration 5178) ===
Q mean: -10.639719
Q std: 15.866799
Actor loss: 10.643671
Action reg: 0.003952
  l1.weight: grad_norm = 0.165170
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.139380
Total gradient norm: 0.341465
=== Actor Training Debug (Iteration 5179) ===
Q mean: -10.586074
Q std: 16.062847
Actor loss: 10.590039
Action reg: 0.003966
  l1.weight: grad_norm = 0.149501
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.136458
Total gradient norm: 0.343605
=== Actor Training Debug (Iteration 5180) ===
Q mean: -11.471157
Q std: 16.200464
Actor loss: 11.475131
Action reg: 0.003974
  l1.weight: grad_norm = 0.199396
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.171096
Total gradient norm: 0.669439
=== Actor Training Debug (Iteration 5181) ===
Q mean: -11.269918
Q std: 16.648727
Actor loss: 11.273896
Action reg: 0.003978
  l1.weight: grad_norm = 0.188853
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.164394
Total gradient norm: 0.489610
=== Actor Training Debug (Iteration 5182) ===
Q mean: -9.036325
Q std: 14.513256
Actor loss: 9.040292
Action reg: 0.003967
  l1.weight: grad_norm = 0.181074
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.135075
Total gradient norm: 0.359361
=== Actor Training Debug (Iteration 5183) ===
Q mean: -10.279885
Q std: 14.831704
Actor loss: 10.283844
Action reg: 0.003959
  l1.weight: grad_norm = 0.171779
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.141588
Total gradient norm: 0.433685
=== Actor Training Debug (Iteration 5184) ===
Q mean: -11.055453
Q std: 15.645978
Actor loss: 11.059411
Action reg: 0.003958
  l1.weight: grad_norm = 0.154949
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.130004
Total gradient norm: 0.420976
=== Actor Training Debug (Iteration 5185) ===
Q mean: -10.649608
Q std: 14.741611
Actor loss: 10.653585
Action reg: 0.003978
  l1.weight: grad_norm = 0.183579
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.174342
Total gradient norm: 0.458724
=== Actor Training Debug (Iteration 5186) ===
Q mean: -9.899712
Q std: 12.259077
Actor loss: 9.903658
Action reg: 0.003946
  l1.weight: grad_norm = 0.171808
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.143270
Total gradient norm: 0.413100
=== Actor Training Debug (Iteration 5187) ===
Q mean: -10.972422
Q std: 16.210876
Actor loss: 10.976379
Action reg: 0.003958
  l1.weight: grad_norm = 0.324328
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.259265
Total gradient norm: 0.676494
=== Actor Training Debug (Iteration 5188) ===
Q mean: -11.530081
Q std: 15.934919
Actor loss: 11.534052
Action reg: 0.003971
  l1.weight: grad_norm = 0.110849
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.087963
Total gradient norm: 0.228820
=== Actor Training Debug (Iteration 5189) ===
Q mean: -9.498441
Q std: 13.615849
Actor loss: 9.502411
Action reg: 0.003970
  l1.weight: grad_norm = 0.109344
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.097267
Total gradient norm: 0.309409
=== Actor Training Debug (Iteration 5190) ===
Q mean: -9.849293
Q std: 15.721282
Actor loss: 9.853262
Action reg: 0.003970
  l1.weight: grad_norm = 0.190955
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.149412
Total gradient norm: 0.403024
=== Actor Training Debug (Iteration 5191) ===
Q mean: -12.279507
Q std: 16.590860
Actor loss: 12.283480
Action reg: 0.003973
  l1.weight: grad_norm = 0.153406
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.144245
Total gradient norm: 0.387522
=== Actor Training Debug (Iteration 5192) ===
Q mean: -9.174669
Q std: 13.521697
Actor loss: 9.178610
Action reg: 0.003940
  l1.weight: grad_norm = 0.121207
  l1.bias: grad_norm = 0.000642
  l2.weight: grad_norm = 0.095343
Total gradient norm: 0.300194
=== Actor Training Debug (Iteration 5193) ===
Q mean: -10.040068
Q std: 13.387071
Actor loss: 10.044050
Action reg: 0.003983
  l1.weight: grad_norm = 0.106247
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.097787
Total gradient norm: 0.284965
=== Actor Training Debug (Iteration 5194) ===
Q mean: -10.433727
Q std: 14.757976
Actor loss: 10.437703
Action reg: 0.003976
  l1.weight: grad_norm = 0.213770
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.184082
Total gradient norm: 0.501870
=== Actor Training Debug (Iteration 5195) ===
Q mean: -11.166779
Q std: 16.859575
Actor loss: 11.170728
Action reg: 0.003949
  l1.weight: grad_norm = 0.208720
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.166394
Total gradient norm: 0.450534
=== Actor Training Debug (Iteration 5196) ===
Q mean: -12.074789
Q std: 16.780781
Actor loss: 12.078753
Action reg: 0.003964
  l1.weight: grad_norm = 0.116890
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.096175
Total gradient norm: 0.272671
=== Actor Training Debug (Iteration 5197) ===
Q mean: -11.704125
Q std: 15.169659
Actor loss: 11.708116
Action reg: 0.003990
  l1.weight: grad_norm = 0.091441
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.078522
Total gradient norm: 0.226029
=== Actor Training Debug (Iteration 5198) ===
Q mean: -10.694847
Q std: 14.223752
Actor loss: 10.698807
Action reg: 0.003960
  l1.weight: grad_norm = 0.194238
  l1.bias: grad_norm = 0.000839
  l2.weight: grad_norm = 0.160963
Total gradient norm: 0.444877
=== Actor Training Debug (Iteration 5199) ===
Q mean: -9.738859
Q std: 13.197409
Actor loss: 9.742822
Action reg: 0.003962
  l1.weight: grad_norm = 0.205360
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.168901
Total gradient norm: 0.490853
=== Actor Training Debug (Iteration 5200) ===
Q mean: -10.771368
Q std: 16.112209
Actor loss: 10.775333
Action reg: 0.003966
  l1.weight: grad_norm = 0.200522
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.163932
Total gradient norm: 0.518490
=== Actor Training Debug (Iteration 5201) ===
Q mean: -10.548419
Q std: 16.005322
Actor loss: 10.552378
Action reg: 0.003958
  l1.weight: grad_norm = 0.242780
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.201265
Total gradient norm: 0.615267
=== Actor Training Debug (Iteration 5202) ===
Q mean: -9.819477
Q std: 15.075705
Actor loss: 9.823421
Action reg: 0.003944
  l1.weight: grad_norm = 0.163984
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.126329
Total gradient norm: 0.378296
=== Actor Training Debug (Iteration 5203) ===
Q mean: -9.882518
Q std: 15.977817
Actor loss: 9.886471
Action reg: 0.003953
  l1.weight: grad_norm = 0.128372
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.116171
Total gradient norm: 0.350565
=== Actor Training Debug (Iteration 5204) ===
Q mean: -11.138254
Q std: 15.030854
Actor loss: 11.142222
Action reg: 0.003968
  l1.weight: grad_norm = 0.234447
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.200388
Total gradient norm: 0.573679
=== Actor Training Debug (Iteration 5205) ===
Q mean: -10.578726
Q std: 13.911922
Actor loss: 10.582693
Action reg: 0.003967
  l1.weight: grad_norm = 0.308383
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.247421
Total gradient norm: 0.624846
=== Actor Training Debug (Iteration 5206) ===
Q mean: -11.911052
Q std: 17.201241
Actor loss: 11.915010
Action reg: 0.003959
  l1.weight: grad_norm = 0.123924
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.103208
Total gradient norm: 0.323724
=== Actor Training Debug (Iteration 5207) ===
Q mean: -11.246468
Q std: 16.729746
Actor loss: 11.250425
Action reg: 0.003958
  l1.weight: grad_norm = 0.193065
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.159785
Total gradient norm: 0.452661
=== Actor Training Debug (Iteration 5208) ===
Q mean: -11.673340
Q std: 16.847740
Actor loss: 11.677287
Action reg: 0.003947
  l1.weight: grad_norm = 0.170841
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.131028
Total gradient norm: 0.341480
=== Actor Training Debug (Iteration 5209) ===
Q mean: -11.070752
Q std: 15.559705
Actor loss: 11.074709
Action reg: 0.003957
  l1.weight: grad_norm = 0.139908
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.129511
Total gradient norm: 0.361855
=== Actor Training Debug (Iteration 5210) ===
Q mean: -12.043053
Q std: 15.129212
Actor loss: 12.047007
Action reg: 0.003954
  l1.weight: grad_norm = 0.193529
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.170937
Total gradient norm: 0.485022
=== Actor Training Debug (Iteration 5211) ===
Q mean: -10.349874
Q std: 15.742306
Actor loss: 10.353839
Action reg: 0.003964
  l1.weight: grad_norm = 0.085958
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.078986
Total gradient norm: 0.228586
=== Actor Training Debug (Iteration 5212) ===
Q mean: -12.301130
Q std: 16.032722
Actor loss: 12.305103
Action reg: 0.003973
  l1.weight: grad_norm = 0.115688
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.093491
Total gradient norm: 0.273536
=== Actor Training Debug (Iteration 5213) ===
Q mean: -10.447136
Q std: 15.698933
Actor loss: 10.451079
Action reg: 0.003944
  l1.weight: grad_norm = 0.158186
  l1.bias: grad_norm = 0.000666
  l2.weight: grad_norm = 0.141324
Total gradient norm: 0.413054
=== Actor Training Debug (Iteration 5214) ===
Q mean: -13.088737
Q std: 17.163673
Actor loss: 13.092724
Action reg: 0.003986
  l1.weight: grad_norm = 0.078012
  l1.bias: grad_norm = 0.000033
  l2.weight: grad_norm = 0.069294
Total gradient norm: 0.208948
=== Actor Training Debug (Iteration 5215) ===
Q mean: -11.013027
Q std: 15.704749
Actor loss: 11.016986
Action reg: 0.003959
  l1.weight: grad_norm = 0.245456
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.209927
Total gradient norm: 0.594287
=== Actor Training Debug (Iteration 5216) ===
Q mean: -10.300309
Q std: 15.210248
Actor loss: 10.304285
Action reg: 0.003976
  l1.weight: grad_norm = 0.128634
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.118150
Total gradient norm: 0.317309
=== Actor Training Debug (Iteration 5217) ===
Q mean: -11.853008
Q std: 16.110680
Actor loss: 11.856974
Action reg: 0.003966
  l1.weight: grad_norm = 0.149588
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.122305
Total gradient norm: 0.336282
=== Actor Training Debug (Iteration 5218) ===
Q mean: -10.149776
Q std: 15.157793
Actor loss: 10.153747
Action reg: 0.003970
  l1.weight: grad_norm = 0.123781
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.124536
Total gradient norm: 0.335454
=== Actor Training Debug (Iteration 5219) ===
Q mean: -10.699607
Q std: 14.908055
Actor loss: 10.703583
Action reg: 0.003976
  l1.weight: grad_norm = 0.112526
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.102750
Total gradient norm: 0.247320
=== Actor Training Debug (Iteration 5220) ===
Q mean: -9.660921
Q std: 14.256633
Actor loss: 9.664903
Action reg: 0.003981
  l1.weight: grad_norm = 0.098951
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.079294
Total gradient norm: 0.263309
=== Actor Training Debug (Iteration 5221) ===
Q mean: -11.652758
Q std: 15.860148
Actor loss: 11.656738
Action reg: 0.003981
  l1.weight: grad_norm = 0.147680
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.119233
Total gradient norm: 0.364507
=== Actor Training Debug (Iteration 5222) ===
Q mean: -11.632933
Q std: 16.228880
Actor loss: 11.636907
Action reg: 0.003974
  l1.weight: grad_norm = 0.140342
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.130780
Total gradient norm: 0.321499
=== Actor Training Debug (Iteration 5223) ===
Q mean: -10.448242
Q std: 15.390992
Actor loss: 10.452214
Action reg: 0.003972
  l1.weight: grad_norm = 0.199686
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.173105
Total gradient norm: 0.556087
=== Actor Training Debug (Iteration 5224) ===
Q mean: -11.593231
Q std: 16.585903
Actor loss: 11.597203
Action reg: 0.003972
  l1.weight: grad_norm = 0.164664
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.122195
Total gradient norm: 0.338929
=== Actor Training Debug (Iteration 5225) ===
Q mean: -11.021755
Q std: 15.548441
Actor loss: 11.025718
Action reg: 0.003962
  l1.weight: grad_norm = 0.262977
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.252737
Total gradient norm: 0.752580
=== Actor Training Debug (Iteration 5226) ===
Q mean: -11.021476
Q std: 15.186472
Actor loss: 11.025444
Action reg: 0.003968
  l1.weight: grad_norm = 0.170083
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.144296
Total gradient norm: 0.421750
=== Actor Training Debug (Iteration 5227) ===
Q mean: -11.642365
Q std: 14.712847
Actor loss: 11.646341
Action reg: 0.003976
  l1.weight: grad_norm = 0.189198
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.136638
Total gradient norm: 0.381093
=== Actor Training Debug (Iteration 5228) ===
Q mean: -10.952990
Q std: 14.827034
Actor loss: 10.956959
Action reg: 0.003969
  l1.weight: grad_norm = 0.112003
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.088175
Total gradient norm: 0.243028
=== Actor Training Debug (Iteration 5229) ===
Q mean: -10.297867
Q std: 15.229357
Actor loss: 10.301847
Action reg: 0.003981
  l1.weight: grad_norm = 0.125342
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.103863
Total gradient norm: 0.309907
=== Actor Training Debug (Iteration 5230) ===
Q mean: -12.006355
Q std: 16.195335
Actor loss: 12.010333
Action reg: 0.003977
  l1.weight: grad_norm = 0.196932
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.171406
Total gradient norm: 0.515125
=== Actor Training Debug (Iteration 5231) ===
Q mean: -10.545742
Q std: 16.097313
Actor loss: 10.549709
Action reg: 0.003968
  l1.weight: grad_norm = 0.175404
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.135665
Total gradient norm: 0.377150
=== Actor Training Debug (Iteration 5232) ===
Q mean: -10.449462
Q std: 15.094470
Actor loss: 10.453427
Action reg: 0.003965
  l1.weight: grad_norm = 0.146166
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.129659
Total gradient norm: 0.359820
=== Actor Training Debug (Iteration 5233) ===
Q mean: -9.769354
Q std: 14.017078
Actor loss: 9.773323
Action reg: 0.003970
  l1.weight: grad_norm = 0.155937
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.129471
Total gradient norm: 0.328334
=== Actor Training Debug (Iteration 5234) ===
Q mean: -10.657742
Q std: 15.626148
Actor loss: 10.661712
Action reg: 0.003970
  l1.weight: grad_norm = 0.301697
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.235012
Total gradient norm: 0.691508
=== Actor Training Debug (Iteration 5235) ===
Q mean: -10.449826
Q std: 14.153358
Actor loss: 10.453783
Action reg: 0.003957
  l1.weight: grad_norm = 0.138386
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.121624
Total gradient norm: 0.353981
=== Actor Training Debug (Iteration 5236) ===
Q mean: -10.399132
Q std: 15.399331
Actor loss: 10.403095
Action reg: 0.003963
  l1.weight: grad_norm = 0.122577
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.111831
Total gradient norm: 0.297863
=== Actor Training Debug (Iteration 5237) ===
Q mean: -12.258020
Q std: 15.899899
Actor loss: 12.261993
Action reg: 0.003973
  l1.weight: grad_norm = 0.440367
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.328884
Total gradient norm: 0.865775
=== Actor Training Debug (Iteration 5238) ===
Q mean: -13.252481
Q std: 17.519592
Actor loss: 13.256466
Action reg: 0.003984
  l1.weight: grad_norm = 0.111438
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.085734
Total gradient norm: 0.258322
=== Actor Training Debug (Iteration 5239) ===
Q mean: -11.071956
Q std: 16.180275
Actor loss: 11.075918
Action reg: 0.003962
  l1.weight: grad_norm = 0.073102
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.069437
Total gradient norm: 0.192524
=== Actor Training Debug (Iteration 5240) ===
Q mean: -10.680026
Q std: 15.655617
Actor loss: 10.683989
Action reg: 0.003963
  l1.weight: grad_norm = 0.148616
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.128447
Total gradient norm: 0.341137
=== Actor Training Debug (Iteration 5241) ===
Q mean: -11.203207
Q std: 15.935699
Actor loss: 11.207156
Action reg: 0.003949
  l1.weight: grad_norm = 0.103137
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.092585
Total gradient norm: 0.268668
=== Actor Training Debug (Iteration 5242) ===
Q mean: -10.737343
Q std: 15.152791
Actor loss: 10.741303
Action reg: 0.003960
  l1.weight: grad_norm = 0.102518
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.090284
Total gradient norm: 0.294226
=== Actor Training Debug (Iteration 5243) ===
Q mean: -13.548837
Q std: 17.837421
Actor loss: 13.552794
Action reg: 0.003956
  l1.weight: grad_norm = 0.121563
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.107318
Total gradient norm: 0.332650
=== Actor Training Debug (Iteration 5244) ===
Q mean: -10.047528
Q std: 14.251421
Actor loss: 10.051503
Action reg: 0.003974
  l1.weight: grad_norm = 0.093276
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.091174
Total gradient norm: 0.232229
=== Actor Training Debug (Iteration 5245) ===
Q mean: -10.023871
Q std: 14.528680
Actor loss: 10.027838
Action reg: 0.003967
  l1.weight: grad_norm = 0.165444
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.141003
Total gradient norm: 0.409901
=== Actor Training Debug (Iteration 5246) ===
Q mean: -11.060645
Q std: 14.665037
Actor loss: 11.064618
Action reg: 0.003973
  l1.weight: grad_norm = 0.116424
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.090315
Total gradient norm: 0.240001
=== Actor Training Debug (Iteration 5247) ===
Q mean: -10.426935
Q std: 14.825191
Actor loss: 10.430904
Action reg: 0.003970
  l1.weight: grad_norm = 0.166671
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.152520
Total gradient norm: 0.451463
=== Actor Training Debug (Iteration 5248) ===
Q mean: -11.122362
Q std: 14.636844
Actor loss: 11.126327
Action reg: 0.003965
  l1.weight: grad_norm = 0.234062
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.191217
Total gradient norm: 0.708534
=== Actor Training Debug (Iteration 5249) ===
Q mean: -11.077592
Q std: 14.547945
Actor loss: 11.081565
Action reg: 0.003973
  l1.weight: grad_norm = 0.196715
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.168061
Total gradient norm: 0.562857
=== Actor Training Debug (Iteration 5250) ===
Q mean: -10.496014
Q std: 15.675372
Actor loss: 10.499963
Action reg: 0.003949
  l1.weight: grad_norm = 0.147354
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.143038
Total gradient norm: 0.411548
=== Actor Training Debug (Iteration 5251) ===
Q mean: -10.804125
Q std: 15.223351
Actor loss: 10.808089
Action reg: 0.003965
  l1.weight: grad_norm = 0.232567
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.185575
Total gradient norm: 0.526132
=== Actor Training Debug (Iteration 5252) ===
Q mean: -10.378583
Q std: 14.558327
Actor loss: 10.382565
Action reg: 0.003982
  l1.weight: grad_norm = 0.096667
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.080545
Total gradient norm: 0.247922
=== Actor Training Debug (Iteration 5253) ===
Q mean: -12.128288
Q std: 15.716483
Actor loss: 12.132263
Action reg: 0.003975
  l1.weight: grad_norm = 0.205828
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.176433
Total gradient norm: 0.537644
=== Actor Training Debug (Iteration 5254) ===
Q mean: -11.287660
Q std: 14.967163
Actor loss: 11.291624
Action reg: 0.003964
  l1.weight: grad_norm = 0.311528
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.239524
Total gradient norm: 0.671405
=== Actor Training Debug (Iteration 5255) ===
Q mean: -9.232258
Q std: 13.255305
Actor loss: 9.236230
Action reg: 0.003972
  l1.weight: grad_norm = 0.201943
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.172018
Total gradient norm: 0.458135
=== Actor Training Debug (Iteration 5256) ===
Q mean: -11.626122
Q std: 16.153845
Actor loss: 11.630094
Action reg: 0.003972
  l1.weight: grad_norm = 0.199750
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.173471
Total gradient norm: 0.519601
=== Actor Training Debug (Iteration 5257) ===
Q mean: -9.777082
Q std: 14.366432
Actor loss: 9.781058
Action reg: 0.003975
  l1.weight: grad_norm = 0.232927
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.196696
Total gradient norm: 0.549371
=== Actor Training Debug (Iteration 5258) ===
Q mean: -11.183725
Q std: 14.636063
Actor loss: 11.187696
Action reg: 0.003970
  l1.weight: grad_norm = 0.087241
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.072041
Total gradient norm: 0.240488
=== Actor Training Debug (Iteration 5259) ===
Q mean: -11.099864
Q std: 14.809846
Actor loss: 11.103833
Action reg: 0.003969
  l1.weight: grad_norm = 0.250034
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.217809
Total gradient norm: 0.716546
=== Actor Training Debug (Iteration 5260) ===
Q mean: -11.029499
Q std: 16.374220
Actor loss: 11.033460
Action reg: 0.003961
  l1.weight: grad_norm = 0.118201
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.092359
Total gradient norm: 0.247135
=== Actor Training Debug (Iteration 5261) ===
Q mean: -10.481833
Q std: 14.638114
Actor loss: 10.485796
Action reg: 0.003963
  l1.weight: grad_norm = 0.188757
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.153807
Total gradient norm: 0.398408
=== Actor Training Debug (Iteration 5262) ===
Q mean: -11.226439
Q std: 14.815397
Actor loss: 11.230405
Action reg: 0.003966
  l1.weight: grad_norm = 0.154294
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.135351
Total gradient norm: 0.412981
=== Actor Training Debug (Iteration 5263) ===
Q mean: -12.428432
Q std: 16.257118
Actor loss: 12.432408
Action reg: 0.003977
  l1.weight: grad_norm = 0.089126
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.077219
Total gradient norm: 0.202030
=== Actor Training Debug (Iteration 5264) ===
Q mean: -12.612444
Q std: 16.241884
Actor loss: 12.616414
Action reg: 0.003970
  l1.weight: grad_norm = 0.169899
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.158093
Total gradient norm: 0.457948
=== Actor Training Debug (Iteration 5265) ===
Q mean: -10.997246
Q std: 14.971190
Actor loss: 11.001209
Action reg: 0.003963
  l1.weight: grad_norm = 0.187319
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.161858
Total gradient norm: 0.440859
=== Actor Training Debug (Iteration 5266) ===
Q mean: -11.221557
Q std: 16.172190
Actor loss: 11.225528
Action reg: 0.003971
  l1.weight: grad_norm = 0.210238
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.173953
Total gradient norm: 0.528760
=== Actor Training Debug (Iteration 5267) ===
Q mean: -11.289013
Q std: 16.584011
Actor loss: 11.292986
Action reg: 0.003973
  l1.weight: grad_norm = 0.065550
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.052581
Total gradient norm: 0.162498
=== Actor Training Debug (Iteration 5268) ===
Q mean: -10.726726
Q std: 14.056616
Actor loss: 10.730695
Action reg: 0.003970
  l1.weight: grad_norm = 0.309218
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.247986
Total gradient norm: 0.678582
=== Actor Training Debug (Iteration 5269) ===
Q mean: -10.799112
Q std: 15.734212
Actor loss: 10.803039
Action reg: 0.003926
  l1.weight: grad_norm = 0.176036
  l1.bias: grad_norm = 0.001223
  l2.weight: grad_norm = 0.175991
Total gradient norm: 0.504463
=== Actor Training Debug (Iteration 5270) ===
Q mean: -12.055341
Q std: 16.027473
Actor loss: 12.059313
Action reg: 0.003972
  l1.weight: grad_norm = 0.190977
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.182496
Total gradient norm: 0.566625
=== Actor Training Debug (Iteration 5271) ===
Q mean: -11.533268
Q std: 16.494211
Actor loss: 11.537232
Action reg: 0.003964
  l1.weight: grad_norm = 0.236296
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.203232
Total gradient norm: 0.525772
=== Actor Training Debug (Iteration 5272) ===
Q mean: -9.616234
Q std: 13.520675
Actor loss: 9.620202
Action reg: 0.003968
  l1.weight: grad_norm = 0.165697
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.131631
Total gradient norm: 0.371097
=== Actor Training Debug (Iteration 5273) ===
Q mean: -9.767700
Q std: 14.779050
Actor loss: 9.771679
Action reg: 0.003979
  l1.weight: grad_norm = 0.148470
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.136646
Total gradient norm: 0.412870
=== Actor Training Debug (Iteration 5274) ===
Q mean: -12.008348
Q std: 15.651832
Actor loss: 12.012323
Action reg: 0.003976
  l1.weight: grad_norm = 0.077872
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.066559
Total gradient norm: 0.189248
=== Actor Training Debug (Iteration 5275) ===
Q mean: -10.583677
Q std: 14.125801
Actor loss: 10.587645
Action reg: 0.003967
  l1.weight: grad_norm = 0.198543
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.154914
Total gradient norm: 0.507429
=== Actor Training Debug (Iteration 5276) ===
Q mean: -10.713984
Q std: 14.257428
Actor loss: 10.717937
Action reg: 0.003953
  l1.weight: grad_norm = 0.175790
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.160476
Total gradient norm: 0.424420
=== Actor Training Debug (Iteration 5277) ===
Q mean: -10.795338
Q std: 14.527136
Actor loss: 10.799300
Action reg: 0.003962
  l1.weight: grad_norm = 0.156727
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.132462
Total gradient norm: 0.330988
=== Actor Training Debug (Iteration 5278) ===
Q mean: -11.407606
Q std: 15.088091
Actor loss: 11.411576
Action reg: 0.003970
  l1.weight: grad_norm = 0.166772
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.141417
Total gradient norm: 0.411178
=== Actor Training Debug (Iteration 5279) ===
Q mean: -10.333035
Q std: 15.490739
Actor loss: 10.337008
Action reg: 0.003973
  l1.weight: grad_norm = 0.135433
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.118955
Total gradient norm: 0.357572
=== Actor Training Debug (Iteration 5280) ===
Q mean: -10.492971
Q std: 14.964549
Actor loss: 10.496937
Action reg: 0.003965
  l1.weight: grad_norm = 0.209847
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.197421
Total gradient norm: 0.629295
=== Actor Training Debug (Iteration 5281) ===
Q mean: -10.184656
Q std: 16.152607
Actor loss: 10.188624
Action reg: 0.003968
  l1.weight: grad_norm = 0.225574
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.186441
Total gradient norm: 0.556594
=== Actor Training Debug (Iteration 5282) ===
Q mean: -11.967539
Q std: 16.724421
Actor loss: 11.971512
Action reg: 0.003973
  l1.weight: grad_norm = 0.219613
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.206226
Total gradient norm: 0.554254
=== Actor Training Debug (Iteration 5283) ===
Q mean: -11.169649
Q std: 16.436419
Actor loss: 11.173601
Action reg: 0.003952
  l1.weight: grad_norm = 0.137004
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.130376
Total gradient norm: 0.356742
=== Actor Training Debug (Iteration 5284) ===
Q mean: -11.411728
Q std: 15.876644
Actor loss: 11.415713
Action reg: 0.003986
  l1.weight: grad_norm = 0.141489
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.124067
Total gradient norm: 0.351123
=== Actor Training Debug (Iteration 5285) ===
Q mean: -11.520094
Q std: 14.699007
Actor loss: 11.524051
Action reg: 0.003957
  l1.weight: grad_norm = 0.265030
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.244317
Total gradient norm: 0.701956
=== Actor Training Debug (Iteration 5286) ===
Q mean: -9.374816
Q std: 15.095275
Actor loss: 9.378785
Action reg: 0.003969
  l1.weight: grad_norm = 0.179554
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.131893
Total gradient norm: 0.395313
=== Actor Training Debug (Iteration 5287) ===
Q mean: -9.842450
Q std: 13.109694
Actor loss: 9.846406
Action reg: 0.003956
  l1.weight: grad_norm = 0.217405
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.186862
Total gradient norm: 0.546766
=== Actor Training Debug (Iteration 5288) ===
Q mean: -10.580635
Q std: 14.429215
Actor loss: 10.584611
Action reg: 0.003976
  l1.weight: grad_norm = 0.092685
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.081722
Total gradient norm: 0.250082
=== Actor Training Debug (Iteration 5289) ===
Q mean: -11.861740
Q std: 16.514542
Actor loss: 11.865711
Action reg: 0.003971
  l1.weight: grad_norm = 0.059587
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.045385
Total gradient norm: 0.131410
=== Actor Training Debug (Iteration 5290) ===
Q mean: -12.278319
Q std: 16.212433
Actor loss: 12.282281
Action reg: 0.003961
  l1.weight: grad_norm = 0.310982
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.234547
Total gradient norm: 0.723077
=== Actor Training Debug (Iteration 5291) ===
Q mean: -10.170040
Q std: 14.337943
Actor loss: 10.174007
Action reg: 0.003967
  l1.weight: grad_norm = 0.190577
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.176672
Total gradient norm: 0.489200
=== Actor Training Debug (Iteration 5292) ===
Q mean: -11.164741
Q std: 16.406002
Actor loss: 11.168690
Action reg: 0.003949
  l1.weight: grad_norm = 0.201870
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.171996
Total gradient norm: 0.471376
=== Actor Training Debug (Iteration 5293) ===
Q mean: -10.978125
Q std: 16.864372
Actor loss: 10.982098
Action reg: 0.003973
  l1.weight: grad_norm = 0.124179
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.107123
Total gradient norm: 0.286268
=== Actor Training Debug (Iteration 5294) ===
Q mean: -11.455591
Q std: 15.708142
Actor loss: 11.459542
Action reg: 0.003951
  l1.weight: grad_norm = 0.302195
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.266341
Total gradient norm: 0.744336
=== Actor Training Debug (Iteration 5295) ===
Q mean: -9.262346
Q std: 15.016244
Actor loss: 9.266310
Action reg: 0.003963
  l1.weight: grad_norm = 0.214810
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.174410
Total gradient norm: 0.485154
=== Actor Training Debug (Iteration 5296) ===
Q mean: -11.448292
Q std: 14.925133
Actor loss: 11.452266
Action reg: 0.003974
  l1.weight: grad_norm = 0.186395
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.166305
Total gradient norm: 0.463743
=== Actor Training Debug (Iteration 5297) ===
Q mean: -10.394061
Q std: 13.911498
Actor loss: 10.398027
Action reg: 0.003967
  l1.weight: grad_norm = 0.415651
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.317469
Total gradient norm: 0.877983
=== Actor Training Debug (Iteration 5298) ===
Q mean: -10.282825
Q std: 15.611709
Actor loss: 10.286798
Action reg: 0.003973
  l1.weight: grad_norm = 0.099087
  l1.bias: grad_norm = 0.000034
  l2.weight: grad_norm = 0.083733
Total gradient norm: 0.233482
=== Actor Training Debug (Iteration 5299) ===
Q mean: -8.557054
Q std: 14.150823
Actor loss: 8.561017
Action reg: 0.003963
  l1.weight: grad_norm = 0.103711
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.091807
Total gradient norm: 0.261483
=== Actor Training Debug (Iteration 5300) ===
Q mean: -10.956705
Q std: 15.868908
Actor loss: 10.960677
Action reg: 0.003972
  l1.weight: grad_norm = 0.152180
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.116508
Total gradient norm: 0.290829
=== Actor Training Debug (Iteration 5301) ===
Q mean: -11.077688
Q std: 14.908194
Actor loss: 11.081647
Action reg: 0.003959
  l1.weight: grad_norm = 0.159108
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.139436
Total gradient norm: 0.376144
=== Actor Training Debug (Iteration 5302) ===
Q mean: -11.967461
Q std: 15.353805
Actor loss: 11.971427
Action reg: 0.003966
  l1.weight: grad_norm = 0.134737
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.108982
Total gradient norm: 0.338837
=== Actor Training Debug (Iteration 5303) ===
Q mean: -11.831095
Q std: 15.171195
Actor loss: 11.835059
Action reg: 0.003964
  l1.weight: grad_norm = 0.217583
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.174137
Total gradient norm: 0.493186
=== Actor Training Debug (Iteration 5304) ===
Q mean: -11.776276
Q std: 17.036827
Actor loss: 11.780251
Action reg: 0.003975
  l1.weight: grad_norm = 0.175732
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.157518
Total gradient norm: 0.425703
=== Actor Training Debug (Iteration 5305) ===
Q mean: -11.303741
Q std: 15.323954
Actor loss: 11.307709
Action reg: 0.003967
  l1.weight: grad_norm = 0.115949
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.102525
Total gradient norm: 0.251251
=== Actor Training Debug (Iteration 5306) ===
Q mean: -11.500259
Q std: 15.178491
Actor loss: 11.504213
Action reg: 0.003954
  l1.weight: grad_norm = 0.147383
  l1.bias: grad_norm = 0.000426
  l2.weight: grad_norm = 0.125208
Total gradient norm: 0.386004
=== Actor Training Debug (Iteration 5307) ===
Q mean: -10.339682
Q std: 15.932663
Actor loss: 10.343661
Action reg: 0.003979
  l1.weight: grad_norm = 0.173996
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.143282
Total gradient norm: 0.452382
=== Actor Training Debug (Iteration 5308) ===
Q mean: -12.070471
Q std: 16.766857
Actor loss: 12.074441
Action reg: 0.003971
  l1.weight: grad_norm = 0.179190
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.152594
Total gradient norm: 0.473433
=== Actor Training Debug (Iteration 5309) ===
Q mean: -11.510156
Q std: 16.303307
Actor loss: 11.514119
Action reg: 0.003964
  l1.weight: grad_norm = 0.233288
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.221639
Total gradient norm: 0.635729
=== Actor Training Debug (Iteration 5310) ===
Q mean: -9.378163
Q std: 12.876054
Actor loss: 9.382099
Action reg: 0.003936
  l1.weight: grad_norm = 0.164339
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.151098
Total gradient norm: 0.374424
=== Actor Training Debug (Iteration 5311) ===
Q mean: -10.011191
Q std: 14.320951
Actor loss: 10.015148
Action reg: 0.003957
  l1.weight: grad_norm = 0.236473
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.216139
Total gradient norm: 0.670528
=== Actor Training Debug (Iteration 5312) ===
Q mean: -10.319921
Q std: 16.057646
Actor loss: 10.323887
Action reg: 0.003965
  l1.weight: grad_norm = 0.167553
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.131817
Total gradient norm: 0.393792
=== Actor Training Debug (Iteration 5313) ===
Q mean: -10.266999
Q std: 15.976378
Actor loss: 10.270966
Action reg: 0.003966
  l1.weight: grad_norm = 0.187393
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.155619
Total gradient norm: 0.471658
=== Actor Training Debug (Iteration 5314) ===
Q mean: -11.215768
Q std: 15.667861
Actor loss: 11.219725
Action reg: 0.003957
  l1.weight: grad_norm = 0.201528
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.165937
Total gradient norm: 0.501688
=== Actor Training Debug (Iteration 5315) ===
Q mean: -11.630756
Q std: 14.471640
Actor loss: 11.634726
Action reg: 0.003969
  l1.weight: grad_norm = 0.140802
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.128607
Total gradient norm: 0.360256
=== Actor Training Debug (Iteration 5316) ===
Q mean: -13.404289
Q std: 18.036135
Actor loss: 13.408266
Action reg: 0.003977
  l1.weight: grad_norm = 0.158779
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.139448
Total gradient norm: 0.388094
=== Actor Training Debug (Iteration 5317) ===
Q mean: -11.509285
Q std: 14.476590
Actor loss: 11.513254
Action reg: 0.003969
  l1.weight: grad_norm = 0.115145
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.086195
Total gradient norm: 0.253689
=== Actor Training Debug (Iteration 5318) ===
Q mean: -11.646727
Q std: 16.956305
Actor loss: 11.650691
Action reg: 0.003964
  l1.weight: grad_norm = 0.098245
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.090109
Total gradient norm: 0.247644
=== Actor Training Debug (Iteration 5319) ===
Q mean: -10.593585
Q std: 15.702685
Actor loss: 10.597548
Action reg: 0.003964
  l1.weight: grad_norm = 0.149862
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.117260
Total gradient norm: 0.309073
=== Actor Training Debug (Iteration 5320) ===
Q mean: -10.516846
Q std: 14.011655
Actor loss: 10.520809
Action reg: 0.003963
  l1.weight: grad_norm = 0.147949
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.137966
Total gradient norm: 0.415816
=== Actor Training Debug (Iteration 5321) ===
Q mean: -12.265621
Q std: 15.955750
Actor loss: 12.269578
Action reg: 0.003956
  l1.weight: grad_norm = 0.157024
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.125781
Total gradient norm: 0.314661
=== Actor Training Debug (Iteration 5322) ===
Q mean: -11.535612
Q std: 15.843382
Actor loss: 11.539577
Action reg: 0.003964
  l1.weight: grad_norm = 0.215458
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.178647
Total gradient norm: 0.500452
=== Actor Training Debug (Iteration 5323) ===
Q mean: -10.733762
Q std: 16.147425
Actor loss: 10.737706
Action reg: 0.003945
  l1.weight: grad_norm = 0.171702
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.164549
Total gradient norm: 0.415335
=== Actor Training Debug (Iteration 5324) ===
Q mean: -9.669086
Q std: 13.899820
Actor loss: 9.673056
Action reg: 0.003969
  l1.weight: grad_norm = 0.072756
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.063658
Total gradient norm: 0.181735
=== Actor Training Debug (Iteration 5325) ===
Q mean: -11.266792
Q std: 15.585876
Actor loss: 11.270757
Action reg: 0.003964
  l1.weight: grad_norm = 0.250270
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.193227
Total gradient norm: 0.568450
=== Actor Training Debug (Iteration 5326) ===
Q mean: -12.186962
Q std: 16.278276
Actor loss: 12.190924
Action reg: 0.003962
  l1.weight: grad_norm = 0.089652
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.080407
Total gradient norm: 0.226994
=== Actor Training Debug (Iteration 5327) ===
Q mean: -11.137137
Q std: 15.585466
Actor loss: 11.141091
Action reg: 0.003954
  l1.weight: grad_norm = 0.164331
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.145539
Total gradient norm: 0.400641
=== Actor Training Debug (Iteration 5328) ===
Q mean: -10.590548
Q std: 12.495996
Actor loss: 10.594505
Action reg: 0.003958
  l1.weight: grad_norm = 0.205582
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.186354
Total gradient norm: 0.446646
=== Actor Training Debug (Iteration 5329) ===
Q mean: -10.702333
Q std: 14.779293
Actor loss: 10.706302
Action reg: 0.003968
  l1.weight: grad_norm = 0.375758
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.264438
Total gradient norm: 0.821087
=== Actor Training Debug (Iteration 5330) ===
Q mean: -11.324053
Q std: 15.365444
Actor loss: 11.328021
Action reg: 0.003968
  l1.weight: grad_norm = 0.162858
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.142123
Total gradient norm: 0.450084
=== Actor Training Debug (Iteration 5331) ===
Q mean: -9.111526
Q std: 12.905374
Actor loss: 9.115481
Action reg: 0.003956
  l1.weight: grad_norm = 0.201972
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.162680
Total gradient norm: 0.460474
=== Actor Training Debug (Iteration 5332) ===
Q mean: -10.123203
Q std: 15.845637
Actor loss: 10.127172
Action reg: 0.003969
  l1.weight: grad_norm = 0.079673
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.066317
Total gradient norm: 0.207839
=== Actor Training Debug (Iteration 5333) ===
Q mean: -12.014927
Q std: 16.233461
Actor loss: 12.018879
Action reg: 0.003952
  l1.weight: grad_norm = 0.127420
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.114264
Total gradient norm: 0.289880
=== Actor Training Debug (Iteration 5334) ===
Q mean: -11.501386
Q std: 15.019535
Actor loss: 11.505336
Action reg: 0.003950
  l1.weight: grad_norm = 0.129548
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.105571
Total gradient norm: 0.325971
=== Actor Training Debug (Iteration 5335) ===
Q mean: -10.724339
Q std: 16.078482
Actor loss: 10.728301
Action reg: 0.003963
  l1.weight: grad_norm = 0.235743
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.194371
Total gradient norm: 0.461741
=== Actor Training Debug (Iteration 5336) ===
Q mean: -12.057161
Q std: 15.468795
Actor loss: 12.061143
Action reg: 0.003981
  l1.weight: grad_norm = 0.163161
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.133194
Total gradient norm: 0.466611
=== Actor Training Debug (Iteration 5337) ===
Q mean: -10.799391
Q std: 14.930060
Actor loss: 10.803354
Action reg: 0.003964
  l1.weight: grad_norm = 0.128677
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.116508
Total gradient norm: 0.314126
=== Actor Training Debug (Iteration 5338) ===
Q mean: -11.767859
Q std: 14.866430
Actor loss: 11.771830
Action reg: 0.003971
  l1.weight: grad_norm = 0.172154
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.145513
Total gradient norm: 0.398092
=== Actor Training Debug (Iteration 5339) ===
Q mean: -10.653126
Q std: 14.365136
Actor loss: 10.657084
Action reg: 0.003958
  l1.weight: grad_norm = 0.164244
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.152157
Total gradient norm: 0.431934
=== Actor Training Debug (Iteration 5340) ===
Q mean: -10.869670
Q std: 14.822824
Actor loss: 10.873629
Action reg: 0.003959
  l1.weight: grad_norm = 0.287216
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.222260
Total gradient norm: 0.725443
=== Actor Training Debug (Iteration 5341) ===
Q mean: -10.821761
Q std: 16.183483
Actor loss: 10.825718
Action reg: 0.003957
  l1.weight: grad_norm = 0.178765
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.165892
Total gradient norm: 0.430798
=== Actor Training Debug (Iteration 5342) ===
Q mean: -9.246010
Q std: 15.097082
Actor loss: 9.249967
Action reg: 0.003957
  l1.weight: grad_norm = 0.131743
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.117412
Total gradient norm: 0.322431
=== Actor Training Debug (Iteration 5343) ===
Q mean: -11.746666
Q std: 15.673409
Actor loss: 11.750621
Action reg: 0.003955
  l1.weight: grad_norm = 0.191250
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.147991
Total gradient norm: 0.460069
=== Actor Training Debug (Iteration 5344) ===
Q mean: -11.083122
Q std: 16.020340
Actor loss: 11.087077
Action reg: 0.003954
  l1.weight: grad_norm = 0.213959
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.183566
Total gradient norm: 0.519661
=== Actor Training Debug (Iteration 5345) ===
Q mean: -11.557371
Q std: 14.517289
Actor loss: 11.561333
Action reg: 0.003962
  l1.weight: grad_norm = 0.155173
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.129676
Total gradient norm: 0.374706
=== Actor Training Debug (Iteration 5346) ===
Q mean: -11.984928
Q std: 15.422424
Actor loss: 11.988881
Action reg: 0.003953
  l1.weight: grad_norm = 0.073832
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.065741
Total gradient norm: 0.187197
=== Actor Training Debug (Iteration 5347) ===
Q mean: -10.411346
Q std: 16.096401
Actor loss: 10.415297
Action reg: 0.003950
  l1.weight: grad_norm = 0.232058
  l1.bias: grad_norm = 0.000307
  l2.weight: grad_norm = 0.202421
Total gradient norm: 0.543063
=== Actor Training Debug (Iteration 5348) ===
Q mean: -11.089520
Q std: 16.073622
Actor loss: 11.093486
Action reg: 0.003965
  l1.weight: grad_norm = 0.174490
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.140765
Total gradient norm: 0.401127
=== Actor Training Debug (Iteration 5349) ===
Q mean: -11.482569
Q std: 16.083639
Actor loss: 11.486519
Action reg: 0.003950
  l1.weight: grad_norm = 0.180331
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.155336
Total gradient norm: 0.427562
=== Actor Training Debug (Iteration 5350) ===
Q mean: -9.816454
Q std: 16.010475
Actor loss: 9.820416
Action reg: 0.003963
  l1.weight: grad_norm = 0.132214
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.121446
Total gradient norm: 0.318076
=== Actor Training Debug (Iteration 5351) ===
Q mean: -11.579203
Q std: 16.474606
Actor loss: 11.583162
Action reg: 0.003960
  l1.weight: grad_norm = 0.118561
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.094911
Total gradient norm: 0.297352
=== Actor Training Debug (Iteration 5352) ===
Q mean: -10.748890
Q std: 14.360933
Actor loss: 10.752847
Action reg: 0.003957
  l1.weight: grad_norm = 0.098072
  l1.bias: grad_norm = 0.000476
  l2.weight: grad_norm = 0.089958
Total gradient norm: 0.259476
=== Actor Training Debug (Iteration 5353) ===
Q mean: -8.084651
Q std: 12.689698
Actor loss: 8.088629
Action reg: 0.003977
  l1.weight: grad_norm = 0.060125
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.050920
Total gradient norm: 0.142155
=== Actor Training Debug (Iteration 5354) ===
Q mean: -10.573111
Q std: 16.505497
Actor loss: 10.577070
Action reg: 0.003960
  l1.weight: grad_norm = 0.154613
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.141608
Total gradient norm: 0.438488
=== Actor Training Debug (Iteration 5355) ===
Q mean: -10.597633
Q std: 15.554674
Actor loss: 10.601598
Action reg: 0.003964
  l1.weight: grad_norm = 0.217015
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.182451
Total gradient norm: 0.559917
=== Actor Training Debug (Iteration 5356) ===
Q mean: -10.151118
Q std: 15.551199
Actor loss: 10.155072
Action reg: 0.003954
  l1.weight: grad_norm = 0.174128
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.150162
Total gradient norm: 0.436023
=== Actor Training Debug (Iteration 5357) ===
Q mean: -10.592374
Q std: 14.670064
Actor loss: 10.596336
Action reg: 0.003962
  l1.weight: grad_norm = 0.121959
  l1.bias: grad_norm = 0.000378
  l2.weight: grad_norm = 0.105754
Total gradient norm: 0.316849
=== Actor Training Debug (Iteration 5358) ===
Q mean: -12.181228
Q std: 15.029023
Actor loss: 12.185201
Action reg: 0.003973
  l1.weight: grad_norm = 0.145787
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.128032
Total gradient norm: 0.326650
=== Actor Training Debug (Iteration 5359) ===
Q mean: -10.488859
Q std: 15.139906
Actor loss: 10.492831
Action reg: 0.003972
  l1.weight: grad_norm = 0.216825
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.178903
Total gradient norm: 0.563818
=== Actor Training Debug (Iteration 5360) ===
Q mean: -9.702887
Q std: 14.940990
Actor loss: 9.706849
Action reg: 0.003963
  l1.weight: grad_norm = 0.183105
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.156799
Total gradient norm: 0.475136
=== Actor Training Debug (Iteration 5361) ===
Q mean: -10.926667
Q std: 14.376260
Actor loss: 10.930647
Action reg: 0.003979
  l1.weight: grad_norm = 0.080081
  l1.bias: grad_norm = 0.000038
  l2.weight: grad_norm = 0.066632
Total gradient norm: 0.172132
=== Actor Training Debug (Iteration 5362) ===
Q mean: -8.943792
Q std: 13.560638
Actor loss: 8.947760
Action reg: 0.003967
  l1.weight: grad_norm = 0.191272
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.153371
Total gradient norm: 0.428177
=== Actor Training Debug (Iteration 5363) ===
Q mean: -10.873891
Q std: 17.344915
Actor loss: 10.877842
Action reg: 0.003952
  l1.weight: grad_norm = 0.126503
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.116026
Total gradient norm: 0.360341
=== Actor Training Debug (Iteration 5364) ===
Q mean: -11.613124
Q std: 15.215462
Actor loss: 11.617078
Action reg: 0.003954
  l1.weight: grad_norm = 0.238056
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.190410
Total gradient norm: 0.564999
=== Actor Training Debug (Iteration 5365) ===
Q mean: -11.312317
Q std: 16.535431
Actor loss: 11.316281
Action reg: 0.003965
  l1.weight: grad_norm = 0.106657
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.100760
Total gradient norm: 0.291089
=== Actor Training Debug (Iteration 5366) ===
Q mean: -9.597143
Q std: 14.621235
Actor loss: 9.601113
Action reg: 0.003971
  l1.weight: grad_norm = 0.163677
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.136195
Total gradient norm: 0.377821
=== Actor Training Debug (Iteration 5367) ===
Q mean: -10.074448
Q std: 14.390724
Actor loss: 10.078421
Action reg: 0.003973
  l1.weight: grad_norm = 0.080570
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.079912
Total gradient norm: 0.258605
=== Actor Training Debug (Iteration 5368) ===
Q mean: -11.276125
Q std: 15.105086
Actor loss: 11.280084
Action reg: 0.003959
  l1.weight: grad_norm = 0.186644
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.169256
Total gradient norm: 0.450054
=== Actor Training Debug (Iteration 5369) ===
Q mean: -10.193460
Q std: 16.146967
Actor loss: 10.197422
Action reg: 0.003961
  l1.weight: grad_norm = 0.103745
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.088933
Total gradient norm: 0.252567
=== Actor Training Debug (Iteration 5370) ===
Q mean: -10.491591
Q std: 13.988038
Actor loss: 10.495536
Action reg: 0.003945
  l1.weight: grad_norm = 0.132900
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.111219
Total gradient norm: 0.304981
=== Actor Training Debug (Iteration 5371) ===
Q mean: -12.728645
Q std: 15.964041
Actor loss: 12.732627
Action reg: 0.003981
  l1.weight: grad_norm = 0.138516
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.113059
Total gradient norm: 0.282114
=== Actor Training Debug (Iteration 5372) ===
Q mean: -10.804120
Q std: 14.487522
Actor loss: 10.808092
Action reg: 0.003972
  l1.weight: grad_norm = 0.063976
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.053756
Total gradient norm: 0.147187
=== Actor Training Debug (Iteration 5373) ===
Q mean: -12.013693
Q std: 15.129561
Actor loss: 12.017654
Action reg: 0.003961
  l1.weight: grad_norm = 0.164250
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.147419
Total gradient norm: 0.384712
=== Actor Training Debug (Iteration 5374) ===
Q mean: -11.657858
Q std: 15.644655
Actor loss: 11.661818
Action reg: 0.003960
  l1.weight: grad_norm = 0.186552
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.159730
Total gradient norm: 0.447198
=== Actor Training Debug (Iteration 5375) ===
Q mean: -10.351544
Q std: 14.368683
Actor loss: 10.355517
Action reg: 0.003973
  l1.weight: grad_norm = 0.153837
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.141396
Total gradient norm: 0.489176
=== Actor Training Debug (Iteration 5376) ===
Q mean: -9.017283
Q std: 13.506183
Actor loss: 9.021258
Action reg: 0.003975
  l1.weight: grad_norm = 0.146569
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.112577
Total gradient norm: 0.269578
=== Actor Training Debug (Iteration 5377) ===
Q mean: -10.112393
Q std: 15.512147
Actor loss: 10.116360
Action reg: 0.003967
  l1.weight: grad_norm = 0.163859
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.152993
Total gradient norm: 0.365867
=== Actor Training Debug (Iteration 5378) ===
Q mean: -9.439096
Q std: 15.158603
Actor loss: 9.443040
Action reg: 0.003943
  l1.weight: grad_norm = 0.231669
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.204287
Total gradient norm: 0.642463
=== Actor Training Debug (Iteration 5379) ===
Q mean: -10.210234
Q std: 15.294742
Actor loss: 10.214209
Action reg: 0.003975
  l1.weight: grad_norm = 0.153980
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.134921
Total gradient norm: 0.337385
=== Actor Training Debug (Iteration 5380) ===
Q mean: -11.608208
Q std: 14.483688
Actor loss: 11.612180
Action reg: 0.003972
  l1.weight: grad_norm = 0.215483
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.194335
Total gradient norm: 0.607224
=== Actor Training Debug (Iteration 5381) ===
Q mean: -11.348161
Q std: 16.731504
Actor loss: 11.352125
Action reg: 0.003965
  l1.weight: grad_norm = 0.196058
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.163654
Total gradient norm: 0.449892
=== Actor Training Debug (Iteration 5382) ===
Q mean: -10.953820
Q std: 15.082065
Actor loss: 10.957792
Action reg: 0.003972
  l1.weight: grad_norm = 0.108957
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.118394
Total gradient norm: 0.328443
=== Actor Training Debug (Iteration 5383) ===
Q mean: -10.339716
Q std: 15.764936
Actor loss: 10.343660
Action reg: 0.003945
  l1.weight: grad_norm = 0.213513
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.203429
Total gradient norm: 0.592905
=== Actor Training Debug (Iteration 5384) ===
Q mean: -10.338749
Q std: 14.738130
Actor loss: 10.342726
Action reg: 0.003976
  l1.weight: grad_norm = 0.095020
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.083399
Total gradient norm: 0.252727
=== Actor Training Debug (Iteration 5385) ===
Q mean: -10.557799
Q std: 15.188142
Actor loss: 10.561769
Action reg: 0.003969
  l1.weight: grad_norm = 0.075960
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.067759
Total gradient norm: 0.208386
=== Actor Training Debug (Iteration 5386) ===
Q mean: -10.392077
Q std: 14.445464
Actor loss: 10.396040
Action reg: 0.003962
  l1.weight: grad_norm = 0.120198
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.101842
Total gradient norm: 0.300892
=== Actor Training Debug (Iteration 5387) ===
Q mean: -11.398228
Q std: 15.164227
Actor loss: 11.402197
Action reg: 0.003970
  l1.weight: grad_norm = 0.115304
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.097221
Total gradient norm: 0.265796
=== Actor Training Debug (Iteration 5388) ===
Q mean: -11.611551
Q std: 16.843496
Actor loss: 11.615512
Action reg: 0.003960
  l1.weight: grad_norm = 0.300953
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.231047
Total gradient norm: 0.640790
=== Actor Training Debug (Iteration 5389) ===
Q mean: -11.919942
Q std: 16.054689
Actor loss: 11.923904
Action reg: 0.003962
  l1.weight: grad_norm = 0.116457
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.100273
Total gradient norm: 0.281942
=== Actor Training Debug (Iteration 5390) ===
Q mean: -10.984743
Q std: 14.436539
Actor loss: 10.988713
Action reg: 0.003970
  l1.weight: grad_norm = 0.119497
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.107259
Total gradient norm: 0.350741
=== Actor Training Debug (Iteration 5391) ===
Q mean: -9.949562
Q std: 14.965786
Actor loss: 9.953522
Action reg: 0.003959
  l1.weight: grad_norm = 0.253103
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.205458
Total gradient norm: 0.530127
=== Actor Training Debug (Iteration 5392) ===
Q mean: -9.689873
Q std: 13.892501
Actor loss: 9.693830
Action reg: 0.003956
  l1.weight: grad_norm = 0.263834
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.232028
Total gradient norm: 0.689274
=== Actor Training Debug (Iteration 5393) ===
Q mean: -9.355474
Q std: 13.624994
Actor loss: 9.359426
Action reg: 0.003952
  l1.weight: grad_norm = 0.176556
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.139054
Total gradient norm: 0.411051
=== Actor Training Debug (Iteration 5394) ===
Q mean: -10.720727
Q std: 15.066710
Actor loss: 10.724694
Action reg: 0.003967
  l1.weight: grad_norm = 0.207569
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.163415
Total gradient norm: 0.485234
=== Actor Training Debug (Iteration 5395) ===
Q mean: -10.716892
Q std: 13.487118
Actor loss: 10.720855
Action reg: 0.003963
  l1.weight: grad_norm = 0.319896
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.260191
Total gradient norm: 0.747860
=== Actor Training Debug (Iteration 5396) ===
Q mean: -10.967792
Q std: 16.544868
Actor loss: 10.971758
Action reg: 0.003966
  l1.weight: grad_norm = 0.081396
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.070887
Total gradient norm: 0.196798
  l1.weight: grad_norm = 0.146456
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.119900
Total gradient norm: 0.353303
=== Actor Training Debug (Iteration 5415) ===
Q mean: -9.172364
Q std: 14.726582
Actor loss: 9.176327
Action reg: 0.003962
  l1.weight: grad_norm = 0.191627
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.156653
Total gradient norm: 0.477344
=== Actor Training Debug (Iteration 5416) ===
Q mean: -10.848238
Q std: 15.466106
Actor loss: 10.852196
Action reg: 0.003958
  l1.weight: grad_norm = 0.108766
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.097840
Total gradient norm: 0.302049
=== Actor Training Debug (Iteration 5417) ===
Q mean: -10.104834
Q std: 15.398602
Actor loss: 10.108794
Action reg: 0.003960
  l1.weight: grad_norm = 0.161209
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.133914
Total gradient norm: 0.420857
=== Actor Training Debug (Iteration 5418) ===
Q mean: -12.495342
Q std: 16.527632
Actor loss: 12.499325
Action reg: 0.003982
  l1.weight: grad_norm = 0.143874
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.129895
Total gradient norm: 0.412928
=== Actor Training Debug (Iteration 5419) ===
Q mean: -11.018335
Q std: 15.330460
Actor loss: 11.022305
Action reg: 0.003969
  l1.weight: grad_norm = 0.143677
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.116019
Total gradient norm: 0.369657
=== Actor Training Debug (Iteration 5420) ===
Q mean: -11.389009
Q std: 14.935570
Actor loss: 11.392963
Action reg: 0.003955
  l1.weight: grad_norm = 0.285265
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.248234
Total gradient norm: 0.821328
=== Actor Training Debug (Iteration 5421) ===
Q mean: -10.578568
Q std: 15.369937
Actor loss: 10.582541
Action reg: 0.003974
  l1.weight: grad_norm = 0.163929
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.157686
Total gradient norm: 0.437008
=== Actor Training Debug (Iteration 5422) ===
Q mean: -12.392354
Q std: 17.002293
Actor loss: 12.396333
Action reg: 0.003979
  l1.weight: grad_norm = 0.218040
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.186468
Total gradient norm: 0.535946
=== Actor Training Debug (Iteration 5423) ===
Q mean: -10.683521
Q std: 16.080975
Actor loss: 10.687490
Action reg: 0.003970
  l1.weight: grad_norm = 0.141917
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.127745
Total gradient norm: 0.436291
=== Actor Training Debug (Iteration 5424) ===
Q mean: -11.623641
Q std: 16.966951
Actor loss: 11.627587
Action reg: 0.003946
  l1.weight: grad_norm = 0.201746
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.176752
Total gradient norm: 0.451916
=== Actor Training Debug (Iteration 5425) ===
Q mean: -10.858093
Q std: 15.279252
Actor loss: 10.862057
Action reg: 0.003964
  l1.weight: grad_norm = 0.151641
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.121697
Total gradient norm: 0.419272
=== Actor Training Debug (Iteration 5426) ===
Q mean: -11.841497
Q std: 14.890649
Actor loss: 11.845456
Action reg: 0.003959
  l1.weight: grad_norm = 0.185556
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.160600
Total gradient norm: 0.459875
=== Actor Training Debug (Iteration 5427) ===
Q mean: -10.875706
Q std: 15.692276
Actor loss: 10.879677
Action reg: 0.003971
  l1.weight: grad_norm = 0.138050
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.110552
Total gradient norm: 0.302019
=== Actor Training Debug (Iteration 5428) ===
Q mean: -11.599847
Q std: 16.209402
Actor loss: 11.603809
Action reg: 0.003962
  l1.weight: grad_norm = 0.264617
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.226934
Total gradient norm: 0.577135
=== Actor Training Debug (Iteration 5429) ===
Q mean: -9.814887
Q std: 15.099519
Actor loss: 9.818851
Action reg: 0.003964
  l1.weight: grad_norm = 0.513934
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.356999
Total gradient norm: 1.090675
=== Actor Training Debug (Iteration 5430) ===
Q mean: -14.277814
Q std: 17.363705
Actor loss: 14.281783
Action reg: 0.003969
  l1.weight: grad_norm = 0.214163
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.183036
Total gradient norm: 0.534657
=== Actor Training Debug (Iteration 5431) ===
Q mean: -10.442791
Q std: 16.528009
Actor loss: 10.446748
Action reg: 0.003957
  l1.weight: grad_norm = 0.282188
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.220836
Total gradient norm: 0.618225
=== Actor Training Debug (Iteration 5432) ===
Q mean: -10.654867
Q std: 14.007381
Actor loss: 10.658843
Action reg: 0.003976
  l1.weight: grad_norm = 0.201524
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.185297
Total gradient norm: 0.507670
=== Actor Training Debug (Iteration 5433) ===
Q mean: -10.639809
Q std: 15.485680
Actor loss: 10.643764
Action reg: 0.003955
  l1.weight: grad_norm = 0.155932
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.147494
Total gradient norm: 0.501848
=== Actor Training Debug (Iteration 5434) ===
Q mean: -10.953817
Q std: 14.894853
Actor loss: 10.957803
Action reg: 0.003985
  l1.weight: grad_norm = 0.085415
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.068861
Total gradient norm: 0.194172
=== Actor Training Debug (Iteration 5435) ===
Q mean: -10.976374
Q std: 16.197924
Actor loss: 10.980349
Action reg: 0.003975
  l1.weight: grad_norm = 0.133225
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.122979
Total gradient norm: 0.341460
=== Actor Training Debug (Iteration 5436) ===
Q mean: -10.568125
Q std: 14.819632
Actor loss: 10.572095
Action reg: 0.003970
  l1.weight: grad_norm = 0.105853
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.096739
Total gradient norm: 0.246054
=== Actor Training Debug (Iteration 5437) ===
Q mean: -10.319683
Q std: 14.581958
Actor loss: 10.323645
Action reg: 0.003962
  l1.weight: grad_norm = 0.195453
  l1.bias: grad_norm = 0.000427
  l2.weight: grad_norm = 0.180150
Total gradient norm: 0.582848
=== Actor Training Debug (Iteration 5438) ===
Q mean: -10.657800
Q std: 15.128454
Actor loss: 10.661765
Action reg: 0.003966
  l1.weight: grad_norm = 0.131874
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.117407
Total gradient norm: 0.347317
=== Actor Training Debug (Iteration 5439) ===
Q mean: -12.582078
Q std: 17.016449
Actor loss: 12.586044
Action reg: 0.003966
  l1.weight: grad_norm = 0.281350
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.251846
Total gradient norm: 0.842297
=== Actor Training Debug (Iteration 5440) ===
Q mean: -12.785536
Q std: 16.286892
Actor loss: 12.789473
Action reg: 0.003937
  l1.weight: grad_norm = 0.167688
  l1.bias: grad_norm = 0.000747
  l2.weight: grad_norm = 0.142772
Total gradient norm: 0.394103
=== Actor Training Debug (Iteration 5441) ===
Q mean: -11.570484
Q std: 16.197866
Actor loss: 11.574456
Action reg: 0.003972
  l1.weight: grad_norm = 0.066780
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.064627
Total gradient norm: 0.205141
=== Actor Training Debug (Iteration 5442) ===
Q mean: -10.502359
Q std: 15.531657
Actor loss: 10.506339
Action reg: 0.003979
  l1.weight: grad_norm = 0.163348
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.147614
Total gradient norm: 0.415905
=== Actor Training Debug (Iteration 5443) ===
Q mean: -9.194772
Q std: 13.461893
Actor loss: 9.198750
Action reg: 0.003979
  l1.weight: grad_norm = 0.053568
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.051238
Total gradient norm: 0.153912
=== Actor Training Debug (Iteration 5444) ===
Q mean: -10.126150
Q std: 15.241505
Actor loss: 10.130115
Action reg: 0.003964
  l1.weight: grad_norm = 0.164640
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.153973
Total gradient norm: 0.404335
=== Actor Training Debug (Iteration 5445) ===
Q mean: -11.322845
Q std: 16.720890
Actor loss: 11.326826
Action reg: 0.003982
  l1.weight: grad_norm = 0.148415
  l1.bias: grad_norm = 0.000051
  l2.weight: grad_norm = 0.122975
Total gradient norm: 0.355367
=== Actor Training Debug (Iteration 5446) ===
Q mean: -9.001175
Q std: 13.686753
Actor loss: 9.005119
Action reg: 0.003945
  l1.weight: grad_norm = 0.175026
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.153703
Total gradient norm: 0.420658
=== Actor Training Debug (Iteration 5447) ===
Q mean: -10.398871
Q std: 14.575642
Actor loss: 10.402833
Action reg: 0.003961
  l1.weight: grad_norm = 0.115723
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.101342
Total gradient norm: 0.294149
=== Actor Training Debug (Iteration 5448) ===
Q mean: -9.322928
Q std: 13.076108
Actor loss: 9.326870
Action reg: 0.003941
  l1.weight: grad_norm = 0.202728
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.188396
Total gradient norm: 0.565496
=== Actor Training Debug (Iteration 5449) ===
Q mean: -11.472033
Q std: 16.573088
Actor loss: 11.475989
Action reg: 0.003957
  l1.weight: grad_norm = 0.169634
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.142345
Total gradient norm: 0.402055
=== Actor Training Debug (Iteration 5450) ===
Q mean: -11.025859
Q std: 15.218712
Actor loss: 11.029837
Action reg: 0.003977
  l1.weight: grad_norm = 0.141101
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.114499
Total gradient norm: 0.324127
=== Actor Training Debug (Iteration 5451) ===
Q mean: -10.714943
Q std: 15.395138
Actor loss: 10.718900
Action reg: 0.003956
  l1.weight: grad_norm = 0.114846
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.100441
Total gradient norm: 0.313785
=== Actor Training Debug (Iteration 5452) ===
Q mean: -10.354719
Q std: 16.154060
Actor loss: 10.358679
Action reg: 0.003959
  l1.weight: grad_norm = 0.149275
  l1.bias: grad_norm = 0.000471
  l2.weight: grad_norm = 0.131196
Total gradient norm: 0.375632
=== Actor Training Debug (Iteration 5453) ===
Q mean: -11.340343
Q std: 15.352654
Actor loss: 11.344296
Action reg: 0.003953
  l1.weight: grad_norm = 0.208233
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.171484
Total gradient norm: 0.432173
=== Actor Training Debug (Iteration 5454) ===
Q mean: -12.545179
Q std: 15.599737
Actor loss: 12.549149
Action reg: 0.003969
  l1.weight: grad_norm = 0.096052
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.084472
Total gradient norm: 0.250966
=== Actor Training Debug (Iteration 5455) ===
Q mean: -12.231760
Q std: 15.156485
Actor loss: 12.235729
Action reg: 0.003970
  l1.weight: grad_norm = 0.124159
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.117055
Total gradient norm: 0.327425
=== Actor Training Debug (Iteration 5456) ===
Q mean: -10.918434
Q std: 15.099739
Actor loss: 10.922387
Action reg: 0.003953
  l1.weight: grad_norm = 0.273381
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.211528
Total gradient norm: 0.583877
=== Actor Training Debug (Iteration 5457) ===
Q mean: -10.744096
Q std: 15.498512
Actor loss: 10.748061
Action reg: 0.003965
  l1.weight: grad_norm = 0.127702
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.103918
Total gradient norm: 0.286275
=== Actor Training Debug (Iteration 5458) ===
Q mean: -12.154345
Q std: 15.612245
Actor loss: 12.158305
Action reg: 0.003961
  l1.weight: grad_norm = 0.250724
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.193800
Total gradient norm: 0.500596
=== Actor Training Debug (Iteration 5459) ===
Q mean: -9.033045
Q std: 13.702236
Actor loss: 9.037004
Action reg: 0.003959
  l1.weight: grad_norm = 0.142746
  l1.bias: grad_norm = 0.000514
  l2.weight: grad_norm = 0.122971
Total gradient norm: 0.335641
=== Actor Training Debug (Iteration 5460) ===
Q mean: -11.531644
Q std: 15.886237
Actor loss: 11.535611
Action reg: 0.003967
  l1.weight: grad_norm = 0.136644
  l1.bias: grad_norm = 0.000441
  l2.weight: grad_norm = 0.131436
Total gradient norm: 0.418423
=== Actor Training Debug (Iteration 5461) ===
Q mean: -9.452219
Q std: 14.134833
Actor loss: 9.456198
Action reg: 0.003979
  l1.weight: grad_norm = 0.105119
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 0.083539
Total gradient norm: 0.224397
=== Actor Training Debug (Iteration 5462) ===
Q mean: -10.969687
Q std: 16.236725
Actor loss: 10.973660
Action reg: 0.003974
  l1.weight: grad_norm = 0.111278
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.090604
Total gradient norm: 0.247602
=== Actor Training Debug (Iteration 5463) ===
Q mean: -10.412519
Q std: 14.473653
Actor loss: 10.416486
Action reg: 0.003968
  l1.weight: grad_norm = 0.148153
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.129833
Total gradient norm: 0.342513
=== Actor Training Debug (Iteration 5464) ===
Q mean: -11.315681
Q std: 16.293245
Actor loss: 11.319645
Action reg: 0.003964
  l1.weight: grad_norm = 0.142481
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.123186
Total gradient norm: 0.378110
=== Actor Training Debug (Iteration 5465) ===
Q mean: -10.787002
Q std: 15.556861
Actor loss: 10.790979
Action reg: 0.003978
  l1.weight: grad_norm = 0.096641
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.084268
Total gradient norm: 0.251652
=== Actor Training Debug (Iteration 5466) ===
Q mean: -10.573645
Q std: 14.535107
Actor loss: 10.577603
Action reg: 0.003959
  l1.weight: grad_norm = 0.265474
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.219526
Total gradient norm: 0.570968
=== Actor Training Debug (Iteration 5467) ===
Q mean: -11.968790
Q std: 16.577450
Actor loss: 11.972747
Action reg: 0.003957
  l1.weight: grad_norm = 0.103666
  l1.bias: grad_norm = 0.000661
  l2.weight: grad_norm = 0.085979
Total gradient norm: 0.252746
=== Actor Training Debug (Iteration 5468) ===
Q mean: -11.712442
Q std: 15.723309
Actor loss: 11.716400
Action reg: 0.003958
  l1.weight: grad_norm = 0.243141
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.216993
Total gradient norm: 0.604565
=== Actor Training Debug (Iteration 5469) ===
Q mean: -10.111591
Q std: 15.065568
Actor loss: 10.115552
Action reg: 0.003960
  l1.weight: grad_norm = 0.144015
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.139461
Total gradient norm: 0.353381
=== Actor Training Debug (Iteration 5470) ===
Q mean: -11.064517
Q std: 14.428069
Actor loss: 11.068483
Action reg: 0.003966
  l1.weight: grad_norm = 0.208248
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.172342
Total gradient norm: 0.402454
=== Actor Training Debug (Iteration 5471) ===
Q mean: -9.192225
Q std: 14.638905
Actor loss: 9.196189
Action reg: 0.003964
  l1.weight: grad_norm = 0.153267
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.147228
Total gradient norm: 0.385909
=== Actor Training Debug (Iteration 5472) ===
Q mean: -9.876949
Q std: 15.092657
Actor loss: 9.880916
Action reg: 0.003967
  l1.weight: grad_norm = 0.389502
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.344524
Total gradient norm: 0.662008
=== Actor Training Debug (Iteration 5473) ===
Q mean: -10.273607
Q std: 15.550682
Actor loss: 10.277570
Action reg: 0.003962
  l1.weight: grad_norm = 0.091799
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.087291
Total gradient norm: 0.225131
=== Actor Training Debug (Iteration 5474) ===
Q mean: -11.524530
Q std: 16.332697
Actor loss: 11.528492
Action reg: 0.003962
  l1.weight: grad_norm = 0.230404
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.202682
Total gradient norm: 0.618397
=== Actor Training Debug (Iteration 5475) ===
Q mean: -11.173371
Q std: 15.279069
Actor loss: 11.177328
Action reg: 0.003956
  l1.weight: grad_norm = 0.418193
  l1.bias: grad_norm = 0.000600
  l2.weight: grad_norm = 0.319549
Total gradient norm: 0.819429
=== Actor Training Debug (Iteration 5476) ===
Q mean: -11.589540
Q std: 16.458580
Actor loss: 11.593504
Action reg: 0.003963
  l1.weight: grad_norm = 0.351278
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.276782
Total gradient norm: 0.763933
=== Actor Training Debug (Iteration 5477) ===
Q mean: -11.879352
Q std: 15.358151
Actor loss: 11.883318
Action reg: 0.003966
  l1.weight: grad_norm = 0.112513
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.098630
Total gradient norm: 0.285403
=== Actor Training Debug (Iteration 5478) ===
Q mean: -10.918775
Q std: 16.222879
Actor loss: 10.922734
Action reg: 0.003960
  l1.weight: grad_norm = 0.179455
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.145207
Total gradient norm: 0.454240
=== Actor Training Debug (Iteration 5479) ===
Q mean: -10.916681
Q std: 16.916334
Actor loss: 10.920655
Action reg: 0.003974
  l1.weight: grad_norm = 0.288375
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.257891
Total gradient norm: 0.907222
=== Actor Training Debug (Iteration 5480) ===
Q mean: -13.724836
Q std: 19.077154
Actor loss: 13.728797
Action reg: 0.003960
  l1.weight: grad_norm = 0.107636
  l1.bias: grad_norm = 0.000690
  l2.weight: grad_norm = 0.094269
Total gradient norm: 0.263205
=== Actor Training Debug (Iteration 5481) ===
Q mean: -9.801721
Q std: 13.562221
Actor loss: 9.805679
Action reg: 0.003959
  l1.weight: grad_norm = 0.128409
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.111302
Total gradient norm: 0.280951
=== Actor Training Debug (Iteration 5482) ===
Q mean: -11.662750
Q std: 14.980417
Actor loss: 11.666711
Action reg: 0.003961
  l1.weight: grad_norm = 0.169382
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.136498
Total gradient norm: 0.387108
=== Actor Training Debug (Iteration 5483) ===
Q mean: -12.122281
Q std: 14.893806
Actor loss: 12.126255
Action reg: 0.003974
  l1.weight: grad_norm = 0.070518
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.063362
Total gradient norm: 0.177588
=== Actor Training Debug (Iteration 5484) ===
Q mean: -10.213264
Q std: 14.145892
Actor loss: 10.217216
Action reg: 0.003952
  l1.weight: grad_norm = 0.117434
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.103686
Total gradient norm: 0.354047
=== Actor Training Debug (Iteration 5485) ===
Q mean: -11.707762
Q std: 15.785287
Actor loss: 11.711735
Action reg: 0.003973
  l1.weight: grad_norm = 0.157702
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.127219
Total gradient norm: 0.348422
=== Actor Training Debug (Iteration 5486) ===
Q mean: -10.915286
Q std: 16.222666
Actor loss: 10.919250
Action reg: 0.003964
  l1.weight: grad_norm = 0.184939
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.153418
Total gradient norm: 0.506443
=== Actor Training Debug (Iteration 5487) ===
Q mean: -11.865126
Q std: 15.959593
Actor loss: 11.869078
Action reg: 0.003952
  l1.weight: grad_norm = 0.201846
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.192455
Total gradient norm: 0.547400
=== Actor Training Debug (Iteration 5488) ===
Q mean: -10.794729
Q std: 14.365295
Actor loss: 10.798702
Action reg: 0.003973
  l1.weight: grad_norm = 0.249875
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.219659
Total gradient norm: 0.737193
=== Actor Training Debug (Iteration 5489) ===
Q mean: -9.282408
Q std: 14.371097
Actor loss: 9.286363
Action reg: 0.003955
  l1.weight: grad_norm = 0.214320
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.165538
Total gradient norm: 0.436103
=== Actor Training Debug (Iteration 5490) ===
Q mean: -12.754223
Q std: 17.384464
Actor loss: 12.758194
Action reg: 0.003971
  l1.weight: grad_norm = 0.175999
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.153987
Total gradient norm: 0.493664
=== Actor Training Debug (Iteration 5491) ===
Q mean: -11.364031
Q std: 16.884859
Actor loss: 11.368006
Action reg: 0.003975
  l1.weight: grad_norm = 0.151819
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.123326
Total gradient norm: 0.321618
=== Actor Training Debug (Iteration 5492) ===
Q mean: -11.564796
Q std: 16.476616
Actor loss: 11.568763
Action reg: 0.003966
  l1.weight: grad_norm = 0.253275
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.215904
Total gradient norm: 0.579742
=== Actor Training Debug (Iteration 5493) ===
Q mean: -11.669897
Q std: 17.548922
Actor loss: 11.673862
Action reg: 0.003965
  l1.weight: grad_norm = 0.258776
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.199356
Total gradient norm: 0.583907
=== Actor Training Debug (Iteration 5494) ===
Q mean: -11.293898
Q std: 15.807386
Actor loss: 11.297836
Action reg: 0.003938
  l1.weight: grad_norm = 0.247579
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.216670
Total gradient norm: 0.653253
=== Actor Training Debug (Iteration 5495) ===
Q mean: -10.629045
Q std: 15.307864
Actor loss: 10.633021
Action reg: 0.003976
  l1.weight: grad_norm = 0.131658
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.112650
Total gradient norm: 0.369259
=== Actor Training Debug (Iteration 5496) ===
Q mean: -11.545386
Q std: 17.275223
Actor loss: 11.549346
Action reg: 0.003960
  l1.weight: grad_norm = 0.180424
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.162599
Total gradient norm: 0.498937
=== Actor Training Debug (Iteration 5497) ===
Q mean: -9.600362
Q std: 16.288809
Actor loss: 9.604327
Action reg: 0.003965
  l1.weight: grad_norm = 0.188178
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.176076
Total gradient norm: 0.474864
=== Actor Training Debug (Iteration 5498) ===
Q mean: -12.273840
Q std: 16.415174
Actor loss: 12.277816
Action reg: 0.003976
  l1.weight: grad_norm = 0.145405
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.119621
Total gradient norm: 0.363385
=== Actor Training Debug (Iteration 5499) ===
Q mean: -11.555704
Q std: 15.888748
Actor loss: 11.559669
Action reg: 0.003964
  l1.weight: grad_norm = 0.178755
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.158603
Total gradient norm: 0.420223
=== Actor Training Debug (Iteration 5500) ===
Q mean: -10.199926
Q std: 15.595632
Actor loss: 10.203874
Action reg: 0.003947
  l1.weight: grad_norm = 0.142622
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.125590
Total gradient norm: 0.369911
  Average reward: -316.169 | Average length: 100.0
Evaluation at episode 105: -316.169
=== Actor Training Debug (Iteration 5501) ===
Q mean: -11.526297
Q std: 15.206527
Actor loss: 11.530270
Action reg: 0.003973
  l1.weight: grad_norm = 0.195563
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.156016
Total gradient norm: 0.489159
=== Actor Training Debug (Iteration 5502) ===
Q mean: -11.021950
Q std: 14.255631
Actor loss: 11.025914
Action reg: 0.003964
  l1.weight: grad_norm = 0.140569
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.124435
Total gradient norm: 0.377025
=== Actor Training Debug (Iteration 5503) ===
Q mean: -11.116841
Q std: 15.764959
Actor loss: 11.120811
Action reg: 0.003969
  l1.weight: grad_norm = 0.205723
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.186582
Total gradient norm: 0.647516
=== Actor Training Debug (Iteration 5504) ===
Q mean: -10.366151
Q std: 16.286476
Actor loss: 10.370118
Action reg: 0.003967
  l1.weight: grad_norm = 0.128145
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.115235
Total gradient norm: 0.316120
=== Actor Training Debug (Iteration 5505) ===
Q mean: -11.753616
Q std: 15.042781
Actor loss: 11.757575
Action reg: 0.003959
  l1.weight: grad_norm = 0.190143
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.169232
Total gradient norm: 0.482927
=== Actor Training Debug (Iteration 5506) ===
Q mean: -11.037017
Q std: 16.457436
Actor loss: 11.040979
Action reg: 0.003963
  l1.weight: grad_norm = 0.228383
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.205117
Total gradient norm: 0.534918
=== Actor Training Debug (Iteration 5507) ===
Q mean: -11.543551
Q std: 14.781120
Actor loss: 11.547518
Action reg: 0.003967
  l1.weight: grad_norm = 0.164597
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.150891
Total gradient norm: 0.483434
=== Actor Training Debug (Iteration 5508) ===
Q mean: -12.265113
Q std: 16.217005
Actor loss: 12.269084
Action reg: 0.003971
  l1.weight: grad_norm = 0.206173
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.167045
Total gradient norm: 0.405082
=== Actor Training Debug (Iteration 5509) ===
Q mean: -10.360889
Q std: 14.514451
Actor loss: 10.364847
Action reg: 0.003957
  l1.weight: grad_norm = 0.190231
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.158569
Total gradient norm: 0.417334
=== Actor Training Debug (Iteration 5510) ===
Q mean: -12.306026
Q std: 16.847729
Actor loss: 12.309983
Action reg: 0.003957
  l1.weight: grad_norm = 0.162598
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.133936
Total gradient norm: 0.362123
=== Actor Training Debug (Iteration 5511) ===
Q mean: -12.907247
Q std: 16.508598
Actor loss: 12.911197
Action reg: 0.003950
  l1.weight: grad_norm = 0.103108
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.084718
Total gradient norm: 0.223120
=== Actor Training Debug (Iteration 5512) ===
Q mean: -12.089813
Q std: 16.435246
Actor loss: 12.093773
Action reg: 0.003959
  l1.weight: grad_norm = 0.153845
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.144655
Total gradient norm: 0.401042
=== Actor Training Debug (Iteration 5513) ===
Q mean: -12.150416
Q std: 15.162189
Actor loss: 12.154384
Action reg: 0.003967
  l1.weight: grad_norm = 0.164949
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.147742
Total gradient norm: 0.417351
=== Actor Training Debug (Iteration 5514) ===
Q mean: -10.264431
Q std: 15.368991
Actor loss: 10.268394
Action reg: 0.003963
  l1.weight: grad_norm = 0.168039
  l1.bias: grad_norm = 0.000656
  l2.weight: grad_norm = 0.140658
Total gradient norm: 0.366939
=== Actor Training Debug (Iteration 5515) ===
Q mean: -9.329823
Q std: 14.160351
Actor loss: 9.333792
Action reg: 0.003969
  l1.weight: grad_norm = 0.225136
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.200429
Total gradient norm: 0.587860
=== Actor Training Debug (Iteration 5516) ===
Q mean: -11.425747
Q std: 16.463848
Actor loss: 11.429710
Action reg: 0.003963
  l1.weight: grad_norm = 0.132129
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.108122
Total gradient norm: 0.298875
=== Actor Training Debug (Iteration 5517) ===
Q mean: -10.372990
Q std: 14.497478
Actor loss: 10.376952
Action reg: 0.003962
  l1.weight: grad_norm = 0.134273
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.106188
Total gradient norm: 0.284104
=== Actor Training Debug (Iteration 5518) ===
Q mean: -11.888987
Q std: 16.567339
Actor loss: 11.892947
Action reg: 0.003961
  l1.weight: grad_norm = 0.231360
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.191392
Total gradient norm: 0.512735
=== Actor Training Debug (Iteration 5519) ===
Q mean: -9.602692
Q std: 14.859270
Actor loss: 9.606666
Action reg: 0.003974
  l1.weight: grad_norm = 0.175587
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.153757
Total gradient norm: 0.398213
=== Actor Training Debug (Iteration 5520) ===
Q mean: -11.713781
Q std: 16.747265
Actor loss: 11.717750
Action reg: 0.003968
  l1.weight: grad_norm = 0.246482
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.208812
Total gradient norm: 0.646344
=== Actor Training Debug (Iteration 5521) ===
Q mean: -10.228763
Q std: 14.608918
Actor loss: 10.232720
Action reg: 0.003958
  l1.weight: grad_norm = 0.183553
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.144758
Total gradient norm: 0.463940
=== Actor Training Debug (Iteration 5522) ===
Q mean: -11.517391
Q std: 17.222229
Actor loss: 11.521354
Action reg: 0.003963
  l1.weight: grad_norm = 0.163040
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.153205
Total gradient norm: 0.396570
=== Actor Training Debug (Iteration 5523) ===
Q mean: -13.305803
Q std: 17.802013
Actor loss: 13.309764
Action reg: 0.003961
  l1.weight: grad_norm = 0.065415
  l1.bias: grad_norm = 0.000801
  l2.weight: grad_norm = 0.054207
Total gradient norm: 0.148130
=== Actor Training Debug (Iteration 5524) ===
Q mean: -10.195259
Q std: 15.106060
Actor loss: 10.199232
Action reg: 0.003973
  l1.weight: grad_norm = 0.116403
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.094659
Total gradient norm: 0.259503
=== Actor Training Debug (Iteration 5525) ===
Q mean: -12.152886
Q std: 16.660740
Actor loss: 12.156836
Action reg: 0.003950
  l1.weight: grad_norm = 0.149965
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.125447
Total gradient norm: 0.388307
=== Actor Training Debug (Iteration 5526) ===
Q mean: -12.126616
Q std: 16.420609
Actor loss: 12.130584
Action reg: 0.003968
  l1.weight: grad_norm = 0.090350
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.078733
Total gradient norm: 0.219511
=== Actor Training Debug (Iteration 5527) ===
Q mean: -10.974707
Q std: 14.211861
Actor loss: 10.978670
Action reg: 0.003964
  l1.weight: grad_norm = 0.132068
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.107665
Total gradient norm: 0.308900
=== Actor Training Debug (Iteration 5528) ===
Q mean: -11.516731
Q std: 15.260082
Actor loss: 11.520682
Action reg: 0.003951
  l1.weight: grad_norm = 0.145089
  l1.bias: grad_norm = 0.000557
  l2.weight: grad_norm = 0.126690
Total gradient norm: 0.382994
=== Actor Training Debug (Iteration 5529) ===
Q mean: -11.201897
Q std: 15.440286
Actor loss: 11.205866
Action reg: 0.003970
  l1.weight: grad_norm = 0.098397
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.085019
Total gradient norm: 0.232041
=== Actor Training Debug (Iteration 5530) ===
Q mean: -10.861855
Q std: 16.412750
Actor loss: 10.865789
Action reg: 0.003935
  l1.weight: grad_norm = 0.290521
  l1.bias: grad_norm = 0.000546
  l2.weight: grad_norm = 0.270538
Total gradient norm: 0.710892
=== Actor Training Debug (Iteration 5531) ===
Q mean: -10.188099
Q std: 14.734335
Actor loss: 10.192045
Action reg: 0.003946
  l1.weight: grad_norm = 0.188070
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.157683
Total gradient norm: 0.472633
=== Actor Training Debug (Iteration 5532) ===
Q mean: -10.932196
Q std: 17.150208
Actor loss: 10.936161
Action reg: 0.003965
  l1.weight: grad_norm = 0.141218
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.116778
Total gradient norm: 0.332831
=== Actor Training Debug (Iteration 5533) ===
Q mean: -9.453041
Q std: 14.168161
Actor loss: 9.457008
Action reg: 0.003967
  l1.weight: grad_norm = 0.180459
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.152721
Total gradient norm: 0.445805
=== Actor Training Debug (Iteration 5534) ===
Q mean: -10.163515
Q std: 13.750145
Actor loss: 10.167478
Action reg: 0.003963
  l1.weight: grad_norm = 0.201249
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.166831
Total gradient norm: 0.461488
=== Actor Training Debug (Iteration 5535) ===
Q mean: -10.254219
Q std: 13.815932
Actor loss: 10.258189
Action reg: 0.003970
  l1.weight: grad_norm = 0.188495
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.172515
Total gradient norm: 0.497894
=== Actor Training Debug (Iteration 5536) ===
Q mean: -10.992055
Q std: 16.018845
Actor loss: 10.996031
Action reg: 0.003976
  l1.weight: grad_norm = 0.098062
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.083378
Total gradient norm: 0.257518
=== Actor Training Debug (Iteration 5537) ===
Q mean: -11.913504
Q std: 16.413849
Actor loss: 11.917471
Action reg: 0.003967
  l1.weight: grad_norm = 0.183920
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.146270
Total gradient norm: 0.440418
=== Actor Training Debug (Iteration 5538) ===
Q mean: -11.441511
Q std: 15.945128
Actor loss: 11.445472
Action reg: 0.003961
  l1.weight: grad_norm = 0.176368
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.137656
Total gradient norm: 0.343330
=== Actor Training Debug (Iteration 5539) ===
Q mean: -10.746632
Q std: 14.342360
Actor loss: 10.750607
Action reg: 0.003975
  l1.weight: grad_norm = 0.137607
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.122230
Total gradient norm: 0.329514
=== Actor Training Debug (Iteration 5540) ===
Q mean: -10.479910
Q std: 15.858324
Actor loss: 10.483869
Action reg: 0.003959
  l1.weight: grad_norm = 0.182427
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.169345
Total gradient norm: 0.516651
=== Actor Training Debug (Iteration 5541) ===
Q mean: -10.737106
Q std: 15.455712
Actor loss: 10.741057
Action reg: 0.003951
  l1.weight: grad_norm = 0.111649
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.087442
Total gradient norm: 0.248897
=== Actor Training Debug (Iteration 5542) ===
Q mean: -11.990793
Q std: 17.314398
Actor loss: 11.994761
Action reg: 0.003967
  l1.weight: grad_norm = 0.157658
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.144341
Total gradient norm: 0.378052
=== Actor Training Debug (Iteration 5543) ===
Q mean: -10.389059
Q std: 15.105307
Actor loss: 10.393026
Action reg: 0.003967
  l1.weight: grad_norm = 0.151554
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.140515
Total gradient norm: 0.471933
=== Actor Training Debug (Iteration 5544) ===
Q mean: -10.645035
Q std: 12.982001
Actor loss: 10.649004
Action reg: 0.003970
  l1.weight: grad_norm = 0.241956
  l1.bias: grad_norm = 0.000440
  l2.weight: grad_norm = 0.203356
Total gradient norm: 0.549206
=== Actor Training Debug (Iteration 5545) ===
Q mean: -10.849814
Q std: 15.479840
Actor loss: 10.853786
Action reg: 0.003971
  l1.weight: grad_norm = 0.081759
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.079626
Total gradient norm: 0.234884
=== Actor Training Debug (Iteration 5546) ===
Q mean: -10.346252
Q std: 16.393265
Actor loss: 10.350194
Action reg: 0.003942
  l1.weight: grad_norm = 0.136930
  l1.bias: grad_norm = 0.001074
  l2.weight: grad_norm = 0.113465
Total gradient norm: 0.353835
=== Actor Training Debug (Iteration 5547) ===
Q mean: -9.832787
Q std: 16.323977
Actor loss: 9.836760
Action reg: 0.003973
  l1.weight: grad_norm = 0.301104
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.249395
Total gradient norm: 0.629080
=== Actor Training Debug (Iteration 5548) ===
Q mean: -11.024104
Q std: 15.651152
Actor loss: 11.028069
Action reg: 0.003965
  l1.weight: grad_norm = 0.078691
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.071069
Total gradient norm: 0.216483
=== Actor Training Debug (Iteration 5549) ===
Q mean: -13.046888
Q std: 18.027279
Actor loss: 13.050845
Action reg: 0.003957
  l1.weight: grad_norm = 0.177958
  l1.bias: grad_norm = 0.000713
  l2.weight: grad_norm = 0.174370
Total gradient norm: 0.478365
=== Actor Training Debug (Iteration 5550) ===
Q mean: -9.368134
Q std: 13.689693
Actor loss: 9.372095
Action reg: 0.003962
  l1.weight: grad_norm = 0.171970
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.149371
Total gradient norm: 0.422784
=== Actor Training Debug (Iteration 5551) ===
Q mean: -10.049932
Q std: 14.351852
Actor loss: 10.053900
Action reg: 0.003967
  l1.weight: grad_norm = 0.074883
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.064725
Total gradient norm: 0.188575
=== Actor Training Debug (Iteration 5552) ===
Q mean: -11.026667
Q std: 15.177550
Actor loss: 11.030623
Action reg: 0.003957
  l1.weight: grad_norm = 0.173836
  l1.bias: grad_norm = 0.000598
  l2.weight: grad_norm = 0.173821
Total gradient norm: 0.609666
=== Actor Training Debug (Iteration 5553) ===
Q mean: -10.506229
Q std: 14.658661
Actor loss: 10.510201
Action reg: 0.003973
  l1.weight: grad_norm = 0.245380
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.250675
Total gradient norm: 0.981869
=== Actor Training Debug (Iteration 5554) ===
Q mean: -11.780826
Q std: 17.138145
Actor loss: 11.784802
Action reg: 0.003977
  l1.weight: grad_norm = 0.142439
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.110715
Total gradient norm: 0.324555
=== Actor Training Debug (Iteration 5555) ===
Q mean: -11.273851
Q std: 15.800183
Actor loss: 11.277803
Action reg: 0.003952
  l1.weight: grad_norm = 0.171925
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.142286
Total gradient norm: 0.415052
=== Actor Training Debug (Iteration 5556) ===
Q mean: -8.826852
Q std: 14.159675
Actor loss: 8.830794
Action reg: 0.003943
  l1.weight: grad_norm = 0.305177
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.246337
Total gradient norm: 0.744331
=== Actor Training Debug (Iteration 5557) ===
Q mean: -11.702487
Q std: 16.832434
Actor loss: 11.706441
Action reg: 0.003954
  l1.weight: grad_norm = 0.243879
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.229779
Total gradient norm: 0.640396
=== Actor Training Debug (Iteration 5558) ===
Q mean: -10.887001
Q std: 16.193329
Actor loss: 10.890953
Action reg: 0.003952
  l1.weight: grad_norm = 0.342429
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.247269
Total gradient norm: 0.726174
=== Actor Training Debug (Iteration 5559) ===
Q mean: -10.180329
Q std: 15.273705
Actor loss: 10.184291
Action reg: 0.003961
  l1.weight: grad_norm = 0.243944
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.202720
Total gradient norm: 0.552093
=== Actor Training Debug (Iteration 5560) ===
Q mean: -10.196850
Q std: 14.379049
Actor loss: 10.200801
Action reg: 0.003951
  l1.weight: grad_norm = 0.092043
  l1.bias: grad_norm = 0.000590
  l2.weight: grad_norm = 0.079389
Total gradient norm: 0.244264
=== Actor Training Debug (Iteration 5561) ===
Q mean: -10.708820
Q std: 15.992343
Actor loss: 10.712785
Action reg: 0.003964
  l1.weight: grad_norm = 0.194843
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.143301
Total gradient norm: 0.399888
=== Actor Training Debug (Iteration 5562) ===
Q mean: -12.007811
Q std: 16.633160
Actor loss: 12.011774
Action reg: 0.003963
  l1.weight: grad_norm = 0.203775
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.172067
Total gradient norm: 0.430734
=== Actor Training Debug (Iteration 5563) ===
Q mean: -11.718159
Q std: 15.934418
Actor loss: 11.722122
Action reg: 0.003964
  l1.weight: grad_norm = 0.136277
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.127682
Total gradient norm: 0.384707
=== Actor Training Debug (Iteration 5564) ===
Q mean: -10.696444
Q std: 15.366345
Actor loss: 10.700414
Action reg: 0.003970
  l1.weight: grad_norm = 0.230609
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.178234
Total gradient norm: 0.508640
=== Actor Training Debug (Iteration 5565) ===
Q mean: -12.076014
Q std: 16.268864
Actor loss: 12.079973
Action reg: 0.003960
  l1.weight: grad_norm = 0.136806
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.130324
Total gradient norm: 0.363817
=== Actor Training Debug (Iteration 5566) ===
Q mean: -10.526979
Q std: 16.046799
Actor loss: 10.530957
Action reg: 0.003977
  l1.weight: grad_norm = 0.137756
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.113804
Total gradient norm: 0.321887
=== Actor Training Debug (Iteration 5567) ===
Q mean: -10.973093
Q std: 15.543051
Actor loss: 10.977059
Action reg: 0.003966
  l1.weight: grad_norm = 0.159757
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.131665
Total gradient norm: 0.360565
=== Actor Training Debug (Iteration 5568) ===
Q mean: -12.187735
Q std: 15.242273
Actor loss: 12.191691
Action reg: 0.003956
  l1.weight: grad_norm = 0.144867
  l1.bias: grad_norm = 0.000618
  l2.weight: grad_norm = 0.123131
Total gradient norm: 0.379914
=== Actor Training Debug (Iteration 5569) ===
Q mean: -9.848705
Q std: 14.125137
Actor loss: 9.852657
Action reg: 0.003952
  l1.weight: grad_norm = 0.270171
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.247071
Total gradient norm: 0.618723
=== Actor Training Debug (Iteration 5570) ===
Q mean: -8.935785
Q std: 13.085112
Actor loss: 8.939754
Action reg: 0.003968
  l1.weight: grad_norm = 0.210731
  l1.bias: grad_norm = 0.000727
  l2.weight: grad_norm = 0.180441
Total gradient norm: 0.477448
=== Actor Training Debug (Iteration 5571) ===
Q mean: -9.557567
Q std: 14.625011
Actor loss: 9.561522
Action reg: 0.003956
  l1.weight: grad_norm = 0.145491
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.141176
Total gradient norm: 0.421220
=== Actor Training Debug (Iteration 5572) ===
Q mean: -11.082900
Q std: 15.187372
Actor loss: 11.086860
Action reg: 0.003959
  l1.weight: grad_norm = 0.175927
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.155398
Total gradient norm: 0.419133
=== Actor Training Debug (Iteration 5573) ===
Q mean: -11.335983
Q std: 15.671435
Actor loss: 11.339944
Action reg: 0.003960
  l1.weight: grad_norm = 0.143389
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.110519
Total gradient norm: 0.318075
=== Actor Training Debug (Iteration 5574) ===
Q mean: -12.466412
Q std: 16.075783
Actor loss: 12.470380
Action reg: 0.003969
  l1.weight: grad_norm = 0.109251
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.095126
Total gradient norm: 0.271832
=== Actor Training Debug (Iteration 5575) ===
Q mean: -11.543186
Q std: 15.359499
Actor loss: 11.547161
Action reg: 0.003975
  l1.weight: grad_norm = 0.264569
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.206111
Total gradient norm: 0.541244
=== Actor Training Debug (Iteration 5576) ===
Q mean: -10.092122
Q std: 15.281510
Actor loss: 10.096085
Action reg: 0.003962
  l1.weight: grad_norm = 0.152482
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.147617
Total gradient norm: 0.413049
=== Actor Training Debug (Iteration 5577) ===
Q mean: -11.385605
Q std: 17.218000
Actor loss: 11.389574
Action reg: 0.003970
  l1.weight: grad_norm = 0.187919
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.168329
Total gradient norm: 0.485336
=== Actor Training Debug (Iteration 5578) ===
Q mean: -9.882076
Q std: 15.566447
Actor loss: 9.886032
Action reg: 0.003956
  l1.weight: grad_norm = 0.255955
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.202846
Total gradient norm: 0.516744
=== Actor Training Debug (Iteration 5579) ===
Q mean: -10.380867
Q std: 16.023537
Actor loss: 10.384844
Action reg: 0.003976
  l1.weight: grad_norm = 0.088868
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.076451
Total gradient norm: 0.218622
=== Actor Training Debug (Iteration 5580) ===
Q mean: -10.491825
Q std: 14.242149
Actor loss: 10.495795
Action reg: 0.003970
  l1.weight: grad_norm = 0.164210
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.132745
Total gradient norm: 0.355203
=== Actor Training Debug (Iteration 5581) ===
Q mean: -11.836282
Q std: 16.840710
Actor loss: 11.840252
Action reg: 0.003971
  l1.weight: grad_norm = 0.139549
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.117893
Total gradient norm: 0.359663
=== Actor Training Debug (Iteration 5582) ===
Q mean: -12.604126
Q std: 17.001377
Actor loss: 12.608085
Action reg: 0.003959
  l1.weight: grad_norm = 0.142370
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.124831
Total gradient norm: 0.336832
=== Actor Training Debug (Iteration 5583) ===
Q mean: -10.323442
Q std: 14.891462
Actor loss: 10.327411
Action reg: 0.003968
  l1.weight: grad_norm = 0.240851
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.199846
Total gradient norm: 0.645221
=== Actor Training Debug (Iteration 5584) ===
Q mean: -9.087122
Q std: 14.182738
Actor loss: 9.091091
Action reg: 0.003969
  l1.weight: grad_norm = 0.137559
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.110517
Total gradient norm: 0.310380
=== Actor Training Debug (Iteration 5585) ===
Q mean: -10.658319
Q std: 13.894335
Actor loss: 10.662284
Action reg: 0.003965
  l1.weight: grad_norm = 0.225084
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.190875
Total gradient norm: 0.539810
=== Actor Training Debug (Iteration 5586) ===
Q mean: -11.727580
Q std: 15.640521
Actor loss: 11.731537
Action reg: 0.003957
  l1.weight: grad_norm = 0.128258
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.105495
Total gradient norm: 0.282528
=== Actor Training Debug (Iteration 5587) ===
Q mean: -12.280951
Q std: 16.076044
Actor loss: 12.284908
Action reg: 0.003957
  l1.weight: grad_norm = 0.192303
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.160449
Total gradient norm: 0.468029
=== Actor Training Debug (Iteration 5588) ===
Q mean: -10.512721
Q std: 12.645276
Actor loss: 10.516663
Action reg: 0.003942
  l1.weight: grad_norm = 0.343221
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.299537
Total gradient norm: 0.742087
=== Actor Training Debug (Iteration 5589) ===
Q mean: -11.279565
Q std: 15.756911
Actor loss: 11.283525
Action reg: 0.003960
  l1.weight: grad_norm = 0.176689
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.149290
Total gradient norm: 0.389745
=== Actor Training Debug (Iteration 5590) ===
Q mean: -10.238907
Q std: 16.072218
Actor loss: 10.242848
Action reg: 0.003942
  l1.weight: grad_norm = 0.176739
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.156408
Total gradient norm: 0.437953
=== Actor Training Debug (Iteration 5591) ===
Q mean: -10.736747
Q std: 15.382753
Actor loss: 10.740700
Action reg: 0.003953
  l1.weight: grad_norm = 0.139807
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.120339
Total gradient norm: 0.309443
=== Actor Training Debug (Iteration 5592) ===
Q mean: -10.419823
Q std: 16.931778
Actor loss: 10.423792
Action reg: 0.003969
  l1.weight: grad_norm = 0.156114
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.133874
Total gradient norm: 0.348507
=== Actor Training Debug (Iteration 5593) ===
Q mean: -11.733511
Q std: 15.739756
Actor loss: 11.737483
Action reg: 0.003972
  l1.weight: grad_norm = 0.098123
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.078361
Total gradient norm: 0.226284
=== Actor Training Debug (Iteration 5594) ===
Q mean: -10.296055
Q std: 14.952264
Actor loss: 10.300035
Action reg: 0.003980
  l1.weight: grad_norm = 0.224605
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.177235
Total gradient norm: 0.413253
=== Actor Training Debug (Iteration 5595) ===
Q mean: -10.291191
Q std: 15.610384
Actor loss: 10.295153
Action reg: 0.003962
  l1.weight: grad_norm = 0.203441
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.176279
Total gradient norm: 0.507451
=== Actor Training Debug (Iteration 5596) ===
Q mean: -9.888630
Q std: 14.796273
Actor loss: 9.892602
Action reg: 0.003972
  l1.weight: grad_norm = 0.240029
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.217789
Total gradient norm: 0.582716
=== Actor Training Debug (Iteration 5597) ===
Q mean: -10.679285
Q std: 15.235402
Actor loss: 10.683233
Action reg: 0.003948
  l1.weight: grad_norm = 0.239363
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.212799
Total gradient norm: 0.628306
=== Actor Training Debug (Iteration 5598) ===
Q mean: -12.059832
Q std: 15.235813
Actor loss: 12.063788
Action reg: 0.003957
  l1.weight: grad_norm = 0.250795
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.221179
Total gradient norm: 0.830227
=== Actor Training Debug (Iteration 5599) ===
Q mean: -9.101146
Q std: 14.469304
Actor loss: 9.105099
Action reg: 0.003953
  l1.weight: grad_norm = 0.832624
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.787521
Total gradient norm: 2.413895
=== Actor Training Debug (Iteration 5600) ===
Q mean: -10.432636
Q std: 15.770513
Actor loss: 10.436606
Action reg: 0.003970
  l1.weight: grad_norm = 0.200441
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.158668
Total gradient norm: 0.422556
=== Actor Training Debug (Iteration 5601) ===
Q mean: -9.821848
Q std: 15.730269
Actor loss: 9.825824
Action reg: 0.003976
  l1.weight: grad_norm = 0.110428
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.098462
Total gradient norm: 0.261088
=== Actor Training Debug (Iteration 5602) ===
Q mean: -11.437607
Q std: 15.402271
Actor loss: 11.441577
Action reg: 0.003970
  l1.weight: grad_norm = 0.115488
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.101224
Total gradient norm: 0.301551
=== Actor Training Debug (Iteration 5603) ===
Q mean: -10.432899
Q std: 15.042562
Actor loss: 10.436840
Action reg: 0.003940
  l1.weight: grad_norm = 0.157450
  l1.bias: grad_norm = 0.000697
  l2.weight: grad_norm = 0.139541
Total gradient norm: 0.374257
=== Actor Training Debug (Iteration 5604) ===
Q mean: -11.245008
Q std: 16.210524
Actor loss: 11.248975
Action reg: 0.003967
  l1.weight: grad_norm = 0.239732
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.207283
Total gradient norm: 0.657201
=== Actor Training Debug (Iteration 5605) ===
Q mean: -12.242287
Q std: 16.169106
Actor loss: 12.246251
Action reg: 0.003964
  l1.weight: grad_norm = 0.141602
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.117684
Total gradient norm: 0.373850
=== Actor Training Debug (Iteration 5606) ===
Q mean: -10.408630
Q std: 15.488605
Actor loss: 10.412585
Action reg: 0.003954
  l1.weight: grad_norm = 0.236497
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.200484
Total gradient norm: 0.569536
=== Actor Training Debug (Iteration 5607) ===
Q mean: -11.360491
Q std: 15.346508
Actor loss: 11.364453
Action reg: 0.003962
  l1.weight: grad_norm = 0.266041
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.215512
Total gradient norm: 0.639765
=== Actor Training Debug (Iteration 5608) ===
Q mean: -10.571100
Q std: 14.711555
Actor loss: 10.575056
Action reg: 0.003955
  l1.weight: grad_norm = 0.144486
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.118105
Total gradient norm: 0.320385
=== Actor Training Debug (Iteration 5609) ===
Q mean: -10.281594
Q std: 16.121128
Actor loss: 10.285553
Action reg: 0.003959
  l1.weight: grad_norm = 0.144656
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.127135
Total gradient norm: 0.379654
=== Actor Training Debug (Iteration 5610) ===
Q mean: -12.579996
Q std: 16.951656
Actor loss: 12.583949
Action reg: 0.003953
  l1.weight: grad_norm = 0.222338
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.212612
Total gradient norm: 0.593756
=== Actor Training Debug (Iteration 5611) ===
Q mean: -11.385576
Q std: 16.038416
Actor loss: 11.389541
Action reg: 0.003964
  l1.weight: grad_norm = 0.177365
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.157714
Total gradient norm: 0.461508
=== Actor Training Debug (Iteration 5612) ===
Q mean: -10.145155
Q std: 14.946914
Actor loss: 10.149125
Action reg: 0.003970
  l1.weight: grad_norm = 0.157868
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.121044
Total gradient norm: 0.325384
=== Actor Training Debug (Iteration 5613) ===
Q mean: -11.783705
Q std: 17.212423
Actor loss: 11.787679
Action reg: 0.003974
  l1.weight: grad_norm = 0.195615
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.178050
Total gradient norm: 0.532742
=== Actor Training Debug (Iteration 5614) ===
Q mean: -11.545046
Q std: 16.561646
Actor loss: 11.549006
Action reg: 0.003960
  l1.weight: grad_norm = 0.178973
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.167159
Total gradient norm: 0.515761
=== Actor Training Debug (Iteration 5615) ===
Q mean: -13.002142
Q std: 15.311120
Actor loss: 13.006108
Action reg: 0.003966
  l1.weight: grad_norm = 0.157508
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.148708
Total gradient norm: 0.437160
=== Actor Training Debug (Iteration 5616) ===
Q mean: -10.747603
Q std: 16.087944
Actor loss: 10.751561
Action reg: 0.003958
  l1.weight: grad_norm = 0.200432
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.137296
Total gradient norm: 0.352185
=== Actor Training Debug (Iteration 5617) ===
Q mean: -10.608193
Q std: 14.520122
Actor loss: 10.612147
Action reg: 0.003954
  l1.weight: grad_norm = 0.129566
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.101458
Total gradient norm: 0.299823
=== Actor Training Debug (Iteration 5618) ===
Q mean: -11.220514
Q std: 15.281105
Actor loss: 11.224475
Action reg: 0.003960
  l1.weight: grad_norm = 0.184813
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.148736
Total gradient norm: 0.460393
=== Actor Training Debug (Iteration 5619) ===
Q mean: -9.740031
Q std: 15.309992
Actor loss: 9.744014
Action reg: 0.003982
  l1.weight: grad_norm = 0.142423
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.136462
Total gradient norm: 0.387568
=== Actor Training Debug (Iteration 5620) ===
Q mean: -11.595518
Q std: 15.197965
Actor loss: 11.599482
Action reg: 0.003964
  l1.weight: grad_norm = 0.134343
  l1.bias: grad_norm = 0.000631
  l2.weight: grad_norm = 0.107238
Total gradient norm: 0.318611
=== Actor Training Debug (Iteration 5621) ===
Q mean: -11.382179
Q std: 14.665170
Actor loss: 11.386151
Action reg: 0.003972
  l1.weight: grad_norm = 0.212376
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.180684
Total gradient norm: 0.566106
=== Actor Training Debug (Iteration 5622) ===
Q mean: -15.054068
Q std: 18.925247
Actor loss: 15.058036
Action reg: 0.003968
  l1.weight: grad_norm = 0.143253
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.126870
Total gradient norm: 0.363278
=== Actor Training Debug (Iteration 5623) ===
Q mean: -11.770691
Q std: 16.397238
Actor loss: 11.774654
Action reg: 0.003963
  l1.weight: grad_norm = 0.198748
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.169634
Total gradient norm: 0.502978
=== Actor Training Debug (Iteration 5624) ===
Q mean: -9.590343
Q std: 15.436907
Actor loss: 9.594320
Action reg: 0.003977
  l1.weight: grad_norm = 0.342517
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.271760
Total gradient norm: 0.891048
=== Actor Training Debug (Iteration 5625) ===
Q mean: -10.104140
Q std: 16.756454
Actor loss: 10.108095
Action reg: 0.003955
  l1.weight: grad_norm = 0.278459
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.247825
Total gradient norm: 0.699268
=== Actor Training Debug (Iteration 5626) ===
Q mean: -9.992324
Q std: 14.757930
Actor loss: 9.996291
Action reg: 0.003967
  l1.weight: grad_norm = 0.140749
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.126069
Total gradient norm: 0.356954
=== Actor Training Debug (Iteration 5627) ===
Q mean: -11.547526
Q std: 15.754263
Actor loss: 11.551492
Action reg: 0.003966
  l1.weight: grad_norm = 0.198815
  l1.bias: grad_norm = 0.000636
  l2.weight: grad_norm = 0.154439
Total gradient norm: 0.404232
=== Actor Training Debug (Iteration 5628) ===
Q mean: -11.871159
Q std: 16.432455
Actor loss: 11.875118
Action reg: 0.003960
  l1.weight: grad_norm = 0.136878
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.121476
Total gradient norm: 0.321574
=== Actor Training Debug (Iteration 5629) ===
Q mean: -11.993936
Q std: 16.773672
Actor loss: 11.997902
Action reg: 0.003966
  l1.weight: grad_norm = 0.125595
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.100242
Total gradient norm: 0.275736
=== Actor Training Debug (Iteration 5630) ===
Q mean: -9.893178
Q std: 15.169589
Actor loss: 9.897138
Action reg: 0.003959
  l1.weight: grad_norm = 0.155397
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.140581
Total gradient norm: 0.393570
=== Actor Training Debug (Iteration 5631) ===
Q mean: -11.742399
Q std: 15.649837
Actor loss: 11.746365
Action reg: 0.003965
  l1.weight: grad_norm = 0.171355
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.151078
Total gradient norm: 0.429417
=== Actor Training Debug (Iteration 5632) ===
Q mean: -9.564865
Q std: 15.600568
Actor loss: 9.568822
Action reg: 0.003957
  l1.weight: grad_norm = 0.249162
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.199594
Total gradient norm: 0.493069
=== Actor Training Debug (Iteration 5633) ===
Q mean: -9.967036
Q std: 14.439383
Actor loss: 9.971009
Action reg: 0.003973
  l1.weight: grad_norm = 0.118626
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.096494
Total gradient norm: 0.291364
=== Actor Training Debug (Iteration 5634) ===
Q mean: -11.073814
Q std: 15.331795
Actor loss: 11.077771
Action reg: 0.003957
  l1.weight: grad_norm = 0.178360
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.169255
Total gradient norm: 0.531929
=== Actor Training Debug (Iteration 5635) ===
Q mean: -10.185302
Q std: 14.782526
Actor loss: 10.189268
Action reg: 0.003967
  l1.weight: grad_norm = 0.140305
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.120706
Total gradient norm: 0.358269
=== Actor Training Debug (Iteration 5636) ===
Q mean: -10.750790
Q std: 14.903794
Actor loss: 10.754771
Action reg: 0.003982
  l1.weight: grad_norm = 0.124589
  l1.bias: grad_norm = 0.000039
  l2.weight: grad_norm = 0.106973
Total gradient norm: 0.329205
=== Actor Training Debug (Iteration 5637) ===
Q mean: -10.636770
Q std: 15.923025
Actor loss: 10.640741
Action reg: 0.003971
  l1.weight: grad_norm = 0.115440
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.126720
Total gradient norm: 0.445700
=== Actor Training Debug (Iteration 5638) ===
Q mean: -11.072323
Q std: 16.266138
Actor loss: 11.076289
Action reg: 0.003966
  l1.weight: grad_norm = 0.146721
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.125145
Total gradient norm: 0.390474
=== Actor Training Debug (Iteration 5639) ===
Q mean: -12.266036
Q std: 18.200321
Actor loss: 12.270009
Action reg: 0.003973
  l1.weight: grad_norm = 0.105154
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.087112
Total gradient norm: 0.242922
=== Actor Training Debug (Iteration 5640) ===
Q mean: -10.297531
Q std: 14.438940
Actor loss: 10.301511
Action reg: 0.003980
  l1.weight: grad_norm = 0.128975
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.113051
Total gradient norm: 0.309810
=== Actor Training Debug (Iteration 5641) ===
Q mean: -11.280257
Q std: 16.567617
Actor loss: 11.284221
Action reg: 0.003964
  l1.weight: grad_norm = 0.131889
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.111001
Total gradient norm: 0.310682
=== Actor Training Debug (Iteration 5642) ===
Q mean: -10.813740
Q std: 15.691509
Actor loss: 10.817688
Action reg: 0.003948
  l1.weight: grad_norm = 0.238261
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.210649
Total gradient norm: 0.572470
=== Actor Training Debug (Iteration 5643) ===
Q mean: -10.680151
Q std: 14.297324
Actor loss: 10.684124
Action reg: 0.003973
  l1.weight: grad_norm = 0.270886
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.196558
Total gradient norm: 0.602057
=== Actor Training Debug (Iteration 5644) ===
Q mean: -11.810719
Q std: 17.217001
Actor loss: 11.814663
Action reg: 0.003945
  l1.weight: grad_norm = 0.320841
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.266377
Total gradient norm: 0.777820
=== Actor Training Debug (Iteration 5645) ===
Q mean: -13.063541
Q std: 17.090637
Actor loss: 13.067502
Action reg: 0.003961
  l1.weight: grad_norm = 0.213713
  l1.bias: grad_norm = 0.000513
  l2.weight: grad_norm = 0.190325
Total gradient norm: 0.553095
=== Actor Training Debug (Iteration 5646) ===
Q mean: -10.726566
Q std: 15.703607
Actor loss: 10.730546
Action reg: 0.003980
  l1.weight: grad_norm = 0.130187
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.107351
Total gradient norm: 0.341004
=== Actor Training Debug (Iteration 5647) ===
Q mean: -12.628587
Q std: 16.618765
Actor loss: 12.632566
Action reg: 0.003980
  l1.weight: grad_norm = 0.292676
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.243542
Total gradient norm: 0.611870
=== Actor Training Debug (Iteration 5648) ===
Q mean: -11.925165
Q std: 17.110977
Actor loss: 11.929135
Action reg: 0.003970
  l1.weight: grad_norm = 0.216197
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.208532
Total gradient norm: 0.615151
=== Actor Training Debug (Iteration 5649) ===
Q mean: -11.604923
Q std: 16.041609
Actor loss: 11.608899
Action reg: 0.003976
  l1.weight: grad_norm = 0.055069
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.051576
Total gradient norm: 0.134655
=== Actor Training Debug (Iteration 5650) ===
Q mean: -12.608727
Q std: 16.646204
Actor loss: 12.612712
Action reg: 0.003985
  l1.weight: grad_norm = 0.131624
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.119379
Total gradient norm: 0.305612
=== Actor Training Debug (Iteration 5651) ===
Q mean: -9.706186
Q std: 14.421555
Actor loss: 9.710138
Action reg: 0.003952
  l1.weight: grad_norm = 0.137660
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.114953
Total gradient norm: 0.291248
=== Actor Training Debug (Iteration 5652) ===
Q mean: -9.367164
Q std: 13.708869
Actor loss: 9.371129
Action reg: 0.003966
  l1.weight: grad_norm = 0.102565
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.092684
Total gradient norm: 0.250207
=== Actor Training Debug (Iteration 5653) ===
Q mean: -11.430145
Q std: 16.392363
Actor loss: 11.434122
Action reg: 0.003977
  l1.weight: grad_norm = 0.082526
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.071596
Total gradient norm: 0.232807
=== Actor Training Debug (Iteration 5654) ===
Q mean: -9.848927
Q std: 14.828253
Actor loss: 9.852875
Action reg: 0.003948
  l1.weight: grad_norm = 0.246428
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.190645
Total gradient norm: 0.572746
=== Actor Training Debug (Iteration 5655) ===
Q mean: -11.602287
Q std: 14.885135
Actor loss: 11.606259
Action reg: 0.003972
  l1.weight: grad_norm = 0.151023
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.137068
Total gradient norm: 0.338707
=== Actor Training Debug (Iteration 5656) ===
Q mean: -10.344091
Q std: 14.587358
Actor loss: 10.348055
Action reg: 0.003963
  l1.weight: grad_norm = 0.231610
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.176320
Total gradient norm: 0.502599
=== Actor Training Debug (Iteration 5657) ===
Q mean: -11.066894
Q std: 15.500227
Actor loss: 11.070866
Action reg: 0.003972
  l1.weight: grad_norm = 0.181106
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.140627
Total gradient norm: 0.415805
=== Actor Training Debug (Iteration 5658) ===
Q mean: -10.629025
Q std: 14.779586
Actor loss: 10.632985
Action reg: 0.003961
  l1.weight: grad_norm = 0.161677
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.141459
Total gradient norm: 0.379675
=== Actor Training Debug (Iteration 5659) ===
Q mean: -11.096733
Q std: 15.319393
Actor loss: 11.100710
Action reg: 0.003977
  l1.weight: grad_norm = 0.115151
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.098083
Total gradient norm: 0.268842
=== Actor Training Debug (Iteration 5660) ===
Q mean: -10.433406
Q std: 15.121811
Actor loss: 10.437367
Action reg: 0.003962
  l1.weight: grad_norm = 0.069475
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.060704
Total gradient norm: 0.173602
=== Actor Training Debug (Iteration 5661) ===
Q mean: -10.562096
Q std: 14.465909
Actor loss: 10.566067
Action reg: 0.003971
  l1.weight: grad_norm = 0.114566
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.102322
Total gradient norm: 0.284534
=== Actor Training Debug (Iteration 5662) ===
Q mean: -10.492170
Q std: 14.611800
Actor loss: 10.496140
Action reg: 0.003970
  l1.weight: grad_norm = 0.169823
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.143621
Total gradient norm: 0.379184
=== Actor Training Debug (Iteration 5663) ===
Q mean: -9.920909
Q std: 15.879124
Actor loss: 9.924868
Action reg: 0.003959
  l1.weight: grad_norm = 0.111721
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.099746
Total gradient norm: 0.296940
=== Actor Training Debug (Iteration 5664) ===
Q mean: -10.599245
Q std: 14.642708
Actor loss: 10.603200
Action reg: 0.003955
  l1.weight: grad_norm = 0.157422
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.147109
Total gradient norm: 0.421240
=== Actor Training Debug (Iteration 5665) ===
Q mean: -12.547838
Q std: 16.711485
Actor loss: 12.551803
Action reg: 0.003964
  l1.weight: grad_norm = 0.079754
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.068435
Total gradient norm: 0.183062
=== Actor Training Debug (Iteration 5666) ===
Q mean: -10.955751
Q std: 15.699788
Actor loss: 10.959713
Action reg: 0.003961
  l1.weight: grad_norm = 0.184510
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.150745
Total gradient norm: 0.413846
=== Actor Training Debug (Iteration 5667) ===
Q mean: -10.684471
Q std: 14.439043
Actor loss: 10.688430
Action reg: 0.003958
  l1.weight: grad_norm = 0.232985
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.186133
Total gradient norm: 0.532801
=== Actor Training Debug (Iteration 5668) ===
Q mean: -10.565229
Q std: 15.720648
Actor loss: 10.569181
Action reg: 0.003952
  l1.weight: grad_norm = 0.175260
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.157782
Total gradient norm: 0.497520
=== Actor Training Debug (Iteration 5669) ===
Q mean: -11.315147
Q std: 15.837162
Actor loss: 11.319088
Action reg: 0.003941
  l1.weight: grad_norm = 0.265797
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.214381
Total gradient norm: 0.574538
=== Actor Training Debug (Iteration 5670) ===
Q mean: -12.354849
Q std: 16.712917
Actor loss: 12.358824
Action reg: 0.003974
  l1.weight: grad_norm = 0.106198
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.103915
Total gradient norm: 0.299955
=== Actor Training Debug (Iteration 5671) ===
Q mean: -11.421694
Q std: 15.499279
Actor loss: 11.425660
Action reg: 0.003967
  l1.weight: grad_norm = 0.206632
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.175895
Total gradient norm: 0.450130
=== Actor Training Debug (Iteration 5672) ===
Q mean: -9.772253
Q std: 12.798883
Actor loss: 9.776229
Action reg: 0.003975
  l1.weight: grad_norm = 0.842126
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.765136
Total gradient norm: 2.464593
=== Actor Training Debug (Iteration 5673) ===
Q mean: -10.033669
Q std: 14.768869
Actor loss: 10.037642
Action reg: 0.003973
  l1.weight: grad_norm = 0.252320
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.200239
Total gradient norm: 0.489816
=== Actor Training Debug (Iteration 5674) ===
Q mean: -10.707515
Q std: 15.423373
Actor loss: 10.711476
Action reg: 0.003961
  l1.weight: grad_norm = 0.115563
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.110533
Total gradient norm: 0.324622
=== Actor Training Debug (Iteration 5675) ===
Q mean: -10.934208
Q std: 15.645112
Actor loss: 10.938181
Action reg: 0.003973
  l1.weight: grad_norm = 0.169598
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.148662
Total gradient norm: 0.354286
=== Actor Training Debug (Iteration 5676) ===
Q mean: -10.966572
Q std: 14.414557
Actor loss: 10.970533
Action reg: 0.003961
  l1.weight: grad_norm = 0.092191
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.085498
Total gradient norm: 0.218151
=== Actor Training Debug (Iteration 5677) ===
Q mean: -12.565975
Q std: 15.952163
Actor loss: 12.569941
Action reg: 0.003965
  l1.weight: grad_norm = 0.297115
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.235148
Total gradient norm: 0.690168
=== Actor Training Debug (Iteration 5678) ===
Q mean: -11.322156
Q std: 14.152956
Actor loss: 11.326119
Action reg: 0.003964
  l1.weight: grad_norm = 0.153274
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.148000
Total gradient norm: 0.435570
=== Actor Training Debug (Iteration 5679) ===
Q mean: -11.566062
Q std: 16.213467
Actor loss: 11.570030
Action reg: 0.003969
  l1.weight: grad_norm = 0.193001
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.166163
Total gradient norm: 0.427419
=== Actor Training Debug (Iteration 5680) ===
Q mean: -10.555837
Q std: 15.513015
Actor loss: 10.559816
Action reg: 0.003979
  l1.weight: grad_norm = 0.044058
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.038373
Total gradient norm: 0.107591
=== Actor Training Debug (Iteration 5681) ===
Q mean: -11.016783
Q std: 15.897994
Actor loss: 11.020755
Action reg: 0.003972
  l1.weight: grad_norm = 0.300693
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.241802
Total gradient norm: 0.670555
=== Actor Training Debug (Iteration 5682) ===
Q mean: -11.418509
Q std: 17.589598
Actor loss: 11.422475
Action reg: 0.003966
  l1.weight: grad_norm = 0.295486
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.228099
Total gradient norm: 0.645433
=== Actor Training Debug (Iteration 5683) ===
Q mean: -13.181086
Q std: 19.029076
Actor loss: 13.185043
Action reg: 0.003958
  l1.weight: grad_norm = 0.122786
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.107339
Total gradient norm: 0.327219
=== Actor Training Debug (Iteration 5684) ===
Q mean: -11.167912
Q std: 15.952878
Actor loss: 11.171883
Action reg: 0.003971
  l1.weight: grad_norm = 0.196974
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.174857
Total gradient norm: 0.552802
=== Actor Training Debug (Iteration 5685) ===
Q mean: -12.178054
Q std: 17.135614
Actor loss: 12.182004
Action reg: 0.003950
  l1.weight: grad_norm = 0.329276
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.295260
Total gradient norm: 0.823651
=== Actor Training Debug (Iteration 5686) ===
Q mean: -9.747656
Q std: 15.064690
Actor loss: 9.751605
Action reg: 0.003949
  l1.weight: grad_norm = 0.304008
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.274100
Total gradient norm: 0.669843
=== Actor Training Debug (Iteration 5687) ===
Q mean: -12.318683
Q std: 15.819942
Actor loss: 12.322657
Action reg: 0.003974
  l1.weight: grad_norm = 0.167789
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.133579
Total gradient norm: 0.375509
=== Actor Training Debug (Iteration 5688) ===
Q mean: -10.727488
Q std: 15.178176
Actor loss: 10.731442
Action reg: 0.003955
  l1.weight: grad_norm = 0.201415
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.187351
Total gradient norm: 0.565606
=== Actor Training Debug (Iteration 5689) ===
Q mean: -10.956264
Q std: 14.973577
Actor loss: 10.960241
Action reg: 0.003976
  l1.weight: grad_norm = 0.176010
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.155377
Total gradient norm: 0.433178
=== Actor Training Debug (Iteration 5690) ===
Q mean: -10.582256
Q std: 14.942338
Actor loss: 10.586233
Action reg: 0.003977
  l1.weight: grad_norm = 0.134736
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.117096
Total gradient norm: 0.392815
=== Actor Training Debug (Iteration 5691) ===
Q mean: -11.672220
Q std: 15.162510
Actor loss: 11.676193
Action reg: 0.003973
  l1.weight: grad_norm = 0.151097
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.126822
Total gradient norm: 0.364008
=== Actor Training Debug (Iteration 5692) ===
Q mean: -10.198736
Q std: 15.572281
Actor loss: 10.202703
Action reg: 0.003967
  l1.weight: grad_norm = 0.242520
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.190829
Total gradient norm: 0.595740
=== Actor Training Debug (Iteration 5693) ===
Q mean: -9.626220
Q std: 13.530529
Actor loss: 9.630197
Action reg: 0.003977
  l1.weight: grad_norm = 0.163812
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.130159
Total gradient norm: 0.367203
=== Actor Training Debug (Iteration 5694) ===
Q mean: -10.869034
Q std: 15.373734
Actor loss: 10.872995
Action reg: 0.003961
  l1.weight: grad_norm = 0.133334
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.115599
Total gradient norm: 0.303519
=== Actor Training Debug (Iteration 5695) ===
Q mean: -10.575029
Q std: 15.333641
Actor loss: 10.578994
Action reg: 0.003965
  l1.weight: grad_norm = 0.159222
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.158731
Total gradient norm: 0.451920
=== Actor Training Debug (Iteration 5696) ===
Q mean: -11.170347
Q std: 14.800339
Actor loss: 11.174316
Action reg: 0.003969
  l1.weight: grad_norm = 0.166968
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.148227
Total gradient norm: 0.434909
=== Actor Training Debug (Iteration 5697) ===
Q mean: -9.662977
Q std: 13.891958
Actor loss: 9.666939
Action reg: 0.003961
  l1.weight: grad_norm = 0.231639
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.190256
Total gradient norm: 0.547721
=== Actor Training Debug (Iteration 5698) ===
Q mean: -10.023190
Q std: 15.999607
Actor loss: 10.027131
Action reg: 0.003941
  l1.weight: grad_norm = 0.369473
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.263833
Total gradient norm: 0.723764
=== Actor Training Debug (Iteration 5699) ===
Q mean: -11.915607
Q std: 16.695906
Actor loss: 11.919581
Action reg: 0.003974
  l1.weight: grad_norm = 0.126167
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.112690
Total gradient norm: 0.347010
=== Actor Training Debug (Iteration 5700) ===
Q mean: -11.895735
Q std: 16.226652
Actor loss: 11.899695
Action reg: 0.003961
  l1.weight: grad_norm = 0.132663
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.108140
Total gradient norm: 0.301097
=== Actor Training Debug (Iteration 5701) ===
Q mean: -11.153263
Q std: 14.841274
Actor loss: 11.157218
Action reg: 0.003955
  l1.weight: grad_norm = 0.145010
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.133629
Total gradient norm: 0.380233
=== Actor Training Debug (Iteration 5702) ===
Q mean: -10.137752
Q std: 13.885341
Actor loss: 10.141714
Action reg: 0.003963
  l1.weight: grad_norm = 0.275304
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.229016
Total gradient norm: 0.644998
=== Actor Training Debug (Iteration 5703) ===
Q mean: -11.347359
Q std: 15.425311
Actor loss: 11.351322
Action reg: 0.003964
  l1.weight: grad_norm = 0.080096
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.076560
Total gradient norm: 0.278939
=== Actor Training Debug (Iteration 5704) ===
Q mean: -11.505146
Q std: 16.890659
Actor loss: 11.509106
Action reg: 0.003960
  l1.weight: grad_norm = 0.172144
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.145059
Total gradient norm: 0.492505
=== Actor Training Debug (Iteration 5705) ===
Q mean: -10.621506
Q std: 13.843730
Actor loss: 10.625488
Action reg: 0.003982
  l1.weight: grad_norm = 0.126471
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.097359
Total gradient norm: 0.278701
=== Actor Training Debug (Iteration 5706) ===
Q mean: -12.662527
Q std: 17.255821
Actor loss: 12.666489
Action reg: 0.003962
  l1.weight: grad_norm = 0.185238
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.171164
Total gradient norm: 0.548338
=== Actor Training Debug (Iteration 5707) ===
Q mean: -10.074028
Q std: 13.309520
Actor loss: 10.077989
Action reg: 0.003960
  l1.weight: grad_norm = 0.224719
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.188731
Total gradient norm: 0.505134
=== Actor Training Debug (Iteration 5708) ===
Q mean: -11.669600
Q std: 16.933788
Actor loss: 11.673573
Action reg: 0.003973
  l1.weight: grad_norm = 0.076710
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.071816
Total gradient norm: 0.212507
=== Actor Training Debug (Iteration 5709) ===
Q mean: -10.301195
Q std: 14.875516
Actor loss: 10.305152
Action reg: 0.003957
  l1.weight: grad_norm = 0.167648
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.142554
Total gradient norm: 0.407883
=== Actor Training Debug (Iteration 5710) ===
Q mean: -11.828750
Q std: 15.994893
Actor loss: 11.832724
Action reg: 0.003974
  l1.weight: grad_norm = 0.138700
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.118471
Total gradient norm: 0.298458
=== Actor Training Debug (Iteration 5711) ===
Q mean: -10.861200
Q std: 14.545681
Actor loss: 10.865167
Action reg: 0.003966
  l1.weight: grad_norm = 0.180729
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.150705
Total gradient norm: 0.452636
=== Actor Training Debug (Iteration 5712) ===
Q mean: -11.308964
Q std: 16.066591
Actor loss: 11.312933
Action reg: 0.003969
  l1.weight: grad_norm = 0.139565
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.117804
Total gradient norm: 0.307426
=== Actor Training Debug (Iteration 5713) ===
Q mean: -11.266181
Q std: 15.242911
Actor loss: 11.270155
Action reg: 0.003974
  l1.weight: grad_norm = 0.108470
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.086479
Total gradient norm: 0.262137
=== Actor Training Debug (Iteration 5714) ===
Q mean: -14.929897
Q std: 18.847376
Actor loss: 14.933871
Action reg: 0.003974
  l1.weight: grad_norm = 0.094227
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.079087
Total gradient norm: 0.238869
=== Actor Training Debug (Iteration 5715) ===
Q mean: -13.514920
Q std: 17.466621
Actor loss: 13.518896
Action reg: 0.003976
  l1.weight: grad_norm = 0.290171
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.236816
Total gradient norm: 0.602572
=== Actor Training Debug (Iteration 5716) ===
Q mean: -11.302654
Q std: 15.293890
Actor loss: 11.306622
Action reg: 0.003967
  l1.weight: grad_norm = 0.104842
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.095072
Total gradient norm: 0.254505
=== Actor Training Debug (Iteration 5717) ===
Q mean: -11.940977
Q std: 16.400360
Actor loss: 11.944951
Action reg: 0.003974
  l1.weight: grad_norm = 0.322921
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.296158
Total gradient norm: 1.081404
=== Actor Training Debug (Iteration 5718) ===
Q mean: -11.320355
Q std: 15.989908
Actor loss: 11.324309
Action reg: 0.003954
  l1.weight: grad_norm = 0.135131
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.101298
Total gradient norm: 0.302628
=== Actor Training Debug (Iteration 5719) ===
Q mean: -11.763648
Q std: 17.040260
Actor loss: 11.767608
Action reg: 0.003959
  l1.weight: grad_norm = 0.151182
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.126812
Total gradient norm: 0.352991
=== Actor Training Debug (Iteration 5720) ===
Q mean: -11.551522
Q std: 14.984561
Actor loss: 11.555486
Action reg: 0.003963
  l1.weight: grad_norm = 0.130196
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.112734
Total gradient norm: 0.390239
=== Actor Training Debug (Iteration 5721) ===
Q mean: -10.843592
Q std: 14.688586
Actor loss: 10.847560
Action reg: 0.003968
  l1.weight: grad_norm = 0.230747
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.199692
Total gradient norm: 0.569419
=== Actor Training Debug (Iteration 5722) ===
Q mean: -10.650726
Q std: 16.453888
Actor loss: 10.654680
Action reg: 0.003954
  l1.weight: grad_norm = 0.167849
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.141234
Total gradient norm: 0.388606
=== Actor Training Debug (Iteration 5723) ===
Q mean: -12.038944
Q std: 16.168190
Actor loss: 12.042908
Action reg: 0.003963
  l1.weight: grad_norm = 0.272295
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.240957
Total gradient norm: 0.605925
=== Actor Training Debug (Iteration 5724) ===
Q mean: -10.627838
Q std: 15.462363
Actor loss: 10.631816
Action reg: 0.003978
  l1.weight: grad_norm = 0.216122
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.164866
Total gradient norm: 0.516045
=== Actor Training Debug (Iteration 5725) ===
Q mean: -10.033732
Q std: 15.069915
Actor loss: 10.037683
Action reg: 0.003951
  l1.weight: grad_norm = 0.430372
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.375178
Total gradient norm: 0.950329
=== Actor Training Debug (Iteration 5726) ===
Q mean: -13.504996
Q std: 17.926638
Actor loss: 13.508959
Action reg: 0.003962
  l1.weight: grad_norm = 0.158525
  l1.bias: grad_norm = 0.000491
  l2.weight: grad_norm = 0.135171
Total gradient norm: 0.430005
=== Actor Training Debug (Iteration 5727) ===
Q mean: -13.197255
Q std: 18.345060
Actor loss: 13.201235
Action reg: 0.003980
  l1.weight: grad_norm = 0.078457
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.059909
Total gradient norm: 0.169161
=== Actor Training Debug (Iteration 5728) ===
Q mean: -10.068822
Q std: 14.467606
Actor loss: 10.072780
Action reg: 0.003958
  l1.weight: grad_norm = 0.150003
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.126644
Total gradient norm: 0.327904
=== Actor Training Debug (Iteration 5729) ===
Q mean: -8.917312
Q std: 13.592092
Actor loss: 8.921277
Action reg: 0.003965
  l1.weight: grad_norm = 0.115639
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.122145
Total gradient norm: 0.324300
=== Actor Training Debug (Iteration 5730) ===
Q mean: -11.687731
Q std: 15.785357
Actor loss: 11.691694
Action reg: 0.003963
  l1.weight: grad_norm = 0.151420
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.132687
Total gradient norm: 0.417897
=== Actor Training Debug (Iteration 5731) ===
Q mean: -11.198076
Q std: 16.428705
Actor loss: 11.202034
Action reg: 0.003958
  l1.weight: grad_norm = 0.146154
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.126634
Total gradient norm: 0.361888
=== Actor Training Debug (Iteration 5732) ===
Q mean: -10.247431
Q std: 14.448145
Actor loss: 10.251393
Action reg: 0.003962
  l1.weight: grad_norm = 0.169126
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.161257
Total gradient norm: 0.408286
=== Actor Training Debug (Iteration 5733) ===
Q mean: -12.896610
Q std: 18.029823
Actor loss: 12.900560
Action reg: 0.003951
  l1.weight: grad_norm = 0.245565
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.226486
Total gradient norm: 0.599247
=== Actor Training Debug (Iteration 5734) ===
Q mean: -9.086750
Q std: 14.624026
Actor loss: 9.090717
Action reg: 0.003967
  l1.weight: grad_norm = 0.187914
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.152518
Total gradient norm: 0.396502
=== Actor Training Debug (Iteration 5735) ===
Q mean: -11.202894
Q std: 15.577929
Actor loss: 11.206843
Action reg: 0.003949
  l1.weight: grad_norm = 0.238232
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.191237
Total gradient norm: 0.544057
=== Actor Training Debug (Iteration 5736) ===
Q mean: -9.633053
Q std: 13.896901
Actor loss: 9.637009
Action reg: 0.003956
  l1.weight: grad_norm = 0.173241
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.148984
Total gradient norm: 0.380053
=== Actor Training Debug (Iteration 5737) ===
Q mean: -8.956244
Q std: 13.549282
Actor loss: 8.960206
Action reg: 0.003962
  l1.weight: grad_norm = 0.129239
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.099548
Total gradient norm: 0.272899
=== Actor Training Debug (Iteration 5738) ===
Q mean: -11.119118
Q std: 14.214632
Actor loss: 11.123072
Action reg: 0.003954
  l1.weight: grad_norm = 0.229714
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.186525
Total gradient norm: 0.490606
=== Actor Training Debug (Iteration 5739) ===
Q mean: -11.624512
Q std: 14.990339
Actor loss: 11.628471
Action reg: 0.003959
  l1.weight: grad_norm = 0.143005
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.126523
Total gradient norm: 0.347493
=== Actor Training Debug (Iteration 5740) ===
Q mean: -7.440683
Q std: 12.382245
Actor loss: 7.444628
Action reg: 0.003945
  l1.weight: grad_norm = 0.330005
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.268271
Total gradient norm: 0.689567
=== Actor Training Debug (Iteration 5741) ===
Q mean: -11.771696
Q std: 17.541903
Actor loss: 11.775655
Action reg: 0.003959
  l1.weight: grad_norm = 0.113903
  l1.bias: grad_norm = 0.000445
  l2.weight: grad_norm = 0.093744
Total gradient norm: 0.254012
=== Actor Training Debug (Iteration 5742) ===
Q mean: -11.156225
Q std: 15.543434
Actor loss: 11.160191
Action reg: 0.003966
  l1.weight: grad_norm = 0.293804
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.222559
Total gradient norm: 0.629854
=== Actor Training Debug (Iteration 5743) ===
Q mean: -10.050425
Q std: 14.856542
Actor loss: 10.054373
Action reg: 0.003948
  l1.weight: grad_norm = 0.191506
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.173981
Total gradient norm: 0.515929
=== Actor Training Debug (Iteration 5744) ===
Q mean: -10.661596
Q std: 14.669796
Actor loss: 10.665556
Action reg: 0.003960
  l1.weight: grad_norm = 0.190883
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.147057
Total gradient norm: 0.437125
=== Actor Training Debug (Iteration 5745) ===
Q mean: -13.386338
Q std: 17.991777
Actor loss: 13.390309
Action reg: 0.003971
  l1.weight: grad_norm = 0.183208
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.162332
Total gradient norm: 0.433123
=== Actor Training Debug (Iteration 5746) ===
Q mean: -11.215396
Q std: 16.103008
Actor loss: 11.219334
Action reg: 0.003937
  l1.weight: grad_norm = 0.379365
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.316555
Total gradient norm: 0.853350
=== Actor Training Debug (Iteration 5747) ===
Q mean: -10.834595
Q std: 14.607178
Actor loss: 10.838564
Action reg: 0.003970
  l1.weight: grad_norm = 0.261664
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.240767
Total gradient norm: 0.737430
=== Actor Training Debug (Iteration 5748) ===
Q mean: -12.184430
Q std: 16.463009
Actor loss: 12.188377
Action reg: 0.003948
  l1.weight: grad_norm = 0.202978
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.172312
Total gradient norm: 0.468809
=== Actor Training Debug (Iteration 5749) ===
Q mean: -10.996515
Q std: 14.203755
Actor loss: 11.000484
Action reg: 0.003969
  l1.weight: grad_norm = 0.105763
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.092043
Total gradient norm: 0.245183
=== Actor Training Debug (Iteration 5750) ===
Q mean: -11.584946
Q std: 15.525308
Actor loss: 11.588911
Action reg: 0.003965
  l1.weight: grad_norm = 0.268832
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.228043
Total gradient norm: 0.597890
=== Actor Training Debug (Iteration 5751) ===
Q mean: -11.808673
Q std: 16.583765
Actor loss: 11.812639
Action reg: 0.003966
  l1.weight: grad_norm = 0.150899
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.135216
Total gradient norm: 0.372894
=== Actor Training Debug (Iteration 5752) ===
Q mean: -11.249529
Q std: 16.046635
Actor loss: 11.253471
Action reg: 0.003942
  l1.weight: grad_norm = 0.144945
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.130463
Total gradient norm: 0.442400
=== Actor Training Debug (Iteration 5753) ===
Q mean: -10.872541
Q std: 14.646440
Actor loss: 10.876498
Action reg: 0.003957
  l1.weight: grad_norm = 0.208083
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.202257
Total gradient norm: 0.566838
=== Actor Training Debug (Iteration 5754) ===
Q mean: -11.197372
Q std: 15.224565
Actor loss: 11.201329
Action reg: 0.003957
  l1.weight: grad_norm = 0.170610
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.136840
Total gradient norm: 0.379403
=== Actor Training Debug (Iteration 5755) ===
Q mean: -11.703556
Q std: 17.422121
Actor loss: 11.707519
Action reg: 0.003963
  l1.weight: grad_norm = 0.189496
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.157049
Total gradient norm: 0.527994
=== Actor Training Debug (Iteration 5756) ===
Q mean: -11.982425
Q std: 15.778934
Actor loss: 11.986378
Action reg: 0.003953
  l1.weight: grad_norm = 0.118394
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.110898
Total gradient norm: 0.347784
=== Actor Training Debug (Iteration 5757) ===
Q mean: -10.801760
Q std: 14.557521
Actor loss: 10.805715
Action reg: 0.003955
  l1.weight: grad_norm = 0.211757
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.190139
Total gradient norm: 0.564158
=== Actor Training Debug (Iteration 5758) ===
Q mean: -11.279083
Q std: 16.047319
Actor loss: 11.283055
Action reg: 0.003972
  l1.weight: grad_norm = 0.142010
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.127176
Total gradient norm: 0.371509
=== Actor Training Debug (Iteration 5759) ===
Q mean: -11.450129
Q std: 16.846184
Actor loss: 11.454095
Action reg: 0.003966
  l1.weight: grad_norm = 0.462896
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.398896
Total gradient norm: 1.010034
=== Actor Training Debug (Iteration 5760) ===
Q mean: -10.354664
Q std: 15.973059
Actor loss: 10.358616
Action reg: 0.003952
  l1.weight: grad_norm = 0.122722
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.109509
Total gradient norm: 0.336538
=== Actor Training Debug (Iteration 5761) ===
Q mean: -10.781676
Q std: 15.242656
Actor loss: 10.785644
Action reg: 0.003967
  l1.weight: grad_norm = 0.164404
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.142929
Total gradient norm: 0.451088
=== Actor Training Debug (Iteration 5762) ===
Q mean: -11.959444
Q std: 15.564894
Actor loss: 11.963408
Action reg: 0.003965
  l1.weight: grad_norm = 0.160295
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.129264
Total gradient norm: 0.357385
=== Actor Training Debug (Iteration 5763) ===
Q mean: -12.119487
Q std: 17.246685
Actor loss: 12.123456
Action reg: 0.003970
  l1.weight: grad_norm = 0.137595
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.117656
Total gradient norm: 0.315272
=== Actor Training Debug (Iteration 5764) ===
Q mean: -10.816508
Q std: 16.128975
Actor loss: 10.820484
Action reg: 0.003976
  l1.weight: grad_norm = 0.105897
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.091510
Total gradient norm: 0.267388
=== Actor Training Debug (Iteration 5765) ===
Q mean: -10.406975
Q std: 15.487892
Actor loss: 10.410933
Action reg: 0.003958
  l1.weight: grad_norm = 0.069388
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.063636
Total gradient norm: 0.169771
=== Actor Training Debug (Iteration 5766) ===
Q mean: -10.766685
Q std: 14.251254
Actor loss: 10.770668
Action reg: 0.003982
  l1.weight: grad_norm = 0.216449
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.182197
Total gradient norm: 0.420740
=== Actor Training Debug (Iteration 5767) ===
Q mean: -12.045147
Q std: 15.761687
Actor loss: 12.049109
Action reg: 0.003962
  l1.weight: grad_norm = 0.205237
  l1.bias: grad_norm = 0.000425
  l2.weight: grad_norm = 0.182661
Total gradient norm: 0.547703
=== Actor Training Debug (Iteration 5768) ===
Q mean: -11.974770
Q std: 16.725679
Actor loss: 11.978723
Action reg: 0.003953
  l1.weight: grad_norm = 0.167778
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.127008
Total gradient norm: 0.354830
=== Actor Training Debug (Iteration 5769) ===
Q mean: -12.466337
Q std: 16.729239
Actor loss: 12.470304
Action reg: 0.003968
  l1.weight: grad_norm = 0.109736
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.107232
Total gradient norm: 0.283427
=== Actor Training Debug (Iteration 5770) ===
Q mean: -10.220444
Q std: 16.414284
Actor loss: 10.224428
Action reg: 0.003984
  l1.weight: grad_norm = 0.131193
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.120022
Total gradient norm: 0.324064
=== Actor Training Debug (Iteration 5771) ===
Q mean: -11.437908
Q std: 15.879017
Actor loss: 11.441877
Action reg: 0.003969
  l1.weight: grad_norm = 0.121864
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.108561
Total gradient norm: 0.280453
=== Actor Training Debug (Iteration 5772) ===
Q mean: -11.919464
Q std: 15.212983
Actor loss: 11.923426
Action reg: 0.003961
  l1.weight: grad_norm = 0.242061
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.202514
Total gradient norm: 0.498303
=== Actor Training Debug (Iteration 5773) ===
Q mean: -11.836411
Q std: 16.034317
Actor loss: 11.840373
Action reg: 0.003963
  l1.weight: grad_norm = 0.173393
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.149824
Total gradient norm: 0.421436
=== Actor Training Debug (Iteration 5774) ===
Q mean: -11.933352
Q std: 16.729548
Actor loss: 11.937301
Action reg: 0.003948
  l1.weight: grad_norm = 0.229526
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.191277
Total gradient norm: 0.544631
=== Actor Training Debug (Iteration 5775) ===
Q mean: -10.896690
Q std: 14.719460
Actor loss: 10.900661
Action reg: 0.003970
  l1.weight: grad_norm = 0.139753
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.119045
Total gradient norm: 0.331600
=== Actor Training Debug (Iteration 5776) ===
Q mean: -11.432693
Q std: 17.074467
Actor loss: 11.436646
Action reg: 0.003953
  l1.weight: grad_norm = 0.167269
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.130227
Total gradient norm: 0.388454
=== Actor Training Debug (Iteration 5777) ===
Q mean: -11.721622
Q std: 17.099848
Actor loss: 11.725590
Action reg: 0.003968
  l1.weight: grad_norm = 0.147451
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.128578
Total gradient norm: 0.350331
=== Actor Training Debug (Iteration 5778) ===
Q mean: -9.662314
Q std: 13.640319
Actor loss: 9.666278
Action reg: 0.003963
  l1.weight: grad_norm = 0.186331
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.162932
Total gradient norm: 0.421213
=== Actor Training Debug (Iteration 5779) ===
Q mean: -9.958353
Q std: 13.898282
Actor loss: 9.962304
Action reg: 0.003951
  l1.weight: grad_norm = 0.234214
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.194067
Total gradient norm: 0.580215
=== Actor Training Debug (Iteration 5780) ===
Q mean: -10.945324
Q std: 13.908041
Actor loss: 10.949277
Action reg: 0.003953
  l1.weight: grad_norm = 0.198862
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.153290
Total gradient norm: 0.514871
=== Actor Training Debug (Iteration 5781) ===
Q mean: -8.782570
Q std: 14.324006
Actor loss: 8.786514
Action reg: 0.003944
  l1.weight: grad_norm = 0.136749
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.116214
Total gradient norm: 0.318926
=== Actor Training Debug (Iteration 5782) ===
Q mean: -10.185286
Q std: 15.701009
Actor loss: 10.189240
Action reg: 0.003955
  l1.weight: grad_norm = 0.161690
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.139940
Total gradient norm: 0.355865
=== Actor Training Debug (Iteration 5783) ===
Q mean: -11.151615
Q std: 14.765556
Actor loss: 11.155579
Action reg: 0.003963
  l1.weight: grad_norm = 0.205885
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.168442
Total gradient norm: 0.475515
=== Actor Training Debug (Iteration 5784) ===
Q mean: -10.598459
Q std: 16.051411
Actor loss: 10.602440
Action reg: 0.003981
  l1.weight: grad_norm = 0.134482
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.116082
Total gradient norm: 0.332852
=== Actor Training Debug (Iteration 5785) ===
Q mean: -11.791358
Q std: 16.137657
Actor loss: 11.795316
Action reg: 0.003957
  l1.weight: grad_norm = 0.129737
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.117111
Total gradient norm: 0.298689
=== Actor Training Debug (Iteration 5786) ===
Q mean: -11.343629
Q std: 16.904514
Actor loss: 11.347571
Action reg: 0.003943
  l1.weight: grad_norm = 0.350745
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.247500
Total gradient norm: 0.723819
=== Actor Training Debug (Iteration 5787) ===
Q mean: -11.298003
Q std: 16.609726
Actor loss: 11.301965
Action reg: 0.003961
  l1.weight: grad_norm = 0.171325
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.138174
Total gradient norm: 0.365970
=== Actor Training Debug (Iteration 5788) ===
Q mean: -9.643600
Q std: 14.275880
Actor loss: 9.647569
Action reg: 0.003968
  l1.weight: grad_norm = 0.171019
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.154381
Total gradient norm: 0.408323
=== Actor Training Debug (Iteration 5789) ===
Q mean: -12.097292
Q std: 15.827401
Actor loss: 12.101220
Action reg: 0.003928
  l1.weight: grad_norm = 0.255772
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.256790
Total gradient norm: 0.724019
=== Actor Training Debug (Iteration 5790) ===
Q mean: -10.940083
Q std: 15.113602
Actor loss: 10.944044
Action reg: 0.003962
  l1.weight: grad_norm = 0.192494
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.175244
Total gradient norm: 0.467943
=== Actor Training Debug (Iteration 5791) ===
Q mean: -10.588606
Q std: 16.389978
Actor loss: 10.592567
Action reg: 0.003961
  l1.weight: grad_norm = 0.256841
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.230650
Total gradient norm: 0.604616
=== Actor Training Debug (Iteration 5792) ===
Q mean: -11.604490
Q std: 17.292019
Actor loss: 11.608453
Action reg: 0.003962
  l1.weight: grad_norm = 0.246301
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.183928
Total gradient norm: 0.551067
=== Actor Training Debug (Iteration 5793) ===
Q mean: -12.913237
Q std: 17.528259
Actor loss: 12.917197
Action reg: 0.003960
  l1.weight: grad_norm = 0.122196
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.115185
Total gradient norm: 0.293435
=== Actor Training Debug (Iteration 5794) ===
Q mean: -9.807224
Q std: 14.475244
Actor loss: 9.811178
Action reg: 0.003954
  l1.weight: grad_norm = 0.128974
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.115832
Total gradient norm: 0.294691
=== Actor Training Debug (Iteration 5795) ===
Q mean: -11.632160
Q std: 15.686082
Actor loss: 11.636124
Action reg: 0.003963
  l1.weight: grad_norm = 0.227793
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.229527
Total gradient norm: 0.684471
=== Actor Training Debug (Iteration 5796) ===
Q mean: -10.929821
Q std: 15.497464
Actor loss: 10.933764
Action reg: 0.003943
  l1.weight: grad_norm = 0.323920
  l1.bias: grad_norm = 0.000928
  l2.weight: grad_norm = 0.275130
Total gradient norm: 0.794653
=== Actor Training Debug (Iteration 5797) ===
Q mean: -12.413555
Q std: 15.662902
Actor loss: 12.417522
Action reg: 0.003967
  l1.weight: grad_norm = 0.080123
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.076135
Total gradient norm: 0.211670
=== Actor Training Debug (Iteration 5798) ===
Q mean: -11.707096
Q std: 15.964482
Actor loss: 11.711061
Action reg: 0.003965
  l1.weight: grad_norm = 0.406744
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.358726
Total gradient norm: 0.970100
=== Actor Training Debug (Iteration 5799) ===
Q mean: -10.534142
Q std: 14.495590
Actor loss: 10.538077
Action reg: 0.003935
  l1.weight: grad_norm = 0.264656
  l1.bias: grad_norm = 0.000855
  l2.weight: grad_norm = 0.224239
Total gradient norm: 0.670769
=== Actor Training Debug (Iteration 5800) ===
Q mean: -10.388693
Q std: 14.012270
Actor loss: 10.392657
Action reg: 0.003964
  l1.weight: grad_norm = 0.245623
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.195770
Total gradient norm: 0.506838
=== Actor Training Debug (Iteration 5801) ===
Q mean: -11.194687
Q std: 15.768186
Actor loss: 11.198640
Action reg: 0.003953
  l1.weight: grad_norm = 0.270889
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.231316
Total gradient norm: 0.730792
=== Actor Training Debug (Iteration 5802) ===
Q mean: -11.573017
Q std: 16.885916
Actor loss: 11.576962
Action reg: 0.003945
  l1.weight: grad_norm = 0.232386
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.179578
Total gradient norm: 0.510822
=== Actor Training Debug (Iteration 5803) ===
Q mean: -9.976537
Q std: 15.580941
Actor loss: 9.980499
Action reg: 0.003963
  l1.weight: grad_norm = 0.140917
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.116295
Total gradient norm: 0.308474
=== Actor Training Debug (Iteration 5804) ===
Q mean: -11.695377
Q std: 17.354067
Actor loss: 11.699325
Action reg: 0.003947
  l1.weight: grad_norm = 0.133716
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.126361
Total gradient norm: 0.344683
=== Actor Training Debug (Iteration 5805) ===
Q mean: -11.999142
Q std: 16.626791
Actor loss: 12.003113
Action reg: 0.003971
  l1.weight: grad_norm = 0.203501
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.186749
Total gradient norm: 0.489809
=== Actor Training Debug (Iteration 5806) ===
Q mean: -10.622271
Q std: 16.138487
Actor loss: 10.626216
Action reg: 0.003946
  l1.weight: grad_norm = 0.392945
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.320566
Total gradient norm: 0.788081
=== Actor Training Debug (Iteration 5807) ===
Q mean: -10.623756
Q std: 15.663530
Actor loss: 10.627704
Action reg: 0.003948
  l1.weight: grad_norm = 0.247196
  l1.bias: grad_norm = 0.000574
  l2.weight: grad_norm = 0.211101
Total gradient norm: 0.560455
=== Actor Training Debug (Iteration 5808) ===
Q mean: -11.409432
Q std: 15.976972
Actor loss: 11.413393
Action reg: 0.003960
  l1.weight: grad_norm = 0.165560
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.145499
Total gradient norm: 0.424315
=== Actor Training Debug (Iteration 5809) ===
Q mean: -11.552919
Q std: 16.434736
Actor loss: 11.556864
Action reg: 0.003944
  l1.weight: grad_norm = 0.229607
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.204101
Total gradient norm: 0.690498
=== Actor Training Debug (Iteration 5810) ===
Q mean: -11.174337
Q std: 16.428120
Actor loss: 11.178298
Action reg: 0.003960
  l1.weight: grad_norm = 0.334157
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.305700
Total gradient norm: 1.100952
=== Actor Training Debug (Iteration 5811) ===
Q mean: -11.201775
Q std: 14.329977
Actor loss: 11.205729
Action reg: 0.003955
  l1.weight: grad_norm = 0.258810
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.195899
Total gradient norm: 0.566672
=== Actor Training Debug (Iteration 5812) ===
Q mean: -10.346584
Q std: 13.845419
Actor loss: 10.350548
Action reg: 0.003964
  l1.weight: grad_norm = 0.189824
  l1.bias: grad_norm = 0.000544
  l2.weight: grad_norm = 0.181553
Total gradient norm: 0.491145
=== Actor Training Debug (Iteration 5813) ===
Q mean: -10.796415
Q std: 15.551111
Actor loss: 10.800373
Action reg: 0.003958
  l1.weight: grad_norm = 0.187573
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.172972
Total gradient norm: 0.481385
=== Actor Training Debug (Iteration 5814) ===
Q mean: -11.910740
Q std: 15.877777
Actor loss: 11.914708
Action reg: 0.003968
  l1.weight: grad_norm = 0.106220
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.095366
Total gradient norm: 0.280829
=== Actor Training Debug (Iteration 5815) ===
Q mean: -13.528608
Q std: 17.484598
Actor loss: 13.532579
Action reg: 0.003971
  l1.weight: grad_norm = 0.120297
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.104045
Total gradient norm: 0.277306
=== Actor Training Debug (Iteration 5816) ===
Q mean: -11.005993
Q std: 14.572721
Actor loss: 11.009927
Action reg: 0.003934
  l1.weight: grad_norm = 0.269976
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.264831
Total gradient norm: 0.809162
=== Actor Training Debug (Iteration 5817) ===
Q mean: -11.274324
Q std: 15.868091
Actor loss: 11.278283
Action reg: 0.003959
  l1.weight: grad_norm = 0.178528
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.163191
Total gradient norm: 0.442090
=== Actor Training Debug (Iteration 5818) ===
Q mean: -10.400698
Q std: 16.793510
Actor loss: 10.404641
Action reg: 0.003943
  l1.weight: grad_norm = 0.249576
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.202262
Total gradient norm: 0.554108
=== Actor Training Debug (Iteration 5819) ===
Q mean: -10.431114
Q std: 15.182539
Actor loss: 10.435070
Action reg: 0.003956
  l1.weight: grad_norm = 0.233661
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.181145
Total gradient norm: 0.556158
=== Actor Training Debug (Iteration 5820) ===
Q mean: -10.600231
Q std: 16.252079
Actor loss: 10.604186
Action reg: 0.003955
  l1.weight: grad_norm = 0.413727
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.332052
Total gradient norm: 0.840864
=== Actor Training Debug (Iteration 5821) ===
Q mean: -10.576231
Q std: 15.386552
Actor loss: 10.580209
Action reg: 0.003978
  l1.weight: grad_norm = 0.150149
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.131634
Total gradient norm: 0.359145
=== Actor Training Debug (Iteration 5822) ===
Q mean: -11.173027
Q std: 15.161908
Actor loss: 11.176990
Action reg: 0.003962
  l1.weight: grad_norm = 0.203677
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.175174
Total gradient norm: 0.490352
=== Actor Training Debug (Iteration 5823) ===
Q mean: -10.296572
Q std: 14.945816
Actor loss: 10.300533
Action reg: 0.003962
  l1.weight: grad_norm = 0.300711
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.248804
Total gradient norm: 0.666944
=== Actor Training Debug (Iteration 5824) ===
Q mean: -12.271510
Q std: 15.559537
Actor loss: 12.275466
Action reg: 0.003956
  l1.weight: grad_norm = 0.257941
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.225144
Total gradient norm: 0.617760
=== Actor Training Debug (Iteration 5825) ===
Q mean: -9.644329
Q std: 14.018950
Actor loss: 9.648292
Action reg: 0.003963
  l1.weight: grad_norm = 0.176881
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.145375
Total gradient norm: 0.362069
=== Actor Training Debug (Iteration 5826) ===
Q mean: -11.629499
Q std: 15.762477
Actor loss: 11.633451
Action reg: 0.003952
  l1.weight: grad_norm = 0.200404
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.165248
Total gradient norm: 0.453441
=== Actor Training Debug (Iteration 5827) ===
Q mean: -11.288349
Q std: 16.696449
Actor loss: 11.292299
Action reg: 0.003950
  l1.weight: grad_norm = 0.188099
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.161574
Total gradient norm: 0.481562
=== Actor Training Debug (Iteration 5828) ===
Q mean: -12.887909
Q std: 16.408262
Actor loss: 12.891870
Action reg: 0.003962
  l1.weight: grad_norm = 0.151211
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.137707
Total gradient norm: 0.380188
=== Actor Training Debug (Iteration 5829) ===
Q mean: -10.548451
Q std: 16.350750
Actor loss: 10.552416
Action reg: 0.003965
  l1.weight: grad_norm = 0.259762
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.215755
Total gradient norm: 0.571753
=== Actor Training Debug (Iteration 5830) ===
Q mean: -12.853261
Q std: 17.698387
Actor loss: 12.857227
Action reg: 0.003966
  l1.weight: grad_norm = 0.165470
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.125723
Total gradient norm: 0.359232
=== Actor Training Debug (Iteration 5831) ===
Q mean: -11.387132
Q std: 15.973451
Actor loss: 11.391094
Action reg: 0.003963
  l1.weight: grad_norm = 0.226706
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.208114
Total gradient norm: 0.534684
=== Actor Training Debug (Iteration 5832) ===
Q mean: -10.713386
Q std: 16.189358
Actor loss: 10.717337
Action reg: 0.003951
  l1.weight: grad_norm = 0.264094
  l1.bias: grad_norm = 0.001262
  l2.weight: grad_norm = 0.214833
Total gradient norm: 0.528051
=== Actor Training Debug (Iteration 5833) ===
Q mean: -10.758492
Q std: 14.815121
Actor loss: 10.762453
Action reg: 0.003962
  l1.weight: grad_norm = 0.176457
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.150720
Total gradient norm: 0.431167
=== Actor Training Debug (Iteration 5834) ===
Q mean: -12.859056
Q std: 17.279999
Actor loss: 12.863015
Action reg: 0.003959
  l1.weight: grad_norm = 0.266295
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.239319
Total gradient norm: 0.604614
=== Actor Training Debug (Iteration 5835) ===
Q mean: -10.352448
Q std: 14.697057
Actor loss: 10.356400
Action reg: 0.003953
  l1.weight: grad_norm = 0.273463
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.239358
Total gradient norm: 0.606021
=== Actor Training Debug (Iteration 5836) ===
Q mean: -11.460455
Q std: 16.027311
Actor loss: 11.464411
Action reg: 0.003956
  l1.weight: grad_norm = 0.279681
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.220293
Total gradient norm: 0.635615
=== Actor Training Debug (Iteration 5837) ===
Q mean: -11.344019
Q std: 14.775823
Actor loss: 11.347979
Action reg: 0.003960
  l1.weight: grad_norm = 0.129495
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.122682
Total gradient norm: 0.325873
=== Actor Training Debug (Iteration 5838) ===
Q mean: -12.295006
Q std: 16.172997
Actor loss: 12.298981
Action reg: 0.003975
  l1.weight: grad_norm = 0.114855
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.091563
Total gradient norm: 0.266316
=== Actor Training Debug (Iteration 5839) ===
Q mean: -11.322174
Q std: 15.905528
Actor loss: 11.326149
Action reg: 0.003975
  l1.weight: grad_norm = 0.176408
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.165281
Total gradient norm: 0.462054
=== Actor Training Debug (Iteration 5840) ===
Q mean: -12.434408
Q std: 15.758428
Actor loss: 12.438377
Action reg: 0.003969
  l1.weight: grad_norm = 0.089472
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.080597
Total gradient norm: 0.255294
=== Actor Training Debug (Iteration 5841) ===
Q mean: -12.099545
Q std: 17.170031
Actor loss: 12.103512
Action reg: 0.003967
  l1.weight: grad_norm = 0.335647
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.278914
Total gradient norm: 0.683636
=== Actor Training Debug (Iteration 5842) ===
Q mean: -11.113173
Q std: 16.181332
Actor loss: 11.117143
Action reg: 0.003970
  l1.weight: grad_norm = 0.154721
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.140635
Total gradient norm: 0.373511
=== Actor Training Debug (Iteration 5843) ===
Q mean: -11.839150
Q std: 16.238691
Actor loss: 11.843082
Action reg: 0.003932
  l1.weight: grad_norm = 0.209722
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.186085
Total gradient norm: 0.507615
=== Actor Training Debug (Iteration 5844) ===
Q mean: -12.036575
Q std: 15.860902
Actor loss: 12.040545
Action reg: 0.003969
  l1.weight: grad_norm = 0.085899
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.068677
Total gradient norm: 0.231917
=== Actor Training Debug (Iteration 5845) ===
Q mean: -8.932491
Q std: 13.695054
Actor loss: 8.936443
Action reg: 0.003952
  l1.weight: grad_norm = 0.168797
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.157733
Total gradient norm: 0.462267
=== Actor Training Debug (Iteration 5846) ===
Q mean: -10.855570
Q std: 16.278582
Actor loss: 10.859544
Action reg: 0.003974
  l1.weight: grad_norm = 0.247564
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.187251
Total gradient norm: 0.491258
=== Actor Training Debug (Iteration 5847) ===
Q mean: -9.753216
Q std: 14.595999
Actor loss: 9.757171
Action reg: 0.003954
  l1.weight: grad_norm = 0.217550
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.178635
Total gradient norm: 0.456197
=== Actor Training Debug (Iteration 5848) ===
Q mean: -12.401117
Q std: 17.477299
Actor loss: 12.405086
Action reg: 0.003968
  l1.weight: grad_norm = 0.256210
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.197887
Total gradient norm: 0.512542
=== Actor Training Debug (Iteration 5849) ===
Q mean: -12.527972
Q std: 16.306360
Actor loss: 12.531917
Action reg: 0.003945
  l1.weight: grad_norm = 0.306657
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.224655
Total gradient norm: 0.660833
=== Actor Training Debug (Iteration 5850) ===
Q mean: -11.722193
Q std: 16.003357
Actor loss: 11.726158
Action reg: 0.003966
  l1.weight: grad_norm = 0.164428
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.144320
Total gradient norm: 0.374426
=== Actor Training Debug (Iteration 5851) ===
Q mean: -12.038843
Q std: 15.157208
Actor loss: 12.042802
Action reg: 0.003958
  l1.weight: grad_norm = 0.250481
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.210590
Total gradient norm: 0.555850
=== Actor Training Debug (Iteration 5852) ===
Q mean: -10.786642
Q std: 15.033886
Actor loss: 10.790604
Action reg: 0.003962
  l1.weight: grad_norm = 0.500740
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.389071
Total gradient norm: 0.984756
=== Actor Training Debug (Iteration 5853) ===
Q mean: -12.345347
Q std: 16.485907
Actor loss: 12.349318
Action reg: 0.003970
  l1.weight: grad_norm = 0.155964
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.115791
Total gradient norm: 0.307192
=== Actor Training Debug (Iteration 5854) ===
Q mean: -9.166903
Q std: 15.344890
Actor loss: 9.170869
Action reg: 0.003966
  l1.weight: grad_norm = 0.286320
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.226090
Total gradient norm: 0.582359
=== Actor Training Debug (Iteration 5855) ===
Q mean: -10.506635
Q std: 15.913333
Actor loss: 10.510595
Action reg: 0.003961
  l1.weight: grad_norm = 0.134674
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.113330
Total gradient norm: 0.343599
=== Actor Training Debug (Iteration 5856) ===
Q mean: -11.982174
Q std: 16.401791
Actor loss: 11.986123
Action reg: 0.003949
  l1.weight: grad_norm = 0.175320
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.141676
Total gradient norm: 0.428960
=== Actor Training Debug (Iteration 5857) ===
Q mean: -11.913475
Q std: 17.062185
Actor loss: 11.917424
Action reg: 0.003949
  l1.weight: grad_norm = 0.065855
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.056091
Total gradient norm: 0.155321
=== Actor Training Debug (Iteration 5858) ===
Q mean: -11.545469
Q std: 15.969336
Actor loss: 11.549438
Action reg: 0.003968
  l1.weight: grad_norm = 0.170662
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.153880
Total gradient norm: 0.443332
=== Actor Training Debug (Iteration 5859) ===
Q mean: -10.980448
Q std: 15.003959
Actor loss: 10.984414
Action reg: 0.003967
  l1.weight: grad_norm = 0.184853
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.166984
Total gradient norm: 0.437803
=== Actor Training Debug (Iteration 5860) ===
Q mean: -10.124850
Q std: 16.271938
Actor loss: 10.128805
Action reg: 0.003955
  l1.weight: grad_norm = 0.252680
  l1.bias: grad_norm = 0.000516
  l2.weight: grad_norm = 0.215085
Total gradient norm: 0.524667
=== Actor Training Debug (Iteration 5861) ===
Q mean: -11.061320
Q std: 15.329750
Actor loss: 11.065270
Action reg: 0.003950
  l1.weight: grad_norm = 0.173224
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.141722
Total gradient norm: 0.404417
=== Actor Training Debug (Iteration 5862) ===
Q mean: -10.334058
Q std: 15.232659
Actor loss: 10.338036
Action reg: 0.003978
  l1.weight: grad_norm = 0.180211
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.161468
Total gradient norm: 0.473853
=== Actor Training Debug (Iteration 5863) ===
Q mean: -10.585360
Q std: 13.968632
Actor loss: 10.589328
Action reg: 0.003968
  l1.weight: grad_norm = 0.131207
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.128131
Total gradient norm: 0.336911
=== Actor Training Debug (Iteration 5864) ===
Q mean: -9.084633
Q std: 14.447042
Actor loss: 9.088584
Action reg: 0.003951
  l1.weight: grad_norm = 0.213104
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.192763
Total gradient norm: 0.498684
=== Actor Training Debug (Iteration 5865) ===
Q mean: -10.519682
Q std: 14.181277
Actor loss: 10.523645
Action reg: 0.003964
  l1.weight: grad_norm = 0.116012
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.101680
Total gradient norm: 0.287270
=== Actor Training Debug (Iteration 5866) ===
Q mean: -11.682045
Q std: 16.044909
Actor loss: 11.686003
Action reg: 0.003957
  l1.weight: grad_norm = 0.241191
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.227140
Total gradient norm: 0.709721
=== Actor Training Debug (Iteration 5867) ===
Q mean: -12.510391
Q std: 16.291943
Actor loss: 12.514355
Action reg: 0.003964
  l1.weight: grad_norm = 0.087026
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.073945
Total gradient norm: 0.221601
=== Actor Training Debug (Iteration 5868) ===
Q mean: -12.000752
Q std: 16.885319
Actor loss: 12.004719
Action reg: 0.003966
  l1.weight: grad_norm = 0.256011
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.219035
Total gradient norm: 0.589179
=== Actor Training Debug (Iteration 5869) ===
Q mean: -10.597389
Q std: 15.841610
Actor loss: 10.601339
Action reg: 0.003950
  l1.weight: grad_norm = 0.153150
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.134224
Total gradient norm: 0.357099
=== Actor Training Debug (Iteration 5870) ===
Q mean: -11.411960
Q std: 15.908134
Actor loss: 11.415927
Action reg: 0.003968
  l1.weight: grad_norm = 0.204928
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.194439
Total gradient norm: 0.569255
=== Actor Training Debug (Iteration 5871) ===
Q mean: -11.652756
Q std: 15.067065
Actor loss: 11.656718
Action reg: 0.003963
  l1.weight: grad_norm = 0.240981
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.205343
Total gradient norm: 0.563897
=== Actor Training Debug (Iteration 5872) ===
Q mean: -11.092787
Q std: 16.537819
Actor loss: 11.096736
Action reg: 0.003949
  l1.weight: grad_norm = 0.266419
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.249560
Total gradient norm: 0.780403
=== Actor Training Debug (Iteration 5873) ===
Q mean: -11.487236
Q std: 16.230320
Actor loss: 11.491212
Action reg: 0.003976
  l1.weight: grad_norm = 0.119389
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.111160
Total gradient norm: 0.290926
=== Actor Training Debug (Iteration 5874) ===
Q mean: -11.783381
Q std: 17.024815
Actor loss: 11.787346
Action reg: 0.003966
  l1.weight: grad_norm = 0.224826
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.206919
Total gradient norm: 0.725753
=== Actor Training Debug (Iteration 5875) ===
Q mean: -11.542517
Q std: 14.908525
Actor loss: 11.546477
Action reg: 0.003960
  l1.weight: grad_norm = 0.141522
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.119017
Total gradient norm: 0.328283
=== Actor Training Debug (Iteration 5876) ===
Q mean: -10.811765
Q std: 15.433386
Actor loss: 10.815732
Action reg: 0.003967
  l1.weight: grad_norm = 0.176988
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.161643
Total gradient norm: 0.410247
=== Actor Training Debug (Iteration 5877) ===
Q mean: -11.038690
Q std: 14.947175
Actor loss: 11.042649
Action reg: 0.003960
  l1.weight: grad_norm = 0.120204
  l1.bias: grad_norm = 0.000067
  l2.weight: grad_norm = 0.100277
Total gradient norm: 0.287208
=== Actor Training Debug (Iteration 5878) ===
Q mean: -9.382914
Q std: 12.265610
Actor loss: 9.386887
Action reg: 0.003973
  l1.weight: grad_norm = 0.170976
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.131611
Total gradient norm: 0.350439
=== Actor Training Debug (Iteration 5879) ===
Q mean: -10.045757
Q std: 15.170426
Actor loss: 10.049717
Action reg: 0.003959
  l1.weight: grad_norm = 0.130845
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.115826
Total gradient norm: 0.327352
=== Actor Training Debug (Iteration 5880) ===
Q mean: -10.207438
Q std: 14.735259
Actor loss: 10.211406
Action reg: 0.003968
  l1.weight: grad_norm = 0.084088
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.077347
Total gradient norm: 0.233072
=== Actor Training Debug (Iteration 5881) ===
Q mean: -11.627087
Q std: 15.347590
Actor loss: 11.631061
Action reg: 0.003974
  l1.weight: grad_norm = 0.057795
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.049981
Total gradient norm: 0.131844
=== Actor Training Debug (Iteration 5882) ===
Q mean: -12.170373
Q std: 16.407326
Actor loss: 12.174333
Action reg: 0.003959
  l1.weight: grad_norm = 0.188629
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.188451
Total gradient norm: 0.513566
=== Actor Training Debug (Iteration 5883) ===
Q mean: -12.406513
Q std: 15.854628
Actor loss: 12.410482
Action reg: 0.003969
  l1.weight: grad_norm = 0.193477
  l1.bias: grad_norm = 0.000079
  l2.weight: grad_norm = 0.158212
Total gradient norm: 0.467582
=== Actor Training Debug (Iteration 5884) ===
Q mean: -12.763247
Q std: 17.368515
Actor loss: 12.767214
Action reg: 0.003967
  l1.weight: grad_norm = 0.155448
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.139789
Total gradient norm: 0.374155
=== Actor Training Debug (Iteration 5885) ===
Q mean: -9.329994
Q std: 13.979868
Actor loss: 9.333923
Action reg: 0.003929
  l1.weight: grad_norm = 0.243187
  l1.bias: grad_norm = 0.000529
  l2.weight: grad_norm = 0.219267
Total gradient norm: 0.586503
=== Actor Training Debug (Iteration 5886) ===
Q mean: -9.438114
Q std: 12.986772
Actor loss: 9.442062
Action reg: 0.003948
  l1.weight: grad_norm = 0.247248
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.193555
Total gradient norm: 0.526066
=== Actor Training Debug (Iteration 5887) ===
Q mean: -10.692069
Q std: 16.034470
Actor loss: 10.696024
Action reg: 0.003955
  l1.weight: grad_norm = 0.117362
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.106489
Total gradient norm: 0.281027
=== Actor Training Debug (Iteration 5888) ===
Q mean: -11.854192
  l1.weight: grad_norm = 0.255398
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.211596
Total gradient norm: 0.521116
=== Actor Training Debug (Iteration 5909) ===
Q mean: -12.482878
Q std: 17.109411
Actor loss: 12.486847
Action reg: 0.003970
  l1.weight: grad_norm = 0.066644
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.061863
Total gradient norm: 0.167910
=== Actor Training Debug (Iteration 5910) ===
Q mean: -12.120758
Q std: 16.696236
Actor loss: 12.124715
Action reg: 0.003956
  l1.weight: grad_norm = 0.153248
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.128899
Total gradient norm: 0.421674
=== Actor Training Debug (Iteration 5911) ===
Q mean: -12.996571
Q std: 17.553926
Actor loss: 13.000546
Action reg: 0.003976
  l1.weight: grad_norm = 0.125307
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.116494
Total gradient norm: 0.333201
=== Actor Training Debug (Iteration 5912) ===
Q mean: -11.885348
Q std: 15.772686
Actor loss: 11.889306
Action reg: 0.003958
  l1.weight: grad_norm = 0.241262
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.230342
Total gradient norm: 0.631347
=== Actor Training Debug (Iteration 5913) ===
Q mean: -12.935905
Q std: 16.298111
Actor loss: 12.939867
Action reg: 0.003962
  l1.weight: grad_norm = 0.187544
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.167656
Total gradient norm: 0.437185
=== Actor Training Debug (Iteration 5914) ===
Q mean: -9.619921
Q std: 13.142418
Actor loss: 9.623884
Action reg: 0.003963
  l1.weight: grad_norm = 0.242393
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.222508
Total gradient norm: 0.661351
=== Actor Training Debug (Iteration 5915) ===
Q mean: -11.971476
Q std: 17.163942
Actor loss: 11.975442
Action reg: 0.003967
  l1.weight: grad_norm = 0.189836
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.157889
Total gradient norm: 0.423616
=== Actor Training Debug (Iteration 5916) ===
Q mean: -10.878411
Q std: 15.141550
Actor loss: 10.882386
Action reg: 0.003975
  l1.weight: grad_norm = 0.208064
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.178883
Total gradient norm: 0.422649
=== Actor Training Debug (Iteration 5917) ===
Q mean: -11.041576
Q std: 16.844389
Actor loss: 11.045542
Action reg: 0.003966
  l1.weight: grad_norm = 0.223052
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.189618
Total gradient norm: 0.537750
=== Actor Training Debug (Iteration 5918) ===
Q mean: -11.818900
Q std: 15.180384
Actor loss: 11.822861
Action reg: 0.003961
  l1.weight: grad_norm = 0.186016
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.154420
Total gradient norm: 0.419814
=== Actor Training Debug (Iteration 5919) ===
Q mean: -11.646889
Q std: 15.610203
Actor loss: 11.650839
Action reg: 0.003950
  l1.weight: grad_norm = 0.161872
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.137654
Total gradient norm: 0.369777
=== Actor Training Debug (Iteration 5920) ===
Q mean: -11.179712
Q std: 15.357687
Actor loss: 11.183681
Action reg: 0.003969
  l1.weight: grad_norm = 0.138512
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.114344
Total gradient norm: 0.304594
=== Actor Training Debug (Iteration 5921) ===
Q mean: -11.101283
Q std: 14.366003
Actor loss: 11.105234
Action reg: 0.003951
  l1.weight: grad_norm = 0.214931
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.170703
Total gradient norm: 0.420882
=== Actor Training Debug (Iteration 5922) ===
Q mean: -10.086688
Q std: 15.932186
Actor loss: 10.090650
Action reg: 0.003962
  l1.weight: grad_norm = 0.157656
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.134756
Total gradient norm: 0.372817
=== Actor Training Debug (Iteration 5923) ===
Q mean: -11.675194
Q std: 15.573331
Actor loss: 11.679151
Action reg: 0.003957
  l1.weight: grad_norm = 0.170435
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.148410
Total gradient norm: 0.369997
=== Actor Training Debug (Iteration 5924) ===
Q mean: -10.787151
Q std: 14.419279
Actor loss: 10.791115
Action reg: 0.003963
  l1.weight: grad_norm = 0.267078
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.251998
Total gradient norm: 0.835032
=== Actor Training Debug (Iteration 5925) ===
Q mean: -11.740717
Q std: 16.409311
Actor loss: 11.744686
Action reg: 0.003969
  l1.weight: grad_norm = 0.297228
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.238282
Total gradient norm: 0.603679
=== Actor Training Debug (Iteration 5926) ===
Q mean: -11.075103
Q std: 14.857373
Actor loss: 11.079062
Action reg: 0.003959
  l1.weight: grad_norm = 0.246617
  l1.bias: grad_norm = 0.000595
  l2.weight: grad_norm = 0.213365
Total gradient norm: 0.577287
=== Actor Training Debug (Iteration 5927) ===
Q mean: -10.705463
Q std: 15.586753
Actor loss: 10.709430
Action reg: 0.003967
  l1.weight: grad_norm = 0.197526
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.162400
Total gradient norm: 0.485387
=== Actor Training Debug (Iteration 5928) ===
Q mean: -10.807448
Q std: 15.865428
Actor loss: 10.811419
Action reg: 0.003971
  l1.weight: grad_norm = 0.098257
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.086666
Total gradient norm: 0.236372
=== Actor Training Debug (Iteration 5929) ===
Q mean: -11.423334
Q std: 16.894506
Actor loss: 11.427300
Action reg: 0.003966
  l1.weight: grad_norm = 0.249556
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.234424
Total gradient norm: 0.713776
=== Actor Training Debug (Iteration 5930) ===
Q mean: -12.196497
Q std: 16.993765
Actor loss: 12.200447
Action reg: 0.003950
  l1.weight: grad_norm = 0.217191
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.191361
Total gradient norm: 0.628166
=== Actor Training Debug (Iteration 5931) ===
Q mean: -12.655193
Q std: 16.825560
Actor loss: 12.659160
Action reg: 0.003966
  l1.weight: grad_norm = 0.075710
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.065758
Total gradient norm: 0.197125
=== Actor Training Debug (Iteration 5932) ===
Q mean: -12.071329
Q std: 16.898586
Actor loss: 12.075303
Action reg: 0.003974
  l1.weight: grad_norm = 0.225811
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.197099
Total gradient norm: 0.632096
=== Actor Training Debug (Iteration 5933) ===
Q mean: -11.618963
Q std: 15.667321
Actor loss: 11.622933
Action reg: 0.003970
  l1.weight: grad_norm = 0.354679
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.299856
Total gradient norm: 0.852064
=== Actor Training Debug (Iteration 5934) ===
Q mean: -13.146866
Q std: 17.016209
Actor loss: 13.150810
Action reg: 0.003944
  l1.weight: grad_norm = 0.220904
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.181902
Total gradient norm: 0.510028
=== Actor Training Debug (Iteration 5935) ===
Q mean: -12.404617
Q std: 16.167841
Actor loss: 12.408577
Action reg: 0.003960
  l1.weight: grad_norm = 0.206859
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.190895
Total gradient norm: 0.489032
=== Actor Training Debug (Iteration 5936) ===
Q mean: -10.393752
Q std: 15.360078
Actor loss: 10.397722
Action reg: 0.003970
  l1.weight: grad_norm = 0.190648
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.180082
Total gradient norm: 0.529864
=== Actor Training Debug (Iteration 5937) ===
Q mean: -11.673265
Q std: 15.669435
Actor loss: 11.677225
Action reg: 0.003960
  l1.weight: grad_norm = 0.217579
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.181673
Total gradient norm: 0.509375
=== Actor Training Debug (Iteration 5938) ===
Q mean: -13.133844
Q std: 17.066360
Actor loss: 13.137786
Action reg: 0.003941
  l1.weight: grad_norm = 0.132425
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.112959
Total gradient norm: 0.290107
=== Actor Training Debug (Iteration 5939) ===
Q mean: -13.204865
Q std: 17.187069
Actor loss: 13.208838
Action reg: 0.003972
  l1.weight: grad_norm = 0.105746
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.096522
Total gradient norm: 0.287635
=== Actor Training Debug (Iteration 5940) ===
Q mean: -10.979322
Q std: 16.057381
Actor loss: 10.983279
Action reg: 0.003957
  l1.weight: grad_norm = 0.751183
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.592350
Total gradient norm: 1.450833
=== Actor Training Debug (Iteration 5941) ===
Q mean: -10.987811
Q std: 15.374926
Actor loss: 10.991752
Action reg: 0.003940
  l1.weight: grad_norm = 0.146031
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.130779
Total gradient norm: 0.420856
=== Actor Training Debug (Iteration 5942) ===
Q mean: -12.325943
Q std: 16.614935
Actor loss: 12.329922
Action reg: 0.003979
  l1.weight: grad_norm = 0.181419
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.133758
Total gradient norm: 0.365169
=== Actor Training Debug (Iteration 5943) ===
Q mean: -11.589878
Q std: 14.586141
Actor loss: 11.593827
Action reg: 0.003949
  l1.weight: grad_norm = 0.136910
  l1.bias: grad_norm = 0.000428
  l2.weight: grad_norm = 0.104996
Total gradient norm: 0.289478
=== Actor Training Debug (Iteration 5944) ===
Q mean: -10.796593
Q std: 14.591343
Actor loss: 10.800551
Action reg: 0.003958
  l1.weight: grad_norm = 0.129545
  l1.bias: grad_norm = 0.000561
  l2.weight: grad_norm = 0.110810
Total gradient norm: 0.293370
=== Actor Training Debug (Iteration 5945) ===
Q mean: -10.554263
Q std: 14.649637
Actor loss: 10.558218
Action reg: 0.003955
  l1.weight: grad_norm = 0.250148
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.219348
Total gradient norm: 0.639365
=== Actor Training Debug (Iteration 5946) ===
Q mean: -10.920216
Q std: 14.842096
Actor loss: 10.924191
Action reg: 0.003975
  l1.weight: grad_norm = 0.108111
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.095064
Total gradient norm: 0.270967
=== Actor Training Debug (Iteration 5947) ===
Q mean: -11.555219
Q std: 16.202826
Actor loss: 11.559184
Action reg: 0.003966
  l1.weight: grad_norm = 0.100484
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.087157
Total gradient norm: 0.230535
=== Actor Training Debug (Iteration 5948) ===
Q mean: -10.270553
Q std: 14.451544
Actor loss: 10.274498
Action reg: 0.003946
  l1.weight: grad_norm = 0.401277
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.300892
Total gradient norm: 0.996232
=== Actor Training Debug (Iteration 5949) ===
Q mean: -11.646345
Q std: 15.674776
Actor loss: 11.650312
Action reg: 0.003968
  l1.weight: grad_norm = 0.246819
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.220787
Total gradient norm: 0.632113
=== Actor Training Debug (Iteration 5950) ===
Q mean: -11.502960
Q std: 15.334871
Actor loss: 11.506920
Action reg: 0.003959
  l1.weight: grad_norm = 0.204667
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.163324
Total gradient norm: 0.492218
=== Actor Training Debug (Iteration 5951) ===
Q mean: -9.795576
Q std: 14.307262
Actor loss: 9.799555
Action reg: 0.003979
  l1.weight: grad_norm = 0.148374
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.133189
Total gradient norm: 0.344781
=== Actor Training Debug (Iteration 5952) ===
Q mean: -13.270199
Q std: 17.149952
Actor loss: 13.274173
Action reg: 0.003974
  l1.weight: grad_norm = 0.117200
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.105430
Total gradient norm: 0.266179
=== Actor Training Debug (Iteration 5953) ===
Q mean: -10.877783
Q std: 15.599973
Actor loss: 10.881740
Action reg: 0.003957
  l1.weight: grad_norm = 0.162948
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.143630
Total gradient norm: 0.354778
=== Actor Training Debug (Iteration 5954) ===
Q mean: -11.595106
Q std: 15.895433
Actor loss: 11.599066
Action reg: 0.003959
  l1.weight: grad_norm = 0.200872
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.193068
Total gradient norm: 0.542765
=== Actor Training Debug (Iteration 5955) ===
Q mean: -11.187692
Q std: 14.445197
Actor loss: 11.191662
Action reg: 0.003970
  l1.weight: grad_norm = 0.137783
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.126222
Total gradient norm: 0.333681
=== Actor Training Debug (Iteration 5956) ===
Q mean: -10.471909
Q std: 14.420168
Actor loss: 10.475883
Action reg: 0.003974
  l1.weight: grad_norm = 0.319970
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.268326
Total gradient norm: 0.743198
=== Actor Training Debug (Iteration 5957) ===
Q mean: -12.190348
Q std: 16.617546
Actor loss: 12.194310
Action reg: 0.003962
  l1.weight: grad_norm = 0.274990
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.205486
Total gradient norm: 0.645091
=== Actor Training Debug (Iteration 5958) ===
Q mean: -10.819864
Q std: 15.083759
Actor loss: 10.823833
Action reg: 0.003969
  l1.weight: grad_norm = 0.221758
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.194651
Total gradient norm: 0.612084
=== Actor Training Debug (Iteration 5959) ===
Q mean: -11.229786
Q std: 15.766551
Actor loss: 11.233744
Action reg: 0.003958
  l1.weight: grad_norm = 0.183468
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.156740
Total gradient norm: 0.423529
=== Actor Training Debug (Iteration 5960) ===
Q mean: -11.059006
Q std: 15.618262
Actor loss: 11.062964
Action reg: 0.003959
  l1.weight: grad_norm = 0.204351
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.153568
Total gradient norm: 0.395771
=== Actor Training Debug (Iteration 5961) ===
Q mean: -11.417993
Q std: 15.850801
Actor loss: 11.421949
Action reg: 0.003956
  l1.weight: grad_norm = 0.243483
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.190673
Total gradient norm: 0.542786
=== Actor Training Debug (Iteration 5962) ===
Q mean: -11.230912
Q std: 16.053947
Actor loss: 11.234880
Action reg: 0.003969
  l1.weight: grad_norm = 0.179105
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.137816
Total gradient norm: 0.383183
=== Actor Training Debug (Iteration 5963) ===
Q mean: -11.945675
Q std: 16.601728
Actor loss: 11.949653
Action reg: 0.003978
  l1.weight: grad_norm = 0.132919
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.119414
Total gradient norm: 0.371626
=== Actor Training Debug (Iteration 5964) ===
Q mean: -11.819605
Q std: 15.835756
Actor loss: 11.823531
Action reg: 0.003926
  l1.weight: grad_norm = 0.181992
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.164248
Total gradient norm: 0.457876
=== Actor Training Debug (Iteration 5965) ===
Q mean: -12.087674
Q std: 16.236641
Actor loss: 12.091613
Action reg: 0.003939
  l1.weight: grad_norm = 0.228585
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.202588
Total gradient norm: 0.579697
=== Actor Training Debug (Iteration 5966) ===
Q mean: -10.495930
Q std: 13.627836
Actor loss: 10.499882
Action reg: 0.003952
  l1.weight: grad_norm = 0.168918
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.126372
Total gradient norm: 0.336000
=== Actor Training Debug (Iteration 5967) ===
Q mean: -12.844261
Q std: 17.598742
Actor loss: 12.848226
Action reg: 0.003964
  l1.weight: grad_norm = 0.182402
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.153254
Total gradient norm: 0.436323
=== Actor Training Debug (Iteration 5968) ===
Q mean: -12.441595
Q std: 17.549467
Actor loss: 12.445559
Action reg: 0.003964
  l1.weight: grad_norm = 0.155017
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.130081
Total gradient norm: 0.369294
=== Actor Training Debug (Iteration 5969) ===
Q mean: -11.534573
Q std: 15.252481
Actor loss: 11.538522
Action reg: 0.003949
  l1.weight: grad_norm = 0.249364
  l1.bias: grad_norm = 0.000625
  l2.weight: grad_norm = 0.203076
Total gradient norm: 0.675333
=== Actor Training Debug (Iteration 5970) ===
Q mean: -11.099220
Q std: 15.165227
Actor loss: 11.103184
Action reg: 0.003964
  l1.weight: grad_norm = 0.292390
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.257526
Total gradient norm: 0.650757
=== Actor Training Debug (Iteration 5971) ===
Q mean: -8.604115
Q std: 14.498293
Actor loss: 8.608084
Action reg: 0.003969
  l1.weight: grad_norm = 0.275961
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.224912
Total gradient norm: 0.564882
=== Actor Training Debug (Iteration 5972) ===
Q mean: -12.506792
Q std: 15.264115
Actor loss: 12.510745
Action reg: 0.003953
  l1.weight: grad_norm = 0.387748
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.301392
Total gradient norm: 0.744350
=== Actor Training Debug (Iteration 5973) ===
Q mean: -12.206755
Q std: 16.597889
Actor loss: 12.210699
Action reg: 0.003945
  l1.weight: grad_norm = 0.191796
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.160298
Total gradient norm: 0.498912
=== Actor Training Debug (Iteration 5974) ===
Q mean: -12.181316
Q std: 18.141092
Actor loss: 12.185286
Action reg: 0.003969
  l1.weight: grad_norm = 0.175547
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.152289
Total gradient norm: 0.459811
=== Actor Training Debug (Iteration 5975) ===
Q mean: -12.041494
Q std: 16.370680
Actor loss: 12.045442
Action reg: 0.003948
  l1.weight: grad_norm = 0.318452
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.217410
Total gradient norm: 0.529772
=== Actor Training Debug (Iteration 5976) ===
Q mean: -10.303620
Q std: 15.697083
Actor loss: 10.307570
Action reg: 0.003950
  l1.weight: grad_norm = 0.136959
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.117346
Total gradient norm: 0.332439
=== Actor Training Debug (Iteration 5977) ===
Q mean: -9.591082
Q std: 14.412087
Actor loss: 9.595030
Action reg: 0.003948
  l1.weight: grad_norm = 0.380231
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.297524
Total gradient norm: 0.785653
=== Actor Training Debug (Iteration 5978) ===
Q mean: -11.901758
Q std: 15.473576
Actor loss: 11.905697
Action reg: 0.003939
  l1.weight: grad_norm = 0.161593
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.117320
Total gradient norm: 0.338892
=== Actor Training Debug (Iteration 5979) ===
Q mean: -11.819439
Q std: 16.034477
Actor loss: 11.823386
Action reg: 0.003947
  l1.weight: grad_norm = 0.319093
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.241866
Total gradient norm: 0.713753
=== Actor Training Debug (Iteration 5980) ===
Q mean: -11.366913
Q std: 15.232450
Actor loss: 11.370880
Action reg: 0.003968
  l1.weight: grad_norm = 0.167282
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.156744
Total gradient norm: 0.457896
=== Actor Training Debug (Iteration 5981) ===
Q mean: -11.123575
Q std: 15.703080
Actor loss: 11.127544
Action reg: 0.003969
  l1.weight: grad_norm = 0.135328
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.114808
Total gradient norm: 0.340179
=== Actor Training Debug (Iteration 5982) ===
Q mean: -12.685648
Q std: 17.454159
Actor loss: 12.689606
Action reg: 0.003958
  l1.weight: grad_norm = 0.186963
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.174076
Total gradient norm: 0.446127
=== Actor Training Debug (Iteration 5983) ===
Q mean: -10.966632
Q std: 16.001936
Actor loss: 10.970579
Action reg: 0.003947
  l1.weight: grad_norm = 0.235917
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.211273
Total gradient norm: 0.548744
=== Actor Training Debug (Iteration 5984) ===
Q mean: -11.776369
Q std: 16.101578
Actor loss: 11.780329
Action reg: 0.003959
  l1.weight: grad_norm = 0.193964
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.157314
Total gradient norm: 0.420503
=== Actor Training Debug (Iteration 5985) ===
Q mean: -11.205679
Q std: 15.963533
Actor loss: 11.209650
Action reg: 0.003972
  l1.weight: grad_norm = 0.209427
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.182804
Total gradient norm: 0.527259
=== Actor Training Debug (Iteration 5986) ===
Q mean: -9.983106
Q std: 15.340826
Actor loss: 9.987070
Action reg: 0.003964
  l1.weight: grad_norm = 0.204838
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.189533
Total gradient norm: 0.453508
=== Actor Training Debug (Iteration 5987) ===
Q mean: -11.429630
Q std: 16.585672
Actor loss: 11.433599
Action reg: 0.003969
  l1.weight: grad_norm = 0.245243
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.217151
Total gradient norm: 0.569413
=== Actor Training Debug (Iteration 5988) ===
Q mean: -10.782207
Q std: 15.298361
Actor loss: 10.786163
Action reg: 0.003956
  l1.weight: grad_norm = 0.237719
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.193603
Total gradient norm: 0.529134
=== Actor Training Debug (Iteration 5989) ===
Q mean: -11.023687
Q std: 16.990438
Actor loss: 11.027644
Action reg: 0.003957
  l1.weight: grad_norm = 0.181975
  l1.bias: grad_norm = 0.000530
  l2.weight: grad_norm = 0.171633
Total gradient norm: 0.425046
=== Actor Training Debug (Iteration 5990) ===
Q mean: -11.120031
Q std: 14.476038
Actor loss: 11.123989
Action reg: 0.003958
  l1.weight: grad_norm = 0.213178
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.178009
Total gradient norm: 0.540025
=== Actor Training Debug (Iteration 5991) ===
Q mean: -10.823314
Q std: 15.683304
Actor loss: 10.827262
Action reg: 0.003948
  l1.weight: grad_norm = 0.195502
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.164344
Total gradient norm: 0.425460
=== Actor Training Debug (Iteration 5992) ===
Q mean: -10.905074
Q std: 16.314445
Actor loss: 10.909025
Action reg: 0.003951
  l1.weight: grad_norm = 0.174939
  l1.bias: grad_norm = 0.000742
  l2.weight: grad_norm = 0.147453
Total gradient norm: 0.466492
=== Actor Training Debug (Iteration 5993) ===
Q mean: -11.002129
Q std: 14.114547
Actor loss: 11.006084
Action reg: 0.003956
  l1.weight: grad_norm = 0.182592
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.160051
Total gradient norm: 0.420549
=== Actor Training Debug (Iteration 5994) ===
Q mean: -9.949978
Q std: 14.794008
Actor loss: 9.953946
Action reg: 0.003969
  l1.weight: grad_norm = 0.152236
  l1.bias: grad_norm = 0.000040
  l2.weight: grad_norm = 0.150737
Total gradient norm: 0.366627
=== Actor Training Debug (Iteration 5995) ===
Q mean: -12.291442
Q std: 16.647375
Actor loss: 12.295421
Action reg: 0.003979
  l1.weight: grad_norm = 0.215591
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.196585
Total gradient norm: 0.539038
=== Actor Training Debug (Iteration 5996) ===
Q mean: -12.121394
Q std: 15.740346
Actor loss: 12.125361
Action reg: 0.003967
  l1.weight: grad_norm = 0.179448
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.162691
Total gradient norm: 0.424442
=== Actor Training Debug (Iteration 5997) ===
Q mean: -13.177816
Q std: 17.086401
Actor loss: 13.181778
Action reg: 0.003962
  l1.weight: grad_norm = 0.172782
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.141598
Total gradient norm: 0.348894
=== Actor Training Debug (Iteration 5998) ===
Q mean: -10.639870
Q std: 13.372570
Actor loss: 10.643814
Action reg: 0.003945
  l1.weight: grad_norm = 0.230775
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.200196
Total gradient norm: 0.539615
=== Actor Training Debug (Iteration 5999) ===
Q mean: -12.104922
Q std: 17.216228
Actor loss: 12.108900
Action reg: 0.003977
  l1.weight: grad_norm = 0.092747
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.084845
Total gradient norm: 0.237102
=== Actor Training Debug (Iteration 6000) ===
Q mean: -10.359566
Q std: 14.507035
Actor loss: 10.363518
Action reg: 0.003952
  l1.weight: grad_norm = 0.272994
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.232571
Total gradient norm: 0.597424
Step 11000: Critic Loss: 1.1162, Actor Loss: 10.3635, Q Value: -10.3596
  Average reward: -312.655 | Average length: 100.0
Evaluation at episode 110: -312.655
=== Actor Training Debug (Iteration 6001) ===
Q mean: -13.453107
Q std: 17.516932
Actor loss: 13.457078
Action reg: 0.003971
  l1.weight: grad_norm = 0.180379
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.146817
Total gradient norm: 0.360644
=== Actor Training Debug (Iteration 6002) ===
Q mean: -11.389198
Q std: 16.133938
Actor loss: 11.393171
Action reg: 0.003973
  l1.weight: grad_norm = 0.071964
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.064418
Total gradient norm: 0.180691
=== Actor Training Debug (Iteration 6003) ===
Q mean: -10.659216
Q std: 13.974283
Actor loss: 10.663158
Action reg: 0.003943
  l1.weight: grad_norm = 0.301285
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.268629
Total gradient norm: 0.728404
=== Actor Training Debug (Iteration 6004) ===
Q mean: -10.833096
Q std: 14.123136
Actor loss: 10.837051
Action reg: 0.003956
  l1.weight: grad_norm = 0.338203
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.317996
Total gradient norm: 0.870897
=== Actor Training Debug (Iteration 6005) ===
Q mean: -10.875846
Q std: 14.279398
Actor loss: 10.879798
Action reg: 0.003952
  l1.weight: grad_norm = 0.173984
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.158850
Total gradient norm: 0.439054
=== Actor Training Debug (Iteration 6006) ===
Q mean: -15.382111
Q std: 19.464901
Actor loss: 15.386072
Action reg: 0.003962
  l1.weight: grad_norm = 0.105954
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.087210
Total gradient norm: 0.246089
=== Actor Training Debug (Iteration 6007) ===
Q mean: -11.622458
Q std: 15.454555
Actor loss: 11.626426
Action reg: 0.003968
  l1.weight: grad_norm = 0.238347
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.206991
Total gradient norm: 0.559323
=== Actor Training Debug (Iteration 6008) ===
Q mean: -11.314837
Q std: 14.651840
Actor loss: 11.318804
Action reg: 0.003968
  l1.weight: grad_norm = 0.129725
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.107635
Total gradient norm: 0.257920
=== Actor Training Debug (Iteration 6009) ===
Q mean: -11.847462
Q std: 15.826194
Actor loss: 11.851423
Action reg: 0.003961
  l1.weight: grad_norm = 0.257427
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.233894
Total gradient norm: 0.620542
=== Actor Training Debug (Iteration 6010) ===
Q mean: -11.621625
Q std: 17.009129
Actor loss: 11.625604
Action reg: 0.003978
  l1.weight: grad_norm = 0.107397
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.088841
Total gradient norm: 0.235887
=== Actor Training Debug (Iteration 6011) ===
Q mean: -11.631448
Q std: 15.453860
Actor loss: 11.635416
Action reg: 0.003969
  l1.weight: grad_norm = 0.159642
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.144379
Total gradient norm: 0.379993
=== Actor Training Debug (Iteration 6012) ===
Q mean: -12.355559
Q std: 16.342928
Actor loss: 12.359522
Action reg: 0.003963
  l1.weight: grad_norm = 0.124769
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.124551
Total gradient norm: 0.365361
=== Actor Training Debug (Iteration 6013) ===
Q mean: -12.336230
Q std: 16.172934
Actor loss: 12.340186
Action reg: 0.003955
  l1.weight: grad_norm = 0.176576
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.160627
Total gradient norm: 0.479334
=== Actor Training Debug (Iteration 6014) ===
Q mean: -13.440140
Q std: 17.491911
Actor loss: 13.444111
Action reg: 0.003971
  l1.weight: grad_norm = 0.059871
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.056655
Total gradient norm: 0.167533
=== Actor Training Debug (Iteration 6015) ===
Q mean: -11.941788
Q std: 15.465525
Actor loss: 11.945751
Action reg: 0.003963
  l1.weight: grad_norm = 0.134798
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.115194
Total gradient norm: 0.325988
=== Actor Training Debug (Iteration 6016) ===
Q mean: -11.695018
Q std: 14.788473
Actor loss: 11.698971
Action reg: 0.003953
  l1.weight: grad_norm = 0.157134
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.159174
Total gradient norm: 0.444307
=== Actor Training Debug (Iteration 6017) ===
Q mean: -12.774899
Q std: 16.783939
Actor loss: 12.778844
Action reg: 0.003945
  l1.weight: grad_norm = 0.122472
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.102218
Total gradient norm: 0.327611
=== Actor Training Debug (Iteration 6018) ===
Q mean: -11.293306
Q std: 16.173042
Actor loss: 11.297264
Action reg: 0.003958
  l1.weight: grad_norm = 0.127844
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.119249
Total gradient norm: 0.318161
=== Actor Training Debug (Iteration 6019) ===
Q mean: -12.779499
Q std: 16.401148
Actor loss: 12.783453
Action reg: 0.003954
  l1.weight: grad_norm = 0.187751
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.172816
Total gradient norm: 0.500304
=== Actor Training Debug (Iteration 6020) ===
Q mean: -10.299379
Q std: 14.644219
Actor loss: 10.303335
Action reg: 0.003956
  l1.weight: grad_norm = 0.189851
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.153798
Total gradient norm: 0.378703
=== Actor Training Debug (Iteration 6021) ===
Q mean: -11.911141
Q std: 16.417795
Actor loss: 11.915111
Action reg: 0.003969
  l1.weight: grad_norm = 0.134790
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.116725
Total gradient norm: 0.379900
=== Actor Training Debug (Iteration 6022) ===
Q mean: -11.020454
Q std: 14.184699
Actor loss: 11.024402
Action reg: 0.003948
  l1.weight: grad_norm = 0.331195
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.273632
Total gradient norm: 0.704238
=== Actor Training Debug (Iteration 6023) ===
Q mean: -12.706164
Q std: 17.032307
Actor loss: 12.710117
Action reg: 0.003953
  l1.weight: grad_norm = 0.241227
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.217231
Total gradient norm: 0.643451
=== Actor Training Debug (Iteration 6024) ===
Q mean: -10.141424
Q std: 15.580033
Actor loss: 10.145382
Action reg: 0.003957
  l1.weight: grad_norm = 0.179101
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.148585
Total gradient norm: 0.385092
=== Actor Training Debug (Iteration 6025) ===
Q mean: -9.890341
Q std: 14.737675
Actor loss: 9.894302
Action reg: 0.003961
  l1.weight: grad_norm = 0.225381
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.209739
Total gradient norm: 0.557141
=== Actor Training Debug (Iteration 6026) ===
Q mean: -12.119751
Q std: 14.791389
Actor loss: 12.123708
Action reg: 0.003957
  l1.weight: grad_norm = 0.098702
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.077911
Total gradient norm: 0.234015
=== Actor Training Debug (Iteration 6027) ===
Q mean: -10.235576
Q std: 15.922928
Actor loss: 10.239520
Action reg: 0.003945
  l1.weight: grad_norm = 0.219647
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.173200
Total gradient norm: 0.413186
=== Actor Training Debug (Iteration 6028) ===
Q mean: -10.733601
Q std: 15.199967
Actor loss: 10.737568
Action reg: 0.003968
  l1.weight: grad_norm = 0.147222
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.125288
Total gradient norm: 0.327146
=== Actor Training Debug (Iteration 6029) ===
Q mean: -11.446978
Q std: 15.307099
Actor loss: 11.450922
Action reg: 0.003945
  l1.weight: grad_norm = 0.194188
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.160497
Total gradient norm: 0.402180
=== Actor Training Debug (Iteration 6030) ===
Q mean: -10.734625
Q std: 15.406637
Actor loss: 10.738588
Action reg: 0.003963
  l1.weight: grad_norm = 0.187551
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.165099
Total gradient norm: 0.408553
=== Actor Training Debug (Iteration 6031) ===
Q mean: -10.228664
Q std: 15.126334
Actor loss: 10.232628
Action reg: 0.003963
  l1.weight: grad_norm = 0.330432
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.303364
Total gradient norm: 0.741770
=== Actor Training Debug (Iteration 6032) ===
Q mean: -9.749371
Q std: 13.131391
Actor loss: 9.753307
Action reg: 0.003936
  l1.weight: grad_norm = 0.215523
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.172782
Total gradient norm: 0.484416
=== Actor Training Debug (Iteration 6033) ===
Q mean: -12.039769
Q std: 15.386779
Actor loss: 12.043726
Action reg: 0.003957
  l1.weight: grad_norm = 0.118964
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.100670
Total gradient norm: 0.280474
=== Actor Training Debug (Iteration 6034) ===
Q mean: -10.954334
Q std: 15.979527
Actor loss: 10.958297
Action reg: 0.003963
  l1.weight: grad_norm = 0.282317
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.220920
Total gradient norm: 0.682802
=== Actor Training Debug (Iteration 6035) ===
Q mean: -10.503372
Q std: 15.140220
Actor loss: 10.507319
Action reg: 0.003948
  l1.weight: grad_norm = 0.251700
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.204731
Total gradient norm: 0.523188
=== Actor Training Debug (Iteration 6036) ===
Q mean: -11.791325
Q std: 15.732448
Actor loss: 11.795278
Action reg: 0.003953
  l1.weight: grad_norm = 0.136663
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.117795
Total gradient norm: 0.298904
=== Actor Training Debug (Iteration 6037) ===
Q mean: -12.036409
Q std: 16.487375
Actor loss: 12.040357
Action reg: 0.003948
  l1.weight: grad_norm = 0.231145
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.207736
Total gradient norm: 0.541550
=== Actor Training Debug (Iteration 6038) ===
Q mean: -12.169209
Q std: 16.077177
Actor loss: 12.173179
Action reg: 0.003969
  l1.weight: grad_norm = 0.150143
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.124702
Total gradient norm: 0.335106
=== Actor Training Debug (Iteration 6039) ===
Q mean: -11.807375
Q std: 16.883533
Actor loss: 11.811325
Action reg: 0.003950
  l1.weight: grad_norm = 0.342113
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.289074
Total gradient norm: 0.750749
=== Actor Training Debug (Iteration 6040) ===
Q mean: -10.909136
Q std: 14.322159
Actor loss: 10.913087
Action reg: 0.003951
  l1.weight: grad_norm = 0.139045
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.129142
Total gradient norm: 0.345280
=== Actor Training Debug (Iteration 6041) ===
Q mean: -10.112907
Q std: 14.593264
Actor loss: 10.116853
Action reg: 0.003945
  l1.weight: grad_norm = 0.202492
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.165794
Total gradient norm: 0.419165
=== Actor Training Debug (Iteration 6042) ===
Q mean: -10.811356
Q std: 16.142721
Actor loss: 10.815310
Action reg: 0.003955
  l1.weight: grad_norm = 0.199151
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.148638
Total gradient norm: 0.395382
=== Actor Training Debug (Iteration 6043) ===
Q mean: -10.253591
Q std: 14.407258
Actor loss: 10.257560
Action reg: 0.003969
  l1.weight: grad_norm = 0.213023
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.164081
Total gradient norm: 0.475467
=== Actor Training Debug (Iteration 6044) ===
Q mean: -11.872506
Q std: 14.531673
Actor loss: 11.876471
Action reg: 0.003964
  l1.weight: grad_norm = 0.137926
  l1.bias: grad_norm = 0.000058
  l2.weight: grad_norm = 0.120657
Total gradient norm: 0.320402
=== Actor Training Debug (Iteration 6045) ===
Q mean: -11.167721
Q std: 16.156710
Actor loss: 11.171677
Action reg: 0.003956
  l1.weight: grad_norm = 0.227179
  l1.bias: grad_norm = 0.000449
  l2.weight: grad_norm = 0.199811
Total gradient norm: 0.525904
=== Actor Training Debug (Iteration 6046) ===
Q mean: -11.468417
Q std: 18.099003
Actor loss: 11.472375
Action reg: 0.003958
  l1.weight: grad_norm = 0.173956
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.159085
Total gradient norm: 0.395643
=== Actor Training Debug (Iteration 6047) ===
Q mean: -11.849440
Q std: 14.751162
Actor loss: 11.853396
Action reg: 0.003957
  l1.weight: grad_norm = 0.228427
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.227479
Total gradient norm: 0.732363
=== Actor Training Debug (Iteration 6048) ===
Q mean: -10.603468
Q std: 13.791605
Actor loss: 10.607430
Action reg: 0.003962
  l1.weight: grad_norm = 0.184478
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.159903
Total gradient norm: 0.410228
=== Actor Training Debug (Iteration 6049) ===
Q mean: -11.095146
Q std: 14.745932
Actor loss: 11.099104
Action reg: 0.003958
  l1.weight: grad_norm = 0.237186
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.182515
Total gradient norm: 0.489160
=== Actor Training Debug (Iteration 6050) ===
Q mean: -12.204548
Q std: 17.434610
Actor loss: 12.208496
Action reg: 0.003948
  l1.weight: grad_norm = 0.290354
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.229070
Total gradient norm: 0.636309
=== Actor Training Debug (Iteration 6051) ===
Q mean: -9.791366
Q std: 13.932365
Actor loss: 9.795325
Action reg: 0.003960
  l1.weight: grad_norm = 0.123089
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.106965
Total gradient norm: 0.258549
=== Actor Training Debug (Iteration 6052) ===
Q mean: -13.003181
Q std: 17.816334
Actor loss: 13.007116
Action reg: 0.003934
  l1.weight: grad_norm = 0.181450
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.166424
Total gradient norm: 0.408007
=== Actor Training Debug (Iteration 6053) ===
Q mean: -9.217302
Q std: 13.707609
Actor loss: 9.221261
Action reg: 0.003959
  l1.weight: grad_norm = 0.185145
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.174596
Total gradient norm: 0.541460
=== Actor Training Debug (Iteration 6054) ===
Q mean: -9.880836
Q std: 13.460584
Actor loss: 9.884773
Action reg: 0.003937
  l1.weight: grad_norm = 0.139171
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.125325
Total gradient norm: 0.309508
=== Actor Training Debug (Iteration 6055) ===
Q mean: -10.672832
Q std: 15.800991
Actor loss: 10.676790
Action reg: 0.003959
  l1.weight: grad_norm = 0.193301
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.155435
Total gradient norm: 0.483109
=== Actor Training Debug (Iteration 6056) ===
Q mean: -11.986046
Q std: 16.243683
Actor loss: 11.990019
Action reg: 0.003973
  l1.weight: grad_norm = 0.097029
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.086129
Total gradient norm: 0.245192
=== Actor Training Debug (Iteration 6057) ===
Q mean: -13.037192
Q std: 18.258011
Actor loss: 13.041154
Action reg: 0.003961
  l1.weight: grad_norm = 0.119775
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.106407
Total gradient norm: 0.320451
=== Actor Training Debug (Iteration 6058) ===
Q mean: -11.751266
Q std: 16.283031
Actor loss: 11.755221
Action reg: 0.003955
  l1.weight: grad_norm = 0.138040
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.113537
Total gradient norm: 0.298152
=== Actor Training Debug (Iteration 6059) ===
Q mean: -13.611067
Q std: 17.241779
Actor loss: 13.615030
Action reg: 0.003964
  l1.weight: grad_norm = 0.178046
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.150973
Total gradient norm: 0.396660
=== Actor Training Debug (Iteration 6060) ===
Q mean: -11.372909
Q std: 16.329786
Actor loss: 11.376880
Action reg: 0.003972
  l1.weight: grad_norm = 0.110405
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.103639
Total gradient norm: 0.297769
=== Actor Training Debug (Iteration 6061) ===
Q mean: -11.210217
Q std: 16.285835
Actor loss: 11.214173
Action reg: 0.003957
  l1.weight: grad_norm = 0.169112
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.153394
Total gradient norm: 0.390237
=== Actor Training Debug (Iteration 6062) ===
Q mean: -11.259212
Q std: 15.792086
Actor loss: 11.263184
Action reg: 0.003972
  l1.weight: grad_norm = 0.222086
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.194889
Total gradient norm: 0.544593
=== Actor Training Debug (Iteration 6063) ===
Q mean: -12.119008
Q std: 16.614361
Actor loss: 12.122971
Action reg: 0.003962
  l1.weight: grad_norm = 0.232731
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.192550
Total gradient norm: 0.550306
=== Actor Training Debug (Iteration 6064) ===
Q mean: -11.646591
Q std: 15.893265
Actor loss: 11.650541
Action reg: 0.003950
  l1.weight: grad_norm = 0.246997
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.207403
Total gradient norm: 0.579385
=== Actor Training Debug (Iteration 6065) ===
Q mean: -10.901386
Q std: 15.945051
Actor loss: 10.905345
Action reg: 0.003959
  l1.weight: grad_norm = 0.155120
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.122734
Total gradient norm: 0.358900
=== Actor Training Debug (Iteration 6066) ===
Q mean: -11.249976
Q std: 16.330128
Actor loss: 11.253922
Action reg: 0.003946
  l1.weight: grad_norm = 0.187393
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.145382
Total gradient norm: 0.347236
=== Actor Training Debug (Iteration 6067) ===
Q mean: -10.727453
Q std: 16.326647
Actor loss: 10.731390
Action reg: 0.003937
  l1.weight: grad_norm = 0.202462
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.175356
Total gradient norm: 0.445058
=== Actor Training Debug (Iteration 6068) ===
Q mean: -11.184242
Q std: 15.433819
Actor loss: 11.188203
Action reg: 0.003960
  l1.weight: grad_norm = 0.195462
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.171920
Total gradient norm: 0.439493
=== Actor Training Debug (Iteration 6069) ===
Q mean: -9.399350
Q std: 14.447069
Actor loss: 9.403324
Action reg: 0.003974
  l1.weight: grad_norm = 0.259634
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.249092
Total gradient norm: 0.673281
=== Actor Training Debug (Iteration 6070) ===
Q mean: -12.034271
Q std: 17.276594
Actor loss: 12.038226
Action reg: 0.003955
  l1.weight: grad_norm = 0.141472
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.105817
Total gradient norm: 0.283103
=== Actor Training Debug (Iteration 6071) ===
Q mean: -11.222198
Q std: 15.652706
Actor loss: 11.226174
Action reg: 0.003976
  l1.weight: grad_norm = 0.682835
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.529031
Total gradient norm: 1.474960
=== Actor Training Debug (Iteration 6072) ===
Q mean: -10.503079
Q std: 13.534896
Actor loss: 10.507039
Action reg: 0.003960
  l1.weight: grad_norm = 0.106764
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.095381
Total gradient norm: 0.253567
=== Actor Training Debug (Iteration 6073) ===
Q mean: -12.498402
Q std: 15.399514
Actor loss: 12.502374
Action reg: 0.003972
  l1.weight: grad_norm = 0.194560
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.170078
Total gradient norm: 0.485996
=== Actor Training Debug (Iteration 6074) ===
Q mean: -8.796249
Q std: 13.515528
Actor loss: 8.800207
Action reg: 0.003958
  l1.weight: grad_norm = 0.136462
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.125038
Total gradient norm: 0.325847
=== Actor Training Debug (Iteration 6075) ===
Q mean: -10.521154
Q std: 14.033027
Actor loss: 10.525125
Action reg: 0.003970
  l1.weight: grad_norm = 0.132655
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.132999
Total gradient norm: 0.328973
=== Actor Training Debug (Iteration 6076) ===
Q mean: -10.873684
Q std: 14.420999
Actor loss: 10.877652
Action reg: 0.003968
  l1.weight: grad_norm = 0.247069
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.199441
Total gradient norm: 0.503749
=== Actor Training Debug (Iteration 6077) ===
Q mean: -12.267479
Q std: 17.818577
Actor loss: 12.271428
Action reg: 0.003949
  l1.weight: grad_norm = 0.132315
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.111450
Total gradient norm: 0.340157
=== Actor Training Debug (Iteration 6078) ===
Q mean: -9.822662
Q std: 14.626194
Actor loss: 9.826613
Action reg: 0.003951
  l1.weight: grad_norm = 0.339166
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.290146
Total gradient norm: 0.678519
=== Actor Training Debug (Iteration 6079) ===
Q mean: -12.672589
Q std: 16.701708
Actor loss: 12.676550
Action reg: 0.003961
  l1.weight: grad_norm = 0.160236
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.143942
Total gradient norm: 0.357880
=== Actor Training Debug (Iteration 6080) ===
Q mean: -11.603154
Q std: 16.968174
Actor loss: 11.607111
Action reg: 0.003957
  l1.weight: grad_norm = 0.447122
  l1.bias: grad_norm = 0.000424
  l2.weight: grad_norm = 0.342579
Total gradient norm: 0.777642
=== Actor Training Debug (Iteration 6081) ===
Q mean: -12.613764
Q std: 17.772249
Actor loss: 12.617738
Action reg: 0.003974
  l1.weight: grad_norm = 0.447661
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.383886
Total gradient norm: 1.192783
=== Actor Training Debug (Iteration 6082) ===
Q mean: -11.672005
Q std: 16.298458
Actor loss: 11.675961
Action reg: 0.003957
  l1.weight: grad_norm = 0.231384
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.198561
Total gradient norm: 0.517756
=== Actor Training Debug (Iteration 6083) ===
Q mean: -11.672612
Q std: 15.601028
Actor loss: 11.676559
Action reg: 0.003947
  l1.weight: grad_norm = 0.204688
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.186086
Total gradient norm: 0.467395
=== Actor Training Debug (Iteration 6084) ===
Q mean: -11.589108
Q std: 15.118078
Actor loss: 11.593066
Action reg: 0.003959
  l1.weight: grad_norm = 0.213046
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.175352
Total gradient norm: 0.414130
=== Actor Training Debug (Iteration 6085) ===
Q mean: -12.917323
Q std: 17.059429
Actor loss: 12.921280
Action reg: 0.003957
  l1.weight: grad_norm = 0.158926
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.137202
Total gradient norm: 0.352657
=== Actor Training Debug (Iteration 6086) ===
Q mean: -12.870654
Q std: 17.360643
Actor loss: 12.874622
Action reg: 0.003968
  l1.weight: grad_norm = 0.230120
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.201434
Total gradient norm: 0.481527
=== Actor Training Debug (Iteration 6087) ===
Q mean: -10.412649
Q std: 13.034304
Actor loss: 10.416608
Action reg: 0.003959
  l1.weight: grad_norm = 0.114281
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.095360
Total gradient norm: 0.254695
=== Actor Training Debug (Iteration 6088) ===
Q mean: -11.060702
Q std: 16.061052
Actor loss: 11.064652
Action reg: 0.003950
  l1.weight: grad_norm = 0.170458
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.137669
Total gradient norm: 0.386609
=== Actor Training Debug (Iteration 6089) ===
Q mean: -12.567039
Q std: 17.014704
Actor loss: 12.570992
Action reg: 0.003952
  l1.weight: grad_norm = 0.342730
  l1.bias: grad_norm = 0.001530
  l2.weight: grad_norm = 0.308705
Total gradient norm: 0.923651
=== Actor Training Debug (Iteration 6090) ===
Q mean: -10.374198
Q std: 14.466582
Actor loss: 10.378165
Action reg: 0.003968
  l1.weight: grad_norm = 0.129759
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.108439
Total gradient norm: 0.292787
=== Actor Training Debug (Iteration 6091) ===
Q mean: -10.569127
Q std: 14.933359
Actor loss: 10.573076
Action reg: 0.003950
  l1.weight: grad_norm = 0.169531
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.138019
Total gradient norm: 0.393355
=== Actor Training Debug (Iteration 6092) ===
Q mean: -10.770675
Q std: 13.516450
Actor loss: 10.774647
Action reg: 0.003972
  l1.weight: grad_norm = 0.272970
  l1.bias: grad_norm = 0.000092
  l2.weight: grad_norm = 0.219271
Total gradient norm: 0.599887
=== Actor Training Debug (Iteration 6093) ===
Q mean: -10.997884
Q std: 14.711974
Actor loss: 11.001853
Action reg: 0.003969
  l1.weight: grad_norm = 0.086630
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.072484
Total gradient norm: 0.204375
=== Actor Training Debug (Iteration 6094) ===
Q mean: -11.797228
Q std: 16.645311
Actor loss: 11.801194
Action reg: 0.003966
  l1.weight: grad_norm = 0.212790
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.179351
Total gradient norm: 0.516350
=== Actor Training Debug (Iteration 6095) ===
Q mean: -11.243607
Q std: 15.380583
Actor loss: 11.247544
Action reg: 0.003937
  l1.weight: grad_norm = 0.123761
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.111541
Total gradient norm: 0.307326
=== Actor Training Debug (Iteration 6096) ===
Q mean: -11.890324
Q std: 16.057924
Actor loss: 11.894287
Action reg: 0.003964
  l1.weight: grad_norm = 0.145771
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.115437
Total gradient norm: 0.318023
=== Actor Training Debug (Iteration 6097) ===
Q mean: -12.786171
Q std: 17.665245
Actor loss: 12.790129
Action reg: 0.003958
  l1.weight: grad_norm = 0.237320
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.200793
Total gradient norm: 0.594797
=== Actor Training Debug (Iteration 6098) ===
Q mean: -10.228937
Q std: 13.669276
Actor loss: 10.232904
Action reg: 0.003967
  l1.weight: grad_norm = 0.216160
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.185232
Total gradient norm: 0.458615
=== Actor Training Debug (Iteration 6099) ===
Q mean: -10.859179
Q std: 14.362580
Actor loss: 10.863137
Action reg: 0.003958
  l1.weight: grad_norm = 0.285480
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.235013
Total gradient norm: 0.599365
=== Actor Training Debug (Iteration 6100) ===
Q mean: -10.144977
Q std: 15.379640
Actor loss: 10.148925
Action reg: 0.003948
  l1.weight: grad_norm = 0.233295
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.196615
Total gradient norm: 0.557791
Episode 111: Steps=100, Reward=-264.458, Buffer_size=11100
=== Actor Training Debug (Iteration 6101) ===
Q mean: -9.779036
Q std: 14.227720
Actor loss: 9.783003
Action reg: 0.003967
  l1.weight: grad_norm = 0.205493
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.164858
Total gradient norm: 0.468261
=== Actor Training Debug (Iteration 6102) ===
Q mean: -10.554675
Q std: 14.195705
Actor loss: 10.558629
Action reg: 0.003954
  l1.weight: grad_norm = 0.212859
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.197607
Total gradient norm: 0.560411
=== Actor Training Debug (Iteration 6103) ===
Q mean: -10.155437
Q std: 15.477551
Actor loss: 10.159383
Action reg: 0.003946
  l1.weight: grad_norm = 0.149136
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.144777
Total gradient norm: 0.378995
=== Actor Training Debug (Iteration 6104) ===
Q mean: -11.808894
Q std: 17.045134
Actor loss: 11.812862
Action reg: 0.003968
  l1.weight: grad_norm = 0.280523
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.236109
Total gradient norm: 0.678647
=== Actor Training Debug (Iteration 6105) ===
Q mean: -12.821248
Q std: 18.425249
Actor loss: 12.825190
Action reg: 0.003942
  l1.weight: grad_norm = 0.230782
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.199965
Total gradient norm: 0.559520
=== Actor Training Debug (Iteration 6106) ===
Q mean: -12.469348
Q std: 17.501505
Actor loss: 12.473306
Action reg: 0.003957
  l1.weight: grad_norm = 0.243276
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.174175
Total gradient norm: 0.449027
=== Actor Training Debug (Iteration 6107) ===
Q mean: -10.360022
Q std: 13.413299
Actor loss: 10.363992
Action reg: 0.003970
  l1.weight: grad_norm = 0.167594
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.144977
Total gradient norm: 0.409858
=== Actor Training Debug (Iteration 6108) ===
Q mean: -11.989868
Q std: 16.281771
Actor loss: 11.993838
Action reg: 0.003971
  l1.weight: grad_norm = 0.166524
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.141052
Total gradient norm: 0.377539
=== Actor Training Debug (Iteration 6109) ===
Q mean: -12.633741
Q std: 16.850113
Actor loss: 12.637700
Action reg: 0.003959
  l1.weight: grad_norm = 0.082867
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.077617
Total gradient norm: 0.225164
=== Actor Training Debug (Iteration 6110) ===
Q mean: -10.883793
Q std: 15.214149
Actor loss: 10.887743
Action reg: 0.003950
  l1.weight: grad_norm = 0.137410
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.124344
Total gradient norm: 0.341755
=== Actor Training Debug (Iteration 6111) ===
Q mean: -11.387929
Q std: 17.012167
Actor loss: 11.391886
Action reg: 0.003957
  l1.weight: grad_norm = 0.479809
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.403697
Total gradient norm: 1.074695
=== Actor Training Debug (Iteration 6112) ===
Q mean: -12.923440
Q std: 16.381498
Actor loss: 12.927403
Action reg: 0.003963
  l1.weight: grad_norm = 0.104863
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.093245
Total gradient norm: 0.238750
=== Actor Training Debug (Iteration 6113) ===
Q mean: -10.939582
Q std: 14.589060
Actor loss: 10.943519
Action reg: 0.003936
  l1.weight: grad_norm = 0.197066
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.181328
Total gradient norm: 0.443571
=== Actor Training Debug (Iteration 6114) ===
Q mean: -11.829281
Q std: 16.676819
Actor loss: 11.833241
Action reg: 0.003960
  l1.weight: grad_norm = 0.349195
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.217027
Total gradient norm: 0.571008
=== Actor Training Debug (Iteration 6115) ===
Q mean: -11.488492
Q std: 14.883368
Actor loss: 11.492453
Action reg: 0.003961
  l1.weight: grad_norm = 0.206858
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.175435
Total gradient norm: 0.433830
=== Actor Training Debug (Iteration 6116) ===
Q mean: -12.314624
Q std: 17.278414
Actor loss: 12.318603
Action reg: 0.003979
  l1.weight: grad_norm = 0.144240
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.133459
Total gradient norm: 0.360754
=== Actor Training Debug (Iteration 6117) ===
Q mean: -10.566916
Q std: 16.708456
Actor loss: 10.570880
Action reg: 0.003964
  l1.weight: grad_norm = 0.224466
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.190913
Total gradient norm: 0.457130
=== Actor Training Debug (Iteration 6118) ===
Q mean: -11.194880
Q std: 14.535838
Actor loss: 11.198851
Action reg: 0.003971
  l1.weight: grad_norm = 0.221760
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.195130
Total gradient norm: 0.483871
=== Actor Training Debug (Iteration 6119) ===
Q mean: -11.356187
Q std: 14.498129
Actor loss: 11.360137
Action reg: 0.003950
  l1.weight: grad_norm = 0.332017
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.333255
Total gradient norm: 0.930749
=== Actor Training Debug (Iteration 6120) ===
Q mean: -10.890539
Q std: 14.940744
Actor loss: 10.894476
Action reg: 0.003937
  l1.weight: grad_norm = 0.218078
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.187508
Total gradient norm: 0.515206
=== Actor Training Debug (Iteration 6121) ===
Q mean: -11.002201
Q std: 15.905709
Actor loss: 11.006173
Action reg: 0.003972
  l1.weight: grad_norm = 0.326240
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.233994
Total gradient norm: 0.616314
=== Actor Training Debug (Iteration 6122) ===
Q mean: -11.558045
Q std: 15.872789
Actor loss: 11.561991
Action reg: 0.003945
  l1.weight: grad_norm = 0.171058
  l1.bias: grad_norm = 0.000456
  l2.weight: grad_norm = 0.150513
Total gradient norm: 0.395155
=== Actor Training Debug (Iteration 6123) ===
Q mean: -11.215129
Q std: 16.352779
Actor loss: 11.219093
Action reg: 0.003964
  l1.weight: grad_norm = 0.164562
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.161013
Total gradient norm: 0.424472
=== Actor Training Debug (Iteration 6124) ===
Q mean: -11.984788
Q std: 16.658939
Actor loss: 11.988729
Action reg: 0.003942
  l1.weight: grad_norm = 0.235723
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.211989
Total gradient norm: 0.655938
=== Actor Training Debug (Iteration 6125) ===
Q mean: -11.045614
Q std: 14.412512
Actor loss: 11.049546
Action reg: 0.003932
  l1.weight: grad_norm = 0.283527
  l1.bias: grad_norm = 0.000626
  l2.weight: grad_norm = 0.239267
Total gradient norm: 0.579747
=== Actor Training Debug (Iteration 6126) ===
Q mean: -10.291254
Q std: 15.035239
Actor loss: 10.295206
Action reg: 0.003952
  l1.weight: grad_norm = 0.208486
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.184025
Total gradient norm: 0.475789
=== Actor Training Debug (Iteration 6127) ===
Q mean: -11.202267
Q std: 14.270971
Actor loss: 11.206233
Action reg: 0.003966
  l1.weight: grad_norm = 0.347085
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.249820
Total gradient norm: 0.697013
=== Actor Training Debug (Iteration 6128) ===
Q mean: -10.101488
Q std: 14.812505
Actor loss: 10.105454
Action reg: 0.003966
  l1.weight: grad_norm = 0.247362
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.233766
Total gradient norm: 0.653998
=== Actor Training Debug (Iteration 6129) ===
Q mean: -11.585150
Q std: 16.494520
Actor loss: 11.589118
Action reg: 0.003969
  l1.weight: grad_norm = 0.190009
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.157066
Total gradient norm: 0.451688
=== Actor Training Debug (Iteration 6130) ===
Q mean: -11.677844
Q std: 16.131956
Actor loss: 11.681804
Action reg: 0.003959
  l1.weight: grad_norm = 0.331051
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.304602
Total gradient norm: 0.877840
=== Actor Training Debug (Iteration 6131) ===
Q mean: -11.462521
Q std: 15.150017
Actor loss: 11.466487
Action reg: 0.003967
  l1.weight: grad_norm = 0.159953
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.151136
Total gradient norm: 0.386720
=== Actor Training Debug (Iteration 6132) ===
Q mean: -11.477968
Q std: 14.386339
Actor loss: 11.481933
Action reg: 0.003964
  l1.weight: grad_norm = 0.205568
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.167331
Total gradient norm: 0.447906
=== Actor Training Debug (Iteration 6133) ===
Q mean: -10.941623
Q std: 15.477184
Actor loss: 10.945579
Action reg: 0.003956
  l1.weight: grad_norm = 0.188860
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.151601
Total gradient norm: 0.482898
=== Actor Training Debug (Iteration 6134) ===
Q mean: -12.269873
Q std: 17.433781
Actor loss: 12.273850
Action reg: 0.003978
  l1.weight: grad_norm = 0.125137
  l1.bias: grad_norm = 0.000052
  l2.weight: grad_norm = 0.094749
Total gradient norm: 0.264049
=== Actor Training Debug (Iteration 6135) ===
Q mean: -12.305684
Q std: 16.508633
Actor loss: 12.309647
Action reg: 0.003962
  l1.weight: grad_norm = 0.260508
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.193786
Total gradient norm: 0.476702
=== Actor Training Debug (Iteration 6136) ===
Q mean: -12.075303
Q std: 14.291297
Actor loss: 12.079260
Action reg: 0.003957
  l1.weight: grad_norm = 0.150614
  l1.bias: grad_norm = 0.000665
  l2.weight: grad_norm = 0.142046
Total gradient norm: 0.425108
=== Actor Training Debug (Iteration 6137) ===
Q mean: -11.878503
Q std: 15.413911
Actor loss: 11.882465
Action reg: 0.003962
  l1.weight: grad_norm = 0.154393
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.130296
Total gradient norm: 0.345537
=== Actor Training Debug (Iteration 6138) ===
Q mean: -11.833430
Q std: 16.675568
Actor loss: 11.837389
Action reg: 0.003959
  l1.weight: grad_norm = 0.153873
  l1.bias: grad_norm = 0.001682
  l2.weight: grad_norm = 0.123856
Total gradient norm: 0.400029
=== Actor Training Debug (Iteration 6139) ===
Q mean: -13.179897
Q std: 16.993385
Actor loss: 13.183855
Action reg: 0.003958
  l1.weight: grad_norm = 0.272790
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.213211
Total gradient norm: 0.713089
=== Actor Training Debug (Iteration 6140) ===
Q mean: -12.241251
Q std: 16.737120
Actor loss: 12.245217
Action reg: 0.003966
  l1.weight: grad_norm = 0.175150
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.131214
Total gradient norm: 0.325626
=== Actor Training Debug (Iteration 6141) ===
Q mean: -12.370730
Q std: 16.585886
Actor loss: 12.374705
Action reg: 0.003975
  l1.weight: grad_norm = 0.114385
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.115246
Total gradient norm: 0.330638
=== Actor Training Debug (Iteration 6142) ===
Q mean: -9.838642
Q std: 15.735401
Actor loss: 9.842602
Action reg: 0.003959
  l1.weight: grad_norm = 0.161591
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.138007
Total gradient norm: 0.353117
=== Actor Training Debug (Iteration 6143) ===
Q mean: -11.180271
Q std: 16.874969
Actor loss: 11.184225
Action reg: 0.003954
  l1.weight: grad_norm = 0.192903
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.170510
Total gradient norm: 0.462446
=== Actor Training Debug (Iteration 6144) ===
Q mean: -10.921410
Q std: 14.797732
Actor loss: 10.925359
Action reg: 0.003949
  l1.weight: grad_norm = 0.673353
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.564934
Total gradient norm: 1.787834
=== Actor Training Debug (Iteration 6145) ===
Q mean: -10.588037
Q std: 15.959500
Actor loss: 10.591976
Action reg: 0.003940
  l1.weight: grad_norm = 0.199830
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.178948
Total gradient norm: 0.464243
=== Actor Training Debug (Iteration 6146) ===
Q mean: -12.919579
Q std: 16.652765
Actor loss: 12.923558
Action reg: 0.003980
  l1.weight: grad_norm = 0.130321
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.122529
Total gradient norm: 0.338062
=== Actor Training Debug (Iteration 6147) ===
Q mean: -11.590818
Q std: 15.780587
Actor loss: 11.594785
Action reg: 0.003966
  l1.weight: grad_norm = 0.106372
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.093016
Total gradient norm: 0.259618
=== Actor Training Debug (Iteration 6148) ===
Q mean: -10.136590
Q std: 14.679693
Actor loss: 10.140546
Action reg: 0.003956
  l1.weight: grad_norm = 0.284149
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.240812
Total gradient norm: 0.611884
=== Actor Training Debug (Iteration 6149) ===
Q mean: -10.817495
Q std: 16.028172
Actor loss: 10.821464
Action reg: 0.003968
  l1.weight: grad_norm = 0.269433
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.250682
Total gradient norm: 0.752112
=== Actor Training Debug (Iteration 6150) ===
Q mean: -10.097067
Q std: 15.729376
Actor loss: 10.101037
Action reg: 0.003970
  l1.weight: grad_norm = 0.218170
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.164345
Total gradient norm: 0.483793
=== Actor Training Debug (Iteration 6151) ===
Q mean: -11.341268
Q std: 15.465679
Actor loss: 11.345210
Action reg: 0.003943
  l1.weight: grad_norm = 0.225572
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.203293
Total gradient norm: 0.527872
=== Actor Training Debug (Iteration 6152) ===
Q mean: -12.319031
Q std: 17.144190
Actor loss: 12.322972
Action reg: 0.003941
  l1.weight: grad_norm = 0.203844
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.172751
Total gradient norm: 0.473183
=== Actor Training Debug (Iteration 6153) ===
Q mean: -11.904322
Q std: 14.473652
Actor loss: 11.908287
Action reg: 0.003966
  l1.weight: grad_norm = 0.179117
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.151018
Total gradient norm: 0.436265
=== Actor Training Debug (Iteration 6154) ===
Q mean: -12.657896
Q std: 17.402590
Actor loss: 12.661847
Action reg: 0.003952
  l1.weight: grad_norm = 0.323232
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.273478
Total gradient norm: 0.971588
=== Actor Training Debug (Iteration 6155) ===
Q mean: -11.433981
Q std: 15.934717
Actor loss: 11.437951
Action reg: 0.003970
  l1.weight: grad_norm = 0.227480
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.185118
Total gradient norm: 0.483005
=== Actor Training Debug (Iteration 6156) ===
Q mean: -11.381257
Q std: 15.712663
Actor loss: 11.385204
Action reg: 0.003947
  l1.weight: grad_norm = 0.163259
  l1.bias: grad_norm = 0.000405
  l2.weight: grad_norm = 0.148868
Total gradient norm: 0.417265
=== Actor Training Debug (Iteration 6157) ===
Q mean: -10.768126
Q std: 15.077212
Actor loss: 10.772094
Action reg: 0.003968
  l1.weight: grad_norm = 0.152883
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.130396
Total gradient norm: 0.323805
=== Actor Training Debug (Iteration 6158) ===
Q mean: -9.481570
Q std: 13.538670
Actor loss: 9.485513
Action reg: 0.003942
  l1.weight: grad_norm = 0.248479
  l1.bias: grad_norm = 0.000792
  l2.weight: grad_norm = 0.207282
Total gradient norm: 0.539353
=== Actor Training Debug (Iteration 6159) ===
Q mean: -11.059818
Q std: 15.058511
Actor loss: 11.063768
Action reg: 0.003950
  l1.weight: grad_norm = 0.175030
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.148465
Total gradient norm: 0.423552
=== Actor Training Debug (Iteration 6160) ===
Q mean: -10.975399
Q std: 16.155695
Actor loss: 10.979347
Action reg: 0.003948
  l1.weight: grad_norm = 0.314911
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.278369
Total gradient norm: 0.717309
=== Actor Training Debug (Iteration 6161) ===
Q mean: -11.646895
Q std: 17.374958
Actor loss: 11.650867
Action reg: 0.003972
  l1.weight: grad_norm = 0.124189
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.099323
Total gradient norm: 0.272410
=== Actor Training Debug (Iteration 6162) ===
Q mean: -9.676034
Q std: 14.507565
Actor loss: 9.679990
Action reg: 0.003956
  l1.weight: grad_norm = 0.152630
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.124299
Total gradient norm: 0.288765
=== Actor Training Debug (Iteration 6163) ===
Q mean: -11.790733
Q std: 16.374691
Actor loss: 11.794683
Action reg: 0.003949
  l1.weight: grad_norm = 0.155004
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.138922
Total gradient norm: 0.381199
=== Actor Training Debug (Iteration 6164) ===
Q mean: -12.249393
Q std: 15.606915
Actor loss: 12.253351
Action reg: 0.003957
  l1.weight: grad_norm = 0.094784
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.087544
Total gradient norm: 0.257582
=== Actor Training Debug (Iteration 6165) ===
Q mean: -11.931760
Q std: 14.923180
Actor loss: 11.935721
Action reg: 0.003962
  l1.weight: grad_norm = 0.282090
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.228065
Total gradient norm: 0.591060
=== Actor Training Debug (Iteration 6166) ===
Q mean: -11.737148
Q std: 16.915981
Actor loss: 11.741118
Action reg: 0.003970
  l1.weight: grad_norm = 0.158772
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.137382
Total gradient norm: 0.368623
=== Actor Training Debug (Iteration 6167) ===
Q mean: -10.513116
Q std: 16.896622
Actor loss: 10.517064
Action reg: 0.003948
  l1.weight: grad_norm = 0.234876
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.185143
Total gradient norm: 0.485575
=== Actor Training Debug (Iteration 6168) ===
Q mean: -12.473892
Q std: 15.510372
Actor loss: 12.477854
Action reg: 0.003961
  l1.weight: grad_norm = 0.178969
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.170994
Total gradient norm: 0.413382
=== Actor Training Debug (Iteration 6169) ===
Q mean: -13.155701
Q std: 17.965792
Actor loss: 13.159636
Action reg: 0.003936
  l1.weight: grad_norm = 0.189840
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.148416
Total gradient norm: 0.433900
=== Actor Training Debug (Iteration 6170) ===
Q mean: -11.986012
Q std: 17.122137
Actor loss: 11.989985
Action reg: 0.003973
  l1.weight: grad_norm = 0.147534
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.127842
Total gradient norm: 0.309523
=== Actor Training Debug (Iteration 6171) ===
Q mean: -12.056995
Q std: 16.966303
Actor loss: 12.060960
Action reg: 0.003964
  l1.weight: grad_norm = 0.281534
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.233698
Total gradient norm: 0.572357
=== Actor Training Debug (Iteration 6172) ===
Q mean: -11.882662
Q std: 16.082230
Actor loss: 11.886620
Action reg: 0.003958
  l1.weight: grad_norm = 0.466227
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.389321
Total gradient norm: 1.002542
=== Actor Training Debug (Iteration 6173) ===
Q mean: -11.105560
Q std: 15.301919
Actor loss: 11.109530
Action reg: 0.003970
  l1.weight: grad_norm = 0.120878
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.092654
Total gradient norm: 0.231471
=== Actor Training Debug (Iteration 6174) ===
Q mean: -11.008797
Q std: 16.884592
Actor loss: 11.012756
Action reg: 0.003959
  l1.weight: grad_norm = 0.234893
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.205087
Total gradient norm: 0.526715
=== Actor Training Debug (Iteration 6175) ===
Q mean: -10.748253
Q std: 16.237339
Actor loss: 10.752213
Action reg: 0.003960
  l1.weight: grad_norm = 0.167988
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.146360
Total gradient norm: 0.397253
=== Actor Training Debug (Iteration 6176) ===
Q mean: -11.745713
Q std: 15.914232
Actor loss: 11.749659
Action reg: 0.003945
  l1.weight: grad_norm = 0.222128
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.178078
Total gradient norm: 0.506844
=== Actor Training Debug (Iteration 6177) ===
Q mean: -12.249342
Q std: 17.447121
Actor loss: 12.253306
Action reg: 0.003964
  l1.weight: grad_norm = 0.195382
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.167433
Total gradient norm: 0.491519
=== Actor Training Debug (Iteration 6178) ===
Q mean: -10.856209
Q std: 15.831810
Actor loss: 10.860160
Action reg: 0.003951
  l1.weight: grad_norm = 0.314334
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.245217
Total gradient norm: 0.614753
=== Actor Training Debug (Iteration 6179) ===
Q mean: -12.824884
Q std: 17.158468
Actor loss: 12.828856
Action reg: 0.003971
  l1.weight: grad_norm = 0.326365
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.246381
Total gradient norm: 0.646284
=== Actor Training Debug (Iteration 6180) ===
Q mean: -10.772827
Q std: 15.192274
Actor loss: 10.776791
Action reg: 0.003963
  l1.weight: grad_norm = 0.260305
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.204781
Total gradient norm: 0.539439
=== Actor Training Debug (Iteration 6181) ===
Q mean: -11.786230
Q std: 16.421421
Actor loss: 11.790202
Action reg: 0.003972
  l1.weight: grad_norm = 0.226029
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.203116
Total gradient norm: 0.593861
=== Actor Training Debug (Iteration 6182) ===
Q mean: -11.156599
Q std: 14.972716
Actor loss: 11.160555
Action reg: 0.003955
  l1.weight: grad_norm = 0.412818
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.357083
Total gradient norm: 0.885988
=== Actor Training Debug (Iteration 6183) ===
Q mean: -13.102522
Q std: 15.773969
Actor loss: 13.106483
Action reg: 0.003960
  l1.weight: grad_norm = 0.330460
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.273246
Total gradient norm: 0.657933
=== Actor Training Debug (Iteration 6184) ===
Q mean: -10.318958
Q std: 13.999036
Actor loss: 10.322923
Action reg: 0.003964
  l1.weight: grad_norm = 0.075475
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.063561
Total gradient norm: 0.154134
=== Actor Training Debug (Iteration 6185) ===
Q mean: -13.419821
Q std: 16.526285
Actor loss: 13.423776
Action reg: 0.003955
  l1.weight: grad_norm = 0.256316
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.207648
Total gradient norm: 0.538788
=== Actor Training Debug (Iteration 6186) ===
Q mean: -12.983751
Q std: 17.176332
Actor loss: 12.987699
Action reg: 0.003947
  l1.weight: grad_norm = 0.262690
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.209193
Total gradient norm: 0.502610
=== Actor Training Debug (Iteration 6187) ===
Q mean: -11.451667
Q std: 16.126822
Actor loss: 11.455615
Action reg: 0.003948
  l1.weight: grad_norm = 0.221392
  l1.bias: grad_norm = 0.000703
  l2.weight: grad_norm = 0.196659
Total gradient norm: 0.525279
=== Actor Training Debug (Iteration 6188) ===
Q mean: -11.632675
Q std: 15.744545
Actor loss: 11.636647
Action reg: 0.003972
  l1.weight: grad_norm = 0.276782
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.247434
Total gradient norm: 0.723009
=== Actor Training Debug (Iteration 6189) ===
Q mean: -12.225238
Q std: 15.958482
Actor loss: 12.229210
Action reg: 0.003972
  l1.weight: grad_norm = 0.242873
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.212922
Total gradient norm: 0.579059
=== Actor Training Debug (Iteration 6190) ===
Q mean: -12.304591
Q std: 15.857188
Actor loss: 12.308534
Action reg: 0.003943
  l1.weight: grad_norm = 0.167052
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.140894
Total gradient norm: 0.373862
=== Actor Training Debug (Iteration 6191) ===
Q mean: -11.776832
Q std: 14.119666
Actor loss: 11.780806
Action reg: 0.003974
  l1.weight: grad_norm = 0.215045
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.186392
Total gradient norm: 0.543744
=== Actor Training Debug (Iteration 6192) ===
Q mean: -12.098666
Q std: 14.414714
Actor loss: 12.102601
Action reg: 0.003935
  l1.weight: grad_norm = 0.221646
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.181259
Total gradient norm: 0.474516
=== Actor Training Debug (Iteration 6193) ===
Q mean: -10.622103
Q std: 15.737768
Actor loss: 10.626034
Action reg: 0.003931
  l1.weight: grad_norm = 0.146910
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.117834
Total gradient norm: 0.320631
=== Actor Training Debug (Iteration 6194) ===
Q mean: -12.362253
Q std: 16.105627
Actor loss: 12.366212
Action reg: 0.003959
  l1.weight: grad_norm = 0.285201
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.258646
Total gradient norm: 0.701300
=== Actor Training Debug (Iteration 6195) ===
Q mean: -10.691845
Q std: 14.531223
Actor loss: 10.695788
Action reg: 0.003943
  l1.weight: grad_norm = 0.150926
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.141925
Total gradient norm: 0.409238
=== Actor Training Debug (Iteration 6196) ===
Q mean: -10.790260
Q std: 14.243035
Actor loss: 10.794221
Action reg: 0.003960
  l1.weight: grad_norm = 0.127910
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.108005
Total gradient norm: 0.293351
=== Actor Training Debug (Iteration 6197) ===
Q mean: -12.341348
Q std: 16.402893
Actor loss: 12.345279
Action reg: 0.003931
  l1.weight: grad_norm = 0.394661
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.339489
Total gradient norm: 0.783375
=== Actor Training Debug (Iteration 6198) ===
Q mean: -10.458374
Q std: 14.323498
Actor loss: 10.462315
Action reg: 0.003940
  l1.weight: grad_norm = 0.282720
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.239719
Total gradient norm: 0.736452
=== Actor Training Debug (Iteration 6199) ===
Q mean: -11.407585
Q std: 16.427813
Actor loss: 11.411536
Action reg: 0.003951
  l1.weight: grad_norm = 0.261778
  l1.bias: grad_norm = 0.000327
  l2.weight: grad_norm = 0.212418
Total gradient norm: 0.603790
=== Actor Training Debug (Iteration 6200) ===
Q mean: -11.662185
Q std: 17.129158
Actor loss: 11.666126
Action reg: 0.003941
  l1.weight: grad_norm = 0.303215
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.252819
Total gradient norm: 0.694478
=== Actor Training Debug (Iteration 6201) ===
Q mean: -11.899857
Q std: 17.366741
Actor loss: 11.903822
Action reg: 0.003965
  l1.weight: grad_norm = 0.104074
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.098354
Total gradient norm: 0.253060
=== Actor Training Debug (Iteration 6202) ===
Q mean: -12.659925
Q std: 15.388552
Actor loss: 12.663885
Action reg: 0.003961
  l1.weight: grad_norm = 0.311707
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.252102
Total gradient norm: 0.647508
=== Actor Training Debug (Iteration 6203) ===
Q mean: -11.673492
Q std: 17.464722
Actor loss: 11.677449
Action reg: 0.003957
  l1.weight: grad_norm = 0.316831
  l1.bias: grad_norm = 0.000494
  l2.weight: grad_norm = 0.278030
Total gradient norm: 0.862184
=== Actor Training Debug (Iteration 6204) ===
Q mean: -12.917428
Q std: 17.479349
Actor loss: 12.921382
Action reg: 0.003954
  l1.weight: grad_norm = 0.242298
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.184437
Total gradient norm: 0.564307
=== Actor Training Debug (Iteration 6205) ===
Q mean: -11.515294
Q std: 15.945350
Actor loss: 11.519236
Action reg: 0.003942
  l1.weight: grad_norm = 0.453303
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.345577
Total gradient norm: 0.917168
=== Actor Training Debug (Iteration 6206) ===
Q mean: -11.390600
Q std: 15.627238
Actor loss: 11.394559
Action reg: 0.003959
  l1.weight: grad_norm = 0.156354
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.132036
Total gradient norm: 0.306931
=== Actor Training Debug (Iteration 6207) ===
Q mean: -10.930595
Q std: 17.099237
Actor loss: 10.934548
Action reg: 0.003953
  l1.weight: grad_norm = 0.191419
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.175424
Total gradient norm: 0.445697
=== Actor Training Debug (Iteration 6208) ===
Q mean: -10.928625
Q std: 15.135034
Actor loss: 10.932587
Action reg: 0.003962
  l1.weight: grad_norm = 0.167150
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.156348
Total gradient norm: 0.447015
=== Actor Training Debug (Iteration 6209) ===
Q mean: -9.227286
Q std: 12.676442
Actor loss: 9.231244
Action reg: 0.003958
  l1.weight: grad_norm = 0.246175
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.209647
Total gradient norm: 0.530006
=== Actor Training Debug (Iteration 6210) ===
Q mean: -13.957245
Q std: 16.814014
Actor loss: 13.961211
Action reg: 0.003966
  l1.weight: grad_norm = 0.173023
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.161236
Total gradient norm: 0.443209
=== Actor Training Debug (Iteration 6211) ===
Q mean: -11.633821
Q std: 16.019510
Actor loss: 11.637787
Action reg: 0.003966
  l1.weight: grad_norm = 0.258819
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.194491
Total gradient norm: 0.520597
=== Actor Training Debug (Iteration 6212) ===
Q mean: -12.204468
Q std: 16.015171
Actor loss: 12.208439
Action reg: 0.003971
  l1.weight: grad_norm = 0.113063
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.096350
Total gradient norm: 0.277695
=== Actor Training Debug (Iteration 6213) ===
Q mean: -11.903320
Q std: 16.277449
Actor loss: 11.907288
Action reg: 0.003967
  l1.weight: grad_norm = 0.267594
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.210863
Total gradient norm: 0.546853
=== Actor Training Debug (Iteration 6214) ===
Q mean: -11.700466
Q std: 16.069141
Actor loss: 11.704423
Action reg: 0.003957
  l1.weight: grad_norm = 0.113910
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.093650
Total gradient norm: 0.229453
=== Actor Training Debug (Iteration 6215) ===
Q mean: -11.779300
Q std: 17.073868
Actor loss: 11.783267
Action reg: 0.003967
  l1.weight: grad_norm = 0.175384
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.137662
Total gradient norm: 0.381320
=== Actor Training Debug (Iteration 6216) ===
Q mean: -11.235395
Q std: 16.091005
Actor loss: 11.239344
Action reg: 0.003948
  l1.weight: grad_norm = 0.157896
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.138272
Total gradient norm: 0.448694
=== Actor Training Debug (Iteration 6217) ===
Q mean: -11.064191
Q std: 16.723261
Actor loss: 11.068166
Action reg: 0.003975
  l1.weight: grad_norm = 0.086350
  l1.bias: grad_norm = 0.000044
  l2.weight: grad_norm = 0.077462
Total gradient norm: 0.204402
=== Actor Training Debug (Iteration 6218) ===
Q mean: -11.442130
Q std: 15.520217
Actor loss: 11.446087
Action reg: 0.003956
  l1.weight: grad_norm = 0.236596
  l1.bias: grad_norm = 0.000839
  l2.weight: grad_norm = 0.196395
Total gradient norm: 0.561487
=== Actor Training Debug (Iteration 6219) ===
Q mean: -11.409173
Q std: 15.356435
Actor loss: 11.413141
Action reg: 0.003968
  l1.weight: grad_norm = 0.268806
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.259677
Total gradient norm: 0.627307
=== Actor Training Debug (Iteration 6220) ===
Q mean: -11.397081
Q std: 14.591245
Actor loss: 11.401048
Action reg: 0.003966
  l1.weight: grad_norm = 0.207816
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.186251
Total gradient norm: 0.539894
=== Actor Training Debug (Iteration 6221) ===
Q mean: -12.225468
Q std: 16.487915
Actor loss: 12.229419
Action reg: 0.003951
  l1.weight: grad_norm = 0.211323
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.165026
Total gradient norm: 0.466638
=== Actor Training Debug (Iteration 6222) ===
Q mean: -10.994465
Q std: 15.056044
Actor loss: 10.998422
Action reg: 0.003957
  l1.weight: grad_norm = 0.175203
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.155623
Total gradient norm: 0.435248
=== Actor Training Debug (Iteration 6223) ===
Q mean: -12.298577
Q std: 16.778811
Actor loss: 12.302545
Action reg: 0.003968
  l1.weight: grad_norm = 0.153774
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.130091
Total gradient norm: 0.433531
=== Actor Training Debug (Iteration 6224) ===
Q mean: -13.467060
Q std: 17.411135
Actor loss: 13.471021
Action reg: 0.003961
  l1.weight: grad_norm = 0.190003
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.142372
Total gradient norm: 0.366632
=== Actor Training Debug (Iteration 6225) ===
Q mean: -11.417222
Q std: 16.159428
Actor loss: 11.421176
Action reg: 0.003954
  l1.weight: grad_norm = 0.235305
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.188171
Total gradient norm: 0.543521
=== Actor Training Debug (Iteration 6226) ===
Q mean: -13.648960
Q std: 17.649158
Actor loss: 13.652927
Action reg: 0.003968
  l1.weight: grad_norm = 0.218900
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.176782
Total gradient norm: 0.442536
=== Actor Training Debug (Iteration 6227) ===
Q mean: -12.083529
Q std: 14.868410
Actor loss: 12.087483
Action reg: 0.003955
  l1.weight: grad_norm = 0.420888
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.339742
Total gradient norm: 0.899526
=== Actor Training Debug (Iteration 6228) ===
Q mean: -12.504908
Q std: 16.569633
Actor loss: 12.508852
Action reg: 0.003944
  l1.weight: grad_norm = 0.286242
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.230989
Total gradient norm: 0.587710
=== Actor Training Debug (Iteration 6229) ===
Q mean: -12.162794
Q std: 16.412161
Actor loss: 12.166748
Action reg: 0.003954
  l1.weight: grad_norm = 0.148153
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.124525
Total gradient norm: 0.329324
=== Actor Training Debug (Iteration 6230) ===
Q mean: -13.640257
Q std: 17.893038
Actor loss: 13.644229
Action reg: 0.003972
  l1.weight: grad_norm = 0.139963
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.114294
Total gradient norm: 0.360429
=== Actor Training Debug (Iteration 6231) ===
Q mean: -11.070569
Q std: 15.246424
Actor loss: 11.074519
Action reg: 0.003950
  l1.weight: grad_norm = 0.310973
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.257489
Total gradient norm: 0.706388
=== Actor Training Debug (Iteration 6232) ===
Q mean: -12.422165
Q std: 15.926346
Actor loss: 12.426107
Action reg: 0.003942
  l1.weight: grad_norm = 0.363923
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.329982
Total gradient norm: 0.967790
=== Actor Training Debug (Iteration 6233) ===
Q mean: -9.562594
Q std: 14.573484
Actor loss: 9.566518
Action reg: 0.003923
  l1.weight: grad_norm = 0.133248
  l1.bias: grad_norm = 0.001644
  l2.weight: grad_norm = 0.118370
Total gradient norm: 0.361252
=== Actor Training Debug (Iteration 6234) ===
Q mean: -12.257771
Q std: 17.160242
Actor loss: 12.261716
Action reg: 0.003944
  l1.weight: grad_norm = 0.152489
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.143704
Total gradient norm: 0.357408
=== Actor Training Debug (Iteration 6235) ===
Q mean: -10.981865
Q std: 16.103735
Actor loss: 10.985817
Action reg: 0.003952
  l1.weight: grad_norm = 0.190778
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.182281
Total gradient norm: 0.530632
=== Actor Training Debug (Iteration 6236) ===
Q mean: -10.144982
Q std: 16.686043
Actor loss: 10.148927
Action reg: 0.003945
  l1.weight: grad_norm = 0.218101
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.180421
Total gradient norm: 0.535885
=== Actor Training Debug (Iteration 6237) ===
Q mean: -11.042900
Q std: 15.270877
Actor loss: 11.046855
Action reg: 0.003955
  l1.weight: grad_norm = 0.327614
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.274299
Total gradient norm: 0.701455
=== Actor Training Debug (Iteration 6238) ===
Q mean: -11.176158
Q std: 14.077836
Actor loss: 11.180101
Action reg: 0.003943
  l1.weight: grad_norm = 0.369205
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.324426
Total gradient norm: 0.953731
=== Actor Training Debug (Iteration 6239) ===
Q mean: -12.203787
Q std: 16.964792
Actor loss: 12.207747
Action reg: 0.003960
  l1.weight: grad_norm = 0.266464
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.223504
Total gradient norm: 0.632313
=== Actor Training Debug (Iteration 6240) ===
Q mean: -11.168898
Q std: 15.455829
Actor loss: 11.172858
Action reg: 0.003960
  l1.weight: grad_norm = 0.274733
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.256454
Total gradient norm: 0.652637
=== Actor Training Debug (Iteration 6241) ===
Q mean: -12.056395
Q std: 17.557995
Actor loss: 12.060341
Action reg: 0.003946
  l1.weight: grad_norm = 0.239837
  l1.bias: grad_norm = 0.000521
  l2.weight: grad_norm = 0.207759
Total gradient norm: 0.518275
=== Actor Training Debug (Iteration 6242) ===
Q mean: -11.621330
Q std: 15.998561
Actor loss: 11.625304
Action reg: 0.003974
  l1.weight: grad_norm = 0.089600
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.068231
Total gradient norm: 0.198993
=== Actor Training Debug (Iteration 6243) ===
Q mean: -13.031769
Q std: 15.977389
Actor loss: 13.035730
Action reg: 0.003961
  l1.weight: grad_norm = 0.243703
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.209664
Total gradient norm: 0.559501
=== Actor Training Debug (Iteration 6244) ===
Q mean: -11.832612
Q std: 17.059137
Actor loss: 11.836554
Action reg: 0.003942
  l1.weight: grad_norm = 0.192648
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.152401
Total gradient norm: 0.421469
=== Actor Training Debug (Iteration 6245) ===
Q mean: -11.795822
Q std: 15.665243
Actor loss: 11.799785
Action reg: 0.003962
  l1.weight: grad_norm = 0.139989
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.114094
Total gradient norm: 0.324974
=== Actor Training Debug (Iteration 6246) ===
Q mean: -11.827607
Q std: 16.372511
Actor loss: 11.831584
Action reg: 0.003976
  l1.weight: grad_norm = 0.127775
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.125310
Total gradient norm: 0.339431
=== Actor Training Debug (Iteration 6247) ===
Q mean: -8.907957
Q std: 11.992503
Actor loss: 8.911901
Action reg: 0.003944
  l1.weight: grad_norm = 0.345122
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.300470
Total gradient norm: 0.898038
=== Actor Training Debug (Iteration 6248) ===
Q mean: -12.148784
Q std: 16.164801
Actor loss: 12.152740
Action reg: 0.003956
  l1.weight: grad_norm = 0.267317
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.212829
Total gradient norm: 0.640383
=== Actor Training Debug (Iteration 6249) ===
Q mean: -11.513449
Q std: 16.022930
Actor loss: 11.517386
Action reg: 0.003938
  l1.weight: grad_norm = 0.223713
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.180009
Total gradient norm: 0.445466
=== Actor Training Debug (Iteration 6250) ===
Q mean: -11.756258
Q std: 15.919550
Actor loss: 11.760227
Action reg: 0.003969
  l1.weight: grad_norm = 0.241018
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.204173
Total gradient norm: 0.590765
=== Actor Training Debug (Iteration 6251) ===
Q mean: -10.586454
Q std: 14.683517
Actor loss: 10.590414
Action reg: 0.003960
  l1.weight: grad_norm = 0.294123
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.239988
Total gradient norm: 0.710999
=== Actor Training Debug (Iteration 6252) ===
Q mean: -10.264098
Q std: 15.367064
Actor loss: 10.268048
Action reg: 0.003950
  l1.weight: grad_norm = 0.361176
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.304338
Total gradient norm: 0.767464
=== Actor Training Debug (Iteration 6253) ===
Q mean: -13.087799
Q std: 15.986751
Actor loss: 13.091773
Action reg: 0.003974
  l1.weight: grad_norm = 0.139694
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.110983
Total gradient norm: 0.358591
=== Actor Training Debug (Iteration 6254) ===
Q mean: -11.530240
Q std: 16.221317
Actor loss: 11.534200
Action reg: 0.003960
  l1.weight: grad_norm = 0.223030
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.191986
Total gradient norm: 0.516790
=== Actor Training Debug (Iteration 6255) ===
Q mean: -11.308228
Q std: 15.182830
Actor loss: 11.312193
Action reg: 0.003966
  l1.weight: grad_norm = 0.083474
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.075301
Total gradient norm: 0.192403
=== Actor Training Debug (Iteration 6256) ===
Q mean: -11.356201
Q std: 16.068146
Actor loss: 11.360176
Action reg: 0.003975
  l1.weight: grad_norm = 0.224267
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.184568
Total gradient norm: 0.443019
=== Actor Training Debug (Iteration 6257) ===
Q mean: -11.886805
Q std: 16.078041
Actor loss: 11.890757
Action reg: 0.003952
  l1.weight: grad_norm = 0.427528
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.376534
Total gradient norm: 0.920253
=== Actor Training Debug (Iteration 6258) ===
Q mean: -11.769823
Q std: 17.608885
Actor loss: 11.773775
Action reg: 0.003952
  l1.weight: grad_norm = 0.215114
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.197704
Total gradient norm: 0.508095
=== Actor Training Debug (Iteration 6259) ===
Q mean: -11.135652
Q std: 15.444871
Actor loss: 11.139605
Action reg: 0.003953
  l1.weight: grad_norm = 0.216657
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.195264
Total gradient norm: 0.562846
=== Actor Training Debug (Iteration 6260) ===
Q mean: -12.987034
Q std: 17.035572
Actor loss: 12.990997
Action reg: 0.003963
  l1.weight: grad_norm = 0.224010
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.194397
Total gradient norm: 0.579594
=== Actor Training Debug (Iteration 6261) ===
Q mean: -10.789268
Q std: 14.731361
Actor loss: 10.793221
Action reg: 0.003953
  l1.weight: grad_norm = 0.220586
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.174464
Total gradient norm: 0.474861
=== Actor Training Debug (Iteration 6262) ===
Q mean: -12.720039
Q std: 16.245871
Actor loss: 12.724010
Action reg: 0.003970
  l1.weight: grad_norm = 0.119540
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.092495
Total gradient norm: 0.251886
=== Actor Training Debug (Iteration 6263) ===
Q mean: -10.888181
Q std: 13.714406
Actor loss: 10.892148
Action reg: 0.003968
  l1.weight: grad_norm = 0.157764
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.140681
Total gradient norm: 0.353644
=== Actor Training Debug (Iteration 6264) ===
Q mean: -11.100822
Q std: 17.575018
Actor loss: 11.104765
Action reg: 0.003943
  l1.weight: grad_norm = 0.221830
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.206128
Total gradient norm: 0.537168
=== Actor Training Debug (Iteration 6265) ===
Q mean: -10.611738
Q std: 16.279556
Actor loss: 10.615700
Action reg: 0.003961
  l1.weight: grad_norm = 0.197587
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.169880
Total gradient norm: 0.430111
=== Actor Training Debug (Iteration 6266) ===
Q mean: -9.810818
Q std: 13.864398
Actor loss: 9.814775
Action reg: 0.003956
  l1.weight: grad_norm = 0.239937
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.189030
Total gradient norm: 0.546911
=== Actor Training Debug (Iteration 6267) ===
Q mean: -11.701513
Q std: 16.235636
Actor loss: 11.705458
Action reg: 0.003944
  l1.weight: grad_norm = 0.272788
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.219193
Total gradient norm: 0.600694
=== Actor Training Debug (Iteration 6268) ===
Q mean: -11.869102
Q std: 15.155441
Actor loss: 11.873056
Action reg: 0.003954
  l1.weight: grad_norm = 0.127234
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.115041
Total gradient norm: 0.340205
=== Actor Training Debug (Iteration 6269) ===
Q mean: -11.728772
Q std: 17.227884
Actor loss: 11.732724
Action reg: 0.003952
  l1.weight: grad_norm = 0.187774
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.167813
Total gradient norm: 0.465687
=== Actor Training Debug (Iteration 6270) ===
Q mean: -10.875341
Q std: 16.120779
Actor loss: 10.879317
Action reg: 0.003976
  l1.weight: grad_norm = 0.249391
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.218532
Total gradient norm: 0.597974
=== Actor Training Debug (Iteration 6271) ===
Q mean: -11.624093
Q std: 17.366873
Actor loss: 11.628046
Action reg: 0.003953
  l1.weight: grad_norm = 0.358101
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.321119
Total gradient norm: 0.890719
=== Actor Training Debug (Iteration 6272) ===
Q mean: -10.967896
Q std: 14.430574
Actor loss: 10.971851
Action reg: 0.003956
  l1.weight: grad_norm = 0.247001
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.256159
Total gradient norm: 0.686270
=== Actor Training Debug (Iteration 6273) ===
Q mean: -11.980030
Q std: 17.119949
Actor loss: 11.983974
Action reg: 0.003944
  l1.weight: grad_norm = 0.358727
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.291930
Total gradient norm: 0.791332
=== Actor Training Debug (Iteration 6274) ===
Q mean: -12.080496
Q std: 14.957907
Actor loss: 12.084431
Action reg: 0.003935
  l1.weight: grad_norm = 0.354140
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.279859
Total gradient norm: 0.707466
=== Actor Training Debug (Iteration 6275) ===
Q mean: -10.080496
Q std: 15.278992
Actor loss: 10.084444
Action reg: 0.003948
  l1.weight: grad_norm = 0.340696
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.308256
Total gradient norm: 0.862408
=== Actor Training Debug (Iteration 6276) ===
Q mean: -10.944977
Q std: 15.329018
Actor loss: 10.948940
Action reg: 0.003963
  l1.weight: grad_norm = 0.113458
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.099397
Total gradient norm: 0.252358
=== Actor Training Debug (Iteration 6277) ===
Q mean: -10.780075
Q std: 14.341115
Actor loss: 10.784017
Action reg: 0.003942
  l1.weight: grad_norm = 0.210111
  l1.bias: grad_norm = 0.000628
  l2.weight: grad_norm = 0.171049
Total gradient norm: 0.462423
=== Actor Training Debug (Iteration 6278) ===
Q mean: -12.228762
Q std: 14.921584
Actor loss: 12.232704
Action reg: 0.003942
  l1.weight: grad_norm = 0.521318
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.438531
Total gradient norm: 1.042861
=== Actor Training Debug (Iteration 6279) ===
Q mean: -11.867501
Q std: 16.586098
Actor loss: 11.871450
Action reg: 0.003949
  l1.weight: grad_norm = 0.273509
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.223816
Total gradient norm: 0.638383
=== Actor Training Debug (Iteration 6280) ===
Q mean: -9.751956
Q std: 14.755369
Actor loss: 9.755910
Action reg: 0.003954
  l1.weight: grad_norm = 0.182236
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.147797
Total gradient norm: 0.397670
=== Actor Training Debug (Iteration 6281) ===
Q mean: -10.843136
Q std: 15.359361
Actor loss: 10.847098
Action reg: 0.003963
  l1.weight: grad_norm = 0.153009
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.134995
Total gradient norm: 0.381535
=== Actor Training Debug (Iteration 6282) ===
Q mean: -10.191429
Q std: 13.703943
Actor loss: 10.195388
Action reg: 0.003959
  l1.weight: grad_norm = 0.206617
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.194733
Total gradient norm: 0.588903
=== Actor Training Debug (Iteration 6283) ===
Q mean: -13.470226
Q std: 17.647432
Actor loss: 13.474186
Action reg: 0.003960
  l1.weight: grad_norm = 0.276545
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.219016
Total gradient norm: 0.543310
=== Actor Training Debug (Iteration 6284) ===
Q mean: -12.826159
Q std: 17.330059
Actor loss: 12.830128
Action reg: 0.003969
  l1.weight: grad_norm = 0.245200
  l1.bias: grad_norm = 0.000081
  l2.weight: grad_norm = 0.187649
Total gradient norm: 0.493410
=== Actor Training Debug (Iteration 6285) ===
Q mean: -12.219154
Q std: 17.440458
Actor loss: 12.223117
Action reg: 0.003962
  l1.weight: grad_norm = 0.311909
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.323079
Total gradient norm: 0.967781
=== Actor Training Debug (Iteration 6286) ===
Q mean: -12.938894
Q std: 16.652216
Actor loss: 12.942865
Action reg: 0.003971
  l1.weight: grad_norm = 0.169731
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.146217
Total gradient norm: 0.391172
=== Actor Training Debug (Iteration 6287) ===
Q mean: -13.588999
Q std: 17.024908
Actor loss: 13.592951
Action reg: 0.003952
  l1.weight: grad_norm = 0.166333
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.134625
Total gradient norm: 0.346327
=== Actor Training Debug (Iteration 6288) ===
Q mean: -10.208759
Q std: 13.996817
Actor loss: 10.212734
Action reg: 0.003975
  l1.weight: grad_norm = 0.160833
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.149028
Total gradient norm: 0.383317
=== Actor Training Debug (Iteration 6289) ===
Q mean: -12.785329
Q std: 16.974854
Actor loss: 12.789296
Action reg: 0.003968
  l1.weight: grad_norm = 0.181842
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.156024
Total gradient norm: 0.479023
=== Actor Training Debug (Iteration 6290) ===
Q mean: -11.917913
Q std: 16.083174
Actor loss: 11.921877
Action reg: 0.003963
  l1.weight: grad_norm = 0.172886
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.144137
Total gradient norm: 0.360641
=== Actor Training Debug (Iteration 6291) ===
Q mean: -10.875381
Q std: 15.144294
Actor loss: 10.879311
Action reg: 0.003929
  l1.weight: grad_norm = 0.167132
  l1.bias: grad_norm = 0.002032
  l2.weight: grad_norm = 0.152752
Total gradient norm: 0.413599
=== Actor Training Debug (Iteration 6292) ===
Q mean: -13.547009
Q std: 17.678337
Actor loss: 13.550964
Action reg: 0.003956
  l1.weight: grad_norm = 0.122515
  l1.bias: grad_norm = 0.003488
  l2.weight: grad_norm = 0.100895
Total gradient norm: 0.347510
=== Actor Training Debug (Iteration 6293) ===
Q mean: -12.939487
Q std: 16.113369
Actor loss: 12.943461
Action reg: 0.003974
  l1.weight: grad_norm = 0.226327
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.185148
Total gradient norm: 0.477534
=== Actor Training Debug (Iteration 6294) ===
Q mean: -12.110929
Q std: 15.615211
Actor loss: 12.114867
Action reg: 0.003938
  l1.weight: grad_norm = 0.356614
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.280940
Total gradient norm: 0.743946
=== Actor Training Debug (Iteration 6295) ===
Q mean: -11.547027
Q std: 15.585346
Actor loss: 11.550990
Action reg: 0.003964
  l1.weight: grad_norm = 0.290950
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.253754
Total gradient norm: 0.616923
=== Actor Training Debug (Iteration 6296) ===
Q mean: -11.532885
Q std: 17.222136
Actor loss: 11.536829
Action reg: 0.003945
  l1.weight: grad_norm = 0.281719
  l1.bias: grad_norm = 0.001425
  l2.weight: grad_norm = 0.227123
Total gradient norm: 0.696965
=== Actor Training Debug (Iteration 6297) ===
Q mean: -11.595032
Q std: 14.512757
Actor loss: 11.598976
Action reg: 0.003944
  l1.weight: grad_norm = 0.261091
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.207114
Total gradient norm: 0.559492
=== Actor Training Debug (Iteration 6298) ===
Q mean: -12.973854
Q std: 16.543993
Actor loss: 12.977828
Action reg: 0.003974
  l1.weight: grad_norm = 0.171507
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.147411
Total gradient norm: 0.361246
=== Actor Training Debug (Iteration 6299) ===
Q mean: -11.681383
Q std: 16.713938
Actor loss: 11.685354
Action reg: 0.003971
  l1.weight: grad_norm = 0.167340
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.144032
Total gradient norm: 0.432017
=== Actor Training Debug (Iteration 6300) ===
Q mean: -11.976486
Q std: 15.378480
Actor loss: 11.980453
Action reg: 0.003967
  l1.weight: grad_norm = 0.467712
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.362505
Total gradient norm: 1.237344
=== Actor Training Debug (Iteration 6301) ===
Q mean: -10.354256
Q std: 13.843372
Actor loss: 10.358201
Action reg: 0.003945
  l1.weight: grad_norm = 0.179129
  l1.bias: grad_norm = 0.000391
  l2.weight: grad_norm = 0.174264
Total gradient norm: 0.456478
=== Actor Training Debug (Iteration 6302) ===
Q mean: -10.865156
Q std: 14.458117
Actor loss: 10.869127
Action reg: 0.003971
  l1.weight: grad_norm = 0.213661
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.180888
Total gradient norm: 0.521600
=== Actor Training Debug (Iteration 6303) ===
Q mean: -14.487274
Q std: 19.319582
Actor loss: 14.491256
Action reg: 0.003982
  l1.weight: grad_norm = 0.228554
  l1.bias: grad_norm = 0.000071
  l2.weight: grad_norm = 0.178945
Total gradient norm: 0.475767
=== Actor Training Debug (Iteration 6304) ===
Q mean: -11.085148
Q std: 15.614613
Actor loss: 11.089107
Action reg: 0.003958
  l1.weight: grad_norm = 0.136269
  l1.bias: grad_norm = 0.000656
  l2.weight: grad_norm = 0.112074
Total gradient norm: 0.346296
=== Actor Training Debug (Iteration 6305) ===
Q mean: -11.233445
Q std: 16.025942
Actor loss: 11.237420
Action reg: 0.003975
  l1.weight: grad_norm = 0.187519
  l1.bias: grad_norm = 0.000488
  l2.weight: grad_norm = 0.166104
Total gradient norm: 0.458876
=== Actor Training Debug (Iteration 6306) ===
Q mean: -11.191198
Q std: 16.856747
Actor loss: 11.195155
Action reg: 0.003957
  l1.weight: grad_norm = 0.213298
  l1.bias: grad_norm = 0.000849
  l2.weight: grad_norm = 0.175575
Total gradient norm: 0.452614
=== Actor Training Debug (Iteration 6307) ===
Q mean: -9.978056
Q std: 16.009485
Actor loss: 9.982015
Action reg: 0.003959
  l1.weight: grad_norm = 0.150245
  l1.bias: grad_norm = 0.000962
  l2.weight: grad_norm = 0.128578
Total gradient norm: 0.326163
=== Actor Training Debug (Iteration 6308) ===
Q mean: -10.624438
Q std: 14.371535
Actor loss: 10.628393
Action reg: 0.003955
  l1.weight: grad_norm = 0.320315
  l1.bias: grad_norm = 0.001056
  l2.weight: grad_norm = 0.242987
Total gradient norm: 0.648928
=== Actor Training Debug (Iteration 6309) ===
Q mean: -13.500691
Q std: 16.318817
Actor loss: 13.504639
Action reg: 0.003948
  l1.weight: grad_norm = 0.210206
  l1.bias: grad_norm = 0.001002
  l2.weight: grad_norm = 0.176183
Total gradient norm: 0.495684
=== Actor Training Debug (Iteration 6310) ===
Q mean: -13.170940
Q std: 17.316662
Actor loss: 13.174898
Action reg: 0.003958
  l1.weight: grad_norm = 0.276543
  l1.bias: grad_norm = 0.000636
  l2.weight: grad_norm = 0.225438
Total gradient norm: 0.643301
=== Actor Training Debug (Iteration 6311) ===
Q mean: -11.682758
Q std: 15.056000
Actor loss: 11.686714
Action reg: 0.003956
  l1.weight: grad_norm = 0.165053
  l1.bias: grad_norm = 0.000692
  l2.weight: grad_norm = 0.140514
Total gradient norm: 0.375342
=== Actor Training Debug (Iteration 6312) ===
Q mean: -11.473043
Q std: 15.893069
Actor loss: 11.477016
Action reg: 0.003973
  l1.weight: grad_norm = 0.218530
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.181960
Total gradient norm: 0.478286
=== Actor Training Debug (Iteration 6313) ===
Q mean: -12.426468
Q std: 15.500422
Actor loss: 12.430436
Action reg: 0.003968
  l1.weight: grad_norm = 0.238221
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.194569
Total gradient norm: 0.500753
=== Actor Training Debug (Iteration 6314) ===
Q mean: -10.358323
Q std: 15.602036
Actor loss: 10.362269
Action reg: 0.003946
  l1.weight: grad_norm = 0.357421
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.298981
Total gradient norm: 0.817001
=== Actor Training Debug (Iteration 6315) ===
Q mean: -12.901857
Q std: 17.551731
Actor loss: 12.905819
Action reg: 0.003962
  l1.weight: grad_norm = 0.308607
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.221284
Total gradient norm: 0.582046
=== Actor Training Debug (Iteration 6316) ===
Q mean: -10.021592
Q std: 15.044119
Actor loss: 10.025564
Action reg: 0.003972
  l1.weight: grad_norm = 0.115495
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.114271
Total gradient norm: 0.359867
=== Actor Training Debug (Iteration 6317) ===
Q mean: -12.455734
Q std: 17.397495
Actor loss: 12.459691
Action reg: 0.003957
  l1.weight: grad_norm = 0.176465
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.164563
Total gradient norm: 0.514662
=== Actor Training Debug (Iteration 6318) ===
Q mean: -10.956535
Q std: 15.741285
Actor loss: 10.960501
Action reg: 0.003966
  l1.weight: grad_norm = 0.118769
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.111291
Total gradient norm: 0.293704
=== Actor Training Debug (Iteration 6319) ===
Q mean: -12.202776
Q std: 16.189390
Actor loss: 12.206738
Action reg: 0.003962
  l1.weight: grad_norm = 0.212074
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.182662
Total gradient norm: 0.554578
=== Actor Training Debug (Iteration 6320) ===
Q mean: -10.419166
Q std: 12.944874
Actor loss: 10.423133
Action reg: 0.003968
  l1.weight: grad_norm = 0.176988
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.165714
Total gradient norm: 0.511284
=== Actor Training Debug (Iteration 6321) ===
Q mean: -11.339986
Q std: 14.437332
Actor loss: 11.343933
Action reg: 0.003948
  l1.weight: grad_norm = 0.125253
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.121903
Total gradient norm: 0.417036
=== Actor Training Debug (Iteration 6322) ===
Q mean: -13.461759
Q std: 18.654963
Actor loss: 13.465724
Action reg: 0.003965
  l1.weight: grad_norm = 0.287647
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.266263
Total gradient norm: 0.606750
=== Actor Training Debug (Iteration 6323) ===
Q mean: -12.334684
Q std: 16.669182
Actor loss: 12.338647
Action reg: 0.003963
  l1.weight: grad_norm = 0.256983
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.194278
Total gradient norm: 0.496538
=== Actor Training Debug (Iteration 6324) ===
Q mean: -12.273926
Q std: 17.596266
Actor loss: 12.277874
Action reg: 0.003948
  l1.weight: grad_norm = 0.227994
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.196148
Total gradient norm: 0.468397
=== Actor Training Debug (Iteration 6325) ===
Q mean: -12.037056
Q std: 16.643648
Actor loss: 12.040999
Action reg: 0.003943
  l1.weight: grad_norm = 0.156642
  l1.bias: grad_norm = 0.000531
  l2.weight: grad_norm = 0.141565
Total gradient norm: 0.367876
=== Actor Training Debug (Iteration 6326) ===
Q mean: -12.566373
Q std: 17.684631
Actor loss: 12.570340
Action reg: 0.003968
  l1.weight: grad_norm = 0.145327
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.128206
Total gradient norm: 0.343292
=== Actor Training Debug (Iteration 6327) ===
Q mean: -12.819005
Q std: 17.205282
Actor loss: 12.822954
Action reg: 0.003949
  l1.weight: grad_norm = 0.175040
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.164142
Total gradient norm: 0.454536
=== Actor Training Debug (Iteration 6328) ===
Q mean: -12.522706
Q std: 15.992102
Actor loss: 12.526665
Action reg: 0.003959
  l1.weight: grad_norm = 0.117579
  l1.bias: grad_norm = 0.001456
  l2.weight: grad_norm = 0.114165
Total gradient norm: 0.293411
=== Actor Training Debug (Iteration 6329) ===
Q mean: -12.682050
Q std: 15.987081
Actor loss: 12.686014
Action reg: 0.003965
  l1.weight: grad_norm = 0.228022
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.184303
Total gradient norm: 0.482252
=== Actor Training Debug (Iteration 6330) ===
Q mean: -11.541429
Q std: 15.940736
Actor loss: 11.545382
Action reg: 0.003953
  l1.weight: grad_norm = 0.392739
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.311158
Total gradient norm: 0.817925
=== Actor Training Debug (Iteration 6331) ===
Q mean: -10.576544
Q std: 14.256018
Actor loss: 10.580504
Action reg: 0.003961
  l1.weight: grad_norm = 0.171112
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.158822
Total gradient norm: 0.404092
=== Actor Training Debug (Iteration 6332) ===
Q mean: -10.483016
Q std: 13.999537
Actor loss: 10.486963
Action reg: 0.003947
  l1.weight: grad_norm = 0.210434
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.195251
Total gradient norm: 0.533738
=== Actor Training Debug (Iteration 6333) ===
Q mean: -11.321163
Q std: 15.831320
Actor loss: 11.325105
Action reg: 0.003941
  l1.weight: grad_norm = 0.348604
  l1.bias: grad_norm = 0.001356
  l2.weight: grad_norm = 0.259204
Total gradient norm: 0.704246
=== Actor Training Debug (Iteration 6334) ===
Q mean: -11.273380
Q std: 15.327209
Actor loss: 11.277341
Action reg: 0.003961
  l1.weight: grad_norm = 0.238874
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.207060
Total gradient norm: 0.598231
=== Actor Training Debug (Iteration 6335) ===
Q mean: -11.573492
Q std: 16.348557
Actor loss: 11.577438
Action reg: 0.003946
  l1.weight: grad_norm = 0.163177
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.125211
Total gradient norm: 0.336271
=== Actor Training Debug (Iteration 6336) ===
Q mean: -12.608866
Q std: 15.822962
Actor loss: 12.612814
Action reg: 0.003948
  l1.weight: grad_norm = 0.299968
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.247335
Total gradient norm: 0.644601
=== Actor Training Debug (Iteration 6337) ===
Q mean: -11.692940
Q std: 15.071434
Actor loss: 11.696897
Action reg: 0.003957
  l1.weight: grad_norm = 0.217152
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.176518
Total gradient norm: 0.487002
=== Actor Training Debug (Iteration 6338) ===
Q mean: -11.862114
Q std: 16.126629
Actor loss: 11.866066
Action reg: 0.003952
  l1.weight: grad_norm = 0.252337
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.227676
Total gradient norm: 0.608401
=== Actor Training Debug (Iteration 6339) ===
Q mean: -11.300719
Q std: 15.649071
Actor loss: 11.304691
Action reg: 0.003972
  l1.weight: grad_norm = 0.115394
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.092442
Total gradient norm: 0.260256
=== Actor Training Debug (Iteration 6340) ===
Q mean: -11.237650
Q std: 15.612942
Actor loss: 11.241611
Action reg: 0.003961
  l1.weight: grad_norm = 0.316506
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.250776
Total gradient norm: 0.619126
=== Actor Training Debug (Iteration 6341) ===
Q mean: -11.525311
Q std: 15.334169
Actor loss: 11.529255
Action reg: 0.003945
  l1.weight: grad_norm = 0.148614
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.118250
Total gradient norm: 0.297037
=== Actor Training Debug (Iteration 6342) ===
Q mean: -11.927483
Q std: 16.663769
Actor loss: 11.931436
Action reg: 0.003953
  l1.weight: grad_norm = 0.124353
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.114050
Total gradient norm: 0.303359
=== Actor Training Debug (Iteration 6343) ===
Q mean: -10.911294
Q std: 15.340577
Actor loss: 10.915251
Action reg: 0.003956
  l1.weight: grad_norm = 0.249077
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.222475
Total gradient norm: 0.695391
=== Actor Training Debug (Iteration 6344) ===
Q mean: -11.837258
Q std: 15.651410
Actor loss: 11.841224
Action reg: 0.003965
  l1.weight: grad_norm = 0.093695
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.086983
Total gradient norm: 0.252549
=== Actor Training Debug (Iteration 6345) ===
Q mean: -11.878848
Q std: 17.503010
Actor loss: 11.882802
Action reg: 0.003954
  l1.weight: grad_norm = 0.194745
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.157195
Total gradient norm: 0.414596
=== Actor Training Debug (Iteration 6346) ===
Q mean: -11.478321
Q std: 16.567402
Actor loss: 11.482272
Action reg: 0.003951
  l1.weight: grad_norm = 0.249074
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.196815
Total gradient norm: 0.541745
=== Actor Training Debug (Iteration 6347) ===
Q mean: -12.863626
Q std: 16.629379
Actor loss: 12.867584
Action reg: 0.003958
  l1.weight: grad_norm = 0.130614
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.116376
Total gradient norm: 0.293571
=== Actor Training Debug (Iteration 6348) ===
Q mean: -13.502827
Q std: 16.360218
Actor loss: 13.506786
Action reg: 0.003959
  l1.weight: grad_norm = 0.172518
  l1.bias: grad_norm = 0.000425
  l2.weight: grad_norm = 0.152072
Total gradient norm: 0.397089
=== Actor Training Debug (Iteration 6349) ===
Q mean: -12.606699
Q std: 17.608847
Actor loss: 12.610635
Action reg: 0.003936
  l1.weight: grad_norm = 0.561579
  l1.bias: grad_norm = 0.000773
  l2.weight: grad_norm = 0.393697
Total gradient norm: 1.056091
=== Actor Training Debug (Iteration 6350) ===
Q mean: -11.774645
Q std: 16.214050
Actor loss: 11.778597
Action reg: 0.003952
  l1.weight: grad_norm = 0.130596
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.115862
Total gradient norm: 0.331293
=== Actor Training Debug (Iteration 6351) ===
Q mean: -12.234719
Q std: 17.172892
Actor loss: 12.238676
Action reg: 0.003957
  l1.weight: grad_norm = 0.283582
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.232866
Total gradient norm: 0.599342
=== Actor Training Debug (Iteration 6352) ===
Q mean: -10.855027
Q std: 14.113615
Actor loss: 10.858998
Action reg: 0.003971
  l1.weight: grad_norm = 0.178832
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.153513
Total gradient norm: 0.417803
=== Actor Training Debug (Iteration 6353) ===
Q mean: -10.652763
Q std: 14.220663
Actor loss: 10.656732
Action reg: 0.003968
  l1.weight: grad_norm = 0.263228
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.231387
Total gradient norm: 0.660854
=== Actor Training Debug (Iteration 6354) ===
Q mean: -11.857843
Q std: 14.479795
Actor loss: 11.861794
Action reg: 0.003951
  l1.weight: grad_norm = 0.117706
  l1.bias: grad_norm = 0.000432
  l2.weight: grad_norm = 0.093707
Total gradient norm: 0.274841
=== Actor Training Debug (Iteration 6355) ===
Q mean: -9.224162
Q std: 14.116642
Actor loss: 9.228122
Action reg: 0.003960
  l1.weight: grad_norm = 0.195243
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.177395
Total gradient norm: 0.478731
=== Actor Training Debug (Iteration 6356) ===
Q mean: -12.957002
Q std: 17.539858
Actor loss: 12.960971
Action reg: 0.003969
  l1.weight: grad_norm = 0.170549
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.125842
Total gradient norm: 0.350999
=== Actor Training Debug (Iteration 6357) ===
Q mean: -12.227172
Q std: 16.342907
Actor loss: 12.231124
Action reg: 0.003952
  l1.weight: grad_norm = 0.102157
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.088309
Total gradient norm: 0.245508
=== Actor Training Debug (Iteration 6358) ===
Q mean: -11.330406
Q std: 15.771433
Actor loss: 11.334350
Action reg: 0.003943
  l1.weight: grad_norm = 0.501696
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.479413
Total gradient norm: 1.641450
=== Actor Training Debug (Iteration 6359) ===
Q mean: -11.515924
Q std: 15.833668
Actor loss: 11.519887
Action reg: 0.003962
  l1.weight: grad_norm = 0.166905
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.153579
Total gradient norm: 0.413592
=== Actor Training Debug (Iteration 6360) ===
Q mean: -12.547251
Q std: 16.354912
Actor loss: 12.551229
Action reg: 0.003978
  l1.weight: grad_norm = 0.128573
  l1.bias: grad_norm = 0.000069
  l2.weight: grad_norm = 0.113253
Total gradient norm: 0.317912
=== Actor Training Debug (Iteration 6361) ===
Q mean: -11.072289
Q std: 15.607672
Actor loss: 11.076245
Action reg: 0.003956
  l1.weight: grad_norm = 0.126565
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.107521
Total gradient norm: 0.291952
=== Actor Training Debug (Iteration 6362) ===
Q mean: -11.542776
Q std: 15.043341
Actor loss: 11.546743
Action reg: 0.003968
  l1.weight: grad_norm = 0.195689
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.155385
Total gradient norm: 0.429387
=== Actor Training Debug (Iteration 6363) ===
Q mean: -13.575050
Q std: 18.116125
Actor loss: 13.579001
Action reg: 0.003951
  l1.weight: grad_norm = 0.118790
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.113715
Total gradient norm: 0.290782
=== Actor Training Debug (Iteration 6364) ===
Q mean: -11.079798
Q std: 14.929280
Actor loss: 11.083735
Action reg: 0.003938
  l1.weight: grad_norm = 0.172531
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.134225
Total gradient norm: 0.356337
=== Actor Training Debug (Iteration 6365) ===
Q mean: -12.255536
Q std: 16.182600
Actor loss: 12.259492
Action reg: 0.003956
  l1.weight: grad_norm = 0.217845
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.181840
Total gradient norm: 0.546063
=== Actor Training Debug (Iteration 6366) ===
Q mean: -10.526228
Q std: 15.542217
Actor loss: 10.530167
Action reg: 0.003938
  l1.weight: grad_norm = 0.226104
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.172567
Total gradient norm: 0.426026
=== Actor Training Debug (Iteration 6367) ===
Q mean: -12.903954
Q std: 17.135574
Actor loss: 12.907890
Action reg: 0.003937
  l1.weight: grad_norm = 0.200685
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.168260
Total gradient norm: 0.464825
=== Actor Training Debug (Iteration 6368) ===
Q mean: -11.733122
Q std: 15.260825
Actor loss: 11.737089
Action reg: 0.003967
  l1.weight: grad_norm = 0.188616
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.152624
Total gradient norm: 0.440774
=== Actor Training Debug (Iteration 6369) ===
Q mean: -11.597409
Q std: 16.581484
Actor loss: 11.601368
Action reg: 0.003958
  l1.weight: grad_norm = 0.196421
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.166290
Total gradient norm: 0.485657
=== Actor Training Debug (Iteration 6370) ===
Q mean: -12.109232
Q std: 14.546174
Actor loss: 12.113193
Action reg: 0.003960
  l1.weight: grad_norm = 0.244923
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.211436
Total gradient norm: 0.531628
=== Actor Training Debug (Iteration 6371) ===
Q mean: -12.396705
Q std: 18.812611
Actor loss: 12.400658
Action reg: 0.003953
  l1.weight: grad_norm = 0.173932
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.129585
Total gradient norm: 0.357011
=== Actor Training Debug (Iteration 6372) ===
Q mean: -11.723625
Q std: 16.477150
Actor loss: 11.727575
Action reg: 0.003951
  l1.weight: grad_norm = 0.163461
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.126236
Total gradient norm: 0.376626
=== Actor Training Debug (Iteration 6373) ===
Q mean: -12.761875
Q std: 17.720781
Actor loss: 12.765828
Action reg: 0.003953
  l1.weight: grad_norm = 0.147718
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.112431
Total gradient norm: 0.277399
=== Actor Training Debug (Iteration 6374) ===
Q mean: -11.845351
Q std: 16.711496
Actor loss: 11.849315
Action reg: 0.003963
  l1.weight: grad_norm = 0.161359
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.132971
Total gradient norm: 0.329179
=== Actor Training Debug (Iteration 6375) ===
Q mean: -12.765793
Q std: 17.072952
Actor loss: 12.769737
Action reg: 0.003945
  l1.weight: grad_norm = 0.158638
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.171640
Total gradient norm: 0.419155
=== Actor Training Debug (Iteration 6376) ===
Q mean: -12.638268
Q std: 17.293369
Actor loss: 12.642212
Action reg: 0.003944
  l1.weight: grad_norm = 0.325641
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.271309
Total gradient norm: 0.681917
=== Actor Training Debug (Iteration 6377) ===
Q mean: -11.148436
Q std: 15.332812
Actor loss: 11.152389
Action reg: 0.003953
  l1.weight: grad_norm = 0.221288
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.190314
Total gradient norm: 0.516912
=== Actor Training Debug (Iteration 6378) ===
Q mean: -11.079170
Q std: 14.539620
Actor loss: 11.083131
Action reg: 0.003960
  l1.weight: grad_norm = 0.268157
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.226499
Total gradient norm: 0.581858
=== Actor Training Debug (Iteration 6379) ===
Q mean: -11.063899
Q std: 14.840327
Actor loss: 11.067859
Action reg: 0.003960
  l1.weight: grad_norm = 0.259006
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.201478
Total gradient norm: 0.574477
=== Actor Training Debug (Iteration 6380) ===
Q mean: -14.352314
Q std: 18.007727
Actor loss: 14.356256
Action reg: 0.003942
  l1.weight: grad_norm = 0.241174
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.199698
Total gradient norm: 0.497699
=== Actor Training Debug (Iteration 6381) ===
Q mean: -10.444559
Q std: 15.725789
Actor loss: 10.448507
Action reg: 0.003948
  l1.weight: grad_norm = 0.268906
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.233894
Total gradient norm: 0.626268
=== Actor Training Debug (Iteration 6382) ===
Q mean: -10.803370
Q std: 15.755253
Actor loss: 10.807334
Action reg: 0.003964
  l1.weight: grad_norm = 0.105019
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.091891
Total gradient norm: 0.317778
=== Actor Training Debug (Iteration 6383) ===
Q mean: -10.315410
Q std: 15.398268
Actor loss: 10.319357
Action reg: 0.003947
  l1.weight: grad_norm = 0.179072
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.160525
Total gradient norm: 0.443860
=== Actor Training Debug (Iteration 6384) ===
Q mean: -12.645184
Q std: 17.645212
Actor loss: 12.649150
Action reg: 0.003967
  l1.weight: grad_norm = 0.168046
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.131210
Total gradient norm: 0.328941
=== Actor Training Debug (Iteration 6385) ===
Q mean: -13.862574
Q std: 17.897022
Actor loss: 13.866540
Action reg: 0.003967
  l1.weight: grad_norm = 0.165615
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.146091
Total gradient norm: 0.377892
=== Actor Training Debug (Iteration 6386) ===
Q mean: -13.497884
Q std: 17.776426
Actor loss: 13.501853
Action reg: 0.003970
  l1.weight: grad_norm = 0.197030
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.147503
Total gradient norm: 0.474688
=== Actor Training Debug (Iteration 6387) ===
Q mean: -12.490083
Q std: 17.467573
Actor loss: 12.494055
Action reg: 0.003972
  l1.weight: grad_norm = 0.303839
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.233576
Total gradient norm: 0.580347
=== Actor Training Debug (Iteration 6388) ===
Q mean: -12.200727
Q std: 16.301479
Actor loss: 12.204694
Action reg: 0.003966
  l1.weight: grad_norm = 0.240539
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.236656
Total gradient norm: 0.651038
=== Actor Training Debug (Iteration 6389) ===
Q mean: -9.591812
Q std: 14.421021
Actor loss: 9.595769
Action reg: 0.003957
  l1.weight: grad_norm = 0.161408
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.128711
Total gradient norm: 0.328404
=== Actor Training Debug (Iteration 6390) ===
Q mean: -12.722551
Q std: 16.586319
Actor loss: 12.726508
Action reg: 0.003957
  l1.weight: grad_norm = 0.070788
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.064889
Total gradient norm: 0.181479
=== Actor Training Debug (Iteration 6391) ===
Q mean: -12.466525
Q std: 17.001841
Actor loss: 12.470484
Action reg: 0.003959
  l1.weight: grad_norm = 0.254792
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.221725
Total gradient norm: 0.612470
=== Actor Training Debug (Iteration 6392) ===
Q mean: -12.716566
Q std: 15.509347
Actor loss: 12.720510
Action reg: 0.003943
  l1.weight: grad_norm = 0.234449
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.212164
Total gradient norm: 0.599542
=== Actor Training Debug (Iteration 6393) ===
Q mean: -13.153649
Q std: 17.756021
Actor loss: 13.157612
Action reg: 0.003963
  l1.weight: grad_norm = 0.110185
  l1.bias: grad_norm = 0.001426
  l2.weight: grad_norm = 0.111917
Total gradient norm: 0.350471
=== Actor Training Debug (Iteration 6394) ===
Q mean: -11.651257
Q std: 15.893902
Actor loss: 11.655211
Action reg: 0.003955
  l1.weight: grad_norm = 0.480716
  l1.bias: grad_norm = 0.001555
  l2.weight: grad_norm = 0.402402
Total gradient norm: 1.118298
=== Actor Training Debug (Iteration 6395) ===
Q mean: -9.895219
Q std: 13.800407
Actor loss: 9.899175
Action reg: 0.003956
  l1.weight: grad_norm = 0.388293
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.351770
Total gradient norm: 0.852677
=== Actor Training Debug (Iteration 6396) ===
Q mean: -10.008562
Q std: 15.202553
Actor loss: 10.012526
Action reg: 0.003963
  l1.weight: grad_norm = 0.157725
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.130838
Total gradient norm: 0.342402
=== Actor Training Debug (Iteration 6397) ===
Q mean: -11.946717
Q std: 15.800650
Actor loss: 11.950666
Action reg: 0.003949
  l1.weight: grad_norm = 0.164356
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.163974
Total gradient norm: 0.441475
=== Actor Training Debug (Iteration 6398) ===
Q mean: -13.225184
Q std: 17.584803
Actor loss: 13.229144
Action reg: 0.003960
  l1.weight: grad_norm = 0.138980
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.112958
Total gradient norm: 0.282520
=== Actor Training Debug (Iteration 6399) ===
Q mean: -10.686661
Q std: 14.425731
Actor loss: 10.690627
Action reg: 0.003966
  l1.weight: grad_norm = 0.201489
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.176305
Total gradient norm: 0.459510
=== Actor Training Debug (Iteration 6400) ===
Q mean: -9.965429
Q std: 13.721768
Actor loss: 9.969388
Action reg: 0.003958
  l1.weight: grad_norm = 1.389266
  l1.bias: grad_norm = 0.001017
  l2.weight: grad_norm = 1.048688
Total gradient norm: 2.249442
=== Actor Training Debug (Iteration 6401) ===
Q mean: -10.655596
Q std: 14.543860
Actor loss: 10.659533
Action reg: 0.003937
  l1.weight: grad_norm = 0.220306
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.186555
Total gradient norm: 0.481508
=== Actor Training Debug (Iteration 6402) ===
Q mean: -11.221653
Q std: 16.072054
Actor loss: 11.225606
Action reg: 0.003953
  l1.weight: grad_norm = 0.166826
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.158010
Total gradient norm: 0.445678
=== Actor Training Debug (Iteration 6403) ===
Q mean: -11.086348
Q std: 15.997173
Actor loss: 11.090289
Action reg: 0.003942
  l1.weight: grad_norm = 0.295586
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.224497
Total gradient norm: 0.570776
=== Actor Training Debug (Iteration 6404) ===
Q mean: -11.694156
Q std: 15.455228
Actor loss: 11.698118
Action reg: 0.003963
  l1.weight: grad_norm = 0.378746
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.280917
Total gradient norm: 0.691456
=== Actor Training Debug (Iteration 6405) ===
Q mean: -11.691405
Q std: 16.121910
Actor loss: 11.695344
Action reg: 0.003939
  l1.weight: grad_norm = 0.216514
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.195990
Total gradient norm: 0.548018
=== Actor Training Debug (Iteration 6406) ===
Q mean: -10.936485
Q std: 14.889295
Actor loss: 10.940438
Action reg: 0.003953
  l1.weight: grad_norm = 0.227436
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.177863
Total gradient norm: 0.443296
=== Actor Training Debug (Iteration 6407) ===
Q mean: -12.212279
Q std: 15.692280
Actor loss: 12.216226
Action reg: 0.003946
  l1.weight: grad_norm = 0.315492
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.299622
Total gradient norm: 0.753665
=== Actor Training Debug (Iteration 6408) ===
Q mean: -13.446201
Q std: 17.113741
Actor loss: 13.450161
Action reg: 0.003960
  l1.weight: grad_norm = 0.304115
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.287193
Total gradient norm: 0.791140
=== Actor Training Debug (Iteration 6409) ===
Q mean: -10.287745
Q std: 16.089565
Actor loss: 10.291712
Action reg: 0.003966
  l1.weight: grad_norm = 0.155990
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.127472
Total gradient norm: 0.348526
=== Actor Training Debug (Iteration 6410) ===
Q mean: -11.541506
Q std: 16.037523
Actor loss: 11.545437
Action reg: 0.003931
  l1.weight: grad_norm = 0.224016
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.188203
Total gradient norm: 0.499534
=== Actor Training Debug (Iteration 6411) ===
Q mean: -12.529852
Q std: 15.978616
Actor loss: 12.533817
Action reg: 0.003965
  l1.weight: grad_norm = 0.228956
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.206088
Total gradient norm: 0.556473
=== Actor Training Debug (Iteration 6412) ===
Q mean: -13.457544
Q std: 17.027735
Actor loss: 13.461497
Action reg: 0.003953
  l1.weight: grad_norm = 0.065232
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.059770
Total gradient norm: 0.163576
=== Actor Training Debug (Iteration 6413) ===
Q mean: -12.582523
Q std: 17.220072
Actor loss: 12.586476
Action reg: 0.003953
  l1.weight: grad_norm = 0.250835
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.235878
Total gradient norm: 0.643547
=== Actor Training Debug (Iteration 6414) ===
Q mean: -12.541571
Q std: 17.751282
Actor loss: 12.545521
Action reg: 0.003950
  l1.weight: grad_norm = 0.239868
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.216008
Total gradient norm: 0.600819
=== Actor Training Debug (Iteration 6415) ===
Q mean: -10.244550
Q std: 15.859953
Actor loss: 10.248512
Action reg: 0.003963
  l1.weight: grad_norm = 0.209581
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.175067
Total gradient norm: 0.509727
=== Actor Training Debug (Iteration 6416) ===
Q mean: -10.376735
Q std: 16.135271
Actor loss: 10.380689
Action reg: 0.003954
  l1.weight: grad_norm = 0.198695
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.163367
Total gradient norm: 0.393190
=== Actor Training Debug (Iteration 6417) ===
Q mean: -10.798395
Q std: 14.970589
Actor loss: 10.802354
Action reg: 0.003959
  l1.weight: grad_norm = 0.227911
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.180138
Total gradient norm: 0.463307
=== Actor Training Debug (Iteration 6418) ===
Q mean: -11.371454
Q std: 16.578588
Actor loss: 11.375430
Action reg: 0.003976
  l1.weight: grad_norm = 0.085681
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.068573
Total gradient norm: 0.197875
=== Actor Training Debug (Iteration 6419) ===
Q mean: -11.512839
Q std: 14.348445
Actor loss: 11.516803
Action reg: 0.003964
  l1.weight: grad_norm = 0.117450
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.110847
Total gradient norm: 0.307400
=== Actor Training Debug (Iteration 6420) ===
Q mean: -11.650957
Q std: 15.541718
Actor loss: 11.654917
Action reg: 0.003960
  l1.weight: grad_norm = 0.301441
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.249359
Total gradient norm: 0.668843
=== Actor Training Debug (Iteration 6421) ===
Q mean: -11.076521
Q std: 14.650778
Actor loss: 11.080474
Action reg: 0.003953
  l1.weight: grad_norm = 0.251301
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.223050
Total gradient norm: 0.666608
=== Actor Training Debug (Iteration 6422) ===
Q mean: -12.101467
Q std: 17.555990
Actor loss: 12.105426
Action reg: 0.003959
  l1.weight: grad_norm = 0.253281
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.213157
Total gradient norm: 0.523905
=== Actor Training Debug (Iteration 6423) ===
Q mean: -12.665837
Q std: 16.326752
Actor loss: 12.669802
Action reg: 0.003965
  l1.weight: grad_norm = 0.284977
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.236471
Total gradient norm: 0.630587
=== Actor Training Debug (Iteration 6424) ===
Q mean: -11.188140
Q std: 14.857376
Actor loss: 11.192098
Action reg: 0.003957
  l1.weight: grad_norm = 0.143073
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.118651
Total gradient norm: 0.356094
=== Actor Training Debug (Iteration 6425) ===
Q mean: -10.274637
Q std: 14.151789
Actor loss: 10.278600
Action reg: 0.003963
  l1.weight: grad_norm = 0.101144
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.086328
Total gradient norm: 0.238370
=== Actor Training Debug (Iteration 6426) ===
Q mean: -11.137297
Q std: 13.953719
Actor loss: 11.141252
Action reg: 0.003955
  l1.weight: grad_norm = 0.120141
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.095088
Total gradient norm: 0.267420
=== Actor Training Debug (Iteration 6427) ===
Q mean: -12.758810
Q std: 16.447529
Actor loss: 12.762753
Action reg: 0.003942
  l1.weight: grad_norm = 0.254576
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.253237
Total gradient norm: 0.598727
=== Actor Training Debug (Iteration 6428) ===
Q mean: -12.127630
Q std: 16.939289
Actor loss: 12.131582
Action reg: 0.003952
  l1.weight: grad_norm = 0.228005
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.215494
Total gradient norm: 0.611829
=== Actor Training Debug (Iteration 6429) ===
Q mean: -12.799129
Q std: 17.692049
Actor loss: 12.803099
Action reg: 0.003969
  l1.weight: grad_norm = 0.237839
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.188210
Total gradient norm: 0.520323
=== Actor Training Debug (Iteration 6430) ===
Q mean: -13.754317
Q std: 16.737640
Actor loss: 13.758291
Action reg: 0.003974
  l1.weight: grad_norm = 0.139040
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.128169
Total gradient norm: 0.338535
=== Actor Training Debug (Iteration 6431) ===
Q mean: -10.432688
Q std: 13.410929
Actor loss: 10.436634
Action reg: 0.003946
  l1.weight: grad_norm = 0.314033
  l1.bias: grad_norm = 0.000414
  l2.weight: grad_norm = 0.237843
Total gradient norm: 0.608541
=== Actor Training Debug (Iteration 6432) ===
Q mean: -11.363850
Q std: 15.351958
Actor loss: 11.367808
Action reg: 0.003958
  l1.weight: grad_norm = 0.255408
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.208570
Total gradient norm: 0.538178
=== Actor Training Debug (Iteration 6433) ===
Q mean: -11.276251
Q std: 14.918983
Actor loss: 11.280212
Action reg: 0.003961
  l1.weight: grad_norm = 0.218823
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.178217
Total gradient norm: 0.483126
=== Actor Training Debug (Iteration 6434) ===
Q mean: -11.791504
Q std: 16.642559
Actor loss: 11.795433
Action reg: 0.003930
  l1.weight: grad_norm = 0.240457
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.203942
Total gradient norm: 0.495413
=== Actor Training Debug (Iteration 6435) ===
Q mean: -13.102441
Q std: 17.271969
Actor loss: 13.106395
Action reg: 0.003954
  l1.weight: grad_norm = 0.187029
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.165046
Total gradient norm: 0.494041
=== Actor Training Debug (Iteration 6436) ===
Q mean: -12.185442
Q std: 15.971873
Actor loss: 12.189400
Action reg: 0.003958
  l1.weight: grad_norm = 0.126619
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.102155
Total gradient norm: 0.287831
=== Actor Training Debug (Iteration 6437) ===
Q mean: -11.924174
Q std: 15.457520
Actor loss: 11.928126
Action reg: 0.003952
  l1.weight: grad_norm = 0.249873
  l1.bias: grad_norm = 0.000207
  l2.weight: grad_norm = 0.202528
Total gradient norm: 0.522634
=== Actor Training Debug (Iteration 6438) ===
Q mean: -11.204540
Q std: 15.071424
Actor loss: 11.208513
Action reg: 0.003973
  l1.weight: grad_norm = 0.241304
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.218944
Total gradient norm: 0.632768
=== Actor Training Debug (Iteration 6439) ===
Q mean: -13.195380
Q std: 17.394300
Actor loss: 13.199348
Action reg: 0.003969
  l1.weight: grad_norm = 0.189287
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.168896
Total gradient norm: 0.542031
=== Actor Training Debug (Iteration 6440) ===
Q mean: -10.665884
Q std: 15.548057
Actor loss: 10.669836
Action reg: 0.003952
  l1.weight: grad_norm = 0.237947
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.187519
Total gradient norm: 0.437148
=== Actor Training Debug (Iteration 6441) ===
Q mean: -11.874510
Q std: 16.134592
Actor loss: 11.878474
Action reg: 0.003964
  l1.weight: grad_norm = 0.238976
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.198887
Total gradient norm: 0.487460
=== Actor Training Debug (Iteration 6442) ===
Q mean: -12.945511
Q std: 16.886908
Actor loss: 12.949466
Action reg: 0.003955
  l1.weight: grad_norm = 0.110283
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.097965
Total gradient norm: 0.253956
=== Actor Training Debug (Iteration 6443) ===
Q mean: -11.777884
Q std: 17.553421
Actor loss: 11.781837
Action reg: 0.003953
  l1.weight: grad_norm = 0.261272
  l1.bias: grad_norm = 0.001566
  l2.weight: grad_norm = 0.219583
Total gradient norm: 0.603615
=== Actor Training Debug (Iteration 6444) ===
Q mean: -11.420746
Q std: 15.355369
Actor loss: 11.424716
Action reg: 0.003970
  l1.weight: grad_norm = 0.181426
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.142421
Total gradient norm: 0.383790
=== Actor Training Debug (Iteration 6445) ===
Q mean: -9.586683
Q std: 13.854339
Actor loss: 9.590643
Action reg: 0.003960
  l1.weight: grad_norm = 0.643359
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.457950
Total gradient norm: 1.072865
=== Actor Training Debug (Iteration 6446) ===
Q mean: -11.785925
Q std: 15.119146
Actor loss: 11.789890
Action reg: 0.003965
  l1.weight: grad_norm = 0.210310
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.163635
Total gradient norm: 0.418182
=== Actor Training Debug (Iteration 6447) ===
Q mean: -11.144735
Q std: 14.724734
Actor loss: 11.148670
Action reg: 0.003935
  l1.weight: grad_norm = 0.233651
  l1.bias: grad_norm = 0.000371
  l2.weight: grad_norm = 0.212022
Total gradient norm: 0.597100
=== Actor Training Debug (Iteration 6448) ===
Q mean: -11.142336
Q std: 16.121273
Actor loss: 11.146291
Action reg: 0.003955
  l1.weight: grad_norm = 0.207300
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.161395
Total gradient norm: 0.436560
=== Actor Training Debug (Iteration 6449) ===
Q mean: -10.017942
Q std: 15.280272
Actor loss: 10.021894
Action reg: 0.003952
  l1.weight: grad_norm = 0.295912
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.249164
Total gradient norm: 0.657786
=== Actor Training Debug (Iteration 6450) ===
Q mean: -11.210568
Q std: 15.838533
Actor loss: 11.214525
Action reg: 0.003957
  l1.weight: grad_norm = 0.176883
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.143475
Total gradient norm: 0.430814
=== Actor Training Debug (Iteration 6451) ===
Q mean: -12.253565
Q std: 15.627938
Actor loss: 12.257519
Action reg: 0.003954
  l1.weight: grad_norm = 0.160375
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.147253
Total gradient norm: 0.416258
=== Actor Training Debug (Iteration 6452) ===
Q mean: -10.822330
Q std: 14.800626
Actor loss: 10.826281
Action reg: 0.003950
  l1.weight: grad_norm = 0.170214
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.128900
Total gradient norm: 0.338652
=== Actor Training Debug (Iteration 6453) ===
Q mean: -11.045069
Q std: 16.531769
Actor loss: 11.049016
Action reg: 0.003947
  l1.weight: grad_norm = 0.377605
  l1.bias: grad_norm = 0.000433
  l2.weight: grad_norm = 0.288292
Total gradient norm: 0.663051
=== Actor Training Debug (Iteration 6454) ===
Q mean: -13.143075
Q std: 16.543676
Actor loss: 13.147031
Action reg: 0.003956
  l1.weight: grad_norm = 0.146394
  l1.bias: grad_norm = 0.000295
  l2.weight: grad_norm = 0.125350
Total gradient norm: 0.360010
=== Actor Training Debug (Iteration 6455) ===
Q mean: -11.475969
Q std: 15.899587
Actor loss: 11.479922
Action reg: 0.003953
  l1.weight: grad_norm = 0.277128
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.234750
Total gradient norm: 0.566055
=== Actor Training Debug (Iteration 6456) ===
Q mean: -12.578131
Q std: 17.720936
Actor loss: 12.582100
Action reg: 0.003969
  l1.weight: grad_norm = 0.230158
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.176873
Total gradient norm: 0.456109
=== Actor Training Debug (Iteration 6457) ===
Q mean: -13.602926
Q std: 16.785049
Actor loss: 13.606891
Action reg: 0.003964
  l1.weight: grad_norm = 0.201997
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.176872
Total gradient norm: 0.507243
=== Actor Training Debug (Iteration 6458) ===
Q mean: -13.434301
Q std: 17.027889
Actor loss: 13.438278
Action reg: 0.003977
  l1.weight: grad_norm = 0.139068
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.114808
Total gradient norm: 0.282252
=== Actor Training Debug (Iteration 6459) ===
Q mean: -10.849108
Q std: 15.479417
Actor loss: 10.853060
Action reg: 0.003952
  l1.weight: grad_norm = 0.205845
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.174389
Total gradient norm: 0.452892
=== Actor Training Debug (Iteration 6460) ===
Q mean: -10.902771
Q std: 15.506706
Actor loss: 10.906714
Action reg: 0.003944
  l1.weight: grad_norm = 0.298911
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.239749
Total gradient norm: 0.669647
=== Actor Training Debug (Iteration 6461) ===
Q mean: -11.277197
Q std: 16.595730
Actor loss: 11.281154
Action reg: 0.003957
  l1.weight: grad_norm = 0.213496
  l1.bias: grad_norm = 0.001423
  l2.weight: grad_norm = 0.181724
Total gradient norm: 0.558870
=== Actor Training Debug (Iteration 6462) ===
Q mean: -11.811600
Q std: 15.257158
Actor loss: 11.815551
Action reg: 0.003951
  l1.weight: grad_norm = 0.260824
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.210439
Total gradient norm: 0.558087
=== Actor Training Debug (Iteration 6463) ===
Q mean: -11.622306
Q std: 16.035976
Actor loss: 11.626236
Action reg: 0.003930
  l1.weight: grad_norm = 0.249531
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.196075
Total gradient norm: 0.566903
=== Actor Training Debug (Iteration 6464) ===
Q mean: -11.832243
Q std: 14.929206
Actor loss: 11.836202
Action reg: 0.003959
  l1.weight: grad_norm = 0.109399
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.100863
Total gradient norm: 0.281845
=== Actor Training Debug (Iteration 6465) ===
Q mean: -11.702031
Q std: 16.371010
Actor loss: 11.705986
Action reg: 0.003955
  l1.weight: grad_norm = 0.193809
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.163227
Total gradient norm: 0.446835
=== Actor Training Debug (Iteration 6466) ===
Q mean: -10.727276
Q std: 13.455447
Actor loss: 10.731255
Action reg: 0.003978
  l1.weight: grad_norm = 0.172408
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.149892
Total gradient norm: 0.409359
=== Actor Training Debug (Iteration 6467) ===
Q mean: -11.001753
Q std: 16.059553
Actor loss: 11.005697
Action reg: 0.003944
  l1.weight: grad_norm = 0.299817
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.207368
Total gradient norm: 0.534559
=== Actor Training Debug (Iteration 6468) ===
Q mean: -11.885153
Q std: 15.054699
Actor loss: 11.889110
Action reg: 0.003957
  l1.weight: grad_norm = 0.279005
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.232273
Total gradient norm: 0.719755
=== Actor Training Debug (Iteration 6469) ===
Q mean: -11.465515
Q std: 16.959787
Actor loss: 11.469471
Action reg: 0.003956
  l1.weight: grad_norm = 0.216907
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.197319
Total gradient norm: 0.599993
=== Actor Training Debug (Iteration 6470) ===
Q mean: -11.144377
Q std: 16.419636
Actor loss: 11.148335
Action reg: 0.003958
  l1.weight: grad_norm = 0.106187
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.091707
Total gradient norm: 0.237185
=== Actor Training Debug (Iteration 6471) ===
Q mean: -10.269392
Q std: 15.803393
Actor loss: 10.273346
Action reg: 0.003954
  l1.weight: grad_norm = 0.287599
  l1.bias: grad_norm = 0.000517
  l2.weight: grad_norm = 0.228144
Total gradient norm: 0.569765
=== Actor Training Debug (Iteration 6472) ===
Q mean: -12.765886
Q std: 16.558458
Actor loss: 12.769840
Action reg: 0.003954
  l1.weight: grad_norm = 0.259305
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.223130
Total gradient norm: 0.558699
=== Actor Training Debug (Iteration 6473) ===
Q mean: -11.729336
Q std: 15.621299
Actor loss: 11.733280
Action reg: 0.003945
  l1.weight: grad_norm = 0.167927
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.158348
Total gradient norm: 0.424707
=== Actor Training Debug (Iteration 6474) ===
Q mean: -13.317937
Q std: 16.675489
Actor loss: 13.321898
Action reg: 0.003962
  l1.weight: grad_norm = 0.218842
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.158024
Total gradient norm: 0.464699
=== Actor Training Debug (Iteration 6475) ===
Q mean: -11.084999
Q std: 13.896171
Actor loss: 11.088944
Action reg: 0.003945
  l1.weight: grad_norm = 0.191289
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.153917
Total gradient norm: 0.438894
=== Actor Training Debug (Iteration 6476) ===
Q mean: -10.707021
Q std: 15.080718
Actor loss: 10.710989
Action reg: 0.003968
  l1.weight: grad_norm = 0.162221
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.147208
Total gradient norm: 0.372108
=== Actor Training Debug (Iteration 6477) ===
Q mean: -13.392767
Q std: 16.329781
Actor loss: 13.396713
Action reg: 0.003946
  l1.weight: grad_norm = 0.177755
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.144126
Total gradient norm: 0.397522
=== Actor Training Debug (Iteration 6478) ===
Q mean: -10.866989
Q std: 15.229027
Actor loss: 10.870952
Action reg: 0.003962
  l1.weight: grad_norm = 0.165364
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.147433
Total gradient norm: 0.376359
=== Actor Training Debug (Iteration 6479) ===
Q mean: -10.969020
Q std: 16.372040
Actor loss: 10.972978
Action reg: 0.003957
  l1.weight: grad_norm = 0.190388
  l1.bias: grad_norm = 0.000066
  l2.weight: grad_norm = 0.162212
Total gradient norm: 0.420675
=== Actor Training Debug (Iteration 6480) ===
Q mean: -10.089394
Q std: 13.686874
Actor loss: 10.093354
Action reg: 0.003961
  l1.weight: grad_norm = 0.309457
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.236974
Total gradient norm: 0.611491
=== Actor Training Debug (Iteration 6481) ===
Q mean: -14.034800
Q std: 16.352072
Actor loss: 14.038754
Action reg: 0.003954
  l1.weight: grad_norm = 0.194592
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.180034
Total gradient norm: 0.517808
=== Actor Training Debug (Iteration 6482) ===
Q mean: -11.820956
Q std: 16.894882
Actor loss: 11.824892
Action reg: 0.003936
  l1.weight: grad_norm = 0.187573
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.170838
Total gradient norm: 0.466678
=== Actor Training Debug (Iteration 6483) ===
Q mean: -11.779550
Q std: 16.999989
Actor loss: 11.783518
Action reg: 0.003968
  l1.weight: grad_norm = 0.143594
  l1.bias: grad_norm = 0.000063
  l2.weight: grad_norm = 0.115004
Total gradient norm: 0.296039
=== Actor Training Debug (Iteration 6484) ===
Q mean: -10.371159
Q std: 13.089551
Actor loss: 10.375109
Action reg: 0.003950
  l1.weight: grad_norm = 0.233597
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.204387
Total gradient norm: 0.562728
=== Actor Training Debug (Iteration 6485) ===
Q mean: -11.721376
Q std: 15.751117
Actor loss: 11.725337
Action reg: 0.003961
  l1.weight: grad_norm = 0.187805
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.156983
Total gradient norm: 0.407960
=== Actor Training Debug (Iteration 6486) ===
Q mean: -10.629307
Q std: 13.859508
Actor loss: 10.633280
Action reg: 0.003973
  l1.weight: grad_norm = 0.210856
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.157208
Total gradient norm: 0.445165
=== Actor Training Debug (Iteration 6487) ===
Q mean: -9.754199
Q std: 15.252496
Actor loss: 9.758160
Action reg: 0.003961
  l1.weight: grad_norm = 0.154829
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.140763
Total gradient norm: 0.383198
=== Actor Training Debug (Iteration 6488) ===
Q mean: -11.530578
Q std: 15.798827
Actor loss: 11.534545
Action reg: 0.003967
  l1.weight: grad_norm = 0.156524
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.132013
Total gradient norm: 0.351168
=== Actor Training Debug (Iteration 6489) ===
Q mean: -12.330774
Q std: 17.245193
Actor loss: 12.334720
Action reg: 0.003946
  l1.weight: grad_norm = 0.190921
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.159598
Total gradient norm: 0.403553
=== Actor Training Debug (Iteration 6490) ===
Q mean: -12.342579
Q std: 15.343977
Actor loss: 12.346541
Action reg: 0.003962
  l1.weight: grad_norm = 0.189678
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.164655
Total gradient norm: 0.447983
=== Actor Training Debug (Iteration 6491) ===
Q mean: -11.322224
Q std: 15.036451
Actor loss: 11.326186
Action reg: 0.003963
  l1.weight: grad_norm = 0.183476
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.161261
Total gradient norm: 0.405626
=== Actor Training Debug (Iteration 6492) ===
Q mean: -12.226243
Q std: 16.663582
Actor loss: 12.230204
Action reg: 0.003961
  l1.weight: grad_norm = 0.198585
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.150894
Total gradient norm: 0.461493
=== Actor Training Debug (Iteration 6493) ===
Q mean: -10.484468
Q std: 14.740003
Actor loss: 10.488426
Action reg: 0.003958
  l1.weight: grad_norm = 0.190772
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.147013
Total gradient norm: 0.377914
=== Actor Training Debug (Iteration 6494) ===
Q mean: -11.535958
Q std: 15.661989
Actor loss: 11.539894
Action reg: 0.003936
  l1.weight: grad_norm = 0.318615
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.283038
Total gradient norm: 0.851163
=== Actor Training Debug (Iteration 6495) ===
Q mean: -12.851527
Q std: 16.820457
Actor loss: 12.855480
Action reg: 0.003953
  l1.weight: grad_norm = 0.333294
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.258242
Total gradient norm: 0.676156
=== Actor Training Debug (Iteration 6496) ===
Q mean: -12.082656
Q std: 16.246344
Actor loss: 12.086610
Action reg: 0.003953
  l1.weight: grad_norm = 0.279428
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.230303
Total gradient norm: 0.599110
=== Actor Training Debug (Iteration 6497) ===
Q mean: -11.268217
Q std: 15.086875
Actor loss: 11.272137
Action reg: 0.003920
  l1.weight: grad_norm = 0.330118
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.279569
Total gradient norm: 0.754682
=== Actor Training Debug (Iteration 6498) ===
Q mean: -10.696331
Q std: 15.204842
Actor loss: 10.700277
Action reg: 0.003947
  l1.weight: grad_norm = 0.263568
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.211421
Total gradient norm: 0.571905
=== Actor Training Debug (Iteration 6499) ===
Q mean: -11.154833
Q std: 14.105865
Actor loss: 11.158784
Action reg: 0.003951
  l1.weight: grad_norm = 0.259591
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.225769
Total gradient norm: 0.550691
=== Actor Training Debug (Iteration 6500) ===
Q mean: -12.877392
Q std: 17.709244
Actor loss: 12.881337
Action reg: 0.003946
  l1.weight: grad_norm = 0.133507
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.123639
Total gradient norm: 0.356780
  Average reward: -313.609 | Average length: 100.0
Evaluation at episode 115: -313.609
=== Actor Training Debug (Iteration 6501) ===
Q mean: -11.410313
Q std: 15.565336
Actor loss: 11.414274
Action reg: 0.003962
  l1.weight: grad_norm = 0.138124
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.121694
Total gradient norm: 0.338090
=== Actor Training Debug (Iteration 6502) ===
Q mean: -12.438263
Q std: 16.358364
Actor loss: 12.442203
Action reg: 0.003939
  l1.weight: grad_norm = 0.265758
  l1.bias: grad_norm = 0.000405
  l2.weight: grad_norm = 0.193281
Total gradient norm: 0.576484
=== Actor Training Debug (Iteration 6503) ===
Q mean: -11.871565
Q std: 14.707789
Actor loss: 11.875523
Action reg: 0.003957
  l1.weight: grad_norm = 0.231537
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.190269
Total gradient norm: 0.506302
=== Actor Training Debug (Iteration 6504) ===
Q mean: -12.051719
Q std: 15.642393
Actor loss: 12.055684
Action reg: 0.003965
  l1.weight: grad_norm = 0.181380
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.163666
Total gradient norm: 0.456769
=== Actor Training Debug (Iteration 6505) ===
Q mean: -11.324754
Q std: 15.671006
Actor loss: 11.328718
Action reg: 0.003964
  l1.weight: grad_norm = 0.177375
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.164254
Total gradient norm: 0.460379
=== Actor Training Debug (Iteration 6506) ===
Q mean: -11.899626
Q std: 15.248477
Actor loss: 11.903590
Action reg: 0.003965
  l1.weight: grad_norm = 0.226680
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.182491
Total gradient norm: 0.472844
=== Actor Training Debug (Iteration 6507) ===
Q mean: -13.196960
Q std: 17.068033
Actor loss: 13.200928
Action reg: 0.003967
  l1.weight: grad_norm = 0.209345
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.174037
Total gradient norm: 0.483901
=== Actor Training Debug (Iteration 6508) ===
Q mean: -12.167809
Q std: 16.911823
Actor loss: 12.171758
Action reg: 0.003948
  l1.weight: grad_norm = 0.259046
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.214470
Total gradient norm: 0.572095
=== Actor Training Debug (Iteration 6509) ===
Q mean: -12.039080
Q std: 17.287571
Actor loss: 12.043032
Action reg: 0.003952
  l1.weight: grad_norm = 0.396204
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.317874
Total gradient norm: 0.789989
=== Actor Training Debug (Iteration 6510) ===
Q mean: -11.761168
Q std: 15.865236
Actor loss: 11.765121
Action reg: 0.003953
  l1.weight: grad_norm = 0.308297
  l1.bias: grad_norm = 0.000396
  l2.weight: grad_norm = 0.300046
Total gradient norm: 0.817927
=== Actor Training Debug (Iteration 6511) ===
Q mean: -12.438402
Q std: 16.940262
Actor loss: 12.442368
Action reg: 0.003965
  l1.weight: grad_norm = 0.179091
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.144467
Total gradient norm: 0.354256
=== Actor Training Debug (Iteration 6512) ===
Q mean: -12.103726
Q std: 18.065708
Actor loss: 12.107683
Action reg: 0.003957
  l1.weight: grad_norm = 0.339553
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.277599
Total gradient norm: 0.722775
=== Actor Training Debug (Iteration 6513) ===
Q mean: -13.892051
Q std: 18.716129
Actor loss: 13.895997
Action reg: 0.003946
  l1.weight: grad_norm = 0.341511
  l1.bias: grad_norm = 0.001682
  l2.weight: grad_norm = 0.255125
Total gradient norm: 0.734221
=== Actor Training Debug (Iteration 6514) ===
Q mean: -10.969777
Q std: 14.523844
Actor loss: 10.973741
Action reg: 0.003963
  l1.weight: grad_norm = 0.251927
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.220926
Total gradient norm: 0.590506
=== Actor Training Debug (Iteration 6515) ===
Q mean: -13.759864
Q std: 17.386152
Actor loss: 13.763823
Action reg: 0.003958
  l1.weight: grad_norm = 0.230089
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.196759
Total gradient norm: 0.520032
=== Actor Training Debug (Iteration 6516) ===
Q mean: -10.805689
Q std: 15.955801
Actor loss: 10.809651
Action reg: 0.003962
  l1.weight: grad_norm = 0.100127
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.081306
Total gradient norm: 0.222229
=== Actor Training Debug (Iteration 6517) ===
Q mean: -12.614367
Q std: 17.008242
Actor loss: 12.618325
Action reg: 0.003957
  l1.weight: grad_norm = 0.315120
  l1.bias: grad_norm = 0.001036
  l2.weight: grad_norm = 0.250138
Total gradient norm: 0.600024
=== Actor Training Debug (Iteration 6518) ===
Q mean: -9.730157
Q std: 13.619947
Actor loss: 9.734128
Action reg: 0.003971
  l1.weight: grad_norm = 0.270458
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.217374
Total gradient norm: 0.577081
=== Actor Training Debug (Iteration 6519) ===
Q mean: -11.442499
Q std: 15.531610
Actor loss: 11.446465
Action reg: 0.003967
  l1.weight: grad_norm = 0.234029
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.188234
Total gradient norm: 0.478348
=== Actor Training Debug (Iteration 6520) ===
Q mean: -10.745781
Q std: 14.887360
Actor loss: 10.749731
Action reg: 0.003951
  l1.weight: grad_norm = 0.190070
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.156339
Total gradient norm: 0.423207
=== Actor Training Debug (Iteration 6521) ===
Q mean: -11.294852
Q std: 15.700102
Actor loss: 11.298792
Action reg: 0.003939
  l1.weight: grad_norm = 0.202353
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.163200
Total gradient norm: 0.492289
=== Actor Training Debug (Iteration 6522) ===
Q mean: -11.805813
Q std: 17.225996
Actor loss: 11.809766
Action reg: 0.003953
  l1.weight: grad_norm = 0.113799
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.093345
Total gradient norm: 0.236574
=== Actor Training Debug (Iteration 6523) ===
Q mean: -12.085228
Q std: 16.106609
Actor loss: 12.089172
Action reg: 0.003944
  l1.weight: grad_norm = 0.102690
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.090268
Total gradient norm: 0.263885
=== Actor Training Debug (Iteration 6524) ===
Q mean: -11.783754
Q std: 15.531227
Actor loss: 11.787716
Action reg: 0.003961
  l1.weight: grad_norm = 0.156260
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.142308
Total gradient norm: 0.365686
=== Actor Training Debug (Iteration 6525) ===
Q mean: -12.947821
Q std: 17.400526
Actor loss: 12.951778
Action reg: 0.003958
  l1.weight: grad_norm = 0.252494
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.211976
Total gradient norm: 0.618171
=== Actor Training Debug (Iteration 6526) ===
Q mean: -13.059421
Q std: 16.871353
Actor loss: 13.063373
Action reg: 0.003952
  l1.weight: grad_norm = 0.252021
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.209522
Total gradient norm: 0.644672
=== Actor Training Debug (Iteration 6527) ===
Q mean: -11.833354
Q std: 15.724395
Actor loss: 11.837309
Action reg: 0.003955
  l1.weight: grad_norm = 0.193194
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.152303
Total gradient norm: 0.448469
=== Actor Training Debug (Iteration 6528) ===
Q mean: -10.139798
Q std: 13.778769
Actor loss: 10.143758
Action reg: 0.003960
  l1.weight: grad_norm = 0.204880
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.177306
Total gradient norm: 0.461345
=== Actor Training Debug (Iteration 6529) ===
Q mean: -10.994534
Q std: 15.392163
Actor loss: 10.998475
Action reg: 0.003941
  l1.weight: grad_norm = 0.129148
  l1.bias: grad_norm = 0.000309
  l2.weight: grad_norm = 0.098155
Total gradient norm: 0.279446
=== Actor Training Debug (Iteration 6530) ===
Q mean: -12.170057
Q std: 17.003735
Actor loss: 12.174017
Action reg: 0.003960
  l1.weight: grad_norm = 0.188208
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.142112
Total gradient norm: 0.362814
=== Actor Training Debug (Iteration 6531) ===
Q mean: -11.625713
Q std: 16.577600
Actor loss: 11.629671
Action reg: 0.003958
  l1.weight: grad_norm = 0.136484
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.123496
Total gradient norm: 0.329437
=== Actor Training Debug (Iteration 6532) ===
Q mean: -11.662850
Q std: 15.358586
Actor loss: 11.666819
Action reg: 0.003969
  l1.weight: grad_norm = 0.062945
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.059609
Total gradient norm: 0.151891
=== Actor Training Debug (Iteration 6533) ===
Q mean: -11.993370
Q std: 15.696983
Actor loss: 11.997317
Action reg: 0.003947
  l1.weight: grad_norm = 0.178311
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.155178
Total gradient norm: 0.423277
=== Actor Training Debug (Iteration 6534) ===
Q mean: -11.015077
Q std: 15.966487
Actor loss: 11.019041
Action reg: 0.003964
  l1.weight: grad_norm = 0.254036
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.209941
Total gradient norm: 0.581386
=== Actor Training Debug (Iteration 6535) ===
Q mean: -11.647644
Q std: 15.929720
Actor loss: 11.651601
Action reg: 0.003957
  l1.weight: grad_norm = 0.139898
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.113127
Total gradient norm: 0.303517
=== Actor Training Debug (Iteration 6536) ===
Q mean: -10.808450
Q std: 15.135036
Actor loss: 10.812414
Action reg: 0.003965
  l1.weight: grad_norm = 0.205224
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.183104
Total gradient norm: 0.429437
=== Actor Training Debug (Iteration 6537) ===
Q mean: -12.518230
Q std: 15.938601
Actor loss: 12.522194
Action reg: 0.003963
  l1.weight: grad_norm = 0.090315
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.070085
Total gradient norm: 0.192935
=== Actor Training Debug (Iteration 6538) ===
Q mean: -12.379978
Q std: 15.702055
Actor loss: 12.383904
Action reg: 0.003925
  l1.weight: grad_norm = 0.322068
  l1.bias: grad_norm = 0.000618
  l2.weight: grad_norm = 0.258427
Total gradient norm: 0.748966
=== Actor Training Debug (Iteration 6539) ===
Q mean: -11.453138
Q std: 14.264864
Actor loss: 11.457098
Action reg: 0.003960
  l1.weight: grad_norm = 0.593173
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.436676
Total gradient norm: 1.091063
=== Actor Training Debug (Iteration 6540) ===
Q mean: -12.850670
Q std: 15.364858
Actor loss: 12.854619
Action reg: 0.003949
  l1.weight: grad_norm = 0.253117
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.204518
Total gradient norm: 0.501457
=== Actor Training Debug (Iteration 6541) ===
Q mean: -11.361523
Q std: 15.081779
Actor loss: 11.365479
Action reg: 0.003957
  l1.weight: grad_norm = 0.287928
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.229758
Total gradient norm: 0.636690
=== Actor Training Debug (Iteration 6542) ===
Q mean: -11.819515
Q std: 16.192783
Actor loss: 11.823480
Action reg: 0.003964
  l1.weight: grad_norm = 0.305499
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.234703
Total gradient norm: 0.618223
=== Actor Training Debug (Iteration 6543) ===
Q mean: -9.618836
Q std: 14.693666
Actor loss: 9.622756
Action reg: 0.003919
  l1.weight: grad_norm = 0.328404
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.254046
Total gradient norm: 0.633677
=== Actor Training Debug (Iteration 6544) ===
Q mean: -11.785769
Q std: 16.481859
Actor loss: 11.789707
Action reg: 0.003939
  l1.weight: grad_norm = 0.332534
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.280867
Total gradient norm: 0.748235
=== Actor Training Debug (Iteration 6545) ===
Q mean: -11.987162
Q std: 16.795824
Actor loss: 11.991126
Action reg: 0.003964
  l1.weight: grad_norm = 0.235702
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.201042
Total gradient norm: 0.535986
=== Actor Training Debug (Iteration 6546) ===
Q mean: -12.948330
Q std: 18.348122
Actor loss: 12.952284
Action reg: 0.003954
  l1.weight: grad_norm = 0.156692
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.134786
Total gradient norm: 0.347755
=== Actor Training Debug (Iteration 6547) ===
Q mean: -11.265984
Q std: 15.244268
Actor loss: 11.269943
Action reg: 0.003959
  l1.weight: grad_norm = 0.233589
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.206647
Total gradient norm: 0.521004
=== Actor Training Debug (Iteration 6548) ===
Q mean: -10.842922
Q std: 15.148730
Actor loss: 10.846886
Action reg: 0.003963
  l1.weight: grad_norm = 0.199810
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.178749
Total gradient norm: 0.473565
=== Actor Training Debug (Iteration 6549) ===
Q mean: -10.298370
Q std: 14.580206
Actor loss: 10.302318
Action reg: 0.003947
  l1.weight: grad_norm = 0.547907
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.513416
Total gradient norm: 1.368550
=== Actor Training Debug (Iteration 6550) ===
Q mean: -12.255104
Q std: 15.337941
Actor loss: 12.259070
Action reg: 0.003966
  l1.weight: grad_norm = 0.243474
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.216128
Total gradient norm: 0.541616
=== Actor Training Debug (Iteration 6551) ===
Q mean: -12.137351
Q std: 16.549103
Actor loss: 12.141296
Action reg: 0.003945
  l1.weight: grad_norm = 0.210396
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.182474
Total gradient norm: 0.520305
=== Actor Training Debug (Iteration 6552) ===
Q mean: -10.487619
Q std: 15.271575
Actor loss: 10.491601
Action reg: 0.003981
  l1.weight: grad_norm = 0.123415
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.112865
Total gradient norm: 0.290434
=== Actor Training Debug (Iteration 6553) ===
Q mean: -12.178909
Q std: 17.226618
Actor loss: 12.182865
Action reg: 0.003956
  l1.weight: grad_norm = 0.251919
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.206930
Total gradient norm: 0.537250
=== Actor Training Debug (Iteration 6554) ===
Q mean: -12.396481
Q std: 17.631933
Actor loss: 12.400447
Action reg: 0.003967
  l1.weight: grad_norm = 0.220011
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.180074
Total gradient norm: 0.481774
=== Actor Training Debug (Iteration 6555) ===
Q mean: -12.078608
Q std: 15.275413
Actor loss: 12.082579
Action reg: 0.003971
  l1.weight: grad_norm = 0.107534
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.083746
Total gradient norm: 0.263180
=== Actor Training Debug (Iteration 6556) ===
Q mean: -12.522597
Q std: 17.066942
Actor loss: 12.526546
Action reg: 0.003948
  l1.weight: grad_norm = 0.135290
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.115723
Total gradient norm: 0.347474
=== Actor Training Debug (Iteration 6557) ===
Q mean: -10.960653
Q std: 15.422221
Actor loss: 10.964607
Action reg: 0.003954
  l1.weight: grad_norm = 0.338617
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.246494
Total gradient norm: 0.653181
=== Actor Training Debug (Iteration 6558) ===
Q mean: -12.216625
Q std: 16.614719
Actor loss: 12.220595
Action reg: 0.003970
  l1.weight: grad_norm = 0.169500
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.155786
Total gradient norm: 0.396283
=== Actor Training Debug (Iteration 6559) ===
Q mean: -10.869355
Q std: 13.988378
Actor loss: 10.873313
Action reg: 0.003958
  l1.weight: grad_norm = 0.250244
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.207116
Total gradient norm: 0.528227
=== Actor Training Debug (Iteration 6560) ===
Q mean: -13.241903
Q std: 17.874535
Actor loss: 13.245854
Action reg: 0.003951
  l1.weight: grad_norm = 0.165336
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.150899
Total gradient norm: 0.422337
=== Actor Training Debug (Iteration 6561) ===
Q mean: -12.897305
Q std: 16.810469
Actor loss: 12.901247
Action reg: 0.003942
  l1.weight: grad_norm = 0.274492
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.228742
Total gradient norm: 0.587388
=== Actor Training Debug (Iteration 6562) ===
Q mean: -12.815608
Q std: 15.790492
Actor loss: 12.819563
Action reg: 0.003955
  l1.weight: grad_norm = 0.149097
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.137652
Total gradient norm: 0.400612
=== Actor Training Debug (Iteration 6563) ===
Q mean: -11.253984
Q std: 15.380071
Actor loss: 11.257946
Action reg: 0.003962
  l1.weight: grad_norm = 0.152397
  l1.bias: grad_norm = 0.000989
  l2.weight: grad_norm = 0.136827
Total gradient norm: 0.374468
=== Actor Training Debug (Iteration 6564) ===
Q mean: -12.690361
Q std: 15.286552
Actor loss: 12.694326
Action reg: 0.003965
  l1.weight: grad_norm = 0.394558
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.356558
Total gradient norm: 0.992721
=== Actor Training Debug (Iteration 6565) ===
Q mean: -12.101429
Q std: 17.500340
Actor loss: 12.105379
Action reg: 0.003950
  l1.weight: grad_norm = 0.330749
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.319416
Total gradient norm: 0.887264
=== Actor Training Debug (Iteration 6566) ===
Q mean: -10.951834
Q std: 16.481342
Actor loss: 10.955779
Action reg: 0.003945
  l1.weight: grad_norm = 0.195089
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.165129
Total gradient norm: 0.433093
=== Actor Training Debug (Iteration 6567) ===
Q mean: -10.913773
Q std: 16.840069
Actor loss: 10.917724
Action reg: 0.003951
  l1.weight: grad_norm = 0.215072
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.192474
Total gradient norm: 0.548539
=== Actor Training Debug (Iteration 6568) ===
Q mean: -13.208834
Q std: 16.536421
Actor loss: 13.212786
Action reg: 0.003952
  l1.weight: grad_norm = 0.141396
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.126748
Total gradient norm: 0.382932
=== Actor Training Debug (Iteration 6569) ===
Q mean: -12.316400
Q std: 16.484869
Actor loss: 12.320331
Action reg: 0.003931
  l1.weight: grad_norm = 0.254086
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.219380
Total gradient norm: 0.611774
=== Actor Training Debug (Iteration 6570) ===
Q mean: -13.103262
Q std: 17.108902
Actor loss: 13.107235
Action reg: 0.003973
  l1.weight: grad_norm = 0.134231
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.114001
Total gradient norm: 0.333570
=== Actor Training Debug (Iteration 6571) ===
Q mean: -12.292355
Q std: 16.759716
Actor loss: 12.296296
Action reg: 0.003942
  l1.weight: grad_norm = 0.205293
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.197393
Total gradient norm: 0.488676
=== Actor Training Debug (Iteration 6572) ===
Q mean: -9.560179
Q std: 14.065126
Actor loss: 9.564122
Action reg: 0.003944
  l1.weight: grad_norm = 0.206777
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.184999
Total gradient norm: 0.515011
=== Actor Training Debug (Iteration 6573) ===
Q mean: -10.427791
Q std: 14.457031
Actor loss: 10.431764
Action reg: 0.003973
  l1.weight: grad_norm = 0.165155
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.141671
Total gradient norm: 0.442296
=== Actor Training Debug (Iteration 6574) ===
Q mean: -10.500686
Q std: 15.118767
Actor loss: 10.504641
Action reg: 0.003955
  l1.weight: grad_norm = 0.107549
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.090969
Total gradient norm: 0.251041
=== Actor Training Debug (Iteration 6575) ===
Q mean: -11.888836
Q std: 15.023209
Actor loss: 11.892815
Action reg: 0.003979
  l1.weight: grad_norm = 0.207004
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.166923
Total gradient norm: 0.459304
=== Actor Training Debug (Iteration 6576) ===
Q mean: -12.635943
Q std: 14.744375
Actor loss: 12.639904
Action reg: 0.003961
  l1.weight: grad_norm = 0.134049
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.122210
Total gradient norm: 0.380999
=== Actor Training Debug (Iteration 6577) ===
Q mean: -9.771219
Q std: 13.904942
Actor loss: 9.775166
Action reg: 0.003947
  l1.weight: grad_norm = 0.170067
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.152897
Total gradient norm: 0.375321
=== Actor Training Debug (Iteration 6578) ===
Q mean: -12.069447
Q std: 16.872654
Actor loss: 12.073404
Action reg: 0.003958
  l1.weight: grad_norm = 0.138820
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.129569
Total gradient norm: 0.350542
=== Actor Training Debug (Iteration 6579) ===
Q mean: -10.938307
Q std: 14.998508
Actor loss: 10.942274
Action reg: 0.003968
  l1.weight: grad_norm = 0.171036
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.143165
Total gradient norm: 0.385277
=== Actor Training Debug (Iteration 6580) ===
Q mean: -12.116714
Q std: 17.898020
Actor loss: 12.120660
Action reg: 0.003946
  l1.weight: grad_norm = 0.188749
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.161368
Total gradient norm: 0.432397
=== Actor Training Debug (Iteration 6581) ===
Q mean: -12.733628
Q std: 17.700569
Actor loss: 12.737599
Action reg: 0.003971
  l1.weight: grad_norm = 0.120321
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.102965
Total gradient norm: 0.253464
=== Actor Training Debug (Iteration 6582) ===
Q mean: -13.272011
Q std: 17.470596
Actor loss: 13.275970
Action reg: 0.003959
  l1.weight: grad_norm = 0.238306
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.217580
Total gradient norm: 0.618934
=== Actor Training Debug (Iteration 6583) ===
Q mean: -12.095670
Q std: 15.791302
Actor loss: 12.099630
Action reg: 0.003961
  l1.weight: grad_norm = 0.323502
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.270035
Total gradient norm: 0.625672
=== Actor Training Debug (Iteration 6584) ===
Q mean: -13.194714
Q std: 18.268959
Actor loss: 13.198674
Action reg: 0.003961
  l1.weight: grad_norm = 0.168796
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.147914
Total gradient norm: 0.391726
=== Actor Training Debug (Iteration 6585) ===
Q mean: -11.542925
Q std: 15.943993
Actor loss: 11.546887
Action reg: 0.003962
  l1.weight: grad_norm = 0.151455
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.140950
Total gradient norm: 0.383989
=== Actor Training Debug (Iteration 6586) ===
Q mean: -12.931894
Q std: 16.948038
Actor loss: 12.935849
Action reg: 0.003954
  l1.weight: grad_norm = 0.099857
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.097973
Total gradient norm: 0.296430
=== Actor Training Debug (Iteration 6587) ===
Q mean: -13.905771
Q std: 17.704906
Actor loss: 13.909736
Action reg: 0.003965
  l1.weight: grad_norm = 0.173113
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.155002
Total gradient norm: 0.421522
=== Actor Training Debug (Iteration 6588) ===
Q mean: -10.243670
Q std: 14.665759
Actor loss: 10.247621
Action reg: 0.003951
  l1.weight: grad_norm = 0.168878
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.142953
Total gradient norm: 0.364522
=== Actor Training Debug (Iteration 6589) ===
Q mean: -11.023899
Q std: 14.199183
Actor loss: 11.027843
Action reg: 0.003944
  l1.weight: grad_norm = 0.385323
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.297925
Total gradient norm: 0.723755
=== Actor Training Debug (Iteration 6590) ===
Q mean: -11.178300
Q std: 14.536822
Actor loss: 11.182255
Action reg: 0.003954
  l1.weight: grad_norm = 0.196317
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.187403
Total gradient norm: 0.524755
=== Actor Training Debug (Iteration 6591) ===
Q mean: -12.683952
Q std: 16.270956
Actor loss: 12.687903
Action reg: 0.003951
  l1.weight: grad_norm = 0.173993
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.152895
Total gradient norm: 0.403779
=== Actor Training Debug (Iteration 6592) ===
Q mean: -11.237570
Q std: 13.843422
Actor loss: 11.241523
Action reg: 0.003953
  l1.weight: grad_norm = 0.312355
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.257742
Total gradient norm: 0.740878
=== Actor Training Debug (Iteration 6593) ===
Q mean: -12.697056
Q std: 17.036015
Actor loss: 12.701018
Action reg: 0.003963
  l1.weight: grad_norm = 0.145573
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.120573
Total gradient norm: 0.321702
=== Actor Training Debug (Iteration 6594) ===
Q mean: -14.010790
Q std: 17.229471
Actor loss: 14.014752
Action reg: 0.003963
  l1.weight: grad_norm = 0.111091
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.098652
Total gradient norm: 0.253539
=== Actor Training Debug (Iteration 6595) ===
Q mean: -11.174373
Q std: 14.634791
Actor loss: 11.178300
Action reg: 0.003927
  l1.weight: grad_norm = 0.166559
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.142751
Total gradient norm: 0.381586
=== Actor Training Debug (Iteration 6596) ===
Q mean: -10.856370
Q std: 16.075594
Actor loss: 10.860336
Action reg: 0.003967
  l1.weight: grad_norm = 0.210945
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.164186
Total gradient norm: 0.427888
=== Actor Training Debug (Iteration 6597) ===
Q mean: -11.349629
Q std: 15.137137
Actor loss: 11.353579
Action reg: 0.003950
  l1.weight: grad_norm = 0.191541
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.173181
Total gradient norm: 0.499074
=== Actor Training Debug (Iteration 6598) ===
Q mean: -10.547526
Q std: 15.400560
Actor loss: 10.551491
Action reg: 0.003964
  l1.weight: grad_norm = 0.211131
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.168993
Total gradient norm: 0.427004
=== Actor Training Debug (Iteration 6599) ===
Q mean: -12.877096
Q std: 17.026091
Actor loss: 12.881038
Action reg: 0.003942
  l1.weight: grad_norm = 0.350882
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.257434
Total gradient norm: 0.656123
=== Actor Training Debug (Iteration 6600) ===
Q mean: -13.263758
Q std: 18.171442
Actor loss: 13.267707
Action reg: 0.003949
  l1.weight: grad_norm = 0.348435
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.268184
Total gradient norm: 0.733808
=== Actor Training Debug (Iteration 6601) ===
Q mean: -10.670963
Q std: 13.083448
Actor loss: 10.674906
Action reg: 0.003942
  l1.weight: grad_norm = 0.351740
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.310898
Total gradient norm: 0.825901
=== Actor Training Debug (Iteration 6602) ===
Q mean: -10.849648
Q std: 15.013274
Actor loss: 10.853602
Action reg: 0.003954
  l1.weight: grad_norm = 0.310675
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.221659
Total gradient norm: 0.554187
=== Actor Training Debug (Iteration 6603) ===
Q mean: -11.397516
Q std: 16.110422
Actor loss: 11.401467
Action reg: 0.003951
  l1.weight: grad_norm = 0.245103
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.212104
Total gradient norm: 0.593497
=== Actor Training Debug (Iteration 6604) ===
Q mean: -13.647735
Q std: 18.047167
Actor loss: 13.651713
Action reg: 0.003979
  l1.weight: grad_norm = 0.094656
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.081768
Total gradient norm: 0.227843
=== Actor Training Debug (Iteration 6605) ===
Q mean: -11.681285
Q std: 18.085484
Actor loss: 11.685233
Action reg: 0.003948
  l1.weight: grad_norm = 0.366372
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.297604
Total gradient norm: 0.751440
=== Actor Training Debug (Iteration 6606) ===
Q mean: -12.807710
Q std: 16.927908
Actor loss: 12.811669
Action reg: 0.003960
  l1.weight: grad_norm = 0.086754
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.070518
Total gradient norm: 0.182164
=== Actor Training Debug (Iteration 6607) ===
Q mean: -10.418896
Q std: 15.724799
Actor loss: 10.422863
Action reg: 0.003967
  l1.weight: grad_norm = 0.154732
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.128917
Total gradient norm: 0.386703
=== Actor Training Debug (Iteration 6608) ===
Q mean: -10.631033
Q std: 14.832758
Actor loss: 10.634957
Action reg: 0.003924
  l1.weight: grad_norm = 0.257580
  l1.bias: grad_norm = 0.001432
  l2.weight: grad_norm = 0.229652
Total gradient norm: 0.595207
=== Actor Training Debug (Iteration 6609) ===
Q mean: -11.328871
Q std: 16.579554
Actor loss: 11.332801
Action reg: 0.003930
  l1.weight: grad_norm = 0.166699
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.156058
Total gradient norm: 0.413548
=== Actor Training Debug (Iteration 6610) ===
Q mean: -11.698519
Q std: 15.433147
Actor loss: 11.702483
Action reg: 0.003964
  l1.weight: grad_norm = 0.296179
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.239840
Total gradient norm: 0.659154
=== Actor Training Debug (Iteration 6611) ===
Q mean: -13.039073
Q std: 17.505276
Actor loss: 13.043014
Action reg: 0.003941
  l1.weight: grad_norm = 0.352889
  l1.bias: grad_norm = 0.000522
  l2.weight: grad_norm = 0.299079
Total gradient norm: 0.816551
=== Actor Training Debug (Iteration 6612) ===
Q mean: -13.340046
Q std: 17.448263
Actor loss: 13.343996
Action reg: 0.003950
  l1.weight: grad_norm = 0.135792
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.096836
Total gradient norm: 0.269643
=== Actor Training Debug (Iteration 6613) ===
Q mean: -10.516453
Q std: 15.171700
Actor loss: 10.520387
Action reg: 0.003934
  l1.weight: grad_norm = 0.216249
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.176646
Total gradient norm: 0.460330
=== Actor Training Debug (Iteration 6614) ===
Q mean: -11.047565
Q std: 15.558277
Actor loss: 11.051535
Action reg: 0.003969
  l1.weight: grad_norm = 0.131443
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.106379
Total gradient norm: 0.292329
=== Actor Training Debug (Iteration 6615) ===
Q mean: -11.180422
Q std: 14.736928
Actor loss: 11.184374
Action reg: 0.003952
  l1.weight: grad_norm = 0.175028
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.148102
Total gradient norm: 0.369981
=== Actor Training Debug (Iteration 6616) ===
Q mean: -13.089018
Q std: 16.038406
Actor loss: 13.092952
Action reg: 0.003934
  l1.weight: grad_norm = 0.208026
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.173069
Total gradient norm: 0.439198
=== Actor Training Debug (Iteration 6617) ===
Q mean: -12.993685
Q std: 15.554894
Actor loss: 12.997638
Action reg: 0.003953
  l1.weight: grad_norm = 0.319392
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.258364
Total gradient norm: 0.645603
=== Actor Training Debug (Iteration 6618) ===
Q mean: -10.982470
Q std: 12.659679
Actor loss: 10.986421
Action reg: 0.003951
  l1.weight: grad_norm = 0.200605
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.163107
Total gradient norm: 0.433645
=== Actor Training Debug (Iteration 6619) ===
Q mean: -11.506317
Q std: 15.100910
Actor loss: 11.510272
Action reg: 0.003955
  l1.weight: grad_norm = 0.190655
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.163697
Total gradient norm: 0.446363
=== Actor Training Debug (Iteration 6620) ===
Q mean: -10.939053
Q std: 16.074766
Actor loss: 10.943006
Action reg: 0.003953
  l1.weight: grad_norm = 0.287652
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.259342
Total gradient norm: 0.708261
=== Actor Training Debug (Iteration 6621) ===
Q mean: -11.662745
Q std: 15.069049
Actor loss: 11.666705
Action reg: 0.003960
  l1.weight: grad_norm = 0.220289
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.193184
Total gradient norm: 0.475731
=== Actor Training Debug (Iteration 6622) ===
Q mean: -13.113261
Q std: 17.874323
Actor loss: 13.117222
Action reg: 0.003961
  l1.weight: grad_norm = 0.289207
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.226145
Total gradient norm: 0.556312
=== Actor Training Debug (Iteration 6623) ===
Q mean: -12.991150
Q std: 17.017506
Actor loss: 12.995105
Action reg: 0.003955
  l1.weight: grad_norm = 0.142405
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.117117
Total gradient norm: 0.361371
=== Actor Training Debug (Iteration 6624) ===
Q mean: -12.908943
Q std: 17.698254
Actor loss: 12.912925
Action reg: 0.003982
  l1.weight: grad_norm = 0.128141
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.105813
Total gradient norm: 0.287031
=== Actor Training Debug (Iteration 6625) ===
Q mean: -12.794724
Q std: 17.199579
Actor loss: 12.798676
Action reg: 0.003951
  l1.weight: grad_norm = 0.228775
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.216867
Total gradient norm: 0.569171
=== Actor Training Debug (Iteration 6626) ===
Q mean: -10.411270
Q std: 13.466592
Actor loss: 10.415213
Action reg: 0.003942
  l1.weight: grad_norm = 0.177027
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.149391
Total gradient norm: 0.383071
=== Actor Training Debug (Iteration 6627) ===
Q mean: -12.508823
Q std: 17.429890
Actor loss: 12.512788
Action reg: 0.003964
  l1.weight: grad_norm = 0.092947
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.076870
Total gradient norm: 0.203209
=== Actor Training Debug (Iteration 6628) ===
Q mean: -10.870735
Q std: 16.353760
Actor loss: 10.874699
Action reg: 0.003964
  l1.weight: grad_norm = 0.121378
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.107196
Total gradient norm: 0.265778
=== Actor Training Debug (Iteration 6629) ===
Q mean: -10.549356
Q std: 14.574132
Actor loss: 10.553304
Action reg: 0.003948
  l1.weight: grad_norm = 0.224115
  l1.bias: grad_norm = 0.001372
  l2.weight: grad_norm = 0.199089
Total gradient norm: 0.570900
=== Actor Training Debug (Iteration 6630) ===
Q mean: -10.704334
Q std: 15.166289
Actor loss: 10.708312
Action reg: 0.003977
  l1.weight: grad_norm = 0.178470
  l1.bias: grad_norm = 0.000055
  l2.weight: grad_norm = 0.157000
Total gradient norm: 0.413250
=== Actor Training Debug (Iteration 6631) ===
Q mean: -13.063299
Q std: 17.735739
Actor loss: 13.067260
Action reg: 0.003961
  l1.weight: grad_norm = 0.346208
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.327332
Total gradient norm: 0.981986
=== Actor Training Debug (Iteration 6632) ===
Q mean: -12.923951
Q std: 17.758064
Actor loss: 12.927889
Action reg: 0.003938
  l1.weight: grad_norm = 0.229258
  l1.bias: grad_norm = 0.001735
  l2.weight: grad_norm = 0.193959
Total gradient norm: 0.535741
=== Actor Training Debug (Iteration 6633) ===
Q mean: -12.475943
Q std: 17.667072
Actor loss: 12.479911
Action reg: 0.003968
  l1.weight: grad_norm = 0.413995
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.378284
Total gradient norm: 1.037148
=== Actor Training Debug (Iteration 6634) ===
Q mean: -11.859915
Q std: 15.103833
Actor loss: 11.863847
Action reg: 0.003932
  l1.weight: grad_norm = 0.218373
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.162218
Total gradient norm: 0.478539
=== Actor Training Debug (Iteration 6635) ===
Q mean: -11.189144
Q std: 14.156364
Actor loss: 11.193108
Action reg: 0.003963
  l1.weight: grad_norm = 0.114785
  l1.bias: grad_norm = 0.000075
  l2.weight: grad_norm = 0.093500
Total gradient norm: 0.262069
=== Actor Training Debug (Iteration 6636) ===
Q mean: -12.180392
Q std: 16.447851
Actor loss: 12.184344
Action reg: 0.003952
  l1.weight: grad_norm = 0.186088
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.167228
Total gradient norm: 0.463006
=== Actor Training Debug (Iteration 6637) ===
Q mean: -10.599264
Q std: 14.822652
Actor loss: 10.603206
Action reg: 0.003942
  l1.weight: grad_norm = 0.314666
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.236769
Total gradient norm: 0.632803
=== Actor Training Debug (Iteration 6638) ===
Q mean: -12.992839
Q std: 16.450583
Actor loss: 12.996813
Action reg: 0.003974
  l1.weight: grad_norm = 0.234188
  l1.bias: grad_norm = 0.000260
  l2.weight: grad_norm = 0.184429
Total gradient norm: 0.431070
=== Actor Training Debug (Iteration 6639) ===
Q mean: -11.827137
Q std: 16.066771
Actor loss: 11.831104
Action reg: 0.003967
  l1.weight: grad_norm = 0.118967
  l1.bias: grad_norm = 0.000359
  l2.weight: grad_norm = 0.105593
Total gradient norm: 0.277871
=== Actor Training Debug (Iteration 6640) ===
Q mean: -12.002665
Q std: 15.774063
Actor loss: 12.006641
Action reg: 0.003976
  l1.weight: grad_norm = 0.129601
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.107146
Total gradient norm: 0.266191
=== Actor Training Debug (Iteration 6641) ===
Q mean: -10.608934
Q std: 16.328463
Actor loss: 10.612885
Action reg: 0.003950
  l1.weight: grad_norm = 0.257673
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.226600
Total gradient norm: 0.573913
=== Actor Training Debug (Iteration 6642) ===
Q mean: -11.157455
Q std: 14.667867
Actor loss: 11.161419
Action reg: 0.003964
  l1.weight: grad_norm = 0.115079
  l1.bias: grad_norm = 0.000505
  l2.weight: grad_norm = 0.105029
Total gradient norm: 0.282306
=== Actor Training Debug (Iteration 6643) ===
Q mean: -13.625429
Q std: 18.089773
Actor loss: 13.629366
Action reg: 0.003937
  l1.weight: grad_norm = 0.323605
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.290453
Total gradient norm: 0.673537
=== Actor Training Debug (Iteration 6644) ===
Q mean: -11.208964
Q std: 14.099874
Actor loss: 11.212914
Action reg: 0.003950
  l1.weight: grad_norm = 0.164471
  l1.bias: grad_norm = 0.000394
  l2.weight: grad_norm = 0.156047
Total gradient norm: 0.416047
=== Actor Training Debug (Iteration 6645) ===
Q mean: -12.282553
Q std: 15.962823
Actor loss: 12.286500
Action reg: 0.003948
  l1.weight: grad_norm = 0.324114
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.295054
Total gradient norm: 0.738295
=== Actor Training Debug (Iteration 6646) ===
Q mean: -12.565338
Q std: 19.156065
Actor loss: 12.569295
Action reg: 0.003957
  l1.weight: grad_norm = 0.113121
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.096845
Total gradient norm: 0.253524
=== Actor Training Debug (Iteration 6647) ===
Q mean: -11.945629
Q std: 15.179987
Actor loss: 11.949574
Action reg: 0.003945
  l1.weight: grad_norm = 0.213599
  l1.bias: grad_norm = 0.000330
  l2.weight: grad_norm = 0.188072
Total gradient norm: 0.507308
=== Actor Training Debug (Iteration 6648) ===
Q mean: -12.198462
Q std: 15.999197
Actor loss: 12.202419
Action reg: 0.003958
  l1.weight: grad_norm = 0.163715
  l1.bias: grad_norm = 0.000544
  l2.weight: grad_norm = 0.141446
Total gradient norm: 0.359405
=== Actor Training Debug (Iteration 6649) ===
Q mean: -11.763117
Q std: 15.715309
Actor loss: 11.767056
Action reg: 0.003939
  l1.weight: grad_norm = 0.277023
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.259963
Total gradient norm: 0.723127
=== Actor Training Debug (Iteration 6650) ===
Q mean: -10.628531
Q std: 13.683608
Actor loss: 10.632483
Action reg: 0.003952
  l1.weight: grad_norm = 0.193139
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.165819
Total gradient norm: 0.436922
=== Actor Training Debug (Iteration 6651) ===
Q mean: -10.155897
Q std: 14.303510
Actor loss: 10.159864
Action reg: 0.003967
  l1.weight: grad_norm = 0.189184
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.156199
Total gradient norm: 0.416829
=== Actor Training Debug (Iteration 6652) ===
Q mean: -12.087753
Q std: 17.313999
Actor loss: 12.091697
Action reg: 0.003943
  l1.weight: grad_norm = 0.354255
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.304027
Total gradient norm: 0.793297
=== Actor Training Debug (Iteration 6653) ===
Q mean: -12.605299
Q std: 16.456572
Actor loss: 12.609248
Action reg: 0.003949
  l1.weight: grad_norm = 0.176230
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.162003
Total gradient norm: 0.436600
=== Actor Training Debug (Iteration 6654) ===
Q mean: -10.370654
Q std: 15.448650
Actor loss: 10.374617
Action reg: 0.003962
  l1.weight: grad_norm = 0.132789
  l1.bias: grad_norm = 0.001350
  l2.weight: grad_norm = 0.103946
Total gradient norm: 0.297153
=== Actor Training Debug (Iteration 6655) ===
Q mean: -12.498000
Q std: 16.943747
Actor loss: 12.501949
Action reg: 0.003949
  l1.weight: grad_norm = 0.233550
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.211880
Total gradient norm: 0.586379
=== Actor Training Debug (Iteration 6656) ===
Q mean: -11.209524
Q std: 15.450943
Actor loss: 11.213479
Action reg: 0.003955
  l1.weight: grad_norm = 0.204525
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.175342
Total gradient norm: 0.503613
=== Actor Training Debug (Iteration 6657) ===
Q mean: -14.703991
Q std: 18.766901
Actor loss: 14.707930
Action reg: 0.003939
  l1.weight: grad_norm = 0.291926
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.238011
Total gradient norm: 0.604656
=== Actor Training Debug (Iteration 6658) ===
Q mean: -11.475900
Q std: 15.864722
Actor loss: 11.479877
Action reg: 0.003976
  l1.weight: grad_norm = 0.144850
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.129069
Total gradient norm: 0.336559
=== Actor Training Debug (Iteration 6659) ===
Q mean: -12.937949
Q std: 17.795492
Actor loss: 12.941889
Action reg: 0.003939
  l1.weight: grad_norm = 0.286347
  l1.bias: grad_norm = 0.002593
  l2.weight: grad_norm = 0.249058
Total gradient norm: 0.592589
=== Actor Training Debug (Iteration 6660) ===
Q mean: -10.780212
Q std: 15.369730
Actor loss: 10.784163
Action reg: 0.003951
  l1.weight: grad_norm = 0.292802
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.253111
Total gradient norm: 0.711806
=== Actor Training Debug (Iteration 6661) ===
Q mean: -11.825736
Q std: 15.421396
Actor loss: 11.829698
Action reg: 0.003962
  l1.weight: grad_norm = 0.155248
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.136359
Total gradient norm: 0.377487
=== Actor Training Debug (Iteration 6662) ===
Q mean: -12.044130
Q std: 16.364445
Actor loss: 12.048082
Action reg: 0.003952
  l1.weight: grad_norm = 0.190513
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.164590
Total gradient norm: 0.421391
=== Actor Training Debug (Iteration 6663) ===
Q mean: -11.135788
Q std: 13.916991
Actor loss: 11.139743
Action reg: 0.003955
  l1.weight: grad_norm = 0.185209
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.175328
Total gradient norm: 0.468979
=== Actor Training Debug (Iteration 6664) ===
Q mean: -11.390402
Q std: 15.593290
Actor loss: 11.394349
Action reg: 0.003948
  l1.weight: grad_norm = 0.134728
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.117956
Total gradient norm: 0.310277
=== Actor Training Debug (Iteration 6665) ===
Q mean: -12.495356
Q std: 15.300235
Actor loss: 12.499310
Action reg: 0.003954
  l1.weight: grad_norm = 0.182256
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.161665
Total gradient norm: 0.411052
=== Actor Training Debug (Iteration 6666) ===
Q mean: -11.711749
Q std: 16.847696
Actor loss: 11.715705
Action reg: 0.003956
  l1.weight: grad_norm = 0.212879
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.184604
Total gradient norm: 0.541318
=== Actor Training Debug (Iteration 6667) ===
Q mean: -12.120321
Q std: 15.419123
Actor loss: 12.124286
Action reg: 0.003964
  l1.weight: grad_norm = 0.120161
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.109508
Total gradient norm: 0.312278
=== Actor Training Debug (Iteration 6668) ===
Q mean: -11.567774
Q std: 15.727864
Actor loss: 11.571729
Action reg: 0.003955
  l1.weight: grad_norm = 0.186231
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.144929
Total gradient norm: 0.395364
=== Actor Training Debug (Iteration 6669) ===
Q mean: -12.635330
Q std: 15.734605
Actor loss: 12.639273
Action reg: 0.003942
  l1.weight: grad_norm = 0.259842
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.225984
Total gradient norm: 0.658795
=== Actor Training Debug (Iteration 6670) ===
Q mean: -12.950217
Q std: 18.577805
Actor loss: 12.954171
Action reg: 0.003954
  l1.weight: grad_norm = 0.242655
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.194291
Total gradient norm: 0.510912
=== Actor Training Debug (Iteration 6671) ===
Q mean: -12.219992
Q std: 15.729335
Actor loss: 12.223940
Action reg: 0.003948
  l1.weight: grad_norm = 0.255918
  l1.bias: grad_norm = 0.000431
  l2.weight: grad_norm = 0.224359
Total gradient norm: 0.609185
=== Actor Training Debug (Iteration 6672) ===
Q mean: -12.714305
Q std: 15.888572
Actor loss: 12.718270
Action reg: 0.003965
  l1.weight: grad_norm = 0.139239
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.123537
Total gradient norm: 0.346388
=== Actor Training Debug (Iteration 6673) ===
Q mean: -11.485613
Q std: 14.349545
Actor loss: 11.489562
Action reg: 0.003949
  l1.weight: grad_norm = 0.213894
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.187586
Total gradient norm: 0.476423
=== Actor Training Debug (Iteration 6674) ===
Q mean: -10.845634
Q std: 14.518807
Actor loss: 10.849592
Action reg: 0.003957
  l1.weight: grad_norm = 0.342045
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.252672
Total gradient norm: 0.566672
=== Actor Training Debug (Iteration 6675) ===
Q mean: -12.654600
Q std: 16.751951
Actor loss: 12.658544
Action reg: 0.003943
  l1.weight: grad_norm = 0.187691
  l1.bias: grad_norm = 0.000414
  l2.weight: grad_norm = 0.181019
Total gradient norm: 0.492363
=== Actor Training Debug (Iteration 6676) ===
Q mean: -9.941870
Q std: 13.463774
Actor loss: 9.945818
Action reg: 0.003948
  l1.weight: grad_norm = 0.192361
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.144179
Total gradient norm: 0.372197
=== Actor Training Debug (Iteration 6677) ===
Q mean: -12.936487
Q std: 17.626488
Actor loss: 12.940456
Action reg: 0.003969
  l1.weight: grad_norm = 0.132479
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.099654
Total gradient norm: 0.296440
=== Actor Training Debug (Iteration 6678) ===
Q mean: -9.981276
Q std: 14.453720
Actor loss: 9.985229
Action reg: 0.003953
  l1.weight: grad_norm = 0.117950
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.101295
Total gradient norm: 0.286843
=== Actor Training Debug (Iteration 6679) ===
Q mean: -10.153415
Q std: 15.819743
Actor loss: 10.157382
Action reg: 0.003967
  l1.weight: grad_norm = 0.122568
  l1.bias: grad_norm = 0.000047
  l2.weight: grad_norm = 0.109049
Total gradient norm: 0.306212
=== Actor Training Debug (Iteration 6680) ===
Q mean: -11.773890
Q std: 17.089750
Actor loss: 11.777837
Action reg: 0.003947
  l1.weight: grad_norm = 0.193123
  l1.bias: grad_norm = 0.000314
  l2.weight: grad_norm = 0.155362
Total gradient norm: 0.448644
=== Actor Training Debug (Iteration 6681) ===
Q mean: -10.876810
Q std: 14.593199
Actor loss: 10.880736
Action reg: 0.003926
  l1.weight: grad_norm = 0.232402
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.214517
Total gradient norm: 0.588408
=== Actor Training Debug (Iteration 6682) ===
Q mean: -11.756441
Q std: 16.532446
Actor loss: 11.760371
Action reg: 0.003930
  l1.weight: grad_norm = 0.451432
  l1.bias: grad_norm = 0.000352
  l2.weight: grad_norm = 0.371251
Total gradient norm: 0.975146
=== Actor Training Debug (Iteration 6683) ===
Q mean: -11.301813
Q std: 14.661081
Actor loss: 11.305744
Action reg: 0.003931
  l1.weight: grad_norm = 0.228717
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.191333
Total gradient norm: 0.472786
=== Actor Training Debug (Iteration 6684) ===
Q mean: -10.233128
Q std: 14.335160
Actor loss: 10.237071
Action reg: 0.003943
  l1.weight: grad_norm = 0.308455
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.259274
Total gradient norm: 0.631681
=== Actor Training Debug (Iteration 6685) ===
Q mean: -10.635763
Q std: 14.955766
Actor loss: 10.639722
Action reg: 0.003959
  l1.weight: grad_norm = 0.228351
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.215520
Total gradient norm: 0.675679
=== Actor Training Debug (Iteration 6686) ===
Q mean: -10.681530
Q std: 14.697307
Actor loss: 10.685485
Action reg: 0.003955
  l1.weight: grad_norm = 0.170861
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.134636
Total gradient norm: 0.343426
=== Actor Training Debug (Iteration 6687) ===
Q mean: -12.869420
Q std: 15.673759
Actor loss: 12.873380
Action reg: 0.003959
  l1.weight: grad_norm = 0.252464
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.253344
Total gradient norm: 0.847419
=== Actor Training Debug (Iteration 6688) ===
Q mean: -9.460684
Q std: 14.832905
Actor loss: 9.464642
Action reg: 0.003958
  l1.weight: grad_norm = 0.197454
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.169330
Total gradient norm: 0.470735
=== Actor Training Debug (Iteration 6689) ===
Q mean: -10.745516
Q std: 15.861526
Actor loss: 10.749459
Action reg: 0.003944
  l1.weight: grad_norm = 0.261280
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.209950
Total gradient norm: 0.523163
=== Actor Training Debug (Iteration 6690) ===
Q mean: -11.982323
Q std: 14.925494
Actor loss: 11.986287
Action reg: 0.003964
  l1.weight: grad_norm = 0.214406
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.196448
Total gradient norm: 0.534448
=== Actor Training Debug (Iteration 6691) ===
Q mean: -11.696190
Q std: 15.373704
Actor loss: 11.700153
Action reg: 0.003964
  l1.weight: grad_norm = 0.278052
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.269000
Total gradient norm: 0.859279
=== Actor Training Debug (Iteration 6692) ===
Q mean: -11.396667
Q std: 15.507984
Actor loss: 11.400600
Action reg: 0.003934
  l1.weight: grad_norm = 0.194229
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.174492
Total gradient norm: 0.476616
=== Actor Training Debug (Iteration 6693) ===
Q mean: -11.082568
Q std: 14.563454
Actor loss: 11.086515
Action reg: 0.003947
  l1.weight: grad_norm = 0.224895
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.187590
Total gradient norm: 0.541766
=== Actor Training Debug (Iteration 6694) ===
Q mean: -12.170095
Q std: 15.915313
Actor loss: 12.174069
Action reg: 0.003974
  l1.weight: grad_norm = 0.242510
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.227876
Total gradient norm: 0.546768
=== Actor Training Debug (Iteration 6695) ===
Q mean: -11.671464
Q std: 15.986642
Actor loss: 11.675424
Action reg: 0.003960
  l1.weight: grad_norm = 0.136001
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.114521
Total gradient norm: 0.337417
=== Actor Training Debug (Iteration 6696) ===
Q mean: -11.100357
Q std: 15.720596
Actor loss: 11.104303
Action reg: 0.003947
  l1.weight: grad_norm = 0.186344
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.140455
Total gradient norm: 0.346171
=== Actor Training Debug (Iteration 6697) ===
Q mean: -11.859250
Q std: 15.865504
Actor loss: 11.863215
Action reg: 0.003965
  l1.weight: grad_norm = 0.170468
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.144673
Total gradient norm: 0.421546
=== Actor Training Debug (Iteration 6698) ===
Q mean: -10.946592
Q std: 13.994288
Actor loss: 10.950535
Action reg: 0.003943
  l1.weight: grad_norm = 0.232001
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.197563
Total gradient norm: 0.584623
=== Actor Training Debug (Iteration 6699) ===
Q mean: -12.043615
Q std: 14.626690
Actor loss: 12.047582
Action reg: 0.003966
  l1.weight: grad_norm = 0.171783
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.175423
Total gradient norm: 0.430363
=== Actor Training Debug (Iteration 6700) ===
Q mean: -12.213274
Q std: 15.785606
Actor loss: 12.217234
Action reg: 0.003960
  l1.weight: grad_norm = 0.400175
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.341711
Total gradient norm: 0.883932
=== Actor Training Debug (Iteration 6701) ===
Q mean: -10.421659
Q std: 13.762668
Actor loss: 10.425615
Action reg: 0.003957
  l1.weight: grad_norm = 0.146638
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.130594
Total gradient norm: 0.367997
=== Actor Training Debug (Iteration 6702) ===
Q mean: -11.608217
Q std: 16.620497
Actor loss: 11.612155
Action reg: 0.003938
  l1.weight: grad_norm = 0.442583
  l1.bias: grad_norm = 0.001309
  l2.weight: grad_norm = 0.395324
Total gradient norm: 0.906736
=== Actor Training Debug (Iteration 6703) ===
Q mean: -12.720690
Q std: 17.817003
Actor loss: 12.724661
Action reg: 0.003971
  l1.weight: grad_norm = 0.115418
  l1.bias: grad_norm = 0.000062
  l2.weight: grad_norm = 0.108452
Total gradient norm: 0.308080
=== Actor Training Debug (Iteration 6704) ===
Q mean: -12.958867
Q std: 16.784161
Actor loss: 12.962829
Action reg: 0.003962
  l1.weight: grad_norm = 0.265839
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.205295
Total gradient norm: 0.583770
=== Actor Training Debug (Iteration 6705) ===
Q mean: -10.741878
Q std: 15.489820
Actor loss: 10.745838
Action reg: 0.003960
  l1.weight: grad_norm = 0.138208
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.107769
Total gradient norm: 0.273633
=== Actor Training Debug (Iteration 6706) ===
Q mean: -13.180244
Q std: 16.111746
Actor loss: 13.184215
Action reg: 0.003970
  l1.weight: grad_norm = 0.115465
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.101594
Total gradient norm: 0.275068
=== Actor Training Debug (Iteration 6707) ===
Q mean: -13.770536
Q std: 17.744783
Actor loss: 13.774495
Action reg: 0.003958
  l1.weight: grad_norm = 0.118706
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.113956
Total gradient norm: 0.324674
=== Actor Training Debug (Iteration 6708) ===
Q mean: -11.148525
Q std: 14.269759
Actor loss: 11.152491
Action reg: 0.003965
  l1.weight: grad_norm = 0.161201
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.148963
Total gradient norm: 0.394085
=== Actor Training Debug (Iteration 6709) ===
Q mean: -12.696462
Q std: 16.133619
Actor loss: 12.700406
Action reg: 0.003944
  l1.weight: grad_norm = 0.310808
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.239209
Total gradient norm: 0.612111
=== Actor Training Debug (Iteration 6710) ===
Q mean: -10.973822
Q std: 15.632521
Actor loss: 10.977777
Action reg: 0.003955
  l1.weight: grad_norm = 0.134475
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.120364
Total gradient norm: 0.328613
=== Actor Training Debug (Iteration 6711) ===
Q mean: -13.296552
Q std: 16.307404
Actor loss: 13.300511
Action reg: 0.003959
  l1.weight: grad_norm = 0.126154
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.111066
Total gradient norm: 0.324624
=== Actor Training Debug (Iteration 6712) ===
Q mean: -11.165331
Q std: 16.185289
Actor loss: 11.169280
Action reg: 0.003949
  l1.weight: grad_norm = 0.218984
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.190556
Total gradient norm: 0.545400
=== Actor Training Debug (Iteration 6713) ===
Q mean: -11.098688
Q std: 14.730095
Actor loss: 11.102654
Action reg: 0.003965
  l1.weight: grad_norm = 0.428210
  l1.bias: grad_norm = 0.000225
  l2.weight: grad_norm = 0.390963
Total gradient norm: 1.151350
=== Actor Training Debug (Iteration 6714) ===
Q mean: -11.262119
Q std: 15.308707
Actor loss: 11.266073
Action reg: 0.003954
  l1.weight: grad_norm = 0.174038
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.154559
Total gradient norm: 0.387877
=== Actor Training Debug (Iteration 6715) ===
Q mean: -12.930182
Q std: 16.615885
Actor loss: 12.934133
Action reg: 0.003950
  l1.weight: grad_norm = 0.182719
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.161574
Action reg: 0.003956
  l1.weight: grad_norm = 0.172009
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.162904
Total gradient norm: 0.410298
=== Actor Training Debug (Iteration 6729) ===
Q mean: -11.758232
Q std: 14.434826
Actor loss: 11.762189
Action reg: 0.003957
  l1.weight: grad_norm = 0.212974
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.192208
Total gradient norm: 0.601215
=== Actor Training Debug (Iteration 6730) ===
Q mean: -10.223102
Q std: 14.027279
Actor loss: 10.227067
Action reg: 0.003965
  l1.weight: grad_norm = 0.117755
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.096914
Total gradient norm: 0.267238
=== Actor Training Debug (Iteration 6731) ===
Q mean: -11.013652
Q std: 15.656019
Actor loss: 11.017605
Action reg: 0.003953
  l1.weight: grad_norm = 0.208723
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.181215
Total gradient norm: 0.480561
=== Actor Training Debug (Iteration 6732) ===
Q mean: -11.902178
Q std: 15.409139
Actor loss: 11.906136
Action reg: 0.003958
  l1.weight: grad_norm = 0.146223
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.115330
Total gradient norm: 0.309895
=== Actor Training Debug (Iteration 6733) ===
Q mean: -11.181744
Q std: 15.096195
Actor loss: 11.185698
Action reg: 0.003954
  l1.weight: grad_norm = 0.254199
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.218716
Total gradient norm: 0.608623
=== Actor Training Debug (Iteration 6734) ===
Q mean: -10.824543
Q std: 14.364071
Actor loss: 10.828491
Action reg: 0.003948
  l1.weight: grad_norm = 0.245880
  l1.bias: grad_norm = 0.000412
  l2.weight: grad_norm = 0.200503
Total gradient norm: 0.533900
=== Actor Training Debug (Iteration 6735) ===
Q mean: -11.958479
Q std: 15.678726
Actor loss: 11.962437
Action reg: 0.003958
  l1.weight: grad_norm = 0.095435
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.076602
Total gradient norm: 0.204622
=== Actor Training Debug (Iteration 6736) ===
Q mean: -13.140747
Q std: 16.599289
Actor loss: 13.144728
Action reg: 0.003980
  l1.weight: grad_norm = 0.194227
  l1.bias: grad_norm = 0.000057
  l2.weight: grad_norm = 0.148559
Total gradient norm: 0.348652
=== Actor Training Debug (Iteration 6737) ===
Q mean: -12.203455
Q std: 13.684448
Actor loss: 12.207433
Action reg: 0.003978
  l1.weight: grad_norm = 0.107277
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.087832
Total gradient norm: 0.225290
=== Actor Training Debug (Iteration 6738) ===
Q mean: -11.556586
Q std: 14.803644
Actor loss: 11.560517
Action reg: 0.003931
  l1.weight: grad_norm = 0.302720
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.253020
Total gradient norm: 0.689683
=== Actor Training Debug (Iteration 6739) ===
Q mean: -11.605131
Q std: 16.215000
Actor loss: 11.609069
Action reg: 0.003938
  l1.weight: grad_norm = 0.244839
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.210107
Total gradient norm: 0.515086
=== Actor Training Debug (Iteration 6740) ===
Q mean: -11.065408
Q std: 14.452330
Actor loss: 11.069360
Action reg: 0.003952
  l1.weight: grad_norm = 0.256127
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.218523
Total gradient norm: 0.634394
=== Actor Training Debug (Iteration 6741) ===
Q mean: -9.539359
Q std: 13.169230
Actor loss: 9.543309
Action reg: 0.003950
  l1.weight: grad_norm = 0.279396
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.217390
Total gradient norm: 0.570941
=== Actor Training Debug (Iteration 6742) ===
Q mean: -12.164539
Q std: 14.956879
Actor loss: 12.168503
Action reg: 0.003963
  l1.weight: grad_norm = 0.309151
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.278531
Total gradient norm: 0.711452
=== Actor Training Debug (Iteration 6743) ===
Q mean: -12.287418
Q std: 15.806089
Actor loss: 12.291368
Action reg: 0.003949
  l1.weight: grad_norm = 0.136471
  l1.bias: grad_norm = 0.000528
  l2.weight: grad_norm = 0.130564
Total gradient norm: 0.306264
=== Actor Training Debug (Iteration 6744) ===
Q mean: -12.959189
Q std: 17.348032
Actor loss: 12.963157
Action reg: 0.003967
  l1.weight: grad_norm = 0.167168
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.132501
Total gradient norm: 0.354971
=== Actor Training Debug (Iteration 6745) ===
Q mean: -11.045050
Q std: 15.380880
Actor loss: 11.049013
Action reg: 0.003963
  l1.weight: grad_norm = 0.209697
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.198702
Total gradient norm: 0.519631
=== Actor Training Debug (Iteration 6746) ===
Q mean: -12.534981
Q std: 16.540026
Actor loss: 12.538932
Action reg: 0.003951
  l1.weight: grad_norm = 0.180516
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.162905
Total gradient norm: 0.370459
=== Actor Training Debug (Iteration 6747) ===
Q mean: -13.537243
Q std: 17.253674
Actor loss: 13.541189
Action reg: 0.003947
  l1.weight: grad_norm = 0.321112
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.254908
Total gradient norm: 0.694182
=== Actor Training Debug (Iteration 6748) ===
Q mean: -12.793907
Q std: 15.943779
Actor loss: 12.797870
Action reg: 0.003962
  l1.weight: grad_norm = 0.095028
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.076167
Total gradient norm: 0.217463
=== Actor Training Debug (Iteration 6749) ===
Q mean: -10.504799
Q std: 14.477774
Actor loss: 10.508764
Action reg: 0.003965
  l1.weight: grad_norm = 0.243349
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.207328
Total gradient norm: 0.533823
=== Actor Training Debug (Iteration 6750) ===
Q mean: -12.514278
Q std: 16.576487
Actor loss: 12.518227
Action reg: 0.003949
  l1.weight: grad_norm = 0.125415
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.105839
Total gradient norm: 0.311491
=== Actor Training Debug (Iteration 6751) ===
Q mean: -10.545053
Q std: 14.461422
Actor loss: 10.548984
Action reg: 0.003930
  l1.weight: grad_norm = 0.266415
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.235645
Total gradient norm: 0.645492
=== Actor Training Debug (Iteration 6752) ===
Q mean: -11.734550
Q std: 16.242115
Actor loss: 11.738507
Action reg: 0.003957
  l1.weight: grad_norm = 0.250582
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.209000
Total gradient norm: 0.593564
=== Actor Training Debug (Iteration 6753) ===
Q mean: -11.652398
Q std: 16.083164
Actor loss: 11.656363
Action reg: 0.003966
  l1.weight: grad_norm = 0.161270
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.143552
Total gradient norm: 0.492485
=== Actor Training Debug (Iteration 6754) ===
Q mean: -13.065843
Q std: 18.169069
Actor loss: 13.069805
Action reg: 0.003963
  l1.weight: grad_norm = 0.108060
  l1.bias: grad_norm = 0.001050
  l2.weight: grad_norm = 0.098509
Total gradient norm: 0.242747
=== Actor Training Debug (Iteration 6755) ===
Q mean: -12.785320
Q std: 17.713366
Actor loss: 12.789294
Action reg: 0.003974
  l1.weight: grad_norm = 0.333076
  l1.bias: grad_norm = 0.000107
  l2.weight: grad_norm = 0.257815
Total gradient norm: 0.733592
=== Actor Training Debug (Iteration 6756) ===
Q mean: -9.888395
Q std: 14.882921
Actor loss: 9.892330
Action reg: 0.003935
  l1.weight: grad_norm = 0.290188
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.242400
Total gradient norm: 0.623750
=== Actor Training Debug (Iteration 6757) ===
Q mean: -11.499290
Q std: 16.255922
Actor loss: 11.503226
Action reg: 0.003936
  l1.weight: grad_norm = 0.271409
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.216645
Total gradient norm: 0.606321
=== Actor Training Debug (Iteration 6758) ===
Q mean: -9.969938
Q std: 13.364309
Actor loss: 9.973899
Action reg: 0.003961
  l1.weight: grad_norm = 0.263158
  l1.bias: grad_norm = 0.000999
  l2.weight: grad_norm = 0.215198
Total gradient norm: 0.624212
=== Actor Training Debug (Iteration 6759) ===
Q mean: -11.810840
Q std: 16.212706
Actor loss: 11.814786
Action reg: 0.003947
  l1.weight: grad_norm = 0.301808
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.257736
Total gradient norm: 0.643118
=== Actor Training Debug (Iteration 6760) ===
Q mean: -10.674623
Q std: 14.827293
Actor loss: 10.678590
Action reg: 0.003966
  l1.weight: grad_norm = 0.118437
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.096803
Total gradient norm: 0.264751
=== Actor Training Debug (Iteration 6761) ===
Q mean: -11.728329
Q std: 17.558670
Actor loss: 11.732280
Action reg: 0.003952
  l1.weight: grad_norm = 0.417392
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.338905
Total gradient norm: 0.791105
=== Actor Training Debug (Iteration 6762) ===
Q mean: -10.004190
Q std: 15.592035
Actor loss: 10.008142
Action reg: 0.003952
  l1.weight: grad_norm = 0.236946
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.201058
Total gradient norm: 0.548686
=== Actor Training Debug (Iteration 6763) ===
Q mean: -12.365250
Q std: 16.276869
Actor loss: 12.369234
Action reg: 0.003984
  l1.weight: grad_norm = 0.121497
  l1.bias: grad_norm = 0.000041
  l2.weight: grad_norm = 0.118688
Total gradient norm: 0.350426
=== Actor Training Debug (Iteration 6764) ===
Q mean: -11.841609
Q std: 15.905375
Actor loss: 11.845572
Action reg: 0.003964
  l1.weight: grad_norm = 0.173248
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.137061
Total gradient norm: 0.353343
=== Actor Training Debug (Iteration 6765) ===
Q mean: -12.343842
Q std: 14.990385
Actor loss: 12.347809
Action reg: 0.003967
  l1.weight: grad_norm = 0.263344
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.204513
Total gradient norm: 0.521007
=== Actor Training Debug (Iteration 6766) ===
Q mean: -12.205039
Q std: 16.639517
Actor loss: 12.209002
Action reg: 0.003963
  l1.weight: grad_norm = 0.234311
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.192334
Total gradient norm: 0.505340
=== Actor Training Debug (Iteration 6767) ===
Q mean: -10.915488
Q std: 13.911334
Actor loss: 10.919449
Action reg: 0.003961
  l1.weight: grad_norm = 0.310085
  l1.bias: grad_norm = 0.000129
  l2.weight: grad_norm = 0.238019
Total gradient norm: 0.609375
=== Actor Training Debug (Iteration 6768) ===
Q mean: -10.720270
Q std: 14.825794
Actor loss: 10.724236
Action reg: 0.003966
  l1.weight: grad_norm = 0.246834
  l1.bias: grad_norm = 0.000275
  l2.weight: grad_norm = 0.209777
Total gradient norm: 0.578617
=== Actor Training Debug (Iteration 6769) ===
Q mean: -12.213091
Q std: 15.732790
Actor loss: 12.217025
Action reg: 0.003934
  l1.weight: grad_norm = 0.232117
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.193301
Total gradient norm: 0.522535
=== Actor Training Debug (Iteration 6770) ===
Q mean: -12.475433
Q std: 16.495985
Actor loss: 12.479396
Action reg: 0.003962
  l1.weight: grad_norm = 0.214838
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.175700
Total gradient norm: 0.445294
=== Actor Training Debug (Iteration 6771) ===
Q mean: -11.705633
Q std: 17.273489
Actor loss: 11.709593
Action reg: 0.003960
  l1.weight: grad_norm = 0.291126
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.226388
Total gradient norm: 0.635509
=== Actor Training Debug (Iteration 6772) ===
Q mean: -12.172290
Q std: 16.135313
Actor loss: 12.176251
Action reg: 0.003962
  l1.weight: grad_norm = 0.139160
  l1.bias: grad_norm = 0.001253
  l2.weight: grad_norm = 0.134161
Total gradient norm: 0.388352
=== Actor Training Debug (Iteration 6773) ===
Q mean: -12.239279
Q std: 16.446953
Actor loss: 12.243229
Action reg: 0.003950
  l1.weight: grad_norm = 0.252523
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.231675
Total gradient norm: 0.565926
=== Actor Training Debug (Iteration 6774) ===
Q mean: -10.965269
Q std: 15.909961
Actor loss: 10.969205
Action reg: 0.003936
  l1.weight: grad_norm = 0.286098
  l1.bias: grad_norm = 0.001058
  l2.weight: grad_norm = 0.240961
Total gradient norm: 0.615188
=== Actor Training Debug (Iteration 6775) ===
Q mean: -12.884645
Q std: 14.699763
Actor loss: 12.888600
Action reg: 0.003955
  l1.weight: grad_norm = 0.212118
  l1.bias: grad_norm = 0.000095
  l2.weight: grad_norm = 0.181927
Total gradient norm: 0.493111
=== Actor Training Debug (Iteration 6776) ===
Q mean: -10.653567
Q std: 14.280643
Actor loss: 10.657507
Action reg: 0.003940
  l1.weight: grad_norm = 0.171508
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.178192
Total gradient norm: 0.693825
=== Actor Training Debug (Iteration 6777) ===
Q mean: -12.483288
Q std: 15.889882
Actor loss: 12.487250
Action reg: 0.003962
  l1.weight: grad_norm = 0.499941
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.424521
Total gradient norm: 1.299358
=== Actor Training Debug (Iteration 6778) ===
Q mean: -11.293089
Q std: 14.697312
Actor loss: 11.297026
Action reg: 0.003936
  l1.weight: grad_norm = 0.301714
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.262813
Total gradient norm: 0.809296
=== Actor Training Debug (Iteration 6779) ===
Q mean: -11.347670
Q std: 15.523820
Actor loss: 11.351614
Action reg: 0.003945
  l1.weight: grad_norm = 0.192892
  l1.bias: grad_norm = 0.000477
  l2.weight: grad_norm = 0.181804
Total gradient norm: 0.450019
=== Actor Training Debug (Iteration 6780) ===
Q mean: -12.069290
Q std: 15.643066
Actor loss: 12.073262
Action reg: 0.003972
  l1.weight: grad_norm = 0.088640
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.072818
Total gradient norm: 0.199978
=== Actor Training Debug (Iteration 6781) ===
Q mean: -11.430960
Q std: 16.168396
Actor loss: 11.434919
Action reg: 0.003960
  l1.weight: grad_norm = 0.122120
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.102120
Total gradient norm: 0.296000
=== Actor Training Debug (Iteration 6782) ===
Q mean: -11.969656
Q std: 15.631650
Actor loss: 11.973605
Action reg: 0.003949
  l1.weight: grad_norm = 0.182067
  l1.bias: grad_norm = 0.000517
  l2.weight: grad_norm = 0.166948
Total gradient norm: 0.531143
=== Actor Training Debug (Iteration 6783) ===
Q mean: -12.842113
Q std: 17.732222
Actor loss: 12.846070
Action reg: 0.003958
  l1.weight: grad_norm = 0.174672
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.143019
Total gradient norm: 0.420183
=== Actor Training Debug (Iteration 6784) ===
Q mean: -11.211289
Q std: 13.763774
Actor loss: 11.215231
Action reg: 0.003942
  l1.weight: grad_norm = 0.284998
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.235311
Total gradient norm: 0.641755
=== Actor Training Debug (Iteration 6785) ===
Q mean: -11.887558
Q std: 16.469086
Actor loss: 11.891516
Action reg: 0.003957
  l1.weight: grad_norm = 0.258621
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.200635
Total gradient norm: 0.551497
=== Actor Training Debug (Iteration 6786) ===
Q mean: -11.153316
Q std: 16.199957
Actor loss: 11.157278
Action reg: 0.003961
  l1.weight: grad_norm = 0.241470
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.203847
Total gradient norm: 0.561780
=== Actor Training Debug (Iteration 6787) ===
Q mean: -11.393311
Q std: 13.967368
Actor loss: 11.397285
Action reg: 0.003975
  l1.weight: grad_norm = 0.135344
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.123548
Total gradient norm: 0.349592
=== Actor Training Debug (Iteration 6788) ===
Q mean: -9.405329
Q std: 13.243490
Actor loss: 9.409289
Action reg: 0.003961
  l1.weight: grad_norm = 0.255564
  l1.bias: grad_norm = 0.000528
  l2.weight: grad_norm = 0.181756
Total gradient norm: 0.508878
=== Actor Training Debug (Iteration 6789) ===
Q mean: -10.996614
Q std: 13.719733
Actor loss: 11.000575
Action reg: 0.003960
  l1.weight: grad_norm = 0.307275
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.261739
Total gradient norm: 0.730470
=== Actor Training Debug (Iteration 6790) ===
Q mean: -12.713795
Q std: 17.218151
Actor loss: 12.717756
Action reg: 0.003962
  l1.weight: grad_norm = 0.171179
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.133829
Total gradient norm: 0.346782
=== Actor Training Debug (Iteration 6791) ===
Q mean: -13.229692
Q std: 17.946299
Actor loss: 13.233640
Action reg: 0.003948
  l1.weight: grad_norm = 0.312680
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.243507
Total gradient norm: 0.608644
=== Actor Training Debug (Iteration 6792) ===
Q mean: -10.243433
Q std: 13.848771
Actor loss: 10.247383
Action reg: 0.003950
  l1.weight: grad_norm = 0.221937
  l1.bias: grad_norm = 0.000408
  l2.weight: grad_norm = 0.189754
Total gradient norm: 0.492868
=== Actor Training Debug (Iteration 6793) ===
Q mean: -13.025967
Q std: 16.672434
Actor loss: 13.029918
Action reg: 0.003951
  l1.weight: grad_norm = 0.179842
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.143031
Total gradient norm: 0.402188
=== Actor Training Debug (Iteration 6794) ===
Q mean: -11.318186
Q std: 16.259758
Actor loss: 11.322145
Action reg: 0.003960
  l1.weight: grad_norm = 0.201365
  l1.bias: grad_norm = 0.000729
  l2.weight: grad_norm = 0.166953
Total gradient norm: 0.470197
=== Actor Training Debug (Iteration 6795) ===
Q mean: -10.922688
Q std: 14.215200
Actor loss: 10.926660
Action reg: 0.003972
  l1.weight: grad_norm = 0.206890
  l1.bias: grad_norm = 0.000162
  l2.weight: grad_norm = 0.179469
Total gradient norm: 0.483717
=== Actor Training Debug (Iteration 6796) ===
Q mean: -10.639727
Q std: 14.004926
Actor loss: 10.643671
Action reg: 0.003944
  l1.weight: grad_norm = 0.176662
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.138171
Total gradient norm: 0.333340
=== Actor Training Debug (Iteration 6797) ===
Q mean: -12.111897
Q std: 15.097045
Actor loss: 12.115862
Action reg: 0.003966
  l1.weight: grad_norm = 0.273690
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.186969
Total gradient norm: 0.459137
=== Actor Training Debug (Iteration 6798) ===
Q mean: -12.382746
Q std: 15.547126
Actor loss: 12.386693
Action reg: 0.003947
  l1.weight: grad_norm = 0.241226
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.203124
Total gradient norm: 0.547197
=== Actor Training Debug (Iteration 6799) ===
Q mean: -11.650272
Q std: 15.642137
Actor loss: 11.654229
Action reg: 0.003956
  l1.weight: grad_norm = 0.168586
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.144613
Total gradient norm: 0.392019
=== Actor Training Debug (Iteration 6800) ===
Q mean: -11.300019
Q std: 16.269714
Actor loss: 11.303947
Action reg: 0.003928
  l1.weight: grad_norm = 0.308832
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.298932
Total gradient norm: 0.969316
=== Actor Training Debug (Iteration 6801) ===
Q mean: -11.020617
Q std: 16.039885
Actor loss: 11.024568
Action reg: 0.003951
  l1.weight: grad_norm = 0.217272
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.177758
Total gradient norm: 0.473048
=== Actor Training Debug (Iteration 6802) ===
Q mean: -10.010859
Q std: 15.008848
Actor loss: 10.014810
Action reg: 0.003950
  l1.weight: grad_norm = 0.239012
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.183282
Total gradient norm: 0.465249
=== Actor Training Debug (Iteration 6803) ===
Q mean: -12.352625
Q std: 17.127613
Actor loss: 12.356594
Action reg: 0.003969
  l1.weight: grad_norm = 0.151170
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.126876
Total gradient norm: 0.323653
=== Actor Training Debug (Iteration 6804) ===
Q mean: -11.658876
Q std: 15.729186
Actor loss: 11.662833
Action reg: 0.003957
  l1.weight: grad_norm = 0.140557
  l1.bias: grad_norm = 0.000090
  l2.weight: grad_norm = 0.138841
Total gradient norm: 0.346182
=== Actor Training Debug (Iteration 6805) ===
Q mean: -10.381165
Q std: 14.918312
Actor loss: 10.385115
Action reg: 0.003950
  l1.weight: grad_norm = 0.203607
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.168254
Total gradient norm: 0.440680
=== Actor Training Debug (Iteration 6806) ===
Q mean: -12.305012
Q std: 14.445584
Actor loss: 12.308975
Action reg: 0.003963
  l1.weight: grad_norm = 0.133537
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.112106
Total gradient norm: 0.297818
=== Actor Training Debug (Iteration 6807) ===
Q mean: -11.781542
Q std: 15.833611
Actor loss: 11.785505
Action reg: 0.003963
  l1.weight: grad_norm = 0.256010
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.209245
Total gradient norm: 0.558342
=== Actor Training Debug (Iteration 6808) ===
Q mean: -11.392980
Q std: 15.625797
Actor loss: 11.396937
Action reg: 0.003958
  l1.weight: grad_norm = 0.221795
  l1.bias: grad_norm = 0.000382
  l2.weight: grad_norm = 0.180567
Total gradient norm: 0.466019
=== Actor Training Debug (Iteration 6809) ===
Q mean: -9.876597
Q std: 13.471642
Actor loss: 9.880537
Action reg: 0.003939
  l1.weight: grad_norm = 0.152770
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.133178
Total gradient norm: 0.396663
=== Actor Training Debug (Iteration 6810) ===
Q mean: -12.664742
Q std: 16.725029
Actor loss: 12.668689
Action reg: 0.003947
  l1.weight: grad_norm = 0.475442
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.373200
Total gradient norm: 0.983231
=== Actor Training Debug (Iteration 6811) ===
Q mean: -11.588820
Q std: 16.227032
Actor loss: 11.592763
Action reg: 0.003944
  l1.weight: grad_norm = 0.258979
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.219231
Total gradient norm: 0.585012
=== Actor Training Debug (Iteration 6812) ===
Q mean: -11.684568
Q std: 14.321704
Actor loss: 11.688508
Action reg: 0.003939
  l1.weight: grad_norm = 0.308517
  l1.bias: grad_norm = 0.000425
  l2.weight: grad_norm = 0.293670
Total gradient norm: 0.684109
=== Actor Training Debug (Iteration 6813) ===
Q mean: -9.968779
Q std: 13.840391
Actor loss: 9.972734
Action reg: 0.003956
  l1.weight: grad_norm = 0.375482
  l1.bias: grad_norm = 0.001471
  l2.weight: grad_norm = 0.300028
Total gradient norm: 0.906268
=== Actor Training Debug (Iteration 6814) ===
Q mean: -11.476957
Q std: 17.389589
Actor loss: 11.480904
Action reg: 0.003946
  l1.weight: grad_norm = 0.314724
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.247004
Total gradient norm: 0.626115
=== Actor Training Debug (Iteration 6815) ===
Q mean: -10.160866
Q std: 15.127729
Actor loss: 10.164810
Action reg: 0.003945
  l1.weight: grad_norm = 0.168320
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.142110
Total gradient norm: 0.372563
=== Actor Training Debug (Iteration 6816) ===
Q mean: -11.825763
Q std: 15.679690
Actor loss: 11.829712
Action reg: 0.003949
  l1.weight: grad_norm = 0.267365
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.213086
Total gradient norm: 0.629291
=== Actor Training Debug (Iteration 6817) ===
Q mean: -12.168877
Q std: 16.886658
Actor loss: 12.172848
Action reg: 0.003971
  l1.weight: grad_norm = 0.208575
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.172191
Total gradient norm: 0.458507
=== Actor Training Debug (Iteration 6818) ===
Q mean: -11.819231
Q std: 15.552004
Actor loss: 11.823175
Action reg: 0.003945
  l1.weight: grad_norm = 0.184797
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.154611
Total gradient norm: 0.400543
=== Actor Training Debug (Iteration 6819) ===
Q mean: -11.499469
Q std: 13.503620
Actor loss: 11.503409
Action reg: 0.003940
  l1.weight: grad_norm = 0.213147
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.193652
Total gradient norm: 0.524138
=== Actor Training Debug (Iteration 6820) ===
Q mean: -13.586976
Q std: 15.655639
Actor loss: 13.590940
Action reg: 0.003964
  l1.weight: grad_norm = 0.189887
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.161883
Total gradient norm: 0.488510
=== Actor Training Debug (Iteration 6821) ===
Q mean: -12.750029
Q std: 18.288500
Actor loss: 12.753978
Action reg: 0.003949
  l1.weight: grad_norm = 0.166190
  l1.bias: grad_norm = 0.000313
  l2.weight: grad_norm = 0.153284
Total gradient norm: 0.428703
=== Actor Training Debug (Iteration 6822) ===
Q mean: -13.445378
Q std: 17.104050
Actor loss: 13.449337
Action reg: 0.003958
  l1.weight: grad_norm = 0.193675
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.173563
Total gradient norm: 0.472510
=== Actor Training Debug (Iteration 6823) ===
Q mean: -10.594997
Q std: 15.150793
Actor loss: 10.598957
Action reg: 0.003960
  l1.weight: grad_norm = 0.212715
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.175987
Total gradient norm: 0.497833
=== Actor Training Debug (Iteration 6824) ===
Q mean: -14.074075
Q std: 19.094898
Actor loss: 14.078036
Action reg: 0.003962
  l1.weight: grad_norm = 0.230388
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.207873
Total gradient norm: 0.546256
=== Actor Training Debug (Iteration 6825) ===
Q mean: -14.666570
Q std: 18.432077
Actor loss: 14.670533
Action reg: 0.003964
  l1.weight: grad_norm = 0.142147
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.113405
Total gradient norm: 0.308528
=== Actor Training Debug (Iteration 6826) ===
Q mean: -11.939282
Q std: 15.384978
Actor loss: 11.943234
Action reg: 0.003952
  l1.weight: grad_norm = 0.471050
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.412001
Total gradient norm: 1.165815
=== Actor Training Debug (Iteration 6827) ===
Q mean: -11.951344
Q std: 16.146915
Actor loss: 11.955276
Action reg: 0.003933
  l1.weight: grad_norm = 0.206563
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.162561
Total gradient norm: 0.480673
=== Actor Training Debug (Iteration 6828) ===
Q mean: -12.601015
Q std: 15.392071
Actor loss: 12.604982
Action reg: 0.003967
  l1.weight: grad_norm = 0.192552
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.160541
Total gradient norm: 0.444577
=== Actor Training Debug (Iteration 6829) ===
Q mean: -11.398319
Q std: 16.043495
Actor loss: 11.402268
Action reg: 0.003949
  l1.weight: grad_norm = 0.129715
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.113587
Total gradient norm: 0.312765
=== Actor Training Debug (Iteration 6830) ===
Q mean: -12.786972
=== Actor Training Debug (Iteration 6862) ===
Q mean: -11.993219
Q std: 15.441306
Actor loss: 11.997180
Action reg: 0.003961
  l1.weight: grad_norm = 0.171627
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.151552
Total gradient norm: 0.427120
=== Actor Training Debug (Iteration 6863) ===
Q mean: -11.757910
Q std: 13.762628
Actor loss: 11.761874
Action reg: 0.003964
  l1.weight: grad_norm = 0.068202
  l1.bias: grad_norm = 0.000185
  l2.weight: grad_norm = 0.060562
Total gradient norm: 0.179219
=== Actor Training Debug (Iteration 6864) ===
Q mean: -12.896535
Q std: 15.777577
Actor loss: 12.900501
Action reg: 0.003967
  l1.weight: grad_norm = 0.335973
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.296223
Total gradient norm: 0.755389
=== Actor Training Debug (Iteration 6865) ===
Q mean: -13.259687
Q std: 17.333536
Actor loss: 13.263652
Action reg: 0.003964
  l1.weight: grad_norm = 0.265181
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.199980
Total gradient norm: 0.496934
=== Actor Training Debug (Iteration 6866) ===
Q mean: -13.411707
Q std: 17.174305
Actor loss: 13.415668
Action reg: 0.003962
  l1.weight: grad_norm = 0.212778
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.164866
Total gradient norm: 0.518943
=== Actor Training Debug (Iteration 6867) ===
Q mean: -11.687960
Q std: 15.411588
Actor loss: 11.691915
Action reg: 0.003955
  l1.weight: grad_norm = 0.396129
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.305587
Total gradient norm: 0.766286
=== Actor Training Debug (Iteration 6868) ===
Q mean: -12.124969
Q std: 14.643334
Actor loss: 12.128942
Action reg: 0.003972
  l1.weight: grad_norm = 0.211109
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.199695
Total gradient norm: 0.540812
=== Actor Training Debug (Iteration 6869) ===
Q mean: -12.547379
Q std: 15.658380
Actor loss: 12.551323
Action reg: 0.003944
  l1.weight: grad_norm = 0.312648
  l1.bias: grad_norm = 0.000525
  l2.weight: grad_norm = 0.241472
Total gradient norm: 0.708037
=== Actor Training Debug (Iteration 6870) ===
Q mean: -11.840896
Q std: 15.754545
Actor loss: 11.844848
Action reg: 0.003952
  l1.weight: grad_norm = 0.261979
  l1.bias: grad_norm = 0.000219
  l2.weight: grad_norm = 0.166768
Total gradient norm: 0.437966
=== Actor Training Debug (Iteration 6871) ===
Q mean: -12.439144
Q std: 17.219414
Actor loss: 12.443105
Action reg: 0.003960
  l1.weight: grad_norm = 0.173328
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.149698
Total gradient norm: 0.395948
=== Actor Training Debug (Iteration 6872) ===
Q mean: -12.395176
Q std: 16.732635
Actor loss: 12.399128
Action reg: 0.003952
  l1.weight: grad_norm = 0.151443
  l1.bias: grad_norm = 0.000098
  l2.weight: grad_norm = 0.148653
Total gradient norm: 0.467970
=== Actor Training Debug (Iteration 6873) ===
Q mean: -11.661745
Q std: 15.632417
Actor loss: 11.665693
Action reg: 0.003948
  l1.weight: grad_norm = 0.252698
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.221775
Total gradient norm: 0.569298
=== Actor Training Debug (Iteration 6874) ===
Q mean: -12.738886
Q std: 16.580374
Actor loss: 12.742856
Action reg: 0.003970
  l1.weight: grad_norm = 0.360459
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.249131
Total gradient norm: 0.677144
=== Actor Training Debug (Iteration 6875) ===
Q mean: -10.879344
Q std: 15.699833
Actor loss: 10.883305
Action reg: 0.003960
  l1.weight: grad_norm = 0.216434
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.186685
Total gradient norm: 0.491444
=== Actor Training Debug (Iteration 6876) ===
Q mean: -11.510967
Q std: 15.363303
Actor loss: 11.514936
Action reg: 0.003969
  l1.weight: grad_norm = 0.192102
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.170964
Total gradient norm: 0.447349
=== Actor Training Debug (Iteration 6877) ===
Q mean: -11.657808
Q std: 14.914366
Actor loss: 11.661783
Action reg: 0.003975
  l1.weight: grad_norm = 0.183460
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.151580
Total gradient norm: 0.404478
=== Actor Training Debug (Iteration 6878) ===
Q mean: -12.910761
Q std: 16.700319
Actor loss: 12.914717
Action reg: 0.003956
  l1.weight: grad_norm = 0.214571
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.190429
Total gradient norm: 0.501960
=== Actor Training Debug (Iteration 6879) ===
Q mean: -12.672347
Q std: 17.151941
Actor loss: 12.676304
Action reg: 0.003957
  l1.weight: grad_norm = 0.123442
  l1.bias: grad_norm = 0.001119
  l2.weight: grad_norm = 0.111971
Total gradient norm: 0.343388
=== Actor Training Debug (Iteration 6880) ===
Q mean: -12.562208
Q std: 16.840403
Actor loss: 12.566182
Action reg: 0.003974
  l1.weight: grad_norm = 0.142877
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.133556
Total gradient norm: 0.343653
=== Actor Training Debug (Iteration 6881) ===
Q mean: -10.903604
Q std: 15.906796
Actor loss: 10.907577
Action reg: 0.003973
  l1.weight: grad_norm = 0.164888
  l1.bias: grad_norm = 0.000982
  l2.weight: grad_norm = 0.132338
Total gradient norm: 0.328267
=== Actor Training Debug (Iteration 6882) ===
Q mean: -11.837102
Q std: 16.113106
Actor loss: 11.841061
Action reg: 0.003958
  l1.weight: grad_norm = 0.259835
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.200883
Total gradient norm: 0.543253
=== Actor Training Debug (Iteration 6883) ===
Q mean: -10.574272
Q std: 13.892231
Actor loss: 10.578213
Action reg: 0.003941
  l1.weight: grad_norm = 0.228872
  l1.bias: grad_norm = 0.001200
  l2.weight: grad_norm = 0.173835
Total gradient norm: 0.438175
=== Actor Training Debug (Iteration 6884) ===
Q mean: -10.955879
Q std: 15.701831
Actor loss: 10.959825
Action reg: 0.003945
  l1.weight: grad_norm = 0.172411
  l1.bias: grad_norm = 0.001366
  l2.weight: grad_norm = 0.138147
Total gradient norm: 0.377270
=== Actor Training Debug (Iteration 6885) ===
Q mean: -12.259497
Q std: 16.110292
Actor loss: 12.263459
Action reg: 0.003963
  l1.weight: grad_norm = 0.195336
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.160782
Total gradient norm: 0.474760
=== Actor Training Debug (Iteration 6886) ===
Q mean: -14.353498
Q std: 18.211365
Actor loss: 14.357466
Action reg: 0.003968
  l1.weight: grad_norm = 0.144894
  l1.bias: grad_norm = 0.000862
  l2.weight: grad_norm = 0.137184
Total gradient norm: 0.374575
=== Actor Training Debug (Iteration 6887) ===
Q mean: -11.151095
Q std: 15.872058
Actor loss: 11.155042
Action reg: 0.003946
  l1.weight: grad_norm = 0.167337
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.138361
Total gradient norm: 0.372062
=== Actor Training Debug (Iteration 6888) ===
Q mean: -11.894361
Q std: 17.218349
Actor loss: 11.898315
Action reg: 0.003955
  l1.weight: grad_norm = 0.249473
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.216552
Total gradient norm: 0.635495
=== Actor Training Debug (Iteration 6889) ===
Q mean: -11.980101
Q std: 14.464119
Actor loss: 11.984064
Action reg: 0.003964
  l1.weight: grad_norm = 0.188503
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.160367
Total gradient norm: 0.426300
=== Actor Training Debug (Iteration 6890) ===
Q mean: -12.007425
Q std: 15.756069
Actor loss: 12.011379
Action reg: 0.003954
  l1.weight: grad_norm = 0.224190
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.180186
Total gradient norm: 0.510307
=== Actor Training Debug (Iteration 6891) ===
Q mean: -11.490058
Q std: 14.662611
Actor loss: 11.494015
Action reg: 0.003957
  l1.weight: grad_norm = 0.290934
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.258963
Total gradient norm: 0.651103
=== Actor Training Debug (Iteration 6892) ===
Q mean: -11.383804
Q std: 17.175087
Actor loss: 11.387766
Action reg: 0.003962
  l1.weight: grad_norm = 0.165507
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.134954
Total gradient norm: 0.323906
=== Actor Training Debug (Iteration 6893) ===
Q mean: -9.851946
Q std: 14.682929
Actor loss: 9.855898
Action reg: 0.003952
  l1.weight: grad_norm = 0.475041
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.359578
Total gradient norm: 0.874590
=== Actor Training Debug (Iteration 6894) ===
Q mean: -10.872890
Q std: 15.835073
Actor loss: 10.876836
Action reg: 0.003947
  l1.weight: grad_norm = 0.280255
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.273763
Total gradient norm: 0.717479
=== Actor Training Debug (Iteration 6895) ===
Q mean: -10.776739
Q std: 15.426240
Actor loss: 10.780705
Action reg: 0.003966
  l1.weight: grad_norm = 0.324709
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.255361
Total gradient norm: 0.616154
=== Actor Training Debug (Iteration 6896) ===
Q mean: -12.149738
Q std: 16.085623
Actor loss: 12.153687
Action reg: 0.003948
  l1.weight: grad_norm = 0.189934
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.166393
Total gradient norm: 0.411671
=== Actor Training Debug (Iteration 6897) ===
Q mean: -12.153735
Q std: 15.195875
Actor loss: 12.157697
Action reg: 0.003962
  l1.weight: grad_norm = 0.217622
  l1.bias: grad_norm = 0.000091
  l2.weight: grad_norm = 0.187290
Total gradient norm: 0.494761
=== Actor Training Debug (Iteration 6898) ===
Q mean: -10.762247
Q std: 14.231863
Actor loss: 10.766209
Action reg: 0.003962
  l1.weight: grad_norm = 0.166615
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.150332
Total gradient norm: 0.511011
=== Actor Training Debug (Iteration 6899) ===
Q mean: -12.237289
Q std: 16.881611
Actor loss: 12.241251
Action reg: 0.003961
  l1.weight: grad_norm = 0.321258
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.254259
Total gradient norm: 0.644432
=== Actor Training Debug (Iteration 6900) ===
Q mean: -13.443403
Q std: 16.973640
Actor loss: 13.447357
Action reg: 0.003954
  l1.weight: grad_norm = 0.416091
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.319819
Total gradient norm: 0.783997
=== Actor Training Debug (Iteration 6901) ===
Q mean: -11.739424
Q std: 15.125476
Actor loss: 11.743376
Action reg: 0.003952
  l1.weight: grad_norm = 0.248829
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.209587
Total gradient norm: 0.552489
=== Actor Training Debug (Iteration 6902) ===
Q mean: -12.457283
Q std: 15.479369
Actor loss: 12.461238
Action reg: 0.003955
  l1.weight: grad_norm = 0.226100
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.171728
Total gradient norm: 0.459625
=== Actor Training Debug (Iteration 6903) ===
Q mean: -12.648168
Q std: 17.060558
Actor loss: 12.652111
Action reg: 0.003944
  l1.weight: grad_norm = 0.206449
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.172983
Total gradient norm: 0.459280
=== Actor Training Debug (Iteration 6904) ===
Q mean: -11.853628
Q std: 15.924273
Actor loss: 11.857596
Action reg: 0.003968
  l1.weight: grad_norm = 0.139549
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.126819
Total gradient norm: 0.332032
=== Actor Training Debug (Iteration 6905) ===
Q mean: -9.585827
Q std: 13.417026
Actor loss: 9.589775
Action reg: 0.003948
  l1.weight: grad_norm = 0.248286
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.222342
Total gradient norm: 0.639520
=== Actor Training Debug (Iteration 6906) ===
Q mean: -14.969352
Q std: 19.358471
Actor loss: 14.973328
Action reg: 0.003976
  l1.weight: grad_norm = 0.142315
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.132114
Total gradient norm: 0.376421
=== Actor Training Debug (Iteration 6907) ===
Q mean: -13.472318
Q std: 18.237795
Actor loss: 13.476280
Action reg: 0.003963
  l1.weight: grad_norm = 0.237431
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.240614
Total gradient norm: 0.557820
=== Actor Training Debug (Iteration 6908) ===
Q mean: -11.187235
Q std: 13.666275
Actor loss: 11.191182
Action reg: 0.003947
  l1.weight: grad_norm = 0.573921
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.420054
Total gradient norm: 1.255930
=== Actor Training Debug (Iteration 6909) ===
Q mean: -13.217159
Q std: 16.135983
Actor loss: 13.221104
Action reg: 0.003945
  l1.weight: grad_norm = 0.338109
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.324584
Total gradient norm: 0.900890
=== Actor Training Debug (Iteration 6910) ===
Q mean: -12.462893
Q std: 16.305849
Actor loss: 12.466835
Action reg: 0.003942
  l1.weight: grad_norm = 0.314035
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.256329
Total gradient norm: 0.582752
=== Actor Training Debug (Iteration 6911) ===
Q mean: -10.879683
Q std: 16.414032
Actor loss: 10.883639
Action reg: 0.003957
  l1.weight: grad_norm = 0.200622
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.166680
Total gradient norm: 0.442205
=== Actor Training Debug (Iteration 6912) ===
Q mean: -11.040008
Q std: 14.584170
Actor loss: 11.043956
Action reg: 0.003949
  l1.weight: grad_norm = 0.216049
  l1.bias: grad_norm = 0.000355
  l2.weight: grad_norm = 0.187172
Total gradient norm: 0.498556
=== Actor Training Debug (Iteration 6913) ===
Q mean: -12.939333
Q std: 15.235663
Actor loss: 12.943290
Action reg: 0.003957
  l1.weight: grad_norm = 0.216533
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.200751
Total gradient norm: 0.566544
=== Actor Training Debug (Iteration 6914) ===
Q mean: -11.888121
Q std: 16.343554
Actor loss: 11.892083
Action reg: 0.003963
  l1.weight: grad_norm = 0.197219
  l1.bias: grad_norm = 0.001279
  l2.weight: grad_norm = 0.168535
Total gradient norm: 0.466704
=== Actor Training Debug (Iteration 6915) ===
Q mean: -11.676595
Q std: 14.361836
Actor loss: 11.680545
Action reg: 0.003950
  l1.weight: grad_norm = 0.284690
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.232879
Total gradient norm: 0.596318
=== Actor Training Debug (Iteration 6916) ===
Q mean: -11.696349
Q std: 15.828297
Actor loss: 11.700313
Action reg: 0.003963
  l1.weight: grad_norm = 0.164743
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.138606
Total gradient norm: 0.368670
=== Actor Training Debug (Iteration 6917) ===
Q mean: -11.780668
Q std: 16.499615
Actor loss: 11.784613
Action reg: 0.003944
  l1.weight: grad_norm = 0.234441
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.182529
Total gradient norm: 0.489880
=== Actor Training Debug (Iteration 6918) ===
Q mean: -10.024109
Q std: 14.368479
Actor loss: 10.028060
Action reg: 0.003951
  l1.weight: grad_norm = 0.300294
  l1.bias: grad_norm = 0.000931
  l2.weight: grad_norm = 0.251805
Total gradient norm: 0.689723
=== Actor Training Debug (Iteration 6919) ===
Q mean: -9.424372
Q std: 11.978226
Actor loss: 9.428323
Action reg: 0.003951
  l1.weight: grad_norm = 0.179877
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.146583
Total gradient norm: 0.351338
=== Actor Training Debug (Iteration 6920) ===
Q mean: -12.293556
Q std: 16.875231
Actor loss: 12.297508
Action reg: 0.003952
  l1.weight: grad_norm = 0.489674
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.417481
Total gradient norm: 1.208665
=== Actor Training Debug (Iteration 6921) ===
Q mean: -10.807645
Q std: 12.727018
Actor loss: 10.811617
Action reg: 0.003972
  l1.weight: grad_norm = 0.180948
  l1.bias: grad_norm = 0.000077
  l2.weight: grad_norm = 0.141054
Total gradient norm: 0.383653
=== Actor Training Debug (Iteration 6922) ===
Q mean: -12.220634
Q std: 17.149862
Actor loss: 12.224574
Action reg: 0.003940
  l1.weight: grad_norm = 0.363499
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.295896
Total gradient norm: 0.737038
=== Actor Training Debug (Iteration 6923) ===
Q mean: -10.811376
Q std: 14.743095
Actor loss: 10.815343
Action reg: 0.003967
  l1.weight: grad_norm = 0.277735
  l1.bias: grad_norm = 0.000123
  l2.weight: grad_norm = 0.269158
Total gradient norm: 0.738198
=== Actor Training Debug (Iteration 6924) ===
Q mean: -10.485821
Q std: 14.717198
Actor loss: 10.489779
Action reg: 0.003959
  l1.weight: grad_norm = 0.127437
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.123174
Total gradient norm: 0.343323
=== Actor Training Debug (Iteration 6925) ===
Q mean: -10.309178
Q std: 14.667081
Actor loss: 10.313136
Action reg: 0.003958
  l1.weight: grad_norm = 0.554952
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.430216
Total gradient norm: 1.181290
=== Actor Training Debug (Iteration 6926) ===
Q mean: -12.304470
Q std: 15.190939
Actor loss: 12.308432
Action reg: 0.003962
  l1.weight: grad_norm = 0.170841
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.144672
Total gradient norm: 0.357598
=== Actor Training Debug (Iteration 6927) ===
Q mean: -11.442271
Q std: 14.351494
Actor loss: 11.446231
Action reg: 0.003960
  l1.weight: grad_norm = 0.177676
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.152650
Total gradient norm: 0.489526
=== Actor Training Debug (Iteration 6928) ===
Q mean: -12.972607
Q std: 16.766321
Actor loss: 12.976580
Action reg: 0.003973
  l1.weight: grad_norm = 0.157406
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.137209
Total gradient norm: 0.391271
=== Actor Training Debug (Iteration 6929) ===
Q mean: -12.547976
Q std: 17.487453
Actor loss: 12.551929
Action reg: 0.003953
  l1.weight: grad_norm = 0.259386
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.229510
Total gradient norm: 0.561786
=== Actor Training Debug (Iteration 6930) ===
Q mean: -12.467024
Q std: 15.313077
Actor loss: 12.470969
Action reg: 0.003946
  l1.weight: grad_norm = 0.414204
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.329850
Total gradient norm: 0.899942
=== Actor Training Debug (Iteration 6931) ===
Q mean: -12.983810
Q std: 16.802835
Actor loss: 12.987764
Action reg: 0.003954
  l1.weight: grad_norm = 0.200450
  l1.bias: grad_norm = 0.000333
  l2.weight: grad_norm = 0.162789
Total gradient norm: 0.442588
=== Actor Training Debug (Iteration 6932) ===
Q mean: -12.973310
Q std: 16.965231
Actor loss: 12.977263
Action reg: 0.003953
  l1.weight: grad_norm = 0.307912
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.273972
Total gradient norm: 0.759150
=== Actor Training Debug (Iteration 6933) ===
Q mean: -11.290974
Q std: 14.721800
Actor loss: 11.294921
Action reg: 0.003948
  l1.weight: grad_norm = 0.197407
  l1.bias: grad_norm = 0.000602
  l2.weight: grad_norm = 0.177403
Total gradient norm: 0.468019
=== Actor Training Debug (Iteration 6934) ===
Q mean: -12.270463
Q std: 15.148489
Actor loss: 12.274429
Action reg: 0.003967
  l1.weight: grad_norm = 0.258633
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.212223
Total gradient norm: 0.555827
=== Actor Training Debug (Iteration 6935) ===
Q mean: -11.732599
Q std: 15.965417
Actor loss: 11.736548
Action reg: 0.003949
  l1.weight: grad_norm = 0.309735
  l1.bias: grad_norm = 0.000599
  l2.weight: grad_norm = 0.282630
Total gradient norm: 0.907805
=== Actor Training Debug (Iteration 6936) ===
Q mean: -12.429955
Q std: 15.649691
Actor loss: 12.433919
Action reg: 0.003964
  l1.weight: grad_norm = 0.268454
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.238478
Total gradient norm: 0.656764
=== Actor Training Debug (Iteration 6937) ===
Q mean: -12.385323
Q std: 16.744884
Actor loss: 12.389276
Action reg: 0.003953
  l1.weight: grad_norm = 0.221105
  l1.bias: grad_norm = 0.000607
  l2.weight: grad_norm = 0.175752
Total gradient norm: 0.455636
=== Actor Training Debug (Iteration 6938) ===
Q mean: -12.257789
Q std: 17.340246
Actor loss: 12.261753
Action reg: 0.003964
  l1.weight: grad_norm = 0.352682
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.280445
Total gradient norm: 0.691917
=== Actor Training Debug (Iteration 6939) ===
Q mean: -10.842659
Q std: 15.046104
Actor loss: 10.846598
Action reg: 0.003938
  l1.weight: grad_norm = 0.206198
  l1.bias: grad_norm = 0.000376
  l2.weight: grad_norm = 0.181686
Total gradient norm: 0.482230
=== Actor Training Debug (Iteration 6940) ===
Q mean: -12.140927
Q std: 14.931845
Actor loss: 12.144872
Action reg: 0.003944
  l1.weight: grad_norm = 0.308486
  l1.bias: grad_norm = 0.000589
  l2.weight: grad_norm = 0.240884
Total gradient norm: 0.623252
=== Actor Training Debug (Iteration 6941) ===
Q mean: -12.194199
Q std: 15.722564
Actor loss: 12.198147
Action reg: 0.003948
  l1.weight: grad_norm = 0.370586
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.370342
Total gradient norm: 1.031843
=== Actor Training Debug (Iteration 6942) ===
Q mean: -11.257034
Q std: 15.387805
Actor loss: 11.261003
Action reg: 0.003969
  l1.weight: grad_norm = 0.084588
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.076249
Total gradient norm: 0.203258
=== Actor Training Debug (Iteration 6943) ===
Q mean: -11.024891
Q std: 15.459648
Actor loss: 11.028859
Action reg: 0.003968
  l1.weight: grad_norm = 0.176177
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.144601
Total gradient norm: 0.370766
=== Actor Training Debug (Iteration 6944) ===
Q mean: -11.841635
Q std: 16.119043
Actor loss: 11.845603
Action reg: 0.003969
  l1.weight: grad_norm = 0.140913
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.125709
Total gradient norm: 0.324424
=== Actor Training Debug (Iteration 6945) ===
Q mean: -11.831961
Q std: 16.963776
Actor loss: 11.835918
Action reg: 0.003958
  l1.weight: grad_norm = 0.270646
  l1.bias: grad_norm = 0.001245
  l2.weight: grad_norm = 0.223013
Total gradient norm: 0.569813
=== Actor Training Debug (Iteration 6946) ===
Q mean: -11.826646
Q std: 13.537177
Actor loss: 11.830616
Action reg: 0.003970
  l1.weight: grad_norm = 0.174514
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.142393
Total gradient norm: 0.376309
=== Actor Training Debug (Iteration 6947) ===
Q mean: -11.712160
Q std: 13.943825
Actor loss: 11.716099
Action reg: 0.003939
  l1.weight: grad_norm = 0.193182
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.157633
Total gradient norm: 0.434087
=== Actor Training Debug (Iteration 6948) ===
Q mean: -12.009352
Q std: 16.094368
Actor loss: 12.013305
Action reg: 0.003953
  l1.weight: grad_norm = 0.300886
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.245464
Total gradient norm: 0.627112
=== Actor Training Debug (Iteration 6949) ===
Q mean: -12.315896
Q std: 16.918310
Actor loss: 12.319859
Action reg: 0.003962
  l1.weight: grad_norm = 0.206112
  l1.bias: grad_norm = 0.001495
  l2.weight: grad_norm = 0.173700
Total gradient norm: 0.444761
=== Actor Training Debug (Iteration 6950) ===
Q mean: -9.255033
Q std: 12.931731
Actor loss: 9.258981
Action reg: 0.003947
  l1.weight: grad_norm = 0.259208
  l1.bias: grad_norm = 0.000973
  l2.weight: grad_norm = 0.197705
Total gradient norm: 0.507913
=== Actor Training Debug (Iteration 6951) ===
Q mean: -11.011636
Q std: 15.245000
Actor loss: 11.015582
Action reg: 0.003946
  l1.weight: grad_norm = 0.237795
  l1.bias: grad_norm = 0.001645
  l2.weight: grad_norm = 0.189828
Total gradient norm: 0.507442
=== Actor Training Debug (Iteration 6952) ===
Q mean: -11.522811
Q std: 14.604089
Actor loss: 11.526749
Action reg: 0.003938
  l1.weight: grad_norm = 0.390711
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.293192
Total gradient norm: 0.747704
=== Actor Training Debug (Iteration 6953) ===
Q mean: -13.584402
Q std: 16.981258
Actor loss: 13.588369
Action reg: 0.003967
  l1.weight: grad_norm = 0.217324
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.170956
Total gradient norm: 0.429794
=== Actor Training Debug (Iteration 6954) ===
Q mean: -12.090710
Q std: 15.448553
Actor loss: 12.094664
Action reg: 0.003954
  l1.weight: grad_norm = 0.310247
  l1.bias: grad_norm = 0.001233
  l2.weight: grad_norm = 0.260798
Total gradient norm: 0.807883
=== Actor Training Debug (Iteration 6955) ===
Q mean: -11.962812
Q std: 14.775596
Actor loss: 11.966771
Action reg: 0.003959
  l1.weight: grad_norm = 0.328951
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.272874
Total gradient norm: 0.730811
=== Actor Training Debug (Iteration 6956) ===
Q mean: -12.431751
Q std: 16.423540
Actor loss: 12.435713
Action reg: 0.003962
  l1.weight: grad_norm = 0.376994
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.333769
Total gradient norm: 0.758664
=== Actor Training Debug (Iteration 6957) ===
Q mean: -13.166117
Q std: 17.128765
Actor loss: 13.170094
Action reg: 0.003977
  l1.weight: grad_norm = 0.168026
  l1.bias: grad_norm = 0.000059
  l2.weight: grad_norm = 0.143959
Total gradient norm: 0.381156
=== Actor Training Debug (Iteration 6958) ===
Q mean: -10.729997
Q std: 15.256506
Actor loss: 10.733968
Action reg: 0.003971
  l1.weight: grad_norm = 0.232039
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.200690
Total gradient norm: 0.512186
=== Actor Training Debug (Iteration 6959) ===
Q mean: -12.657804
Q std: 18.021696
Actor loss: 12.661770
Action reg: 0.003966
  l1.weight: grad_norm = 0.197250
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.161115
Total gradient norm: 0.425701
=== Actor Training Debug (Iteration 6960) ===
Q mean: -10.483074
Q std: 15.514908
Actor loss: 10.487033
Action reg: 0.003958
  l1.weight: grad_norm = 0.175802
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.151365
Total gradient norm: 0.421002
=== Actor Training Debug (Iteration 6961) ===
Q mean: -11.629572
Q std: 14.872650
Actor loss: 11.633527
Action reg: 0.003955
  l1.weight: grad_norm = 0.238396
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.219428
Total gradient norm: 0.519341
=== Actor Training Debug (Iteration 6962) ===
Q mean: -11.584994
Q std: 15.436871
Actor loss: 11.588953
Action reg: 0.003958
  l1.weight: grad_norm = 0.380609
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.278575
Total gradient norm: 0.726090
=== Actor Training Debug (Iteration 6963) ===
Q mean: -11.485807
Q std: 14.272364
Actor loss: 11.489754
Action reg: 0.003947
  l1.weight: grad_norm = 0.274416
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.235087
Total gradient norm: 0.608416
=== Actor Training Debug (Iteration 6964) ===
Q mean: -12.457769
Q std: 14.974580
Actor loss: 12.461720
Action reg: 0.003951
  l1.weight: grad_norm = 0.156415
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.135515
Total gradient norm: 0.338529
=== Actor Training Debug (Iteration 6965) ===
Q mean: -13.134024
Q std: 17.581203
Actor loss: 13.137980
Action reg: 0.003956
  l1.weight: grad_norm = 0.187325
  l1.bias: grad_norm = 0.000262
  l2.weight: grad_norm = 0.178508
Total gradient norm: 0.442089
=== Actor Training Debug (Iteration 6966) ===
Q mean: -9.957246
Q std: 13.280781
Actor loss: 9.961195
Action reg: 0.003949
  l1.weight: grad_norm = 0.270499
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.243086
Total gradient norm: 0.656814
=== Actor Training Debug (Iteration 6967) ===
Q mean: -12.650801
Q std: 14.921532
Actor loss: 12.654772
Action reg: 0.003972
  l1.weight: grad_norm = 0.246527
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.205965
Total gradient norm: 0.490263
=== Actor Training Debug (Iteration 6968) ===
Q mean: -12.323973
Q std: 15.995309
Actor loss: 12.327936
Action reg: 0.003964
  l1.weight: grad_norm = 0.211020
  l1.bias: grad_norm = 0.000085
  l2.weight: grad_norm = 0.181099
Total gradient norm: 0.538054
=== Actor Training Debug (Iteration 6969) ===
Q mean: -12.060545
Q std: 16.935127
Actor loss: 12.064515
Action reg: 0.003970
  l1.weight: grad_norm = 0.208634
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.167606
Total gradient norm: 0.425499
=== Actor Training Debug (Iteration 6970) ===
Q mean: -11.193629
Q std: 16.717043
Actor loss: 11.197560
Action reg: 0.003931
  l1.weight: grad_norm = 0.225080
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.186609
Total gradient norm: 0.520070
=== Actor Training Debug (Iteration 6971) ===
Q mean: -13.289850
Q std: 17.503986
Actor loss: 13.293813
Action reg: 0.003962
  l1.weight: grad_norm = 0.136162
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.119220
Total gradient norm: 0.295427
=== Actor Training Debug (Iteration 6972) ===
Q mean: -12.608261
Q std: 16.839087
Actor loss: 12.612213
Action reg: 0.003952
  l1.weight: grad_norm = 0.347615
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.302713
Total gradient norm: 0.934995
=== Actor Training Debug (Iteration 6973) ===
Q mean: -12.587545
Q std: 15.966492
Actor loss: 12.591502
Action reg: 0.003957
  l1.weight: grad_norm = 0.277486
  l1.bias: grad_norm = 0.000393
  l2.weight: grad_norm = 0.216715
Total gradient norm: 0.616662
=== Actor Training Debug (Iteration 6974) ===
Q mean: -10.946854
Q std: 16.557806
Actor loss: 10.950824
Action reg: 0.003970
  l1.weight: grad_norm = 0.218963
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.185150
Total gradient norm: 0.483690
=== Actor Training Debug (Iteration 6975) ===
Q mean: -11.378268
Q std: 16.634800
Actor loss: 11.382225
Action reg: 0.003957
  l1.weight: grad_norm = 0.244801
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.209774
Total gradient norm: 0.589937
=== Actor Training Debug (Iteration 6976) ===
Q mean: -11.406695
Q std: 16.512184
Actor loss: 11.410648
Action reg: 0.003953
  l1.weight: grad_norm = 0.183928
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.152967
Total gradient norm: 0.384182
=== Actor Training Debug (Iteration 6977) ===
Q mean: -13.188832
Q std: 16.498926
Actor loss: 13.192792
Action reg: 0.003959
  l1.weight: grad_norm = 0.155600
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.145578
Total gradient norm: 0.413039
=== Actor Training Debug (Iteration 6978) ===
Q mean: -12.278468
Q std: 15.167005
Actor loss: 12.282431
Action reg: 0.003962
  l1.weight: grad_norm = 0.326215
  l1.bias: grad_norm = 0.000398
  l2.weight: grad_norm = 0.271429
Total gradient norm: 0.711045
=== Actor Training Debug (Iteration 6979) ===
Q mean: -11.655031
Q std: 15.654094
Actor loss: 11.658987
Action reg: 0.003955
  l1.weight: grad_norm = 0.394233
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.292365
Total gradient norm: 0.737866
=== Actor Training Debug (Iteration 6980) ===
Q mean: -13.135017
Q std: 16.466784
Actor loss: 13.138984
Action reg: 0.003966
  l1.weight: grad_norm = 0.126711
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.125682
Total gradient norm: 0.404703
=== Actor Training Debug (Iteration 6981) ===
Q mean: -11.264135
Q std: 16.902351
Actor loss: 11.268098
Action reg: 0.003963
  l1.weight: grad_norm = 0.183399
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.151949
Total gradient norm: 0.401654
=== Actor Training Debug (Iteration 6982) ===
Q mean: -10.799017
Q std: 14.788594
Actor loss: 10.802979
Action reg: 0.003962
  l1.weight: grad_norm = 0.180673
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.138284
Total gradient norm: 0.365112
=== Actor Training Debug (Iteration 6983) ===
Q mean: -13.418888
Q std: 17.695995
Actor loss: 13.422840
Action reg: 0.003952
  l1.weight: grad_norm = 0.220878
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.187972
Total gradient norm: 0.525080
=== Actor Training Debug (Iteration 6984) ===
Q mean: -13.783781
Q std: 16.381346
Actor loss: 13.787763
Action reg: 0.003982
  l1.weight: grad_norm = 0.110689
  l1.bias: grad_norm = 0.000050
  l2.weight: grad_norm = 0.086911
Total gradient norm: 0.230255
=== Actor Training Debug (Iteration 6985) ===
Q mean: -10.369826
Q std: 13.402205
Actor loss: 10.373775
Action reg: 0.003949
  l1.weight: grad_norm = 0.227107
  l1.bias: grad_norm = 0.000253
  l2.weight: grad_norm = 0.187808
Total gradient norm: 0.465413
=== Actor Training Debug (Iteration 6986) ===
Q mean: -10.381274
Q std: 14.245042
Actor loss: 10.385235
Action reg: 0.003960
  l1.weight: grad_norm = 0.111033
  l1.bias: grad_norm = 0.001508
  l2.weight: grad_norm = 0.103946
Total gradient norm: 0.328908
=== Actor Training Debug (Iteration 6987) ===
Q mean: -11.906160
Q std: 15.697615
Actor loss: 11.910137
Action reg: 0.003977
  l1.weight: grad_norm = 0.223455
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.193819
Total gradient norm: 0.556034
=== Actor Training Debug (Iteration 6988) ===
Q mean: -14.042328
Q std: 17.970785
Actor loss: 14.046285
Action reg: 0.003956
  l1.weight: grad_norm = 0.429421
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.334317
Total gradient norm: 0.854793
=== Actor Training Debug (Iteration 6989) ===
Q mean: -13.677013
Q std: 17.264029
Actor loss: 13.680963
Action reg: 0.003949
  l1.weight: grad_norm = 0.359313
  l1.bias: grad_norm = 0.001043
  l2.weight: grad_norm = 0.296076
Total gradient norm: 0.791013
=== Actor Training Debug (Iteration 6990) ===
Q mean: -14.405186
Q std: 18.489853
Actor loss: 14.409144
Action reg: 0.003959
  l1.weight: grad_norm = 0.262683
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.225876
Total gradient norm: 0.525494
=== Actor Training Debug (Iteration 6991) ===
Q mean: -13.148793
Q std: 16.002361
Actor loss: 13.152747
Action reg: 0.003954
  l1.weight: grad_norm = 0.379018
  l1.bias: grad_norm = 0.001103
  l2.weight: grad_norm = 0.299189
Total gradient norm: 0.722158
=== Actor Training Debug (Iteration 6992) ===
Q mean: -10.918145
Q std: 14.775819
Actor loss: 10.922091
Action reg: 0.003946
  l1.weight: grad_norm = 0.275849
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.230428
Total gradient norm: 0.710764
=== Actor Training Debug (Iteration 6993) ===
Q mean: -12.138987
Q std: 16.151793
Actor loss: 12.142948
Action reg: 0.003962
  l1.weight: grad_norm = 0.247091
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.227873
Total gradient norm: 0.596419
=== Actor Training Debug (Iteration 6994) ===
Q mean: -11.983908
Q std: 16.995712
Actor loss: 11.987852
Action reg: 0.003944
  l1.weight: grad_norm = 0.191128
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.162235
Total gradient norm: 0.472574
=== Actor Training Debug (Iteration 6995) ===
Q mean: -11.629072
Q std: 14.245067
Actor loss: 11.633021
Action reg: 0.003950
  l1.weight: grad_norm = 0.152074
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.124150
Total gradient norm: 0.290580
=== Actor Training Debug (Iteration 6996) ===
Q mean: -12.104610
Q std: 15.190361
Actor loss: 12.108571
Action reg: 0.003961
  l1.weight: grad_norm = 0.412714
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.339579
Total gradient norm: 0.880195
=== Actor Training Debug (Iteration 6997) ===
Q mean: -11.606258
Q std: 16.151146
Actor loss: 11.610214
Action reg: 0.003956
  l1.weight: grad_norm = 0.419911
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.335814
Total gradient norm: 0.769593
=== Actor Training Debug (Iteration 6998) ===
Q mean: -11.705154
Q std: 15.992117
Actor loss: 11.709108
Action reg: 0.003954
  l1.weight: grad_norm = 0.208898
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.185818
Total gradient norm: 0.539365
=== Actor Training Debug (Iteration 6999) ===
Q mean: -13.558573
Q std: 17.818357
Actor loss: 13.562534
Action reg: 0.003962
  l1.weight: grad_norm = 0.182011
  l1.bias: grad_norm = 0.000436
  l2.weight: grad_norm = 0.155790
Total gradient norm: 0.420311
=== Actor Training Debug (Iteration 7000) ===
Q mean: -10.232079
Q std: 14.133495
Actor loss: 10.236042
Action reg: 0.003964
  l1.weight: grad_norm = 0.160678
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.133741
Total gradient norm: 0.404485
Step 12000: Critic Loss: 1.6876, Actor Loss: 10.2360, Q Value: -10.2321
  Average reward: -325.912 | Average length: 100.0
Evaluation at episode 120: -325.912
=== Actor Training Debug (Iteration 7001) ===
Q mean: -14.136782
Q std: 19.219711
Actor loss: 14.140733
Action reg: 0.003951
  l1.weight: grad_norm = 0.256180
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.205075
Total gradient norm: 0.590943
=== Actor Training Debug (Iteration 7002) ===
Q mean: -12.003246
Q std: 15.374571
Actor loss: 12.007194
Action reg: 0.003947
  l1.weight: grad_norm = 0.273673
  l1.bias: grad_norm = 0.000135
  l2.weight: grad_norm = 0.226025
Total gradient norm: 0.638044
=== Actor Training Debug (Iteration 7003) ===
Q mean: -12.655910
Q std: 16.175491
Actor loss: 12.659831
Action reg: 0.003920
  l1.weight: grad_norm = 0.174616
  l1.bias: grad_norm = 0.000763
  l2.weight: grad_norm = 0.157228
Total gradient norm: 0.484725
=== Actor Training Debug (Iteration 7004) ===
Q mean: -10.313144
Q std: 14.983466
Actor loss: 10.317098
Action reg: 0.003954
  l1.weight: grad_norm = 0.306346
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.222805
Total gradient norm: 0.596298
=== Actor Training Debug (Iteration 7005) ===
Q mean: -11.876432
Q std: 16.509132
Actor loss: 11.880398
Action reg: 0.003965
  l1.weight: grad_norm = 0.182020
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.132215
Total gradient norm: 0.336158
=== Actor Training Debug (Iteration 7006) ===
Q mean: -12.724953
Q std: 17.398529
Actor loss: 12.728926
Action reg: 0.003973
  l1.weight: grad_norm = 0.236042
  l1.bias: grad_norm = 0.000110
  l2.weight: grad_norm = 0.198742
Total gradient norm: 0.507826
=== Actor Training Debug (Iteration 7007) ===
Q mean: -11.793247
Q std: 17.112581
Actor loss: 11.797193
Action reg: 0.003946
  l1.weight: grad_norm = 0.465166
  l1.bias: grad_norm = 0.000668
  l2.weight: grad_norm = 0.326026
Total gradient norm: 0.865318
=== Actor Training Debug (Iteration 7008) ===
Q mean: -11.201740
Q std: 15.987040
Actor loss: 11.205697
Action reg: 0.003957
  l1.weight: grad_norm = 0.661457
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.473814
Total gradient norm: 1.565451
=== Actor Training Debug (Iteration 7009) ===
Q mean: -13.034592
Q std: 17.370245
Actor loss: 13.038545
Action reg: 0.003953
  l1.weight: grad_norm = 0.242664
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.220835
Total gradient norm: 0.518746
=== Actor Training Debug (Iteration 7010) ===
Q mean: -9.895376
Q std: 13.878197
Actor loss: 9.899321
Action reg: 0.003944
  l1.weight: grad_norm = 0.440310
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.351889
Total gradient norm: 0.850856
=== Actor Training Debug (Iteration 7011) ===
Q mean: -10.876226
Q std: 14.118970
Actor loss: 10.880178
Action reg: 0.003952
  l1.weight: grad_norm = 0.279131
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.228540
Total gradient norm: 0.621447
=== Actor Training Debug (Iteration 7012) ===
Q mean: -10.203737
Q std: 14.415193
Actor loss: 10.207698
Action reg: 0.003961
  l1.weight: grad_norm = 0.187603
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.161273
Total gradient norm: 0.387680
=== Actor Training Debug (Iteration 7013) ===
Q mean: -12.129358
Q std: 16.269291
Actor loss: 12.133318
Action reg: 0.003959
  l1.weight: grad_norm = 0.198555
  l1.bias: grad_norm = 0.000244
  l2.weight: grad_norm = 0.166219
Total gradient norm: 0.417511
=== Actor Training Debug (Iteration 7014) ===
Q mean: -11.341644
Q std: 14.976653
Actor loss: 11.345604
Action reg: 0.003960
  l1.weight: grad_norm = 0.174807
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.144161
Total gradient norm: 0.398467
=== Actor Training Debug (Iteration 7015) ===
Q mean: -11.764746
Q std: 15.148600
Actor loss: 11.768701
Action reg: 0.003955
  l1.weight: grad_norm = 0.500996
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.384982
Total gradient norm: 0.993463
=== Actor Training Debug (Iteration 7016) ===
Q mean: -13.239027
Q std: 18.229197
Actor loss: 13.242970
Action reg: 0.003943
  l1.weight: grad_norm = 0.187402
  l1.bias: grad_norm = 0.000350
  l2.weight: grad_norm = 0.173296
Total gradient norm: 0.447712
=== Actor Training Debug (Iteration 7017) ===
Q mean: -12.756496
Q std: 15.274911
Actor loss: 12.760461
Action reg: 0.003964
  l1.weight: grad_norm = 0.286349
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.241106
Total gradient norm: 0.654959
=== Actor Training Debug (Iteration 7018) ===
Q mean: -11.103933
Q std: 15.337158
Actor loss: 11.107871
Action reg: 0.003938
  l1.weight: grad_norm = 0.274444
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.215098
Total gradient norm: 0.565276
=== Actor Training Debug (Iteration 7019) ===
Q mean: -11.956724
Q std: 16.426981
Actor loss: 11.960677
Action reg: 0.003953
  l1.weight: grad_norm = 0.210357
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.166152
Total gradient norm: 0.451956
=== Actor Training Debug (Iteration 7020) ===
Q mean: -12.905521
Q std: 16.838030
Actor loss: 12.909484
Action reg: 0.003963
  l1.weight: grad_norm = 0.214486
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.183067
Total gradient norm: 0.465517
=== Actor Training Debug (Iteration 7021) ===
Q mean: -11.120884
Q std: 15.407267
Actor loss: 11.124849
Action reg: 0.003965
  l1.weight: grad_norm = 0.159388
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.126195
Total gradient norm: 0.338660
=== Actor Training Debug (Iteration 7022) ===
Q mean: -11.714699
Q std: 16.440680
Actor loss: 11.718650
Action reg: 0.003951
  l1.weight: grad_norm = 0.288538
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.246917
Total gradient norm: 0.658941
=== Actor Training Debug (Iteration 7023) ===
Q mean: -12.861721
Q std: 17.047476
Actor loss: 12.865702
Action reg: 0.003981
  l1.weight: grad_norm = 0.154906
  l1.bias: grad_norm = 0.000061
  l2.weight: grad_norm = 0.139540
Total gradient norm: 0.365767
=== Actor Training Debug (Iteration 7024) ===
Q mean: -12.215851
Q std: 14.671645
Actor loss: 12.219824
Action reg: 0.003973
  l1.weight: grad_norm = 0.260529
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.204564
Total gradient norm: 0.568318
=== Actor Training Debug (Iteration 7025) ===
Q mean: -10.640657
Q std: 14.495439
Actor loss: 10.644618
Action reg: 0.003960
  l1.weight: grad_norm = 0.164424
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.150297
Total gradient norm: 0.456161
=== Actor Training Debug (Iteration 7026) ===
Q mean: -12.364217
Q std: 16.879366
Actor loss: 12.368157
Action reg: 0.003941
  l1.weight: grad_norm = 0.189197
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.148021
Total gradient norm: 0.361744
=== Actor Training Debug (Iteration 7027) ===
Q mean: -13.015309
Q std: 18.754860
Actor loss: 13.019262
Action reg: 0.003953
  l1.weight: grad_norm = 0.145047
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.128591
Total gradient norm: 0.339466
=== Actor Training Debug (Iteration 7028) ===
Q mean: -13.278982
Q std: 15.187559
Actor loss: 13.282939
Action reg: 0.003957
  l1.weight: grad_norm = 0.228106
  l1.bias: grad_norm = 0.001144
  l2.weight: grad_norm = 0.194439
Total gradient norm: 0.536602
=== Actor Training Debug (Iteration 7029) ===
Q mean: -14.062071
Q std: 16.867979
Actor loss: 14.066030
Action reg: 0.003958
  l1.weight: grad_norm = 0.205147
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.187264
Total gradient norm: 0.481281
=== Actor Training Debug (Iteration 7030) ===
Q mean: -12.667063
Q std: 17.524099
Actor loss: 12.671028
Action reg: 0.003965
  l1.weight: grad_norm = 0.515877
  l1.bias: grad_norm = 0.000298
  l2.weight: grad_norm = 0.412801
Total gradient norm: 1.165452
=== Actor Training Debug (Iteration 7031) ===
Q mean: -11.633327
Q std: 13.905690
Actor loss: 11.637295
Action reg: 0.003968
  l1.weight: grad_norm = 0.237809
  l1.bias: grad_norm = 0.001021
  l2.weight: grad_norm = 0.231831
Total gradient norm: 0.661209
=== Actor Training Debug (Iteration 7032) ===
Q mean: -13.999004
Q std: 18.074755
Actor loss: 14.002966
Action reg: 0.003961
  l1.weight: grad_norm = 0.166190
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.144759
Total gradient norm: 0.383048
=== Actor Training Debug (Iteration 7033) ===
Q mean: -11.409611
Q std: 13.601117
Actor loss: 11.413560
Action reg: 0.003950
  l1.weight: grad_norm = 0.322178
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.306307
Total gradient norm: 0.918715
=== Actor Training Debug (Iteration 7034) ===
Q mean: -11.844908
Q std: 15.171329
Actor loss: 11.848863
Action reg: 0.003955
  l1.weight: grad_norm = 0.361706
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.307610
Total gradient norm: 0.832430
=== Actor Training Debug (Iteration 7035) ===
Q mean: -11.625450
Q std: 14.375691
Actor loss: 11.629414
Action reg: 0.003964
  l1.weight: grad_norm = 0.251831
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.216459
Total gradient norm: 0.598382
=== Actor Training Debug (Iteration 7036) ===
Q mean: -12.497178
Q std: 16.971575
Actor loss: 12.501135
Action reg: 0.003956
  l1.weight: grad_norm = 0.217226
  l1.bias: grad_norm = 0.000153
  l2.weight: grad_norm = 0.206865
Total gradient norm: 0.551834
=== Actor Training Debug (Iteration 7037) ===
Q mean: -11.597387
Q std: 15.243174
Actor loss: 11.601346
Action reg: 0.003959
  l1.weight: grad_norm = 0.431216
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.363428
Total gradient norm: 0.987531
=== Actor Training Debug (Iteration 7038) ===
Q mean: -11.354994
Q std: 14.414762
Actor loss: 11.358951
Action reg: 0.003957
  l1.weight: grad_norm = 0.347041
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.301066
Total gradient norm: 0.712965
=== Actor Training Debug (Iteration 7039) ===
Q mean: -10.829829
Q std: 14.203307
Actor loss: 10.833773
Action reg: 0.003944
  l1.weight: grad_norm = 0.288369
  l1.bias: grad_norm = 0.001103
  l2.weight: grad_norm = 0.229366
Total gradient norm: 0.577306
=== Actor Training Debug (Iteration 7040) ===
Q mean: -9.928731
Q std: 10.981791
Actor loss: 9.932675
Action reg: 0.003944
  l1.weight: grad_norm = 0.322542
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.284814
Total gradient norm: 0.880983
=== Actor Training Debug (Iteration 7041) ===
Q mean: -12.311828
Q std: 15.998840
Actor loss: 12.315778
Action reg: 0.003950
  l1.weight: grad_norm = 0.347166
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.271008
Total gradient norm: 0.684373
=== Actor Training Debug (Iteration 7042) ===
Q mean: -11.417726
Q std: 15.343308
Actor loss: 11.421688
Action reg: 0.003962
  l1.weight: grad_norm = 0.178770
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.139674
Total gradient norm: 0.390095
=== Actor Training Debug (Iteration 7043) ===
Q mean: -13.111281
Q std: 16.179852
Actor loss: 13.115225
Action reg: 0.003943
  l1.weight: grad_norm = 0.169336
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.156612
Total gradient norm: 0.466345
=== Actor Training Debug (Iteration 7044) ===
Q mean: -11.482031
Q std: 15.280436
Actor loss: 11.486007
Action reg: 0.003976
  l1.weight: grad_norm = 0.138691
  l1.bias: grad_norm = 0.000112
  l2.weight: grad_norm = 0.122274
Total gradient norm: 0.303944
=== Actor Training Debug (Iteration 7045) ===
Q mean: -11.997955
Q std: 16.527531
Actor loss: 12.001892
Action reg: 0.003936
  l1.weight: grad_norm = 0.208280
  l1.bias: grad_norm = 0.000584
  l2.weight: grad_norm = 0.182460
Total gradient norm: 0.553675
=== Actor Training Debug (Iteration 7046) ===
Q mean: -12.976454
Q std: 18.084965
Actor loss: 12.980404
Action reg: 0.003950
  l1.weight: grad_norm = 0.206589
  l1.bias: grad_norm = 0.000543
  l2.weight: grad_norm = 0.201165
Total gradient norm: 0.546523
=== Actor Training Debug (Iteration 7047) ===
Q mean: -13.913046
Q std: 18.141506
Actor loss: 13.916991
Action reg: 0.003946
  l1.weight: grad_norm = 0.243225
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.212096
Total gradient norm: 0.534253
=== Actor Training Debug (Iteration 7048) ===
Q mean: -12.122629
Q std: 16.060635
Actor loss: 12.126587
Action reg: 0.003958
  l1.weight: grad_norm = 0.228713
  l1.bias: grad_norm = 0.000628
  l2.weight: grad_norm = 0.174935
Total gradient norm: 0.432174
=== Actor Training Debug (Iteration 7049) ===
Q mean: -11.177410
Q std: 15.492730
Actor loss: 11.181384
Action reg: 0.003974
  l1.weight: grad_norm = 0.164290
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.154890
Total gradient norm: 0.468281
=== Actor Training Debug (Iteration 7050) ===
Q mean: -12.189894
Q std: 15.994929
Actor loss: 12.193870
Action reg: 0.003976
  l1.weight: grad_norm = 0.159017
  l1.bias: grad_norm = 0.000193
  l2.weight: grad_norm = 0.129388
Total gradient norm: 0.309850
=== Actor Training Debug (Iteration 7051) ===
Q mean: -12.429439
Q std: 17.117615
Actor loss: 12.433368
Action reg: 0.003929
  l1.weight: grad_norm = 0.426407
  l1.bias: grad_norm = 0.000480
  l2.weight: grad_norm = 0.333831
Total gradient norm: 0.836811
=== Actor Training Debug (Iteration 7052) ===
Q mean: -14.362487
Q std: 18.285351
Actor loss: 14.366453
Action reg: 0.003967
  l1.weight: grad_norm = 0.164356
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.141312
Total gradient norm: 0.363603
=== Actor Training Debug (Iteration 7053) ===
Q mean: -12.601624
Q std: 15.736277
Actor loss: 12.605590
Action reg: 0.003967
  l1.weight: grad_norm = 0.277884
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.256133
Total gradient norm: 0.718689
=== Actor Training Debug (Iteration 7054) ===
Q mean: -10.726442
Q std: 15.261768
Actor loss: 10.730394
Action reg: 0.003952
  l1.weight: grad_norm = 0.199861
  l1.bias: grad_norm = 0.000670
  l2.weight: grad_norm = 0.162794
Total gradient norm: 0.420251
=== Actor Training Debug (Iteration 7055) ===
Q mean: -12.000914
Q std: 17.472687
Actor loss: 12.004874
Action reg: 0.003961
  l1.weight: grad_norm = 0.182166
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.144116
Total gradient norm: 0.417278
=== Actor Training Debug (Iteration 7056) ===
Q mean: -10.187878
Q std: 13.982709
Actor loss: 10.191843
Action reg: 0.003965
  l1.weight: grad_norm = 0.276611
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.237853
Total gradient norm: 0.660890
=== Actor Training Debug (Iteration 7057) ===
Q mean: -12.639937
Q std: 14.851179
Actor loss: 12.643899
Action reg: 0.003961
  l1.weight: grad_norm = 0.198750
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.160323
Total gradient norm: 0.438251
=== Actor Training Debug (Iteration 7058) ===
Q mean: -13.331505
Q std: 16.944323
Actor loss: 13.335450
Action reg: 0.003945
  l1.weight: grad_norm = 0.532980
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.428386
Total gradient norm: 1.169000
=== Actor Training Debug (Iteration 7059) ===
Q mean: -12.677954
Q std: 17.138191
Actor loss: 12.681922
Action reg: 0.003969
  l1.weight: grad_norm = 0.236215
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.215215
Total gradient norm: 0.542941
=== Actor Training Debug (Iteration 7060) ===
Q mean: -13.137840
Q std: 18.097759
Actor loss: 13.141794
Action reg: 0.003954
  l1.weight: grad_norm = 0.365477
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.312949
Total gradient norm: 0.906398
=== Actor Training Debug (Iteration 7061) ===
Q mean: -12.318971
Q std: 16.138908
Actor loss: 12.322945
Action reg: 0.003974
  l1.weight: grad_norm = 0.247948
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.209627
Total gradient norm: 0.554427
=== Actor Training Debug (Iteration 7062) ===
Q mean: -12.559681
Q std: 16.043909
Actor loss: 12.563611
Action reg: 0.003930
  l1.weight: grad_norm = 0.181800
  l1.bias: grad_norm = 0.003663
  l2.weight: grad_norm = 0.151372
Total gradient norm: 0.421420
=== Actor Training Debug (Iteration 7063) ===
Q mean: -12.972080
Q std: 15.956012
Actor loss: 12.976027
Action reg: 0.003947
  l1.weight: grad_norm = 0.285558
  l1.bias: grad_norm = 0.001217
  l2.weight: grad_norm = 0.256462
Total gradient norm: 0.777934
=== Actor Training Debug (Iteration 7064) ===
Q mean: -12.334703
Q std: 15.111447
Actor loss: 12.338655
Action reg: 0.003951
  l1.weight: grad_norm = 0.234518
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.202766
Total gradient norm: 0.559376
=== Actor Training Debug (Iteration 7065) ===
Q mean: -12.950471
Q std: 16.742294
Actor loss: 12.954428
Action reg: 0.003956
  l1.weight: grad_norm = 0.178998
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.155295
Total gradient norm: 0.371782
=== Actor Training Debug (Iteration 7066) ===
Q mean: -11.669827
Q std: 14.281187
Actor loss: 11.673780
Action reg: 0.003953
  l1.weight: grad_norm = 0.246126
  l1.bias: grad_norm = 0.001343
  l2.weight: grad_norm = 0.204170
Total gradient norm: 0.555830
=== Actor Training Debug (Iteration 7067) ===
Q mean: -12.086768
Q std: 15.912853
Actor loss: 12.090727
Action reg: 0.003959
  l1.weight: grad_norm = 0.375762
  l1.bias: grad_norm = 0.000148
  l2.weight: grad_norm = 0.326751
Total gradient norm: 0.868500
=== Actor Training Debug (Iteration 7068) ===
Q mean: -12.742680
Q std: 17.740999
Actor loss: 12.746648
Action reg: 0.003968
  l1.weight: grad_norm = 0.153932
  l1.bias: grad_norm = 0.000448
  l2.weight: grad_norm = 0.130873
Total gradient norm: 0.339931
=== Actor Training Debug (Iteration 7069) ===
Q mean: -13.161065
Q std: 17.415205
Actor loss: 13.165030
Action reg: 0.003966
  l1.weight: grad_norm = 0.115567
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.092721
Total gradient norm: 0.250143
=== Actor Training Debug (Iteration 7070) ===
Q mean: -12.475854
Q std: 14.498672
Actor loss: 12.479817
Action reg: 0.003964
  l1.weight: grad_norm = 0.193862
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.193159
Total gradient norm: 0.515403
=== Actor Training Debug (Iteration 7071) ===
Q mean: -11.910691
Q std: 15.939473
Actor loss: 11.914658
Action reg: 0.003966
  l1.weight: grad_norm = 0.246262
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.216231
Total gradient norm: 0.562829
=== Actor Training Debug (Iteration 7072) ===
Q mean: -10.787140
Q std: 12.346911
Actor loss: 10.791094
Action reg: 0.003954
  l1.weight: grad_norm = 0.183843
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.161687
Total gradient norm: 0.374543
=== Actor Training Debug (Iteration 7073) ===
Q mean: -10.020452
Q std: 12.719035
Actor loss: 10.024395
Action reg: 0.003943
  l1.weight: grad_norm = 0.297770
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.241095
Total gradient norm: 0.580114
=== Actor Training Debug (Iteration 7074) ===
Q mean: -12.551513
Q std: 16.477669
Actor loss: 12.555476
Action reg: 0.003964
  l1.weight: grad_norm = 0.229323
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.184767
Total gradient norm: 0.449698
=== Actor Training Debug (Iteration 7075) ===
Q mean: -11.364863
Q std: 15.328932
Actor loss: 11.368819
Action reg: 0.003956
  l1.weight: grad_norm = 0.246829
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.194557
Total gradient norm: 0.519407
=== Actor Training Debug (Iteration 7076) ===
Q mean: -12.305183
Q std: 17.060392
Actor loss: 12.309118
Action reg: 0.003935
  l1.weight: grad_norm = 0.272251
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.239943
Total gradient norm: 0.582134
=== Actor Training Debug (Iteration 7077) ===
Q mean: -12.051458
Q std: 15.857097
Actor loss: 12.055435
Action reg: 0.003976
  l1.weight: grad_norm = 0.197446
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.155999
Total gradient norm: 0.385707
=== Actor Training Debug (Iteration 7078) ===
Q mean: -12.008144
Q std: 15.504634
Actor loss: 12.012103
Action reg: 0.003959
  l1.weight: grad_norm = 0.092666
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.083770
Total gradient norm: 0.242926
=== Actor Training Debug (Iteration 7079) ===
Q mean: -13.299105
Q std: 17.089977
Actor loss: 13.303064
Action reg: 0.003960
  l1.weight: grad_norm = 0.232670
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.209125
Total gradient norm: 0.476146
=== Actor Training Debug (Iteration 7080) ===
Q mean: -11.957542
Q std: 13.440742
Actor loss: 11.961496
Action reg: 0.003954
  l1.weight: grad_norm = 0.155726
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.136752
Total gradient norm: 0.382878
=== Actor Training Debug (Iteration 7081) ===
Q mean: -11.314793
Q std: 15.533523
Actor loss: 11.318757
Action reg: 0.003965
  l1.weight: grad_norm = 0.321971
  l1.bias: grad_norm = 0.000118
  l2.weight: grad_norm = 0.245835
Total gradient norm: 0.648029
=== Actor Training Debug (Iteration 7082) ===
Q mean: -12.505263
Q std: 16.264324
Actor loss: 12.509218
Action reg: 0.003955
  l1.weight: grad_norm = 0.286706
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.221001
Total gradient norm: 0.486549
=== Actor Training Debug (Iteration 7083) ===
Q mean: -12.774331
Q std: 17.442286
Actor loss: 12.778284
Action reg: 0.003953
  l1.weight: grad_norm = 0.140566
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.124990
Total gradient norm: 0.363410
=== Actor Training Debug (Iteration 7084) ===
Q mean: -13.649109
Q std: 15.889284
Actor loss: 13.653072
Action reg: 0.003963
  l1.weight: grad_norm = 0.191493
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.174495
Total gradient norm: 0.567219
=== Actor Training Debug (Iteration 7085) ===
Q mean: -13.072005
Q std: 17.413912
Actor loss: 13.075972
Action reg: 0.003966
  l1.weight: grad_norm = 0.216032
  l1.bias: grad_norm = 0.000191
  l2.weight: grad_norm = 0.185942
Total gradient norm: 0.538207
=== Actor Training Debug (Iteration 7086) ===
Q mean: -11.856562
Q std: 15.790515
Actor loss: 11.860538
Action reg: 0.003976
  l1.weight: grad_norm = 0.126323
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.107353
Total gradient norm: 0.308237
=== Actor Training Debug (Iteration 7087) ===
Q mean: -11.880527
Q std: 14.901042
Actor loss: 11.884499
Action reg: 0.003971
  l1.weight: grad_norm = 0.225855
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.222634
Total gradient norm: 0.554826
=== Actor Training Debug (Iteration 7088) ===
Q mean: -11.921970
Q std: 14.155736
Actor loss: 11.925920
Action reg: 0.003949
  l1.weight: grad_norm = 0.091625
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.080200
Total gradient norm: 0.203799
=== Actor Training Debug (Iteration 7089) ===
Q mean: -10.935455
Q std: 13.226896
Actor loss: 10.939390
Action reg: 0.003935
  l1.weight: grad_norm = 0.126749
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.117573
Total gradient norm: 0.310016
=== Actor Training Debug (Iteration 7090) ===
Q mean: -12.102623
Q std: 14.481759
Actor loss: 12.106560
Action reg: 0.003936
  l1.weight: grad_norm = 0.277611
  l1.bias: grad_norm = 0.000633
  l2.weight: grad_norm = 0.233226
Total gradient norm: 0.679909
=== Actor Training Debug (Iteration 7091) ===
Q mean: -12.089705
Q std: 14.893867
Actor loss: 12.093651
Action reg: 0.003947
  l1.weight: grad_norm = 0.211809
  l1.bias: grad_norm = 0.000523
  l2.weight: grad_norm = 0.172761
Total gradient norm: 0.487506
=== Actor Training Debug (Iteration 7092) ===
Q mean: -12.718138
Q std: 16.477856
Actor loss: 12.722078
Action reg: 0.003941
  l1.weight: grad_norm = 0.503295
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.422577
Total gradient norm: 1.168632
=== Actor Training Debug (Iteration 7093) ===
Q mean: -11.291948
Q std: 15.789283
Actor loss: 11.295903
Action reg: 0.003955
  l1.weight: grad_norm = 0.127271
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.102223
Total gradient norm: 0.282877
=== Actor Training Debug (Iteration 7094) ===
Q mean: -12.718777
Q std: 16.583868
Actor loss: 12.722743
Action reg: 0.003967
  l1.weight: grad_norm = 0.230319
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.171445
Total gradient norm: 0.416932
=== Actor Training Debug (Iteration 7095) ===
Q mean: -14.220893
Q std: 17.861715
Actor loss: 14.224834
Action reg: 0.003942
  l1.weight: grad_norm = 0.245568
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.212311
Total gradient norm: 0.528837
=== Actor Training Debug (Iteration 7096) ===
Q mean: -11.880075
Q std: 15.810053
Actor loss: 11.884034
Action reg: 0.003960
  l1.weight: grad_norm = 0.351449
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.254575
Total gradient norm: 0.708913
=== Actor Training Debug (Iteration 7097) ===
Q mean: -12.645103
Q std: 15.240816
Actor loss: 12.649054
Action reg: 0.003951
  l1.weight: grad_norm = 0.184654
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.155125
Total gradient norm: 0.426199
=== Actor Training Debug (Iteration 7098) ===
Q mean: -11.652513
Q std: 14.396627
Actor loss: 11.656470
Action reg: 0.003958
  l1.weight: grad_norm = 0.236913
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.176044
Total gradient norm: 0.428227
=== Actor Training Debug (Iteration 7099) ===
Q mean: -9.860352
Q std: 13.390527
Actor loss: 9.864301
Action reg: 0.003949
  l1.weight: grad_norm = 0.184546
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.157706
Total gradient norm: 0.405880
=== Actor Training Debug (Iteration 7100) ===
Q mean: -13.076356
Q std: 16.799032
Actor loss: 13.080310
Action reg: 0.003954
  l1.weight: grad_norm = 0.214327
  l1.bias: grad_norm = 0.001548
  l2.weight: grad_norm = 0.180317
Total gradient norm: 0.485329
Episode 121: Steps=100, Reward=-273.160, Buffer_size=12100
=== Actor Training Debug (Iteration 7101) ===
Q mean: -12.499146
Q std: 16.364939
Actor loss: 12.503102
Action reg: 0.003956
  l1.weight: grad_norm = 0.227814
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.188488
Total gradient norm: 0.533028
=== Actor Training Debug (Iteration 7102) ===
Q mean: -12.673605
Q std: 15.148172
Actor loss: 12.677570
Action reg: 0.003966
  l1.weight: grad_norm = 0.160623
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.148200
Total gradient norm: 0.388600
=== Actor Training Debug (Iteration 7103) ===
Q mean: -9.385972
Q std: 12.705733
Actor loss: 9.389941
Action reg: 0.003969
  l1.weight: grad_norm = 0.243319
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.202721
Total gradient norm: 0.507212
=== Actor Training Debug (Iteration 7104) ===
Q mean: -12.736605
Q std: 15.540101
Actor loss: 12.740549
Action reg: 0.003944
  l1.weight: grad_norm = 0.338908
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.316260
Total gradient norm: 0.980988
=== Actor Training Debug (Iteration 7105) ===
Q mean: -11.379718
Q std: 14.088026
Actor loss: 11.383649
Action reg: 0.003931
  l1.weight: grad_norm = 0.274149
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.246230
Total gradient norm: 0.614751
=== Actor Training Debug (Iteration 7106) ===
Q mean: -11.134790
Q std: 15.913128
Actor loss: 11.138756
Action reg: 0.003965
  l1.weight: grad_norm = 0.158459
  l1.bias: grad_norm = 0.000144
  l2.weight: grad_norm = 0.133628
Total gradient norm: 0.388067
=== Actor Training Debug (Iteration 7107) ===
Q mean: -12.402159
Q std: 16.255938
Actor loss: 12.406111
Action reg: 0.003952
  l1.weight: grad_norm = 0.295548
  l1.bias: grad_norm = 0.000197
  l2.weight: grad_norm = 0.218982
Total gradient norm: 0.608677
=== Actor Training Debug (Iteration 7108) ===
Q mean: -14.550838
Q std: 19.086361
Actor loss: 14.554806
Action reg: 0.003968
  l1.weight: grad_norm = 0.234719
  l1.bias: grad_norm = 0.001014
  l2.weight: grad_norm = 0.209736
Total gradient norm: 0.557523
=== Actor Training Debug (Iteration 7109) ===
Q mean: -12.031582
Q std: 15.939291
Actor loss: 12.035552
Action reg: 0.003970
  l1.weight: grad_norm = 0.268459
  l1.bias: grad_norm = 0.001080
  l2.weight: grad_norm = 0.216350
Total gradient norm: 0.650987
=== Actor Training Debug (Iteration 7110) ===
Q mean: -10.290372
Q std: 13.152894
Actor loss: 10.294324
Action reg: 0.003952
  l1.weight: grad_norm = 0.428485
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.329966
Total gradient norm: 0.933128
=== Actor Training Debug (Iteration 7111) ===
Q mean: -12.047701
Q std: 15.616184
Actor loss: 12.051659
Action reg: 0.003958
  l1.weight: grad_norm = 0.193775
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.168771
Total gradient norm: 0.478353
=== Actor Training Debug (Iteration 7112) ===
Q mean: -13.504711
Q std: 17.289797
Actor loss: 13.508656
Action reg: 0.003944
  l1.weight: grad_norm = 0.191740
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.165377
Total gradient norm: 0.509363
=== Actor Training Debug (Iteration 7113) ===
Q mean: -11.986243
Q std: 15.140932
Actor loss: 11.990191
Action reg: 0.003948
  l1.weight: grad_norm = 0.270192
  l1.bias: grad_norm = 0.000266
  l2.weight: grad_norm = 0.232173
Total gradient norm: 0.615644
=== Actor Training Debug (Iteration 7114) ===
Q mean: -11.791463
Q std: 15.084255
Actor loss: 11.795441
Action reg: 0.003978
  l1.weight: grad_norm = 0.463025
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.338480
Total gradient norm: 0.952007
=== Actor Training Debug (Iteration 7115) ===
Q mean: -14.958005
Q std: 18.130877
Actor loss: 14.961952
Action reg: 0.003948
  l1.weight: grad_norm = 0.139437
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.120638
Total gradient norm: 0.329067
=== Actor Training Debug (Iteration 7116) ===
Q mean: -13.484556
Q std: 18.178741
Actor loss: 13.488510
Action reg: 0.003954
  l1.weight: grad_norm = 0.257490
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.223079
Total gradient norm: 0.622720
=== Actor Training Debug (Iteration 7117) ===
Q mean: -11.366624
Q std: 14.136311
Actor loss: 11.370569
Action reg: 0.003946
  l1.weight: grad_norm = 0.223046
  l1.bias: grad_norm = 0.001241
  l2.weight: grad_norm = 0.195831
Total gradient norm: 0.553687
=== Actor Training Debug (Iteration 7118) ===
Q mean: -11.680413
Q std: 16.437080
Actor loss: 11.684366
Action reg: 0.003953
  l1.weight: grad_norm = 0.198546
  l1.bias: grad_norm = 0.000503
  l2.weight: grad_norm = 0.169412
Total gradient norm: 0.449752
=== Actor Training Debug (Iteration 7119) ===
Q mean: -11.081023
Q std: 14.296213
Actor loss: 11.084982
Action reg: 0.003959
  l1.weight: grad_norm = 0.188609
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.168103
Total gradient norm: 0.422922
=== Actor Training Debug (Iteration 7120) ===
Q mean: -13.203699
Q std: 16.188543
Actor loss: 13.207664
Action reg: 0.003964
  l1.weight: grad_norm = 0.196892
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.169415
Total gradient norm: 0.438944
=== Actor Training Debug (Iteration 7121) ===
Q mean: -11.612326
Q std: 14.954968
Actor loss: 11.616261
Action reg: 0.003934
  l1.weight: grad_norm = 0.127714
  l1.bias: grad_norm = 0.000492
  l2.weight: grad_norm = 0.115067
Total gradient norm: 0.303071
=== Actor Training Debug (Iteration 7122) ===
Q mean: -12.194325
Q std: 15.474545
Actor loss: 12.198295
Action reg: 0.003969
  l1.weight: grad_norm = 0.163213
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.141837
Total gradient norm: 0.335755
=== Actor Training Debug (Iteration 7123) ===
Q mean: -12.868794
Q std: 16.040545
Actor loss: 12.872748
Action reg: 0.003953
  l1.weight: grad_norm = 0.273262
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.218369
Total gradient norm: 0.615128
=== Actor Training Debug (Iteration 7124) ===
Q mean: -12.916973
Q std: 15.944711
Actor loss: 12.920924
Action reg: 0.003951
  l1.weight: grad_norm = 0.310011
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.262990
Total gradient norm: 0.815048
=== Actor Training Debug (Iteration 7125) ===
Q mean: -9.945019
Q std: 14.789678
Actor loss: 9.948955
Action reg: 0.003936
  l1.weight: grad_norm = 0.204382
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.163074
Total gradient norm: 0.452720
=== Actor Training Debug (Iteration 7126) ===
Q mean: -13.074923
Q std: 16.264809
Actor loss: 13.078853
Action reg: 0.003930
  l1.weight: grad_norm = 0.316822
  l1.bias: grad_norm = 0.000464
  l2.weight: grad_norm = 0.283874
Total gradient norm: 0.753247
=== Actor Training Debug (Iteration 7127) ===
Q mean: -12.412649
Q std: 16.268732
Actor loss: 12.416610
Action reg: 0.003960
  l1.weight: grad_norm = 0.204577
  l1.bias: grad_norm = 0.001370
  l2.weight: grad_norm = 0.186640
Total gradient norm: 0.488864
=== Actor Training Debug (Iteration 7128) ===
Q mean: -10.853449
Q std: 14.785587
Actor loss: 10.857412
Action reg: 0.003964
  l1.weight: grad_norm = 0.548573
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.455527
Total gradient norm: 1.239875
=== Actor Training Debug (Iteration 7129) ===
Q mean: -11.872538
Q std: 14.822997
Actor loss: 11.876485
Action reg: 0.003947
  l1.weight: grad_norm = 0.112451
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.096094
Total gradient norm: 0.262213
=== Actor Training Debug (Iteration 7130) ===
Q mean: -13.401452
Q std: 15.659562
Actor loss: 13.405404
Action reg: 0.003952
  l1.weight: grad_norm = 0.284447
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.227941
Total gradient norm: 0.595095
=== Actor Training Debug (Iteration 7131) ===
Q mean: -13.455442
Q std: 16.694324
Actor loss: 13.459408
Action reg: 0.003965
  l1.weight: grad_norm = 0.304384
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.239653
Total gradient norm: 0.681126
=== Actor Training Debug (Iteration 7132) ===
Q mean: -11.090569
Q std: 14.520582
Actor loss: 11.094524
Action reg: 0.003956
  l1.weight: grad_norm = 0.207120
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.168391
Total gradient norm: 0.455373
=== Actor Training Debug (Iteration 7133) ===
Q mean: -11.384910
Q std: 16.052692
Actor loss: 11.388842
Action reg: 0.003932
  l1.weight: grad_norm = 0.198518
  l1.bias: grad_norm = 0.000541
  l2.weight: grad_norm = 0.174458
Total gradient norm: 0.447310
=== Actor Training Debug (Iteration 7134) ===
Q mean: -11.859545
Q std: 15.643180
Actor loss: 11.863499
Action reg: 0.003954
  l1.weight: grad_norm = 0.223703
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.182343
Total gradient norm: 0.527197
=== Actor Training Debug (Iteration 7135) ===
Q mean: -13.106864
Q std: 17.307680
Actor loss: 13.110828
Action reg: 0.003964
  l1.weight: grad_norm = 0.342066
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.248638
Total gradient norm: 0.651144
=== Actor Training Debug (Iteration 7136) ===
Q mean: -13.735804
Q std: 17.348169
Actor loss: 13.739764
Action reg: 0.003961
  l1.weight: grad_norm = 0.232972
  l1.bias: grad_norm = 0.000186
  l2.weight: grad_norm = 0.184814
Total gradient norm: 0.534044
=== Actor Training Debug (Iteration 7137) ===
Q mean: -10.526527
Q std: 14.079488
Actor loss: 10.530503
Action reg: 0.003976
  l1.weight: grad_norm = 0.127139
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.110027
Total gradient norm: 0.265822
=== Actor Training Debug (Iteration 7138) ===
Q mean: -13.788902
Q std: 18.164858
Actor loss: 13.792846
Action reg: 0.003943
  l1.weight: grad_norm = 0.329518
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.296740
Total gradient norm: 0.801322
=== Actor Training Debug (Iteration 7139) ===
Q mean: -12.524178
Q std: 17.195621
Actor loss: 12.528127
Action reg: 0.003949
  l1.weight: grad_norm = 0.291257
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.241279
Total gradient norm: 0.651839
=== Actor Training Debug (Iteration 7140) ===
Q mean: -11.584660
Q std: 14.895859
Actor loss: 11.588637
Action reg: 0.003978
  l1.weight: grad_norm = 0.284361
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.230424
Total gradient norm: 0.620563
=== Actor Training Debug (Iteration 7141) ===
Q mean: -13.029982
Q std: 16.017368
Actor loss: 13.033955
Action reg: 0.003973
  l1.weight: grad_norm = 0.131819
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.123092
Total gradient norm: 0.301422
=== Actor Training Debug (Iteration 7142) ===
Q mean: -14.360065
Q std: 17.356371
Actor loss: 14.364018
Action reg: 0.003953
  l1.weight: grad_norm = 0.367514
  l1.bias: grad_norm = 0.001158
  l2.weight: grad_norm = 0.304925
Total gradient norm: 0.814731
=== Actor Training Debug (Iteration 7143) ===
Q mean: -12.678167
Q std: 15.279602
Actor loss: 12.682137
Action reg: 0.003969
  l1.weight: grad_norm = 0.242224
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.217296
Total gradient norm: 0.648263
=== Actor Training Debug (Iteration 7144) ===
Q mean: -11.496576
Q std: 16.132240
Actor loss: 11.500538
Action reg: 0.003962
  l1.weight: grad_norm = 0.236512
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.190724
Total gradient norm: 0.484162
=== Actor Training Debug (Iteration 7145) ===
Q mean: -11.934137
Q std: 15.662674
Actor loss: 11.938104
Action reg: 0.003966
  l1.weight: grad_norm = 0.117759
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.111583
Total gradient norm: 0.256503
=== Actor Training Debug (Iteration 7146) ===
Q mean: -11.391190
Q std: 15.264791
Actor loss: 11.395154
Action reg: 0.003964
  l1.weight: grad_norm = 0.219070
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.182702
Total gradient norm: 0.462906
=== Actor Training Debug (Iteration 7147) ===
Q mean: -14.205938
Q std: 17.931236
Actor loss: 14.209893
Action reg: 0.003955
  l1.weight: grad_norm = 0.220251
  l1.bias: grad_norm = 0.000344
  l2.weight: grad_norm = 0.177933
Total gradient norm: 0.489904
=== Actor Training Debug (Iteration 7148) ===
Q mean: -11.918897
Q std: 16.509684
Actor loss: 11.922846
Action reg: 0.003949
  l1.weight: grad_norm = 0.159055
  l1.bias: grad_norm = 0.000205
  l2.weight: grad_norm = 0.133071
Total gradient norm: 0.374231
=== Actor Training Debug (Iteration 7149) ===
Q mean: -11.523884
Q std: 15.776027
Actor loss: 11.527840
Action reg: 0.003956
  l1.weight: grad_norm = 0.192011
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.159195
Total gradient norm: 0.428852
=== Actor Training Debug (Iteration 7150) ===
Q mean: -12.159105
Q std: 16.362532
Actor loss: 12.163055
Action reg: 0.003950
  l1.weight: grad_norm = 0.309596
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.256776
Total gradient norm: 0.666131
=== Actor Training Debug (Iteration 7151) ===
Q mean: -11.654896
Q std: 16.052952
Actor loss: 11.658839
Action reg: 0.003944
  l1.weight: grad_norm = 0.219236
  l1.bias: grad_norm = 0.000255
  l2.weight: grad_norm = 0.183190
Total gradient norm: 0.543323
=== Actor Training Debug (Iteration 7152) ===
Q mean: -11.503118
Q std: 13.920065
Actor loss: 11.507054
Action reg: 0.003936
  l1.weight: grad_norm = 0.266578
  l1.bias: grad_norm = 0.000529
  l2.weight: grad_norm = 0.218315
Total gradient norm: 0.568330
=== Actor Training Debug (Iteration 7153) ===
Q mean: -12.178123
Q std: 15.979864
Actor loss: 12.182081
Action reg: 0.003958
  l1.weight: grad_norm = 0.267610
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.210534
Total gradient norm: 0.511309
=== Actor Training Debug (Iteration 7154) ===
Q mean: -13.055463
Q std: 17.385517
Actor loss: 13.059417
Action reg: 0.003953
  l1.weight: grad_norm = 0.209520
  l1.bias: grad_norm = 0.000540
  l2.weight: grad_norm = 0.185960
Total gradient norm: 0.478813
=== Actor Training Debug (Iteration 7155) ===
Q mean: -11.031487
Q std: 15.182463
Actor loss: 11.035437
Action reg: 0.003949
  l1.weight: grad_norm = 0.179601
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.178761
Total gradient norm: 0.534884
=== Actor Training Debug (Iteration 7156) ===
Q mean: -12.599741
Q std: 15.967288
Actor loss: 12.603692
Action reg: 0.003951
  l1.weight: grad_norm = 0.441517
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.314326
Total gradient norm: 0.779739
=== Actor Training Debug (Iteration 7157) ===
Q mean: -14.549481
Q std: 17.772175
Actor loss: 14.553435
Action reg: 0.003954
  l1.weight: grad_norm = 0.231516
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.201957
Total gradient norm: 0.525616
=== Actor Training Debug (Iteration 7158) ===
Q mean: -12.501989
Q std: 16.130169
Actor loss: 12.505951
Action reg: 0.003961
  l1.weight: grad_norm = 0.154330
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.132843
Total gradient norm: 0.325450
=== Actor Training Debug (Iteration 7159) ===
Q mean: -12.631840
Q std: 17.758780
Actor loss: 12.635789
Action reg: 0.003949
  l1.weight: grad_norm = 0.312891
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.274212
Total gradient norm: 0.742695
=== Actor Training Debug (Iteration 7160) ===
Q mean: -12.507427
Q std: 16.934031
Actor loss: 12.511391
Action reg: 0.003964
  l1.weight: grad_norm = 0.170032
  l1.bias: grad_norm = 0.000065
  l2.weight: grad_norm = 0.138017
Total gradient norm: 0.475234
=== Actor Training Debug (Iteration 7161) ===
Q mean: -12.729839
Q std: 16.552141
Actor loss: 12.733799
Action reg: 0.003960
  l1.weight: grad_norm = 0.207780
  l1.bias: grad_norm = 0.001018
  l2.weight: grad_norm = 0.184334
Total gradient norm: 0.493730
=== Actor Training Debug (Iteration 7162) ===
Q mean: -11.489645
Q std: 15.513053
Actor loss: 11.493609
Action reg: 0.003964
  l1.weight: grad_norm = 0.210905
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.159358
Total gradient norm: 0.396557
=== Actor Training Debug (Iteration 7163) ===
Q mean: -11.221550
Q std: 15.436982
Actor loss: 11.225493
Action reg: 0.003944
  l1.weight: grad_norm = 0.176003
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.142358
Total gradient norm: 0.397322
=== Actor Training Debug (Iteration 7164) ===
Q mean: -12.438843
Q std: 16.897013
Actor loss: 12.442795
Action reg: 0.003952
  l1.weight: grad_norm = 0.327553
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.284532
Total gradient norm: 0.789984
=== Actor Training Debug (Iteration 7165) ===
Q mean: -12.063179
Q std: 15.869843
Actor loss: 12.067144
Action reg: 0.003965
  l1.weight: grad_norm = 0.205395
  l1.bias: grad_norm = 0.001092
  l2.weight: grad_norm = 0.186641
Total gradient norm: 0.534062
=== Actor Training Debug (Iteration 7166) ===
Q mean: -14.231642
Q std: 17.238310
Actor loss: 14.235569
Action reg: 0.003927
  l1.weight: grad_norm = 0.320048
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.261469
Total gradient norm: 0.755998
=== Actor Training Debug (Iteration 7167) ===
Q mean: -13.112257
Q std: 16.962767
Actor loss: 13.116199
Action reg: 0.003942
  l1.weight: grad_norm = 0.278486
  l1.bias: grad_norm = 0.000397
  l2.weight: grad_norm = 0.231629
Total gradient norm: 0.609481
=== Actor Training Debug (Iteration 7168) ===
Q mean: -10.226433
Q std: 14.724337
Actor loss: 10.230384
Action reg: 0.003951
  l1.weight: grad_norm = 0.527625
  l1.bias: grad_norm = 0.000439
  l2.weight: grad_norm = 0.495007
Total gradient norm: 1.348264
=== Actor Training Debug (Iteration 7169) ===
Q mean: -11.517414
Q std: 15.580959
Actor loss: 11.521367
Action reg: 0.003953
  l1.weight: grad_norm = 0.238283
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.200167
Total gradient norm: 0.493543
=== Actor Training Debug (Iteration 7170) ===
Q mean: -12.198668
Q std: 15.293809
Actor loss: 12.202626
Action reg: 0.003959
  l1.weight: grad_norm = 0.394495
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.328162
Total gradient norm: 0.891232
=== Actor Training Debug (Iteration 7171) ===
Q mean: -12.987400
Q std: 17.361269
Actor loss: 12.991358
Action reg: 0.003958
  l1.weight: grad_norm = 0.294854
  l1.bias: grad_norm = 0.000158
  l2.weight: grad_norm = 0.230757
Total gradient norm: 0.560401
=== Actor Training Debug (Iteration 7172) ===
Q mean: -11.975100
Q std: 15.753562
Actor loss: 11.979032
Action reg: 0.003932
  l1.weight: grad_norm = 0.333259
  l1.bias: grad_norm = 0.000422
  l2.weight: grad_norm = 0.255856
Total gradient norm: 0.671586
=== Actor Training Debug (Iteration 7173) ===
Q mean: -12.502615
Q std: 15.981796
Actor loss: 12.506570
Action reg: 0.003954
  l1.weight: grad_norm = 0.277834
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.242644
Total gradient norm: 0.590995
=== Actor Training Debug (Iteration 7174) ===
Q mean: -11.772485
Q std: 16.362539
Actor loss: 11.776432
Action reg: 0.003947
  l1.weight: grad_norm = 0.336729
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.263197
Total gradient norm: 0.627243
=== Actor Training Debug (Iteration 7175) ===
Q mean: -11.367601
Q std: 16.138504
Actor loss: 11.371556
Action reg: 0.003955
  l1.weight: grad_norm = 0.151004
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.119868
Total gradient norm: 0.308452
=== Actor Training Debug (Iteration 7176) ===
Q mean: -11.970695
Q std: 15.048553
Actor loss: 11.974632
Action reg: 0.003937
  l1.weight: grad_norm = 0.267089
  l1.bias: grad_norm = 0.000817
  l2.weight: grad_norm = 0.216949
Total gradient norm: 0.611468
=== Actor Training Debug (Iteration 7177) ===
Q mean: -11.211957
Q std: 15.038898
Actor loss: 11.215900
Action reg: 0.003943
  l1.weight: grad_norm = 0.143407
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.131835
Total gradient norm: 0.400012
=== Actor Training Debug (Iteration 7178) ===
Q mean: -12.934275
Q std: 15.526708
Actor loss: 12.938240
Action reg: 0.003965
  l1.weight: grad_norm = 0.161658
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.141572
Total gradient norm: 0.378935
=== Actor Training Debug (Iteration 7179) ===
Q mean: -12.731191
Q std: 15.258443
Actor loss: 12.735168
Action reg: 0.003977
  l1.weight: grad_norm = 0.183638
  l1.bias: grad_norm = 0.000060
  l2.weight: grad_norm = 0.151146
Total gradient norm: 0.413661
=== Actor Training Debug (Iteration 7180) ===
Q mean: -12.689335
Q std: 16.126842
Actor loss: 12.693291
Action reg: 0.003956
  l1.weight: grad_norm = 0.340505
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.303976
Total gradient norm: 0.780358
=== Actor Training Debug (Iteration 7181) ===
Q mean: -14.839700
Q std: 18.635628
Actor loss: 14.843664
Action reg: 0.003964
  l1.weight: grad_norm = 0.140856
  l1.bias: grad_norm = 0.000137
  l2.weight: grad_norm = 0.139504
Total gradient norm: 0.380812
=== Actor Training Debug (Iteration 7182) ===
Q mean: -13.794110
Q std: 17.685240
Actor loss: 13.798073
Action reg: 0.003962
  l1.weight: grad_norm = 0.137605
  l1.bias: grad_norm = 0.000202
  l2.weight: grad_norm = 0.128769
Total gradient norm: 0.320320
=== Actor Training Debug (Iteration 7183) ===
Q mean: -11.872610
Q std: 14.839045
Actor loss: 11.876554
Action reg: 0.003944
  l1.weight: grad_norm = 0.274061
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.224983
Total gradient norm: 0.675963
=== Actor Training Debug (Iteration 7184) ===
Q mean: -11.546099
Q std: 15.444688
Actor loss: 11.550038
Action reg: 0.003939
  l1.weight: grad_norm = 0.273042
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.209652
Total gradient norm: 0.546898
=== Actor Training Debug (Iteration 7185) ===
Q mean: -13.145082
Q std: 19.162935
Actor loss: 13.149035
Action reg: 0.003953
  l1.weight: grad_norm = 0.364296
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.255012
Total gradient norm: 0.667594
=== Actor Training Debug (Iteration 7186) ===
Q mean: -11.042542
Q std: 14.918436
Actor loss: 11.046492
Action reg: 0.003950
  l1.weight: grad_norm = 0.424219
  l1.bias: grad_norm = 0.000967
  l2.weight: grad_norm = 0.334092
Total gradient norm: 0.800093
=== Actor Training Debug (Iteration 7187) ===
Q mean: -12.711446
Q std: 16.557497
Actor loss: 12.715387
Action reg: 0.003941
  l1.weight: grad_norm = 0.276738
  l1.bias: grad_norm = 0.000388
  l2.weight: grad_norm = 0.220761
Total gradient norm: 0.505845
=== Actor Training Debug (Iteration 7188) ===
Q mean: -11.216840
Q std: 14.700959
Actor loss: 11.220794
Action reg: 0.003954
  l1.weight: grad_norm = 0.168462
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.134865
Total gradient norm: 0.353811
=== Actor Training Debug (Iteration 7189) ===
Q mean: -10.441885
Q std: 15.041327
Actor loss: 10.445815
Action reg: 0.003930
  l1.weight: grad_norm = 0.280646
  l1.bias: grad_norm = 0.000840
  l2.weight: grad_norm = 0.213603
Total gradient norm: 0.561755
=== Actor Training Debug (Iteration 7190) ===
Q mean: -11.378691
Q std: 14.689424
Actor loss: 11.382655
Action reg: 0.003964
  l1.weight: grad_norm = 0.188858
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.154419
Total gradient norm: 0.374944
=== Actor Training Debug (Iteration 7191) ===
Q mean: -10.622921
Q std: 14.475400
Actor loss: 10.626883
Action reg: 0.003962
  l1.weight: grad_norm = 0.203732
  l1.bias: grad_norm = 0.000247
  l2.weight: grad_norm = 0.173048
Total gradient norm: 0.442231
=== Actor Training Debug (Iteration 7192) ===
Q mean: -12.414170
Q std: 17.163372
Actor loss: 12.418124
Action reg: 0.003954
  l1.weight: grad_norm = 0.304043
  l1.bias: grad_norm = 0.000210
  l2.weight: grad_norm = 0.237952
Total gradient norm: 0.632529
=== Actor Training Debug (Iteration 7193) ===
Q mean: -13.881477
Q std: 16.856709
Actor loss: 13.885428
Action reg: 0.003951
  l1.weight: grad_norm = 0.278558
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.208791
Total gradient norm: 0.557205
=== Actor Training Debug (Iteration 7194) ===
Q mean: -12.020603
Q std: 15.252900
Actor loss: 12.024573
Action reg: 0.003970
  l1.weight: grad_norm = 0.202326
  l1.bias: grad_norm = 0.000093
  l2.weight: grad_norm = 0.174486
Total gradient norm: 0.424451
=== Actor Training Debug (Iteration 7195) ===
Q mean: -12.852175
Q std: 16.998245
Actor loss: 12.856132
Action reg: 0.003957
  l1.weight: grad_norm = 0.276479
  l1.bias: grad_norm = 0.000246
  l2.weight: grad_norm = 0.223196
Total gradient norm: 0.568691
=== Actor Training Debug (Iteration 7196) ===
Q mean: -13.204552
Q std: 17.021622
Actor loss: 13.208491
Action reg: 0.003940
  l1.weight: grad_norm = 0.376115
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.313154
Total gradient norm: 0.728143
=== Actor Training Debug (Iteration 7197) ===
Q mean: -13.249668
Q std: 17.052143
Actor loss: 13.253625
Action reg: 0.003956
  l1.weight: grad_norm = 0.462287
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.352749
Total gradient norm: 1.022962
=== Actor Training Debug (Iteration 7198) ===
Q mean: -12.289373
Q std: 14.879350
Actor loss: 12.293329
Action reg: 0.003956
  l1.weight: grad_norm = 0.157142
  l1.bias: grad_norm = 0.000276
  l2.weight: grad_norm = 0.140935
Total gradient norm: 0.366925
=== Actor Training Debug (Iteration 7199) ===
Q mean: -12.829250
Q std: 17.414526
Actor loss: 12.833183
Action reg: 0.003933
  l1.weight: grad_norm = 0.340518
  l1.bias: grad_norm = 0.000837
  l2.weight: grad_norm = 0.278362
Total gradient norm: 0.676587
=== Actor Training Debug (Iteration 7200) ===
Q mean: -13.209612
Q std: 16.161255
Actor loss: 13.213579
Action reg: 0.003967
  l1.weight: grad_norm = 0.160667
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.138438
Total gradient norm: 0.384710
=== Actor Training Debug (Iteration 7201) ===
Q mean: -12.005604
Q std: 15.969096
Actor loss: 12.009562
Action reg: 0.003959
  l1.weight: grad_norm = 0.157454
  l1.bias: grad_norm = 0.002169
  l2.weight: grad_norm = 0.126732
Total gradient norm: 0.374412
=== Actor Training Debug (Iteration 7202) ===
Q mean: -12.096975
Q std: 14.994263
Actor loss: 12.100950
Action reg: 0.003975
  l1.weight: grad_norm = 0.146000
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.127791
Total gradient norm: 0.358080
=== Actor Training Debug (Iteration 7203) ===
Q mean: -12.505404
Q std: 15.084610
Actor loss: 12.509370
Action reg: 0.003966
  l1.weight: grad_norm = 0.202250
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.180387
Total gradient norm: 0.553239
=== Actor Training Debug (Iteration 7204) ===
Q mean: -12.971428
Q std: 17.271955
Actor loss: 12.975388
Action reg: 0.003960
  l1.weight: grad_norm = 0.174840
  l1.bias: grad_norm = 0.000290
  l2.weight: grad_norm = 0.146459
Total gradient norm: 0.389525
=== Actor Training Debug (Iteration 7205) ===
Q mean: -12.564274
Q std: 16.688227
Actor loss: 12.568245
Action reg: 0.003971
  l1.weight: grad_norm = 0.241446
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.199756
Total gradient norm: 0.509784
=== Actor Training Debug (Iteration 7206) ===
Q mean: -12.442904
Q std: 15.955680
Actor loss: 12.446853
Action reg: 0.003949
  l1.weight: grad_norm = 0.169267
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.160228
Total gradient norm: 0.471714
=== Actor Training Debug (Iteration 7207) ===
Q mean: -10.545576
Q std: 13.719715
Actor loss: 10.549526
Action reg: 0.003950
  l1.weight: grad_norm = 0.217544
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.189706
Total gradient norm: 0.494914
=== Actor Training Debug (Iteration 7208) ===
Q mean: -11.128081
Q std: 15.082388
Actor loss: 11.132026
Action reg: 0.003945
  l1.weight: grad_norm = 0.328259
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.266151
Total gradient norm: 0.706935
=== Actor Training Debug (Iteration 7209) ===
Q mean: -13.795889
Q std: 17.852570
Actor loss: 13.799848
Action reg: 0.003958
  l1.weight: grad_norm = 0.265017
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.224873
Total gradient norm: 0.547692
=== Actor Training Debug (Iteration 7210) ===
Q mean: -13.251529
Q std: 17.069456
Actor loss: 13.255489
Action reg: 0.003961
  l1.weight: grad_norm = 0.246564
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.194583
Total gradient norm: 0.523916
=== Actor Training Debug (Iteration 7211) ===
Q mean: -13.382957
Q std: 15.227574
Actor loss: 13.386925
Action reg: 0.003967
  l1.weight: grad_norm = 0.244537
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.212302
Total gradient norm: 0.580443
=== Actor Training Debug (Iteration 7212) ===
Q mean: -10.628261
Q std: 14.217167
Actor loss: 10.632220
Action reg: 0.003959
  l1.weight: grad_norm = 0.194278
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.156262
Total gradient norm: 0.404779
=== Actor Training Debug (Iteration 7213) ===
Q mean: -12.331426
Q std: 16.955191
Actor loss: 12.335391
Action reg: 0.003965
  l1.weight: grad_norm = 0.200969
  l1.bias: grad_norm = 0.000078
  l2.weight: grad_norm = 0.173183
Total gradient norm: 0.477149
=== Actor Training Debug (Iteration 7214) ===
Q mean: -11.051648
Q std: 14.645001
Actor loss: 11.055603
Action reg: 0.003955
  l1.weight: grad_norm = 0.170614
  l1.bias: grad_norm = 0.000101
  l2.weight: grad_norm = 0.131596
Total gradient norm: 0.352541
=== Actor Training Debug (Iteration 7215) ===
Q mean: -12.667736
Q std: 16.890631
Actor loss: 12.671686
Action reg: 0.003950
  l1.weight: grad_norm = 0.180251
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.155626
Total gradient norm: 0.424229
=== Actor Training Debug (Iteration 7216) ===
Q mean: -12.039686
Q std: 15.394003
Actor loss: 12.043641
Action reg: 0.003955
  l1.weight: grad_norm = 0.186482
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.143607
Total gradient norm: 0.360537
=== Actor Training Debug (Iteration 7217) ===
Q mean: -11.913787
Q std: 15.067004
Actor loss: 11.917750
Action reg: 0.003964
  l1.weight: grad_norm = 0.195622
  l1.bias: grad_norm = 0.000178
  l2.weight: grad_norm = 0.159464
Total gradient norm: 0.466463
=== Actor Training Debug (Iteration 7218) ===
Q mean: -14.187975
Q std: 16.540920
Actor loss: 14.191934
Action reg: 0.003959
  l1.weight: grad_norm = 0.216455
  l1.bias: grad_norm = 0.000943
  l2.weight: grad_norm = 0.166107
Total gradient norm: 0.415839
=== Actor Training Debug (Iteration 7219) ===
Q mean: -10.601880
Q std: 13.453909
Actor loss: 10.605841
Action reg: 0.003961
  l1.weight: grad_norm = 0.121985
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.110945
Total gradient norm: 0.311109
=== Actor Training Debug (Iteration 7220) ===
Q mean: -11.088398
Q std: 16.131998
Actor loss: 11.092323
Action reg: 0.003925
  l1.weight: grad_norm = 0.234614
  l1.bias: grad_norm = 0.000304
  l2.weight: grad_norm = 0.216292
Total gradient norm: 0.598912
=== Actor Training Debug (Iteration 7221) ===
Q mean: -12.907206
Q std: 16.938652
Actor loss: 12.911153
Action reg: 0.003947
  l1.weight: grad_norm = 0.344814
  l1.bias: grad_norm = 0.000312
  l2.weight: grad_norm = 0.309927
Total gradient norm: 0.845764
=== Actor Training Debug (Iteration 7222) ===
Q mean: -12.698394
Q std: 16.343792
Actor loss: 12.702332
Action reg: 0.003939
  l1.weight: grad_norm = 0.282031
  l1.bias: grad_norm = 0.000165
  l2.weight: grad_norm = 0.229736
Total gradient norm: 0.586214
=== Actor Training Debug (Iteration 7223) ===
Q mean: -11.694149
Q std: 15.348882
Actor loss: 11.698101
Action reg: 0.003952
  l1.weight: grad_norm = 0.275827
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.247777
Total gradient norm: 0.672377
=== Actor Training Debug (Iteration 7224) ===
Q mean: -11.975975
Q std: 16.602484
Actor loss: 11.979923
Action reg: 0.003948
  l1.weight: grad_norm = 0.417885
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.413883
Total gradient norm: 1.082527
=== Actor Training Debug (Iteration 7225) ===
Q mean: -11.646004
Q std: 14.313331
Actor loss: 11.649963
Action reg: 0.003959
  l1.weight: grad_norm = 0.255867
  l1.bias: grad_norm = 0.001146
  l2.weight: grad_norm = 0.205931
Total gradient norm: 0.584823
=== Actor Training Debug (Iteration 7226) ===
Q mean: -12.612226
Q std: 17.825279
Actor loss: 12.616183
Action reg: 0.003957
  l1.weight: grad_norm = 0.312043
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.263736
Total gradient norm: 0.682359
=== Actor Training Debug (Iteration 7227) ===
Q mean: -11.487890
Q std: 17.196308
Actor loss: 11.491855
Action reg: 0.003964
  l1.weight: grad_norm = 0.301112
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.232044
Total gradient norm: 0.622758
=== Actor Training Debug (Iteration 7228) ===
Q mean: -11.753092
Q std: 14.656919
Actor loss: 11.757054
Action reg: 0.003962
  l1.weight: grad_norm = 0.294194
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.258144
Total gradient norm: 0.673317
=== Actor Training Debug (Iteration 7229) ===
Q mean: -12.559304
Q std: 16.727467
Actor loss: 12.563254
Action reg: 0.003950
  l1.weight: grad_norm = 0.199049
  l1.bias: grad_norm = 0.000833
  l2.weight: grad_norm = 0.184973
Total gradient norm: 0.443471
=== Actor Training Debug (Iteration 7230) ===
Q mean: -10.831604
Q std: 13.549719
Actor loss: 10.835570
Action reg: 0.003967
  l1.weight: grad_norm = 0.245283
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.206001
Total gradient norm: 0.485614
=== Actor Training Debug (Iteration 7231) ===
Q mean: -12.919836
Q std: 15.883306
Actor loss: 12.923774
Action reg: 0.003937
  l1.weight: grad_norm = 0.334068
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.283390
Total gradient norm: 0.688760
=== Actor Training Debug (Iteration 7232) ===
Q mean: -12.713041
Q std: 16.152431
Actor loss: 12.716990
Action reg: 0.003949
  l1.weight: grad_norm = 0.263943
  l1.bias: grad_norm = 0.000147
  l2.weight: grad_norm = 0.217244
Total gradient norm: 0.593406
=== Actor Training Debug (Iteration 7233) ===
Q mean: -11.536285
Q std: 17.010714
Actor loss: 11.540230
Action reg: 0.003945
  l1.weight: grad_norm = 0.173158
  l1.bias: grad_norm = 0.001019
  l2.weight: grad_norm = 0.139528
Total gradient norm: 0.378455
=== Actor Training Debug (Iteration 7234) ===
Q mean: -11.929895
Q std: 15.373160
Actor loss: 11.933868
Action reg: 0.003973
  l1.weight: grad_norm = 0.097063
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.089665
Total gradient norm: 0.231514
=== Actor Training Debug (Iteration 7235) ===
Q mean: -11.917486
Q std: 14.756990
Actor loss: 11.921455
Action reg: 0.003969
  l1.weight: grad_norm = 0.154595
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.119593
Total gradient norm: 0.287339
=== Actor Training Debug (Iteration 7236) ===
Q mean: -12.881042
Q std: 16.687012
Actor loss: 12.885004
Action reg: 0.003961
  l1.weight: grad_norm = 0.186484
  l1.bias: grad_norm = 0.000089
  l2.weight: grad_norm = 0.143358
Total gradient norm: 0.383843
=== Actor Training Debug (Iteration 7237) ===
Q mean: -11.801314
Q std: 15.487797
Actor loss: 11.805264
Action reg: 0.003950
  l1.weight: grad_norm = 0.312003
  l1.bias: grad_norm = 0.000697
  l2.weight: grad_norm = 0.251553
Total gradient norm: 0.758570
=== Actor Training Debug (Iteration 7238) ===
Q mean: -14.431013
Q std: 17.200594
Actor loss: 14.434965
Action reg: 0.003952
  l1.weight: grad_norm = 0.183257
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.163728
Total gradient norm: 0.462513
=== Actor Training Debug (Iteration 7239) ===
Q mean: -12.427568
Q std: 14.689876
Actor loss: 12.431542
Action reg: 0.003974
  l1.weight: grad_norm = 0.336355
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.274924
Total gradient norm: 0.781618
=== Actor Training Debug (Iteration 7240) ===
Q mean: -12.833765
Q std: 16.566404
Actor loss: 12.837728
Action reg: 0.003962
  l1.weight: grad_norm = 0.239004
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.217811
Total gradient norm: 0.555288
=== Actor Training Debug (Iteration 7241) ===
Q mean: -10.454347
Q std: 14.726774
Actor loss: 10.458306
Action reg: 0.003960
  l1.weight: grad_norm = 0.126340
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.109341
Total gradient norm: 0.282200
=== Actor Training Debug (Iteration 7242) ===
Q mean: -13.325060
Q std: 18.279945
Actor loss: 13.329033
Action reg: 0.003973
  l1.weight: grad_norm = 0.212120
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.196103
Total gradient norm: 0.636829
=== Actor Training Debug (Iteration 7243) ===
Q mean: -11.968179
Q std: 16.598951
Actor loss: 11.972134
Action reg: 0.003955
  l1.weight: grad_norm = 0.233654
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.161752
Total gradient norm: 0.412983
=== Actor Training Debug (Iteration 7244) ===
Q mean: -13.270275
Q std: 17.981562
Actor loss: 13.274224
Action reg: 0.003949
  l1.weight: grad_norm = 0.168919
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.137559
Total gradient norm: 0.358875
=== Actor Training Debug (Iteration 7245) ===
Q mean: -10.260447
Q std: 13.805737
Actor loss: 10.264384
Action reg: 0.003938
  l1.weight: grad_norm = 0.141221
  l1.bias: grad_norm = 0.000518
  l2.weight: grad_norm = 0.132926
Total gradient norm: 0.307389
=== Actor Training Debug (Iteration 7246) ===
Q mean: -13.789055
Q std: 18.458662
Actor loss: 13.792987
Action reg: 0.003932
  l1.weight: grad_norm = 0.252841
  l1.bias: grad_norm = 0.000634
  l2.weight: grad_norm = 0.230988
Total gradient norm: 0.698661
=== Actor Training Debug (Iteration 7247) ===
Q mean: -13.297277
Q std: 15.965242
Actor loss: 13.301232
Action reg: 0.003955
  l1.weight: grad_norm = 0.360765
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.338868
Total gradient norm: 0.836545
=== Actor Training Debug (Iteration 7248) ===
Q mean: -11.249910
Q std: 14.654176
Actor loss: 11.253845
Action reg: 0.003935
  l1.weight: grad_norm = 0.233794
  l1.bias: grad_norm = 0.000292
  l2.weight: grad_norm = 0.195272
Total gradient norm: 0.435104
=== Actor Training Debug (Iteration 7249) ===
Q mean: -12.651022
Q std: 16.884775
Actor loss: 12.654978
Action reg: 0.003956
  l1.weight: grad_norm = 0.179554
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.151453
Total gradient norm: 0.425993
=== Actor Training Debug (Iteration 7250) ===
Q mean: -13.039190
Q std: 17.567877
Actor loss: 13.043157
Action reg: 0.003966
  l1.weight: grad_norm = 0.259110
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.236438
Total gradient norm: 0.631133
=== Actor Training Debug (Iteration 7251) ===
Q mean: -11.837911
Q std: 14.991673
Actor loss: 11.841859
Action reg: 0.003948
  l1.weight: grad_norm = 0.123350
  l1.bias: grad_norm = 0.002132
  l2.weight: grad_norm = 0.107227
Total gradient norm: 0.306333
=== Actor Training Debug (Iteration 7252) ===
Q mean: -11.842966
Q std: 14.593090
Actor loss: 11.846930
Action reg: 0.003963
  l1.weight: grad_norm = 0.333619
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.288642
Total gradient norm: 0.750930
=== Actor Training Debug (Iteration 7253) ===
Q mean: -11.958008
Q std: 14.985398
Actor loss: 11.961972
Action reg: 0.003964
  l1.weight: grad_norm = 0.244991
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.214850
Total gradient norm: 0.570208
=== Actor Training Debug (Iteration 7254) ===
Q mean: -13.708791
Q std: 17.237379
Actor loss: 13.712725
Action reg: 0.003934
  l1.weight: grad_norm = 0.381766
  l1.bias: grad_norm = 0.001022
  l2.weight: grad_norm = 0.325401
Total gradient norm: 1.059645
=== Actor Training Debug (Iteration 7255) ===
Q mean: -12.728048
Q std: 16.196945
Actor loss: 12.731997
Action reg: 0.003948
  l1.weight: grad_norm = 0.388718
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.295579
Total gradient norm: 0.771109
=== Actor Training Debug (Iteration 7256) ===
Q mean: -10.256025
Q std: 14.861727
Actor loss: 10.259976
Action reg: 0.003951
  l1.weight: grad_norm = 0.204766
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.183900
Total gradient norm: 0.493072
=== Actor Training Debug (Iteration 7257) ===
Q mean: -12.161381
Q std: 15.981815
Actor loss: 12.165341
Action reg: 0.003961
  l1.weight: grad_norm = 1.042411
  l1.bias: grad_norm = 0.000842
  l2.weight: grad_norm = 0.795265
Total gradient norm: 2.133931
=== Actor Training Debug (Iteration 7258) ===
Q mean: -12.750992
Q std: 17.893862
Actor loss: 12.754954
Action reg: 0.003962
  l1.weight: grad_norm = 0.263453
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.207097
Total gradient norm: 0.512918
=== Actor Training Debug (Iteration 7259) ===
Q mean: -12.043119
Q std: 17.038431
Actor loss: 12.047076
Action reg: 0.003957
  l1.weight: grad_norm = 0.650430
  l1.bias: grad_norm = 0.000363
  l2.weight: grad_norm = 0.552221
Total gradient norm: 1.612120
=== Actor Training Debug (Iteration 7260) ===
Q mean: -12.772139
Q std: 16.628736
Actor loss: 12.776101
Action reg: 0.003962
  l1.weight: grad_norm = 0.419596
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.382031
Total gradient norm: 0.916405
=== Actor Training Debug (Iteration 7261) ===
Q mean: -12.149284
Q std: 16.653568
Actor loss: 12.153238
Action reg: 0.003954
  l1.weight: grad_norm = 0.282101
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.246528
Total gradient norm: 0.582847
=== Actor Training Debug (Iteration 7262) ===
Q mean: -13.457335
Q std: 16.680960
Actor loss: 13.461287
Action reg: 0.003953
  l1.weight: grad_norm = 0.207793
  l1.bias: grad_norm = 0.000206
  l2.weight: grad_norm = 0.201872
Total gradient norm: 0.552127
=== Actor Training Debug (Iteration 7263) ===
Q mean: -12.436614
Q std: 15.213031
Actor loss: 12.440570
Action reg: 0.003956
  l1.weight: grad_norm = 0.123379
  l1.bias: grad_norm = 0.001090
  l2.weight: grad_norm = 0.113033
Total gradient norm: 0.294285
=== Actor Training Debug (Iteration 7264) ===
Q mean: -10.656034
Q std: 14.186965
Actor loss: 10.659980
Action reg: 0.003945
  l1.weight: grad_norm = 0.177270
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.167105
Total gradient norm: 0.424765
=== Actor Training Debug (Iteration 7265) ===
Q mean: -12.482525
Q std: 16.894165
Actor loss: 12.486443
Action reg: 0.003918
  l1.weight: grad_norm = 0.306613
  l1.bias: grad_norm = 0.000900
  l2.weight: grad_norm = 0.239859
Total gradient norm: 0.668376
=== Actor Training Debug (Iteration 7266) ===
Q mean: -11.224234
Q std: 14.048519
Actor loss: 11.228176
Action reg: 0.003942
  l1.weight: grad_norm = 0.152595
  l1.bias: grad_norm = 0.000969
  l2.weight: grad_norm = 0.127982
Total gradient norm: 0.355613
=== Actor Training Debug (Iteration 7267) ===
Q mean: -11.913736
Q std: 15.447993
Actor loss: 11.917698
Action reg: 0.003962
  l1.weight: grad_norm = 0.199238
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.171796
Total gradient norm: 0.463816
=== Actor Training Debug (Iteration 7268) ===
Q mean: -11.913704
Q std: 16.271023
Actor loss: 11.917664
Action reg: 0.003960
  l1.weight: grad_norm = 0.238614
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.198358
Total gradient norm: 0.539239
=== Actor Training Debug (Iteration 7269) ===
Q mean: -11.698698
Q std: 14.110408
Actor loss: 11.702659
Action reg: 0.003960
  l1.weight: grad_norm = 0.124360
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.110206
Total gradient norm: 0.277854
=== Actor Training Debug (Iteration 7270) ===
Q mean: -13.547491
Q std: 18.058992
Actor loss: 13.551455
Action reg: 0.003964
  l1.weight: grad_norm = 0.136393
  l1.bias: grad_norm = 0.000938
  l2.weight: grad_norm = 0.117438
Total gradient norm: 0.343791
=== Actor Training Debug (Iteration 7271) ===
Q mean: -13.276735
Q std: 16.381174
Actor loss: 13.280697
Action reg: 0.003961
  l1.weight: grad_norm = 0.239269
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.216285
Total gradient norm: 0.612582
=== Actor Training Debug (Iteration 7272) ===
Q mean: -10.738829
Q std: 15.128754
Actor loss: 10.742794
Action reg: 0.003966
  l1.weight: grad_norm = 0.180271
  l1.bias: grad_norm = 0.000395
  l2.weight: grad_norm = 0.181977
Total gradient norm: 0.544902
=== Actor Training Debug (Iteration 7273) ===
Q mean: -12.135510
Q std: 16.450872
Actor loss: 12.139469
Action reg: 0.003959
  l1.weight: grad_norm = 0.234114
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.202742
Total gradient norm: 0.542556
=== Actor Training Debug (Iteration 7274) ===
Q mean: -12.302693
Q std: 15.364531
Actor loss: 12.306645
Action reg: 0.003952
  l1.weight: grad_norm = 0.327374
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.268781
Total gradient norm: 0.804536
=== Actor Training Debug (Iteration 7275) ===
Q mean: -13.331190
Q std: 15.213576
Actor loss: 13.335155
Action reg: 0.003965
  l1.weight: grad_norm = 0.079167
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.071832
Total gradient norm: 0.199159
=== Actor Training Debug (Iteration 7276) ===
Q mean: -11.473458
Q std: 13.908232
Actor loss: 11.477406
Action reg: 0.003948
  l1.weight: grad_norm = 0.203874
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.178457
Total gradient norm: 0.491097
=== Actor Training Debug (Iteration 7277) ===
Q mean: -10.962694
Q std: 13.976107
Actor loss: 10.966641
Action reg: 0.003948
  l1.weight: grad_norm = 0.189389
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.165725
Total gradient norm: 0.472881
=== Actor Training Debug (Iteration 7278) ===
Q mean: -13.577612
Q std: 16.536959
Actor loss: 13.581565
Action reg: 0.003953
  l1.weight: grad_norm = 0.224762
  l1.bias: grad_norm = 0.000867
  l2.weight: grad_norm = 0.174466
Total gradient norm: 0.411741
=== Actor Training Debug (Iteration 7279) ===
Q mean: -12.927535
Q std: 16.285301
Actor loss: 12.931490
Action reg: 0.003955
  l1.weight: grad_norm = 0.113744
  l1.bias: grad_norm = 0.000143
  l2.weight: grad_norm = 0.099848
Total gradient norm: 0.269442
=== Actor Training Debug (Iteration 7280) ===
Q mean: -11.419425
Q std: 15.089953
Actor loss: 11.423366
Action reg: 0.003941
  l1.weight: grad_norm = 0.235617
  l1.bias: grad_norm = 0.000291
  l2.weight: grad_norm = 0.200504
Total gradient norm: 0.501502
=== Actor Training Debug (Iteration 7281) ===
Q mean: -12.653039
Q std: 15.409234
Actor loss: 12.657004
Action reg: 0.003966
  l1.weight: grad_norm = 0.215374
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.172712
Total gradient norm: 0.454549
=== Actor Training Debug (Iteration 7282) ===
Q mean: -13.116102
Q std: 16.966711
Actor loss: 13.120070
Action reg: 0.003968
  l1.weight: grad_norm = 0.256896
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.201872
Total gradient norm: 0.548849
=== Actor Training Debug (Iteration 7283) ===
Q mean: -12.196943
Q std: 14.736085
Actor loss: 12.200899
Action reg: 0.003956
  l1.weight: grad_norm = 0.242042
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.177090
Total gradient norm: 0.475282
=== Actor Training Debug (Iteration 7284) ===
Q mean: -12.353146
Q std: 16.083567
Actor loss: 12.357084
Action reg: 0.003939
  l1.weight: grad_norm = 0.402816
  l1.bias: grad_norm = 0.000931
  l2.weight: grad_norm = 0.323309
Total gradient norm: 0.791202
=== Actor Training Debug (Iteration 7285) ===
Q mean: -12.877028
Q std: 16.814838
Actor loss: 12.880980
Action reg: 0.003952
  l1.weight: grad_norm = 0.272627
  l1.bias: grad_norm = 0.000360
  l2.weight: grad_norm = 0.220598
Total gradient norm: 0.588399
=== Actor Training Debug (Iteration 7286) ===
Q mean: -13.206917
Q std: 16.643944
Actor loss: 13.210879
Action reg: 0.003962
  l1.weight: grad_norm = 0.187968
  l1.bias: grad_norm = 0.000105
  l2.weight: grad_norm = 0.163145
Total gradient norm: 0.422525
=== Actor Training Debug (Iteration 7287) ===
Q mean: -11.571798
Q std: 14.756405
Actor loss: 11.575743
Action reg: 0.003945
  l1.weight: grad_norm = 0.478951
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.380348
Total gradient norm: 1.091226
=== Actor Training Debug (Iteration 7288) ===
Q mean: -12.694744
Q std: 16.340687
Actor loss: 12.698693
Action reg: 0.003949
  l1.weight: grad_norm = 0.207851
  l1.bias: grad_norm = 0.000159
  l2.weight: grad_norm = 0.182097
Total gradient norm: 0.394965
=== Actor Training Debug (Iteration 7289) ===
Q mean: -12.660286
Q std: 15.087919
Actor loss: 12.664229
Action reg: 0.003944
  l1.weight: grad_norm = 0.251822
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.218909
Total gradient norm: 0.623769
=== Actor Training Debug (Iteration 7290) ===
Q mean: -12.644534
Q std: 16.727537
Actor loss: 12.648478
Action reg: 0.003943
  l1.weight: grad_norm = 0.291530
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.230908
Total gradient norm: 0.586836
=== Actor Training Debug (Iteration 7291) ===
Q mean: -12.822374
Q std: 16.201168
Actor loss: 12.826326
Action reg: 0.003952
  l1.weight: grad_norm = 0.202823
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.186838
Total gradient norm: 0.523756
=== Actor Training Debug (Iteration 7292) ===
Q mean: -10.806952
Q std: 12.627561
Actor loss: 10.810907
Action reg: 0.003956
  l1.weight: grad_norm = 0.292922
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.242715
Total gradient norm: 0.588854
=== Actor Training Debug (Iteration 7293) ===
Q mean: -14.636880
Q std: 17.731070
Actor loss: 14.640841
Action reg: 0.003961
  l1.weight: grad_norm = 0.206752
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.200052
Total gradient norm: 0.662289
=== Actor Training Debug (Iteration 7294) ===
Q mean: -13.753899
Q std: 18.870171
Actor loss: 13.757859
Action reg: 0.003961
  l1.weight: grad_norm = 0.207383
  l1.bias: grad_norm = 0.000211
  l2.weight: grad_norm = 0.169754
Total gradient norm: 0.491136
=== Actor Training Debug (Iteration 7295) ===
Q mean: -11.135609
Q std: 15.188375
Actor loss: 11.139570
Action reg: 0.003962
  l1.weight: grad_norm = 0.234670
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.178373
Total gradient norm: 0.457156
=== Actor Training Debug (Iteration 7296) ===
Q mean: -12.646077
Q std: 15.319734
Actor loss: 12.650031
Action reg: 0.003954
  l1.weight: grad_norm = 0.171976
  l1.bias: grad_norm = 0.000484
  l2.weight: grad_norm = 0.157290
Total gradient norm: 0.396851
=== Actor Training Debug (Iteration 7297) ===
Q mean: -12.045377
Q std: 15.475796
Actor loss: 12.049337
Action reg: 0.003961
  l1.weight: grad_norm = 0.273160
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.230733
Total gradient norm: 0.623825
=== Actor Training Debug (Iteration 7298) ===
Q mean: -11.975729
Q std: 14.271824
Actor loss: 11.979692
Action reg: 0.003964
  l1.weight: grad_norm = 0.236714
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.215614
Total gradient norm: 0.534342
=== Actor Training Debug (Iteration 7299) ===
Q mean: -11.976552
Q std: 14.099670
Actor loss: 11.980527
Action reg: 0.003975
  l1.weight: grad_norm = 0.209225
  l1.bias: grad_norm = 0.000106
  l2.weight: grad_norm = 0.183498
Total gradient norm: 0.465535
=== Actor Training Debug (Iteration 7300) ===
Q mean: -12.193213
Q std: 13.965192
Actor loss: 12.197169
Action reg: 0.003957
  l1.weight: grad_norm = 0.247950
  l1.bias: grad_norm = 0.000208
  l2.weight: grad_norm = 0.203194
Total gradient norm: 0.587864
=== Actor Training Debug (Iteration 7301) ===
Q mean: -14.551676
Q std: 17.748508
Actor loss: 14.555636
Action reg: 0.003961
  l1.weight: grad_norm = 0.256555
  l1.bias: grad_norm = 0.001224
  l2.weight: grad_norm = 0.214784
Total gradient norm: 0.606207
=== Actor Training Debug (Iteration 7302) ===
Q mean: -13.828768
Q std: 17.692881
Actor loss: 13.832736
Action reg: 0.003968
  l1.weight: grad_norm = 0.184539
  l1.bias: grad_norm = 0.000230
  l2.weight: grad_norm = 0.162324
Total gradient norm: 0.449094
=== Actor Training Debug (Iteration 7303) ===
Q mean: -12.204456
Q std: 15.032162
Actor loss: 12.208410
Action reg: 0.003954
  l1.weight: grad_norm = 0.266794
  l1.bias: grad_norm = 0.000627
  l2.weight: grad_norm = 0.252702
Total gradient norm: 0.707627
=== Actor Training Debug (Iteration 7304) ===
Q mean: -13.308399
Q std: 17.029947
Actor loss: 13.312353
Action reg: 0.003954
  l1.weight: grad_norm = 0.218926
  l1.bias: grad_norm = 0.000154
  l2.weight: grad_norm = 0.191734
Total gradient norm: 0.525542
=== Actor Training Debug (Iteration 7305) ===
Q mean: -11.433796
Q std: 14.866344
Actor loss: 11.437747
Action reg: 0.003951
  l1.weight: grad_norm = 0.320691
  l1.bias: grad_norm = 0.000560
  l2.weight: grad_norm = 0.255503
Total gradient norm: 0.664324
=== Actor Training Debug (Iteration 7306) ===
Q mean: -11.634433
Q std: 15.943724
Actor loss: 11.638380
Action reg: 0.003947
  l1.weight: grad_norm = 0.222964
  l1.bias: grad_norm = 0.001166
  l2.weight: grad_norm = 0.193850
Total gradient norm: 0.512753
=== Actor Training Debug (Iteration 7307) ===
Q mean: -11.724989
Q std: 14.329754
Actor loss: 11.728924
Action reg: 0.003935
  l1.weight: grad_norm = 1.142768
  l1.bias: grad_norm = 0.000537
  l2.weight: grad_norm = 1.023762
Total gradient norm: 3.389138
=== Actor Training Debug (Iteration 7308) ===
Q mean: -13.521461
Q std: 17.425627
Actor loss: 13.525411
Action reg: 0.003949
  l1.weight: grad_norm = 0.177299
  l1.bias: grad_norm = 0.000373
  l2.weight: grad_norm = 0.156273
Total gradient norm: 0.385537
=== Actor Training Debug (Iteration 7309) ===
Q mean: -12.932175
Q std: 16.633015
Actor loss: 12.936126
Action reg: 0.003951
  l1.weight: grad_norm = 0.268106
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.222620
Total gradient norm: 0.563473
=== Actor Training Debug (Iteration 7310) ===
Q mean: -10.882654
Q std: 14.175508
Actor loss: 10.886619
Action reg: 0.003964
  l1.weight: grad_norm = 0.306572
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.268778
Total gradient norm: 0.742214
=== Actor Training Debug (Iteration 7311) ===
Q mean: -12.544406
Q std: 16.798822
Actor loss: 12.548360
Action reg: 0.003954
  l1.weight: grad_norm = 0.197477
  l1.bias: grad_norm = 0.000516
  l2.weight: grad_norm = 0.155324
Total gradient norm: 0.370863
=== Actor Training Debug (Iteration 7312) ===
Q mean: -11.324068
Q std: 14.481320
Actor loss: 11.328012
Action reg: 0.003945
  l1.weight: grad_norm = 0.273523
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.237155
Total gradient norm: 0.689186
=== Actor Training Debug (Iteration 7313) ===
Q mean: -14.771950
Q std: 15.632210
Actor loss: 14.775895
Action reg: 0.003946
  l1.weight: grad_norm = 0.124308
  l1.bias: grad_norm = 0.000254
  l2.weight: grad_norm = 0.108910
Total gradient norm: 0.282972
=== Actor Training Debug (Iteration 7314) ===
Q mean: -12.748415
Q std: 15.764009
Actor loss: 12.752378
Action reg: 0.003962
  l1.weight: grad_norm = 0.159075
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.134526
Total gradient norm: 0.413686
=== Actor Training Debug (Iteration 7315) ===
Q mean: -13.208406
Q std: 16.912930
Actor loss: 13.212366
Action reg: 0.003959
  l1.weight: grad_norm = 0.262277
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.231999
Total gradient norm: 0.569252
=== Actor Training Debug (Iteration 7316) ===
Q mean: -11.025309
Q std: 14.772168
Actor loss: 11.029266
Action reg: 0.003958
  l1.weight: grad_norm = 0.356129
  l1.bias: grad_norm = 0.000152
  l2.weight: grad_norm = 0.265918
Total gradient norm: 0.740165
=== Actor Training Debug (Iteration 7317) ===
Q mean: -13.484180
Q std: 17.528801
Actor loss: 13.488163
Action reg: 0.003982
  l1.weight: grad_norm = 0.150327
  l1.bias: grad_norm = 0.000073
  l2.weight: grad_norm = 0.129573
Total gradient norm: 0.349790
=== Actor Training Debug (Iteration 7318) ===
Q mean: -10.677122
Q std: 14.755024
Actor loss: 10.681067
Action reg: 0.003944
  l1.weight: grad_norm = 0.235673
  l1.bias: grad_norm = 0.001147
  l2.weight: grad_norm = 0.192440
Total gradient norm: 0.488255
=== Actor Training Debug (Iteration 7319) ===
Q mean: -13.000200
Q std: 15.105739
Actor loss: 13.004156
Action reg: 0.003956
  l1.weight: grad_norm = 0.276397
  l1.bias: grad_norm = 0.000104
  l2.weight: grad_norm = 0.239170
Total gradient norm: 0.613542
=== Actor Training Debug (Iteration 7320) ===
Q mean: -10.060932
Q std: 13.115451
Actor loss: 10.064884
Action reg: 0.003952
  l1.weight: grad_norm = 0.372371
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.317873
Total gradient norm: 0.887921
=== Actor Training Debug (Iteration 7321) ===
Q mean: -11.585478
Q std: 15.649920
Actor loss: 11.589437
Action reg: 0.003960
  l1.weight: grad_norm = 0.370088
  l1.bias: grad_norm = 0.002466
  l2.weight: grad_norm = 0.292405
Total gradient norm: 0.749608
=== Actor Training Debug (Iteration 7322) ===
Q mean: -10.346388
Q std: 14.949215
Actor loss: 10.350352
Action reg: 0.003964
  l1.weight: grad_norm = 0.203555
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.164611
Total gradient norm: 0.439545
=== Actor Training Debug (Iteration 7323) ===
Q mean: -11.709370
Q std: 15.139843
Actor loss: 11.713339
Action reg: 0.003969
  l1.weight: grad_norm = 0.340191
  l1.bias: grad_norm = 0.001219
  l2.weight: grad_norm = 0.282297
Total gradient norm: 0.696884
=== Actor Training Debug (Iteration 7324) ===
Q mean: -12.649807
Q std: 15.068639
Actor loss: 12.653756
Action reg: 0.003949
  l1.weight: grad_norm = 0.274172
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.241204
Total gradient norm: 0.615574
=== Actor Training Debug (Iteration 7325) ===
Q mean: -11.206997
Q std: 13.965626
Actor loss: 11.210956
Action reg: 0.003959
  l1.weight: grad_norm = 0.315122
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.259926
Total gradient norm: 0.674684
=== Actor Training Debug (Iteration 7326) ===
Q mean: -11.639490
Q std: 15.587115
Actor loss: 11.643466
Action reg: 0.003976
  l1.weight: grad_norm = 0.130082
  l1.bias: grad_norm = 0.001005
  l2.weight: grad_norm = 0.116958
Total gradient norm: 0.351954
=== Actor Training Debug (Iteration 7327) ===
Q mean: -12.574322
Q std: 17.423988
Actor loss: 12.578273
Action reg: 0.003952
  l1.weight: grad_norm = 0.168817
  l1.bias: grad_norm = 0.000171
  l2.weight: grad_norm = 0.146776
Total gradient norm: 0.369521
=== Actor Training Debug (Iteration 7328) ===
Q mean: -12.695724
Q std: 15.859821
Actor loss: 12.699677
Action reg: 0.003952
  l1.weight: grad_norm = 0.227004
  l1.bias: grad_norm = 0.000467
  l2.weight: grad_norm = 0.198769
Total gradient norm: 0.646218
=== Actor Training Debug (Iteration 7329) ===
Q mean: -12.219679
Q std: 15.981993
Actor loss: 12.223619
Action reg: 0.003940
  l1.weight: grad_norm = 0.414293
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.337122
Total gradient norm: 0.789342
=== Actor Training Debug (Iteration 7330) ===
Q mean: -11.198914
Q std: 14.588010
Actor loss: 11.202861
Action reg: 0.003947
  l1.weight: grad_norm = 0.476880
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.385229
Total gradient norm: 1.123809
=== Actor Training Debug (Iteration 7331) ===
Q mean: -11.573658
Q std: 17.253683
Actor loss: 11.577607
Action reg: 0.003949
  l1.weight: grad_norm = 0.332711
  l1.bias: grad_norm = 0.000849
  l2.weight: grad_norm = 0.270530
Total gradient norm: 0.703757
=== Actor Training Debug (Iteration 7332) ===
Q mean: -12.031871
Q std: 15.121266
Actor loss: 12.035829
Action reg: 0.003958
  l1.weight: grad_norm = 0.273548
  l1.bias: grad_norm = 0.000151
  l2.weight: grad_norm = 0.217824
Total gradient norm: 0.625762
=== Actor Training Debug (Iteration 7333) ===
Q mean: -12.182728
Q std: 14.819084
Actor loss: 12.186678
Action reg: 0.003950
  l1.weight: grad_norm = 0.193296
  l1.bias: grad_norm = 0.000109
  l2.weight: grad_norm = 0.165827
Total gradient norm: 0.441788
=== Actor Training Debug (Iteration 7334) ===
Q mean: -12.682283
Q std: 14.663372
Actor loss: 12.686261
Action reg: 0.003978
  l1.weight: grad_norm = 0.276030
  l1.bias: grad_norm = 0.000083
  l2.weight: grad_norm = 0.212650
Total gradient norm: 0.501054
=== Actor Training Debug (Iteration 7335) ===
Q mean: -12.329865
Q std: 15.562106
Actor loss: 12.333815
Action reg: 0.003949
  l1.weight: grad_norm = 0.546803
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.429934
Total gradient norm: 1.030904
=== Actor Training Debug (Iteration 7336) ===
Q mean: -12.874819
Q std: 16.210474
Actor loss: 12.878777
Action reg: 0.003957
  l1.weight: grad_norm = 0.275112
  l1.bias: grad_norm = 0.000582
  l2.weight: grad_norm = 0.246056
Total gradient norm: 0.579331
=== Actor Training Debug (Iteration 7337) ===
Q mean: -13.177073
Q std: 17.405500
Actor loss: 13.181020
Action reg: 0.003947
  l1.weight: grad_norm = 0.197564
  l1.bias: grad_norm = 0.000510
  l2.weight: grad_norm = 0.170205
Total gradient norm: 0.463863
=== Actor Training Debug (Iteration 7338) ===
Q mean: -13.413891
Q std: 17.275381
Actor loss: 13.417859
Action reg: 0.003968
  l1.weight: grad_norm = 0.135586
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.113251
Total gradient norm: 0.295416
=== Actor Training Debug (Iteration 7339) ===
Q mean: -11.690762
Q std: 13.600353
Actor loss: 11.694709
Action reg: 0.003948
  l1.weight: grad_norm = 0.344992
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.306807
Total gradient norm: 0.735321
=== Actor Training Debug (Iteration 7340) ===
Q mean: -13.722213
Q std: 19.177633
Actor loss: 13.726145
Action reg: 0.003932
  l1.weight: grad_norm = 0.266694
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.211320
Total gradient norm: 0.558622
=== Actor Training Debug (Iteration 7341) ===
Q mean: -13.400314
Q std: 17.576387
Actor loss: 13.404288
Action reg: 0.003974
  l1.weight: grad_norm = 0.309310
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.296453
Total gradient norm: 0.896456
=== Actor Training Debug (Iteration 7342) ===
Q mean: -12.937502
Q std: 16.533167
Actor loss: 12.941461
Action reg: 0.003959
  l1.weight: grad_norm = 0.229281
  l1.bias: grad_norm = 0.000086
  l2.weight: grad_norm = 0.191213
Total gradient norm: 0.526539
=== Actor Training Debug (Iteration 7343) ===
Q mean: -10.217428
Q std: 14.048510
Actor loss: 10.221390
Action reg: 0.003961
  l1.weight: grad_norm = 0.220313
  l1.bias: grad_norm = 0.000087
  l2.weight: grad_norm = 0.202203
Total gradient norm: 0.477620
=== Actor Training Debug (Iteration 7344) ===
Q mean: -12.236010
Q std: 17.973507
Actor loss: 12.239962
Action reg: 0.003952
  l1.weight: grad_norm = 0.267528
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.241038
Total gradient norm: 0.647141
=== Actor Training Debug (Iteration 7345) ===
Q mean: -14.774542
Q std: 17.717756
Actor loss: 14.778502
Action reg: 0.003959
  l1.weight: grad_norm = 0.255578
  l1.bias: grad_norm = 0.001095
  l2.weight: grad_norm = 0.218775
Total gradient norm: 0.549398
=== Actor Training Debug (Iteration 7346) ===
Q mean: -12.228499
Q std: 15.311950
Actor loss: 12.232454
Action reg: 0.003955
  l1.weight: grad_norm = 0.317286
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.253756
Total gradient norm: 0.615249
=== Actor Training Debug (Iteration 7347) ===
Q mean: -14.240011
Q std: 18.246326
Actor loss: 14.243987
Action reg: 0.003976
  l1.weight: grad_norm = 0.129379
  l1.bias: grad_norm = 0.000036
  l2.weight: grad_norm = 0.112321
Total gradient norm: 0.282781
=== Actor Training Debug (Iteration 7348) ===
Q mean: -11.414254
Q std: 15.310390
Actor loss: 11.418213
Action reg: 0.003959
  l1.weight: grad_norm = 0.374880
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.284367
Total gradient norm: 0.740070
=== Actor Training Debug (Iteration 7349) ===
Q mean: -13.280489
Q std: 17.812685
Actor loss: 13.284453
Action reg: 0.003965
  l1.weight: grad_norm = 0.244068
  l1.bias: grad_norm = 0.000999
  l2.weight: grad_norm = 0.203046
Total gradient norm: 0.485370
=== Actor Training Debug (Iteration 7350) ===
Q mean: -12.204493
Q std: 15.306050
Actor loss: 12.208440
Action reg: 0.003948
  l1.weight: grad_norm = 0.416463
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.313947
Total gradient norm: 0.889513
=== Actor Training Debug (Iteration 7351) ===
Q mean: -12.264672
Q std: 15.865885
Actor loss: 12.268632
Action reg: 0.003960
  l1.weight: grad_norm = 0.271325
  l1.bias: grad_norm = 0.000120
  l2.weight: grad_norm = 0.228284
Total gradient norm: 0.573286
=== Actor Training Debug (Iteration 7352) ===
Q mean: -11.578428
Q std: 14.765705
Actor loss: 11.582388
Action reg: 0.003959
  l1.weight: grad_norm = 0.233627
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.203805
Total gradient norm: 0.529864
=== Actor Training Debug (Iteration 7353) ===
Q mean: -14.877262
Q std: 17.678387
Actor loss: 14.881214
Action reg: 0.003952
  l1.weight: grad_norm = 0.254899
  l1.bias: grad_norm = 0.000277
  l2.weight: grad_norm = 0.240218
Total gradient norm: 0.591890
=== Actor Training Debug (Iteration 7354) ===
Q mean: -11.145265
Q std: 15.636479
Actor loss: 11.149223
Action reg: 0.003959
  l1.weight: grad_norm = 0.230869
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.189378
Total gradient norm: 0.560922
=== Actor Training Debug (Iteration 7355) ===
Q mean: -11.735633
Q std: 16.548714
Actor loss: 11.739598
Action reg: 0.003965
  l1.weight: grad_norm = 0.161199
  l1.bias: grad_norm = 0.000119
  l2.weight: grad_norm = 0.134525
Total gradient norm: 0.334321
=== Actor Training Debug (Iteration 7356) ===
Q mean: -13.375124
Q std: 17.451117
Actor loss: 13.379076
Action reg: 0.003952
  l1.weight: grad_norm = 0.226594
  l1.bias: grad_norm = 0.000881
  l2.weight: grad_norm = 0.174242
Total gradient norm: 0.443794
=== Actor Training Debug (Iteration 7357) ===
Q mean: -12.681669
Q std: 16.427952
Actor loss: 12.685619
Action reg: 0.003950
  l1.weight: grad_norm = 0.363083
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.310613
Total gradient norm: 0.811216
=== Actor Training Debug (Iteration 7358) ===
Q mean: -10.816374
Q std: 13.437386
Actor loss: 10.820319
Action reg: 0.003945
  l1.weight: grad_norm = 0.427799
  l1.bias: grad_norm = 0.000248
  l2.weight: grad_norm = 0.385025
Total gradient norm: 1.114829
=== Actor Training Debug (Iteration 7359) ===
Q mean: -13.071204
Q std: 16.952618
Actor loss: 13.075145
Action reg: 0.003940
  l1.weight: grad_norm = 0.157376
  l1.bias: grad_norm = 0.001183
  l2.weight: grad_norm = 0.142878
Total gradient norm: 0.362134
=== Actor Training Debug (Iteration 7360) ===
Q mean: -12.895295
Q std: 15.082478
Actor loss: 12.899254
Action reg: 0.003959
  l1.weight: grad_norm = 0.221993
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.202757
Total gradient norm: 0.550810
=== Actor Training Debug (Iteration 7361) ===
Q mean: -11.735832
Q std: 18.104223
Actor loss: 11.739789
Action reg: 0.003957
  l1.weight: grad_norm = 0.317660
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.265000
Total gradient norm: 0.701451
=== Actor Training Debug (Iteration 7362) ===
Q mean: -12.311077
Q std: 15.673216
Actor loss: 12.315036
Action reg: 0.003959
  l1.weight: grad_norm = 0.204344
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.178604
Total gradient norm: 0.505855
=== Actor Training Debug (Iteration 7363) ===
Q mean: -10.512234
Q std: 14.160987
Actor loss: 10.516191
Action reg: 0.003957
  l1.weight: grad_norm = 0.156001
  l1.bias: grad_norm = 0.000099
  l2.weight: grad_norm = 0.130942
Total gradient norm: 0.355869
=== Actor Training Debug (Iteration 7364) ===
Q mean: -12.176783
Q std: 16.782480
Actor loss: 12.180748
Action reg: 0.003966
  l1.weight: grad_norm = 0.211294
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.184537
Total gradient norm: 0.508855
=== Actor Training Debug (Iteration 7365) ===
Q mean: -12.237819
Q std: 14.664965
Actor loss: 12.241766
Action reg: 0.003948
  l1.weight: grad_norm = 0.536849
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.425474
Total gradient norm: 1.039772
=== Actor Training Debug (Iteration 7366) ===
Q mean: -11.977152
Q std: 15.234590
Actor loss: 11.981112
Action reg: 0.003959
  l1.weight: grad_norm = 0.179215
  l1.bias: grad_norm = 0.000216
  l2.weight: grad_norm = 0.151590
Total gradient norm: 0.423405
=== Actor Training Debug (Iteration 7367) ===
Q mean: -12.001459
Q std: 15.595898
Actor loss: 12.005405
Action reg: 0.003947
  l1.weight: grad_norm = 0.319563
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.286987
Total gradient norm: 0.756274
=== Actor Training Debug (Iteration 7368) ===
Q mean: -11.384217
Q std: 14.686226
Actor loss: 11.388175
Action reg: 0.003957
  l1.weight: grad_norm = 0.228673
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.186387
Total gradient norm: 0.502762
=== Actor Training Debug (Iteration 7369) ===
Q mean: -12.135269
Q std: 13.936899
Actor loss: 12.139231
Action reg: 0.003962
  l1.weight: grad_norm = 0.197566
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.157943
Total gradient norm: 0.418845
=== Actor Training Debug (Iteration 7370) ===
Q mean: -11.147640
Q std: 14.196373
Actor loss: 11.151570
Action reg: 0.003930
  l1.weight: grad_norm = 0.467843
  l1.bias: grad_norm = 0.000737
  l2.weight: grad_norm = 0.407358
Total gradient norm: 1.188537
=== Actor Training Debug (Iteration 7371) ===
Q mean: -12.933760
Q std: 16.440502
Actor loss: 12.937706
Action reg: 0.003947
  l1.weight: grad_norm = 0.311124
  l1.bias: grad_norm = 0.000306
  l2.weight: grad_norm = 0.260113
Total gradient norm: 0.645068
=== Actor Training Debug (Iteration 7372) ===
Q mean: -14.135662
Q std: 18.810812
Actor loss: 14.139632
Action reg: 0.003970
  l1.weight: grad_norm = 0.254834
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.232106
Total gradient norm: 0.672187
=== Actor Training Debug (Iteration 7373) ===
Q mean: -12.046864
Q std: 16.032475
Actor loss: 12.050817
Action reg: 0.003953
  l1.weight: grad_norm = 0.395880
  l1.bias: grad_norm = 0.000851
  l2.weight: grad_norm = 0.305597
Total gradient norm: 0.791080
=== Actor Training Debug (Iteration 7374) ===
Q mean: -12.764450
Q std: 15.564998
Actor loss: 12.768418
Action reg: 0.003968
  l1.weight: grad_norm = 0.454765
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.397135
Total gradient norm: 1.027203
=== Actor Training Debug (Iteration 7375) ===
Q mean: -12.801043
Q std: 15.806060
Actor loss: 12.804986
Action reg: 0.003944
  l1.weight: grad_norm = 0.264297
  l1.bias: grad_norm = 0.000732
  l2.weight: grad_norm = 0.225919
Total gradient norm: 0.539906
=== Actor Training Debug (Iteration 7376) ===
Q mean: -12.094427
Q std: 14.189643
Actor loss: 12.098395
Action reg: 0.003968
  l1.weight: grad_norm = 0.274317
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.243006
Total gradient norm: 0.626342
=== Actor Training Debug (Iteration 7377) ===
Q mean: -11.455795
Q std: 14.617245
Actor loss: 11.459754
Action reg: 0.003959
  l1.weight: grad_norm = 0.232693
  l1.bias: grad_norm = 0.000559
  l2.weight: grad_norm = 0.188752
Total gradient norm: 0.492608
=== Actor Training Debug (Iteration 7378) ===
Q mean: -12.780342
Q std: 15.344563
Actor loss: 12.784299
Action reg: 0.003956
  l1.weight: grad_norm = 0.251012
  l1.bias: grad_norm = 0.000567
  l2.weight: grad_norm = 0.223292
Total gradient norm: 0.652899
=== Actor Training Debug (Iteration 7379) ===
Q mean: -12.542232
Q std: 16.447384
Actor loss: 12.546195
Action reg: 0.003963
  l1.weight: grad_norm = 0.268123
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.218820
Total gradient norm: 0.556939
=== Actor Training Debug (Iteration 7380) ===
Q mean: -12.670403
Q std: 16.732637
Actor loss: 12.674352
Action reg: 0.003949
  l1.weight: grad_norm = 0.366151
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.282243
Total gradient norm: 0.684702
=== Actor Training Debug (Iteration 7381) ===
Q mean: -12.452292
Q std: 15.454064
Actor loss: 12.456264
Action reg: 0.003971
  l1.weight: grad_norm = 0.178284
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.153774
Total gradient norm: 0.394542
=== Actor Training Debug (Iteration 7382) ===
Q mean: -12.645738
Q std: 16.303204
Actor loss: 12.649693
Action reg: 0.003954
  l1.weight: grad_norm = 0.393128
  l1.bias: grad_norm = 0.000469
  l2.weight: grad_norm = 0.329163
Total gradient norm: 0.824566
=== Actor Training Debug (Iteration 7383) ===
Q mean: -12.672174
Q std: 15.954956
Actor loss: 12.676142
Action reg: 0.003967
  l1.weight: grad_norm = 0.278115
  l1.bias: grad_norm = 0.000256
  l2.weight: grad_norm = 0.275842
Total gradient norm: 0.631291
=== Actor Training Debug (Iteration 7384) ===
Q mean: -12.035659
Q std: 16.521589
Actor loss: 12.039617
Action reg: 0.003957
  l1.weight: grad_norm = 0.246462
  l1.bias: grad_norm = 0.001039
  l2.weight: grad_norm = 0.229493
Total gradient norm: 0.644744
=== Actor Training Debug (Iteration 7385) ===
Q mean: -12.453633
Q std: 16.294386
Actor loss: 12.457582
Action reg: 0.003949
  l1.weight: grad_norm = 0.221952
  l1.bias: grad_norm = 0.000502
  l2.weight: grad_norm = 0.187085
Total gradient norm: 0.474025
=== Actor Training Debug (Iteration 7386) ===
Q mean: -12.072196
Q std: 16.278002
Actor loss: 12.076134
Action reg: 0.003938
  l1.weight: grad_norm = 0.414826
  l1.bias: grad_norm = 0.000513
  l2.weight: grad_norm = 0.342152
Total gradient norm: 0.823306
=== Actor Training Debug (Iteration 7387) ===
Q mean: -13.746410
Q std: 16.265366
Actor loss: 13.750371
Action reg: 0.003961
  l1.weight: grad_norm = 0.213557
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.183279
Total gradient norm: 0.500800
=== Actor Training Debug (Iteration 7388) ===
Q mean: -14.659971
Q std: 17.594837
Actor loss: 14.663938
Action reg: 0.003966
  l1.weight: grad_norm = 0.272685
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.215316
Total gradient norm: 0.596761
=== Actor Training Debug (Iteration 7389) ===
Q mean: -11.957087
Q std: 16.332815
Actor loss: 11.961038
Action reg: 0.003951
  l1.weight: grad_norm = 0.202313
  l1.bias: grad_norm = 0.000145
  l2.weight: grad_norm = 0.167095
Total gradient norm: 0.421351
=== Actor Training Debug (Iteration 7390) ===
Q mean: -11.715770
Q std: 16.414577
Actor loss: 11.719714
Action reg: 0.003944
  l1.weight: grad_norm = 0.187495
  l1.bias: grad_norm = 0.000240
  l2.weight: grad_norm = 0.154701
Total gradient norm: 0.411093
=== Actor Training Debug (Iteration 7391) ===
Q mean: -12.424689
Q std: 17.040339
Actor loss: 12.428631
Action reg: 0.003941
  l1.weight: grad_norm = 0.156220
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.141907
Total gradient norm: 0.367232
=== Actor Training Debug (Iteration 7392) ===
Q mean: -13.035614
Q std: 17.547800
Actor loss: 13.039573
Action reg: 0.003958
  l1.weight: grad_norm = 0.238162
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.195364
Total gradient norm: 0.514257
=== Actor Training Debug (Iteration 7393) ===
Q mean: -12.070421
Q std: 16.167316
Actor loss: 12.074384
Action reg: 0.003963
  l1.weight: grad_norm = 0.184650
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.176521
Total gradient norm: 0.516832
=== Actor Training Debug (Iteration 7394) ===
Q mean: -12.610224
Q std: 15.914510
Actor loss: 12.614184
Action reg: 0.003960
  l1.weight: grad_norm = 0.414499
  l1.bias: grad_norm = 0.001808
  l2.weight: grad_norm = 0.357882
Total gradient norm: 0.935621
=== Actor Training Debug (Iteration 7395) ===
Q mean: -11.527098
Q std: 16.060663
Actor loss: 11.531059
Action reg: 0.003962
  l1.weight: grad_norm = 0.312912
  l1.bias: grad_norm = 0.000997
  l2.weight: grad_norm = 0.269047
Total gradient norm: 0.651104
=== Actor Training Debug (Iteration 7396) ===
Q mean: -12.649512
Q std: 17.270515
Actor loss: 12.653463
Action reg: 0.003951
  l1.weight: grad_norm = 0.359078
  l1.bias: grad_norm = 0.002131
  l2.weight: grad_norm = 0.290231
Total gradient norm: 0.772978
=== Actor Training Debug (Iteration 7397) ===
Q mean: -12.421543
Q std: 15.821323
Actor loss: 12.425495
Action reg: 0.003952
  l1.weight: grad_norm = 0.240707
  l1.bias: grad_norm = 0.001255
  l2.weight: grad_norm = 0.206206
Total gradient norm: 0.561889
=== Actor Training Debug (Iteration 7398) ===
Q mean: -11.927057
Q std: 16.941936
Actor loss: 11.930984
Action reg: 0.003928
  l1.weight: grad_norm = 0.406986
  l1.bias: grad_norm = 0.000704
  l2.weight: grad_norm = 0.334083
Total gradient norm: 0.853224
=== Actor Training Debug (Iteration 7399) ===
Q mean: -12.277186
Q std: 15.403678
Actor loss: 12.281134
Action reg: 0.003947
  l1.weight: grad_norm = 0.348100
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.291525
Total gradient norm: 0.850286
=== Actor Training Debug (Iteration 7400) ===
Q mean: -12.038467
Q std: 15.809679
Actor loss: 12.042434
Action reg: 0.003967
  l1.weight: grad_norm = 0.237861
  l1.bias: grad_norm = 0.000337
  l2.weight: grad_norm = 0.195438
Total gradient norm: 0.516248
=== Actor Training Debug (Iteration 7401) ===
Q mean: -12.161876
Q std: 16.235306
Actor loss: 12.165836
Action reg: 0.003961
  l1.weight: grad_norm = 0.248749
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.193422
Total gradient norm: 0.473195
=== Actor Training Debug (Iteration 7402) ===
Q mean: -13.144308
Q std: 17.465422
Actor loss: 13.148240
Action reg: 0.003932
  l1.weight: grad_norm = 0.151692
  l1.bias: grad_norm = 0.000930
  l2.weight: grad_norm = 0.124612
Total gradient norm: 0.312539
=== Actor Training Debug (Iteration 7403) ===
Q mean: -13.902236
Q std: 17.960133
Actor loss: 13.906176
Action reg: 0.003940
  l1.weight: grad_norm = 0.243370
  l1.bias: grad_norm = 0.000965
  l2.weight: grad_norm = 0.178751
Total gradient norm: 0.482782
=== Actor Training Debug (Iteration 7404) ===
Q mean: -12.109719
Q std: 15.630626
Actor loss: 12.113686
Action reg: 0.003967
  l1.weight: grad_norm = 0.246760
  l1.bias: grad_norm = 0.000328
  l2.weight: grad_norm = 0.203689
Total gradient norm: 0.522225
=== Actor Training Debug (Iteration 7405) ===
Q mean: -12.881399
Q std: 15.489720
Actor loss: 12.885350
Action reg: 0.003951
  l1.weight: grad_norm = 0.382514
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.337301
Total gradient norm: 0.780693
=== Actor Training Debug (Iteration 7406) ===
Q mean: -12.389713
Q std: 14.959484
Actor loss: 12.393657
Action reg: 0.003943
  l1.weight: grad_norm = 0.353162
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.294145
Total gradient norm: 0.744616
=== Actor Training Debug (Iteration 7407) ===
Q mean: -12.893433
Q std: 16.206039
Actor loss: 12.897388
Action reg: 0.003956
  l1.weight: grad_norm = 0.163519
  l1.bias: grad_norm = 0.000545
  l2.weight: grad_norm = 0.142586
Total gradient norm: 0.502689
=== Actor Training Debug (Iteration 7408) ===
Q mean: -10.978603
Q std: 14.154429
Actor loss: 10.982548
Action reg: 0.003945
  l1.weight: grad_norm = 0.395807
  l1.bias: grad_norm = 0.000662
  l2.weight: grad_norm = 0.342356
Total gradient norm: 0.918207
=== Actor Training Debug (Iteration 7409) ===
Q mean: -11.582142
Q std: 15.976486
Actor loss: 11.586081
Action reg: 0.003939
  l1.weight: grad_norm = 0.197771
  l1.bias: grad_norm = 0.000595
  l2.weight: grad_norm = 0.170002
Total gradient norm: 0.426173
=== Actor Training Debug (Iteration 7410) ===
Q mean: -12.954105
Q std: 17.791080
Actor loss: 12.958055
Action reg: 0.003949
  l1.weight: grad_norm = 0.157948
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.142861
Total gradient norm: 0.376486
=== Actor Training Debug (Iteration 7411) ===
Q mean: -13.764286
Q std: 16.874002
Actor loss: 13.768247
Action reg: 0.003960
  l1.weight: grad_norm = 0.246075
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.215768
Total gradient norm: 0.571721
=== Actor Training Debug (Iteration 7412) ===
Q mean: -11.136032
Q std: 13.205394
Actor loss: 11.139985
Action reg: 0.003953
  l1.weight: grad_norm = 0.264057
  l1.bias: grad_norm = 0.001049
  l2.weight: grad_norm = 0.240963
Total gradient norm: 0.650737
=== Actor Training Debug (Iteration 7413) ===
Q mean: -11.665950
Q std: 14.406589
Actor loss: 11.669908
Action reg: 0.003957
  l1.weight: grad_norm = 0.183166
  l1.bias: grad_norm = 0.000192
  l2.weight: grad_norm = 0.158887
Total gradient norm: 0.412596
=== Actor Training Debug (Iteration 7414) ===
Q mean: -12.840928
Q std: 17.221840
Actor loss: 12.844888
Action reg: 0.003959
  l1.weight: grad_norm = 0.276866
  l1.bias: grad_norm = 0.000226
  l2.weight: grad_norm = 0.259067
Total gradient norm: 0.636949
=== Actor Training Debug (Iteration 7415) ===
Q mean: -11.687277
Q std: 14.416546
Actor loss: 11.691236
Action reg: 0.003959
  l1.weight: grad_norm = 0.152016
  l1.bias: grad_norm = 0.000418
  l2.weight: grad_norm = 0.134216
Total gradient norm: 0.340125
=== Actor Training Debug (Iteration 7416) ===
Q mean: -11.291248
Q std: 14.783961
Actor loss: 11.295199
Action reg: 0.003951
  l1.weight: grad_norm = 0.237693
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.220877
Total gradient norm: 0.599762
=== Actor Training Debug (Iteration 7417) ===
Q mean: -11.605400
Q std: 14.284100
Actor loss: 11.609362
Action reg: 0.003962
  l1.weight: grad_norm = 0.233930
  l1.bias: grad_norm = 0.000150
  l2.weight: grad_norm = 0.191360
Total gradient norm: 0.496411
=== Actor Training Debug (Iteration 7418) ===
Q mean: -11.983757
Q std: 14.733585
Actor loss: 11.987697
Action reg: 0.003940
  l1.weight: grad_norm = 0.336235
  l1.bias: grad_norm = 0.001435
  l2.weight: grad_norm = 0.303276
Total gradient norm: 0.773217
=== Actor Training Debug (Iteration 7419) ===
Q mean: -11.016382
Q std: 14.880387
Actor loss: 11.020320
Action reg: 0.003938
  l1.weight: grad_norm = 0.337599
  l1.bias: grad_norm = 0.000402
  l2.weight: grad_norm = 0.273571
Total gradient norm: 0.690088
=== Actor Training Debug (Iteration 7420) ===
Q mean: -10.979439
Q std: 13.443252
Actor loss: 10.983395
Action reg: 0.003956
  l1.weight: grad_norm = 0.491445
  l1.bias: grad_norm = 0.000272
  l2.weight: grad_norm = 0.383474
Total gradient norm: 0.965723
=== Actor Training Debug (Iteration 7421) ===
Q mean: -11.551329
Q std: 14.326624
Actor loss: 11.555260
Action reg: 0.003931
  l1.weight: grad_norm = 0.209041
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.172327
Total gradient norm: 0.463612
=== Actor Training Debug (Iteration 7422) ===
Q mean: -11.062754
Q std: 14.158402
Actor loss: 11.066710
Action reg: 0.003957
  l1.weight: grad_norm = 0.419624
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.340473
Total gradient norm: 0.886144
=== Actor Training Debug (Iteration 7423) ===
Q mean: -11.333828
Q std: 16.147875
Actor loss: 11.337770
Action reg: 0.003942
  l1.weight: grad_norm = 0.181323
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.135596
Total gradient norm: 0.364457
=== Actor Training Debug (Iteration 7424) ===
Q mean: -11.687981
Q std: 13.546115
Actor loss: 11.691932
Action reg: 0.003951
  l1.weight: grad_norm = 0.424135
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.361014
Total gradient norm: 0.873650
=== Actor Training Debug (Iteration 7425) ===
Q mean: -12.963912
Q std: 17.463978
Actor loss: 12.967874
Action reg: 0.003962
  l1.weight: grad_norm = 0.296040
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.231481
Total gradient norm: 0.612466
=== Actor Training Debug (Iteration 7426) ===
Q mean: -12.122126
Q std: 14.531434
Actor loss: 12.126087
Action reg: 0.003961
  l1.weight: grad_norm = 0.244842
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.193913
Total gradient norm: 0.519585
=== Actor Training Debug (Iteration 7427) ===
Q mean: -11.606955
Q std: 13.965535
Actor loss: 11.610902
Action reg: 0.003947
  l1.weight: grad_norm = 0.232772
  l1.bias: grad_norm = 0.000655
  l2.weight: grad_norm = 0.193076
Total gradient norm: 0.497205
=== Actor Training Debug (Iteration 7428) ===
Q mean: -12.907457
Q std: 15.999992
Actor loss: 12.911425
Action reg: 0.003967
  l1.weight: grad_norm = 0.120379
  l1.bias: grad_norm = 0.000108
  l2.weight: grad_norm = 0.106025
Total gradient norm: 0.289525
=== Actor Training Debug (Iteration 7429) ===
Q mean: -12.526896
Q std: 14.955676
Actor loss: 12.530841
Action reg: 0.003945
  l1.weight: grad_norm = 0.186833
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.168021
Total gradient norm: 0.433763
=== Actor Training Debug (Iteration 7430) ===
Q mean: -13.165064
Q std: 18.685783
Actor loss: 13.169013
Action reg: 0.003949
  l1.weight: grad_norm = 0.376981
  l1.bias: grad_norm = 0.000299
  l2.weight: grad_norm = 0.285478
Total gradient norm: 0.711906
=== Actor Training Debug (Iteration 7431) ===
Q mean: -12.538813
Q std: 15.410462
Actor loss: 12.542756
Action reg: 0.003943
  l1.weight: grad_norm = 0.133216
  l1.bias: grad_norm = 0.000407
  l2.weight: grad_norm = 0.121481
Total gradient norm: 0.308351
=== Actor Training Debug (Iteration 7432) ===
Q mean: -11.551302
Q std: 15.883565
Actor loss: 11.555248
Action reg: 0.003946
  l1.weight: grad_norm = 0.289601
  l1.bias: grad_norm = 0.000168
  l2.weight: grad_norm = 0.256990
Total gradient norm: 0.638994
=== Actor Training Debug (Iteration 7433) ===
Q mean: -11.730549
Q std: 15.212881
Actor loss: 11.734491
Action reg: 0.003943
  l1.weight: grad_norm = 0.370957
  l1.bias: grad_norm = 0.000166
  l2.weight: grad_norm = 0.304365
Total gradient norm: 0.743443
=== Actor Training Debug (Iteration 7434) ===
Q mean: -11.963800
Q std: 15.006123
Actor loss: 11.967765
Action reg: 0.003964
  l1.weight: grad_norm = 0.233638
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.198307
Total gradient norm: 0.528413
=== Actor Training Debug (Iteration 7435) ===
Q mean: -11.018791
Q std: 14.017921
Actor loss: 11.022758
Action reg: 0.003967
  l1.weight: grad_norm = 0.238640
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.186376
Total gradient norm: 0.522570
=== Actor Training Debug (Iteration 7436) ===
Q mean: -13.232019
Q std: 17.653938
Actor loss: 13.235983
Action reg: 0.003963
  l1.weight: grad_norm = 0.262412
  l1.bias: grad_norm = 0.000494
  l2.weight: grad_norm = 0.201879
Total gradient norm: 0.508491
=== Actor Training Debug (Iteration 7437) ===
Q mean: -15.184938
Q std: 18.964846
Actor loss: 15.188901
Action reg: 0.003963
  l1.weight: grad_norm = 0.132094
  l1.bias: grad_norm = 0.000189
  l2.weight: grad_norm = 0.106422
Total gradient norm: 0.262574
=== Actor Training Debug (Iteration 7438) ===
Q mean: -13.428532
Q std: 16.305504
Actor loss: 13.432479
Action reg: 0.003947
  l1.weight: grad_norm = 0.234074
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.201484
Total gradient norm: 0.492767
=== Actor Training Debug (Iteration 7439) ===
Q mean: -12.406863
Q std: 14.596507
Actor loss: 12.410838
Action reg: 0.003975
  l1.weight: grad_norm = 0.096958
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.076294
Total gradient norm: 0.195860
=== Actor Training Debug (Iteration 7440) ===
Q mean: -13.582435
Q std: 17.699535
Actor loss: 13.586390
Action reg: 0.003956
  l1.weight: grad_norm = 0.222622
  l1.bias: grad_norm = 0.000139
  l2.weight: grad_norm = 0.191639
Total gradient norm: 0.543393
=== Actor Training Debug (Iteration 7441) ===
Q mean: -13.058964
Q std: 16.622303
Actor loss: 13.062921
Action reg: 0.003957
  l1.weight: grad_norm = 0.240262
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.194404
Total gradient norm: 0.521268
=== Actor Training Debug (Iteration 7442) ===
Q mean: -13.488215
Q std: 15.624411
Actor loss: 13.492170
Action reg: 0.003955
  l1.weight: grad_norm = 0.271272
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.222349
Total gradient norm: 0.637837
=== Actor Training Debug (Iteration 7443) ===
Q mean: -11.802429
Q std: 14.566623
Actor loss: 11.806381
Action reg: 0.003952
  l1.weight: grad_norm = 0.399843
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.341686
Total gradient norm: 0.883341
=== Actor Training Debug (Iteration 7444) ===
Q mean: -12.081683
Q std: 16.033876
Actor loss: 12.085628
Action reg: 0.003944
  l1.weight: grad_norm = 0.282387
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.219261
Total gradient norm: 0.550711
=== Actor Training Debug (Iteration 7445) ===
Q mean: -13.761200
Q std: 17.155857
Actor loss: 13.765172
Action reg: 0.003972
  l1.weight: grad_norm = 0.130254
  l1.bias: grad_norm = 0.000064
  l2.weight: grad_norm = 0.118745
Total gradient norm: 0.319557
=== Actor Training Debug (Iteration 7446) ===
Q mean: -12.447351
Q std: 16.678986
Actor loss: 12.451306
Action reg: 0.003955
  l1.weight: grad_norm = 0.225111
  l1.bias: grad_norm = 0.000487
  l2.weight: grad_norm = 0.201567
Total gradient norm: 0.503463
=== Actor Training Debug (Iteration 7447) ===
Q mean: -11.741343
Q std: 15.789305
Actor loss: 11.745296
Action reg: 0.003954
  l1.weight: grad_norm = 0.481064
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.429085
Total gradient norm: 1.072434
=== Actor Training Debug (Iteration 7448) ===
Q mean: -13.936988
Q std: 18.930983
Actor loss: 13.940944
Action reg: 0.003956
  l1.weight: grad_norm = 0.272415
  l1.bias: grad_norm = 0.001641
  l2.weight: grad_norm = 0.267272
Total gradient norm: 0.774594
=== Actor Training Debug (Iteration 7449) ===
Q mean: -12.423438
Q std: 15.073748
Actor loss: 12.427380
Action reg: 0.003942
  l1.weight: grad_norm = 0.186271
  l1.bias: grad_norm = 0.000251
  l2.weight: grad_norm = 0.151714
Total gradient norm: 0.412370
=== Actor Training Debug (Iteration 7450) ===
Q mean: -12.174966
Q std: 15.723705
Actor loss: 12.178935
Action reg: 0.003969
  l1.weight: grad_norm = 0.186275
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.164001
Total gradient norm: 0.424252
=== Actor Training Debug (Iteration 7451) ===
Q mean: -11.912642
Q std: 14.848147
Actor loss: 11.916593
Action reg: 0.003951
  l1.weight: grad_norm = 0.244646
  l1.bias: grad_norm = 0.001126
  l2.weight: grad_norm = 0.209849
Total gradient norm: 0.485246
=== Actor Training Debug (Iteration 7452) ===
Q mean: -11.858318
Q std: 14.843404
Actor loss: 11.862268
Action reg: 0.003950
  l1.weight: grad_norm = 0.320907
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.251257
Total gradient norm: 0.718261
=== Actor Training Debug (Iteration 7453) ===
Q mean: -13.055841
Q std: 15.450365
Actor loss: 13.059789
Action reg: 0.003947
  l1.weight: grad_norm = 0.273677
  l1.bias: grad_norm = 0.000749
  l2.weight: grad_norm = 0.251234
Total gradient norm: 0.624202
=== Actor Training Debug (Iteration 7454) ===
Q mean: -12.944413
Q std: 14.700401
Actor loss: 12.948376
Action reg: 0.003962
  l1.weight: grad_norm = 0.215555
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.175883
Total gradient norm: 0.410563
=== Actor Training Debug (Iteration 7455) ===
Q mean: -13.162663
Q std: 17.424967
Actor loss: 13.166612
Action reg: 0.003949
  l1.weight: grad_norm = 0.210445
  l1.bias: grad_norm = 0.000830
  l2.weight: grad_norm = 0.197545
Total gradient norm: 0.548022
=== Actor Training Debug (Iteration 7456) ===
Q mean: -13.921991
Q std: 17.914707
Actor loss: 13.925952
Action reg: 0.003961
  l1.weight: grad_norm = 0.234215
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.207664
Total gradient norm: 0.480878
=== Actor Training Debug (Iteration 7457) ===
Q mean: -12.610356
Q std: 15.357160
Actor loss: 12.614302
Action reg: 0.003945
  l1.weight: grad_norm = 0.219827
  l1.bias: grad_norm = 0.000465
  l2.weight: grad_norm = 0.190684
Total gradient norm: 0.509014
=== Actor Training Debug (Iteration 7458) ===
Q mean: -12.953922
Q std: 16.158838
Actor loss: 12.957869
Action reg: 0.003947
  l1.weight: grad_norm = 0.154621
  l1.bias: grad_norm = 0.000532
  l2.weight: grad_norm = 0.141465
Total gradient norm: 0.411700
=== Actor Training Debug (Iteration 7459) ===
Q mean: -12.379623
Q std: 17.022394
Actor loss: 12.383566
Action reg: 0.003943
  l1.weight: grad_norm = 0.182971
  l1.bias: grad_norm = 0.000425
  l2.weight: grad_norm = 0.160385
Total gradient norm: 0.407506
=== Actor Training Debug (Iteration 7460) ===
Q mean: -13.846087
Q std: 16.845678
Actor loss: 13.850043
Action reg: 0.003956
  l1.weight: grad_norm = 0.357741
  l1.bias: grad_norm = 0.000289
  l2.weight: grad_norm = 0.339275
Total gradient norm: 0.932876
=== Actor Training Debug (Iteration 7461) ===
Q mean: -14.518242
Q std: 17.971518
Actor loss: 14.522189
Action reg: 0.003947
  l1.weight: grad_norm = 0.177840
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.148356
Total gradient norm: 0.390621
=== Actor Training Debug (Iteration 7462) ===
Q mean: -12.218192
Q std: 13.995234
Actor loss: 12.222147
Action reg: 0.003955
  l1.weight: grad_norm = 0.143279
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.115581
Total gradient norm: 0.322373
=== Actor Training Debug (Iteration 7463) ===
Q mean: -12.474630
Q std: 16.077900
Actor loss: 12.478593
Action reg: 0.003962
  l1.weight: grad_norm = 0.242611
  l1.bias: grad_norm = 0.000088
  l2.weight: grad_norm = 0.194325
Total gradient norm: 0.528230
=== Actor Training Debug (Iteration 7464) ===
Q mean: -13.780719
Q std: 15.977767
Actor loss: 13.784677
Action reg: 0.003958
  l1.weight: grad_norm = 0.145823
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.125748
Total gradient norm: 0.370246
=== Actor Training Debug (Iteration 7465) ===
Q mean: -13.840414
Q std: 17.042694
Actor loss: 13.844371
Action reg: 0.003957
  l1.weight: grad_norm = 0.237209
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.198103
Total gradient norm: 0.534072
=== Actor Training Debug (Iteration 7466) ===
Q mean: -13.340549
Q std: 15.843685
Actor loss: 13.344520
Action reg: 0.003970
  l1.weight: grad_norm = 0.222526
  l1.bias: grad_norm = 0.001045
  l2.weight: grad_norm = 0.200596
Total gradient norm: 0.514863
=== Actor Training Debug (Iteration 7467) ===
Q mean: -14.227655
Q std: 18.765692
Actor loss: 14.231626
Action reg: 0.003970
  l1.weight: grad_norm = 0.157619
  l1.bias: grad_norm = 0.000163
  l2.weight: grad_norm = 0.155829
Total gradient norm: 0.417230
=== Actor Training Debug (Iteration 7468) ===
Q mean: -12.721212
Q std: 15.855392
Actor loss: 12.725188
Action reg: 0.003976
  l1.weight: grad_norm = 0.178581
  l1.bias: grad_norm = 0.000780
  l2.weight: grad_norm = 0.153625
Total gradient norm: 0.435112
=== Actor Training Debug (Iteration 7469) ===
Q mean: -12.571557
Q std: 14.933652
Actor loss: 12.575504
Action reg: 0.003948
  l1.weight: grad_norm = 0.273189
  l1.bias: grad_norm = 0.001448
  l2.weight: grad_norm = 0.232896
Total gradient norm: 0.559392
=== Actor Training Debug (Iteration 7470) ===
Q mean: -12.709999
Q std: 15.180461
Actor loss: 12.713960
Action reg: 0.003960
  l1.weight: grad_norm = 0.233295
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.199162
Total gradient norm: 0.533047
=== Actor Training Debug (Iteration 7471) ===
Q mean: -13.662100
Q std: 17.670500
Actor loss: 13.666037
Action reg: 0.003936
  l1.weight: grad_norm = 0.307352
  l1.bias: grad_norm = 0.000791
  l2.weight: grad_norm = 0.243376
Total gradient norm: 0.607581
=== Actor Training Debug (Iteration 7472) ===
Q mean: -13.220346
Q std: 17.909597
Actor loss: 13.224312
Action reg: 0.003966
  l1.weight: grad_norm = 0.206857
  l1.bias: grad_norm = 0.000131
  l2.weight: grad_norm = 0.168425
Total gradient norm: 0.418422
=== Actor Training Debug (Iteration 7473) ===
Q mean: -12.059742
Q std: 15.616342
Actor loss: 12.063708
Action reg: 0.003966
  l1.weight: grad_norm = 0.402084
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.350905
Total gradient norm: 0.961487
=== Actor Training Debug (Iteration 7474) ===
Q mean: -13.226855
Q std: 16.041315
Actor loss: 13.230823
Action reg: 0.003967
  l1.weight: grad_norm = 0.184062
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.168791
Total gradient norm: 0.459611
=== Actor Training Debug (Iteration 7475) ===
Q mean: -11.801533
Q std: 13.292905
Actor loss: 11.805472
Action reg: 0.003940
  l1.weight: grad_norm = 0.264278
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.251032
Total gradient norm: 0.612978
=== Actor Training Debug (Iteration 7476) ===
Q mean: -13.726693
Q std: 15.646359
Actor loss: 13.730629
Action reg: 0.003936
  l1.weight: grad_norm = 0.656217
  l1.bias: grad_norm = 0.000492
  l2.weight: grad_norm = 0.528836
Total gradient norm: 1.315441
=== Actor Training Debug (Iteration 7477) ===
Q mean: -12.560211
Q std: 15.680180
Actor loss: 12.564154
Action reg: 0.003943
  l1.weight: grad_norm = 0.233140
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.194045
Total gradient norm: 0.532707
=== Actor Training Debug (Iteration 7478) ===
Q mean: -13.828362
Q std: 17.253702
Actor loss: 13.832299
Action reg: 0.003937
  l1.weight: grad_norm = 0.273704
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.241560
Total gradient norm: 0.697079
=== Actor Training Debug (Iteration 7479) ===
Q mean: -11.570431
Q std: 13.840457
Actor loss: 11.574382
Action reg: 0.003951
  l1.weight: grad_norm = 0.281771
  l1.bias: grad_norm = 0.000340
  l2.weight: grad_norm = 0.256667
Total gradient norm: 0.626852
=== Actor Training Debug (Iteration 7480) ===
Q mean: -12.279204
Q std: 15.300439
Actor loss: 12.283141
Action reg: 0.003937
  l1.weight: grad_norm = 0.301165
  l1.bias: grad_norm = 0.000777
  l2.weight: grad_norm = 0.288648
Total gradient norm: 0.868840
=== Actor Training Debug (Iteration 7481) ===
Q mean: -10.531485
Q std: 14.919107
Actor loss: 10.535437
Action reg: 0.003952
  l1.weight: grad_norm = 0.231807
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.181522
Total gradient norm: 0.476678
=== Actor Training Debug (Iteration 7482) ===
Q mean: -12.751944
Q std: 16.033379
Actor loss: 12.755907
Action reg: 0.003964
  l1.weight: grad_norm = 0.210001
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.185195
Total gradient norm: 0.491387
=== Actor Training Debug (Iteration 7483) ===
Q mean: -13.784066
Q std: 16.755123
Actor loss: 13.788021
Action reg: 0.003955
  l1.weight: grad_norm = 0.147612
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.122814
Total gradient norm: 0.344227
=== Actor Training Debug (Iteration 7484) ===
Q mean: -13.838430
Q std: 17.777605
Actor loss: 13.842377
Action reg: 0.003947
  l1.weight: grad_norm = 0.285202
  l1.bias: grad_norm = 0.000404
  l2.weight: grad_norm = 0.231696
Total gradient norm: 0.633566
=== Actor Training Debug (Iteration 7485) ===
Q mean: -11.956001
Q std: 14.594068
Actor loss: 11.959933
Action reg: 0.003932
  l1.weight: grad_norm = 0.459584
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.367084
Total gradient norm: 0.930297
=== Actor Training Debug (Iteration 7486) ===
Q mean: -13.084242
Q std: 17.197067
Actor loss: 13.088188
Action reg: 0.003946
  l1.weight: grad_norm = 0.204397
  l1.bias: grad_norm = 0.000718
  l2.weight: grad_norm = 0.188322
Total gradient norm: 0.498529
=== Actor Training Debug (Iteration 7487) ===
Q mean: -13.309036
Q std: 16.179722
Actor loss: 13.312970
Action reg: 0.003934
  l1.weight: grad_norm = 0.264597
  l1.bias: grad_norm = 0.000667
  l2.weight: grad_norm = 0.228364
Total gradient norm: 0.621255
=== Actor Training Debug (Iteration 7488) ===
Q mean: -12.364389
Q std: 16.533068
Actor loss: 12.368332
Action reg: 0.003943
  l1.weight: grad_norm = 0.199491
  l1.bias: grad_norm = 0.000917
  l2.weight: grad_norm = 0.178528
Total gradient norm: 0.510579
=== Actor Training Debug (Iteration 7489) ===
Q mean: -13.737442
Q std: 16.767994
Actor loss: 13.741385
Action reg: 0.003944
  l1.weight: grad_norm = 0.240042
  l1.bias: grad_norm = 0.000420
  l2.weight: grad_norm = 0.229978
Total gradient norm: 0.589083
=== Actor Training Debug (Iteration 7490) ===
Q mean: -11.645084
Q std: 15.218058
Actor loss: 11.649036
Action reg: 0.003952
  l1.weight: grad_norm = 0.227456
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.181518
Total gradient norm: 0.459390
=== Actor Training Debug (Iteration 7491) ===
Q mean: -12.087922
Q std: 15.655971
Actor loss: 12.091882
Action reg: 0.003960
  l1.weight: grad_norm = 0.275688
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.241199
Total gradient norm: 0.606405
=== Actor Training Debug (Iteration 7492) ===
Q mean: -12.818129
Q std: 17.052101
Actor loss: 12.822083
Action reg: 0.003954
  l1.weight: grad_norm = 0.265208
  l1.bias: grad_norm = 0.000339
  l2.weight: grad_norm = 0.224664
Total gradient norm: 0.617270
=== Actor Training Debug (Iteration 7493) ===
Q mean: -12.284587
Q std: 15.393584
Actor loss: 12.288549
Action reg: 0.003963
  l1.weight: grad_norm = 0.164029
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.161071
Total gradient norm: 0.452638
=== Actor Training Debug (Iteration 7494) ===
Q mean: -12.204794
Q std: 16.462534
Actor loss: 12.208758
Action reg: 0.003965
  l1.weight: grad_norm = 0.233996
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.191123
Total gradient norm: 0.492046
=== Actor Training Debug (Iteration 7495) ===
Q mean: -14.569798
Q std: 18.353094
Actor loss: 14.573771
Action reg: 0.003973
  l1.weight: grad_norm = 0.210355
  l1.bias: grad_norm = 0.000074
  l2.weight: grad_norm = 0.172407
Total gradient norm: 0.472965
=== Actor Training Debug (Iteration 7496) ===
Q mean: -12.792904
Q std: 15.080745
Actor loss: 12.796862
Action reg: 0.003957
  l1.weight: grad_norm = 0.212469
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.184601
Total gradient norm: 0.585970
=== Actor Training Debug (Iteration 7497) ===
Q mean: -12.431213
Q std: 15.126871
Actor loss: 12.435160
Action reg: 0.003946
  l1.weight: grad_norm = 0.179539
  l1.bias: grad_norm = 0.000570
  l2.weight: grad_norm = 0.166481
Total gradient norm: 0.441832
=== Actor Training Debug (Iteration 7498) ===
Q mean: -13.370901
Q std: 16.308212
Actor loss: 13.374856
Action reg: 0.003955
  l1.weight: grad_norm = 0.197237
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.176333
Total gradient norm: 0.459175
=== Actor Training Debug (Iteration 7499) ===
Q mean: -12.104876
Q std: 17.070650
Actor loss: 12.108809
Action reg: 0.003934
  l1.weight: grad_norm = 0.187417
  l1.bias: grad_norm = 0.001145
  l2.weight: grad_norm = 0.159740
Total gradient norm: 0.437697
=== Actor Training Debug (Iteration 7500) ===
Q mean: -12.323799
Q std: 15.730522
Actor loss: 12.327769
Action reg: 0.003970
  l1.weight: grad_norm = 0.368808
  l1.bias: grad_norm = 0.000170
  l2.weight: grad_norm = 0.270516
Total gradient norm: 0.731682
  Average reward: -323.191 | Average length: 100.0
Evaluation at episode 125: -323.191
=== Actor Training Debug (Iteration 7501) ===
Q mean: -13.315983
Q std: 18.139059
Actor loss: 13.319917
Action reg: 0.003934
  l1.weight: grad_norm = 0.213405
  l1.bias: grad_norm = 0.001425
  l2.weight: grad_norm = 0.187443
Total gradient norm: 0.521965
=== Actor Training Debug (Iteration 7502) ===
Q mean: -11.389286
Q std: 14.043325
Actor loss: 11.393216
Action reg: 0.003930
  l1.weight: grad_norm = 0.267859
  l1.bias: grad_norm = 0.001264
  l2.weight: grad_norm = 0.215376
Total gradient norm: 0.552488
=== Actor Training Debug (Iteration 7503) ===
Q mean: -13.307340
Q std: 15.565926
Actor loss: 13.311297
Action reg: 0.003958
  l1.weight: grad_norm = 0.145590
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.121276
Total gradient norm: 0.308653
=== Actor Training Debug (Iteration 7504) ===
Q mean: -13.369606
Q std: 17.346394
Actor loss: 13.373563
Action reg: 0.003956
  l1.weight: grad_norm = 0.576921
  l1.bias: grad_norm = 0.000368
  l2.weight: grad_norm = 0.425646
Total gradient norm: 1.065358
=== Actor Training Debug (Iteration 7505) ===
Q mean: -14.455029
Q std: 16.300381
Actor loss: 14.458962
Action reg: 0.003934
  l1.weight: grad_norm = 0.312891
  l1.bias: grad_norm = 0.003095
  l2.weight: grad_norm = 0.258303
Total gradient norm: 0.679032
=== Actor Training Debug (Iteration 7506) ===
Q mean: -13.078560
Q std: 17.352505
Actor loss: 13.082523
Action reg: 0.003964
  l1.weight: grad_norm = 0.214536
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.186696
Total gradient norm: 0.483336
=== Actor Training Debug (Iteration 7507) ===
Q mean: -11.579659
Q std: 14.021791
Actor loss: 11.583605
Action reg: 0.003946
  l1.weight: grad_norm = 0.257256
  l1.bias: grad_norm = 0.000944
  l2.weight: grad_norm = 0.213755
Total gradient norm: 0.618461
=== Actor Training Debug (Iteration 7508) ===
Q mean: -12.124266
Q std: 16.494637
Actor loss: 12.128221
Action reg: 0.003955
  l1.weight: grad_norm = 0.194821
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.170218
Total gradient norm: 0.463404
=== Actor Training Debug (Iteration 7509) ===
Q mean: -13.076668
Q std: 17.766541
Actor loss: 13.080627
Action reg: 0.003960
  l1.weight: grad_norm = 0.151302
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.133499
Total gradient norm: 0.369431
=== Actor Training Debug (Iteration 7510) ===
Q mean: -11.669525
Q std: 15.744208
Actor loss: 11.673456
Action reg: 0.003931
  l1.weight: grad_norm = 0.221652
  l1.bias: grad_norm = 0.000485
  l2.weight: grad_norm = 0.197879
Total gradient norm: 0.490057
=== Actor Training Debug (Iteration 7511) ===
Q mean: -12.310341
Q std: 14.621986
Actor loss: 12.314297
Action reg: 0.003956
  l1.weight: grad_norm = 0.299561
  l1.bias: grad_norm = 0.000476
  l2.weight: grad_norm = 0.262072
Total gradient norm: 0.657452
=== Actor Training Debug (Iteration 7512) ===
Q mean: -11.316914
Q std: 14.928167
Actor loss: 11.320853
Action reg: 0.003940
  l1.weight: grad_norm = 0.294179
  l1.bias: grad_norm = 0.001062
  l2.weight: grad_norm = 0.251002
Total gradient norm: 0.671685
=== Actor Training Debug (Iteration 7513) ===
Q mean: -11.101789
Q std: 12.218796
Actor loss: 11.105739
Action reg: 0.003950
  l1.weight: grad_norm = 0.283857
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.228333
Total gradient norm: 0.592732
=== Actor Training Debug (Iteration 7514) ===
Q mean: -12.012945
Q std: 17.523462
Actor loss: 12.016885
Action reg: 0.003939
  l1.weight: grad_norm = 0.239757
  l1.bias: grad_norm = 0.000497
  l2.weight: grad_norm = 0.200218
Total gradient norm: 0.478935
=== Actor Training Debug (Iteration 7515) ===
Q mean: -11.580187
Q std: 13.466088
Actor loss: 11.584162
Action reg: 0.003975
  l1.weight: grad_norm = 0.154831
  l1.bias: grad_norm = 0.000045
  l2.weight: grad_norm = 0.130425
Total gradient norm: 0.329378
=== Actor Training Debug (Iteration 7516) ===
Q mean: -13.106878
Q std: 17.480728
Actor loss: 13.110841
Action reg: 0.003962
  l1.weight: grad_norm = 0.248097
  l1.bias: grad_norm = 0.000126
  l2.weight: grad_norm = 0.199438
Total gradient norm: 0.527580
=== Actor Training Debug (Iteration 7517) ===
Q mean: -12.521074
Q std: 15.287183
Actor loss: 12.525001
Action reg: 0.003926
  l1.weight: grad_norm = 0.247650
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.218892
Total gradient norm: 0.572886
=== Actor Training Debug (Iteration 7518) ===
Q mean: -12.910861
Q std: 16.451139
Actor loss: 12.914805
Action reg: 0.003944
  l1.weight: grad_norm = 0.320919
  l1.bias: grad_norm = 0.000217
  l2.weight: grad_norm = 0.246306
Total gradient norm: 0.644376
=== Actor Training Debug (Iteration 7519) ===
Q mean: -11.116304
Q std: 13.789121
Actor loss: 11.120258
Action reg: 0.003954
  l1.weight: grad_norm = 0.279416
  l1.bias: grad_norm = 0.000997
  l2.weight: grad_norm = 0.239899
Total gradient norm: 0.694378
=== Actor Training Debug (Iteration 7520) ===
Q mean: -14.140382
Q std: 17.767582
Actor loss: 14.144346
Action reg: 0.003964
  l1.weight: grad_norm = 0.290581
  l1.bias: grad_norm = 0.002466
  l2.weight: grad_norm = 0.268955
Total gradient norm: 0.762547
=== Actor Training Debug (Iteration 7521) ===
Q mean: -11.005999
Q std: 15.846480
Actor loss: 11.009953
Action reg: 0.003954
  l1.weight: grad_norm = 0.224219
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.196131
Total gradient norm: 0.451501
=== Actor Training Debug (Iteration 7522) ===
Q mean: -13.285658
Q std: 18.035820
Actor loss: 13.289596
Action reg: 0.003938
  l1.weight: grad_norm = 0.339674
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.293592
Total gradient norm: 0.679125
=== Actor Training Debug (Iteration 7523) ===
Q mean: -11.722405
Q std: 15.132977
Actor loss: 11.726340
Action reg: 0.003935
  l1.weight: grad_norm = 0.246286
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.222537
Total gradient norm: 0.587411
=== Actor Training Debug (Iteration 7524) ===
Q mean: -13.877966
Q std: 18.061737
Actor loss: 13.881926
Action reg: 0.003960
  l1.weight: grad_norm = 0.338405
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.283340
Total gradient norm: 0.741852
=== Actor Training Debug (Iteration 7525) ===
Q mean: -13.132750
Q std: 16.362513
Actor loss: 13.136688
Action reg: 0.003939
  l1.weight: grad_norm = 0.384338
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.300390
Total gradient norm: 0.704058
=== Actor Training Debug (Iteration 7526) ===
Q mean: -12.101033
Q std: 15.767433
Actor loss: 12.104978
Action reg: 0.003944
  l1.weight: grad_norm = 0.215149
  l1.bias: grad_norm = 0.000715
  l2.weight: grad_norm = 0.186662
Total gradient norm: 0.453741
=== Actor Training Debug (Iteration 7527) ===
Q mean: -11.230852
Q std: 14.797925
Actor loss: 11.234812
Action reg: 0.003959
  l1.weight: grad_norm = 0.161730
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.139480
Total gradient norm: 0.375664
=== Actor Training Debug (Iteration 7528) ===
Q mean: -11.616472
Q std: 15.008701
Actor loss: 11.620413
Action reg: 0.003941
  l1.weight: grad_norm = 0.475499
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.382033
Total gradient norm: 1.101023
=== Actor Training Debug (Iteration 7529) ===
Q mean: -13.031333
Q std: 17.180180
Actor loss: 13.035298
Action reg: 0.003966
  l1.weight: grad_norm = 0.147579
  l1.bias: grad_norm = 0.000121
  l2.weight: grad_norm = 0.126776
Total gradient norm: 0.330870
=== Actor Training Debug (Iteration 7530) ===
Q mean: -12.592704
Q std: 15.583265
Actor loss: 12.596674
Action reg: 0.003970
  l1.weight: grad_norm = 0.276702
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.242974
Total gradient norm: 0.666765
=== Actor Training Debug (Iteration 7531) ===
Q mean: -10.939531
Q std: 14.466738
Actor loss: 10.943485
Action reg: 0.003954
  l1.weight: grad_norm = 0.251299
  l1.bias: grad_norm = 0.000442
  l2.weight: grad_norm = 0.214838
Total gradient norm: 0.650423
=== Actor Training Debug (Iteration 7532) ===
Q mean: -13.179608
Q std: 16.558094
Actor loss: 13.183567
Action reg: 0.003959
  l1.weight: grad_norm = 0.316931
  l1.bias: grad_norm = 0.000335
  l2.weight: grad_norm = 0.276601
Total gradient norm: 0.694540
=== Actor Training Debug (Iteration 7533) ===
Q mean: -11.301634
Q std: 15.361190
Actor loss: 11.305576
Action reg: 0.003942
  l1.weight: grad_norm = 0.148088
  l1.bias: grad_norm = 0.000764
  l2.weight: grad_norm = 0.113809
Total gradient norm: 0.305280
=== Actor Training Debug (Iteration 7534) ===
Q mean: -13.335825
Q std: 16.538866
Actor loss: 13.339787
Action reg: 0.003963
  l1.weight: grad_norm = 0.597948
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.526735
Total gradient norm: 1.392333
=== Actor Training Debug (Iteration 7535) ===
Q mean: -11.971830
Q std: 14.669057
Actor loss: 11.975786
Action reg: 0.003956
  l1.weight: grad_norm = 0.234844
  l1.bias: grad_norm = 0.000293
  l2.weight: grad_norm = 0.194713
Total gradient norm: 0.474860
=== Actor Training Debug (Iteration 7536) ===
Q mean: -12.281124
Q std: 16.540676
Actor loss: 12.285090
Action reg: 0.003967
  l1.weight: grad_norm = 0.148322
  l1.bias: grad_norm = 0.000750
  l2.weight: grad_norm = 0.126203
Total gradient norm: 0.350095
=== Actor Training Debug (Iteration 7537) ===
Q mean: -13.016361
Q std: 15.081087
Actor loss: 13.020319
Action reg: 0.003958
  l1.weight: grad_norm = 0.182023
  l1.bias: grad_norm = 0.000378
  l2.weight: grad_norm = 0.153697
Total gradient norm: 0.456643
=== Actor Training Debug (Iteration 7538) ===
Q mean: -10.428180
Q std: 14.651392
Actor loss: 10.432123
Action reg: 0.003943
  l1.weight: grad_norm = 0.251456
  l1.bias: grad_norm = 0.000700
  l2.weight: grad_norm = 0.200455
Total gradient norm: 0.601696
=== Actor Training Debug (Iteration 7539) ===
Q mean: -11.347266
Q std: 13.277710
Actor loss: 11.351233
Action reg: 0.003966
  l1.weight: grad_norm = 0.184784
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.154127
Total gradient norm: 0.397815
=== Actor Training Debug (Iteration 7540) ===
Q mean: -13.902524
Q std: 16.480028
Actor loss: 13.906480
Action reg: 0.003956
  l1.weight: grad_norm = 0.326268
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.293746
Total gradient norm: 0.678696
=== Actor Training Debug (Iteration 7541) ===
Q mean: -12.574127
Q std: 15.178571
Actor loss: 12.578058
Action reg: 0.003931
  l1.weight: grad_norm = 0.238524
  l1.bias: grad_norm = 0.001228
  l2.weight: grad_norm = 0.184352
Total gradient norm: 0.521373
=== Actor Training Debug (Iteration 7542) ===
Q mean: -10.989073
Q std: 14.110513
Actor loss: 10.993018
Action reg: 0.003946
  l1.weight: grad_norm = 0.153037
  l1.bias: grad_norm = 0.001438
  l2.weight: grad_norm = 0.145298
Total gradient norm: 0.397969
=== Actor Training Debug (Iteration 7543) ===
Q mean: -12.581449
Q std: 16.842211
Actor loss: 12.585387
Action reg: 0.003938
  l1.weight: grad_norm = 0.167339
  l1.bias: grad_norm = 0.000586
  l2.weight: grad_norm = 0.146536
Total gradient norm: 0.366277
=== Actor Training Debug (Iteration 7544) ===
Q mean: -12.706120
Q std: 15.872015
Actor loss: 12.710083
Action reg: 0.003962
  l1.weight: grad_norm = 0.370342
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.305853
Total gradient norm: 0.873313
=== Actor Training Debug (Iteration 7545) ===
Q mean: -12.905036
Q std: 17.056116
Actor loss: 12.908984
Action reg: 0.003948
  l1.weight: grad_norm = 0.459562
  l1.bias: grad_norm = 0.000475
  l2.weight: grad_norm = 0.393676
Total gradient norm: 1.023103
=== Actor Training Debug (Iteration 7546) ===
Q mean: -12.402376
Q std: 13.493865
Actor loss: 12.406324
Action reg: 0.003949
  l1.weight: grad_norm = 0.159123
  l1.bias: grad_norm = 0.001236
  l2.weight: grad_norm = 0.133689
Total gradient norm: 0.388429
=== Actor Training Debug (Iteration 7547) ===
Q mean: -12.903559
Q std: 16.735271
Actor loss: 12.907518
Action reg: 0.003960
  l1.weight: grad_norm = 0.105083
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.091784
Total gradient norm: 0.237908
=== Actor Training Debug (Iteration 7548) ===
Q mean: -11.585873
Q std: 14.570823
Actor loss: 11.589828
Action reg: 0.003955
  l1.weight: grad_norm = 0.314899
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.268992
Total gradient norm: 0.698529
=== Actor Training Debug (Iteration 7549) ===
Q mean: -11.963406
Q std: 17.662029
Actor loss: 11.967354
Action reg: 0.003948
  l1.weight: grad_norm = 0.420152
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.340598
Total gradient norm: 0.899920
=== Actor Training Debug (Iteration 7550) ===
Q mean: -13.184042
Q std: 16.059528
Actor loss: 13.187985
Action reg: 0.003944
  l1.weight: grad_norm = 0.397562
  l1.bias: grad_norm = 0.000872
  l2.weight: grad_norm = 0.325054
Total gradient norm: 0.906409
=== Actor Training Debug (Iteration 7551) ===
Q mean: -12.881795
Q std: 14.986712
Actor loss: 12.885759
Action reg: 0.003965
  l1.weight: grad_norm = 0.325951
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.258816
Total gradient norm: 0.705607
=== Actor Training Debug (Iteration 7552) ===
Q mean: -13.477133
Q std: 16.587036
Actor loss: 13.481108
Action reg: 0.003975
  l1.weight: grad_norm = 0.248478
  l1.bias: grad_norm = 0.000142
  l2.weight: grad_norm = 0.223432
Total gradient norm: 0.571123
=== Actor Training Debug (Iteration 7553) ===
Q mean: -12.243449
Q std: 15.974485
Actor loss: 12.247414
Action reg: 0.003964
  l1.weight: grad_norm = 0.245527
  l1.bias: grad_norm = 0.000375
  l2.weight: grad_norm = 0.180063
Total gradient norm: 0.482209
=== Actor Training Debug (Iteration 7554) ===
Q mean: -12.594481
Q std: 16.394337
Actor loss: 12.598439
Action reg: 0.003958
  l1.weight: grad_norm = 0.297371
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.243189
Total gradient norm: 0.618626
=== Actor Training Debug (Iteration 7555) ===
Q mean: -11.690517
Q std: 15.702544
Actor loss: 11.694470
Action reg: 0.003953
  l1.weight: grad_norm = 0.227122
  l1.bias: grad_norm = 0.001814
  l2.weight: grad_norm = 0.204211
Total gradient norm: 0.521887
=== Actor Training Debug (Iteration 7556) ===
Q mean: -11.145058
Q std: 12.077344
Actor loss: 11.149025
Action reg: 0.003968
  l1.weight: grad_norm = 0.139571
  l1.bias: grad_norm = 0.000054
  l2.weight: grad_norm = 0.133908
Total gradient norm: 0.427762
=== Actor Training Debug (Iteration 7557) ===
Q mean: -11.811199
Q std: 15.260264
Actor loss: 11.815157
Action reg: 0.003957
  l1.weight: grad_norm = 0.211021
  l1.bias: grad_norm = 0.001186
  l2.weight: grad_norm = 0.178436
Total gradient norm: 0.509152
=== Actor Training Debug (Iteration 7558) ===
Q mean: -13.369404
Q std: 14.675095
Actor loss: 13.373348
Action reg: 0.003945
  l1.weight: grad_norm = 0.428658
  l1.bias: grad_norm = 0.000724
  l2.weight: grad_norm = 0.366883
Total gradient norm: 0.823707
=== Actor Training Debug (Iteration 7559) ===
Q mean: -10.498040
Q std: 15.054088
Actor loss: 10.501980
Action reg: 0.003940
  l1.weight: grad_norm = 0.201204
  l1.bias: grad_norm = 0.001308
  l2.weight: grad_norm = 0.159801
Total gradient norm: 0.419782
=== Actor Training Debug (Iteration 7560) ===
Q mean: -15.097656
Q std: 19.607779
Actor loss: 15.101611
Action reg: 0.003955
  l1.weight: grad_norm = 0.240453
  l1.bias: grad_norm = 0.000848
  l2.weight: grad_norm = 0.210442
Total gradient norm: 0.602987
=== Actor Training Debug (Iteration 7561) ===
Q mean: -13.338552
Q std: 17.203009
Actor loss: 13.342497
Action reg: 0.003944
  l1.weight: grad_norm = 0.198711
  l1.bias: grad_norm = 0.000540
  l2.weight: grad_norm = 0.170526
Total gradient norm: 0.422877
=== Actor Training Debug (Iteration 7562) ===
Q mean: -11.793925
Q std: 14.408752
Actor loss: 11.797883
Action reg: 0.003958
  l1.weight: grad_norm = 0.206131
  l1.bias: grad_norm = 0.001024
  l2.weight: grad_norm = 0.178245
Total gradient norm: 0.509817
=== Actor Training Debug (Iteration 7563) ===
Q mean: -12.163549
Q std: 15.455220
Actor loss: 12.167513
Action reg: 0.003963
  l1.weight: grad_norm = 0.259885
  l1.bias: grad_norm = 0.000231
  l2.weight: grad_norm = 0.224527
Total gradient norm: 0.547391
=== Actor Training Debug (Iteration 7564) ===
Q mean: -12.667933
Q std: 16.258940
Actor loss: 12.671889
Action reg: 0.003957
  l1.weight: grad_norm = 0.537197
  l1.bias: grad_norm = 0.001007
  l2.weight: grad_norm = 0.436972
Total gradient norm: 1.154326
=== Actor Training Debug (Iteration 7565) ===
Q mean: -10.755795
Q std: 14.937979
Actor loss: 10.759750
Action reg: 0.003955
  l1.weight: grad_norm = 0.226250
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.191901
Total gradient norm: 0.488120
=== Actor Training Debug (Iteration 7566) ===
Q mean: -12.217707
Q std: 15.571186
Actor loss: 12.221645
Action reg: 0.003938
  l1.weight: grad_norm = 0.205280
  l1.bias: grad_norm = 0.001352
  l2.weight: grad_norm = 0.180729
Total gradient norm: 0.518309
=== Actor Training Debug (Iteration 7567) ===
Q mean: -13.583204
Q std: 17.427393
Actor loss: 13.587167
Action reg: 0.003963
  l1.weight: grad_norm = 0.155708
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.120648
Total gradient norm: 0.297966
=== Actor Training Debug (Iteration 7568) ===
Q mean: -11.256963
Q std: 13.697311
Actor loss: 11.260917
Action reg: 0.003954
  l1.weight: grad_norm = 0.407614
  l1.bias: grad_norm = 0.001018
  l2.weight: grad_norm = 0.354882
Total gradient norm: 0.994810
=== Actor Training Debug (Iteration 7569) ===
Q mean: -13.180841
Q std: 16.492924
Actor loss: 13.184791
Action reg: 0.003949
  l1.weight: grad_norm = 0.296828
  l1.bias: grad_norm = 0.001247
  l2.weight: grad_norm = 0.248256
Total gradient norm: 0.620010
=== Actor Training Debug (Iteration 7570) ===
Q mean: -12.820594
Q std: 16.209358
Actor loss: 12.824569
Action reg: 0.003975
  l1.weight: grad_norm = 0.255641
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.230758
Total gradient norm: 0.649103
=== Actor Training Debug (Iteration 7571) ===
Q mean: -15.241959
Q std: 18.884268
Actor loss: 15.245920
Action reg: 0.003962
  l1.weight: grad_norm = 0.456912
  l1.bias: grad_norm = 0.000241
  l2.weight: grad_norm = 0.348509
Total gradient norm: 0.951027
=== Actor Training Debug (Iteration 7572) ===
Q mean: -13.961570
Q std: 17.762363
Actor loss: 13.965524
Action reg: 0.003954
  l1.weight: grad_norm = 0.228947
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.189985
Total gradient norm: 0.546151
=== Actor Training Debug (Iteration 7573) ===
Q mean: -12.599370
Q std: 17.167545
Actor loss: 12.603325
Action reg: 0.003955
  l1.weight: grad_norm = 0.261902
  l1.bias: grad_norm = 0.000406
  l2.weight: grad_norm = 0.206345
Total gradient norm: 0.503627
=== Actor Training Debug (Iteration 7574) ===
Q mean: -11.542950
Q std: 15.342248
Actor loss: 11.546885
Action reg: 0.003935
  l1.weight: grad_norm = 0.321055
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.277123
Total gradient norm: 0.631685
=== Actor Training Debug (Iteration 7575) ===
Q mean: -11.524880
Q std: 14.305521
Actor loss: 11.528835
Action reg: 0.003955
  l1.weight: grad_norm = 0.304647
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.255501
Total gradient norm: 0.633575
=== Actor Training Debug (Iteration 7576) ===
Q mean: -13.500789
Q std: 17.425735
Actor loss: 13.504750
Action reg: 0.003961
  l1.weight: grad_norm = 0.196490
  l1.bias: grad_norm = 0.000719
  l2.weight: grad_norm = 0.167652
Total gradient norm: 0.396213
=== Actor Training Debug (Iteration 7577) ===
Q mean: -14.091911
Q std: 17.447300
Actor loss: 14.095847
Action reg: 0.003936
  l1.weight: grad_norm = 0.251029
  l1.bias: grad_norm = 0.000886
  l2.weight: grad_norm = 0.200944
Total gradient norm: 0.480417
=== Actor Training Debug (Iteration 7578) ===
Q mean: -13.061892
Q std: 15.397806
Actor loss: 13.065865
Action reg: 0.003973
  l1.weight: grad_norm = 0.256580
  l1.bias: grad_norm = 0.000082
  l2.weight: grad_norm = 0.200999
Total gradient norm: 0.483549
=== Actor Training Debug (Iteration 7579) ===
Q mean: -12.680168
Q std: 14.745915
Actor loss: 12.684113
Action reg: 0.003945
  l1.weight: grad_norm = 0.460022
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.374936
Total gradient norm: 1.181964
=== Actor Training Debug (Iteration 7580) ===
Q mean: -12.315638
Q std: 16.128033
Actor loss: 12.319613
Action reg: 0.003975
  l1.weight: grad_norm = 0.336376
  l1.bias: grad_norm = 0.000242
  l2.weight: grad_norm = 0.247199
Total gradient norm: 0.580993
=== Actor Training Debug (Iteration 7581) ===
Q mean: -12.171346
Q std: 15.888079
Actor loss: 12.175302
Action reg: 0.003956
  l1.weight: grad_norm = 0.334181
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.305229
Total gradient norm: 0.769622
=== Actor Training Debug (Iteration 7582) ===
Q mean: -11.106577
Q std: 13.952688
Actor loss: 11.110532
Action reg: 0.003955
  l1.weight: grad_norm = 0.206979
  l1.bias: grad_norm = 0.000356
  l2.weight: grad_norm = 0.169053
Total gradient norm: 0.443558
=== Actor Training Debug (Iteration 7583) ===
Q mean: -12.500885
Q std: 15.262371
Actor loss: 12.504848
Action reg: 0.003963
  l1.weight: grad_norm = 0.148119
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.130381
Total gradient norm: 0.362529
=== Actor Training Debug (Iteration 7584) ===
Q mean: -13.245599
Q std: 17.250477
Actor loss: 13.249577
Action reg: 0.003978
  l1.weight: grad_norm = 0.111990
  l1.bias: grad_norm = 0.000237
  l2.weight: grad_norm = 0.106241
Total gradient norm: 0.284294
=== Actor Training Debug (Iteration 7585) ===
Q mean: -14.306252
Q std: 18.503489
Actor loss: 14.310212
Action reg: 0.003960
  l1.weight: grad_norm = 0.267144
  l1.bias: grad_norm = 0.000279
  l2.weight: grad_norm = 0.208784
Total gradient norm: 0.561599
=== Actor Training Debug (Iteration 7586) ===
Q mean: -12.503187
Q std: 16.076477
Actor loss: 12.507141
Action reg: 0.003954
  l1.weight: grad_norm = 0.408651
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.333361
Total gradient norm: 0.881499
=== Actor Training Debug (Iteration 7587) ===
Q mean: -11.091136
Q std: 14.257372
Actor loss: 11.095078
Action reg: 0.003942
  l1.weight: grad_norm = 0.258346
  l1.bias: grad_norm = 0.000526
  l2.weight: grad_norm = 0.214068
Total gradient norm: 0.572255
=== Actor Training Debug (Iteration 7588) ===
Q mean: -14.469040
Q std: 18.788841
Actor loss: 14.472985
Action reg: 0.003945
  l1.weight: grad_norm = 0.418749
  l1.bias: grad_norm = 0.000415
  l2.weight: grad_norm = 0.367475
Total gradient norm: 0.961467
=== Actor Training Debug (Iteration 7589) ===
Q mean: -11.647812
Q std: 13.707224
Actor loss: 11.651734
Action reg: 0.003923
  l1.weight: grad_norm = 0.246406
  l1.bias: grad_norm = 0.000659
  l2.weight: grad_norm = 0.201200
Total gradient norm: 0.513116
=== Actor Training Debug (Iteration 7590) ===
Q mean: -12.070399
Q std: 15.363725
Actor loss: 12.074341
Action reg: 0.003941
  l1.weight: grad_norm = 0.157506
  l1.bias: grad_norm = 0.000461
  l2.weight: grad_norm = 0.134734
Total gradient norm: 0.352554
=== Actor Training Debug (Iteration 7591) ===
Q mean: -13.251979
Q std: 16.889109
Actor loss: 13.255941
Action reg: 0.003962
  l1.weight: grad_norm = 0.329940
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.271700
Total gradient norm: 0.658115
=== Actor Training Debug (Iteration 7592) ===
Q mean: -12.995236
Q std: 16.692497
Actor loss: 12.999160
Action reg: 0.003924
  l1.weight: grad_norm = 2.072491
  l1.bias: grad_norm = 0.000923
  l2.weight: grad_norm = 1.591121
Total gradient norm: 3.634066
=== Actor Training Debug (Iteration 7593) ===
Q mean: -13.862024
Q std: 17.722420
Actor loss: 13.865981
Action reg: 0.003956
  l1.weight: grad_norm = 0.195030
  l1.bias: grad_norm = 0.000127
  l2.weight: grad_norm = 0.171384
Total gradient norm: 0.476066
=== Actor Training Debug (Iteration 7594) ===
Q mean: -13.530684
Q std: 16.313494
Actor loss: 13.534626
Action reg: 0.003943
  l1.weight: grad_norm = 0.508819
  l1.bias: grad_norm = 0.000301
  l2.weight: grad_norm = 0.383538
Total gradient norm: 0.939550
=== Actor Training Debug (Iteration 7595) ===
Q mean: -11.422630
Q std: 12.310210
Actor loss: 11.426587
Action reg: 0.003957
  l1.weight: grad_norm = 0.195902
  l1.bias: grad_norm = 0.000358
  l2.weight: grad_norm = 0.176894
Total gradient norm: 0.476600
=== Actor Training Debug (Iteration 7596) ===
Q mean: -13.766171
Q std: 16.713413
Actor loss: 13.770125
Action reg: 0.003954
  l1.weight: grad_norm = 0.211599
  l1.bias: grad_norm = 0.000156
  l2.weight: grad_norm = 0.191182
Total gradient norm: 0.546164
=== Actor Training Debug (Iteration 7597) ===
Q mean: -12.706177
Q std: 16.121170
Actor loss: 12.710126
Action reg: 0.003949
  l1.weight: grad_norm = 0.332402
  l1.bias: grad_norm = 0.000308
  l2.weight: grad_norm = 0.287861
Total gradient norm: 0.682101
=== Actor Training Debug (Iteration 7598) ===
Q mean: -12.092027
Q std: 18.185270
Actor loss: 12.095968
Action reg: 0.003942
  l1.weight: grad_norm = 0.788267
  l1.bias: grad_norm = 0.000324
  l2.weight: grad_norm = 0.677049
Total gradient norm: 1.985683
=== Actor Training Debug (Iteration 7599) ===
Q mean: -12.143078
Q std: 16.578712
Actor loss: 12.147024
Action reg: 0.003946
  l1.weight: grad_norm = 0.237849
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.195297
Total gradient norm: 0.511655
=== Actor Training Debug (Iteration 7600) ===
Q mean: -12.302525
Q std: 15.025830
Actor loss: 12.306485
Action reg: 0.003961
  l1.weight: grad_norm = 0.169246
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.150199
Total gradient norm: 0.438653
=== Actor Training Debug (Iteration 7601) ===
Q mean: -10.775141
Q std: 15.594994
Actor loss: 10.779067
Action reg: 0.003926
  l1.weight: grad_norm = 0.245936
  l1.bias: grad_norm = 0.001483
  l2.weight: grad_norm = 0.225225
Total gradient norm: 0.535926
=== Actor Training Debug (Iteration 7602) ===
Q mean: -13.260280
Q std: 16.526270
Actor loss: 13.264218
Action reg: 0.003939
  l1.weight: grad_norm = 0.212358
  l1.bias: grad_norm = 0.000886
  l2.weight: grad_norm = 0.178644
Total gradient norm: 0.500265
=== Actor Training Debug (Iteration 7603) ===
Q mean: -10.382754
Q std: 12.939355
Actor loss: 10.386681
Action reg: 0.003926
  l1.weight: grad_norm = 0.418051
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.343219
Total gradient norm: 0.864902
=== Actor Training Debug (Iteration 7604) ===
Q mean: -12.359295
Q std: 15.747845
Actor loss: 12.363242
Action reg: 0.003947
  l1.weight: grad_norm = 0.355711
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.297779
Total gradient norm: 0.754520
=== Actor Training Debug (Iteration 7605) ===
Q mean: -12.353544
Q std: 16.506634
Actor loss: 12.357493
Action reg: 0.003950
  l1.weight: grad_norm = 0.238010
  l1.bias: grad_norm = 0.001244
  l2.weight: grad_norm = 0.217749
Total gradient norm: 0.619094
=== Actor Training Debug (Iteration 7606) ===
Q mean: -11.694630
Q std: 15.554817
Actor loss: 11.698557
Action reg: 0.003928
  l1.weight: grad_norm = 0.337170
  l1.bias: grad_norm = 0.001044
  l2.weight: grad_norm = 0.283644
Total gradient norm: 0.762304
=== Actor Training Debug (Iteration 7607) ===
Q mean: -12.845457
Q std: 16.481960
Actor loss: 12.849392
Action reg: 0.003935
  l1.weight: grad_norm = 1.068906
  l1.bias: grad_norm = 0.000726
  l2.weight: grad_norm = 0.890786
Total gradient norm: 2.365918
=== Actor Training Debug (Iteration 7608) ===
Q mean: -10.669244
Q std: 12.609035
Actor loss: 10.673191
Action reg: 0.003947
  l1.weight: grad_norm = 0.177095
  l1.bias: grad_norm = 0.001757
  l2.weight: grad_norm = 0.167081
Total gradient norm: 0.537717
=== Actor Training Debug (Iteration 7609) ===
Q mean: -12.610342
Q std: 16.870611
Actor loss: 12.614281
Action reg: 0.003939
  l1.weight: grad_norm = 0.570949
  l1.bias: grad_norm = 0.001680
  l2.weight: grad_norm = 0.468606
Total gradient norm: 1.311463
=== Actor Training Debug (Iteration 7610) ===
Q mean: -12.047893
Q std: 14.281537
Actor loss: 12.051826
Action reg: 0.003933
  l1.weight: grad_norm = 0.499319
  l1.bias: grad_norm = 0.002327
  l2.weight: grad_norm = 0.382346
Total gradient norm: 0.974917
=== Actor Training Debug (Iteration 7611) ===
Q mean: -13.354130
Q std: 16.472351
Actor loss: 13.358083
Action reg: 0.003953
  l1.weight: grad_norm = 0.110176
  l1.bias: grad_norm = 0.001147
  l2.weight: grad_norm = 0.100803
Total gradient norm: 0.288374
=== Actor Training Debug (Iteration 7612) ===
Q mean: -11.299140
Q std: 14.988302
Actor loss: 11.303085
Action reg: 0.003945
  l1.weight: grad_norm = 0.272046
  l1.bias: grad_norm = 0.002236
  l2.weight: grad_norm = 0.227588
Total gradient norm: 0.620241
=== Actor Training Debug (Iteration 7613) ===
Q mean: -14.068013
Q std: 18.140308
Actor loss: 14.071970
Action reg: 0.003957
  l1.weight: grad_norm = 0.258425
  l1.bias: grad_norm = 0.001274
  l2.weight: grad_norm = 0.229942
Total gradient norm: 0.651997
=== Actor Training Debug (Iteration 7614) ===
Q mean: -12.708551
Q std: 16.864164
Actor loss: 12.712495
Action reg: 0.003943
  l1.weight: grad_norm = 0.231129
  l1.bias: grad_norm = 0.002139
  l2.weight: grad_norm = 0.196575
Total gradient norm: 0.532023
=== Actor Training Debug (Iteration 7615) ===
Q mean: -13.093967
Q std: 17.995615
Actor loss: 13.097904
Action reg: 0.003937
  l1.weight: grad_norm = 0.216627
  l1.bias: grad_norm = 0.000762
  l2.weight: grad_norm = 0.180169
Total gradient norm: 0.481173
=== Actor Training Debug (Iteration 7616) ===
Q mean: -13.834755
Q std: 17.394407
Actor loss: 13.838716
Action reg: 0.003960
  l1.weight: grad_norm = 0.192603
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.190738
Total gradient norm: 0.462986
=== Actor Training Debug (Iteration 7617) ===
Q mean: -12.101967
Q std: 16.796432
Actor loss: 12.105918
Action reg: 0.003951
  l1.weight: grad_norm = 0.290731
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.263633
Total gradient norm: 0.680400
=== Actor Training Debug (Iteration 7618) ===
Q mean: -12.173526
Q std: 15.043347
Actor loss: 12.177491
Action reg: 0.003965
  l1.weight: grad_norm = 0.231824
  l1.bias: grad_norm = 0.000100
  l2.weight: grad_norm = 0.164584
Total gradient norm: 0.411665
=== Actor Training Debug (Iteration 7619) ===
Q mean: -11.052517
Q std: 14.142829
Actor loss: 11.056466
Action reg: 0.003949
  l1.weight: grad_norm = 0.682962
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.553911
Total gradient norm: 1.573564
=== Actor Training Debug (Iteration 7620) ===
Q mean: -11.181576
Q std: 15.414537
Actor loss: 11.185528
Action reg: 0.003952
  l1.weight: grad_norm = 0.259609
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.219293
Total gradient norm: 0.561547
=== Actor Training Debug (Iteration 7621) ===
Q mean: -12.264992
Q std: 14.613423
Actor loss: 12.268960
Action reg: 0.003968
  l1.weight: grad_norm = 0.197633
  l1.bias: grad_norm = 0.000102
  l2.weight: grad_norm = 0.161510
Total gradient norm: 0.428994
=== Actor Training Debug (Iteration 7622) ===
Q mean: -12.507797
Q std: 13.939337
Actor loss: 12.511754
Action reg: 0.003957
  l1.weight: grad_norm = 0.228149
  l1.bias: grad_norm = 0.000250
  l2.weight: grad_norm = 0.189899
Total gradient norm: 0.483904
=== Actor Training Debug (Iteration 7623) ===
Q mean: -13.227116
Q std: 17.054871
Actor loss: 13.231081
Action reg: 0.003965
  l1.weight: grad_norm = 0.238586
  l1.bias: grad_norm = 0.000175
  l2.weight: grad_norm = 0.199637
Total gradient norm: 0.524186
=== Actor Training Debug (Iteration 7624) ===
Q mean: -13.100225
Q std: 16.010292
Actor loss: 13.104170
Action reg: 0.003944
  l1.weight: grad_norm = 0.206515
  l1.bias: grad_norm = 0.000483
  l2.weight: grad_norm = 0.171291
Total gradient norm: 0.484436
=== Actor Training Debug (Iteration 7625) ===
Q mean: -12.935511
Q std: 16.276169
Actor loss: 12.939455
Action reg: 0.003945
  l1.weight: grad_norm = 0.344122
  l1.bias: grad_norm = 0.000221
  l2.weight: grad_norm = 0.321720
Total gradient norm: 0.979323
=== Actor Training Debug (Iteration 7626) ===
Q mean: -12.192904
Q std: 13.794450
Actor loss: 12.196852
Action reg: 0.003948
  l1.weight: grad_norm = 0.151730
  l1.bias: grad_norm = 0.000809
  l2.weight: grad_norm = 0.120053
Total gradient norm: 0.333401
=== Actor Training Debug (Iteration 7627) ===
Q mean: -12.280263
Q std: 15.067911
Actor loss: 12.284204
Action reg: 0.003942
  l1.weight: grad_norm = 0.237340
  l1.bias: grad_norm = 0.001610
  l2.weight: grad_norm = 0.224252
Total gradient norm: 0.615910
=== Actor Training Debug (Iteration 7628) ===
Q mean: -11.778505
Q std: 14.957649
Actor loss: 11.782446
Action reg: 0.003941
  l1.weight: grad_norm = 0.287004
  l1.bias: grad_norm = 0.001012
  l2.weight: grad_norm = 0.245783
Total gradient norm: 0.645471
=== Actor Training Debug (Iteration 7629) ===
Q mean: -14.012337
Q std: 18.784224
Actor loss: 14.016277
Action reg: 0.003941
  l1.weight: grad_norm = 0.741044
  l1.bias: grad_norm = 0.001011
  l2.weight: grad_norm = 0.632894
Total gradient norm: 1.983374
=== Actor Training Debug (Iteration 7630) ===
Q mean: -11.536403
Q std: 14.676291
Actor loss: 11.540347
Action reg: 0.003945
  l1.weight: grad_norm = 0.302555
  l1.bias: grad_norm = 0.000518
  l2.weight: grad_norm = 0.255457
Total gradient norm: 0.714385
=== Actor Training Debug (Iteration 7631) ===
Q mean: -14.812078
Q std: 16.703524
Actor loss: 14.816043
Action reg: 0.003964
  l1.weight: grad_norm = 0.225759
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.207908
Total gradient norm: 0.575196
=== Actor Training Debug (Iteration 7632) ===
Q mean: -12.555666
Q std: 15.018100
Actor loss: 12.559591
Action reg: 0.003925
  l1.weight: grad_norm = 0.318367
  l1.bias: grad_norm = 0.001312
  l2.weight: grad_norm = 0.299093
Total gradient norm: 0.794293
=== Actor Training Debug (Iteration 7633) ===
Q mean: -11.762293
Q std: 14.546431
Actor loss: 11.766258
Action reg: 0.003966
  l1.weight: grad_norm = 0.248883
  l1.bias: grad_norm = 0.000511
  l2.weight: grad_norm = 0.219236
Total gradient norm: 0.552900
=== Actor Training Debug (Iteration 7634) ===
Q mean: -13.898132
Q std: 18.721199
Actor loss: 13.902083
Action reg: 0.003951
  l1.weight: grad_norm = 0.183623
  l1.bias: grad_norm = 0.000923
  l2.weight: grad_norm = 0.151212
Total gradient norm: 0.368528
=== Actor Training Debug (Iteration 7635) ===
Q mean: -12.361503
Q std: 15.381423
Actor loss: 12.365463
Action reg: 0.003960
  l1.weight: grad_norm = 0.295814
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.241986
Total gradient norm: 0.598847
=== Actor Training Debug (Iteration 7636) ===
Q mean: -12.851621
Q std: 16.433558
Actor loss: 12.855566
Action reg: 0.003946
  l1.weight: grad_norm = 0.287959
  l1.bias: grad_norm = 0.000868
  l2.weight: grad_norm = 0.253334
Total gradient norm: 0.632239
=== Actor Training Debug (Iteration 7637) ===
Q mean: -14.844020
Q std: 18.099068
Actor loss: 14.847982
Action reg: 0.003962
  l1.weight: grad_norm = 0.417172
  l1.bias: grad_norm = 0.000410
  l2.weight: grad_norm = 0.371806
Total gradient norm: 0.966764
=== Actor Training Debug (Iteration 7638) ===
Q mean: -12.169265
Q std: 16.960203
Actor loss: 12.173237
Action reg: 0.003972
  l1.weight: grad_norm = 0.055216
  l1.bias: grad_norm = 0.000280
  l2.weight: grad_norm = 0.048370
Total gradient norm: 0.127994
=== Actor Training Debug (Iteration 7639) ===
Q mean: -11.387001
Q std: 14.769225
Actor loss: 11.390943
Action reg: 0.003942
  l1.weight: grad_norm = 0.295758
  l1.bias: grad_norm = 0.000634
  l2.weight: grad_norm = 0.254557
Total gradient norm: 0.711272
=== Actor Training Debug (Iteration 7640) ===
Q mean: -13.125134
Q std: 17.471746
Actor loss: 13.129087
Action reg: 0.003954
  l1.weight: grad_norm = 0.275431
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.226578
Total gradient norm: 0.627318
=== Actor Training Debug (Iteration 7641) ===
Q mean: -13.673159
Q std: 16.975107
Actor loss: 13.677127
Action reg: 0.003968
  l1.weight: grad_norm = 0.298055
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.255265
Total gradient norm: 0.682278
=== Actor Training Debug (Iteration 7642) ===
Q mean: -13.581717
Q std: 16.012064
Actor loss: 13.585672
Action reg: 0.003956
  l1.weight: grad_norm = 0.235181
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.200696
Total gradient norm: 0.539518
=== Actor Training Debug (Iteration 7643) ===
Q mean: -13.865490
Q std: 17.027121
Actor loss: 13.869443
Action reg: 0.003953
  l1.weight: grad_norm = 0.240713
  l1.bias: grad_norm = 0.000428
  l2.weight: grad_norm = 0.189896
Total gradient norm: 0.503365
=== Actor Training Debug (Iteration 7644) ===
Q mean: -11.828640
Q std: 14.398047
Actor loss: 11.832591
Action reg: 0.003951
  l1.weight: grad_norm = 0.305389
  l1.bias: grad_norm = 0.000556
  l2.weight: grad_norm = 0.328844
Total gradient norm: 0.907708
=== Actor Training Debug (Iteration 7645) ===
Q mean: -11.647118
Q std: 14.485045
Actor loss: 11.651073
Action reg: 0.003955
  l1.weight: grad_norm = 0.228314
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.198533
Total gradient norm: 0.522652
=== Actor Training Debug (Iteration 7646) ===
Q mean: -12.789326
Q std: 16.812969
Actor loss: 12.793299
Action reg: 0.003973
  l1.weight: grad_norm = 0.217234
  l1.bias: grad_norm = 0.000080
  l2.weight: grad_norm = 0.207009
Total gradient norm: 0.544709
=== Actor Training Debug (Iteration 7647) ===
Q mean: -12.567358
Q std: 16.732628
Actor loss: 12.571303
Action reg: 0.003946
  l1.weight: grad_norm = 0.298222
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.283622
Total gradient norm: 0.934122
=== Actor Training Debug (Iteration 7648) ===
Q mean: -12.893852
Q std: 16.578671
Actor loss: 12.897799
Action reg: 0.003947
  l1.weight: grad_norm = 0.283704
  l1.bias: grad_norm = 0.000238
  l2.weight: grad_norm = 0.241469
Total gradient norm: 0.672226
=== Actor Training Debug (Iteration 7649) ===
Q mean: -11.609070
Q std: 16.494608
Actor loss: 11.613028
Action reg: 0.003958
  l1.weight: grad_norm = 0.640036
  l1.bias: grad_norm = 0.000367
  l2.weight: grad_norm = 0.574902
Total gradient norm: 1.772714
=== Actor Training Debug (Iteration 7650) ===
Q mean: -13.589956
Q std: 17.632393
Actor loss: 13.593909
Action reg: 0.003953
  l1.weight: grad_norm = 0.325929
  l1.bias: grad_norm = 0.000573
  l2.weight: grad_norm = 0.297145
Total gradient norm: 0.750914
=== Actor Training Debug (Iteration 7651) ===
Q mean: -12.354738
Q std: 15.712392
Actor loss: 12.358714
Action reg: 0.003975
  l1.weight: grad_norm = 0.151546
  l1.bias: grad_norm = 0.000076
  l2.weight: grad_norm = 0.140090
Total gradient norm: 0.402859
=== Actor Training Debug (Iteration 7652) ===
Q mean: -12.819323
Q std: 16.915918
Actor loss: 12.823298
Action reg: 0.003975
  l1.weight: grad_norm = 0.131077
  l1.bias: grad_norm = 0.000053
  l2.weight: grad_norm = 0.109138
Total gradient norm: 0.296427
=== Actor Training Debug (Iteration 7653) ===
Q mean: -12.248457
Q std: 14.286088
Actor loss: 12.252407
Action reg: 0.003950
  l1.weight: grad_norm = 0.172626
  l1.bias: grad_norm = 0.002595
  l2.weight: grad_norm = 0.139362
Total gradient norm: 0.358220
=== Actor Training Debug (Iteration 7654) ===
Q mean: -10.761671
Q std: 12.973829
Actor loss: 10.765634
Action reg: 0.003963
  l1.weight: grad_norm = 0.396328
  l1.bias: grad_norm = 0.000146
  l2.weight: grad_norm = 0.322181
Total gradient norm: 0.835471
=== Actor Training Debug (Iteration 7655) ===
Q mean: -10.227901
Q std: 13.608067
Actor loss: 10.231863
Action reg: 0.003961
  l1.weight: grad_norm = 0.394430
  l1.bias: grad_norm = 0.001303
  l2.weight: grad_norm = 0.370464
Total gradient norm: 1.043960
=== Actor Training Debug (Iteration 7656) ===
Q mean: -12.313568
Q std: 14.623120
Actor loss: 12.317523
Action reg: 0.003955
  l1.weight: grad_norm = 0.253145
  l1.bias: grad_norm = 0.000853
  l2.weight: grad_norm = 0.193288
Total gradient norm: 0.464552
=== Actor Training Debug (Iteration 7657) ===
Q mean: -14.025240
Q std: 16.456812
Actor loss: 14.029195
Action reg: 0.003955
  l1.weight: grad_norm = 0.294851
  l1.bias: grad_norm = 0.000239
  l2.weight: grad_norm = 0.240941
Total gradient norm: 0.682287
=== Actor Training Debug (Iteration 7658) ===
Q mean: -12.471145
Q std: 15.653774
Actor loss: 12.475104
Action reg: 0.003960
  l1.weight: grad_norm = 0.259830
  l1.bias: grad_norm = 0.000986
  l2.weight: grad_norm = 0.239201
Total gradient norm: 0.597603
=== Actor Training Debug (Iteration 7659) ===
Q mean: -10.776212
Q std: 13.268673
Actor loss: 10.780171
Action reg: 0.003960
  l1.weight: grad_norm = 0.351293
  l1.bias: grad_norm = 0.000196
  l2.weight: grad_norm = 0.274350
Total gradient norm: 0.695387
=== Actor Training Debug (Iteration 7660) ===
Q mean: -13.280876
Q std: 18.388123
Actor loss: 13.284801
Action reg: 0.003926
  l1.weight: grad_norm = 0.359957
  l1.bias: grad_norm = 0.001874
  l2.weight: grad_norm = 0.330513
Total gradient norm: 0.886896
=== Actor Training Debug (Iteration 7661) ===
Q mean: -14.198676
Q std: 17.909428
Actor loss: 14.202637
Action reg: 0.003960
  l1.weight: grad_norm = 0.346931
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.280859
Total gradient norm: 0.819202
=== Actor Training Debug (Iteration 7662) ===
Q mean: -11.940361
Q std: 14.944029
Actor loss: 11.944327
Action reg: 0.003966
  l1.weight: grad_norm = 0.186259
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.147863
Total gradient norm: 0.414051
=== Actor Training Debug (Iteration 7663) ===
Q mean: -13.976137
Q std: 19.174139
Actor loss: 13.980112
Action reg: 0.003975
  l1.weight: grad_norm = 0.188979
  l1.bias: grad_norm = 0.000068
  l2.weight: grad_norm = 0.156855
Total gradient norm: 0.448392
=== Actor Training Debug (Iteration 7664) ===
Q mean: -12.698237
Q std: 16.648012
Actor loss: 12.702184
Action reg: 0.003946
  l1.weight: grad_norm = 0.308870
  l1.bias: grad_norm = 0.000731
  l2.weight: grad_norm = 0.239722
Total gradient norm: 0.590988
=== Actor Training Debug (Iteration 7665) ===
Q mean: -11.465590
Q std: 15.361894
Actor loss: 11.469538
Action reg: 0.003947
  l1.weight: grad_norm = 0.339247
  l1.bias: grad_norm = 0.001028
  l2.weight: grad_norm = 0.270270
Total gradient norm: 0.760774
=== Actor Training Debug (Iteration 7666) ===
Q mean: -11.216034
Q std: 13.245270
Actor loss: 11.219982
Action reg: 0.003948
  l1.weight: grad_norm = 0.325029
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.265911
Total gradient norm: 0.680399
=== Actor Training Debug (Iteration 7667) ===
Q mean: -12.185886
Q std: 12.910276
Actor loss: 12.189852
Action reg: 0.003965
  l1.weight: grad_norm = 0.193285
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.156130
Total gradient norm: 0.398971
=== Actor Training Debug (Iteration 7668) ===
Q mean: -12.191965
Q std: 15.602312
Actor loss: 12.195929
Action reg: 0.003963
  l1.weight: grad_norm = 0.436515
  l1.bias: grad_norm = 0.000437
  l2.weight: grad_norm = 0.350741
Total gradient norm: 0.987528
=== Actor Training Debug (Iteration 7669) ===
Q mean: -13.391711
Q std: 17.329731
Actor loss: 13.395678
Action reg: 0.003967
  l1.weight: grad_norm = 0.242017
  l1.bias: grad_norm = 0.000117
  l2.weight: grad_norm = 0.226727
Total gradient norm: 0.678463
=== Actor Training Debug (Iteration 7670) ===
Q mean: -13.683944
Q std: 15.652270
Actor loss: 13.687902
Action reg: 0.003958
  l1.weight: grad_norm = 0.358314
  l1.bias: grad_norm = 0.001449
  l2.weight: grad_norm = 0.292514
Total gradient norm: 0.852853
=== Actor Training Debug (Iteration 7671) ===
Q mean: -11.395861
Q std: 14.915148
Actor loss: 11.399822
Action reg: 0.003962
  l1.weight: grad_norm = 0.232355
  l1.bias: grad_norm = 0.000200
  l2.weight: grad_norm = 0.170133
Total gradient norm: 0.416612
=== Actor Training Debug (Iteration 7672) ===
Q mean: -13.058084
Q std: 16.447105
Actor loss: 13.062058
Action reg: 0.003975
  l1.weight: grad_norm = 0.159281
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.108607
Total gradient norm: 0.271280
=== Actor Training Debug (Iteration 7673) ===
Q mean: -14.489270
Q std: 17.552820
Actor loss: 14.493231
Action reg: 0.003961
  l1.weight: grad_norm = 0.117857
  l1.bias: grad_norm = 0.000194
  l2.weight: grad_norm = 0.110532
Total gradient norm: 0.267894
=== Actor Training Debug (Iteration 7674) ===
Q mean: -14.328190
Q std: 16.197348
Actor loss: 14.332153
Action reg: 0.003964
  l1.weight: grad_norm = 0.271028
  l1.bias: grad_norm = 0.000204
  l2.weight: grad_norm = 0.232066
Total gradient norm: 0.548471
=== Actor Training Debug (Iteration 7675) ===
Q mean: -11.885408
Q std: 14.473043
Actor loss: 11.889352
Action reg: 0.003944
  l1.weight: grad_norm = 0.278150
  l1.bias: grad_norm = 0.000341
  l2.weight: grad_norm = 0.259179
Total gradient norm: 0.638715
=== Actor Training Debug (Iteration 7676) ===
Q mean: -14.422833
Q std: 17.648514
Actor loss: 14.426803
Action reg: 0.003969
  l1.weight: grad_norm = 0.301419
  l1.bias: grad_norm = 0.000141
  l2.weight: grad_norm = 0.233854
Total gradient norm: 0.566438
=== Actor Training Debug (Iteration 7677) ===
Q mean: -13.607204
Q std: 17.641842
Actor loss: 13.611157
Action reg: 0.003953
  l1.weight: grad_norm = 0.304908
  l1.bias: grad_norm = 0.000584
  l2.weight: grad_norm = 0.259787
Total gradient norm: 0.734123
=== Actor Training Debug (Iteration 7678) ===
Q mean: -12.429211
Q std: 14.992887
Actor loss: 12.433155
Action reg: 0.003944
  l1.weight: grad_norm = 0.225853
  l1.bias: grad_norm = 0.000413
  l2.weight: grad_norm = 0.190805
Total gradient norm: 0.496483
=== Actor Training Debug (Iteration 7679) ===
Q mean: -12.301247
Q std: 16.668139
Actor loss: 12.305187
Action reg: 0.003940
  l1.weight: grad_norm = 0.176085
  l1.bias: grad_norm = 0.001584
  l2.weight: grad_norm = 0.149880
Total gradient norm: 0.416590
=== Actor Training Debug (Iteration 7680) ===
Q mean: -13.509708
Q std: 15.495636
Actor loss: 13.513665
Action reg: 0.003957
  l1.weight: grad_norm = 0.138216
  l1.bias: grad_norm = 0.001811
  l2.weight: grad_norm = 0.116730
Total gradient norm: 0.324605
=== Actor Training Debug (Iteration 7681) ===
Q mean: -13.180471
Q std: 15.790859
Actor loss: 13.184400
Action reg: 0.003928
  l1.weight: grad_norm = 0.312739
  l1.bias: grad_norm = 0.001196
  l2.weight: grad_norm = 0.267809
Total gradient norm: 0.630335
=== Actor Training Debug (Iteration 7682) ===
Q mean: -13.533390
Q std: 18.241484
Actor loss: 13.537353
Action reg: 0.003963
  l1.weight: grad_norm = 0.211009
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.184073
Total gradient norm: 0.470397
=== Actor Training Debug (Iteration 7683) ===
Q mean: -13.471928
Q std: 17.009640
Actor loss: 13.475884
Action reg: 0.003957
  l1.weight: grad_norm = 0.177670
  l1.bias: grad_norm = 0.000184
  l2.weight: grad_norm = 0.144402
Total gradient norm: 0.367175
=== Actor Training Debug (Iteration 7684) ===
Q mean: -12.273464
Q std: 15.755850
Actor loss: 12.277423
Action reg: 0.003959
  l1.weight: grad_norm = 0.238967
  l1.bias: grad_norm = 0.000288
  l2.weight: grad_norm = 0.226703
Total gradient norm: 0.619529
=== Actor Training Debug (Iteration 7685) ===
Q mean: -11.057252
Q std: 14.340994
Actor loss: 11.061214
Action reg: 0.003962
  l1.weight: grad_norm = 0.221335
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.166689
Total gradient norm: 0.432778
=== Actor Training Debug (Iteration 7686) ===
Q mean: -12.442728
Q std: 13.490554
Actor loss: 12.446690
Action reg: 0.003961
  l1.weight: grad_norm = 0.272310
  l1.bias: grad_norm = 0.000249
  l2.weight: grad_norm = 0.230113
Total gradient norm: 0.616241
=== Actor Training Debug (Iteration 7687) ===
Q mean: -12.568842
Q std: 16.423275
Actor loss: 12.572794
Action reg: 0.003952
  l1.weight: grad_norm = 0.209603
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.178901
Total gradient norm: 0.440113
=== Actor Training Debug (Iteration 7688) ===
Q mean: -13.412231
Q std: 16.141745
Actor loss: 13.416191
Action reg: 0.003959
  l1.weight: grad_norm = 0.367844
  l1.bias: grad_norm = 0.000349
  l2.weight: grad_norm = 0.304712
Total gradient norm: 0.745531
=== Actor Training Debug (Iteration 7689) ===
Q mean: -12.641388
Q std: 18.567619
Actor loss: 12.645348
Action reg: 0.003959
  l1.weight: grad_norm = 0.266084
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.213804
Total gradient norm: 0.546511
=== Actor Training Debug (Iteration 7690) ===
Q mean: -12.173685
Q std: 16.685852
Actor loss: 12.177629
Action reg: 0.003945
  l1.weight: grad_norm = 0.291008
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.277970
Total gradient norm: 0.712132
=== Actor Training Debug (Iteration 7691) ===
Q mean: -14.274841
Q std: 17.466137
Actor loss: 14.278811
Action reg: 0.003970
  l1.weight: grad_norm = 0.143164
  l1.bias: grad_norm = 0.000164
  l2.weight: grad_norm = 0.113783
Total gradient norm: 0.274303
=== Actor Training Debug (Iteration 7692) ===
Q mean: -13.233536
Q std: 17.356346
Actor loss: 13.237480
Action reg: 0.003944
  l1.weight: grad_norm = 0.387154
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.340871
Total gradient norm: 1.080578
=== Actor Training Debug (Iteration 7693) ===
Q mean: -12.965178
Q std: 15.592162
Actor loss: 12.969137
Action reg: 0.003960
  l1.weight: grad_norm = 0.396444
  l1.bias: grad_norm = 0.000390
  l2.weight: grad_norm = 0.325893
Total gradient norm: 0.860833
=== Actor Training Debug (Iteration 7694) ===
Q mean: -15.675957
Q std: 18.856623
Actor loss: 15.679885
Action reg: 0.003928
  l1.weight: grad_norm = 0.461735
  l1.bias: grad_norm = 0.001229
  l2.weight: grad_norm = 0.327024
Total gradient norm: 0.895441
=== Actor Training Debug (Iteration 7695) ===
Q mean: -12.904533
Q std: 16.095203
Actor loss: 12.908452
Action reg: 0.003919
  l1.weight: grad_norm = 0.327274
  l1.bias: grad_norm = 0.001026
  l2.weight: grad_norm = 0.275487
Total gradient norm: 0.700817
=== Actor Training Debug (Iteration 7696) ===
Q mean: -12.918535
Q std: 16.982605
Actor loss: 12.922498
Action reg: 0.003962
  l1.weight: grad_norm = 0.566010
  l1.bias: grad_norm = 0.000236
  l2.weight: grad_norm = 0.409394
Total gradient norm: 0.994958
=== Actor Training Debug (Iteration 7697) ===
Q mean: -13.701675
Q std: 15.512547
Actor loss: 13.705628
Action reg: 0.003953
  l1.weight: grad_norm = 0.293572
  l1.bias: grad_norm = 0.000334
  l2.weight: grad_norm = 0.255213
Total gradient norm: 0.740765
=== Actor Training Debug (Iteration 7698) ===
Q mean: -12.947402
Q std: 16.116808
Actor loss: 12.951337
Action reg: 0.003935
  l1.weight: grad_norm = 0.179986
  l1.bias: grad_norm = 0.001530
  l2.weight: grad_norm = 0.137784
Total gradient norm: 0.391830
=== Actor Training Debug (Iteration 7699) ===
Q mean: -13.343800
Q std: 16.926489
Actor loss: 13.347751
Action reg: 0.003951
  l1.weight: grad_norm = 0.274525
  l1.bias: grad_norm = 0.000536
  l2.weight: grad_norm = 0.220289
Total gradient norm: 0.596875
=== Actor Training Debug (Iteration 7700) ===
Q mean: -10.621201
Q std: 14.617009
Actor loss: 10.625161
Action reg: 0.003961
  l1.weight: grad_norm = 0.196976
  l1.bias: grad_norm = 0.000297
  l2.weight: grad_norm = 0.174931
Total gradient norm: 0.477871
=== Actor Training Debug (Iteration 7701) ===
Q mean: -11.743340
Q std: 15.478904
Actor loss: 11.747295
Action reg: 0.003956
  l1.weight: grad_norm = 0.402444
  l1.bias: grad_norm = 0.000361
  l2.weight: grad_norm = 0.338072
Total gradient norm: 0.871387
=== Actor Training Debug (Iteration 7702) ===
Q mean: -13.595525
Q std: 15.926137
Actor loss: 13.599475
Action reg: 0.003950
  l1.weight: grad_norm = 0.217953
  l1.bias: grad_norm = 0.000529
  l2.weight: grad_norm = 0.191615
Total gradient norm: 0.453510
=== Actor Training Debug (Iteration 7703) ===
Q mean: -12.431231
Q std: 14.422014
Actor loss: 12.435146
Action reg: 0.003916
  l1.weight: grad_norm = 0.336395
  l1.bias: grad_norm = 0.001498
  l2.weight: grad_norm = 0.281202
Total gradient norm: 0.736882
=== Actor Training Debug (Iteration 7704) ===
Q mean: -9.814486
Q std: 11.614081
Actor loss: 9.818453
Action reg: 0.003968
  l1.weight: grad_norm = 0.208435
  l1.bias: grad_norm = 0.000331
  l2.weight: grad_norm = 0.171367
Total gradient norm: 0.461850
=== Actor Training Debug (Iteration 7705) ===
Q mean: -11.274391
Q std: 14.397497
Actor loss: 11.278358
Action reg: 0.003967
  l1.weight: grad_norm = 0.377901
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.308095
Total gradient norm: 0.675247
=== Actor Training Debug (Iteration 7706) ===
Q mean: -13.226120
Q std: 16.212214
Actor loss: 13.230051
Action reg: 0.003931
  l1.weight: grad_norm = 0.957322
  l1.bias: grad_norm = 0.001182
  l2.weight: grad_norm = 0.849279
Total gradient norm: 2.031105
=== Actor Training Debug (Iteration 7707) ===
Q mean: -13.709127
Q std: 16.121071
Actor loss: 13.713080
Action reg: 0.003953
  l1.weight: grad_norm = 0.167017
  l1.bias: grad_norm = 0.002313
  l2.weight: grad_norm = 0.155738
Total gradient norm: 0.390509
=== Actor Training Debug (Iteration 7708) ===
Q mean: -12.454976
Q std: 16.177370
Actor loss: 12.458909
Action reg: 0.003933
  l1.weight: grad_norm = 0.222750
  l1.bias: grad_norm = 0.001362
  l2.weight: grad_norm = 0.178581
Total gradient norm: 0.460885
=== Actor Training Debug (Iteration 7709) ===
Q mean: -13.016594
Q std: 15.836432
Actor loss: 13.020542
Action reg: 0.003948
  l1.weight: grad_norm = 0.327143
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.261086
Total gradient norm: 0.741236
=== Actor Training Debug (Iteration 7710) ===
Q mean: -11.976007
Q std: 14.840757
Actor loss: 11.979951
Action reg: 0.003943
  l1.weight: grad_norm = 0.438845
  l1.bias: grad_norm = 0.000852
  l2.weight: grad_norm = 0.380199
Total gradient norm: 1.148655
=== Actor Training Debug (Iteration 7711) ===
Q mean: -12.451663
Q std: 15.561849
Actor loss: 12.455612
Action reg: 0.003950
  l1.weight: grad_norm = 0.269802
  l1.bias: grad_norm = 0.000384
  l2.weight: grad_norm = 0.232436
Total gradient norm: 0.600107
=== Actor Training Debug (Iteration 7712) ===
Q mean: -12.771708
Q std: 16.899803
Actor loss: 12.775674
Action reg: 0.003966
  l1.weight: grad_norm = 0.181247
  l1.bias: grad_norm = 0.000841
  l2.weight: grad_norm = 0.146388
Total gradient norm: 0.393547
=== Actor Training Debug (Iteration 7713) ===
Q mean: -12.805762
Q std: 16.547245
Actor loss: 12.809710
Action reg: 0.003947
  l1.weight: grad_norm = 0.214064
  l1.bias: grad_norm = 0.000370
  l2.weight: grad_norm = 0.193570
Total gradient norm: 0.494308
=== Actor Training Debug (Iteration 7714) ===
Q mean: -15.674471
Q std: 18.220808
Actor loss: 15.678432
Action reg: 0.003962
  l1.weight: grad_norm = 0.241199
  l1.bias: grad_norm = 0.000174
  l2.weight: grad_norm = 0.236157
Total gradient norm: 0.635366
=== Actor Training Debug (Iteration 7715) ===
Q mean: -13.741158
Q std: 17.187231
Actor loss: 13.745115
Action reg: 0.003957
  l1.weight: grad_norm = 0.227226
  l1.bias: grad_norm = 0.000155
  l2.weight: grad_norm = 0.184787
Total gradient norm: 0.549972
=== Actor Training Debug (Iteration 7716) ===
Q mean: -12.229825
Q std: 15.572556
Actor loss: 12.233756
Action reg: 0.003931
  l1.weight: grad_norm = 1.511627
  l1.bias: grad_norm = 0.000822
  l2.weight: grad_norm = 1.209433
Total gradient norm: 3.711376
=== Actor Training Debug (Iteration 7717) ===
Q mean: -10.472006
Q std: 14.483968
Actor loss: 10.475952
Action reg: 0.003946
  l1.weight: grad_norm = 0.377071
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.308741
Total gradient norm: 0.840956
=== Actor Training Debug (Iteration 7718) ===
Q mean: -11.571853
Q std: 14.157659
Actor loss: 11.575798
Action reg: 0.003945
  l1.weight: grad_norm = 0.255738
  l1.bias: grad_norm = 0.000181
  l2.weight: grad_norm = 0.218816
Total gradient norm: 0.631073
=== Actor Training Debug (Iteration 7719) ===
Q mean: -13.544949
Q std: 17.016289
Actor loss: 13.548917
Action reg: 0.003968
  l1.weight: grad_norm = 0.316348
  l1.bias: grad_norm = 0.001169
  l2.weight: grad_norm = 0.291859
Total gradient norm: 0.759507
=== Actor Training Debug (Iteration 7720) ===
Q mean: -13.046526
Q std: 16.254301
Actor loss: 13.050475
Action reg: 0.003949
  l1.weight: grad_norm = 0.289532
  l1.bias: grad_norm = 0.000140
  l2.weight: grad_norm = 0.240930
Total gradient norm: 0.553252
=== Actor Training Debug (Iteration 7721) ===
Q mean: -12.260963
Q std: 14.865819
Actor loss: 12.264903
Action reg: 0.003940
  l1.weight: grad_norm = 0.195591
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.166876
Total gradient norm: 0.453030
=== Actor Training Debug (Iteration 7722) ===
Q mean: -15.694180
Q std: 18.941435
Actor loss: 15.698131
Action reg: 0.003951
  l1.weight: grad_norm = 0.405188
  l1.bias: grad_norm = 0.000500
  l2.weight: grad_norm = 0.323258
Total gradient norm: 0.868812
=== Actor Training Debug (Iteration 7723) ===
Q mean: -12.987297
Q std: 15.483036
Actor loss: 12.991227
Action reg: 0.003930
  l1.weight: grad_norm = 0.486242
  l1.bias: grad_norm = 0.000486
  l2.weight: grad_norm = 0.423470
Total gradient norm: 1.299881
=== Actor Training Debug (Iteration 7724) ===
Q mean: -11.964500
Q std: 15.588359
Actor loss: 11.968452
Action reg: 0.003952
  l1.weight: grad_norm = 0.211859
  l1.bias: grad_norm = 0.000224
  l2.weight: grad_norm = 0.182553
Total gradient norm: 0.501413
=== Actor Training Debug (Iteration 7725) ===
Q mean: -12.169560
Q std: 16.813801
Actor loss: 12.173532
Action reg: 0.003972
  l1.weight: grad_norm = 0.163445
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.135739
Total gradient norm: 0.385360
=== Actor Training Debug (Iteration 7726) ===
Q mean: -14.180751
Q std: 18.774267
Actor loss: 14.184717
Action reg: 0.003966
  l1.weight: grad_norm = 0.323080
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.274829
Total gradient norm: 0.747982
=== Actor Training Debug (Iteration 7727) ===
Q mean: -13.524897
Q std: 18.069803
Actor loss: 13.528839
Action reg: 0.003943
  l1.weight: grad_norm = 0.359678
  l1.bias: grad_norm = 0.000229
  l2.weight: grad_norm = 0.326170
Total gradient norm: 0.793572
=== Actor Training Debug (Iteration 7728) ===
Q mean: -12.565580
Q std: 17.674627
Actor loss: 12.569520
Action reg: 0.003940
  l1.weight: grad_norm = 0.354490
  l1.bias: grad_norm = 0.000473
  l2.weight: grad_norm = 0.283352
Total gradient norm: 0.717934
=== Actor Training Debug (Iteration 7729) ===
Q mean: -11.034111
Q std: 14.439421
Actor loss: 11.038072
Action reg: 0.003961
  l1.weight: grad_norm = 0.191125
  l1.bias: grad_norm = 0.000235
  l2.weight: grad_norm = 0.172565
Total gradient norm: 0.476043
=== Actor Training Debug (Iteration 7730) ===
Q mean: -15.322123
Q std: 19.038094
Actor loss: 15.326092
Action reg: 0.003970
  l1.weight: grad_norm = 0.235576
  l1.bias: grad_norm = 0.000070
  l2.weight: grad_norm = 0.219802
Total gradient norm: 0.563246
=== Actor Training Debug (Iteration 7731) ===
Q mean: -12.517121
Q std: 16.555170
Actor loss: 12.521067
Action reg: 0.003945
  l1.weight: grad_norm = 0.354145
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.312430
Total gradient norm: 0.941964
=== Actor Training Debug (Iteration 7732) ===
Q mean: -13.830714
Q std: 16.578121
Actor loss: 13.834677
Action reg: 0.003963
  l1.weight: grad_norm = 0.343520
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.261593
Total gradient norm: 0.654191
=== Actor Training Debug (Iteration 7733) ===
Q mean: -14.193127
Q std: 16.365757
Actor loss: 14.197083
Action reg: 0.003956
  l1.weight: grad_norm = 0.295149
  l1.bias: grad_norm = 0.000228
  l2.weight: grad_norm = 0.250297
Total gradient norm: 0.616639
=== Actor Training Debug (Iteration 7734) ===
Q mean: -11.849634
Q std: 14.025507
Actor loss: 11.853596
Action reg: 0.003961
  l1.weight: grad_norm = 0.423013
  l1.bias: grad_norm = 0.000169
  l2.weight: grad_norm = 0.356275
Total gradient norm: 0.933196
=== Actor Training Debug (Iteration 7735) ===
Q mean: -13.857793
Q std: 18.501787
Actor loss: 13.861747
Action reg: 0.003954
  l1.weight: grad_norm = 0.216493
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.200234
Total gradient norm: 0.569583
=== Actor Training Debug (Iteration 7736) ===
Q mean: -11.693173
Q std: 15.647668
Actor loss: 11.697124
Action reg: 0.003950
  l1.weight: grad_norm = 0.374314
  l1.bias: grad_norm = 0.000199
  l2.weight: grad_norm = 0.308774
Total gradient norm: 0.835363
=== Actor Training Debug (Iteration 7737) ===
Q mean: -13.139585
Q std: 17.755516
Actor loss: 13.143550
Action reg: 0.003965
  l1.weight: grad_norm = 0.296485
  l1.bias: grad_norm = 0.000180
  l2.weight: grad_norm = 0.210172
Total gradient norm: 0.542459
=== Actor Training Debug (Iteration 7738) ===
Q mean: -12.729845
Q std: 16.344278
Actor loss: 12.733802
Action reg: 0.003957
  l1.weight: grad_norm = 1.575201
  l1.bias: grad_norm = 0.000941
  l2.weight: grad_norm = 1.313090
Total gradient norm: 3.608731
=== Actor Training Debug (Iteration 7739) ===
Q mean: -14.211605
Q std: 17.467690
Actor loss: 14.215526
Action reg: 0.003920
  l1.weight: grad_norm = 0.301732
  l1.bias: grad_norm = 0.000711
  l2.weight: grad_norm = 0.264923
Total gradient norm: 0.687013
=== Actor Training Debug (Iteration 7740) ===
Q mean: -13.442536
Q std: 17.281559
Actor loss: 13.446471
Action reg: 0.003935
  l1.weight: grad_norm = 0.471700
  l1.bias: grad_norm = 0.000305
  l2.weight: grad_norm = 0.348731
Total gradient norm: 1.027668
=== Actor Training Debug (Iteration 7741) ===
Q mean: -10.780220
Q std: 12.606270
Actor loss: 10.784174
Action reg: 0.003954
  l1.weight: grad_norm = 0.274240
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.233935
Total gradient norm: 0.611586
=== Actor Training Debug (Iteration 7742) ===
Q mean: -13.124378
Q std: 16.934324
Actor loss: 13.128315
Action reg: 0.003937
  l1.weight: grad_norm = 0.432467
  l1.bias: grad_norm = 0.000443
  l2.weight: grad_norm = 0.391205
Total gradient norm: 1.028265
=== Actor Training Debug (Iteration 7743) ===
Q mean: -13.412719
Q std: 15.183032
Actor loss: 13.416675
Action reg: 0.003956
  l1.weight: grad_norm = 0.335214
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.317321
Total gradient norm: 0.767261
=== Actor Training Debug (Iteration 7744) ===
Q mean: -13.449329
Q std: 16.167034
Actor loss: 13.453286
Action reg: 0.003957
  l1.weight: grad_norm = 0.282409
  l1.bias: grad_norm = 0.000122
  l2.weight: grad_norm = 0.272889
Total gradient norm: 0.941966
=== Actor Training Debug (Iteration 7745) ===
Q mean: -14.145151
Q std: 19.004728
Actor loss: 14.149093
Action reg: 0.003941
  l1.weight: grad_norm = 0.341713
  l1.bias: grad_norm = 0.000198
  l2.weight: grad_norm = 0.281018
Total gradient norm: 0.655466
=== Actor Training Debug (Iteration 7746) ===
Q mean: -11.674109
Q std: 16.461000
Actor loss: 11.678071
Action reg: 0.003963
  l1.weight: grad_norm = 0.306094
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.238060
Total gradient norm: 0.639280
=== Actor Training Debug (Iteration 7747) ===
Q mean: -11.663654
Q std: 15.553977
Actor loss: 11.667591
Action reg: 0.003937
  l1.weight: grad_norm = 0.291074
  l1.bias: grad_norm = 0.000939
  l2.weight: grad_norm = 0.224256
Total gradient norm: 0.598733
=== Actor Training Debug (Iteration 7748) ===
Q mean: -12.791886
Q std: 15.638221
Actor loss: 12.795857
Action reg: 0.003971
  l1.weight: grad_norm = 0.289732
  l1.bias: grad_norm = 0.000136
  l2.weight: grad_norm = 0.216904
Total gradient norm: 0.573036
=== Actor Training Debug (Iteration 7749) ===
Q mean: -9.737242
Q std: 12.926330
Actor loss: 9.741187
Action reg: 0.003945
  l1.weight: grad_norm = 0.222810
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.186634
Total gradient norm: 0.502527
=== Actor Training Debug (Iteration 7750) ===
Q mean: -13.726964
Q std: 16.537024
Actor loss: 13.730906
Action reg: 0.003943
  l1.weight: grad_norm = 0.173467
  l1.bias: grad_norm = 0.000429
  l2.weight: grad_norm = 0.159071
Total gradient norm: 0.485245
=== Actor Training Debug (Iteration 7751) ===
Q mean: -13.777519
Q std: 16.469856
Actor loss: 13.781471
Action reg: 0.003952
  l1.weight: grad_norm = 0.423822
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.350109
Total gradient norm: 0.892706
=== Actor Training Debug (Iteration 7752) ===
Q mean: -11.588469
Q std: 15.718562
Actor loss: 11.592429
Action reg: 0.003961
  l1.weight: grad_norm = 0.296463
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.252002
Total gradient norm: 0.591000
=== Actor Training Debug (Iteration 7753) ===
Q mean: -12.270825
Q std: 14.841472
Actor loss: 12.274778
Action reg: 0.003953
  l1.weight: grad_norm = 0.297162
  l1.bias: grad_norm = 0.000322
  l2.weight: grad_norm = 0.247593
Total gradient norm: 0.616437
=== Actor Training Debug (Iteration 7754) ===
Q mean: -12.661453
Q std: 16.080746
Actor loss: 12.665429
Action reg: 0.003976
  l1.weight: grad_norm = 0.183576
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.151683
Total gradient norm: 0.393035
=== Actor Training Debug (Iteration 7755) ===
Q mean: -13.983562
Q std: 18.010202
Actor loss: 13.987519
Action reg: 0.003957
  l1.weight: grad_norm = 0.180341
  l1.bias: grad_norm = 0.000300
  l2.weight: grad_norm = 0.141921
Total gradient norm: 0.378285
=== Actor Training Debug (Iteration 7756) ===
Q mean: -11.981914
Q std: 15.663149
Actor loss: 11.985858
Action reg: 0.003945
  l1.weight: grad_norm = 0.277968
  l1.bias: grad_norm = 0.000245
  l2.weight: grad_norm = 0.220954
Total gradient norm: 0.658139
=== Actor Training Debug (Iteration 7757) ===
Q mean: -13.761757
Q std: 17.213570
Actor loss: 13.765700
Action reg: 0.003944
  l1.weight: grad_norm = 0.274825
  l1.bias: grad_norm = 0.000332
  l2.weight: grad_norm = 0.251164
Total gradient norm: 0.675762
=== Actor Training Debug (Iteration 7758) ===
Q mean: -11.749262
Q std: 15.152452
Actor loss: 11.753214
Action reg: 0.003952
  l1.weight: grad_norm = 0.144233
  l1.bias: grad_norm = 0.000508
  l2.weight: grad_norm = 0.139956
Total gradient norm: 0.341098
=== Actor Training Debug (Iteration 7759) ===
Q mean: -12.335281
Q std: 17.035723
Actor loss: 12.339244
Action reg: 0.003962
  l1.weight: grad_norm = 0.236050
  l1.bias: grad_norm = 0.000315
  l2.weight: grad_norm = 0.219726
Total gradient norm: 0.539185
=== Actor Training Debug (Iteration 7760) ===
Q mean: -13.792496
Q std: 16.630281
Actor loss: 13.796471
Action reg: 0.003975
  l1.weight: grad_norm = 0.361937
  l1.bias: grad_norm = 0.000177
  l2.weight: grad_norm = 0.346992
Total gradient norm: 1.118520
=== Actor Training Debug (Iteration 7761) ===
Q mean: -12.110263
Q std: 15.373075
Actor loss: 12.114219
Action reg: 0.003956
  l1.weight: grad_norm = 0.284598
  l1.bias: grad_norm = 0.000132
  l2.weight: grad_norm = 0.250133
Total gradient norm: 0.622565
=== Actor Training Debug (Iteration 7762) ===
Q mean: -12.658521
Q std: 15.126042
Actor loss: 12.662481
Action reg: 0.003960
  l1.weight: grad_norm = 0.218800
  l1.bias: grad_norm = 0.000134
  l2.weight: grad_norm = 0.187259
Total gradient norm: 0.541543
=== Actor Training Debug (Iteration 7763) ===
Q mean: -10.862904
Q std: 15.100183
Actor loss: 10.866845
Action reg: 0.003942
  l1.weight: grad_norm = 0.330417
  l1.bias: grad_norm = 0.000387
  l2.weight: grad_norm = 0.296851
Total gradient norm: 0.754666
=== Actor Training Debug (Iteration 7764) ===
Q mean: -12.229021
Q std: 15.106992
Actor loss: 12.232973
Action reg: 0.003952
  l1.weight: grad_norm = 0.374807
  l1.bias: grad_norm = 0.000234
  l2.weight: grad_norm = 0.312343
Total gradient norm: 0.810460
=== Actor Training Debug (Iteration 7765) ===
Q mean: -13.154988
Q std: 14.232158
Actor loss: 13.158948
Action reg: 0.003959
  l1.weight: grad_norm = 0.202537
  l1.bias: grad_norm = 0.000588
  l2.weight: grad_norm = 0.173572
Total gradient norm: 0.439332
=== Actor Training Debug (Iteration 7766) ===
Q mean: -11.301202
Q std: 13.474494
Actor loss: 11.305153
Action reg: 0.003951
  l1.weight: grad_norm = 0.457626
  l1.bias: grad_norm = 0.001015
  l2.weight: grad_norm = 0.361081
Total gradient norm: 0.990224
=== Actor Training Debug (Iteration 7767) ===
Q mean: -12.110921
Q std: 13.670833
Actor loss: 12.114860
Action reg: 0.003939
  l1.weight: grad_norm = 0.204104
  l1.bias: grad_norm = 0.000447
  l2.weight: grad_norm = 0.179194
Total gradient norm: 0.453733
=== Actor Training Debug (Iteration 7768) ===
Q mean: -12.593872
Q std: 14.989953
Actor loss: 12.597816
Action reg: 0.003945
  l1.weight: grad_norm = 0.463275
  l1.bias: grad_norm = 0.000474
  l2.weight: grad_norm = 0.359752
Total gradient norm: 0.964359
=== Actor Training Debug (Iteration 7769) ===
Q mean: -13.493847
Q std: 16.781338
Actor loss: 13.497794
Action reg: 0.003947
  l1.weight: grad_norm = 0.417094
  l1.bias: grad_norm = 0.000270
  l2.weight: grad_norm = 0.322937
Total gradient norm: 0.734868
=== Actor Training Debug (Iteration 7770) ===
Q mean: -12.582481
Q std: 15.807888
Actor loss: 12.586405
Action reg: 0.003924
  l1.weight: grad_norm = 0.296754
  l1.bias: grad_norm = 0.001516
  l2.weight: grad_norm = 0.253547
Total gradient norm: 0.694298
=== Actor Training Debug (Iteration 7771) ===
Q mean: -13.004795
Q std: 15.573118
Actor loss: 13.008729
Action reg: 0.003934
  l1.weight: grad_norm = 0.228692
  l1.bias: grad_norm = 0.001315
  l2.weight: grad_norm = 0.196515
Total gradient norm: 0.586045
=== Actor Training Debug (Iteration 7772) ===
Q mean: -13.535479
Q std: 16.516703
Actor loss: 13.539432
Action reg: 0.003953
  l1.weight: grad_norm = 0.274546
  l1.bias: grad_norm = 0.000353
  l2.weight: grad_norm = 0.228293
Total gradient norm: 0.632646
=== Actor Training Debug (Iteration 7773) ===
Q mean: -15.467219
Q std: 17.278553
Actor loss: 15.471166
Action reg: 0.003946
  l1.weight: grad_norm = 0.226382
  l1.bias: grad_norm = 0.000547
  l2.weight: grad_norm = 0.188847
Total gradient norm: 0.514010
=== Actor Training Debug (Iteration 7774) ===
Q mean: -12.435547
Q std: 15.936379
Actor loss: 12.439514
Action reg: 0.003967
  l1.weight: grad_norm = 0.441936
  l1.bias: grad_norm = 0.000149
  l2.weight: grad_norm = 0.387479
Total gradient norm: 0.972165
=== Actor Training Debug (Iteration 7775) ===
Q mean: -13.792291
Q std: 17.094170
Actor loss: 13.796239
Action reg: 0.003948
  l1.weight: grad_norm = 0.304487
  l1.bias: grad_norm = 0.000325
  l2.weight: grad_norm = 0.256321
Total gradient norm: 0.665548
=== Actor Training Debug (Iteration 7776) ===
Q mean: -11.981647
Q std: 14.693219
Actor loss: 11.985610
Action reg: 0.003963
  l1.weight: grad_norm = 0.158571
  l1.bias: grad_norm = 0.000096
  l2.weight: grad_norm = 0.145697
Total gradient norm: 0.427345
=== Actor Training Debug (Iteration 7777) ===
Q mean: -11.565195
Q std: 13.798361
Actor loss: 11.569155
Action reg: 0.003960
  l1.weight: grad_norm = 0.215326
  l1.bias: grad_norm = 0.001048
  l2.weight: grad_norm = 0.199953
Total gradient norm: 0.557058
=== Actor Training Debug (Iteration 7778) ===
Q mean: -11.162014
Q std: 13.067951
Actor loss: 11.165977
Action reg: 0.003963
  l1.weight: grad_norm = 0.222602
  l1.bias: grad_norm = 0.000103
  l2.weight: grad_norm = 0.207584
Total gradient norm: 0.530937
=== Actor Training Debug (Iteration 7779) ===
Q mean: -11.839574
Q std: 15.740804
Actor loss: 11.843530
Action reg: 0.003956
  l1.weight: grad_norm = 0.220060
  l1.bias: grad_norm = 0.000310
  l2.weight: grad_norm = 0.190317
Total gradient norm: 0.533005
=== Actor Training Debug (Iteration 7780) ===
Q mean: -13.375371
Q std: 16.695160
Actor loss: 13.379323
Action reg: 0.003952
  l1.weight: grad_norm = 0.407101
  l1.bias: grad_norm = 0.000479
  l2.weight: grad_norm = 0.327962
Total gradient norm: 0.816988
=== Actor Training Debug (Iteration 7781) ===
Q mean: -13.289771
Q std: 17.323479
Actor loss: 13.293732
Action reg: 0.003961
  l1.weight: grad_norm = 0.482445
  l1.bias: grad_norm = 0.000585
  l2.weight: grad_norm = 0.405617
Total gradient norm: 1.160865
=== Actor Training Debug (Iteration 7782) ===
Q mean: -12.928524
Q std: 15.796235
Actor loss: 12.932461
Action reg: 0.003936
  l1.weight: grad_norm = 0.208751
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.179464
Total gradient norm: 0.461714
=== Actor Training Debug (Iteration 7783) ===
Q mean: -12.227779
Q std: 13.631069
Actor loss: 12.231743
Action reg: 0.003963
  l1.weight: grad_norm = 0.217569
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.177975
Total gradient norm: 0.500163
=== Actor Training Debug (Iteration 7784) ===
Q mean: -11.675273
Q std: 16.198776
Actor loss: 11.679217
Action reg: 0.003944
  l1.weight: grad_norm = 0.448963
  l1.bias: grad_norm = 0.000343
  l2.weight: grad_norm = 0.333129
Total gradient norm: 0.823700
=== Actor Training Debug (Iteration 7785) ===
Q mean: -11.139640
Q std: 16.247351
Actor loss: 11.143604
Action reg: 0.003965
  l1.weight: grad_norm = 0.332748
  l1.bias: grad_norm = 0.000213
  l2.weight: grad_norm = 0.300550
Total gradient norm: 0.733322
=== Actor Training Debug (Iteration 7786) ===
Q mean: -13.460923
Q std: 16.659019
Actor loss: 13.464886
Action reg: 0.003962
  l1.weight: grad_norm = 0.490692
  l1.bias: grad_norm = 0.000311
  l2.weight: grad_norm = 0.401173
Total gradient norm: 1.072149
=== Actor Training Debug (Iteration 7787) ===
Q mean: -12.659691
Q std: 15.835370
Actor loss: 12.663636
Action reg: 0.003945
  l1.weight: grad_norm = 0.245039
  l1.bias: grad_norm = 0.001292
  l2.weight: grad_norm = 0.213559
Total gradient norm: 0.597222
=== Actor Training Debug (Iteration 7788) ===
Q mean: -10.803715
Q std: 13.536236
Actor loss: 10.807678
Action reg: 0.003963
  l1.weight: grad_norm = 0.255569
  l1.bias: grad_norm = 0.000899
  l2.weight: grad_norm = 0.216576
Total gradient norm: 0.517639
=== Actor Training Debug (Iteration 7789) ===
Q mean: -11.955214
Q std: 15.042620
Actor loss: 11.959156
Action reg: 0.003942
  l1.weight: grad_norm = 0.238545
  l1.bias: grad_norm = 0.001408
  l2.weight: grad_norm = 0.195665
Total gradient norm: 0.460011
=== Actor Training Debug (Iteration 7790) ===
Q mean: -11.891813
Q std: 16.006266
Actor loss: 11.895766
Action reg: 0.003953
  l1.weight: grad_norm = 0.283888
  l1.bias: grad_norm = 0.000489
  l2.weight: grad_norm = 0.255700
Total gradient norm: 0.654971
=== Actor Training Debug (Iteration 7791) ===
Q mean: -12.402950
Q std: 15.911867
Actor loss: 12.406902
Action reg: 0.003952
  l1.weight: grad_norm = 0.250039
  l1.bias: grad_norm = 0.000753
  l2.weight: grad_norm = 0.209694
Total gradient norm: 0.519546
=== Actor Training Debug (Iteration 7792) ===
Q mean: -13.077557
Q std: 16.273590
Actor loss: 13.081502
Action reg: 0.003945
  l1.weight: grad_norm = 0.414979
  l1.bias: grad_norm = 0.000513
  l2.weight: grad_norm = 0.293188
Total gradient norm: 0.783374
=== Actor Training Debug (Iteration 7793) ===
Q mean: -11.523376
Q std: 14.364276
Actor loss: 11.527330
Action reg: 0.003955
  l1.weight: grad_norm = 0.180888
  l1.bias: grad_norm = 0.000414
  l2.weight: grad_norm = 0.158252
Total gradient norm: 0.471049
=== Actor Training Debug (Iteration 7794) ===
Q mean: -13.705227
Q std: 17.001951
Actor loss: 13.709188
Action reg: 0.003961
  l1.weight: grad_norm = 0.278001
  l1.bias: grad_norm = 0.000342
  l2.weight: grad_norm = 0.212218
Total gradient norm: 0.564598
=== Actor Training Debug (Iteration 7795) ===
Q mean: -12.103563
Q std: 14.756336
Actor loss: 12.107543
Action reg: 0.003980
  l1.weight: grad_norm = 0.124517
  l1.bias: grad_norm = 0.000048
  l2.weight: grad_norm = 0.110256
Total gradient norm: 0.275971
=== Actor Training Debug (Iteration 7796) ===
Q mean: -12.190443
Q std: 15.771112
Actor loss: 12.194396
Action reg: 0.003953
  l1.weight: grad_norm = 0.270792
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.212833
Total gradient norm: 0.537487
=== Actor Training Debug (Iteration 7797) ===
Q mean: -12.307846
Q std: 15.906934
Actor loss: 12.311813
Action reg: 0.003968
  l1.weight: grad_norm = 0.380360
  l1.bias: grad_norm = 0.000179
  l2.weight: grad_norm = 0.294431
Total gradient norm: 0.752360
=== Actor Training Debug (Iteration 7798) ===
Q mean: -12.268415
Q std: 14.136267
Actor loss: 12.272360
Action reg: 0.003944
  l1.weight: grad_norm = 0.270721
  l1.bias: grad_norm = 0.000587
  l2.weight: grad_norm = 0.217064
Total gradient norm: 0.598471
=== Actor Training Debug (Iteration 7799) ===
Q mean: -12.520554
Q std: 15.463891
Actor loss: 12.524514
Action reg: 0.003961
  l1.weight: grad_norm = 0.196543
  l1.bias: grad_norm = 0.000215
  l2.weight: grad_norm = 0.172432
Total gradient norm: 0.461330
=== Actor Training Debug (Iteration 7800) ===
Q mean: -13.032663
Q std: 16.001671
Actor loss: 13.036615
Action reg: 0.003952
  l1.weight: grad_norm = 0.254183
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.222025
Total gradient norm: 0.552381
=== Actor Training Debug (Iteration 7801) ===
Q mean: -12.175388
Q std: 15.349710
Actor loss: 12.179340
Action reg: 0.003952
  l1.weight: grad_norm = 0.269524
  l1.bias: grad_norm = 0.000506
  l2.weight: grad_norm = 0.220285
Total gradient norm: 0.547574
=== Actor Training Debug (Iteration 7802) ===
Q mean: -13.857801
Q std: 17.718267
Actor loss: 13.861749
Action reg: 0.003947
  l1.weight: grad_norm = 0.637550
  l1.bias: grad_norm = 0.000937
  l2.weight: grad_norm = 0.499697
Total gradient norm: 1.154385
=== Actor Training Debug (Iteration 7803) ===
Q mean: -12.143530
Q std: 16.603132
Actor loss: 12.147481
Action reg: 0.003951
  l1.weight: grad_norm = 0.198886
  l1.bias: grad_norm = 0.000667
  l2.weight: grad_norm = 0.185420
Total gradient norm: 0.557840
=== Actor Training Debug (Iteration 7804) ===
Q mean: -12.727791
Q std: 15.538824
Actor loss: 12.731762
Action reg: 0.003971
  l1.weight: grad_norm = 0.188901
  l1.bias: grad_norm = 0.000176
  l2.weight: grad_norm = 0.176515
Total gradient norm: 0.481419
=== Actor Training Debug (Iteration 7805) ===
Q mean: -13.476002
Q std: 15.518458
Actor loss: 13.479947
Action reg: 0.003945
  l1.weight: grad_norm = 0.186249
  l1.bias: grad_norm = 0.000821
  l2.weight: grad_norm = 0.158869
Total gradient norm: 0.375174
=== Actor Training Debug (Iteration 7806) ===
Q mean: -11.490871
Q std: 14.610343
Actor loss: 11.494813
Action reg: 0.003942
  l1.weight: grad_norm = 0.343886
  l1.bias: grad_norm = 0.000569
  l2.weight: grad_norm = 0.301970
Total gradient norm: 0.876209
=== Actor Training Debug (Iteration 7807) ===
Q mean: -12.970777
Q std: 15.987461
Actor loss: 12.974705
Action reg: 0.003928
  l1.weight: grad_norm = 0.371373
  l1.bias: grad_norm = 0.000933
  l2.weight: grad_norm = 0.297818
Total gradient norm: 0.813538
=== Actor Training Debug (Iteration 7808) ===
Q mean: -10.757363
Q std: 11.853877
Actor loss: 10.761318
Action reg: 0.003955
  l1.weight: grad_norm = 0.292780
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.231785
Total gradient norm: 0.683398
=== Actor Training Debug (Iteration 7809) ===
Q mean: -10.568789
Q std: 14.505515
Actor loss: 10.572747
Action reg: 0.003958
  l1.weight: grad_norm = 0.270531
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.256848
Total gradient norm: 0.758441
=== Actor Training Debug (Iteration 7810) ===
Q mean: -12.672140
Q std: 16.113340
Actor loss: 12.676080
Action reg: 0.003940
  l1.weight: grad_norm = 0.261177
  l1.bias: grad_norm = 0.000302
  l2.weight: grad_norm = 0.225062
Total gradient norm: 0.598570
=== Actor Training Debug (Iteration 7811) ===
Q mean: -13.070912
Q std: 13.944203
Actor loss: 13.074869
Action reg: 0.003957
  l1.weight: grad_norm = 0.364776
  l1.bias: grad_norm = 0.001620
  l2.weight: grad_norm = 0.303528
Total gradient norm: 0.708801
=== Actor Training Debug (Iteration 7812) ===
Q mean: -12.969243
Q std: 15.955797
Actor loss: 12.973190
Action reg: 0.003948
  l1.weight: grad_norm = 0.334034
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.244192
Total gradient norm: 0.601289
=== Actor Training Debug (Iteration 7813) ===
Q mean: -14.513385
Q std: 18.368378
Actor loss: 14.517312
Action reg: 0.003928
  l1.weight: grad_norm = 0.477688
  l1.bias: grad_norm = 0.000346
  l2.weight: grad_norm = 0.409043
Total gradient norm: 1.096175
=== Actor Training Debug (Iteration 7814) ===
Q mean: -13.071024
Q std: 15.152795
Actor loss: 13.074984
Action reg: 0.003960
  l1.weight: grad_norm = 0.196577
  l1.bias: grad_norm = 0.000357
  l2.weight: grad_norm = 0.176785
Total gradient norm: 0.442485
=== Actor Training Debug (Iteration 7815) ===
Q mean: -10.926004
Q std: 13.769431
Actor loss: 10.929964
Action reg: 0.003960
  l1.weight: grad_norm = 0.188239
  l1.bias: grad_norm = 0.000855
  l2.weight: grad_norm = 0.170819
Total gradient norm: 0.519170
=== Actor Training Debug (Iteration 7816) ===
Q mean: -11.869013
Q std: 11.761577
Actor loss: 11.872967
Action reg: 0.003954
  l1.weight: grad_norm = 0.207777
  l1.bias: grad_norm = 0.000383
  l2.weight: grad_norm = 0.184019
Total gradient norm: 0.442225
=== Actor Training Debug (Iteration 7817) ===
Q mean: -13.235479
Q std: 14.689002
Actor loss: 13.239447
Action reg: 0.003967
  l1.weight: grad_norm = 0.251618
  l1.bias: grad_norm = 0.000097
  l2.weight: grad_norm = 0.195402
Total gradient norm: 0.526448
=== Actor Training Debug (Iteration 7818) ===
Q mean: -14.132602
Q std: 17.764980
Actor loss: 14.136554
Action reg: 0.003952
  l1.weight: grad_norm = 0.323898
  l1.bias: grad_norm = 0.000201
  l2.weight: grad_norm = 0.293829
Total gradient norm: 0.759946
=== Actor Training Debug (Iteration 7819) ===
Q mean: -12.016632
Q std: 14.911764
Actor loss: 12.020578
Action reg: 0.003946
  l1.weight: grad_norm = 0.249384
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.208527
Total gradient norm: 0.539819
=== Actor Training Debug (Iteration 7820) ===
Q mean: -13.617848
Q std: 15.427238
Actor loss: 13.621795
Action reg: 0.003946
  l1.weight: grad_norm = 0.454067
  l1.bias: grad_norm = 0.000263
  l2.weight: grad_norm = 0.343274
Total gradient norm: 0.900703
=== Actor Training Debug (Iteration 7821) ===
Q mean: -11.197760
Q std: 14.517688
Actor loss: 11.201719
Action reg: 0.003959
  l1.weight: grad_norm = 0.164048
  l1.bias: grad_norm = 0.000354
  l2.weight: grad_norm = 0.144678
Total gradient norm: 0.361981
=== Actor Training Debug (Iteration 7822) ===
Q mean: -11.593507
Q std: 14.628592
Actor loss: 11.597428
Action reg: 0.003922
  l1.weight: grad_norm = 0.334403
  l1.bias: grad_norm = 0.000718
  l2.weight: grad_norm = 0.302596
Total gradient norm: 0.818635
=== Actor Training Debug (Iteration 7823) ===
Q mean: -11.666285
Q std: 13.589484
Actor loss: 11.670225
Action reg: 0.003941
  l1.weight: grad_norm = 0.282510
  l1.bias: grad_norm = 0.005336
  l2.weight: grad_norm = 0.229704
Total gradient norm: 0.612283
=== Actor Training Debug (Iteration 7824) ===
Q mean: -12.837521
Q std: 16.135513
Actor loss: 12.841475
Action reg: 0.003955
  l1.weight: grad_norm = 0.220828
  l1.bias: grad_norm = 0.000490
  l2.weight: grad_norm = 0.195093
Total gradient norm: 0.479260
=== Actor Training Debug (Iteration 7825) ===
Q mean: -12.934564
Q std: 15.448168
Actor loss: 12.938507
Action reg: 0.003944
  l1.weight: grad_norm = 0.268311
  l1.bias: grad_norm = 0.000946
  l2.weight: grad_norm = 0.233706
Total gradient norm: 0.616751
=== Actor Training Debug (Iteration 7826) ===
Q mean: -10.891546
Q std: 14.097815
Actor loss: 10.895493
Action reg: 0.003946
  l1.weight: grad_norm = 0.334871
  l1.bias: grad_norm = 0.000190
  l2.weight: grad_norm = 0.284610
Total gradient norm: 0.749066
=== Actor Training Debug (Iteration 7827) ===
Q mean: -12.606871
Q std: 14.599598
Actor loss: 12.610800
Action reg: 0.003929
  l1.weight: grad_norm = 0.257524
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.210524
Total gradient norm: 0.566265
=== Actor Training Debug (Iteration 7828) ===
Q mean: -13.456491
Q std: 16.924225
Actor loss: 13.460452
Action reg: 0.003962
  l1.weight: grad_norm = 0.298999
  l1.bias: grad_norm = 0.000462
  l2.weight: grad_norm = 0.244126
Total gradient norm: 0.639909
=== Actor Training Debug (Iteration 7829) ===
Q mean: -13.053362
Q std: 17.149340
Actor loss: 13.057305
Action reg: 0.003944
  l1.weight: grad_norm = 0.194033
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.171579
Total gradient norm: 0.462353
=== Actor Training Debug (Iteration 7830) ===
Q mean: -12.927809
Q std: 16.151939
Actor loss: 12.931771
Action reg: 0.003963
  l1.weight: grad_norm = 0.350804
  l1.bias: grad_norm = 0.000257
  l2.weight: grad_norm = 0.286081
Total gradient norm: 0.717878
=== Actor Training Debug (Iteration 7831) ===
Q mean: -12.515560
Q std: 14.534239
Actor loss: 12.519526
Action reg: 0.003967
  l1.weight: grad_norm = 0.315088
  l1.bias: grad_norm = 0.000212
  l2.weight: grad_norm = 0.262801
Total gradient norm: 0.644144
=== Actor Training Debug (Iteration 7832) ===
Q mean: -12.045248
Q std: 15.216625
Actor loss: 12.049208
Action reg: 0.003959
  l1.weight: grad_norm = 0.383345
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.321674
Total gradient norm: 0.754735
=== Actor Training Debug (Iteration 7833) ===
Q mean: -12.442497
Q std: 15.925471
Actor loss: 12.446415
Action reg: 0.003918
  l1.weight: grad_norm = 0.260789
  l1.bias: grad_norm = 0.000435
  l2.weight: grad_norm = 0.212212
Total gradient norm: 0.544377
=== Actor Training Debug (Iteration 7834) ===
Q mean: -12.425741
Q std: 15.738422
Actor loss: 12.429682
Action reg: 0.003941
  l1.weight: grad_norm = 0.402973
  l1.bias: grad_norm = 0.000675
  l2.weight: grad_norm = 0.359097
Total gradient norm: 0.901582
=== Actor Training Debug (Iteration 7835) ===
Q mean: -12.055481
Q std: 15.198322
Actor loss: 12.059438
Action reg: 0.003957
  l1.weight: grad_norm = 0.189772
  l1.bias: grad_norm = 0.000610
  l2.weight: grad_norm = 0.160504
Total gradient norm: 0.407321
=== Actor Training Debug (Iteration 7836) ===
Q mean: -12.018143
Q std: 15.233087
Actor loss: 12.022113
Action reg: 0.003971
  l1.weight: grad_norm = 0.187775
  l1.bias: grad_norm = 0.000094
  l2.weight: grad_norm = 0.145611
Total gradient norm: 0.417575
=== Actor Training Debug (Iteration 7837) ===
Q mean: -13.504982
Q std: 16.902031
Actor loss: 13.508950
Action reg: 0.003968
  l1.weight: grad_norm = 0.147915
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.137871
Total gradient norm: 0.349833
=== Actor Training Debug (Iteration 7838) ===
Q mean: -13.035528
Q std: 14.510137
Actor loss: 13.039492
Action reg: 0.003964
  l1.weight: grad_norm = 0.245707
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.233633
Total gradient norm: 0.564973
=== Actor Training Debug (Iteration 7839) ===
Q mean: -13.090523
Q std: 14.899278
Actor loss: 13.094470
Action reg: 0.003947
  l1.weight: grad_norm = 0.302698
  l1.bias: grad_norm = 0.000601
  l2.weight: grad_norm = 0.274212
Total gradient norm: 0.717078
=== Actor Training Debug (Iteration 7840) ===
Q mean: -12.541880
Q std: 15.058099
Actor loss: 12.545845
Action reg: 0.003965
  l1.weight: grad_norm = 0.175197
  l1.bias: grad_norm = 0.000273
  l2.weight: grad_norm = 0.157522
Total gradient norm: 0.414831
=== Actor Training Debug (Iteration 7841) ===
Q mean: -13.253461
Q std: 15.587579
Actor loss: 13.257391
Action reg: 0.003930
  l1.weight: grad_norm = 0.377144
  l1.bias: grad_norm = 0.000681
  l2.weight: grad_norm = 0.314756
Total gradient norm: 0.754260
=== Actor Training Debug (Iteration 7842) ===
Q mean: -12.923199
Q std: 14.479036
Actor loss: 12.927139
Action reg: 0.003941
  l1.weight: grad_norm = 0.242419
  l1.bias: grad_norm = 0.000580
  l2.weight: grad_norm = 0.193362
Total gradient norm: 0.480835
=== Actor Training Debug (Iteration 7843) ===
Q mean: -12.755950
Q std: 16.830706
Actor loss: 12.759892
Action reg: 0.003942
  l1.weight: grad_norm = 0.447743
  l1.bias: grad_norm = 0.000416
  l2.weight: grad_norm = 0.358717
Total gradient norm: 0.914229
=== Actor Training Debug (Iteration 7844) ===
Q mean: -12.662658
Q std: 16.556179
Actor loss: 12.666600
Action reg: 0.003943
  l1.weight: grad_norm = 0.265414
  l1.bias: grad_norm = 0.000377
  l2.weight: grad_norm = 0.218777
Total gradient norm: 0.594197
=== Actor Training Debug (Iteration 7845) ===
Q mean: -14.118691
Q std: 15.327334
Actor loss: 14.122635
Action reg: 0.003943
  l1.weight: grad_norm = 0.481844
  l1.bias: grad_norm = 0.001224
  l2.weight: grad_norm = 0.376041
Total gradient norm: 1.080744
=== Actor Training Debug (Iteration 7846) ===
Q mean: -13.507833
Q std: 17.031073
Actor loss: 13.511775
Action reg: 0.003941
  l1.weight: grad_norm = 0.195456
  l1.bias: grad_norm = 0.000812
  l2.weight: grad_norm = 0.178252
Total gradient norm: 0.478983
=== Actor Training Debug (Iteration 7847) ===
Q mean: -12.744439
Q std: 14.125487
Actor loss: 12.748398
Action reg: 0.003959
  l1.weight: grad_norm = 0.303587
  l1.bias: grad_norm = 0.000285
  l2.weight: grad_norm = 0.264636
Total gradient norm: 0.649079
=== Actor Training Debug (Iteration 7848) ===
Q mean: -12.150731
Q std: 15.407467
Actor loss: 12.154675
Action reg: 0.003944
  l1.weight: grad_norm = 0.271753
  l1.bias: grad_norm = 0.000423
  l2.weight: grad_norm = 0.220318
Total gradient norm: 0.627142
=== Actor Training Debug (Iteration 7849) ===
Q mean: -11.991415
Q std: 14.531682
Actor loss: 11.995367
Action reg: 0.003952
  l1.weight: grad_norm = 0.312215
  l1.bias: grad_norm = 0.001059
  l2.weight: grad_norm = 0.254768
Total gradient norm: 0.605658
=== Actor Training Debug (Iteration 7850) ===
Q mean: -11.891422
Q std: 14.651531
Actor loss: 11.895377
Action reg: 0.003955
  l1.weight: grad_norm = 0.154202
  l1.bias: grad_norm = 0.000113
  l2.weight: grad_norm = 0.125882
Total gradient norm: 0.367286
=== Actor Training Debug (Iteration 7851) ===
Q mean: -14.332130
Q std: 16.552969
Actor loss: 14.336083
Action reg: 0.003953
  l1.weight: grad_norm = 0.263579
  l1.bias: grad_norm = 0.000432
  l2.weight: grad_norm = 0.219334
Total gradient norm: 0.553368
=== Actor Training Debug (Iteration 7852) ===
Q mean: -14.649639
Q std: 19.659557
Actor loss: 14.653603
Action reg: 0.003963
  l1.weight: grad_norm = 0.283054
  l1.bias: grad_norm = 0.001233
  l2.weight: grad_norm = 0.239398
Total gradient norm: 0.665870
=== Actor Training Debug (Iteration 7853) ===
Q mean: -12.619225
Q std: 16.557262
Actor loss: 12.623191
Action reg: 0.003967
  l1.weight: grad_norm = 0.346646
  l1.bias: grad_norm = 0.000161
  l2.weight: grad_norm = 0.311218
Total gradient norm: 0.867426
=== Actor Training Debug (Iteration 7854) ===
Q mean: -13.879892
Q std: 15.755272
Actor loss: 13.883839
Action reg: 0.003947
  l1.weight: grad_norm = 0.369534
  l1.bias: grad_norm = 0.000268
  l2.weight: grad_norm = 0.289400
Total gradient norm: 0.788911
=== Actor Training Debug (Iteration 7855) ===
Q mean: -11.012587
Q std: 13.345177
Actor loss: 11.016543
Action reg: 0.003957
  l1.weight: grad_norm = 0.329708
  l1.bias: grad_norm = 0.001289
  l2.weight: grad_norm = 0.268296
Total gradient norm: 0.897946
=== Actor Training Debug (Iteration 7856) ===
Q mean: -12.331301
Q std: 14.882538
Actor loss: 12.335252
Action reg: 0.003952
  l1.weight: grad_norm = 0.215554
  l1.bias: grad_norm = 0.000265
  l2.weight: grad_norm = 0.179873
Total gradient norm: 0.477231
=== Actor Training Debug (Iteration 7857) ===
Q mean: -14.337815
Q std: 18.789019
Actor loss: 14.341762
Action reg: 0.003946
  l1.weight: grad_norm = 0.339792
  l1.bias: grad_norm = 0.000320
  l2.weight: grad_norm = 0.314399
Total gradient norm: 0.841092
=== Actor Training Debug (Iteration 7858) ===
Q mean: -12.404818
Q std: 16.260771
Actor loss: 12.408764
Action reg: 0.003946
  l1.weight: grad_norm = 0.154618
  l1.bias: grad_norm = 0.000949
  l2.weight: grad_norm = 0.131022
Total gradient norm: 0.375767
=== Actor Training Debug (Iteration 7859) ===
Q mean: -12.009757
Q std: 15.577922
Actor loss: 12.013705
Action reg: 0.003948
  l1.weight: grad_norm = 0.251482
  l1.bias: grad_norm = 0.000389
  l2.weight: grad_norm = 0.196267
Total gradient norm: 0.534586
=== Actor Training Debug (Iteration 7860) ===
Q mean: -14.025814
Q std: 15.891495
Actor loss: 14.029759
Action reg: 0.003945
  l1.weight: grad_norm = 0.488285
  l1.bias: grad_norm = 0.000392
  l2.weight: grad_norm = 0.352375
Total gradient norm: 0.880510
=== Actor Training Debug (Iteration 7861) ===
Q mean: -10.923262
Q std: 12.445839
Actor loss: 10.927202
Action reg: 0.003941
  l1.weight: grad_norm = 0.216057
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.188360
Total gradient norm: 0.453771
=== Actor Training Debug (Iteration 7862) ===
Q mean: -10.841602
Q std: 14.801482
Actor loss: 10.845551
Action reg: 0.003948
  l1.weight: grad_norm = 0.307274
  l1.bias: grad_norm = 0.001012
  l2.weight: grad_norm = 0.254650
Total gradient norm: 0.702768
=== Actor Training Debug (Iteration 7863) ===
Q mean: -12.791536
Q std: 15.797463
Actor loss: 12.795492
Action reg: 0.003956
  l1.weight: grad_norm = 0.332442
  l1.bias: grad_norm = 0.000499
  l2.weight: grad_norm = 0.273710
Total gradient norm: 0.648427
=== Actor Training Debug (Iteration 7864) ===
Q mean: -12.667597
Q std: 15.726225
Actor loss: 12.671532
Action reg: 0.003935
  l1.weight: grad_norm = 0.464110
  l1.bias: grad_norm = 0.001451
  l2.weight: grad_norm = 0.355282
Total gradient norm: 0.861075
=== Actor Training Debug (Iteration 7865) ===
Q mean: -11.634374
Q std: 15.288283
Actor loss: 11.638344
Action reg: 0.003970
  l1.weight: grad_norm = 0.127151
  l1.bias: grad_norm = 0.000056
  l2.weight: grad_norm = 0.108406
Total gradient norm: 0.281268
=== Actor Training Debug (Iteration 7866) ===
Q mean: -12.347248
Q std: 15.683078
Actor loss: 12.351214
Action reg: 0.003966
  l1.weight: grad_norm = 0.289794
  l1.bias: grad_norm = 0.000111
  l2.weight: grad_norm = 0.257896
Total gradient norm: 0.685857
=== Actor Training Debug (Iteration 7867) ===
Q mean: -12.782288
Q std: 16.665836
Actor loss: 12.786238
Action reg: 0.003950
  l1.weight: grad_norm = 0.303012
  l1.bias: grad_norm = 0.000571
  l2.weight: grad_norm = 0.236636
Total gradient norm: 0.625700
=== Actor Training Debug (Iteration 7868) ===
Q mean: -12.692704
Q std: 15.387176
Actor loss: 12.696663
Action reg: 0.003959
  l1.weight: grad_norm = 0.131652
  l1.bias: grad_norm = 0.000232
  l2.weight: grad_norm = 0.102546
Total gradient norm: 0.259680
=== Actor Training Debug (Iteration 7869) ===
Q mean: -13.290707
Q std: 16.270565
Actor loss: 13.294657
Action reg: 0.003950
  l1.weight: grad_norm = 0.181242
  l1.bias: grad_norm = 0.000630
  l2.weight: grad_norm = 0.141107
Total gradient norm: 0.429161
=== Actor Training Debug (Iteration 7870) ===
Q mean: -12.434855
Q std: 15.717028
Actor loss: 12.438814
Action reg: 0.003960
  l1.weight: grad_norm = 0.217852
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.181594
Total gradient norm: 0.469238
=== Actor Training Debug (Iteration 7871) ===
Q mean: -12.567293
Q std: 16.664156
Actor loss: 12.571228
Action reg: 0.003935
  l1.weight: grad_norm = 0.307282
  l1.bias: grad_norm = 0.001190
  l2.weight: grad_norm = 0.265247
Total gradient norm: 0.769084
=== Actor Training Debug (Iteration 7872) ===
Q mean: -13.074261
Q std: 17.575830
Actor loss: 13.078177
Action reg: 0.003917
  l1.weight: grad_norm = 0.448560
  l1.bias: grad_norm = 0.000716
  l2.weight: grad_norm = 0.350023
Total gradient norm: 0.925698
=== Actor Training Debug (Iteration 7873) ===
Q mean: -12.565927
Q std: 16.012209
Actor loss: 12.569875
Action reg: 0.003948
  l1.weight: grad_norm = 0.192517
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.157462
Total gradient norm: 0.415046
=== Actor Training Debug (Iteration 7874) ===
Q mean: -13.146336
Q std: 17.315796
Actor loss: 13.150291
Action reg: 0.003956
  l1.weight: grad_norm = 0.348592
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.299522
Total gradient norm: 0.786785
=== Actor Training Debug (Iteration 7875) ===
Q mean: -12.531340
Q std: 14.541103
Actor loss: 12.535284
Action reg: 0.003944
  l1.weight: grad_norm = 0.291483
  l1.bias: grad_norm = 0.000220
  l2.weight: grad_norm = 0.222124
Total gradient norm: 0.527157
=== Actor Training Debug (Iteration 7876) ===
Q mean: -12.510223
Q std: 15.707007
Actor loss: 12.514176
Action reg: 0.003953
  l1.weight: grad_norm = 0.610990
  l1.bias: grad_norm = 0.000222
  l2.weight: grad_norm = 0.488318
Total gradient norm: 1.303787
=== Actor Training Debug (Iteration 7877) ===
Q mean: -12.387804
Q std: 16.354502
Actor loss: 12.391738
Action reg: 0.003934
  l1.weight: grad_norm = 0.315702
  l1.bias: grad_norm = 0.001437
  l2.weight: grad_norm = 0.259649
Total gradient norm: 0.681305
=== Actor Training Debug (Iteration 7878) ===
Q mean: -11.866318
Q std: 12.372487
Actor loss: 11.870264
Action reg: 0.003947
  l1.weight: grad_norm = 0.461543
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.350612
Total gradient norm: 0.955569
=== Actor Training Debug (Iteration 7879) ===
Q mean: -11.568991
Q std: 15.357732
Actor loss: 11.572951
Action reg: 0.003961
  l1.weight: grad_norm = 0.284661
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.241990
Total gradient norm: 0.638386
=== Actor Training Debug (Iteration 7880) ===
Q mean: -12.321547
Q std: 15.040492
Actor loss: 12.325505
Action reg: 0.003958
  l1.weight: grad_norm = 0.234075
  l1.bias: grad_norm = 0.000259
  l2.weight: grad_norm = 0.184277
Total gradient norm: 0.473616
=== Actor Training Debug (Iteration 7881) ===
Q mean: -12.829012
Q std: 16.310629
Actor loss: 12.832975
Action reg: 0.003963
  l1.weight: grad_norm = 0.197727
  l1.bias: grad_norm = 0.000130
  l2.weight: grad_norm = 0.185642
Total gradient norm: 0.490762
=== Actor Training Debug (Iteration 7882) ===
Q mean: -11.382932
Q std: 13.059446
Actor loss: 11.386897
Action reg: 0.003966
  l1.weight: grad_norm = 0.136372
  l1.bias: grad_norm = 0.000362
  l2.weight: grad_norm = 0.121052
Total gradient norm: 0.288602
=== Actor Training Debug (Iteration 7883) ===
Q mean: -12.029381
Q std: 14.833741
Actor loss: 12.033332
Action reg: 0.003952
  l1.weight: grad_norm = 0.263181
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.239534
Total gradient norm: 0.644529
=== Actor Training Debug (Iteration 7884) ===
Q mean: -12.674108
Q std: 15.921679
Actor loss: 12.678072
Action reg: 0.003964
  l1.weight: grad_norm = 0.323504
  l1.bias: grad_norm = 0.000116
  l2.weight: grad_norm = 0.257055
Total gradient norm: 0.585742
=== Actor Training Debug (Iteration 7885) ===
Q mean: -13.591925
Q std: 17.839880
Actor loss: 13.595872
Action reg: 0.003947
  l1.weight: grad_norm = 0.276652
  l1.bias: grad_norm = 0.000743
  l2.weight: grad_norm = 0.230117
Total gradient norm: 0.639309
=== Actor Training Debug (Iteration 7886) ===
Q mean: -12.944411
Q std: 15.382878
Actor loss: 12.948341
Action reg: 0.003930
  l1.weight: grad_norm = 0.263351
  l1.bias: grad_norm = 0.001410
  l2.weight: grad_norm = 0.226839
Total gradient norm: 0.576327
=== Actor Training Debug (Iteration 7887) ===
Q mean: -14.177741
Q std: 15.312851
Actor loss: 14.181678
Action reg: 0.003937
  l1.weight: grad_norm = 0.359924
  l1.bias: grad_norm = 0.000943
  l2.weight: grad_norm = 0.282282
Total gradient norm: 0.763260
=== Actor Training Debug (Iteration 7888) ===
Q mean: -12.152742
Q std: 15.404061
Actor loss: 12.156699
Action reg: 0.003956
  l1.weight: grad_norm = 0.229300
  l1.bias: grad_norm = 0.000269
  l2.weight: grad_norm = 0.191824
Total gradient norm: 0.500059
=== Actor Training Debug (Iteration 7889) ===
Q mean: -14.590994
Q std: 16.835812
Actor loss: 14.594953
Action reg: 0.003958
  l1.weight: grad_norm = 0.186798
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.154253
Total gradient norm: 0.430815
=== Actor Training Debug (Iteration 7890) ===
Q mean: -11.870197
Q std: 14.076846
Actor loss: 11.874141
Action reg: 0.003943
  l1.weight: grad_norm = 0.750347
  l1.bias: grad_norm = 0.001193
  l2.weight: grad_norm = 0.573320
Total gradient norm: 1.572512
=== Actor Training Debug (Iteration 7891) ===
Q mean: -12.435674
Q std: 15.606320
Actor loss: 12.439626
Action reg: 0.003952
  l1.weight: grad_norm = 0.673182
  l1.bias: grad_norm = 0.000786
  l2.weight: grad_norm = 0.494322
Total gradient norm: 1.117774
=== Actor Training Debug (Iteration 7892) ===
Q mean: -12.441717
Q std: 13.913671
Actor loss: 12.445656
Action reg: 0.003939
  l1.weight: grad_norm = 0.357919
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.288276
Total gradient norm: 0.695089
=== Actor Training Debug (Iteration 7893) ===
Q mean: -13.774705
Q std: 16.347555
Actor loss: 13.778645
Action reg: 0.003940
  l1.weight: grad_norm = 0.224334
  l1.bias: grad_norm = 0.000706
  l2.weight: grad_norm = 0.189154
Total gradient norm: 0.461407
=== Actor Training Debug (Iteration 7894) ===
Q mean: -15.269251
Q std: 19.071075
Actor loss: 15.273201
Action reg: 0.003950
  l1.weight: grad_norm = 0.165771
  l1.bias: grad_norm = 0.001047
  l2.weight: grad_norm = 0.147715
Total gradient norm: 0.356532
=== Actor Training Debug (Iteration 7895) ===
Q mean: -12.842321
Q std: 15.218538
Actor loss: 12.846286
Action reg: 0.003965
  l1.weight: grad_norm = 0.258092
  l1.bias: grad_norm = 0.000364
  l2.weight: grad_norm = 0.233522
Total gradient norm: 0.688384
=== Actor Training Debug (Iteration 7896) ===
Q mean: -13.225663
Q std: 15.787120
Actor loss: 13.229614
Action reg: 0.003951
  l1.weight: grad_norm = 0.434239
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.384308
Total gradient norm: 0.968351
=== Actor Training Debug (Iteration 7897) ===
Q mean: -11.777523
Q std: 14.738778
Actor loss: 11.781459
Action reg: 0.003936
  l1.weight: grad_norm = 0.279078
  l1.bias: grad_norm = 0.000707
  l2.weight: grad_norm = 0.265283
Total gradient norm: 0.692662
=== Actor Training Debug (Iteration 7898) ===
Q mean: -13.052139
Q std: 14.955550
Actor loss: 13.056089
Action reg: 0.003950
  l1.weight: grad_norm = 0.382411
  l1.bias: grad_norm = 0.000463
  l2.weight: grad_norm = 0.323202
Total gradient norm: 0.832681
=== Actor Training Debug (Iteration 7899) ===
Q mean: -14.469713
Q std: 17.077774
Actor loss: 14.473665
Action reg: 0.003952
  l1.weight: grad_norm = 0.167452
  l1.bias: grad_norm = 0.003326
  l2.weight: grad_norm = 0.160902
Total gradient norm: 0.439950
=== Actor Training Debug (Iteration 7900) ===
Q mean: -13.080852
Q std: 15.749165
Actor loss: 13.084812
Action reg: 0.003960
  l1.weight: grad_norm = 0.161902
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.153163
Total gradient norm: 0.367585
=== Actor Training Debug (Iteration 7901) ===
Q mean: -12.239456
Q std: 15.942533
Actor loss: 12.243396
Action reg: 0.003940
  l1.weight: grad_norm = 0.854292
  l1.bias: grad_norm = 0.001874
  l2.weight: grad_norm = 0.623827
Total gradient norm: 1.676118
=== Actor Training Debug (Iteration 7902) ===
Q mean: -12.909475
Q std: 15.660254
Actor loss: 12.913431
Action reg: 0.003955
  l1.weight: grad_norm = 0.267132
  l1.bias: grad_norm = 0.000282
  l2.weight: grad_norm = 0.233706
Total gradient norm: 0.607732
=== Actor Training Debug (Iteration 7903) ===
Q mean: -12.539324
Q std: 16.685209
Actor loss: 12.543262
Action reg: 0.003939
  l1.weight: grad_norm = 0.372750
  l1.bias: grad_norm = 0.000336
  l2.weight: grad_norm = 0.318296
Total gradient norm: 0.796598
=== Actor Training Debug (Iteration 7904) ===
Q mean: -12.726639
Q std: 15.190458
Actor loss: 12.730601
Action reg: 0.003963
  l1.weight: grad_norm = 0.138479
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.117433
Total gradient norm: 0.331503
=== Actor Training Debug (Iteration 7905) ===
Q mean: -13.155982
Q std: 14.534112
Actor loss: 13.159935
Action reg: 0.003953
  l1.weight: grad_norm = 0.206589
  l1.bias: grad_norm = 0.003631
  l2.weight: grad_norm = 0.165085
Total gradient norm: 0.418491
=== Actor Training Debug (Iteration 7906) ===
Q mean: -12.809984
Q std: 17.594631
Actor loss: 12.813907
Action reg: 0.003922
  l1.weight: grad_norm = 0.286549
  l1.bias: grad_norm = 0.000386
  l2.weight: grad_norm = 0.253761
Total gradient norm: 0.674182
=== Actor Training Debug (Iteration 7907) ===
Q mean: -15.230827
Q std: 18.060841
Actor loss: 15.234778
Action reg: 0.003951
  l1.weight: grad_norm = 0.331396
  l1.bias: grad_norm = 0.000283
  l2.weight: grad_norm = 0.292026
Total gradient norm: 0.820147
=== Actor Training Debug (Iteration 7908) ===
Q mean: -14.189389
Q std: 17.635443
Actor loss: 14.193352
Action reg: 0.003963
  l1.weight: grad_norm = 0.264284
  l1.bias: grad_norm = 0.000167
  l2.weight: grad_norm = 0.236271
Total gradient norm: 0.629224
=== Actor Training Debug (Iteration 7909) ===
Q mean: -13.642801
Q std: 16.153555
Actor loss: 13.646741
Action reg: 0.003940
  l1.weight: grad_norm = 0.547571
  l1.bias: grad_norm = 0.000329
  l2.weight: grad_norm = 0.439327
Total gradient norm: 1.229847
=== Actor Training Debug (Iteration 7910) ===
Q mean: -13.546194
Q std: 14.822778
Actor loss: 13.550146
Action reg: 0.003952
  l1.weight: grad_norm = 0.168424
  l1.bias: grad_norm = 0.000173
  l2.weight: grad_norm = 0.145864
Total gradient norm: 0.350299
=== Actor Training Debug (Iteration 7911) ===
Q mean: -13.713332
Q std: 17.578197
Actor loss: 13.717300
Action reg: 0.003969
  l1.weight: grad_norm = 0.112172
  l1.bias: grad_norm = 0.000046
  l2.weight: grad_norm = 0.108137
Total gradient norm: 0.278297
=== Actor Training Debug (Iteration 7912) ===
Q mean: -12.358027
Q std: 16.098167
Actor loss: 12.361985
Action reg: 0.003959
  l1.weight: grad_norm = 0.224714
  l1.bias: grad_norm = 0.000183
  l2.weight: grad_norm = 0.212193
Total gradient norm: 0.558396
=== Actor Training Debug (Iteration 7913) ===
Q mean: -12.489475
Q std: 15.007435
Actor loss: 12.493433
Action reg: 0.003957
  l1.weight: grad_norm = 0.170167
  l1.bias: grad_norm = 0.000125
  l2.weight: grad_norm = 0.152102
Total gradient norm: 0.381608
=== Actor Training Debug (Iteration 7914) ===
Q mean: -12.834880
Q std: 17.060101
Actor loss: 12.838843
Action reg: 0.003964
  l1.weight: grad_norm = 0.142108
  l1.bias: grad_norm = 0.000271
  l2.weight: grad_norm = 0.136456
Total gradient norm: 0.361627
=== Actor Training Debug (Iteration 7915) ===
Q mean: -12.630165
Q std: 16.778774
Actor loss: 12.634124
Action reg: 0.003959
  l1.weight: grad_norm = 0.266095
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.229858
Total gradient norm: 0.674650
=== Actor Training Debug (Iteration 7916) ===
Q mean: -13.143873
Q std: 18.147894
Actor loss: 13.147801
Action reg: 0.003929
  l1.weight: grad_norm = 0.427519
  l1.bias: grad_norm = 0.001251
  l2.weight: grad_norm = 0.391558
Total gradient norm: 1.073582
=== Actor Training Debug (Iteration 7917) ===
Q mean: -13.150553
Q std: 15.009571
Actor loss: 13.154485
Action reg: 0.003932
  l1.weight: grad_norm = 0.188242
  l1.bias: grad_norm = 0.001687
  l2.weight: grad_norm = 0.157655
Total gradient norm: 0.392100
=== Actor Training Debug (Iteration 7918) ===
Q mean: -12.190710
Q std: 16.453424
Actor loss: 12.194644
Action reg: 0.003934
  l1.weight: grad_norm = 0.373254
  l1.bias: grad_norm = 0.001397
  l2.weight: grad_norm = 0.330382
Total gradient norm: 0.839068
=== Actor Training Debug (Iteration 7919) ===
Q mean: -12.279287
Q std: 16.685101
Actor loss: 12.283236
Action reg: 0.003948
  l1.weight: grad_norm = 0.198189
  l1.bias: grad_norm = 0.000611
  l2.weight: grad_norm = 0.192030
Total gradient norm: 0.528203
=== Actor Training Debug (Iteration 7920) ===
Q mean: -15.149773
Q std: 18.450897
Actor loss: 15.153740
Action reg: 0.003967
  l1.weight: grad_norm = 0.224072
  l1.bias: grad_norm = 0.000446
  l2.weight: grad_norm = 0.188739
Total gradient norm: 0.534295
=== Actor Training Debug (Iteration 7921) ===
Q mean: -13.173631
Q std: 16.779425
Actor loss: 13.177589
Action reg: 0.003959
  l1.weight: grad_norm = 0.161087
  l1.bias: grad_norm = 0.000372
  l2.weight: grad_norm = 0.143755
Total gradient norm: 0.403563
=== Actor Training Debug (Iteration 7922) ===
Q mean: -12.108681
Q std: 14.542791
Actor loss: 12.112619
Action reg: 0.003939
  l1.weight: grad_norm = 0.368013
  l1.bias: grad_norm = 0.000650
  l2.weight: grad_norm = 0.346490
Total gradient norm: 0.785138
=== Actor Training Debug (Iteration 7923) ===
Q mean: -13.577789
Q std: 17.417118
Actor loss: 13.581733
Action reg: 0.003943
  l1.weight: grad_norm = 0.299039
  l1.bias: grad_norm = 0.001518
  l2.weight: grad_norm = 0.234108
Total gradient norm: 0.626385
=== Actor Training Debug (Iteration 7924) ===
Q mean: -14.364456
Q std: 15.775908
Actor loss: 14.368413
Action reg: 0.003957
  l1.weight: grad_norm = 0.221779
  l1.bias: grad_norm = 0.000128
  l2.weight: grad_norm = 0.183748
Total gradient norm: 0.500812
=== Actor Training Debug (Iteration 7925) ===
Q mean: -12.716969
Q std: 15.989131
Actor loss: 12.720919
Action reg: 0.003949
  l1.weight: grad_norm = 0.173166
  l1.bias: grad_norm = 0.000654
  l2.weight: grad_norm = 0.138822
Total gradient norm: 0.346392
=== Actor Training Debug (Iteration 7926) ===
Q mean: -12.028316
Q std: 16.157707
Actor loss: 12.032274
Action reg: 0.003959
  l1.weight: grad_norm = 0.327953
  l1.bias: grad_norm = 0.000115
  l2.weight: grad_norm = 0.285753
Total gradient norm: 0.789114
=== Actor Training Debug (Iteration 7927) ===
Q mean: -15.028286
Q std: 16.961815
Actor loss: 15.032253
Action reg: 0.003968
  l1.weight: grad_norm = 0.165462
  l1.bias: grad_norm = 0.000133
  l2.weight: grad_norm = 0.141168
Total gradient norm: 0.366418
=== Actor Training Debug (Iteration 7928) ===
Q mean: -12.406979
Q std: 14.585455
Actor loss: 12.410895
Action reg: 0.003917
  l1.weight: grad_norm = 0.290035
  l1.bias: grad_norm = 0.000945
  l2.weight: grad_norm = 0.238587
Total gradient norm: 0.584557
=== Actor Training Debug (Iteration 7929) ===
Q mean: -13.555975
Q std: 15.257368
Actor loss: 13.559916
Action reg: 0.003942
  l1.weight: grad_norm = 0.205847
  l1.bias: grad_norm = 0.000602
  l2.weight: grad_norm = 0.185128
Total gradient norm: 0.460303
=== Actor Training Debug (Iteration 7930) ===
Q mean: -14.347267
Q std: 17.394918
Actor loss: 14.351224
Action reg: 0.003957
  l1.weight: grad_norm = 0.265902
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.220002
Total gradient norm: 0.539859
=== Actor Training Debug (Iteration 7931) ===
Q mean: -13.571003
Q std: 17.023783
Actor loss: 13.574949
Action reg: 0.003946
  l1.weight: grad_norm = 0.546987
  l1.bias: grad_norm = 0.000556
  l2.weight: grad_norm = 0.534333
Total gradient norm: 1.345310
=== Actor Training Debug (Iteration 7932) ===
Q mean: -12.318013
Q std: 15.132034
Actor loss: 12.321977
Action reg: 0.003963
  l1.weight: grad_norm = 0.244566
  l1.bias: grad_norm = 0.000274
  l2.weight: grad_norm = 0.223017
Total gradient norm: 0.616724
=== Actor Training Debug (Iteration 7933) ===
Q mean: -13.358796
Q std: 16.275730
Actor loss: 13.362747
Action reg: 0.003951
  l1.weight: grad_norm = 0.346801
  l1.bias: grad_norm = 0.000505
  l2.weight: grad_norm = 0.255860
Total gradient norm: 0.663195
=== Actor Training Debug (Iteration 7934) ===
Q mean: -11.708626
Q std: 11.904608
Actor loss: 11.712589
Action reg: 0.003963
  l1.weight: grad_norm = 0.226161
  l1.bias: grad_norm = 0.000084
  l2.weight: grad_norm = 0.191926
Total gradient norm: 0.498251
=== Actor Training Debug (Iteration 7935) ===
Q mean: -13.516989
Q std: 17.271152
Actor loss: 13.520922
Action reg: 0.003933
  l1.weight: grad_norm = 0.198628
  l1.bias: grad_norm = 0.000651
  l2.weight: grad_norm = 0.171652
Total gradient norm: 0.449294
=== Actor Training Debug (Iteration 7936) ===
Q mean: -12.823895
Q std: 15.725854
Actor loss: 12.827846
Action reg: 0.003951
  l1.weight: grad_norm = 0.191147
  l1.bias: grad_norm = 0.000267
  l2.weight: grad_norm = 0.164924
Total gradient norm: 0.442134
=== Actor Training Debug (Iteration 7937) ===
Q mean: -13.798565
Q std: 17.071339
Actor loss: 13.802524
Action reg: 0.003959
  l1.weight: grad_norm = 0.139092
  l1.bias: grad_norm = 0.000564
  l2.weight: grad_norm = 0.119258
Total gradient norm: 0.346344
=== Actor Training Debug (Iteration 7938) ===
Q mean: -13.884387
Q std: 17.589930
Actor loss: 13.888334
Action reg: 0.003947
  l1.weight: grad_norm = 0.339633
  l1.bias: grad_norm = 0.000160
  l2.weight: grad_norm = 0.258336
Total gradient norm: 0.619414
=== Actor Training Debug (Iteration 7939) ===
Q mean: -14.390933
Q std: 16.084017
Actor loss: 14.394889
Action reg: 0.003956
  l1.weight: grad_norm = 0.210217
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.177941
Total gradient norm: 0.487619
=== Actor Training Debug (Iteration 7940) ===
Q mean: -12.194752
Q std: 15.290309
Actor loss: 12.198704
Action reg: 0.003952
  l1.weight: grad_norm = 0.175818
  l1.bias: grad_norm = 0.001598
  l2.weight: grad_norm = 0.162267
Total gradient norm: 0.430609
=== Actor Training Debug (Iteration 7941) ===
Q mean: -14.404207
Q std: 16.343496
Actor loss: 14.408151
Action reg: 0.003943
  l1.weight: grad_norm = 0.176094
  l1.bias: grad_norm = 0.001692
  l2.weight: grad_norm = 0.152345
Total gradient norm: 0.383441
=== Actor Training Debug (Iteration 7942) ===
Q mean: -11.733061
Q std: 14.670427
Actor loss: 11.737012
Action reg: 0.003951
  l1.weight: grad_norm = 0.318249
  l1.bias: grad_norm = 0.000218
  l2.weight: grad_norm = 0.253786
Total gradient norm: 0.694146
=== Actor Training Debug (Iteration 7943) ===
Q mean: -12.748584
Q std: 16.121269
Actor loss: 12.752508
Action reg: 0.003924
  l1.weight: grad_norm = 0.696087
  l1.bias: grad_norm = 0.002212
  l2.weight: grad_norm = 0.502853
Total gradient norm: 1.271195
=== Actor Training Debug (Iteration 7944) ===
Q mean: -11.815434
Q std: 15.375766
Actor loss: 11.819371
Action reg: 0.003936
  l1.weight: grad_norm = 0.354992
  l1.bias: grad_norm = 0.000512
  l2.weight: grad_norm = 0.307393
Total gradient norm: 0.846841
=== Actor Training Debug (Iteration 7945) ===
Q mean: -13.028273
Q std: 15.779613
Actor loss: 13.032220
Action reg: 0.003947
  l1.weight: grad_norm = 0.177001
  l1.bias: grad_norm = 0.001300
  l2.weight: grad_norm = 0.155300
Total gradient norm: 0.439256
=== Actor Training Debug (Iteration 7946) ===
Q mean: -12.629515
Q std: 16.657536
Actor loss: 12.633476
Action reg: 0.003962
  l1.weight: grad_norm = 0.454253
  l1.bias: grad_norm = 0.000399
  l2.weight: grad_norm = 0.375210
Total gradient norm: 0.922938
=== Actor Training Debug (Iteration 7947) ===
Q mean: -13.074291
Q std: 16.582453
Actor loss: 13.078234
Action reg: 0.003942
  l1.weight: grad_norm = 0.280580
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.242840
Total gradient norm: 0.547269
=== Actor Training Debug (Iteration 7948) ===
Q mean: -12.994933
Q std: 16.868303
Actor loss: 12.998878
Action reg: 0.003945
  l1.weight: grad_norm = 0.202083
  l1.bias: grad_norm = 0.000482
  l2.weight: grad_norm = 0.163472
Total gradient norm: 0.417049
=== Actor Training Debug (Iteration 7949) ===
Q mean: -12.733341
Q std: 16.408684
Actor loss: 12.737294
Action reg: 0.003953
  l1.weight: grad_norm = 0.157339
  l1.bias: grad_norm = 0.000515
  l2.weight: grad_norm = 0.140452
Total gradient norm: 0.356681
=== Actor Training Debug (Iteration 7950) ===
Q mean: -14.818647
Q std: 17.603260
Actor loss: 14.822600
Action reg: 0.003953
  l1.weight: grad_norm = 0.219071
  l1.bias: grad_norm = 0.000338
  l2.weight: grad_norm = 0.174674
Total gradient norm: 0.474107
=== Actor Training Debug (Iteration 7951) ===
Q mean: -13.411911
Q std: 16.040989
Actor loss: 13.415848
Action reg: 0.003937
  l1.weight: grad_norm = 0.233783
  l1.bias: grad_norm = 0.000403
  l2.weight: grad_norm = 0.185754
Total gradient norm: 0.482768
=== Actor Training Debug (Iteration 7952) ===
Q mean: -12.663263
Q std: 16.206255
Actor loss: 12.667190
Action reg: 0.003926
  l1.weight: grad_norm = 0.182087
  l1.bias: grad_norm = 0.000603
  l2.weight: grad_norm = 0.153297
Total gradient norm: 0.430578
=== Actor Training Debug (Iteration 7953) ===
Q mean: -14.071005
Q std: 17.553947
Actor loss: 14.074965
Action reg: 0.003959
  l1.weight: grad_norm = 0.129956
  l1.bias: grad_norm = 0.000209
  l2.weight: grad_norm = 0.109783
Total gradient norm: 0.307100
=== Actor Training Debug (Iteration 7954) ===
Q mean: -13.824461
Q std: 17.268549
Actor loss: 13.828379
Action reg: 0.003917
  l1.weight: grad_norm = 0.245094
  l1.bias: grad_norm = 0.001262
  l2.weight: grad_norm = 0.195681
Total gradient norm: 0.507446
=== Actor Training Debug (Iteration 7955) ===
Q mean: -12.702096
Q std: 16.074926
Actor loss: 12.706017
Action reg: 0.003922
  l1.weight: grad_norm = 0.493037
  l1.bias: grad_norm = 0.001501
  l2.weight: grad_norm = 0.376627
Total gradient norm: 0.938498
=== Actor Training Debug (Iteration 7956) ===
Q mean: -12.591312
Q std: 15.687072
Actor loss: 12.595237
Action reg: 0.003925
  l1.weight: grad_norm = 0.267085
  l1.bias: grad_norm = 0.000687
  l2.weight: grad_norm = 0.244176
Total gradient norm: 0.662205
=== Actor Training Debug (Iteration 7957) ===
Q mean: -11.851989
Q std: 15.080029
Actor loss: 11.855901
Action reg: 0.003912
  l1.weight: grad_norm = 0.207528
  l1.bias: grad_norm = 0.001512
  l2.weight: grad_norm = 0.169782
Total gradient norm: 0.462402
=== Actor Training Debug (Iteration 7958) ===
Q mean: -12.160465
Q std: 14.047603
Actor loss: 12.164419
Action reg: 0.003954
  l1.weight: grad_norm = 0.986392
  l1.bias: grad_norm = 0.000369
  l2.weight: grad_norm = 0.771877
Total gradient norm: 2.092169
=== Actor Training Debug (Iteration 7959) ===
Q mean: -12.065113
Q std: 16.065107
Actor loss: 12.069064
Action reg: 0.003951
  l1.weight: grad_norm = 0.547135
  l1.bias: grad_norm = 0.000258
  l2.weight: grad_norm = 0.476793
Total gradient norm: 1.178842
=== Actor Training Debug (Iteration 7960) ===
Q mean: -12.230434
Q std: 14.779196
Actor loss: 12.234385
Action reg: 0.003950
  l1.weight: grad_norm = 0.330045
  l1.bias: grad_norm = 0.000321
  l2.weight: grad_norm = 0.288840
Total gradient norm: 0.770542
=== Actor Training Debug (Iteration 7961) ===
Q mean: -12.551929
Q std: 15.574400
Actor loss: 12.555836
Action reg: 0.003906
  l1.weight: grad_norm = 0.342391
  l1.bias: grad_norm = 0.000624
  l2.weight: grad_norm = 0.297528
Total gradient norm: 0.842518
=== Actor Training Debug (Iteration 7962) ===
Q mean: -10.800714
Q std: 14.291292
Actor loss: 10.804673
Action reg: 0.003960
  l1.weight: grad_norm = 0.238117
  l1.bias: grad_norm = 0.000323
  l2.weight: grad_norm = 0.214339
Total gradient norm: 0.655971
=== Actor Training Debug (Iteration 7963) ===
Q mean: -13.628750
Q std: 16.764618
Actor loss: 13.632704
Action reg: 0.003954
  l1.weight: grad_norm = 0.192157
  l1.bias: grad_norm = 0.000451
  l2.weight: grad_norm = 0.149698
Total gradient norm: 0.392108
=== Actor Training Debug (Iteration 7964) ===
Q mean: -13.445217
Q std: 16.709940
Actor loss: 13.449170
Action reg: 0.003953
  l1.weight: grad_norm = 0.236957
  l1.bias: grad_norm = 0.000264
  l2.weight: grad_norm = 0.198940
Total gradient norm: 0.545730
=== Actor Training Debug (Iteration 7965) ===
Q mean: -14.310416
Q std: 18.058826
Actor loss: 14.314353
Action reg: 0.003936
  l1.weight: grad_norm = 0.327439
  l1.bias: grad_norm = 0.000634
  l2.weight: grad_norm = 0.257419
Total gradient norm: 0.624660
=== Actor Training Debug (Iteration 7966) ===
Q mean: -13.763756
Q std: 17.703142
Actor loss: 13.767712
Action reg: 0.003956
  l1.weight: grad_norm = 0.294663
  l1.bias: grad_norm = 0.000227
  l2.weight: grad_norm = 0.261509
Total gradient norm: 0.632893
=== Actor Training Debug (Iteration 7967) ===
Q mean: -12.254983
Q std: 16.155273
Actor loss: 12.258913
Action reg: 0.003930
  l1.weight: grad_norm = 0.402529
  l1.bias: grad_norm = 0.000296
  l2.weight: grad_norm = 0.314074
Total gradient norm: 0.827025
=== Actor Training Debug (Iteration 7968) ===
Q mean: -11.773735
Q std: 14.381537
Actor loss: 11.777667
Action reg: 0.003932
  l1.weight: grad_norm = 0.513957
  l1.bias: grad_norm = 0.000684
  l2.weight: grad_norm = 0.409389
Total gradient norm: 0.996334
=== Actor Training Debug (Iteration 7969) ===
Q mean: -11.787434
Q std: 14.974104
Actor loss: 11.791388
Action reg: 0.003954
  l1.weight: grad_norm = 0.221675
  l1.bias: grad_norm = 0.000286
  l2.weight: grad_norm = 0.193071
Total gradient norm: 0.580408
=== Actor Training Debug (Iteration 7970) ===
Q mean: -13.468552
Q std: 15.418951
Actor loss: 13.472488
Action reg: 0.003937
  l1.weight: grad_norm = 0.253599
  l1.bias: grad_norm = 0.000772
  l2.weight: grad_norm = 0.210016
Total gradient norm: 0.472039
=== Actor Training Debug (Iteration 7971) ===
Q mean: -12.771820
Q std: 17.092550
Actor loss: 12.775749
Action reg: 0.003929
  l1.weight: grad_norm = 0.245011
  l1.bias: grad_norm = 0.001573
  l2.weight: grad_norm = 0.213969
Total gradient norm: 0.479755
=== Actor Training Debug (Iteration 7972) ===
Q mean: -14.011078
Q std: 18.816900
Actor loss: 14.015027
Action reg: 0.003949
  l1.weight: grad_norm = 0.237825
  l1.bias: grad_norm = 0.001897
  l2.weight: grad_norm = 0.195007
Total gradient norm: 0.512139
=== Actor Training Debug (Iteration 7973) ===
Q mean: -13.135024
Q std: 16.391710
Actor loss: 13.138979
Action reg: 0.003955
  l1.weight: grad_norm = 0.264143
  l1.bias: grad_norm = 0.000243
  l2.weight: grad_norm = 0.246454
Total gradient norm: 0.592257
=== Actor Training Debug (Iteration 7974) ===
Q mean: -12.725367
Q std: 16.100073
Actor loss: 12.729322
Action reg: 0.003956
  l1.weight: grad_norm = 0.326329
  l1.bias: grad_norm = 0.000203
  l2.weight: grad_norm = 0.301617
Total gradient norm: 0.730508
=== Actor Training Debug (Iteration 7975) ===
Q mean: -12.489470
Q std: 16.141388
Actor loss: 12.493429
Action reg: 0.003960
  l1.weight: grad_norm = 0.295698
  l1.bias: grad_norm = 0.001440
  l2.weight: grad_norm = 0.239067
Total gradient norm: 0.586871
=== Actor Training Debug (Iteration 7976) ===
Q mean: -13.150676
Q std: 14.979259
Actor loss: 13.154631
Action reg: 0.003955
  l1.weight: grad_norm = 0.166728
  l1.bias: grad_norm = 0.001790
  l2.weight: grad_norm = 0.134683
Total gradient norm: 0.351543
=== Actor Training Debug (Iteration 7977) ===
Q mean: -13.101114
Q std: 15.377736
Actor loss: 13.105055
Action reg: 0.003941
  l1.weight: grad_norm = 0.276296
  l1.bias: grad_norm = 0.003163
  l2.weight: grad_norm = 0.237798
Total gradient norm: 0.599691
=== Actor Training Debug (Iteration 7978) ===
Q mean: -12.594858
Q std: 14.598170
Actor loss: 12.598804
Action reg: 0.003945
  l1.weight: grad_norm = 0.244036
  l1.bias: grad_norm = 0.000294
  l2.weight: grad_norm = 0.217698
Total gradient norm: 0.625111
=== Actor Training Debug (Iteration 7979) ===
Q mean: -13.744724
Q std: 16.454266
Actor loss: 13.748698
Action reg: 0.003974
  l1.weight: grad_norm = 0.166970
  l1.bias: grad_norm = 0.000072
  l2.weight: grad_norm = 0.129567
Total gradient norm: 0.310678
=== Actor Training Debug (Iteration 7980) ===
Q mean: -12.190235
Q std: 15.154649
Actor loss: 12.194198
Action reg: 0.003963
  l1.weight: grad_norm = 0.143048
  l1.bias: grad_norm = 0.000157
  l2.weight: grad_norm = 0.114777
Total gradient norm: 0.316491
=== Actor Training Debug (Iteration 7981) ===
Q mean: -13.183996
Q std: 14.771296
Actor loss: 13.187955
Action reg: 0.003959
  l1.weight: grad_norm = 0.320465
  l1.bias: grad_norm = 0.000318
  l2.weight: grad_norm = 0.287485
Total gradient norm: 0.748715
=== Actor Training Debug (Iteration 7982) ===
Q mean: -14.805032
Q std: 17.463448
Actor loss: 14.808967
Action reg: 0.003934
  l1.weight: grad_norm = 0.141418
  l1.bias: grad_norm = 0.000374
  l2.weight: grad_norm = 0.126120
Total gradient norm: 0.294991
=== Actor Training Debug (Iteration 7983) ===
Q mean: -12.764395
Q std: 14.884873
Actor loss: 12.768316
Action reg: 0.003922
  l1.weight: grad_norm = 0.549733
  l1.bias: grad_norm = 0.000793
  l2.weight: grad_norm = 0.393643
Total gradient norm: 1.065583
=== Actor Training Debug (Iteration 7984) ===
Q mean: -13.319611
Q std: 17.883553
Actor loss: 13.323544
Action reg: 0.003933
  l1.weight: grad_norm = 0.298491
  l1.bias: grad_norm = 0.001322
  l2.weight: grad_norm = 0.248120
Total gradient norm: 0.631887
=== Actor Training Debug (Iteration 7985) ===
Q mean: -12.048210
Q std: 14.194033
Actor loss: 12.052145
Action reg: 0.003935
  l1.weight: grad_norm = 0.391832
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.308531
Total gradient norm: 0.792405
=== Actor Training Debug (Iteration 7986) ===
Q mean: -13.322014
Q std: 16.655092
Actor loss: 13.325963
Action reg: 0.003949
  l1.weight: grad_norm = 0.734813
  l1.bias: grad_norm = 0.000409
  l2.weight: grad_norm = 0.617311
Total gradient norm: 1.697768
=== Actor Training Debug (Iteration 7987) ===
Q mean: -12.233536
Q std: 15.424828
Actor loss: 12.237463
Action reg: 0.003927
  l1.weight: grad_norm = 0.382616
  l1.bias: grad_norm = 0.000460
  l2.weight: grad_norm = 0.315530
Total gradient norm: 0.822726
=== Actor Training Debug (Iteration 7988) ===
Q mean: -12.671100
Q std: 15.669827
Actor loss: 12.675056
Action reg: 0.003956
  l1.weight: grad_norm = 0.150248
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.126940
Total gradient norm: 0.329740
=== Actor Training Debug (Iteration 7989) ===
Q mean: -12.374846
Q std: 15.829095
Actor loss: 12.378778
Action reg: 0.003933
  l1.weight: grad_norm = 0.247257
  l1.bias: grad_norm = 0.001624
  l2.weight: grad_norm = 0.234773
Total gradient norm: 0.622085
=== Actor Training Debug (Iteration 7990) ===
Q mean: -12.925594
Q std: 14.956399
Actor loss: 12.929546
Action reg: 0.003952
  l1.weight: grad_norm = 0.221914
  l1.bias: grad_norm = 0.000317
  l2.weight: grad_norm = 0.189880
Total gradient norm: 0.477888
=== Actor Training Debug (Iteration 7991) ===
Q mean: -11.762176
Q std: 16.105679
Actor loss: 11.766118
Action reg: 0.003943
  l1.weight: grad_norm = 0.267662
  l1.bias: grad_norm = 0.001372
  l2.weight: grad_norm = 0.221899
Total gradient norm: 0.590004
=== Actor Training Debug (Iteration 7992) ===
Q mean: -11.765707
Q std: 15.757033
Actor loss: 11.769641
Action reg: 0.003934
  l1.weight: grad_norm = 0.272666
  l1.bias: grad_norm = 0.000366
  l2.weight: grad_norm = 0.213443
Total gradient norm: 0.528529
=== Actor Training Debug (Iteration 7993) ===
Q mean: -11.306360
Q std: 14.499722
Actor loss: 11.310301
Action reg: 0.003941
  l1.weight: grad_norm = 0.395503
  l1.bias: grad_norm = 0.000348
  l2.weight: grad_norm = 0.334583
Total gradient norm: 0.886426
=== Actor Training Debug (Iteration 7994) ===
Q mean: -13.475588
Q std: 15.803453
Actor loss: 13.479537
Action reg: 0.003949
  l1.weight: grad_norm = 0.230479
  l1.bias: grad_norm = 0.000520
  l2.weight: grad_norm = 0.189377
Total gradient norm: 0.490482
=== Actor Training Debug (Iteration 7995) ===
Q mean: -11.924811
Q std: 15.531215
Actor loss: 11.928765
Action reg: 0.003953
  l1.weight: grad_norm = 0.266604
  l1.bias: grad_norm = 0.000401
  l2.weight: grad_norm = 0.235249
Total gradient norm: 0.620878
=== Actor Training Debug (Iteration 7996) ===
Q mean: -13.752310
Q std: 16.094749
Actor loss: 13.756251
Action reg: 0.003942
  l1.weight: grad_norm = 0.374294
  l1.bias: grad_norm = 0.000319
  l2.weight: grad_norm = 0.348381
Total gradient norm: 0.874377
=== Actor Training Debug (Iteration 7997) ===
Q mean: -13.386708
Q std: 16.014957
Actor loss: 13.390679
Action reg: 0.003971
  l1.weight: grad_norm = 0.425148
  l1.bias: grad_norm = 0.000287
  l2.weight: grad_norm = 0.366950
Total gradient norm: 0.860522
=== Actor Training Debug (Iteration 7998) ===
Q mean: -13.797153
Q std: 16.585972
Actor loss: 13.801110
Action reg: 0.003957
  l1.weight: grad_norm = 0.297602
  l1.bias: grad_norm = 0.000214
  l2.weight: grad_norm = 0.271776
Total gradient norm: 0.700268
=== Actor Training Debug (Iteration 7999) ===
Q mean: -14.737055
Q std: 17.887569
Actor loss: 14.741003
Action reg: 0.003948
  l1.weight: grad_norm = 0.363186
  l1.bias: grad_norm = 0.000458
  l2.weight: grad_norm = 0.294720
Total gradient norm: 0.753174
=== Actor Training Debug (Iteration 8000) ===
Q mean: -12.678588
Q std: 14.930913
Actor loss: 12.682545
Action reg: 0.003957
  l1.weight: grad_norm = 0.190611
  l1.bias: grad_norm = 0.000182
  l2.weight: grad_norm = 0.162209
Total gradient norm: 0.399756
Step 13000: Critic Loss: 1.4416, Actor Loss: 12.6825, Q Value: -12.6786
  Average reward: -324.404 | Average length: 100.0
Evaluation at episode 130: -324.404
=== Actor Training Debug (Iteration 8001) ===
Q mean: -12.178501
Q std: 13.123650
Actor loss: 12.182453
Action reg: 0.003952
  l1.weight: grad_norm = 0.154807
  l1.bias: grad_norm = 0.000832
  l2.weight: grad_norm = 0.128521
Total gradient norm: 0.324789
=== Actor Training Debug (Iteration 8002) ===
Q mean: -13.235753
Q std: 16.147026
Actor loss: 13.239695
Action reg: 0.003941
  l1.weight: grad_norm = 0.335241
  l1.bias: grad_norm = 0.000261
  l2.weight: grad_norm = 0.268631
Total gradient norm: 0.686212
=== Actor Training Debug (Iteration 8003) ===
Q mean: -11.539692
Q std: 13.776880
Actor loss: 11.543656
Action reg: 0.003965
  l1.weight: grad_norm = 0.237545
  l1.bias: grad_norm = 0.000233
  l2.weight: grad_norm = 0.205218
Total gradient norm: 0.525145
=== Actor Training Debug (Iteration 8004) ===
Q mean: -10.879141
Q std: 13.210711
Actor loss: 10.883075
Action reg: 0.003934
  l1.weight: grad_norm = 0.143235
  l1.bias: grad_norm = 0.000695
  l2.weight: grad_norm = 0.130073
Total gradient norm: 0.329800
=== Actor Training Debug (Iteration 8005) ===
Q mean: -13.031633
Q std: 17.009054
Actor loss: 13.035568
Action reg: 0.003935
  l1.weight: grad_norm = 0.245349
  l1.bias: grad_norm = 0.000284
  l2.weight: grad_norm = 0.189399
Total gradient norm: 0.492459
=== Actor Training Debug (Iteration 8006) ===
Q mean: -14.103799
Q std: 17.165987
Actor loss: 14.107757
Action reg: 0.003957
  l1.weight: grad_norm = 0.444277
  l1.bias: grad_norm = 0.000380
  l2.weight: grad_norm = 0.421493
Total gradient norm: 1.022427
=== Actor Training Debug (Iteration 8007) ===
Q mean: -13.719136
Q std: 14.688526
Actor loss: 13.723087
Action reg: 0.003951
  l1.weight: grad_norm = 0.168374
  l1.bias: grad_norm = 0.000316
  l2.weight: grad_norm = 0.157185
Total gradient norm: 0.426683
=== Actor Training Debug (Iteration 8008) ===
Q mean: -13.890930
Q std: 16.520626
Actor loss: 13.894897
Action reg: 0.003968
  l1.weight: grad_norm = 0.306711
  l1.bias: grad_norm = 0.000124
  l2.weight: grad_norm = 0.259605
Total gradient norm: 0.647189
=== Actor Training Debug (Iteration 8009) ===
Q mean: -12.553874
Q std: 13.366660
Actor loss: 12.557835
Action reg: 0.003960
  l1.weight: grad_norm = 0.232074
  l1.bias: grad_norm = 0.000430
  l2.weight: grad_norm = 0.192303
Total gradient norm: 0.520520
=== Actor Training Debug (Iteration 8010) ===
Q mean: -13.363194
Q std: 18.071852
Actor loss: 13.367161
Action reg: 0.003968
  l1.weight: grad_norm = 0.222528
  l1.bias: grad_norm = 0.000138
  l2.weight: grad_norm = 0.205482
Total gradient norm: 0.556758
=== Actor Training Debug (Iteration 8011) ===
Q mean: -12.866447
Q std: 16.068830
Actor loss: 12.870394
Action reg: 0.003947
  l1.weight: grad_norm = 0.231386
  l1.bias: grad_norm = 0.000678
  l2.weight: grad_norm = 0.205938
Total gradient norm: 0.509061
=== Actor Training Debug (Iteration 8012) ===
Q mean: -12.396280
Q std: 16.211136
Actor loss: 12.400237
Action reg: 0.003956
  l1.weight: grad_norm = 0.350973
  l1.bias: grad_norm = 0.000188
  l2.weight: grad_norm = 0.294042
Total gradient norm: 0.723085
=== Actor Training Debug (Iteration 8013) ===
Q mean: -14.005268
Q std: 16.210142
Actor loss: 14.009230
Action reg: 0.003962
  l1.weight: grad_norm = 0.160472
  l1.bias: grad_norm = 0.000519
  l2.weight: grad_norm = 0.141929
Total gradient norm: 0.373329
=== Actor Training Debug (Iteration 8014) ===
Q mean: -12.297230
Q std: 14.898568
Actor loss: 12.301169
Action reg: 0.003940
  l1.weight: grad_norm = 0.189925
  l1.bias: grad_norm = 0.000379
  l2.weight: grad_norm = 0.175083
Total gradient norm: 0.431299
=== Actor Training Debug (Iteration 8015) ===
Q mean: -11.789400
Q std: 15.742723
Actor loss: 11.793341
Action reg: 0.003940
  l1.weight: grad_norm = 0.730766
  l1.bias: grad_norm = 0.000760
  l2.weight: grad_norm = 0.630821
Total gradient norm: 1.625111
=== Actor Training Debug (Iteration 8016) ===
Q mean: -13.849443
Q std: 16.167528
Actor loss: 13.853381
Action reg: 0.003938
  l1.weight: grad_norm = 0.465335
  l1.bias: grad_norm = 0.000419
  l2.weight: grad_norm = 0.396004
Total gradient norm: 0.994406
=== Actor Training Debug (Iteration 8017) ===
Q mean: -13.403141
Q std: 16.948135
Actor loss: 13.407101
Action reg: 0.003959
  l1.weight: grad_norm = 0.292052
  l1.bias: grad_norm = 0.000434
  l2.weight: grad_norm = 0.249171
Total gradient norm: 0.559617
=== Actor Training Debug (Iteration 8018) ===
Q mean: -15.141001
Q std: 16.616261
Actor loss: 15.144945
Action reg: 0.003944
  l1.weight: grad_norm = 0.284447
  l1.bias: grad_norm = 0.001144
  l2.weight: grad_norm = 0.246022
Total gradient norm: 0.575126
=== Actor Training Debug (Iteration 8019) ===
Q mean: -13.698553
Q std: 15.352995
Actor loss: 13.702505
Action reg: 0.003952
  l1.weight: grad_norm = 0.390575
  l1.bias: grad_norm = 0.000345
  l2.weight: grad_norm = 0.371454
Total gradient norm: 0.821906
=== Actor Training Debug (Iteration 8020) ===
Q mean: -12.343993
Q std: 15.394056
Actor loss: 12.347926
Action reg: 0.003933
  l1.weight: grad_norm = 0.274540
  l1.bias: grad_norm = 0.000662
  l2.weight: grad_norm = 0.217789
Total gradient norm: 0.599337
=== Actor Training Debug (Iteration 8021) ===
Q mean: -11.094514
Q std: 14.317620
Actor loss: 11.098466
Action reg: 0.003952
  l1.weight: grad_norm = 0.453208
  l1.bias: grad_norm = 0.000347
  l2.weight: grad_norm = 0.449017
Total gradient norm: 1.328443
=== Actor Training Debug (Iteration 8022) ===
Q mean: -10.916502
Q std: 14.138194
Actor loss: 10.920439
Action reg: 0.003936
  l1.weight: grad_norm = 0.237356
  l1.bias: grad_norm = 0.001160
  l2.weight: grad_norm = 0.196117
Total gradient norm: 0.477239
=== Actor Training Debug (Iteration 8023) ===
Q mean: -12.045936
Q std: 14.562539
Actor loss: 12.049859
Action reg: 0.003923
  l1.weight: grad_norm = 0.183829
  l1.bias: grad_norm = 0.000772
  l2.weight: grad_norm = 0.165423
Total gradient norm: 0.404594
=== Actor Training Debug (Iteration 8024) ===
Q mean: -11.715483
Q std: 13.440399
Actor loss: 11.719424
Action reg: 0.003941
  l1.weight: grad_norm = 0.332177
  l1.bias: grad_norm = 0.000660
  l2.weight: grad_norm = 0.302216
Total gradient norm: 0.781526
=== Actor Training Debug (Iteration 8025) ===
Q mean: -13.302818
Q std: 15.362685
Actor loss: 13.306745
Action reg: 0.003927
  l1.weight: grad_norm = 0.398839
  l1.bias: grad_norm = 0.000538
  l2.weight: grad_norm = 0.374906
Total gradient norm: 0.962690
=== Actor Training Debug (Iteration 8026) ===
Q mean: -13.034655
Q std: 16.678522
Actor loss: 13.038585
Action reg: 0.003931
  l1.weight: grad_norm = 0.446974
  l1.bias: grad_norm = 0.000421
  l2.weight: grad_norm = 0.358669
Total gradient norm: 0.911267
=== Actor Training Debug (Iteration 8027) ===
Q mean: -11.792763
Q std: 14.390044
Actor loss: 11.796694
Action reg: 0.003931
  l1.weight: grad_norm = 0.306893
  l1.bias: grad_norm = 0.000459
  l2.weight: grad_norm = 0.251434
Total gradient norm: 0.620021
=== Actor Training Debug (Iteration 8028) ===
Q mean: -15.420109
Q std: 18.871992
Actor loss: 15.424066
Action reg: 0.003956
  l1.weight: grad_norm = 0.204791
  l1.bias: grad_norm = 0.000476
  l2.weight: grad_norm = 0.163647
Total gradient norm: 0.405899
=== Actor Training Debug (Iteration 8029) ===
Q mean: -12.758216
Q std: 17.657846
Actor loss: 12.762169
Action reg: 0.003953
  l1.weight: grad_norm = 0.301395
  l1.bias: grad_norm = 0.000326
  l2.weight: grad_norm = 0.244185
Total gradient norm: 0.624735
=== Actor Training Debug (Iteration 8030) ===
Q mean: -13.016935
Q std: 14.572659
Actor loss: 13.020890
Action reg: 0.003955
  l1.weight: grad_norm = 0.225699
  l1.bias: grad_norm = 0.000195
  l2.weight: grad_norm = 0.189582
Total gradient norm: 0.437736
=== Actor Training Debug (Iteration 8031) ===
Q mean: -14.494360
Q std: 16.820665
Actor loss: 14.498302
Action reg: 0.003941
  l1.weight: grad_norm = 0.232328
  l1.bias: grad_norm = 0.000593
  l2.weight: grad_norm = 0.194280
Total gradient norm: 0.540440
=== Actor Training Debug (Iteration 8032) ===
Q mean: -11.467757
Q std: 15.139464
Actor loss: 11.471690
Action reg: 0.003933
  l1.weight: grad_norm = 0.283878
  l1.bias: grad_norm = 0.000722
  l2.weight: grad_norm = 0.257014
Total gradient norm: 0.674612
=== Actor Training Debug (Iteration 8033) ===
Q mean: -14.336243
Q std: 16.703672
Actor loss: 14.340190
Action reg: 0.003947
  l1.weight: grad_norm = 0.275565
  l1.bias: grad_norm = 0.000385
  l2.weight: grad_norm = 0.244148
Total gradient norm: 0.637737
=== Actor Training Debug (Iteration 8034) ===
Q mean: -11.897365
Q std: 13.765050
Actor loss: 11.901319
Action reg: 0.003954
  l1.weight: grad_norm = 0.370528
  l1.bias: grad_norm = 0.000621
  l2.weight: grad_norm = 0.319007
Total gradient norm: 0.805048
=== Actor Training Debug (Iteration 8035) ===
Q mean: -14.282109
Q std: 16.732393
Actor loss: 14.286050
Action reg: 0.003941
  l1.weight: grad_norm = 0.258902
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.219228
Total gradient norm: 0.627418
=== Actor Training Debug (Iteration 8036) ===
Q mean: -11.395851
Q std: 14.360010
Actor loss: 11.399803
Action reg: 0.003952
  l1.weight: grad_norm = 0.327782
  l1.bias: grad_norm = 0.000303
  l2.weight: grad_norm = 0.283697
Total gradient norm: 0.752930
=== Actor Training Debug (Iteration 8037) ===
Q mean: -13.611824
Q std: 18.024162
Actor loss: 13.615789
Action reg: 0.003965
  l1.weight: grad_norm = 0.298669
  l1.bias: grad_norm = 0.000114
  l2.weight: grad_norm = 0.237124
Total gradient norm: 0.558514
=== Actor Training Debug (Iteration 8038) ===
Q mean: -13.147869
Q std: 16.287029
Actor loss: 13.151826
Action reg: 0.003957
  l1.weight: grad_norm = 0.141866
  l1.bias: grad_norm = 0.000712
  l2.weight: grad_norm = 0.117995
Total gradient norm: 0.317765
=== Actor Training Debug (Iteration 8039) ===
Q mean: -10.804878
Q std: 11.496140
Actor loss: 10.808813
Action reg: 0.003935
  l1.weight: grad_norm = 0.259701
  l1.bias: grad_norm = 0.000880
  l2.weight: grad_norm = 0.210841
Total gradient norm: 0.573892
=== Actor Training Debug (Iteration 8040) ===
Q mean: -13.875576
Q std: 15.463162
Actor loss: 13.879542
Action reg: 0.003967
  l1.weight: grad_norm = 0.411942
  l1.bias: grad_norm = 0.000187
  l2.weight: grad_norm = 0.352503
Total gradient norm: 0.834904
=== Actor Training Debug (Iteration 8041) ===
Q mean: -13.040409
Q std: 14.740077
Actor loss: 13.044349
Action reg: 0.003940
  l1.weight: grad_norm = 0.318175
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.300508
Total gradient norm: 0.718296
=== Actor Training Debug (Iteration 8042) ===
Q mean: -12.030462
Q std: 14.635160
Actor loss: 12.034390
Action reg: 0.003929
  l1.weight: grad_norm = 0.414097
  l1.bias: grad_norm = 0.001315
  l2.weight: grad_norm = 0.339528
Total gradient norm: 1.019889
=== Actor Training Debug (Iteration 8043) ===
Q mean: -11.823547
Q std: 14.301414
Actor loss: 11.827497
Action reg: 0.003949
  l1.weight: grad_norm = 0.399345
  l1.bias: grad_norm = 0.000172
  l2.weight: grad_norm = 0.342645
Total gradient norm: 0.883742
=== Actor Training Debug (Iteration 8044) ===
Q mean: -13.283443
Q std: 16.595016
Actor loss: 13.287376
Action reg: 0.003933
  l1.weight: grad_norm = 0.297326
  l1.bias: grad_norm = 0.000619
  l2.weight: grad_norm = 0.249026
Total gradient norm: 0.655794
=== Actor Training Debug (Iteration 8045) ===
Q mean: -12.696701
Q std: 14.446093
Actor loss: 12.700637
Action reg: 0.003936
  l1.weight: grad_norm = 0.693711
  l1.bias: grad_norm = 0.000662
  l2.weight: grad_norm = 0.549930
Total gradient norm: 1.474291
=== Actor Training Debug (Iteration 8046) ===
Q mean: -11.546309
Q std: 14.606038
Actor loss: 11.550249
Action reg: 0.003940
  l1.weight: grad_norm = 0.312348
  l1.bias: grad_norm = 0.000701
  l2.weight: grad_norm = 0.253486
Total gradient norm: 0.622838
=== Actor Training Debug (Iteration 8047) ===
Q mean: -11.985132
Q std: 13.616000
Actor loss: 11.989051
Action reg: 0.003919
  l1.weight: grad_norm = 0.426384
  l1.bias: grad_norm = 0.000535
  l2.weight: grad_norm = 0.364079
Total gradient norm: 0.905145
=== Actor Training Debug (Iteration 8048) ===
Q mean: -13.130401
Q std: 15.841077
Actor loss: 13.134332
Action reg: 0.003931
  l1.weight: grad_norm = 0.296247
  l1.bias: grad_norm = 0.000695
  l2.weight: grad_norm = 0.264698
Total gradient norm: 0.723974
=== Actor Training Debug (Iteration 8049) ===
Q mean: -11.295266
Q std: 15.638084
Actor loss: 11.299208
Action reg: 0.003942
  l1.weight: grad_norm = 0.234984
  l1.bias: grad_norm = 0.000278
  l2.weight: grad_norm = 0.209367
Total gradient norm: 0.491141
=== Actor Training Debug (Iteration 8050) ===
Q mean: -13.008536
Q std: 14.581027
Actor loss: 13.012492
Action reg: 0.003956
  l1.weight: grad_norm = 0.164852
  l1.bias: grad_norm = 0.000223
  l2.weight: grad_norm = 0.141449
Total gradient norm: 0.314942
=== Actor Training Debug (Iteration 8051) ===
Q mean: -12.959344
Q std: 16.388088
Actor loss: 12.963279
Action reg: 0.003935
  l1.weight: grad_norm = 0.900119
  l1.bias: grad_norm = 0.001683
  l2.weight: grad_norm = 0.668009
Total gradient norm: 1.962647
=== Actor Training Debug (Iteration 8052) ===
Q mean: -14.320307
Q std: 15.452402
Actor loss: 14.324261
Action reg: 0.003954
  l1.weight: grad_norm = 0.188606
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.169459
Total gradient norm: 0.403407
=== Actor Training Debug (Iteration 8053) ===
Q mean: -14.569889
Q std: 18.346416
Actor loss: 14.573817
Action reg: 0.003929
  l1.weight: grad_norm = 0.225409
  l1.bias: grad_norm = 0.000517
  l2.weight: grad_norm = 0.193944
Total gradient norm: 0.517158
=== Actor Training Debug (Iteration 8054) ===
Q mean: -13.540314
Q std: 17.687191
Actor loss: 13.544252
Action reg: 0.003939
  l1.weight: grad_norm = 0.138493
  l1.bias: grad_norm = 0.000466
  l2.weight: grad_norm = 0.123277
Total gradient norm: 0.301670
=== Actor Training Debug (Iteration 8055) ===
Q mean: -11.484068
Q std: 13.912920
Actor loss: 11.487994
Action reg: 0.003926
  l1.weight: grad_norm = 0.376634
  l1.bias: grad_norm = 0.000563
  l2.weight: grad_norm = 0.278203
Total gradient norm: 0.724615
=== Actor Training Debug (Iteration 8056) ===
Q mean: -14.129271
Q std: 15.883553
Actor loss: 14.133223
Action reg: 0.003952
  l1.weight: grad_norm = 0.272570
  l1.bias: grad_norm = 0.000252
  l2.weight: grad_norm = 0.255205
Total gradient norm: 0.742352
=== Actor Training Debug (Iteration 8057) ===
Q mean: -14.008305
Q std: 17.304749
Actor loss: 14.012247
Action reg: 0.003942
  l1.weight: grad_norm = 0.356517
  l1.bias: grad_norm = 0.001402
  l2.weight: grad_norm = 0.294172
Total gradient norm: 0.745545
=== Actor Training Debug (Iteration 8058) ===
Q mean: -12.836954
Q std: 14.549566
Actor loss: 12.840906
Action reg: 0.003952
  l1.weight: grad_norm = 0.236402
  l1.bias: grad_norm = 0.000813
  l2.weight: grad_norm = 0.196010
Total gradient norm: 0.554369
=== Actor Training Debug (Iteration 8059) ===
Q mean: -13.289909
Q std: 16.436527
Actor loss: 13.293866
Action reg: 0.003957
  l1.weight: grad_norm = 0.547730
  l1.bias: grad_norm = 0.001498
  l2.weight: grad_norm = 0.452923
Total gradient norm: 1.089577
=== Actor Training Debug (Iteration 8060) ===
Q mean: -13.243145
Q std: 16.339884
Actor loss: 13.247061
Action reg: 0.003916
  l1.weight: grad_norm = 0.303756
  l1.bias: grad_norm = 0.001073
  l2.weight: grad_norm = 0.259650
Total gradient norm: 0.731331
=== Actor Training Debug (Iteration 8061) ===
Q mean: -12.934580
Q std: 15.886955
Actor loss: 12.938540
Action reg: 0.003960
  l1.weight: grad_norm = 0.251648
  l1.bias: grad_norm = 0.000381
  l2.weight: grad_norm = 0.236608
Total gradient norm: 0.543553
=== Actor Training Debug (Iteration 8062) ===
Q mean: -13.635641
Q std: 17.635069
Actor loss: 13.639593
Action reg: 0.003952
  l1.weight: grad_norm = 0.292472
  l1.bias: grad_norm = 0.000378
  l2.weight: grad_norm = 0.229176
Total gradient norm: 0.538296
=== Actor Training Debug (Iteration 8063) ===
Q mean: -11.604547
Q std: 13.423706
Actor loss: 11.608490
Action reg: 0.003943
  l1.weight: grad_norm = 0.397596
  l1.bias: grad_norm = 0.000455
  l2.weight: grad_norm = 0.352632
Total gradient norm: 0.878392
=== Actor Training Debug (Iteration 8064) ===
Q mean: -13.656637
Q std: 17.180487
Actor loss: 13.660564
Action reg: 0.003927
  l1.weight: grad_norm = 0.291444
  l1.bias: grad_norm = 0.000496
  l2.weight: grad_norm = 0.270762
Total gradient norm: 0.635706
=== Actor Training Debug (Iteration 8065) ===
Q mean: -11.733177
Q std: 15.717933
Actor loss: 11.737101
Action reg: 0.003923
  l1.weight: grad_norm = 0.291738
  l1.bias: grad_norm = 0.000648
  l2.weight: grad_norm = 0.256541
Total gradient norm: 0.710967
=== Actor Training Debug (Iteration 8066) ===
Q mean: -14.553567
Q std: 17.139160
Actor loss: 14.557506
Action reg: 0.003939
  l1.weight: grad_norm = 0.367700
  l1.bias: grad_norm = 0.000664
  l2.weight: grad_norm = 0.366955
Total gradient norm: 1.043430
=== Actor Training Debug (Iteration 8067) ===
Q mean: -11.711317
Q std: 14.451332
Actor loss: 11.715271
Action reg: 0.003954
  l1.weight: grad_norm = 0.286926
  l1.bias: grad_norm = 0.000281
  l2.weight: grad_norm = 0.224917
Total gradient norm: 0.552065
=== Actor Training Debug (Iteration 8068) ===
Q mean: -14.346142
Q std: 17.576578
Actor loss: 14.350111
Action reg: 0.003969
  l1.weight: grad_norm = 0.235912
  l1.bias: grad_norm = 0.000450
  l2.weight: grad_norm = 0.205457
Total gradient norm: 0.502129
=== Actor Training Debug (Iteration 8069) ===
Q mean: -13.986404
Q std: 17.121283
Actor loss: 13.990370
Action reg: 0.003966
  l1.weight: grad_norm = 0.167370
  l1.bias: grad_norm = 0.000568
  l2.weight: grad_norm = 0.146059
Total gradient norm: 0.391296
=== Actor Training Debug (Iteration 8070) ===
Q mean: -13.543308
Q std: 17.017010
Actor loss: 13.547226
Action reg: 0.003918
  l1.weight: grad_norm = 0.379550
  l1.bias: grad_norm = 0.001591
  l2.weight: grad_norm = 0.307968
Total gradient norm: 0.966238
=== Actor Training Debug (Iteration 8071) ===
Q mean: -11.880565
Q std: 13.882832
Actor loss: 11.884462
Action reg: 0.003898
  l1.weight: grad_norm = 0.227574
  l1.bias: grad_norm = 0.000856
  l2.weight: grad_norm = 0.193422
Total gradient norm: 0.462937
=== Actor Training Debug (Iteration 8072) ===
Q mean: -13.393337
Q std: 15.371228
Actor loss: 13.397266
Action reg: 0.003929
  l1.weight: grad_norm = 0.277648
  l1.bias: grad_norm = 0.001609
  l2.weight: grad_norm = 0.260442
Total gradient norm: 0.675769
=== Actor Training Debug (Iteration 8073) ===
Q mean: -12.782302
Q std: 17.213766
Actor loss: 12.786266
Action reg: 0.003964
  l1.weight: grad_norm = 0.217090
  l1.bias: grad_norm = 0.000835
  l2.weight: grad_norm = 0.198134
Total gradient norm: 0.556364
=== Actor Training Debug (Iteration 8074) ===
Q mean: -13.805483
Q std: 16.990278
Actor loss: 13.809437
Action reg: 0.003954
  l1.weight: grad_norm = 0.830964
  l1.bias: grad_norm = 0.000835
  l2.weight: grad_norm = 0.837423
Total gradient norm: 2.095974
=== Actor Training Debug (Iteration 8075) ===
Q mean: -13.523601
Q std: 16.787786
Actor loss: 13.527534
Action reg: 0.003934
  l1.weight: grad_norm = 0.371955
  l1.bias: grad_norm = 0.000457
  l2.weight: grad_norm = 0.335859
Total gradient norm: 0.810277
=== Actor Training Debug (Iteration 8076) ===
Q mean: -12.137188
Q std: 13.780460
Actor loss: 12.141114
Action reg: 0.003926
  l1.weight: grad_norm = 0.196569
  l1.bias: grad_norm = 0.002160
  l2.weight: grad_norm = 0.170869
Total gradient norm: 0.405678
=== Actor Training Debug (Iteration 8077) ===
Q mean: -14.545454
Q std: 16.634899
Actor loss: 14.549389
Action reg: 0.003935
  l1.weight: grad_norm = 0.536692
  l1.bias: grad_norm = 0.000862
  l2.weight: grad_norm = 0.395489
Total gradient norm: 1.082792
=== Actor Training Debug (Iteration 8078) ===
Q mean: -15.334171
Q std: 18.728308
Actor loss: 15.338107
Action reg: 0.003936
  l1.weight: grad_norm = 0.328570
  l1.bias: grad_norm = 0.000882
  l2.weight: grad_norm = 0.278459
Total gradient norm: 0.707493
=== Actor Training Debug (Iteration 8079) ===
Q mean: -13.206580
Q std: 16.922552
Actor loss: 13.210502
Action reg: 0.003922
  l1.weight: grad_norm = 0.362323
  l1.bias: grad_norm = 0.001497
  l2.weight: grad_norm = 0.285194
Total gradient norm: 0.730873
=== Actor Training Debug (Iteration 8080) ===
Q mean: -13.636221
Q std: 15.386402
Actor loss: 13.640182
Action reg: 0.003962
  l1.weight: grad_norm = 0.296219
  l1.bias: grad_norm = 0.001298
  l2.weight: grad_norm = 0.250012
Total gradient norm: 0.644416
=== Actor Training Debug (Iteration 8081) ===
Q mean: -14.440128
Q std: 16.471514
Actor loss: 14.444073
Action reg: 0.003944
  l1.weight: grad_norm = 0.417160
  l1.bias: grad_norm = 0.000351
  l2.weight: grad_norm = 0.322696
